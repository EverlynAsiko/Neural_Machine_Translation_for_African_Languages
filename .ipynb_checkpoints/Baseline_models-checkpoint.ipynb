{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz8WB8I_MZlH"
   },
   "source": [
    "# Summary of Baseline Models \n",
    "\n",
    "**Overview:**\n",
    "1. Text preprocessing\n",
    "2. Inputs of the transformer\n",
    "3. Workings of a transformer: *Submitted write up*\n",
    "4. Results of baseline models\n",
    "\n",
    "Codes are adapted from Masakhane reverse model notebook: https://github.com/masakhane-io/masakhane-mt/blob/master/starter_notebook_into_English_training.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS8ZWHMS7JLs"
   },
   "source": [
    "#### Setting up locations and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1yiVhhB6l_p",
    "outputId": "b9309c7d-7a9d-4aea-ced1-95d69f44a719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Linking to drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7NUnPw-P7-Uz"
   },
   "outputs": [],
   "source": [
    "# Importing needed libraries for preprocessing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "iXfU_ii3__jA",
    "outputId": "a15b2dc2-674e-4625-ab2f-81106f6ff8fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.8.0+cu101\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 763.5 MB 15 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0+cu102\n",
      "    Uninstalling torch-1.9.0+cu102:\n",
      "      Successfully uninstalled torch-1.9.0+cu102\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.8.0+cu101\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "# Install Pytorch with GPU support v1.8.0.\n",
    "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H0QiN_EHEE-s"
   },
   "outputs": [],
   "source": [
    "# Filtering warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q2_ah4uX8JyW"
   },
   "outputs": [],
   "source": [
    "# Loading the drive\n",
    "import os\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D0v_iKryEpww"
   },
   "outputs": [],
   "source": [
    "# Setting source and target languages\n",
    "source_language = \"en\"\n",
    "target_language1 = \"lg\"\n",
    "target_language2 = \"rw\"\n",
    "target_language3 = \"lh\"\n",
    "\n",
    "os.environ[\"src\"] = source_language \n",
    "os.environ[\"tgt1\"] = target_language1\n",
    "os.environ[\"tgt2\"] = target_language2\n",
    "os.environ[\"tgt3\"] = target_language3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djKD3lcy7Amu"
   },
   "source": [
    "# Getting Data\n",
    "\n",
    "JW300 to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk6qPrUeDg2L"
   },
   "outputs": [],
   "source": [
    "! pip install opustools-pkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exPe8zJI9V1d"
   },
   "source": [
    "Helper procedures for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oB37VKOD6I1B"
   },
   "outputs": [],
   "source": [
    "def split_srctgt(df, target_language):\n",
    "  # Splitting train,validation and test\n",
    "  num_valid = int(0.01 * df.shape[0])\n",
    "\n",
    "  dev = df.tail(num_valid) \n",
    "  print(dev.shape)\n",
    "  stripped = df.drop(df.tail(num_valid).index)\n",
    "  test = stripped.tail(num_valid)\n",
    "  print(test.shape)\n",
    "  stripped2 = stripped.drop(stripped.tail(num_valid).index)\n",
    "\n",
    "  # Creating files: Train\n",
    "  with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
    "    for index, row in stripped2.iterrows():\n",
    "      src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "      trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "\n",
    "  # Dev (1%)\n",
    "  with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
    "    for index, row in dev.iterrows():\n",
    "      src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "      trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "\n",
    "  # Test (1%)\n",
    "  with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as trg_file:\n",
    "    for index, row in test.iterrows():\n",
    "      src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "      trg_file.write(row[\"target_sentence\"]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s88HJtkqHwTl"
   },
   "outputs": [],
   "source": [
    "# Code adapted from https://www.geeksforgeeks.org/count-number-of-lines-in-a-text-file-in-python/\n",
    "# Count lines in a file\n",
    "def count_lines(filename):\n",
    "  # Opening a file\n",
    "  file = open(filename,\"r\")\n",
    "  Counter = 0\n",
    "    \n",
    "  # Reading from file\n",
    "  Content = file.read()\n",
    "  CoList = Content.split(\"\\n\")\n",
    "    \n",
    "  for i in CoList:\n",
    "      if i:\n",
    "          Counter += 1\n",
    "            \n",
    "  return Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlau_4qKHxvi"
   },
   "outputs": [],
   "source": [
    "def generating_BPE(source_language, target_language):\n",
    "  # Apply BPE splits to the development and test data.\n",
    "  os.environ[\"src\"] = source_language \n",
    "  os.environ[\"tgt1\"] = target_language\n",
    "  ! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt1 -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt1\n",
    "\n",
    "  # Apply BPE splits to the development and test data.\n",
    "  ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "  ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt1 < train.$tgt1 > train.bpe.$tgt1\n",
    "\n",
    "  ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "  ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt1 < dev.$tgt1 > dev.bpe.$tgt1\n",
    "  ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "  ! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt1 < test.$tgt1 > test.bpe.$tgt1\n",
    "\n",
    "  # Create that vocab using build_vocab\n",
    "  ! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "  ! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt1 --output_path vocab.txt\n",
    "\n",
    "  print('Done generating BPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ZrPmbdBKji"
   },
   "source": [
    "## Luganda   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfSeE3eeb8E"
   },
   "source": [
    "### Turning data from JW300 to dataframe\n",
    "\n",
    "**Do not rerun**: Load pandas dataframe instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGl9jROhLCzo"
   },
   "outputs": [],
   "source": [
    "# Changing to Luganda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvWydhNxLNR_"
   },
   "outputs": [],
   "source": [
    "# Downloading our corpus\n",
    "! opus_read -d JW300 -s $src -t $tgt1 -wm moses -w jw300.$src jw300.$tgt1 -q\n",
    "\n",
    "# extract the corpus file\n",
    "! gunzip JW300_latest_xml_$src-$tgt1.xml.gz\n",
    "\n",
    "# TMX file to dataframe\n",
    "source_file = 'jw300.' + source_language\n",
    "target_file = 'jw300.' + target_language1\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
    "with open(source_file) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Skip sentences that are contained in the test set.\n",
    "        if line.strip() not in en_test_sents:\n",
    "            source.append(line.strip())\n",
    "        else:\n",
    "            skip_lines.append(i)             \n",
    "with open(target_file) as f:\n",
    "    for j, line in enumerate(f):\n",
    "        # Only add to corpus if corresponding source was not skipped.\n",
    "        if j not in skip_lines:\n",
    "            target.append(line.strip())\n",
    "    \n",
    "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
    "    \n",
    "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "df.head(3)\n",
    "\n",
    "# Luganda training set\n",
    "df.to_csv('Luganda.csv',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGimJSBX4PJ5"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "GZsYvkKf5M15",
    "outputId": "955ca620-de30-495e-8323-8c9da50f0025"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "      <td>Akatabo kano tekatundibwa .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "      <td>OMUTWE OGULI KUNGULU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "      <td>Bayibuli yamalirizibwa okuwandiikibwa emyaka n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_sentence                                    target_sentence\n",
       "0                 This publication is not for sale .                        Akatabo kano tekatundibwa .\n",
       "1                                      COVER SUBJECT                               OMUTWE OGULI KUNGULU\n",
       "2  The Bible was completed about two thousand yea...  Bayibuli yamalirizibwa okuwandiikibwa emyaka n..."
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lug = pd.read_csv(\"Luganda.csv\")\n",
    "lug.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kbxd6FvEYtz"
   },
   "outputs": [],
   "source": [
    "# drop duplicate translations\n",
    "df_pp = lug.drop_duplicates()\n",
    "\n",
    "# drop conflicting translations\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Shuffle the data to remove bias in dev set selection.\n",
    "df_pp = df_pp.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRtw68PW7oQF"
   },
   "outputs": [],
   "source": [
    "# reset the index of the training set after previous filtering\n",
    "df_pp.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU2ApSnZDWYl"
   },
   "outputs": [],
   "source": [
    "df_pp.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2wSBFDn_3RM",
    "outputId": "473c0b3f-ef99-4eb2-debf-4bfeaebf2e93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index              0\n",
       "source_sentence    0\n",
       "target_sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsDo5Kh6-8nA",
    "outputId": "4c4a69e6-6753-4616-e95b-55a0b1987d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2270, 3)\n",
      "(2270, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating files for luganda and english\n",
    "split_srctgt(df_pp,target_language1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_gYokMh0-4C",
    "outputId": "f2ea4b40-fd58-4017-ca64-73fd2184b0a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227005, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKdG4XfH_Bqg",
    "outputId": "2bbec219-30a4-4e7f-b166-67b67e9c78e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train files: 222465 222465\n",
      "Number of sentences in valid files: 2270 2270\n",
      "Number of sentences in test files: 2270 2270\n"
     ]
    }
   ],
   "source": [
    "# Luganda files\n",
    "lg_train = count_lines('train.lg')\n",
    "lg_dev = count_lines('dev.lg')\n",
    "lg_test = count_lines('test.lg')\n",
    "\n",
    "print(\"Number of sentences in train files:\", lg_train, count_lines('train.en'))\n",
    "print(\"Number of sentences in valid files:\", lg_dev, count_lines('dev.en'))\n",
    "print(\"Number of sentences in test files:\", lg_test, count_lines('test.en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbUR2xvV4x12",
    "outputId": "b220cd7f-247a-477e-e265-fab231c595c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227005"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_train+lg_dev+lg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0GnjVaB_5a2a",
    "outputId": "bc15f927-9241-4e66-d509-4eb3b913192c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.bpe.en <==\n",
      "Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "At last , I have the st@@ able family life that I always cr@@ av@@ ed , and I have the loving Father that I always wanted .\n",
      "I was a new husband , only 25 years old and very in@@ experienced , but off we went with confidence in Jehovah .\n",
      "What can you do to show these de@@ a@@ f brothers personal attention ?\n",
      "R@@ ef@@ er@@ r@@ ing to what the rul@@ er@@ ship of God’s Son will accompl@@ ish , Isaiah 9 : 7 says : “ The very z@@ eal of Jehovah of arm@@ ies will do this . ”\n",
      "Jesus is the m@@ igh@@ ti@@ est of all of Jehovah’s spirit sons .\n",
      "The ste@@ ad@@ f@@ ast example set by J@@ ac@@ o@@ b and R@@ ac@@ he@@ l no doubt had a powerful effect on their son Joseph , influ@@ enc@@ ing how he would hand@@ le t@@ ests of his own faith .\n",
      "When s@@ ent@@ enc@@ ing “ the orig@@ in@@ al ser@@ p@@ ent , ” Satan the Devil , God said : “ I shall put en@@ m@@ ity between you and the woman and between your se@@ ed and her se@@ ed . He will br@@ u@@ ise you in the head and you will br@@ u@@ ise him in the h@@ ee@@ l . ”\n",
      "W@@ ill this or@@ de@@ al bring David down to S@@ he@@ ol in g@@ ri@@ ef and dis@@ gr@@ ace ?\n",
      "How can Christian love help to streng@@ then the marriage b@@ ond ?\n",
      "\n",
      "==> train.bpe.lg <==\n",
      "Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "N@@ ze ne mukyala wange tuli basanyufu , era nk@@ imanyi nti Katonda anj@@ agala nnyo .\n",
      "Mu kiseera ekyo , nnali nn@@ aak@@ aw@@ asa , nga ndi wa myaka 25 gy@@ okka , era nga s@@ ir@@ ina b@@ um@@ any@@ irivu . Naye nnali muk@@ akafu nti Yakuwa yali ajja ku@@ nn@@ yamba .\n",
      "Mu kibiina k@@ yo bwe mu@@ ba nga mulimu bak@@ igg@@ ala , oyinza kukola ki okulaga nti o@@ faayo ku b’@@ oluganda abo ?\n",
      "Isaaya 9 : 7 wal@@ aga nti Omwana wa Katonda y@@ andibadde Kabaka era nti yand@@ ikol@@ edde abantu ebintu ebirungi bingi .\n",
      "Yesu y’@@ as@@ inga obuyinza mu ba@@ ana ba Yakuwa bonna ab’@@ omwoyo .\n",
      "Eky@@ okulabirako ekirungi Yakobo ne L@@ aak@@ e@@ eri kye baat@@ ek@@ awo mu kw@@ oleka obug@@ umiikiriza ky@@ akwata nnyo ku mutabani waabwe Yusufu , era ekyo ky@@ amu@@ yamba nnyo bwe yay@@ olekagana n’@@ embeera ez@@ aag@@ ez@@ esa okukkiriza kwe .\n",
      "Bwe yali as@@ alira omusango “ omus@@ ot@@ a ogw’@@ edda , ” Setaani Omulyolyomi , Katonda yagamba : “ O@@ bul@@ abe n’@@ abu@@ teek@@ anga wakati wo n’@@ omukazi , era ne wakati w’@@ ez@@ zadde l@@ yo n’@@ ez@@ zadde ly’@@ omukazi : ( ez@@ zadde ly’@@ omukazi ) l@@ iri@@ ku@@ be@@ t@@ ent@@ a omutwe , naawe ol@@ ir@@ ib@@ et@@ ent@@ a ekis@@ inzi@@ iro . ”\n",
      "Em@@ beera eno en@@ zibu en@@ e@@ er@@ eetera Dawudi okuk@@ ka em@@ ag@@ om@@ be nga mun@@ aku@@ w@@ avu ?\n",
      "Okwagala kw’@@ Ekikristaayo ku@@ yinza kutya okuny@@ weza obufumbo ?\n",
      "\n",
      "==> train.en <==\n",
      "Eventually , however , the truths I learned from the Bible began to sink deeper into my heart . I realized that if I wanted to serve Jehovah , I had to change my political viewpoints and associations .\n",
      "At last , I have the stable family life that I always craved , and I have the loving Father that I always wanted .\n",
      "I was a new husband , only 25 years old and very inexperienced , but off we went with confidence in Jehovah .\n",
      "What can you do to show these deaf brothers personal attention ?\n",
      "Referring to what the rulership of God’s Son will accomplish , Isaiah 9 : 7 says : “ The very zeal of Jehovah of armies will do this . ”\n",
      "Jesus is the mightiest of all of Jehovah’s spirit sons .\n",
      "The steadfast example set by Jacob and Rachel no doubt had a powerful effect on their son Joseph , influencing how he would handle tests of his own faith .\n",
      "When sentencing “ the original serpent , ” Satan the Devil , God said : “ I shall put enmity between you and the woman and between your seed and her seed . He will bruise you in the head and you will bruise him in the heel . ”\n",
      "Will this ordeal bring David down to Sheol in grief and disgrace ?\n",
      "How can Christian love help to strengthen the marriage bond ?\n",
      "\n",
      "==> train.lg <==\n",
      "Naye oluvannyuma lw’ekiseera , nnatandika okukolera ku mazima ge nnali njiga , era nnakiraba nti okusobola okuweereza Yakuwa nnalina okuva mu by’obufuzi n’okuleka emikwano emibi gye nnalina .\n",
      "Nze ne mukyala wange tuli basanyufu , era nkimanyi nti Katonda anjagala nnyo .\n",
      "Mu kiseera ekyo , nnali nnaakawasa , nga ndi wa myaka 25 gyokka , era nga sirina bumanyirivu . Naye nnali mukakafu nti Yakuwa yali ajja kunnyamba .\n",
      "Mu kibiina kyo bwe muba nga mulimu bakiggala , oyinza kukola ki okulaga nti ofaayo ku b’oluganda abo ?\n",
      "Isaaya 9 : 7 walaga nti Omwana wa Katonda yandibadde Kabaka era nti yandikoledde abantu ebintu ebirungi bingi .\n",
      "Yesu y’asinga obuyinza mu baana ba Yakuwa bonna ab’omwoyo .\n",
      "Ekyokulabirako ekirungi Yakobo ne Laakeeri kye baatekawo mu kwoleka obugumiikiriza kyakwata nnyo ku mutabani waabwe Yusufu , era ekyo kyamuyamba nnyo bwe yayolekagana n’embeera ezaagezesa okukkiriza kwe .\n",
      "Bwe yali asalira omusango “ omusota ogw’edda , ” Setaani Omulyolyomi , Katonda yagamba : “ Obulabe n’abuteekanga wakati wo n’omukazi , era ne wakati w’ezzadde lyo n’ezzadde ly’omukazi : ( ezzadde ly’omukazi ) lirikubetenta omutwe , naawe oliribetenta ekisinziiro . ”\n",
      "Embeera eno enzibu eneereetera Dawudi okukka emagombe nga munakuwavu ?\n",
      "Okwagala kw’Ekikristaayo kuyinza kutya okunyweza obufumbo ?\n",
      "==> dev.bpe.en <==\n",
      "How important is the Kingdom message ?\n",
      "▪ How Sh@@ ould We “ An@@ s@@ w@@ er E@@ ach Per@@ son ” ?\n",
      "* Jesus made another covenant with them to rule together with him in his Kingdom .\n",
      "How , then , can we keep clean in a world that is mor@@ ally un@@ clean ?\n",
      "It was how they were tre@@ ated .\n",
      "Mat@@ eri@@ al@@ ism may not see@@ m to be an is@@ sue of lo@@ yal@@ ty , but it is .\n",
      "In any case , Psalm 9@@ 0 shows that the life of im@@ perfect humans is sh@@ ort .\n",
      "How can Jesus ’ example help us to de@@ al with the fl@@ aws of others ?\n",
      "E@@ li@@ j@@ ah did not as@@ c@@ end to the heavens that are the spiritual dw@@ ell@@ ing place of Jehovah and his ang@@ el@@ ic sons .\n",
      "That we sim@@ ply cannot s@@ oci@@ al@@ ize with un@@ believers and hope to suff@@ er no ill con@@ se@@ qu@@ ences .\n",
      "\n",
      "==> dev.bpe.lg <==\n",
      "Obu@@ baka bw’@@ Obwakabaka bu@@ kulu kw@@ enk@@ ana wa ?\n",
      "▪ Tuyinza T@@ ut@@ ya “ Oku@@ ddamu Buli M@@ untu ” ?\n",
      "* Yesu yakola end@@ agaano endala nabo basobole okufu@@ g@@ ira awamu naye mu Bwakabaka bwe .\n",
      "Kati olwo tuyinza tutya okusigala nga tuli bay@@ on@@ jo mu nsi et@@ ali nn@@ yon@@ jo mu bya mp@@ isa ?\n",
      "Okus@@ ingira ddala engeri gye baali bay@@ is@@ ib@@ wamu ye y@@ anny@@ amba okuk@@ iraba nti be bantu ba Katonda .\n",
      "Okuba n’@@ omwoyo gw’@@ okwagala ebintu kir@@ emesa omuntu okuba omwesigwa , wadde nga kino si kyangu kulaba .\n",
      "Mu buli ngeri , Zabbuli 9@@ 0 eraga nti obulamu bw’@@ abantu abat@@ atuukiridde b@@ um@@ pi .\n",
      "Tuyinza tutya okukoppa Yesu mu ngeri gye tuy@@ is@@ aamu abalala nga bak@@ oze ensobi ?\n",
      "Eri@@ ya te@@ yag@@ enda mu ggulu Yakuwa awamu ne bamalayika gye bab@@ eera .\n",
      "B@@ itu@@ yigiriza nti te@@ tusobola kwe@@ wala ku@@ gwa mu mit@@ aw@@ aana singa tukola omuk@@ wano ku bantu abat@@ ali bakkiriza .\n",
      "\n",
      "==> dev.en <==\n",
      "If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "Why are false gods valueless ?\n",
      "We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "Daniel , realizing that the training would likely lead to a conflict with Jehovah’s Law , discussed the matter with the official in charge .\n",
      "12 / 1 Bitumen as Mortar , 7 / 1 Bricks in Ancient Egypt , 1 / 1\n",
      "Yes , all who are alive and who are exercising faith in Jesus and the ransom when this system reaches its end will escape the consequences of God’s final day of wrath .\n",
      "Then they had to build the ark , which was no quick task considering its size and the size of Noah’s family .\n",
      "Our God - given work of Kingdom proclamation is a privilege beyond compare .\n",
      "So he urges them : “ Make friends for yourselves by means of the unrighteous riches , so that when such fail , they [ Jehovah and Jesus ] may receive you into the everlasting dwelling places . ”\n",
      "\n",
      "==> dev.lg <==\n",
      "Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "Danyeri bwe yamanya nti okutendekebwa okwo kwandimuleetedde okumenya Amateeka ga Katonda , ensonga eyo yagyogerako n’omukungu eyali avunaanyizibwa .\n",
      "Beera n’Endowooza Ennuŋŋamu ng’Obufumbo Bwo Bulimu Ebizibu , 5 / 15\n",
      "Yee , enteekateeka eno ey’ebintu bw’eriba ezikirizibwa , abo bonna abalisangibwa nga bakkiririza mu Yesu ne mu kinunulo bajja kuwona okuzikirizibwa ku lunaku lw’obusungu bwa Katonda .\n",
      "Oluvannyuma baatandika omulimu gw’okuzimba eryato , ogutaali mwangu bw’olowooza ku bunene bwalyo n’omuwendo gw’abantu abaali mu maka ga Nuuwa .\n",
      "Omulimu gw’okulangirira Obwakabaka bwa Katonda , nkizo ya maanyi nnyo .\n",
      "Eno ye nsonga lwaki yabagamba nti : “ Mwekolere emikwano nga mukozesa eby’obugagga ebitali bya butuukirivu , bwe biggwaawo , [ Yakuwa ne Yesu ] balyoke babasembeze mu bifo eby’okubeeramu eby’olubeerera . ”\n"
     ]
    }
   ],
   "source": [
    "! head train.*\n",
    "! head dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "o49g7MLK9nyR",
    "outputId": "97ad41fc-eaa8-465d-a5f9-c9aba881227c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Luganda/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Requirement already satisfied: numpy==1.20.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.20.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.9.0)\n",
      "Requirement already satisfied: sacrebleu>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.5.1)\n",
      "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.3.7)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (5.4.1)\n",
      "Requirement already satisfied: pylint in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.9.6)\n",
      "Requirement already satisfied: six==1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.12.0)\n",
      "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3) (2.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: astroid<2.7,>=2.6.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (2.6.5)\n",
      "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (5.9.3)\n",
      "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.5->pylint->joeynmt==1.3) (1.6.0)\n",
      "Requirement already satisfied: typed-ast<1.5,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.5->pylint->joeynmt==1.3) (1.4.3)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=0e52ab8a7b6d4cce683e99b4b8ded97fd49ec56afad56e6077345316e246e98c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d1wpidi2/wheels/b8/3e/ec/4da3b842b3679715f7cd3b4065c087c62dd0fcb0ab5f55b80c\n",
      "Successfully built joeynmt\n",
      "Installing collected packages: joeynmt\n",
      "  Attempting uninstall: joeynmt\n",
      "    Found existing installation: joeynmt 1.3\n",
      "    Uninstalling joeynmt-1.3:\n",
      "      Successfully uninstalled joeynmt-1.3\n",
      "Successfully installed joeynmt-1.3\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BczMIOX_BWUK",
    "outputId": "0d64b5c5-1c1a-4857-b580-435003fe202c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating BPE\n"
     ]
    }
   ],
   "source": [
    "generating_BPE(source_language,target_language1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0VWdvDJB-Dx",
    "outputId": "52b826cc-878c-4ff4-fb41-5284efb25138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shareddrives/NMT_for_African_Language/Luganda\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8fPfOMmAVW4",
    "outputId": "bfe11245-ef25-4991-c5d1-612e5fc18a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Luganda Sentences\n",
      "Bal@@ angirira eri abantu amawulire amalungi ag@@ as@@ ing@@ ir@@ ayo ddala obulungi .\n",
      "Am@@ aanyi ga Katonda ge ga@@ as@@ obozesa eky@@ amag@@ ero ekyo okubaawo ng’@@ era bwe ga@@ as@@ ob@@ ozes@@ anga E@@ ris@@ a okukola eby@@ amag@@ ero ng’@@ ak@@ yali mul@@ amu .\n",
      "Ob@@ uk@@ akafu O@@ bul@@ aga nti Katonda A@@ wa Abantu Be O@@ bul@@ agirizi , 4 / 15\n",
      "Abantu abasinga obungi mu nsi teba@@ agal@@ ana era eyo ye nsonga lwaki oluusi bee@@ wuun@@ ya bwe bal@@ aba Abajulirwa ba Yakuwa nga ba@@ agal@@ ana .\n",
      "N’@@ obuvunaanyizibwa ob@@ wat@@ u@@ weebwa Katonda mu maka tulina okweyongera okubu@@ twala nga bu@@ kulu .\n",
      "Combined BPE Vocab\n",
      "taayo\n",
      "meet@@\n",
      "\\\n",
      "ŋ\n",
      "ʺ\n",
      "£\n",
      "”@@\n",
      "Prover@@\n",
      "Ó@@\n",
      "erusaalemi\n"
     ]
    }
   ],
   "source": [
    "# Some output\n",
    "! echo \"BPE Luganda Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt1\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V42gvMwCBO73"
   },
   "source": [
    "## Kinyarwanda  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpcQDoQvZyFf"
   },
   "outputs": [],
   "source": [
    "# Changing to Kinyarwanda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzBBzWMalTpE"
   },
   "source": [
    "### Turning data from JW300 to dataframe\n",
    "\n",
    "**Do not rerun**: Load pandas dataframe instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qu20Nfj7BStY",
    "outputId": "e3617a69-3905-4143-86b3-c6339a971751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-rw.xml.gz not found. The following files are available for downloading:\n",
      "\n",
      "   5 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/en-rw.xml.gz\n",
      " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/en.zip\n",
      "  48 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/rw.zip\n",
      "\n",
      " 316 MB Total size\n",
      "./JW300_latest_xml_en-rw.xml.gz ... 100% of 5 MB\n",
      "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
      "./JW300_latest_xml_rw.zip ... 100% of 48 MB\n"
     ]
    }
   ],
   "source": [
    "# Downloading our corpus\n",
    "! opus_read -d JW300 -s $src -t $tgt2 -wm moses -w jw300.$src jw300.$tgt2 -q\n",
    "\n",
    "# extract the corpus file\n",
    "! gunzip JW300_latest_xml_$src-$tgt2.xml.gz\n",
    "\n",
    "# TMX file to dataframe\n",
    "source_file = 'jw300.' + source_language\n",
    "target_file = 'jw300.' + target_language2\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
    "with open(source_file) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Skip sentences that are contained in the test set.\n",
    "        if line.strip() not in en_test_sents:\n",
    "            source.append(line.strip())\n",
    "        else:\n",
    "            skip_lines.append(i)             \n",
    "with open(target_file) as f:\n",
    "    for j, line in enumerate(f):\n",
    "        # Only add to corpus if corresponding source was not skipped.\n",
    "        if j not in skip_lines:\n",
    "            target.append(line.strip())\n",
    "    \n",
    "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
    "    \n",
    "df2 = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "df2.head(3)\n",
    "\n",
    "# Kinyarwanda training set\n",
    "df2.to_csv('Kinyarwanda.csv',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBqgEGvO4ZVO"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "ecfAcP7E6jE4",
    "outputId": "ccd34c5f-ba1f-48d4-ce28-32f4ebd62317"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Deaf Praise Jehovah</td>\n",
       "      <td>Ibipfamatwi Bisingiza Yehova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BY AWAKE !</td>\n",
       "      <td>BY AWAKE !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CORRESPONDENT IN NIGERIA</td>\n",
       "      <td>CORRESPONDENT IN NIGERIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source_sentence               target_sentence\n",
       "0   The Deaf Praise Jehovah  Ibipfamatwi Bisingiza Yehova\n",
       "1                BY AWAKE !                    BY AWAKE !\n",
       "2  CORRESPONDENT IN NIGERIA      CORRESPONDENT IN NIGERIA"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwa = pd.read_csv(\"Kinyarwanda.csv\")\n",
    "rwa.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44E_RLf16jE5"
   },
   "outputs": [],
   "source": [
    "# drop duplicate translations\n",
    "df_pp = rwa.drop_duplicates()\n",
    "\n",
    "# drop conflicting translations\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Shuffle the data to remove bias in dev set selection.\n",
    "df_pp = df_pp.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQNOmwQo6jE6"
   },
   "outputs": [],
   "source": [
    "# reset the index of the training set after previous filtering\n",
    "df_pp.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dBLmIWEHTnX"
   },
   "outputs": [],
   "source": [
    "df_pp.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7ZVTmQDG4UR",
    "outputId": "8cd09321-6cb5-4b1b-c5c8-c106f817ebc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index              0\n",
       "source_sentence    0\n",
       "target_sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5MG_TDd6jE7",
    "outputId": "fdfa2e89-b18e-4a2c-e4c8-2e18af3e2c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4368, 3)\n",
      "(4368, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating files for Kinyarwanda and english\n",
    "split_srctgt(df_pp, target_language2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Avvf8TIC5qE",
    "outputId": "bb095f86-9bdb-4905-c52d-edb592072e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train files: 428127 428127\n",
      "Number of sentences in valid files: 4368 4368\n",
      "Number of sentences in test files: 4368 4368\n"
     ]
    }
   ],
   "source": [
    "# Kinyarwanda files\n",
    "rw_train = count_lines('train.rw')\n",
    "rw_dev = count_lines('dev.rw')\n",
    "rw_test = count_lines('test.rw')\n",
    "\n",
    "print(\"Number of sentences in train files:\", rw_train, count_lines('train.en'))\n",
    "print(\"Number of sentences in valid files:\", rw_dev, count_lines('dev.en'))\n",
    "print(\"Number of sentences in test files:\", rw_test, count_lines('test.en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKwikPLX2D7R",
    "outputId": "f26e1dcc-9c8b-41de-844f-cd59b6672f2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436863, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_m7g-SWP632r",
    "outputId": "3e941b9d-24cb-469d-e405-76f691d9083b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436863"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw_train+rw_test+rw_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1ST0TsRf6jE8",
    "outputId": "d9c95e73-b195-4c05-880e-ea726a5b3c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.bpe.en <==\n",
      "R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "You will see the time when God br@@ ings righteous rule to all the earth , und@@ o@@ ing the d@@ am@@ age and inj@@ ust@@ ice brought by human rul@@ er@@ ship .\n",
      "Let us consider f@@ ive reas@@ ons why we should want to follow the Christ .\n",
      "Even in the Bible , the id@@ ea of pers@@ u@@ as@@ ion som@@ et@@ im@@ es has n@@ eg@@ ative con@@ no@@ t@@ ations , den@@ ot@@ ing a cor@@ rup@@ ting or a lead@@ ing as@@ tr@@ ay .\n",
      "For God’s servants to be deliv@@ ered , Satan and his ent@@ ire world@@ wide system of things need to be rem@@ ov@@ ed .\n",
      "I had never heard that name used in my ch@@ urch .\n",
      "S@@ imp@@ ly having authority or a wid@@ er name recogn@@ ition is not the important thing .\n",
      "M@@ ost people do not believe in the spir@@ its .\n",
      "And others are encourag@@ ed to be merc@@ if@@ ul , for merc@@ y beg@@ ets merc@@ y . ​ — Luke 6 : 38 .\n",
      "Like such ro@@ o@@ ts in earth@@ ’s no@@ ur@@ ish@@ ing so@@ il , our m@@ inds and hearts need to del@@ v@@ e exp@@ ans@@ ively into God’s Word and d@@ raw from its life - giving wat@@ ers .\n",
      "\n",
      "==> train.bpe.rw <==\n",
      "A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "U@@ z@@ aba uh@@ ari igihe Imana iz@@ ashy@@ iraho ubutegetsi buk@@ iranuka ku isi hose , ik@@ av@@ an@@ aho ibibi n’@@ akar@@ eng@@ ane byat@@ ewe n’@@ ubutegetsi bw’@@ abantu .\n",
      "Nim@@ ucyo dusuzume impamvu esh@@ anu z@@ agombye gutuma tw@@ ifuza guk@@ urikira Kristo .\n",
      "Nd@@ etse no muri Bibiliya , igit@@ ekerezo cyo kw@@ emeza umuntu ikintu , rimwe na rimwe cy@@ umvikana mu buryo bub@@ i , kig@@ as@@ obanura k@@ osh@@ ya , cyangwa kuy@@ ob@@ ya .\n",
      "Kugira ngo abagaragu b’Imana baz@@ ac@@ ung@@ ur@@ we , Satani na gahunda ye y’@@ ibintu yose yo ku isi hose big@@ omba kuv@@ an@@ waho .\n",
      "Mu idini n@@ abag@@ amo sin@@ ari nar@@ igeze n@@ umva bak@@ oresha iryo z@@ ina .\n",
      "G@@ uh@@ abwa ubut@@ ware gusa cyangwa kugira umw@@ anya ukomeye si cyo kintu cy’@@ ingenzi .\n",
      "Abantu benshi ntib@@ emera imy@@ uka .\n",
      "Iyo tug@@ iriye abantu imbabazi na bo bib@@ at@@ era kugira imbabazi , kuko imbabazi zit@@ urwa izindi . — Luka 6 : 38 .\n",
      "Nk’uko iyo m@@ izi ig@@ ab@@ urira ig@@ iti ib@@ iv@@ uye mu but@@ aka buk@@ ung@@ ah@@ aye , ubwenge n’@@ umutima byacu big@@ omba guc@@ eng@@ era mu Ijambo ry’Imana maze bik@@ av@@ om@@ amo amazi atanga ubuzima .\n",
      "\n",
      "==> train.en <==\n",
      "Right after his baptism , he “ went off into Arabia ” ​ — either the Syrian Desert or possibly some quiet place on the Arabian Peninsula that was conducive to meditation .\n",
      "You will see the time when God brings righteous rule to all the earth , undoing the damage and injustice brought by human rulership .\n",
      "Let us consider five reasons why we should want to follow the Christ .\n",
      "Even in the Bible , the idea of persuasion sometimes has negative connotations , denoting a corrupting or a leading astray .\n",
      "For God’s servants to be delivered , Satan and his entire worldwide system of things need to be removed .\n",
      "I had never heard that name used in my church .\n",
      "Simply having authority or a wider name recognition is not the important thing .\n",
      "Most people do not believe in the spirits .\n",
      "And others are encouraged to be merciful , for mercy begets mercy . ​ — Luke 6 : 38 .\n",
      "Like such roots in earth’s nourishing soil , our minds and hearts need to delve expansively into God’s Word and draw from its life - giving waters .\n",
      "\n",
      "==> train.rw <==\n",
      "Ashobora kuba yaragiye ahantu hatuje mu Butayu bwa Siriya cyangwa se wenda ku Mwigimbakirwa wa Arabiya , uri mu burasirazuba bw’Inyanja Itukura , kugira ngo hamufashe gutekereza .\n",
      "Uzaba uhari igihe Imana izashyiraho ubutegetsi bukiranuka ku isi hose , ikavanaho ibibi n’akarengane byatewe n’ubutegetsi bw’abantu .\n",
      "Nimucyo dusuzume impamvu eshanu zagombye gutuma twifuza gukurikira Kristo .\n",
      "Ndetse no muri Bibiliya , igitekerezo cyo kwemeza umuntu ikintu , rimwe na rimwe cyumvikana mu buryo bubi , kigasobanura koshya , cyangwa kuyobya .\n",
      "Kugira ngo abagaragu b’Imana bazacungurwe , Satani na gahunda ye y’ibintu yose yo ku isi hose bigomba kuvanwaho .\n",
      "Mu idini nabagamo sinari narigeze numva bakoresha iryo zina .\n",
      "Guhabwa ubutware gusa cyangwa kugira umwanya ukomeye si cyo kintu cy’ingenzi .\n",
      "Abantu benshi ntibemera imyuka .\n",
      "Iyo tugiriye abantu imbabazi na bo bibatera kugira imbabazi , kuko imbabazi ziturwa izindi . — Luka 6 : 38 .\n",
      "Nk’uko iyo mizi igaburira igiti ibivuye mu butaka bukungahaye , ubwenge n’umutima byacu bigomba gucengera mu Ijambo ry’Imana maze bikavomamo amazi atanga ubuzima .\n",
      "==> dev.bpe.en <==\n",
      "Rather , he end@@ e@@ av@@ or@@ ed to use P@@ ol@@ ish words that were “ very close to every@@ day spe@@ ech . ”\n",
      "□ What prot@@ ection does divine instruc@@ tion provide for young people ?\n",
      "S@@ tr@@ ing@@ ed in@@ str@@ um@@ ents includ@@ ed l@@ ut@@ es , har@@ p@@ s , and ten - str@@ ing@@ ed in@@ str@@ um@@ ents .\n",
      "As the polit@@ ical situation in the B@@ al@@ tic St@@ ates deter@@ i@@ or@@ ated , ant@@ i - Wit@@ ness s@@ ent@@ im@@ ents gre@@ w and our preaching work was ban@@ ned in L@@ ith@@ u@@ an@@ ia as well .\n",
      "N@@ ever@@ th@@ eless , Jesus ass@@ ured his followers that holy spirit would be with them in car@@ ry@@ ing out the work that he had given to them .\n",
      "A decis@@ ion was made based on Bible principles , and this decis@@ ion was un@@ if@@ or@@ m@@ ly accep@@ ted . ​ — Acts 15 : 1 - 29 .\n",
      "B@@ ut@@ ter@@ f@@ ly\n",
      "● Pe@@ ople with com@@ prom@@ ised imm@@ un@@ e sy@@ st@@ ems\n",
      "In add@@ ition , while many in the comm@@ unity view the nam@@ ing cer@@ em@@ on@@ y as an important r@@ ite of pass@@ age , Christians should be s@@ ens@@ itive to the consci@@ ences of others and consider the im@@ press@@ ion that is given to un@@ believers .\n",
      "Why does Isaiah 30 : 21 speak of Jehovah’s word as coming from “ beh@@ ind you , ” since the pre@@ c@@ ed@@ ing ver@@ se Is@@ a 30 : 20 plac@@ es Jehovah in f@@ ron@@ t by saying , “ Your eyes must become eyes see@@ ing your Gr@@ and In@@ struc@@ t@@ or ” ?\n",
      "\n",
      "==> dev.bpe.rw <==\n",
      "Ahubwo y@@ ih@@ at@@ iye gukoresha amagambo y’@@ I@@ g@@ ip@@ ol@@ onye “ as@@ a cyane n’@@ ay@@ ak@@ oresh@@ waga na rub@@ anda . ”\n",
      "□ Ni ub@@ uhe bur@@ inzi inyigisho z@@ iv@@ a ku Mana zih@@ a abak@@ iri bato ?\n",
      "Mu bik@@ oresho by’@@ umuz@@ ika by@@ ak@@ oresh@@ waga , harimo ne@@ bel@@ u , in@@ anga n’@@ in@@ anga z’@@ imir@@ ya ic@@ um@@ i .\n",
      "Uko ibintu byag@@ endaga biz@@ amba muri L@@ itu@@ wan@@ iya , L@@ at@@ iv@@ iya na Es@@ it@@ on@@ iya , ni na ko abantu bar@@ ush@@ ag@@ aho kw@@ anga Abahamya , kandi no muri L@@ itu@@ wan@@ iya umurimo wacu wo kubwiriza wag@@ eze aho ur@@ ab@@ uz@@ anywa .\n",
      "Icyakora , Yesu y@@ ij@@ eje abigishwa be ko umwuka wera wari kub@@ ashy@@ igikira mu gihe bari kuba bas@@ ohoza umurimo yari yab@@ ash@@ inze .\n",
      "H@@ afash@@ we umwanzuro ush@@ ingiye ku mah@@ ame ya Bibiliya kandi amatorero yose yar@@ aw@@ emeye . — Ibyakozwe 15 : 1 - 29 .\n",
      "Ik@@ iny@@ ug@@ uny@@ ugu\n",
      "● Abantu bafite umubiri ufite ubushobozi buk@@ e bwo kur@@ wanya indwara\n",
      "Byongeye kandi , mu gihe abantu benshi babona ko umuh@@ ango wo kwita umwana izina ari umuh@@ ango w’@@ ingenzi uf@@ itanye isano n’uko umwana ava mu but@@ uro bw’@@ imy@@ uka y’@@ abak@@ ur@@ ambere ak@@ aza mu isi y’abantu , Abakristo bo bag@@ ombye kwit@@ ond@@ a ku bw’@@ imit@@ im@@ anama y’@@ abandi kandi bag@@ at@@ ekereza uko abantu bad@@ ah@@ uje ukwizera bari bub@@ if@@ ate .\n",
      "Kuki muri Yesaya 30 : 21 h@@ avuga ko ijambo rya Yehova rit@@ uruka “ inyuma , ” mu gihe umur@@ ongo ub@@ anz@@ iriza uwo uvuga ko Yehova ari imbere ugira uti ‘ amaso yawe az@@ ajya ar@@ eba ukw@@ igisha ’ ?\n",
      "\n",
      "==> dev.en <==\n",
      "“ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "“ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "How will Christ “ go on to victory ” and complete his conquest ?\n",
      "They are very straightforward in thought and action .\n",
      "Gifts were lavished upon them , and the city gave them a large pension for life .\n",
      "Paul pointed out the importance of perseverance in walking with God when he wrote : “ Finally , brothers , we request you and exhort you by the Lord Jesus , just as you received the instruction from us on how you ought to walk and please God , just as you are in fact walking , that you would keep on doing it more fully . ” — 1 Thessalonians 4 : 1 .\n",
      "We Need an Organization Today\n",
      "Find practical ways to offer help to the bereaved\n",
      "Dead too were any expectations that Jesus would free the Jews of the Roman yoke .\n",
      "\n",
      "==> dev.rw <==\n",
      "Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "Ntibakunda guca ku ruhande , haba mu byo batekereza no mu byo bakora .\n",
      "Bahundagazwagaho impano , kandi umujyi wabahaga amafaranga menshi azabatunga kugeza igihe bazapfira .\n",
      "Intumwa Pawulo yagaragaje akamaro ko kwihangana mu kugendana n’Imana , ubwo yandikaga agira ati “ n’uko , bene Data , ibisigaye , turabinginga tubahugurira mu Mwami Yesu , kugira ngo , nk’uko mwabibwiwe natwe uko mukwiriye kugenda no kunezeza Imana , mube ariko mugenda ndeste murusheho . ” ​ —⁠ 1 Abatesalonike 4 : 1 .\n",
      "Muri iki gihe dukeneye umuteguro\n",
      "Ibintu wakora kugira ngo uhumurize uwapfushije\n",
      "Ibyiringiro ibyo ari byo byose by’uko Yesu yari kuzabohora Abayahudi akabakura mu bubata bw’Abaroma na byo byari byayoyotse .\n"
     ]
    }
   ],
   "source": [
    "! head train.*\n",
    "! head dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Ie8EzVjj6jE-",
    "outputId": "7d4b50cd-08bc-4873-93c7-73d579c2ef45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'joeynmt'...\n",
      "remote: Enumerating objects: 3127, done.\u001b[K\n",
      "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
      "remote: Total 3127 (delta 101), reused 142 (delta 91), pack-reused 2951\u001b[K\n",
      "Receiving objects: 100% (3127/3127), 8.09 MiB | 10.21 MiB/s, done.\n",
      "Resolving deltas: 100% (2130/2130), done.\n",
      "Checking out files: 100% (119/119), done.\n",
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language2/Kinyarwanda/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Requirement already satisfied: numpy==1.20.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.20.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.9.0)\n",
      "Requirement already satisfied: sacrebleu>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.5.1)\n",
      "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.3.7)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (5.4.1)\n",
      "Requirement already satisfied: pylint in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.9.6)\n",
      "Requirement already satisfied: six==1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.12.0)\n",
      "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3) (2.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (5.9.3)\n",
      "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: astroid<2.7,>=2.6.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (2.6.5)\n",
      "Requirement already satisfied: typed-ast<1.5,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.5->pylint->joeynmt==1.3) (1.4.3)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.5->pylint->joeynmt==1.3) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=3cacdacaa38544a917b16723dd8b8a0628fa0797665e78eecd6943e22dd857cf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9vd6a6_/wheels/05/d0/4c/bbaa8576251f43522babbaa38ad4d2b15e8b6ec44cbe2c492d\n",
      "Successfully built joeynmt\n",
      "Installing collected packages: joeynmt\n",
      "  Attempting uninstall: joeynmt\n",
      "    Found existing installation: joeynmt 1.3\n",
      "    Uninstalling joeynmt-1.3:\n",
      "      Successfully uninstalled joeynmt-1.3\n",
      "Successfully installed joeynmt-1.3\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rrv8B5fz6jE_",
    "outputId": "845da700-aaa9-4526-80b0-a0e3ce29afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating BPE\n"
     ]
    }
   ],
   "source": [
    "generating_BPE(source_language,target_language2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8TlReca7gzj",
    "outputId": "3388c93e-1cef-4ce7-accd-47f01d0f4e34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6yj6p4s6jFA",
    "outputId": "dee2757d-70b6-4f64-865b-8197fee171c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Kinyarwanda Sentences\n",
      "Ijambo ry’@@ ik@@ igiriki ry@@ ahind@@ uw@@ emo “ urukundo r@@ ur@@ angwa n’@@ ubw@@ uz@@ u , ” ry@@ erekeza ku mur@@ unga ukomeye uh@@ uza abantu bagize umuryango umwe bak@@ und@@ ana kandi b@@ afash@@ anya .\n",
      "( N@@ eh@@ em@@ iya 1 : 1 – 6 : 19 )\n",
      "Mu by’ukuri se , hari iyo igaragaza ?\n",
      "I@@ bintu bibi bit@@ ug@@ eraho twese muri iki gihe byat@@ ewe na cya gik@@ orwa k@@ ibi cyo kw@@ igom@@ eka .\n",
      "B@@ ahawe am@@ ac@@ umbi ir@@ uh@@ ande rw’@@ urus@@ engero .\n",
      "Combined BPE Vocab\n",
      "Ï@@\n",
      "ʺ\n",
      "⁄\n",
      "Ă@@\n",
      "̄@@\n",
      "ointed\n",
      "̆\n",
      "ḥ\n",
      "ḍ@@\n",
      "Ā@@\n"
     ]
    }
   ],
   "source": [
    "# Some output\n",
    "! echo \"BPE Kinyarwanda Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt2\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcIZiMnYTUwH"
   },
   "source": [
    "## Luhyia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOckEV2ylfUb"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_OHPxeVUBu9"
   },
   "outputs": [],
   "source": [
    "# Changing to Luhyia directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "XkuQpTrCXtRw",
    "outputId": "a09b90c3-efaa-4189-e997-fad11b2b4794"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_sentence</th>\n",
       "      <th>source_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7949</th>\n",
       "      <td>Ne omundu yesi naba narusiakhwo likhuwa liosi ...</td>\n",
       "      <td>and if anyone takes away from the words of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7950</th>\n",
       "      <td>Ulia ourusinjia obuloli khumakhuwa kano koosi ...</td>\n",
       "      <td>He who testifies to these things says , “ Sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>Obukoosia obwa Omwami Yesu bube khubandu ba Ny...</td>\n",
       "      <td>The grace of our Lord Jesus Christ be with you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        target_sentence                                    source_sentence\n",
       "7949  Ne omundu yesi naba narusiakhwo likhuwa liosi ...  and if anyone takes away from the words of the...\n",
       "7950  Ulia ourusinjia obuloli khumakhuwa kano koosi ...  He who testifies to these things says , “ Sure...\n",
       "7951  Obukoosia obwa Omwami Yesu bube khubandu ba Ny...  The grace of our Lord Jesus Christ be with you..."
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luh = pd.read_csv(\"Luhya.csv\")\n",
    "luh.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-svP6rWOPys"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the data\n",
    "#  import nltk\n",
    "# nltk.download('punkt')\n",
    "# from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# luh['target_sentence'] = luh['0'].apply(lambda x: ' '.join(word_tokenize(x)))\n",
    "# luh['source_sentence'] = luh['1'].apply(lambda x: ' '.join(word_tokenize(x)))\n",
    "# luh = luh.drop(['0', '1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJOVwI2dYBE1"
   },
   "outputs": [],
   "source": [
    "# drop duplicate translations\n",
    "df_pp = luh.drop_duplicates()\n",
    "\n",
    "# drop conflicting translations\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Shuffle the data to remove bias in dev set selection.\n",
    "df_pp = df_pp.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bV_lgFo4Y-Cz"
   },
   "outputs": [],
   "source": [
    "# reset the index of the training set after previous filtering\n",
    "df_pp.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnK2bzR5Iyrt"
   },
   "outputs": [],
   "source": [
    "df_pp.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQ1BttpTIyrw",
    "outputId": "98c64db8-68a3-416c-f320-ca4a9c99e96f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index              0\n",
       "target_sentence    0\n",
       "source_sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xmV0_jwIyry",
    "outputId": "9525c5c9-965f-4f30-9562-256b703c87ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 3)\n",
      "(79, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating files for Luhya and english\n",
    "split_srctgt(df_pp,target_language3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDEBoePrdiqK",
    "outputId": "d6b88b6b-b4cc-4f7e-83e8-ddae397f8b3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7907, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hh-sBSJZIyr1",
    "outputId": "89342e89-9425-40e3-90d5-c06e0596e441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train files: 7749 7749\n",
      "Number of sentences in valid files: 79 79\n",
      "Number of sentences in test files: 79 79\n"
     ]
    }
   ],
   "source": [
    "# Luhya files\n",
    "lh_train = count_lines('train.lh')\n",
    "lh_dev = count_lines('dev.lh')\n",
    "lh_test = count_lines('test.lh')\n",
    "\n",
    "print(\"Number of sentences in train files:\", lh_train, count_lines('train.en'))\n",
    "print(\"Number of sentences in valid files:\", lh_dev, count_lines('dev.en'))\n",
    "print(\"Number of sentences in test files:\", lh_test, count_lines('test.en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3VXPmZk7wuF",
    "outputId": "3926466f-2102-47be-ee25-90bf38930aae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7907"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lh_train+lh_test+lh_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mgJ5EqjBZmnU",
    "outputId": "e444f952-bb87-436e-9785-652eff6488d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.en <==\n",
      "Then Pilate entered the Praetorium again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "If anyone thinks himself to be a prophet or spiritual , let him acknowledge that the things which I write to you are the commandments of the Lord .\n",
      "Every branch in Me that does not bear fruit He takes away ; and every branch that bears fruit He prunes , that it may bear more fruit .\n",
      "Demetrius has a good testimony from all , and from the truth itself . And we also bear witness , and you know that our testimony is true .\n",
      "And supper being ended , the devil having already put it into the heart of Judas Iscariot , Simon ’ s son , to betray Him ,\n",
      "imploring us with much urgency that we would receive the gift and the fellowship of the ministering to the saints .\n",
      "It is written in the prophets , ‘ And they shall all be taught by God. ’ Therefore everyone who has heard and learned from the Father comes to Me .\n",
      "For those who are such do not serve our Lord Jesus Christ , but their own belly , and by smooth words and flattering speech deceive the hearts of the simple .\n",
      "So when he had received food , he was strengthened . Then Saul spent some days with the disciples at Damascus .\n",
      "Therefore if you have not been faithful in the unrighteous mammon , who will commit to your trust the true riches ?\n",
      "\n",
      "==> train.lh <==\n",
      "Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "Omundu yesi owilolanga mbu , nomurumwa , wa Nyasaye noho mbu ali neshihaanwa eshia Roho okhuula amanye khandi afuchilile mbu , aka , emuhandichilanga kano nelilako elia Omwami .\n",
      "Aremanga buli lisaka , mwisie elilamanga ebiamo ta , ne akhalilanga buli lisaka , eliamanga ebiamo kho mbu , libe lilayi nilimeeta okhwama , ebiamo ebinji .\n",
      "Buli mundu amwitsoominjia Demeterio ; ne obwatieli , bwene bumwitsoominjia . Ne nasi emeetakhwo obuloli , bwanje , ne mumanyile mbu akemboola nakatoto . Amashesio Kokhumalilisia ,\n",
      "Yesu nende abeechibe bali nibaliitsanga eshiokhulia , eshia hamukoloba . Setani yali namalile okhura mu Yuda omwana wa Simoni Isikarioti amapaaro kokhukhoba Yesu. ,\n",
      "bakhusaba nibakhusaaya okhufuchililwa okhusanga , mukhukhonya abakristo bashiabwe aba Yudea .\n",
      "Abalakusi , bahandika mbu , ‘ Buli mundu aliechesibwa nende , Nyasaye. ’ Kho oyo yesi ouhulilanga aka Papa nende , okhweka okhurula khuye , yetsa khwisie .\n",
      "Okhuba , abakholanga amakhuwa kario shibakhalabanilanga Kristo , Omwami wefwe ta , habula bakhalabanilanga tsinda , tsiabwe abeene . Bekhoonyelanga amakhuwa kabwe , kokhukaatilisia nende akokhulaha khulwa okhukaatia , amapaaro kabateshele .\n",
      "Ne olwa yamala , okhulia eshiokhulia , omubilikwe kwanyoola amaani . Saulo ayaala Injiili Damasiko Saulo yamenya Damasiko halala nabasuubili khulwa , tsinyanga tsindiiti .\n",
      "Kho , nimulaba abesiikwa mubuyinda , bwomushialo shino ta , mwakhaba murie abesiikwa , mubuyinda bwatoto ?\n",
      "==> dev.en <==\n",
      "and the cares of this world , the deceitfulness of riches , and the desires for other things entering in choke the word , and it becomes unfruitful .\n",
      "Now concerning the ministering to the saints , it is superfluous for me to write to you ;\n",
      "nor thieves , nor covetous , nor drunkards , nor revilers , nor extortioners will inherit the kingdom of God .\n",
      "Therefore , in the resurrection , when they rise , whose wife will she be ? For all seven had her as wife . ”\n",
      "Then he said , “ Lord , I believe ! ” And he worshiped Him .\n",
      "And many of them said , “ He has a demon and is mad . Why do you listen to Him ? ”\n",
      "So we may boldly say : “ The Lord is my helper ; I will not fear.What can man do to me ? ”\n",
      "So when Jesus heard these things , He said to him , “ You still lack one thing . Sell all that you have and distribute to the poor , and you will have treasure in heaven ; and come , follow Me . ”\n",
      "Immediately he went up to Jesus and said , “ Greetings , Rabbi ! ” and kissed Him .\n",
      "looking for the blessed hope and glorious appearing of our great God and Savior Jesus Christ ,\n",
      "\n",
      "==> dev.lh <==\n",
      "habula okhuhendela amakhuwa komushialo okhuchama obuyinda nende obwikombi bwebindu bindi binjilanga mubo mana bitiiya likhuwa elo nibilikhola , okhulekha okhwama ebiamo .\n",
      "Shibulaho eshichila enzililile okhumuhandichila , khwikhuwa liobukhoonyi khubakristo ba Yudea ,\n",
      "abeefi abalang'u , abameesi , abachikhanga abashiabwe nende , abanuuli , shibalinyoola obwami bwa Nyasaye tawe. ,\n",
      "Kho , khunyanga yobulamushi , omukhasi oyo yakhabe owa , wina khubandu musafu abo , shichila yali omukhasi owa , boosi musafu ? ”\n",
      "Naye naboola ari , “ Omwami , esuubiile. ” Mana , nasikama hasi niyenamila Yesu .\n",
      "Abanji khubo baboola bari , “ Ali neshishieno khandi nomulalu ! , Eshichila nimurecheresia akaboolanga nishina ? ”\n",
      "Kho chende khume ikholo nikhuboola , khuri “ Omwami niye omuhabini wanje shindalaritsanga tawe. , Omundu namundu anyala okhukhola shiina ? ”\n",
      "Ne olwa Yesu yahulila ario , yamuboolela ari “ Oshileemilekhwo eshindu shilala eshiokhukhola . Tsia , okusie ebindu biosi ebia oli ninabio , mana okabile , abamanani amapesa konyoolamwo . Ne olaba nelibiishilo , mwikulu ; mana witse unondekhwo . ”\n",
      "Yuda natsia shilunji khu Yesu namuboolela ari “ Omulembe kube khwiwe Omwechesia , ” ne , namufumbeelela .\n",
      "Khwitsuulemwo obunaayi nikhulindililanga Inyanga yokhukalukha khwoluyali olwa , Nyasaye wefwe omukhongo nende Omuhonia wefwe Yesu , Kristo .\n",
      "==> test.en <==\n",
      "And I heard the number of those who were sealed . One hundred and forty-four thousand of all the tribes of the children of Israel were sealed :\n",
      "For He will finish the work and cut it short in righteousness , Because the Lord will make a short work upon the earth . ”\n",
      "But be that as it may , I did not burden you . Nevertheless , being crafty , I caught you by cunning !\n",
      "He also swore to her , “ Whatever you ask me , I will give you , up to half my kingdom . ”\n",
      "Remember those who rule over you , who have spoken the word of God to you , whose faith follow , considering the outcome of their conduct .\n",
      "he himself shall also drink of the wine of the wrath of God , which is poured out full strength into the cup of His indignation . He shall be tormented with fire and brimstone in the presence of the holy angels and in the presence of the Lamb .\n",
      "For David himself said by the Holy Spirit : ‘ The Lord said to my Lord , “ Sit at My right hand , Till I make Your enemies Your footstool . ” ’\n",
      "You ask and do not receive , because you ask amiss , that you may spend it on your pleasures .\n",
      "But this He spoke concerning the Spirit , whom those believing in Him would receive ; for the Holy Spirit was not yet given , because Jesus was not yet glorified .\n",
      "Now you , therefore , together with the council , suggest to the commander that he be brought down to you tomorrow , as though you were going to make further inquiries concerning him ; but we are ready to kill him before he comes near . ”\n",
      "\n",
      "==> test.lh <==\n",
      "Mana , nemboolelwa mbu obunji obwa abo abarebwakhwo , eshibalikho eshia Nyasaye khubweni bwabwe bali , ebikhumila eshikhumi amakhumi kane nabine . , Barula mutsimbia tsiosi etsia Abaisiraeli ekhumi natsibili ; ,\n",
      "okhuba Omwami alabakhalachila eshiina , lwangu abomushialo boosi . ”\n",
      "Mulafuchilisania ninasi , mbu shindamusitohela tawe . Nebutswa abandi baboola , mbu ndali omuchesichesi , ne nemutila nobubeeyi .\n",
      "Mana namwitsubila ari , “ Shiosi shiosi , shiosaba ndalakhuhelesia , kata niyakhaba inusu , yoburuchi bwanje . ”\n",
      "Mwitsulilenje abo abalinji abemilisi benywe khale abamuboolela oburume obwa Nyasaye . Yiliilisiekhwo , shinga olwa bamenyanga khandi shinga olwa bafwa ; mana , mulonde khubusuubili bwabwe .\n",
      "yesi , okhuula alanywa idivai yoburuma obwa Nyasaye eyitsushilwe mushikombe shioburuma obwa Nyasaye niyilatsokasibwemwo eshindu shiosi ta . Ne alanyasibwa , mumulilo nende mubunoro obusambanga imbeli , wabamalaika abatakatifu nende imbeli weshimeeme. ,\n",
      "Daudi , mwene niyemiliilwe nende Roho Omutakatifu , yarumbula , ari ‘ OMWAMI Nyasaye yaboolela Omwami wanje ari yikhala khumukhono kwanje omusaatsa okhuula olwa ndakhare abasukubo hasi webilenjebio ! ’ ,\n",
      "Ne olwa , musaba , shimunyolanga ta , shichila amapaaro kenywe , namabi ; musabanga ebiamwenya okhukholela emienya , chienywe abeene .\n",
      "Yesu yali , naboolanga amakhuwa ako khu Roho , owa , abali , nibamusuubila bali nibatsitsanga okhunyoola . Kata kario , Roho tsana oyo yali nashili okhuhaanwa ta , shichila Yesu , yali nashili okhuchinguulwa muluyali . Abandu bahukhana khu aka Yesu ,\n",
      "Khulweshio , inywe nende , abeshiina rumasie amakhuwa khumusinjilili welihe wa , Roma areere Paulo khwinywe , ne mwikaatie mbu , mwenyanga okhukhalaka eshiinashie habwene . Nefu , khulaba nikwirecheshe okhumwiira nashili kata okhuula , hano tawe . ”\n"
     ]
    }
   ],
   "source": [
    "! head train.*\n",
    "! head dev.*\n",
    "! head test.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "AgQOGzLyZwKT",
    "outputId": "9a29d5ad-7917-49be-99c5-1869da51a175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'joeynmt'...\n",
      "remote: Enumerating objects: 3127, done.\u001b[K\n",
      "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
      "remote: Total 3127 (delta 101), reused 142 (delta 91), pack-reused 2951\u001b[K\n",
      "Receiving objects: 100% (3127/3127), 8.09 MiB | 2.56 MiB/s, done.\n",
      "Resolving deltas: 100% (2130/2130), done.\n",
      "Checking out files: 100% (119/119), done.\n",
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language2/Luhya/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Requirement already satisfied: numpy==1.20.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.20.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.9.0)\n",
      "Requirement already satisfied: sacrebleu>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.5.1)\n",
      "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.3.7)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (5.4.1)\n",
      "Requirement already satisfied: pylint in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.9.6)\n",
      "Requirement already satisfied: six==1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.12.0)\n",
      "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3) (2.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (5.9.3)\n",
      "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: astroid<2.7,>=2.6.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (2.6.5)\n",
      "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Requirement already satisfied: typed-ast<1.5,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.5->pylint->joeynmt==1.3) (1.4.3)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.5->pylint->joeynmt==1.3) (1.6.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=cc548e225f1e5f8856e2d0c6d56bbbdce9da7e0dd865c8fdc9e58161c27d4d0f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-iy1o87m9/wheels/1a/da/7f/b69f82575dcbb509b7f08d1fac43b3c6e9dd4fa32d257ab360\n",
      "Successfully built joeynmt\n",
      "Installing collected packages: joeynmt\n",
      "  Attempting uninstall: joeynmt\n",
      "    Found existing installation: joeynmt 1.3\n",
      "    Uninstalling joeynmt-1.3:\n",
      "      Successfully uninstalled joeynmt-1.3\n",
      "Successfully installed joeynmt-1.3\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7pdwWNLaLS6",
    "outputId": "db246251-477a-4953-ef88-f6647e2d01b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating BPE\n"
     ]
    }
   ],
   "source": [
    "generating_BPE(source_language,target_language3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLjVDZNo7_S3",
    "outputId": "a4a20c39-0e32-4ae0-abaa-434e62f1fa14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shareddrives/NMT_for_African_Language/Luhya\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYFWd4nDbgD2",
    "outputId": "1246be2d-d07d-4c3e-c424-aeb014e0aa7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Luhya Sentences\n",
      "B@@ ach@@ am@@ anga , okhwikh@@ ala khub@@ if@@ umb@@ i habundu w@@ oluyali mum@@ as@@ abo , nende ebif@@ umb@@ i bi@@ oluyali bi@@ e@@ imbeli mutsis@@ in@@ agog@@ i. ,\n",
      "Mana abandu abali nibe@@ mi@@ ile imbeli nibamu@@ h@@ alab@@ ila , nibamuboolela mbu ah@@ ol@@ eele ts@@ i , nebutswa y@@ ame@@ eta , butswa okhul@@ anjilisia obutinyu ari , “ Omwana wa Daudi ! , W@@ umb@@ eele tsimbabasi ! ”\n",
      "Mana , nibe@@ mba ol@@ wimb@@ o oluyia bari “ N@@ iwe ou@@ kw@@ anile okhu@@ bukula eshi@@ tabu eshik@@ anye , nende okhw@@ ik@@ ula ebib@@ ali@@ kho bi@@ ashi@@ o. , Okhuba w@@ erwa , ne khulwa okhufw@@ akhwo khw@@ eshit@@ is@@ o , w@@ areera khu Nyasaye abandu okhurula mu@@ buli olwibulo olul@@ imi amahanga nende okhurula mu@@ tsimbia tsi@@ osi. ,\n",
      "habula ow@@ enya , okhuba omukhongo mwinywe , okhuula abe omukhalabani , w@@ ab@@ oosi .\n",
      "A@@ bukula ob@@ ise bi@@ hel@@ ile okhwi@@ h@@ enga lik@@ ond@@ ol@@ ie , ne olwa , ar@@ ulaho , y@@ ebil@@ ila bwangu shinga lw@@ obw@@ en@@ ibwe bu@@ f@@ wan@@ a. ,\n",
      "Combined BPE Vocab\n",
      "tside\n",
      "Jac@@\n",
      "Dav@@\n",
      "blas@@\n",
      "multitud@@\n",
      "syn@@\n",
      "speak@@\n",
      "toge@@\n",
      "til\n",
      "welve\n"
     ]
    }
   ],
   "source": [
    "# Some output\n",
    "! echo \"BPE Luhya Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt3\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9VOJvLygjfG"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLaDEY0U6kzN"
   },
   "source": [
    "## Luganda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "piNsl_221dNI"
   },
   "outputs": [],
   "source": [
    "# Changing to Luganda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMcloryKqBoT",
    "outputId": "8124a5bc-5d9e-4e5a-b9a3-0bb6aa502cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "V0kQqdMr1xIw",
    "outputId": "889f0dd1-75b7-424e-faf1-86d82feac84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 96 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 22.2 MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 6.8 MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.9.6-py3-none-any.whl (375 kB)\n",
      "\u001b[K     |████████████████████████████████| 375 kB 49.2 MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Collecting astroid<2.7,>=2.6.5\n",
      "  Downloading astroid-2.6.6-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 48.7 MB/s \n",
      "\u001b[?25hCollecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.9.3-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 53.3 MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
      "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
      "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 39.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=1950631f414af3a2d7d975f0a5b642704ba452e4e39e5528d82301ca50b576e9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0jrte8c0/wheels/5a/e3/58/127db8e0efc85b7b7d68a71532b4707175984a9b88277d1d7f\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68433 sha256=c9d74e1995919b7f5e6b9aa11d4331b78db1382ecbbb57b75ac56b8a4e615947\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
      "Successfully built joeynmt wrapt\n",
      "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
      "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed astroid-2.6.6 isort-5.9.3 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.6 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "code",
    "id": "M4pO-ZQogqZU"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "path = \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\"\n",
    "name = '%s%s' % (target_language1, source_language)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language1}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language1}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"{path}/train.bpe\"\n",
    "    dev:   \"{path}/dev.bpe\"\n",
    "    test:  \"{path}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"{path}/vocab.txt\"\n",
    "    trg_vocab: \"{path}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"joeynmt/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 1000\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 2000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 200\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer\"\n",
    "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, path=path, source_language=source_language, target_language1=target_language1)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fq3kiHP1Bj6p",
    "outputId": "e40724f6-bb14-4f7e-ad7c-a9f6017df3f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-01 17:28:14,592 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-01 17:28:14,630 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-01 17:28:18,793 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-01 17:28:19,061 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-01 17:28:19,108 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-01 17:28:19,132 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-01 17:28:19,133 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-01 17:28:19,337 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-01 17:28:19.489900: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-01 17:28:20,568 - INFO - joeynmt.training - Total params: 12151040\n",
      "2021-08-01 17:28:23,821 - INFO - joeynmt.helpers - cfg.name                           : lgen_reverse_transformer\n",
      "2021-08-01 17:28:23,821 - INFO - joeynmt.helpers - cfg.data.src                       : lg\n",
      "2021-08-01 17:28:23,821 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-01 17:28:23,821 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\n",
      "2021-08-01 17:28:23,821 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\n",
      "2021-08-01 17:28:23,821 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\n",
      "2021-08-01 17:28:23,822 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-01 17:28:23,822 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-01 17:28:23,822 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-01 17:28:23,822 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-08-01 17:28:23,822 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-08-01 17:28:23,822 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-01 17:28:23,822 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-01 17:28:23,823 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-01 17:28:23,824 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-01 17:28:23,824 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-01 17:28:23,824 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-01 17:28:23,824 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-01 17:28:23,824 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-01 17:28:23,824 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-01 17:28:23,824 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-01 17:28:23,825 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-01 17:28:23,825 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-01 17:28:23,825 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-01 17:28:23,825 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-01 17:28:23,825 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-01 17:28:23,825 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-01 17:28:23,825 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-01 17:28:23,825 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-01 17:28:23,826 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lgen_reverse_transformer\n",
      "2021-08-01 17:28:23,826 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-01 17:28:23,826 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-01 17:28:23,826 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-01 17:28:23,826 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-01 17:28:23,826 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-01 17:28:23,826 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-01 17:28:23,826 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-01 17:28:23,827 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-01 17:28:23,827 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-01 17:28:23,827 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-01 17:28:23,827 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-01 17:28:23,827 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-01 17:28:23,827 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-01 17:28:23,827 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-01 17:28:23,827 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-01 17:28:23,828 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-01 17:28:23,828 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-01 17:28:23,828 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-01 17:28:23,828 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-01 17:28:23,828 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-01 17:28:23,828 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-01 17:28:23,829 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-01 17:28:23,829 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-01 17:28:23,829 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-01 17:28:23,829 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-01 17:28:23,830 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-01 17:28:23,830 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-01 17:28:23,830 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-01 17:28:23,830 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-01 17:28:23,830 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-01 17:28:23,830 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-01 17:28:23,830 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 222129,\n",
      "\tvalid 2270,\n",
      "\ttest 2270\n",
      "2021-08-01 17:28:23,831 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ itical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-01 17:28:23,831 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-01 17:28:23,831 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-01 17:28:23,831 - INFO - joeynmt.helpers - Number of Src words (types): 4261\n",
      "2021-08-01 17:28:23,831 - INFO - joeynmt.helpers - Number of Trg words (types): 4261\n",
      "2021-08-01 17:28:23,832 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4261),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4261))\n",
      "2021-08-01 17:28:23,857 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-01 17:28:23,858 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-01 17:28:49,032 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.445251, Tokens per Sec:    16857, Lr: 0.000300\n",
      "2021-08-01 17:29:14,482 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.046136, Tokens per Sec:    17015, Lr: 0.000300\n",
      "2021-08-01 17:29:40,296 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.329511, Tokens per Sec:    16643, Lr: 0.000300\n",
      "2021-08-01 17:30:06,860 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.578785, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-01 17:30:33,725 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.192496, Tokens per Sec:    16152, Lr: 0.000300\n",
      "2021-08-01 17:31:00,524 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.347645, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-08-01 17:31:27,110 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.229730, Tokens per Sec:    16376, Lr: 0.000300\n",
      "2021-08-01 17:31:53,610 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.021344, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-08-01 17:32:20,188 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.957528, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-01 17:32:46,618 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.065270, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-08-01 17:34:34,583 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 17:34:34,583 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 17:34:34,583 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 17:34:35,294 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 17:34:35,295 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 17:34:36,111 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 17:34:36,113 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 17:34:36,114 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 17:34:36,114 - INFO - joeynmt.training - \tHypothesis: We can be a person in the way of the spirit , we will be a person , and we have been been been been been been been to be a person .\n",
      "2021-08-01 17:34:36,114 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 17:34:36,114 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 17:34:36,115 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 17:34:36,115 - INFO - joeynmt.training - \tHypothesis: Jesus , we can be a Bible Bible Bible Bible and to be a Bible - based news .\n",
      "2021-08-01 17:34:36,115 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 17:34:36,115 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 17:34:36,116 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 17:34:36,116 - INFO - joeynmt.training - \tHypothesis: Why is the good news of the congregation ?\n",
      "2021-08-01 17:34:36,116 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 17:34:36,116 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 17:34:36,117 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 17:34:36,117 - INFO - joeynmt.training - \tHypothesis: We have been been been been been been been been been been been been been been been been to the good news .\n",
      "2021-08-01 17:34:36,117 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2000: bleu:   1.91, loss: 225537.1250, ppl:  42.4846, duration: 109.4988s\n",
      "2021-08-01 17:35:03,136 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.870050, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-01 17:35:29,664 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.637267, Tokens per Sec:    16328, Lr: 0.000300\n",
      "2021-08-01 17:35:56,522 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.413856, Tokens per Sec:    16281, Lr: 0.000300\n",
      "2021-08-01 17:36:14,293 - INFO - joeynmt.training - Epoch   1: total training loss 11779.67\n",
      "2021-08-01 17:36:14,293 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-01 17:36:23,478 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     3.593796, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-08-01 17:36:50,022 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     3.718138, Tokens per Sec:    15997, Lr: 0.000300\n",
      "2021-08-01 17:37:16,785 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     3.716854, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-08-01 17:37:43,017 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     3.213428, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-01 17:38:09,847 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     3.163186, Tokens per Sec:    16295, Lr: 0.000300\n",
      "2021-08-01 17:38:36,566 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.321622, Tokens per Sec:    16458, Lr: 0.000300\n",
      "2021-08-01 17:39:03,095 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     3.312460, Tokens per Sec:    16238, Lr: 0.000300\n",
      "2021-08-01 17:40:03,545 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 17:40:03,546 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 17:40:03,546 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 17:40:04,202 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 17:40:04,202 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 17:40:05,300 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 17:40:05,302 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 17:40:05,302 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 17:40:05,302 - INFO - joeynmt.training - \tHypothesis: When we learn about the faith and faith , we have to imitate our faith in our faith , we need to imitate him .\n",
      "2021-08-01 17:40:05,302 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 17:40:05,303 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 17:40:05,303 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 17:40:05,303 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to learn from the Bible and to help us to be able to be a good news .\n",
      "2021-08-01 17:40:05,304 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 17:40:05,304 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 17:40:05,304 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 17:40:05,304 - INFO - joeynmt.training - \tHypothesis: Why are the people of people who are not able ?\n",
      "2021-08-01 17:40:05,305 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 17:40:05,305 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 17:40:05,305 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 17:40:05,305 - INFO - joeynmt.training - \tHypothesis: Jehovah is not a love for love for love for love .\n",
      "2021-08-01 17:40:05,306 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     4000: bleu:   6.79, loss: 186507.2188, ppl:  22.2055, duration: 62.2096s\n",
      "2021-08-01 17:40:32,021 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     3.440751, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-01 17:40:58,713 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     3.192516, Tokens per Sec:    15886, Lr: 0.000300\n",
      "2021-08-01 17:41:25,227 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     3.404165, Tokens per Sec:    16307, Lr: 0.000300\n",
      "2021-08-01 17:41:51,771 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     3.045619, Tokens per Sec:    16242, Lr: 0.000300\n",
      "2021-08-01 17:42:18,190 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     3.284563, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-01 17:42:44,783 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     3.289277, Tokens per Sec:    16265, Lr: 0.000300\n",
      "2021-08-01 17:43:11,380 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     3.324648, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-01 17:43:21,203 - INFO - joeynmt.training - Epoch   2: total training loss 8939.79\n",
      "2021-08-01 17:43:21,204 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-01 17:43:38,121 - INFO - joeynmt.training - Epoch   3, Step:     5600, Batch Loss:     3.034959, Tokens per Sec:    15640, Lr: 0.000300\n",
      "2021-08-01 17:44:04,547 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     2.936081, Tokens per Sec:    16142, Lr: 0.000300\n",
      "2021-08-01 17:44:31,336 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     3.268961, Tokens per Sec:    16166, Lr: 0.000300\n",
      "2021-08-01 17:45:30,012 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 17:45:30,013 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 17:45:30,013 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 17:45:30,628 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 17:45:30,628 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 17:45:31,384 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 17:45:31,385 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 17:45:31,386 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 17:45:31,386 - INFO - joeynmt.training - \tHypothesis: When we learn from faith and faith , we must imitate him , and we can imitate him .\n",
      "2021-08-01 17:45:31,386 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 17:45:31,386 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 17:45:31,387 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 17:45:31,387 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible and to help us to share in the Scriptures .\n",
      "2021-08-01 17:45:31,387 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 17:45:31,387 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 17:45:31,388 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 17:45:31,388 - INFO - joeynmt.training - \tHypothesis: Why is the people of false religion ?\n",
      "2021-08-01 17:45:31,388 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 17:45:31,388 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 17:45:31,389 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 17:45:31,389 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with love .\n",
      "2021-08-01 17:45:31,389 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     6000: bleu:   9.80, loss: 169341.7656, ppl:  16.6931, duration: 60.0526s\n",
      "2021-08-01 17:45:58,382 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     2.932145, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-01 17:46:24,951 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     2.955273, Tokens per Sec:    16231, Lr: 0.000300\n",
      "2021-08-01 17:46:51,671 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     3.132366, Tokens per Sec:    16318, Lr: 0.000300\n",
      "2021-08-01 17:47:18,215 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     2.816672, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-08-01 17:47:44,762 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     3.225974, Tokens per Sec:    16509, Lr: 0.000300\n",
      "2021-08-01 17:48:11,048 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     3.088301, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-01 17:48:37,730 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     2.963092, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-08-01 17:49:04,269 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     3.009310, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-08-01 17:49:30,926 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     2.859577, Tokens per Sec:    16315, Lr: 0.000300\n",
      "2021-08-01 17:49:57,415 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     2.847639, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-01 17:50:54,208 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 17:50:54,209 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 17:50:54,209 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 17:50:54,790 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 17:50:54,790 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 17:50:55,870 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 17:50:55,871 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 17:50:55,871 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 17:50:55,871 - INFO - joeynmt.training - \tHypothesis: When we consider how we imitate Jesus ’ faith and imitate him , we can imitate him .\n",
      "2021-08-01 17:50:55,871 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 17:50:55,871 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 17:50:55,872 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 17:50:55,872 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the Scriptures and to make a good reason for the Scriptures .\n",
      "2021-08-01 17:50:55,872 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 17:50:55,872 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 17:50:55,872 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 17:50:55,872 - INFO - joeynmt.training - \tHypothesis: Why are the creation of false religion ?\n",
      "2021-08-01 17:50:55,873 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 17:50:55,873 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 17:50:55,873 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 17:50:55,873 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a perfect relationship with his people .\n",
      "2021-08-01 17:50:55,873 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     8000: bleu:  11.67, loss: 158586.5156, ppl:  13.9602, duration: 58.4578s\n",
      "2021-08-01 17:51:22,899 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     2.093415, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-01 17:51:23,841 - INFO - joeynmt.training - Epoch   3: total training loss 7986.00\n",
      "2021-08-01 17:51:23,842 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-01 17:51:49,924 - INFO - joeynmt.training - Epoch   4, Step:     8400, Batch Loss:     2.913724, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-01 17:52:16,564 - INFO - joeynmt.training - Epoch   4, Step:     8600, Batch Loss:     2.916263, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-08-01 17:52:43,147 - INFO - joeynmt.training - Epoch   4, Step:     8800, Batch Loss:     2.849996, Tokens per Sec:    16300, Lr: 0.000300\n",
      "2021-08-01 17:53:09,575 - INFO - joeynmt.training - Epoch   4, Step:     9000, Batch Loss:     2.893636, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-01 17:53:36,262 - INFO - joeynmt.training - Epoch   4, Step:     9200, Batch Loss:     2.437320, Tokens per Sec:    16370, Lr: 0.000300\n",
      "2021-08-01 17:54:02,660 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     2.737631, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-08-01 17:54:29,292 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     2.462929, Tokens per Sec:    16286, Lr: 0.000300\n",
      "2021-08-01 17:54:55,808 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     2.638008, Tokens per Sec:    16252, Lr: 0.000300\n",
      "2021-08-01 17:55:22,680 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     2.318091, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-08-01 17:56:09,203 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 17:56:09,203 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 17:56:09,203 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 17:56:09,819 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 17:56:09,819 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 17:56:10,539 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 17:56:10,540 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 17:56:10,540 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 17:56:10,540 - INFO - joeynmt.training - \tHypothesis: When we discuss how faith and imitate him , we are able to think .\n",
      "2021-08-01 17:56:10,540 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 17:56:10,541 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 17:56:10,541 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 17:56:10,541 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible and to make decisions that is possible .\n",
      "2021-08-01 17:56:10,541 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 17:56:10,542 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 17:56:10,542 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 17:56:10,542 - INFO - joeynmt.training - \tHypothesis: Why are the creation of false religion ?\n",
      "2021-08-01 17:56:10,542 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 17:56:10,543 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 17:56:10,543 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 17:56:10,543 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with his people .\n",
      "2021-08-01 17:56:10,543 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    10000: bleu:  13.83, loss: 150443.3125, ppl:  12.1928, duration: 47.8629s\n",
      "2021-08-01 17:56:37,433 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     2.741991, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-08-01 17:57:04,276 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     2.667877, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-08-01 17:57:30,708 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     2.757224, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-01 17:57:57,442 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     2.698728, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-08-01 17:58:15,726 - INFO - joeynmt.training - Epoch   4: total training loss 7443.93\n",
      "2021-08-01 17:58:15,726 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-01 17:58:24,222 - INFO - joeynmt.training - Epoch   5, Step:    11000, Batch Loss:     2.074014, Tokens per Sec:    15435, Lr: 0.000300\n",
      "2021-08-01 17:58:50,820 - INFO - joeynmt.training - Epoch   5, Step:    11200, Batch Loss:     2.568467, Tokens per Sec:    16299, Lr: 0.000300\n",
      "2021-08-01 17:59:17,413 - INFO - joeynmt.training - Epoch   5, Step:    11400, Batch Loss:     2.396975, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-01 17:59:44,002 - INFO - joeynmt.training - Epoch   5, Step:    11600, Batch Loss:     2.868691, Tokens per Sec:    16537, Lr: 0.000300\n",
      "2021-08-01 18:00:10,560 - INFO - joeynmt.training - Epoch   5, Step:    11800, Batch Loss:     2.558549, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-01 18:00:37,047 - INFO - joeynmt.training - Epoch   5, Step:    12000, Batch Loss:     2.761343, Tokens per Sec:    16319, Lr: 0.000300\n",
      "2021-08-01 18:01:39,467 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:01:39,468 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:01:39,468 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:01:40,060 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:01:40,061 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:01:41,180 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:01:41,181 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:01:41,181 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:01:41,181 - INFO - joeynmt.training - \tHypothesis: When we consider how we imitate faith and imitate him , we feel as we are speaking .\n",
      "2021-08-01 18:01:41,181 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:01:41,182 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:01:41,182 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:01:41,182 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our efforts to make a regular Bible study with people if we are able to be able to be able to be able to be able to be able to be able to be able to be in the Scriptures .\n",
      "2021-08-01 18:01:41,182 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:01:41,182 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:01:41,182 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:01:41,183 - INFO - joeynmt.training - \tHypothesis: Why are the gods of false gods ?\n",
      "2021-08-01 18:01:41,183 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:01:41,183 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:01:41,183 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:01:41,183 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with his people .\n",
      "2021-08-01 18:01:41,183 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    12000: bleu:  15.39, loss: 144066.6094, ppl:  10.9665, duration: 64.1357s\n",
      "2021-08-01 18:02:07,998 - INFO - joeynmt.training - Epoch   5, Step:    12200, Batch Loss:     2.616100, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-08-01 18:02:34,748 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     2.691506, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-01 18:03:01,456 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     2.616323, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-01 18:03:27,886 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     2.509830, Tokens per Sec:    16224, Lr: 0.000300\n",
      "2021-08-01 18:03:54,222 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     2.544008, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-01 18:04:20,932 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     2.546552, Tokens per Sec:    16365, Lr: 0.000300\n",
      "2021-08-01 18:04:47,107 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     2.459097, Tokens per Sec:    16136, Lr: 0.000300\n",
      "2021-08-01 18:05:13,809 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     2.525299, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-01 18:05:24,115 - INFO - joeynmt.training - Epoch   5: total training loss 7097.46\n",
      "2021-08-01 18:05:24,116 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-01 18:05:40,266 - INFO - joeynmt.training - Epoch   6, Step:    13800, Batch Loss:     2.292874, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-01 18:06:07,026 - INFO - joeynmt.training - Epoch   6, Step:    14000, Batch Loss:     2.480920, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-01 18:07:02,389 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:07:02,389 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:07:02,389 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:07:03,032 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:07:03,033 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:07:04,038 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:07:04,039 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:07:04,039 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:07:04,039 - INFO - joeynmt.training - \tHypothesis: When we examine how we display faith and imitate him , we are speaking about him .\n",
      "2021-08-01 18:07:04,039 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:07:04,040 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:07:04,040 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:07:04,040 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to study the Scriptures as well as the Scriptures .\n",
      "2021-08-01 18:07:04,040 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:07:04,041 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:07:04,041 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:07:04,041 - INFO - joeynmt.training - \tHypothesis: Why do the gods of false gods ?\n",
      "2021-08-01 18:07:04,041 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:07:04,042 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:07:04,042 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:07:04,042 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with his people .\n",
      "2021-08-01 18:07:04,042 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    14000: bleu:  17.03, loss: 139397.2969, ppl:  10.1475, duration: 57.0154s\n",
      "2021-08-01 18:07:30,962 - INFO - joeynmt.training - Epoch   6, Step:    14200, Batch Loss:     2.391213, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-08-01 18:07:57,684 - INFO - joeynmt.training - Epoch   6, Step:    14400, Batch Loss:     2.407305, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-08-01 18:08:24,250 - INFO - joeynmt.training - Epoch   6, Step:    14600, Batch Loss:     2.865177, Tokens per Sec:    16179, Lr: 0.000300\n",
      "2021-08-01 18:08:50,724 - INFO - joeynmt.training - Epoch   6, Step:    14800, Batch Loss:     2.573312, Tokens per Sec:    16129, Lr: 0.000300\n",
      "2021-08-01 18:09:17,253 - INFO - joeynmt.training - Epoch   6, Step:    15000, Batch Loss:     2.926923, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-08-01 18:09:43,934 - INFO - joeynmt.training - Epoch   6, Step:    15200, Batch Loss:     2.540850, Tokens per Sec:    16309, Lr: 0.000300\n",
      "2021-08-01 18:10:10,745 - INFO - joeynmt.training - Epoch   6, Step:    15400, Batch Loss:     2.365563, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-08-01 18:10:37,424 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     2.247773, Tokens per Sec:    16507, Lr: 0.000300\n",
      "2021-08-01 18:11:04,086 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     2.910677, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-01 18:11:30,812 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     2.610346, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-08-01 18:12:25,175 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:12:25,176 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:12:25,176 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:12:25,763 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:12:25,764 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:12:26,828 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:12:26,829 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:12:26,829 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:12:26,829 - INFO - joeynmt.training - \tHypothesis: When we examine how we exercise faith and imitating him , we pray to us .\n",
      "2021-08-01 18:12:26,829 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:12:26,830 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:12:26,830 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:12:26,830 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to try to read the Scriptures if we are able to be able .\n",
      "2021-08-01 18:12:26,830 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:12:26,831 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:12:26,831 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:12:26,831 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unable to do so ?\n",
      "2021-08-01 18:12:26,831 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:12:26,831 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:12:26,831 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:12:26,832 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with his people .\n",
      "2021-08-01 18:12:26,832 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    16000: bleu:  18.17, loss: 134982.5781, ppl:   9.4295, duration: 56.0189s\n",
      "2021-08-01 18:12:53,708 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     2.627644, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-01 18:13:20,170 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     2.739438, Tokens per Sec:    16302, Lr: 0.000300\n",
      "2021-08-01 18:13:22,036 - INFO - joeynmt.training - Epoch   6: total training loss 6789.09\n",
      "2021-08-01 18:13:22,036 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-01 18:13:46,947 - INFO - joeynmt.training - Epoch   7, Step:    16600, Batch Loss:     2.248729, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-08-01 18:14:13,584 - INFO - joeynmt.training - Epoch   7, Step:    16800, Batch Loss:     1.829999, Tokens per Sec:    16337, Lr: 0.000300\n",
      "2021-08-01 18:14:39,993 - INFO - joeynmt.training - Epoch   7, Step:    17000, Batch Loss:     2.633024, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-08-01 18:15:06,698 - INFO - joeynmt.training - Epoch   7, Step:    17200, Batch Loss:     2.386568, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-01 18:15:33,467 - INFO - joeynmt.training - Epoch   7, Step:    17400, Batch Loss:     2.490070, Tokens per Sec:    16391, Lr: 0.000300\n",
      "2021-08-01 18:16:00,315 - INFO - joeynmt.training - Epoch   7, Step:    17600, Batch Loss:     2.516992, Tokens per Sec:    16118, Lr: 0.000300\n",
      "2021-08-01 18:16:26,936 - INFO - joeynmt.training - Epoch   7, Step:    17800, Batch Loss:     2.445810, Tokens per Sec:    15997, Lr: 0.000300\n",
      "2021-08-01 18:16:53,311 - INFO - joeynmt.training - Epoch   7, Step:    18000, Batch Loss:     2.371435, Tokens per Sec:    16106, Lr: 0.000300\n",
      "2021-08-01 18:17:48,206 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:17:48,206 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:17:48,207 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:17:48,793 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:17:48,793 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:17:49,499 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:17:49,500 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:17:49,500 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:17:49,500 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking about us .\n",
      "2021-08-01 18:17:49,500 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:17:49,501 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:17:49,501 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:17:49,501 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we have a Bible - preaching work and try to read the Scriptures as possible .\n",
      "2021-08-01 18:17:49,501 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:17:49,502 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:17:49,502 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:17:49,502 - INFO - joeynmt.training - \tHypothesis: Why is the gods of false gods ?\n",
      "2021-08-01 18:17:49,502 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:17:49,503 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:17:49,503 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:17:49,503 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-01 18:17:49,503 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    18000: bleu:  18.91, loss: 132093.6562, ppl:   8.9873, duration: 56.1915s\n",
      "2021-08-01 18:18:16,334 - INFO - joeynmt.training - Epoch   7, Step:    18200, Batch Loss:     2.395896, Tokens per Sec:    15901, Lr: 0.000300\n",
      "2021-08-01 18:18:43,156 - INFO - joeynmt.training - Epoch   7, Step:    18400, Batch Loss:     2.714970, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-01 18:19:09,905 - INFO - joeynmt.training - Epoch   7, Step:    18600, Batch Loss:     2.414679, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-01 18:19:36,289 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     2.696185, Tokens per Sec:    16180, Lr: 0.000300\n",
      "2021-08-01 18:20:03,026 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     2.770996, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-01 18:20:24,344 - INFO - joeynmt.training - Epoch   7: total training loss 6586.55\n",
      "2021-08-01 18:20:24,344 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-01 18:20:29,836 - INFO - joeynmt.training - Epoch   8, Step:    19200, Batch Loss:     2.279621, Tokens per Sec:    14825, Lr: 0.000300\n",
      "2021-08-01 18:20:56,655 - INFO - joeynmt.training - Epoch   8, Step:    19400, Batch Loss:     2.453982, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-01 18:21:23,444 - INFO - joeynmt.training - Epoch   8, Step:    19600, Batch Loss:     2.240343, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-08-01 18:21:49,802 - INFO - joeynmt.training - Epoch   8, Step:    19800, Batch Loss:     2.520926, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-01 18:22:16,748 - INFO - joeynmt.training - Epoch   8, Step:    20000, Batch Loss:     1.931082, Tokens per Sec:    15919, Lr: 0.000300\n",
      "2021-08-01 18:23:12,241 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:23:12,241 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:23:12,242 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:23:12,889 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:23:12,889 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:23:14,007 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:23:14,007 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:23:14,008 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:23:14,008 - INFO - joeynmt.training - \tHypothesis: When we examine how he exercised faith and imitate him , he is speaking about us .\n",
      "2021-08-01 18:23:14,008 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:23:14,008 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:23:14,008 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:23:14,009 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to read the Scriptures as possible .\n",
      "2021-08-01 18:23:14,009 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:23:14,009 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:23:14,009 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:23:14,009 - INFO - joeynmt.training - \tHypothesis: Why is the gods of false religion ?\n",
      "2021-08-01 18:23:14,010 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:23:14,010 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:23:14,010 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:23:14,010 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with his people in a way of love .\n",
      "2021-08-01 18:23:14,011 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    20000: bleu:  19.28, loss: 128846.1875, ppl:   8.5150, duration: 57.2619s\n",
      "2021-08-01 18:23:40,835 - INFO - joeynmt.training - Epoch   8, Step:    20200, Batch Loss:     2.462815, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-01 18:24:07,723 - INFO - joeynmt.training - Epoch   8, Step:    20400, Batch Loss:     2.363463, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-08-01 18:24:34,244 - INFO - joeynmt.training - Epoch   8, Step:    20600, Batch Loss:     2.591301, Tokens per Sec:    16305, Lr: 0.000300\n",
      "2021-08-01 18:25:01,320 - INFO - joeynmt.training - Epoch   8, Step:    20800, Batch Loss:     2.482634, Tokens per Sec:    16194, Lr: 0.000300\n",
      "2021-08-01 18:25:27,708 - INFO - joeynmt.training - Epoch   8, Step:    21000, Batch Loss:     2.389707, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-08-01 18:25:54,613 - INFO - joeynmt.training - Epoch   8, Step:    21200, Batch Loss:     2.411156, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-01 18:26:21,486 - INFO - joeynmt.training - Epoch   8, Step:    21400, Batch Loss:     2.701908, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-01 18:26:48,192 - INFO - joeynmt.training - Epoch   8, Step:    21600, Batch Loss:     2.586113, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-01 18:27:14,888 - INFO - joeynmt.training - Epoch   8, Step:    21800, Batch Loss:     2.441049, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-08-01 18:27:28,948 - INFO - joeynmt.training - Epoch   8: total training loss 6398.97\n",
      "2021-08-01 18:27:28,948 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-01 18:27:41,922 - INFO - joeynmt.training - Epoch   9, Step:    22000, Batch Loss:     2.397460, Tokens per Sec:    15899, Lr: 0.000300\n",
      "2021-08-01 18:28:37,088 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:28:37,088 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:28:37,089 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:28:37,669 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:28:37,669 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:28:38,353 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:28:38,354 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:28:38,354 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:28:38,354 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 18:28:38,355 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:28:38,355 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:28:38,355 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:28:38,355 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible as we preach and to read the Scriptures as possible .\n",
      "2021-08-01 18:28:38,355 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:28:38,356 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:28:38,356 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:28:38,356 - INFO - joeynmt.training - \tHypothesis: Why do the gods of false gods have been unknown ?\n",
      "2021-08-01 18:28:38,356 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:28:38,357 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:28:38,357 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:28:38,357 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with his people .\n",
      "2021-08-01 18:28:38,357 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    22000: bleu:  19.88, loss: 126759.3594, ppl:   8.2247, duration: 56.4348s\n",
      "2021-08-01 18:29:05,166 - INFO - joeynmt.training - Epoch   9, Step:    22200, Batch Loss:     2.321356, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-08-01 18:29:32,184 - INFO - joeynmt.training - Epoch   9, Step:    22400, Batch Loss:     2.562083, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-01 18:29:58,993 - INFO - joeynmt.training - Epoch   9, Step:    22600, Batch Loss:     2.332464, Tokens per Sec:    15907, Lr: 0.000300\n",
      "2021-08-01 18:30:25,571 - INFO - joeynmt.training - Epoch   9, Step:    22800, Batch Loss:     2.161592, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-08-01 18:30:52,431 - INFO - joeynmt.training - Epoch   9, Step:    23000, Batch Loss:     2.141715, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-01 18:31:19,316 - INFO - joeynmt.training - Epoch   9, Step:    23200, Batch Loss:     1.978007, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-08-01 18:31:45,989 - INFO - joeynmt.training - Epoch   9, Step:    23400, Batch Loss:     1.869749, Tokens per Sec:    16178, Lr: 0.000300\n",
      "2021-08-01 18:32:13,118 - INFO - joeynmt.training - Epoch   9, Step:    23600, Batch Loss:     2.110966, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-08-01 18:32:39,776 - INFO - joeynmt.training - Epoch   9, Step:    23800, Batch Loss:     2.373964, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-08-01 18:33:06,614 - INFO - joeynmt.training - Epoch   9, Step:    24000, Batch Loss:     2.318262, Tokens per Sec:    16053, Lr: 0.000300\n",
      "2021-08-01 18:33:59,524 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:33:59,524 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:33:59,525 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:34:00,154 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:34:00,154 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:34:01,257 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:34:01,258 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:34:01,258 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:34:01,259 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking about us .\n",
      "2021-08-01 18:34:01,259 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:34:01,259 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:34:01,259 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:34:01,259 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures as possible .\n",
      "2021-08-01 18:34:01,259 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:34:01,260 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:34:01,260 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:34:01,260 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have been hidden ?\n",
      "2021-08-01 18:34:01,260 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:34:01,261 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:34:01,261 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:34:01,261 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a way of love .\n",
      "2021-08-01 18:34:01,261 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    24000: bleu:  20.26, loss: 124511.3672, ppl:   7.9230, duration: 54.6463s\n",
      "2021-08-01 18:34:28,386 - INFO - joeynmt.training - Epoch   9, Step:    24200, Batch Loss:     2.445127, Tokens per Sec:    15763, Lr: 0.000300\n",
      "2021-08-01 18:34:55,370 - INFO - joeynmt.training - Epoch   9, Step:    24400, Batch Loss:     2.411335, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-08-01 18:35:21,719 - INFO - joeynmt.training - Epoch   9, Step:    24600, Batch Loss:     2.120450, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-08-01 18:35:27,485 - INFO - joeynmt.training - Epoch   9: total training loss 6236.65\n",
      "2021-08-01 18:35:27,485 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-01 18:35:48,923 - INFO - joeynmt.training - Epoch  10, Step:    24800, Batch Loss:     2.251621, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-08-01 18:36:15,688 - INFO - joeynmt.training - Epoch  10, Step:    25000, Batch Loss:     2.327286, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-01 18:36:42,160 - INFO - joeynmt.training - Epoch  10, Step:    25200, Batch Loss:     2.269682, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-08-01 18:37:09,125 - INFO - joeynmt.training - Epoch  10, Step:    25400, Batch Loss:     2.032950, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-01 18:37:35,730 - INFO - joeynmt.training - Epoch  10, Step:    25600, Batch Loss:     2.365821, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-08-01 18:38:02,720 - INFO - joeynmt.training - Epoch  10, Step:    25800, Batch Loss:     2.144172, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-01 18:38:29,606 - INFO - joeynmt.training - Epoch  10, Step:    26000, Batch Loss:     2.403073, Tokens per Sec:    16267, Lr: 0.000300\n",
      "2021-08-01 18:39:25,949 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:39:25,950 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:39:25,950 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:39:26,555 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:39:26,556 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:39:27,979 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:39:27,980 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:39:27,980 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:39:27,980 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking about us .\n",
      "2021-08-01 18:39:27,981 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:39:27,981 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:39:27,981 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:39:27,981 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we have a full use of the Bible by preaching and to read the Scriptures when it is possible .\n",
      "2021-08-01 18:39:27,982 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:39:27,982 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:39:27,982 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:39:27,982 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have been hoped ?\n",
      "2021-08-01 18:39:27,982 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:39:27,983 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:39:27,983 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:39:27,983 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a way of love .\n",
      "2021-08-01 18:39:27,983 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    26000: bleu:  20.81, loss: 122831.8750, ppl:   7.7049, duration: 58.3770s\n",
      "2021-08-01 18:39:54,730 - INFO - joeynmt.training - Epoch  10, Step:    26200, Batch Loss:     2.129850, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-01 18:40:21,778 - INFO - joeynmt.training - Epoch  10, Step:    26400, Batch Loss:     1.765059, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-01 18:40:48,665 - INFO - joeynmt.training - Epoch  10, Step:    26600, Batch Loss:     1.862785, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-01 18:41:15,367 - INFO - joeynmt.training - Epoch  10, Step:    26800, Batch Loss:     1.833255, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-01 18:41:41,936 - INFO - joeynmt.training - Epoch  10, Step:    27000, Batch Loss:     2.328158, Tokens per Sec:    15907, Lr: 0.000300\n",
      "2021-08-01 18:42:09,217 - INFO - joeynmt.training - Epoch  10, Step:    27200, Batch Loss:     1.927181, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-01 18:42:32,897 - INFO - joeynmt.training - Epoch  10: total training loss 6105.65\n",
      "2021-08-01 18:42:32,897 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-01 18:42:36,188 - INFO - joeynmt.training - Epoch  11, Step:    27400, Batch Loss:     2.413668, Tokens per Sec:    13940, Lr: 0.000300\n",
      "2021-08-01 18:43:02,719 - INFO - joeynmt.training - Epoch  11, Step:    27600, Batch Loss:     2.147354, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-01 18:43:29,429 - INFO - joeynmt.training - Epoch  11, Step:    27800, Batch Loss:     2.286979, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-08-01 18:43:56,301 - INFO - joeynmt.training - Epoch  11, Step:    28000, Batch Loss:     2.333133, Tokens per Sec:    16239, Lr: 0.000300\n",
      "2021-08-01 18:44:56,395 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:44:56,395 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:44:56,395 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:44:57,031 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:44:57,032 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:44:57,765 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:44:57,766 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:44:57,766 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:44:57,766 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 18:44:57,766 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:44:57,767 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:44:57,767 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:44:57,767 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible as we preach and to read the Scriptures as possible .\n",
      "2021-08-01 18:44:57,767 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:44:57,768 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:44:57,768 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:44:57,768 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have been distincted ?\n",
      "2021-08-01 18:44:57,768 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:44:57,769 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:44:57,769 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:44:57,769 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-01 18:44:57,770 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    28000: bleu:  21.38, loss: 121200.5000, ppl:   7.4988, duration: 61.4683s\n",
      "2021-08-01 18:45:24,916 - INFO - joeynmt.training - Epoch  11, Step:    28200, Batch Loss:     2.416580, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-01 18:45:51,933 - INFO - joeynmt.training - Epoch  11, Step:    28400, Batch Loss:     2.213430, Tokens per Sec:    16100, Lr: 0.000300\n",
      "2021-08-01 18:46:18,608 - INFO - joeynmt.training - Epoch  11, Step:    28600, Batch Loss:     2.123226, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-01 18:46:45,396 - INFO - joeynmt.training - Epoch  11, Step:    28800, Batch Loss:     2.193737, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-08-01 18:47:12,387 - INFO - joeynmt.training - Epoch  11, Step:    29000, Batch Loss:     2.424174, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-08-01 18:47:39,084 - INFO - joeynmt.training - Epoch  11, Step:    29200, Batch Loss:     2.493588, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-08-01 18:48:06,052 - INFO - joeynmt.training - Epoch  11, Step:    29400, Batch Loss:     2.396359, Tokens per Sec:    15924, Lr: 0.000300\n",
      "2021-08-01 18:48:32,726 - INFO - joeynmt.training - Epoch  11, Step:    29600, Batch Loss:     2.392074, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-08-01 18:48:59,497 - INFO - joeynmt.training - Epoch  11, Step:    29800, Batch Loss:     2.179560, Tokens per Sec:    16109, Lr: 0.000300\n",
      "2021-08-01 18:49:26,164 - INFO - joeynmt.training - Epoch  11, Step:    30000, Batch Loss:     2.147829, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-01 18:50:21,454 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:50:21,454 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:50:21,455 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:50:22,041 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:50:22,042 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:50:22,787 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:50:22,788 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:50:22,788 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:50:22,788 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 18:50:22,789 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:50:22,789 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:50:22,789 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:50:22,789 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible by preaching and to read the Scriptures as possible .\n",
      "2021-08-01 18:50:22,789 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:50:22,790 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:50:22,790 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:50:22,790 - INFO - joeynmt.training - \tHypothesis: Why are false gods not uncertain ?\n",
      "2021-08-01 18:50:22,790 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:50:22,791 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:50:22,791 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:50:22,791 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a way of love .\n",
      "2021-08-01 18:50:22,791 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    30000: bleu:  21.73, loss: 119474.7109, ppl:   7.2867, duration: 56.6269s\n",
      "2021-08-01 18:50:37,611 - INFO - joeynmt.training - Epoch  11: total training loss 5989.97\n",
      "2021-08-01 18:50:37,612 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-01 18:50:50,012 - INFO - joeynmt.training - Epoch  12, Step:    30200, Batch Loss:     2.225847, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-01 18:51:16,924 - INFO - joeynmt.training - Epoch  12, Step:    30400, Batch Loss:     2.125236, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-01 18:51:43,751 - INFO - joeynmt.training - Epoch  12, Step:    30600, Batch Loss:     2.211937, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-08-01 18:52:10,569 - INFO - joeynmt.training - Epoch  12, Step:    30800, Batch Loss:     2.121207, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-08-01 18:52:37,084 - INFO - joeynmt.training - Epoch  12, Step:    31000, Batch Loss:     2.357105, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-01 18:53:04,112 - INFO - joeynmt.training - Epoch  12, Step:    31200, Batch Loss:     2.424760, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-01 18:53:31,046 - INFO - joeynmt.training - Epoch  12, Step:    31400, Batch Loss:     2.136483, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-08-01 18:53:57,888 - INFO - joeynmt.training - Epoch  12, Step:    31600, Batch Loss:     2.296996, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-01 18:54:24,824 - INFO - joeynmt.training - Epoch  12, Step:    31800, Batch Loss:     2.495503, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-08-01 18:54:51,687 - INFO - joeynmt.training - Epoch  12, Step:    32000, Batch Loss:     1.717128, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-08-01 18:55:49,751 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 18:55:49,751 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 18:55:49,752 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 18:55:50,344 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 18:55:50,344 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 18:55:51,050 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 18:55:51,051 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 18:55:51,051 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 18:55:51,051 - INFO - joeynmt.training - \tHypothesis: As we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 18:55:51,051 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 18:55:51,052 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 18:55:51,052 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 18:55:51,052 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we have a regular use of the Bible by preaching and to read the Scriptures as possible .\n",
      "2021-08-01 18:55:51,052 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 18:55:51,053 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 18:55:51,053 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 18:55:51,053 - INFO - joeynmt.training - \tHypothesis: Why do false gods not be hidden ?\n",
      "2021-08-01 18:55:51,053 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 18:55:51,053 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 18:55:51,054 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 18:55:51,054 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 18:55:51,054 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    32000: bleu:  21.99, loss: 118343.3047, ppl:   7.1509, duration: 59.3668s\n",
      "2021-08-01 18:56:18,093 - INFO - joeynmt.training - Epoch  12, Step:    32200, Batch Loss:     2.010375, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-01 18:56:45,035 - INFO - joeynmt.training - Epoch  12, Step:    32400, Batch Loss:     2.169233, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-08-01 18:57:11,767 - INFO - joeynmt.training - Epoch  12, Step:    32600, Batch Loss:     2.183496, Tokens per Sec:    15898, Lr: 0.000300\n",
      "2021-08-01 18:57:38,589 - INFO - joeynmt.training - Epoch  12, Step:    32800, Batch Loss:     2.335322, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-01 18:57:44,493 - INFO - joeynmt.training - Epoch  12: total training loss 5898.83\n",
      "2021-08-01 18:57:44,494 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-01 18:58:05,640 - INFO - joeynmt.training - Epoch  13, Step:    33000, Batch Loss:     2.304633, Tokens per Sec:    15628, Lr: 0.000300\n",
      "2021-08-01 18:58:32,400 - INFO - joeynmt.training - Epoch  13, Step:    33200, Batch Loss:     2.269942, Tokens per Sec:    16243, Lr: 0.000300\n",
      "2021-08-01 18:58:58,997 - INFO - joeynmt.training - Epoch  13, Step:    33400, Batch Loss:     1.938771, Tokens per Sec:    15713, Lr: 0.000300\n",
      "2021-08-01 18:59:25,724 - INFO - joeynmt.training - Epoch  13, Step:    33600, Batch Loss:     1.982405, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-08-01 18:59:52,512 - INFO - joeynmt.training - Epoch  13, Step:    33800, Batch Loss:     2.460201, Tokens per Sec:    16008, Lr: 0.000300\n",
      "2021-08-01 19:00:19,549 - INFO - joeynmt.training - Epoch  13, Step:    34000, Batch Loss:     2.089518, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-08-01 19:01:12,314 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:01:12,314 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:01:12,315 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:01:12,958 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:01:12,959 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:01:13,703 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:01:13,705 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:01:13,706 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:01:13,707 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:01:13,707 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:01:13,708 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:01:13,708 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:01:13,708 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and try to read the Scriptures as possible .\n",
      "2021-08-01 19:01:13,708 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:01:13,709 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:01:13,709 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:01:13,709 - INFO - joeynmt.training - \tHypothesis: Why is false gods not unique ?\n",
      "2021-08-01 19:01:13,709 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:01:13,710 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:01:13,710 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:01:13,710 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 19:01:13,710 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    34000: bleu:  22.08, loss: 117510.2031, ppl:   7.0526, duration: 54.1613s\n",
      "2021-08-01 19:01:40,544 - INFO - joeynmt.training - Epoch  13, Step:    34200, Batch Loss:     2.263207, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-01 19:02:07,284 - INFO - joeynmt.training - Epoch  13, Step:    34400, Batch Loss:     2.135078, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-01 19:02:34,147 - INFO - joeynmt.training - Epoch  13, Step:    34600, Batch Loss:     2.121622, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-01 19:03:01,094 - INFO - joeynmt.training - Epoch  13, Step:    34800, Batch Loss:     2.268966, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-08-01 19:03:27,675 - INFO - joeynmt.training - Epoch  13, Step:    35000, Batch Loss:     2.315447, Tokens per Sec:    16136, Lr: 0.000300\n",
      "2021-08-01 19:03:54,669 - INFO - joeynmt.training - Epoch  13, Step:    35200, Batch Loss:     2.105262, Tokens per Sec:    16284, Lr: 0.000300\n",
      "2021-08-01 19:04:21,644 - INFO - joeynmt.training - Epoch  13, Step:    35400, Batch Loss:     2.070856, Tokens per Sec:    16316, Lr: 0.000300\n",
      "2021-08-01 19:04:45,097 - INFO - joeynmt.training - Epoch  13: total training loss 5814.68\n",
      "2021-08-01 19:04:45,097 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-01 19:04:48,700 - INFO - joeynmt.training - Epoch  14, Step:    35600, Batch Loss:     1.914193, Tokens per Sec:    14715, Lr: 0.000300\n",
      "2021-08-01 19:05:15,713 - INFO - joeynmt.training - Epoch  14, Step:    35800, Batch Loss:     1.932799, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-01 19:05:42,338 - INFO - joeynmt.training - Epoch  14, Step:    36000, Batch Loss:     1.929338, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-01 19:06:40,114 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:06:40,114 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:06:40,115 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:06:40,710 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:06:40,710 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:06:41,427 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:06:41,428 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:06:41,428 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:06:41,428 - INFO - joeynmt.training - \tHypothesis: As we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:06:41,428 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:06:41,429 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:06:41,429 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:06:41,429 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and effort to read the Scriptures if possible .\n",
      "2021-08-01 19:06:41,429 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:06:41,429 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:06:41,430 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:06:41,430 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have any nature ?\n",
      "2021-08-01 19:06:41,430 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:06:41,430 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:06:41,430 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:06:41,431 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is associated with his people in a loving way .\n",
      "2021-08-01 19:06:41,431 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    36000: bleu:  22.24, loss: 116700.9531, ppl:   6.9583, duration: 59.0927s\n",
      "2021-08-01 19:07:08,560 - INFO - joeynmt.training - Epoch  14, Step:    36200, Batch Loss:     2.049145, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-01 19:07:35,440 - INFO - joeynmt.training - Epoch  14, Step:    36400, Batch Loss:     1.706311, Tokens per Sec:    15814, Lr: 0.000300\n",
      "2021-08-01 19:08:02,165 - INFO - joeynmt.training - Epoch  14, Step:    36600, Batch Loss:     2.105110, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-01 19:08:28,777 - INFO - joeynmt.training - Epoch  14, Step:    36800, Batch Loss:     2.340329, Tokens per Sec:    16149, Lr: 0.000300\n",
      "2021-08-01 19:08:55,747 - INFO - joeynmt.training - Epoch  14, Step:    37000, Batch Loss:     2.140791, Tokens per Sec:    15989, Lr: 0.000300\n",
      "2021-08-01 19:09:22,296 - INFO - joeynmt.training - Epoch  14, Step:    37200, Batch Loss:     2.123647, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-01 19:09:49,219 - INFO - joeynmt.training - Epoch  14, Step:    37400, Batch Loss:     2.097912, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-08-01 19:10:16,156 - INFO - joeynmt.training - Epoch  14, Step:    37600, Batch Loss:     2.053391, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-08-01 19:10:42,871 - INFO - joeynmt.training - Epoch  14, Step:    37800, Batch Loss:     2.203102, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-08-01 19:11:09,924 - INFO - joeynmt.training - Epoch  14, Step:    38000, Batch Loss:     2.063038, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-08-01 19:12:03,405 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:12:03,405 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:12:03,405 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:12:04,050 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:12:04,050 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:12:04,803 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:12:04,804 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:12:04,804 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:12:04,805 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he speaks of us .\n",
      "2021-08-01 19:12:04,805 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:12:04,805 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:12:04,806 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:12:04,806 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and regularly read the Scriptures when possible .\n",
      "2021-08-01 19:12:04,806 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:12:04,806 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:12:04,806 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:12:04,807 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a negative influence ?\n",
      "2021-08-01 19:12:04,807 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:12:04,807 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:12:04,808 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:12:04,808 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is associated with his people in a loving way .\n",
      "2021-08-01 19:12:04,808 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    38000: bleu:  22.65, loss: 115166.0312, ppl:   6.7830, duration: 54.8831s\n",
      "2021-08-01 19:12:31,859 - INFO - joeynmt.training - Epoch  14, Step:    38200, Batch Loss:     2.034267, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-08-01 19:12:47,187 - INFO - joeynmt.training - Epoch  14: total training loss 5746.08\n",
      "2021-08-01 19:12:47,187 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-01 19:12:59,307 - INFO - joeynmt.training - Epoch  15, Step:    38400, Batch Loss:     1.981293, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-08-01 19:13:25,795 - INFO - joeynmt.training - Epoch  15, Step:    38600, Batch Loss:     2.273469, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-08-01 19:13:52,687 - INFO - joeynmt.training - Epoch  15, Step:    38800, Batch Loss:     2.109359, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-01 19:14:19,460 - INFO - joeynmt.training - Epoch  15, Step:    39000, Batch Loss:     1.894373, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-01 19:14:46,183 - INFO - joeynmt.training - Epoch  15, Step:    39200, Batch Loss:     2.212736, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-08-01 19:15:13,219 - INFO - joeynmt.training - Epoch  15, Step:    39400, Batch Loss:     2.016652, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-08-01 19:15:39,980 - INFO - joeynmt.training - Epoch  15, Step:    39600, Batch Loss:     2.131851, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-01 19:16:06,503 - INFO - joeynmt.training - Epoch  15, Step:    39800, Batch Loss:     2.171517, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-01 19:16:33,143 - INFO - joeynmt.training - Epoch  15, Step:    40000, Batch Loss:     2.080407, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-01 19:17:25,092 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:17:25,093 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:17:25,093 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:17:25,676 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:17:25,676 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:17:26,396 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:17:26,397 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:17:26,397 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:17:26,397 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:17:26,397 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:17:26,398 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:17:26,398 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:17:26,398 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and strive to read the Scriptures if possible .\n",
      "2021-08-01 19:17:26,398 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:17:26,398 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:17:26,398 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:17:26,399 - INFO - joeynmt.training - \tHypothesis: Why are false gods not uncertain ?\n",
      "2021-08-01 19:17:26,399 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:17:26,399 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:17:26,399 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:17:26,399 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 19:17:26,400 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    40000: bleu:  22.84, loss: 114566.8125, ppl:   6.7158, duration: 53.2565s\n",
      "2021-08-01 19:17:53,059 - INFO - joeynmt.training - Epoch  15, Step:    40200, Batch Loss:     2.174529, Tokens per Sec:    16337, Lr: 0.000300\n",
      "2021-08-01 19:18:20,026 - INFO - joeynmt.training - Epoch  15, Step:    40400, Batch Loss:     2.258149, Tokens per Sec:    16176, Lr: 0.000300\n",
      "2021-08-01 19:18:46,689 - INFO - joeynmt.training - Epoch  15, Step:    40600, Batch Loss:     2.173314, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-01 19:19:13,439 - INFO - joeynmt.training - Epoch  15, Step:    40800, Batch Loss:     2.061116, Tokens per Sec:    16393, Lr: 0.000300\n",
      "2021-08-01 19:19:39,865 - INFO - joeynmt.training - Epoch  15, Step:    41000, Batch Loss:     2.032334, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-01 19:19:45,269 - INFO - joeynmt.training - Epoch  15: total training loss 5665.28\n",
      "2021-08-01 19:19:45,269 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-01 19:20:06,757 - INFO - joeynmt.training - Epoch  16, Step:    41200, Batch Loss:     2.164466, Tokens per Sec:    15665, Lr: 0.000300\n",
      "2021-08-01 19:20:33,181 - INFO - joeynmt.training - Epoch  16, Step:    41400, Batch Loss:     2.119072, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-01 19:20:59,946 - INFO - joeynmt.training - Epoch  16, Step:    41600, Batch Loss:     2.190097, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-01 19:21:26,627 - INFO - joeynmt.training - Epoch  16, Step:    41800, Batch Loss:     1.934586, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-08-01 19:21:53,238 - INFO - joeynmt.training - Epoch  16, Step:    42000, Batch Loss:     2.210731, Tokens per Sec:    16326, Lr: 0.000300\n",
      "2021-08-01 19:22:46,689 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:22:46,690 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:22:46,690 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:22:47,275 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:22:47,276 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:22:47,972 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:22:47,972 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:22:47,973 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:22:47,973 - INFO - joeynmt.training - \tHypothesis: As we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:22:47,973 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:22:47,973 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:22:47,974 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:22:47,974 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-01 19:22:47,974 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:22:47,974 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:22:47,975 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:22:47,975 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unique ?\n",
      "2021-08-01 19:22:47,975 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:22:47,975 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:22:47,975 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:22:47,976 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is associating with his people in a loving way .\n",
      "2021-08-01 19:22:47,976 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    42000: bleu:  23.15, loss: 113640.1094, ppl:   6.6131, duration: 54.7369s\n",
      "2021-08-01 19:23:14,771 - INFO - joeynmt.training - Epoch  16, Step:    42200, Batch Loss:     1.701832, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-01 19:23:41,482 - INFO - joeynmt.training - Epoch  16, Step:    42400, Batch Loss:     2.099201, Tokens per Sec:    16275, Lr: 0.000300\n",
      "2021-08-01 19:24:07,880 - INFO - joeynmt.training - Epoch  16, Step:    42600, Batch Loss:     2.088349, Tokens per Sec:    16140, Lr: 0.000300\n",
      "2021-08-01 19:24:34,121 - INFO - joeynmt.training - Epoch  16, Step:    42800, Batch Loss:     2.084416, Tokens per Sec:    16316, Lr: 0.000300\n",
      "2021-08-01 19:25:00,864 - INFO - joeynmt.training - Epoch  16, Step:    43000, Batch Loss:     1.737578, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-01 19:25:27,187 - INFO - joeynmt.training - Epoch  16, Step:    43200, Batch Loss:     2.174750, Tokens per Sec:    16378, Lr: 0.000300\n",
      "2021-08-01 19:25:53,770 - INFO - joeynmt.training - Epoch  16, Step:    43400, Batch Loss:     1.973951, Tokens per Sec:    16487, Lr: 0.000300\n",
      "2021-08-01 19:26:20,074 - INFO - joeynmt.training - Epoch  16, Step:    43600, Batch Loss:     2.039058, Tokens per Sec:    16219, Lr: 0.000300\n",
      "2021-08-01 19:26:43,433 - INFO - joeynmt.training - Epoch  16: total training loss 5629.62\n",
      "2021-08-01 19:26:43,434 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-08-01 19:26:46,733 - INFO - joeynmt.training - Epoch  17, Step:    43800, Batch Loss:     2.004266, Tokens per Sec:    14695, Lr: 0.000300\n",
      "2021-08-01 19:27:13,000 - INFO - joeynmt.training - Epoch  17, Step:    44000, Batch Loss:     2.223106, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-08-01 19:28:04,307 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:28:04,307 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:28:04,308 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:28:04,945 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:28:04,945 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:28:05,674 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:28:05,675 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:28:05,675 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:28:05,675 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:28:05,676 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:28:05,676 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:28:05,676 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:28:05,676 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and striving to read the Scriptures if possible .\n",
      "2021-08-01 19:28:05,677 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:28:05,677 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:28:05,677 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:28:05,677 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have uncertain things ?\n",
      "2021-08-01 19:28:05,678 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:28:05,678 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:28:05,678 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:28:05,678 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is associated with his people in a loving way .\n",
      "2021-08-01 19:28:05,678 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    44000: bleu:  23.44, loss: 113218.7812, ppl:   6.5670, duration: 52.6777s\n",
      "2021-08-01 19:28:32,099 - INFO - joeynmt.training - Epoch  17, Step:    44200, Batch Loss:     1.740333, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-08-01 19:28:58,541 - INFO - joeynmt.training - Epoch  17, Step:    44400, Batch Loss:     1.874511, Tokens per Sec:    16531, Lr: 0.000300\n",
      "2021-08-01 19:29:25,242 - INFO - joeynmt.training - Epoch  17, Step:    44600, Batch Loss:     1.989591, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-01 19:29:51,522 - INFO - joeynmt.training - Epoch  17, Step:    44800, Batch Loss:     2.108735, Tokens per Sec:    16219, Lr: 0.000300\n",
      "2021-08-01 19:30:17,930 - INFO - joeynmt.training - Epoch  17, Step:    45000, Batch Loss:     2.154950, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-08-01 19:30:44,199 - INFO - joeynmt.training - Epoch  17, Step:    45200, Batch Loss:     2.012474, Tokens per Sec:    16226, Lr: 0.000300\n",
      "2021-08-01 19:31:10,935 - INFO - joeynmt.training - Epoch  17, Step:    45400, Batch Loss:     2.191172, Tokens per Sec:    16208, Lr: 0.000300\n",
      "2021-08-01 19:31:37,321 - INFO - joeynmt.training - Epoch  17, Step:    45600, Batch Loss:     2.317599, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-08-01 19:32:03,716 - INFO - joeynmt.training - Epoch  17, Step:    45800, Batch Loss:     2.045626, Tokens per Sec:    16223, Lr: 0.000300\n",
      "2021-08-01 19:32:30,227 - INFO - joeynmt.training - Epoch  17, Step:    46000, Batch Loss:     1.704297, Tokens per Sec:    16349, Lr: 0.000300\n",
      "2021-08-01 19:33:22,977 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:33:22,977 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:33:22,978 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:33:23,568 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:33:23,568 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:33:24,251 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:33:24,252 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:33:24,252 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:33:24,252 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:33:24,253 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:33:24,253 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:33:24,253 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:33:24,253 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-01 19:33:24,254 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:33:24,254 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:33:24,254 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:33:24,254 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a negative influence ?\n",
      "2021-08-01 19:33:24,254 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:33:24,255 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:33:24,255 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:33:24,255 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 19:33:24,255 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    46000: bleu:  23.64, loss: 111947.1953, ppl:   6.4296, duration: 54.0276s\n",
      "2021-08-01 19:33:50,474 - INFO - joeynmt.training - Epoch  17, Step:    46200, Batch Loss:     1.935026, Tokens per Sec:    16233, Lr: 0.000300\n",
      "2021-08-01 19:34:17,010 - INFO - joeynmt.training - Epoch  17, Step:    46400, Batch Loss:     2.114599, Tokens per Sec:    16210, Lr: 0.000300\n",
      "2021-08-01 19:34:33,100 - INFO - joeynmt.training - Epoch  17: total training loss 5600.60\n",
      "2021-08-01 19:34:33,101 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-08-01 19:34:43,802 - INFO - joeynmt.training - Epoch  18, Step:    46600, Batch Loss:     1.996387, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-08-01 19:35:10,224 - INFO - joeynmt.training - Epoch  18, Step:    46800, Batch Loss:     2.014101, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-01 19:35:36,460 - INFO - joeynmt.training - Epoch  18, Step:    47000, Batch Loss:     1.945955, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-01 19:36:03,284 - INFO - joeynmt.training - Epoch  18, Step:    47200, Batch Loss:     2.019654, Tokens per Sec:    16196, Lr: 0.000300\n",
      "2021-08-01 19:36:29,560 - INFO - joeynmt.training - Epoch  18, Step:    47400, Batch Loss:     2.120168, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-01 19:36:56,095 - INFO - joeynmt.training - Epoch  18, Step:    47600, Batch Loss:     2.204053, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-08-01 19:37:22,573 - INFO - joeynmt.training - Epoch  18, Step:    47800, Batch Loss:     1.875845, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-01 19:37:49,000 - INFO - joeynmt.training - Epoch  18, Step:    48000, Batch Loss:     2.123718, Tokens per Sec:    16243, Lr: 0.000300\n",
      "2021-08-01 19:38:42,046 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:38:42,047 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:38:42,047 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:38:42,637 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:38:42,637 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:38:43,320 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:38:43,320 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:38:43,320 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:38:43,321 - INFO - joeynmt.training - \tHypothesis: As we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:38:43,321 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:38:43,323 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:38:43,323 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:38:43,323 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures when possible .\n",
      "2021-08-01 19:38:43,323 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:38:43,324 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:38:43,324 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:38:43,324 - INFO - joeynmt.training - \tHypothesis: Why are false gods not uncertain ?\n",
      "2021-08-01 19:38:43,324 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:38:43,324 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:38:43,325 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:38:43,325 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 19:38:43,325 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    48000: bleu:  23.90, loss: 111625.4297, ppl:   6.3953, duration: 54.3243s\n",
      "2021-08-01 19:39:10,034 - INFO - joeynmt.training - Epoch  18, Step:    48200, Batch Loss:     2.088205, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-08-01 19:39:36,468 - INFO - joeynmt.training - Epoch  18, Step:    48400, Batch Loss:     2.054420, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-01 19:40:02,819 - INFO - joeynmt.training - Epoch  18, Step:    48600, Batch Loss:     1.868909, Tokens per Sec:    16248, Lr: 0.000300\n",
      "2021-08-01 19:40:29,719 - INFO - joeynmt.training - Epoch  18, Step:    48800, Batch Loss:     1.953177, Tokens per Sec:    16388, Lr: 0.000300\n",
      "2021-08-01 19:40:56,264 - INFO - joeynmt.training - Epoch  18, Step:    49000, Batch Loss:     2.372877, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-08-01 19:41:22,906 - INFO - joeynmt.training - Epoch  18, Step:    49200, Batch Loss:     1.869243, Tokens per Sec:    16388, Lr: 0.000300\n",
      "2021-08-01 19:41:30,925 - INFO - joeynmt.training - Epoch  18: total training loss 5528.46\n",
      "2021-08-01 19:41:30,926 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-08-01 19:41:49,686 - INFO - joeynmt.training - Epoch  19, Step:    49400, Batch Loss:     1.998130, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-01 19:42:16,396 - INFO - joeynmt.training - Epoch  19, Step:    49600, Batch Loss:     2.072613, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-01 19:42:42,870 - INFO - joeynmt.training - Epoch  19, Step:    49800, Batch Loss:     1.925968, Tokens per Sec:    16354, Lr: 0.000300\n",
      "2021-08-01 19:43:09,737 - INFO - joeynmt.training - Epoch  19, Step:    50000, Batch Loss:     2.030840, Tokens per Sec:    16196, Lr: 0.000300\n",
      "2021-08-01 19:44:01,948 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:44:01,948 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:44:01,949 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:44:02,588 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:44:02,588 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:44:03,352 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:44:03,354 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:44:03,354 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:44:03,354 - INFO - joeynmt.training - \tHypothesis: As we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:44:03,354 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:44:03,358 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:44:03,358 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:44:03,358 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and by studying the Scriptures if possible .\n",
      "2021-08-01 19:44:03,358 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:44:03,359 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:44:03,359 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:44:03,359 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have any negative influence ?\n",
      "2021-08-01 19:44:03,359 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:44:03,360 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:44:03,360 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:44:03,360 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is close to his people in a loving way .\n",
      "2021-08-01 19:44:03,361 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    50000: bleu:  24.05, loss: 110886.5078, ppl:   6.3173, duration: 53.6234s\n",
      "2021-08-01 19:44:30,422 - INFO - joeynmt.training - Epoch  19, Step:    50200, Batch Loss:     2.077672, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-01 19:44:56,816 - INFO - joeynmt.training - Epoch  19, Step:    50400, Batch Loss:     2.046990, Tokens per Sec:    16359, Lr: 0.000300\n",
      "2021-08-01 19:45:23,418 - INFO - joeynmt.training - Epoch  19, Step:    50600, Batch Loss:     1.920264, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-08-01 19:45:50,088 - INFO - joeynmt.training - Epoch  19, Step:    50800, Batch Loss:     1.553872, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-01 19:46:16,287 - INFO - joeynmt.training - Epoch  19, Step:    51000, Batch Loss:     2.043303, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-01 19:46:42,976 - INFO - joeynmt.training - Epoch  19, Step:    51200, Batch Loss:     1.947509, Tokens per Sec:    16434, Lr: 0.000300\n",
      "2021-08-01 19:47:09,540 - INFO - joeynmt.training - Epoch  19, Step:    51400, Batch Loss:     1.781476, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-01 19:47:35,979 - INFO - joeynmt.training - Epoch  19, Step:    51600, Batch Loss:     2.033335, Tokens per Sec:    16375, Lr: 0.000300\n",
      "2021-08-01 19:48:02,439 - INFO - joeynmt.training - Epoch  19, Step:    51800, Batch Loss:     2.210352, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-08-01 19:48:27,157 - INFO - joeynmt.training - Epoch  19: total training loss 5464.16\n",
      "2021-08-01 19:48:27,157 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-08-01 19:48:29,310 - INFO - joeynmt.training - Epoch  20, Step:    52000, Batch Loss:     2.034342, Tokens per Sec:    14678, Lr: 0.000300\n",
      "2021-08-01 19:49:22,683 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:49:22,683 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:49:22,683 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:49:23,281 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:49:23,281 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:49:23,981 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:49:23,982 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:49:23,982 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:49:23,983 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:49:23,983 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:49:23,983 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:49:23,983 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:49:23,984 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-01 19:49:23,984 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:49:23,984 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:49:23,984 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:49:23,985 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-01 19:49:23,985 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:49:23,985 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:49:23,985 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:49:23,986 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 19:49:23,986 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    52000: bleu:  24.19, loss: 110297.2969, ppl:   6.2557, duration: 54.6753s\n",
      "2021-08-01 19:49:50,651 - INFO - joeynmt.training - Epoch  20, Step:    52200, Batch Loss:     2.140847, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-01 19:50:17,141 - INFO - joeynmt.training - Epoch  20, Step:    52400, Batch Loss:     2.038621, Tokens per Sec:    15838, Lr: 0.000300\n",
      "2021-08-01 19:50:43,628 - INFO - joeynmt.training - Epoch  20, Step:    52600, Batch Loss:     2.205279, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-08-01 19:51:10,140 - INFO - joeynmt.training - Epoch  20, Step:    52800, Batch Loss:     1.836955, Tokens per Sec:    16376, Lr: 0.000300\n",
      "2021-08-01 19:51:36,786 - INFO - joeynmt.training - Epoch  20, Step:    53000, Batch Loss:     1.958873, Tokens per Sec:    16142, Lr: 0.000300\n",
      "2021-08-01 19:52:03,490 - INFO - joeynmt.training - Epoch  20, Step:    53200, Batch Loss:     1.870931, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-01 19:52:29,911 - INFO - joeynmt.training - Epoch  20, Step:    53400, Batch Loss:     2.128533, Tokens per Sec:    16510, Lr: 0.000300\n",
      "2021-08-01 19:52:56,263 - INFO - joeynmt.training - Epoch  20, Step:    53600, Batch Loss:     1.953762, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-08-01 19:53:22,812 - INFO - joeynmt.training - Epoch  20, Step:    53800, Batch Loss:     1.981221, Tokens per Sec:    16310, Lr: 0.000300\n",
      "2021-08-01 19:53:49,284 - INFO - joeynmt.training - Epoch  20, Step:    54000, Batch Loss:     2.006211, Tokens per Sec:    16280, Lr: 0.000300\n",
      "2021-08-01 19:54:40,722 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:54:40,723 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:54:40,723 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 19:54:41,301 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 19:54:41,301 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 19:54:42,353 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 19:54:42,354 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 19:54:42,354 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 19:54:42,355 - INFO - joeynmt.training - \tHypothesis: As we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 19:54:42,355 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 19:54:42,355 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 19:54:42,355 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 19:54:42,355 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and to read the Scriptures if possible .\n",
      "2021-08-01 19:54:42,356 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 19:54:42,356 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 19:54:42,356 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 19:54:42,356 - INFO - joeynmt.training - \tHypothesis: Why are false gods unless ?\n",
      "2021-08-01 19:54:42,356 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 19:54:42,357 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 19:54:42,357 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 19:54:42,357 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 19:54:42,357 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    54000: bleu:  23.93, loss: 109763.0312, ppl:   6.2004, duration: 53.0728s\n",
      "2021-08-01 19:55:09,207 - INFO - joeynmt.training - Epoch  20, Step:    54200, Batch Loss:     2.073109, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-01 19:55:35,873 - INFO - joeynmt.training - Epoch  20, Step:    54400, Batch Loss:     2.117132, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-01 19:56:02,310 - INFO - joeynmt.training - Epoch  20, Step:    54600, Batch Loss:     1.972799, Tokens per Sec:    16247, Lr: 0.000300\n",
      "2021-08-01 19:56:18,638 - INFO - joeynmt.training - Epoch  20: total training loss 5445.32\n",
      "2021-08-01 19:56:18,638 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-08-01 19:56:29,106 - INFO - joeynmt.training - Epoch  21, Step:    54800, Batch Loss:     2.020086, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-08-01 19:56:55,772 - INFO - joeynmt.training - Epoch  21, Step:    55000, Batch Loss:     2.050739, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-01 19:57:21,922 - INFO - joeynmt.training - Epoch  21, Step:    55200, Batch Loss:     2.218496, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-01 19:57:48,478 - INFO - joeynmt.training - Epoch  21, Step:    55400, Batch Loss:     2.103028, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-08-01 19:58:15,108 - INFO - joeynmt.training - Epoch  21, Step:    55600, Batch Loss:     2.045799, Tokens per Sec:    16322, Lr: 0.000300\n",
      "2021-08-01 19:58:41,609 - INFO - joeynmt.training - Epoch  21, Step:    55800, Batch Loss:     2.273182, Tokens per Sec:    16457, Lr: 0.000300\n",
      "2021-08-01 19:59:08,391 - INFO - joeynmt.training - Epoch  21, Step:    56000, Batch Loss:     2.094229, Tokens per Sec:    16320, Lr: 0.000300\n",
      "2021-08-01 19:59:59,976 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 19:59:59,977 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 19:59:59,977 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:00:00,603 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:00:00,604 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:00:01,324 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:00:01,325 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:00:01,325 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:00:01,325 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:00:01,325 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:00:01,326 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:00:01,327 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:00:01,328 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-01 20:00:01,328 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:00:01,328 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:00:01,328 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:00:01,329 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a variety of things ?\n",
      "2021-08-01 20:00:01,329 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:00:01,329 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:00:01,329 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:00:01,329 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:00:01,330 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    56000: bleu:  24.38, loss: 109255.6641, ppl:   6.1483, duration: 52.9386s\n",
      "2021-08-01 20:00:27,966 - INFO - joeynmt.training - Epoch  21, Step:    56200, Batch Loss:     2.130153, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-08-01 20:00:54,159 - INFO - joeynmt.training - Epoch  21, Step:    56400, Batch Loss:     2.204028, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-01 20:01:20,743 - INFO - joeynmt.training - Epoch  21, Step:    56600, Batch Loss:     1.890511, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-01 20:01:47,219 - INFO - joeynmt.training - Epoch  21, Step:    56800, Batch Loss:     2.217894, Tokens per Sec:    16461, Lr: 0.000300\n",
      "2021-08-01 20:02:13,739 - INFO - joeynmt.training - Epoch  21, Step:    57000, Batch Loss:     2.213592, Tokens per Sec:    16114, Lr: 0.000300\n",
      "2021-08-01 20:02:40,329 - INFO - joeynmt.training - Epoch  21, Step:    57200, Batch Loss:     2.040290, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-08-01 20:03:07,153 - INFO - joeynmt.training - Epoch  21, Step:    57400, Batch Loss:     1.927005, Tokens per Sec:    16488, Lr: 0.000300\n",
      "2021-08-01 20:03:14,687 - INFO - joeynmt.training - Epoch  21: total training loss 5404.36\n",
      "2021-08-01 20:03:14,688 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-08-01 20:03:33,751 - INFO - joeynmt.training - Epoch  22, Step:    57600, Batch Loss:     2.038106, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-08-01 20:04:00,585 - INFO - joeynmt.training - Epoch  22, Step:    57800, Batch Loss:     1.616167, Tokens per Sec:    16404, Lr: 0.000300\n",
      "2021-08-01 20:04:27,060 - INFO - joeynmt.training - Epoch  22, Step:    58000, Batch Loss:     2.098937, Tokens per Sec:    16333, Lr: 0.000300\n",
      "2021-08-01 20:05:19,320 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:05:19,320 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:05:19,320 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:05:20,556 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:05:20,557 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:05:20,557 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:05:20,557 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:05:20,558 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:05:20,558 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:05:20,558 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:05:20,559 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-01 20:05:20,559 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:05:20,560 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:05:20,560 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:05:20,560 - INFO - joeynmt.training - \tHypothesis: Why are false gods not uncertain ?\n",
      "2021-08-01 20:05:20,560 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:05:20,561 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:05:20,561 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:05:20,561 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:05:20,561 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step    58000: bleu:  24.33, loss: 109463.9375, ppl:   6.1696, duration: 53.5002s\n",
      "2021-08-01 20:05:47,399 - INFO - joeynmt.training - Epoch  22, Step:    58200, Batch Loss:     1.918994, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-08-01 20:06:14,037 - INFO - joeynmt.training - Epoch  22, Step:    58400, Batch Loss:     2.001975, Tokens per Sec:    16182, Lr: 0.000300\n",
      "2021-08-01 20:06:40,437 - INFO - joeynmt.training - Epoch  22, Step:    58600, Batch Loss:     1.933352, Tokens per Sec:    16334, Lr: 0.000300\n",
      "2021-08-01 20:07:06,802 - INFO - joeynmt.training - Epoch  22, Step:    58800, Batch Loss:     2.117209, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-01 20:07:33,258 - INFO - joeynmt.training - Epoch  22, Step:    59000, Batch Loss:     1.999450, Tokens per Sec:    16449, Lr: 0.000300\n",
      "2021-08-01 20:07:59,849 - INFO - joeynmt.training - Epoch  22, Step:    59200, Batch Loss:     1.880932, Tokens per Sec:    16157, Lr: 0.000300\n",
      "2021-08-01 20:08:26,023 - INFO - joeynmt.training - Epoch  22, Step:    59400, Batch Loss:     2.055977, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-01 20:08:52,658 - INFO - joeynmt.training - Epoch  22, Step:    59600, Batch Loss:     2.123688, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-08-01 20:09:19,327 - INFO - joeynmt.training - Epoch  22, Step:    59800, Batch Loss:     1.879623, Tokens per Sec:    16402, Lr: 0.000300\n",
      "2021-08-01 20:09:45,720 - INFO - joeynmt.training - Epoch  22, Step:    60000, Batch Loss:     2.047382, Tokens per Sec:    16397, Lr: 0.000300\n",
      "2021-08-01 20:10:36,934 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:10:36,934 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:10:36,935 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:10:37,517 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:10:37,517 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:10:38,273 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:10:38,274 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:10:38,274 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:10:38,275 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he speaks to us .\n",
      "2021-08-01 20:10:38,275 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:10:38,275 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:10:38,275 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:10:38,276 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-01 20:10:38,276 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:10:38,276 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:10:38,276 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:10:38,277 - INFO - joeynmt.training - \tHypothesis: Why are false gods unique ?\n",
      "2021-08-01 20:10:38,277 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:10:38,277 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:10:38,277 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:10:38,277 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:10:38,278 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step    60000: bleu:  24.36, loss: 108825.4688, ppl:   6.1045, duration: 52.5575s\n",
      "2021-08-01 20:11:03,823 - INFO - joeynmt.training - Epoch  22: total training loss 5370.94\n",
      "2021-08-01 20:11:03,823 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-08-01 20:11:05,132 - INFO - joeynmt.training - Epoch  23, Step:    60200, Batch Loss:     1.833255, Tokens per Sec:    11448, Lr: 0.000300\n",
      "2021-08-01 20:11:31,828 - INFO - joeynmt.training - Epoch  23, Step:    60400, Batch Loss:     1.978406, Tokens per Sec:    16478, Lr: 0.000300\n",
      "2021-08-01 20:11:58,278 - INFO - joeynmt.training - Epoch  23, Step:    60600, Batch Loss:     1.866800, Tokens per Sec:    16327, Lr: 0.000300\n",
      "2021-08-01 20:12:24,780 - INFO - joeynmt.training - Epoch  23, Step:    60800, Batch Loss:     1.267207, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-08-01 20:12:51,485 - INFO - joeynmt.training - Epoch  23, Step:    61000, Batch Loss:     2.060478, Tokens per Sec:    16124, Lr: 0.000300\n",
      "2021-08-01 20:13:17,862 - INFO - joeynmt.training - Epoch  23, Step:    61200, Batch Loss:     2.295913, Tokens per Sec:    16364, Lr: 0.000300\n",
      "2021-08-01 20:13:44,316 - INFO - joeynmt.training - Epoch  23, Step:    61400, Batch Loss:     1.669954, Tokens per Sec:    16212, Lr: 0.000300\n",
      "2021-08-01 20:14:10,735 - INFO - joeynmt.training - Epoch  23, Step:    61600, Batch Loss:     1.845083, Tokens per Sec:    16216, Lr: 0.000300\n",
      "2021-08-01 20:14:37,299 - INFO - joeynmt.training - Epoch  23, Step:    61800, Batch Loss:     2.198405, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-08-01 20:15:04,238 - INFO - joeynmt.training - Epoch  23, Step:    62000, Batch Loss:     1.985226, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-08-01 20:15:57,017 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:15:57,018 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:15:57,018 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:15:57,652 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:15:57,652 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:15:58,394 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:15:58,396 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:15:58,396 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:15:58,396 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he speaks to us .\n",
      "2021-08-01 20:15:58,397 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:15:58,397 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:15:58,397 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:15:58,397 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in preaching and to read the Scriptures if possible .\n",
      "2021-08-01 20:15:58,398 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:15:58,398 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:15:58,398 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:15:58,398 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-01 20:15:58,398 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:15:58,399 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:15:58,399 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:15:58,399 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:15:58,399 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    62000: bleu:  24.70, loss: 108211.8828, ppl:   6.0425, duration: 54.1609s\n",
      "2021-08-01 20:16:25,401 - INFO - joeynmt.training - Epoch  23, Step:    62200, Batch Loss:     1.871793, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-08-01 20:16:51,918 - INFO - joeynmt.training - Epoch  23, Step:    62400, Batch Loss:     1.992326, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-08-01 20:17:18,698 - INFO - joeynmt.training - Epoch  23, Step:    62600, Batch Loss:     1.946430, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-01 20:17:45,120 - INFO - joeynmt.training - Epoch  23, Step:    62800, Batch Loss:     2.061099, Tokens per Sec:    16383, Lr: 0.000300\n",
      "2021-08-01 20:18:01,372 - INFO - joeynmt.training - Epoch  23: total training loss 5326.38\n",
      "2021-08-01 20:18:01,373 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-08-01 20:18:12,130 - INFO - joeynmt.training - Epoch  24, Step:    63000, Batch Loss:     1.975385, Tokens per Sec:    15323, Lr: 0.000300\n",
      "2021-08-01 20:18:38,678 - INFO - joeynmt.training - Epoch  24, Step:    63200, Batch Loss:     1.646857, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-08-01 20:19:05,513 - INFO - joeynmt.training - Epoch  24, Step:    63400, Batch Loss:     1.983538, Tokens per Sec:    16334, Lr: 0.000300\n",
      "2021-08-01 20:19:31,859 - INFO - joeynmt.training - Epoch  24, Step:    63600, Batch Loss:     1.843592, Tokens per Sec:    16466, Lr: 0.000300\n",
      "2021-08-01 20:19:58,528 - INFO - joeynmt.training - Epoch  24, Step:    63800, Batch Loss:     1.804968, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-01 20:20:25,180 - INFO - joeynmt.training - Epoch  24, Step:    64000, Batch Loss:     2.129141, Tokens per Sec:    16300, Lr: 0.000300\n",
      "2021-08-01 20:21:18,203 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:21:18,203 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:21:18,203 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:21:18,811 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:21:18,811 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:21:19,559 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:21:19,560 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:21:19,560 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:21:19,560 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:21:19,560 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:21:19,561 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:21:19,561 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:21:19,561 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-01 20:21:19,561 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:21:19,562 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:21:19,562 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:21:19,562 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-01 20:21:19,562 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:21:19,563 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:21:19,563 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:21:19,563 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:21:19,563 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step    64000: bleu:  24.73, loss: 107587.0312, ppl:   5.9801, duration: 54.3826s\n",
      "2021-08-01 20:21:46,241 - INFO - joeynmt.training - Epoch  24, Step:    64200, Batch Loss:     1.893078, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-01 20:22:13,007 - INFO - joeynmt.training - Epoch  24, Step:    64400, Batch Loss:     1.528394, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-01 20:22:39,368 - INFO - joeynmt.training - Epoch  24, Step:    64600, Batch Loss:     1.966627, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-01 20:23:06,035 - INFO - joeynmt.training - Epoch  24, Step:    64800, Batch Loss:     1.711541, Tokens per Sec:    16377, Lr: 0.000300\n",
      "2021-08-01 20:23:32,459 - INFO - joeynmt.training - Epoch  24, Step:    65000, Batch Loss:     2.074402, Tokens per Sec:    16220, Lr: 0.000300\n",
      "2021-08-01 20:23:59,050 - INFO - joeynmt.training - Epoch  24, Step:    65200, Batch Loss:     1.968364, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-01 20:24:25,493 - INFO - joeynmt.training - Epoch  24, Step:    65400, Batch Loss:     2.145235, Tokens per Sec:    16311, Lr: 0.000300\n",
      "2021-08-01 20:24:52,226 - INFO - joeynmt.training - Epoch  24, Step:    65600, Batch Loss:     2.112034, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-08-01 20:24:59,784 - INFO - joeynmt.training - Epoch  24: total training loss 5307.12\n",
      "2021-08-01 20:24:59,784 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-08-01 20:25:19,112 - INFO - joeynmt.training - Epoch  25, Step:    65800, Batch Loss:     1.649358, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-01 20:25:45,744 - INFO - joeynmt.training - Epoch  25, Step:    66000, Batch Loss:     2.062182, Tokens per Sec:    16152, Lr: 0.000300\n",
      "2021-08-01 20:26:38,909 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:26:38,909 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:26:38,910 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:26:39,525 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:26:39,526 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:26:40,576 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:26:40,577 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:26:40,577 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:26:40,578 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:26:40,578 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:26:40,578 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:26:40,578 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:26:40,579 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-01 20:26:40,579 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:26:40,579 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:26:40,579 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:26:40,579 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unusual ?\n",
      "2021-08-01 20:26:40,580 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:26:40,580 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:26:40,580 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:26:40,580 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:26:40,581 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    66000: bleu:  25.01, loss: 107153.3125, ppl:   5.9371, duration: 54.8356s\n",
      "2021-08-01 20:27:07,489 - INFO - joeynmt.training - Epoch  25, Step:    66200, Batch Loss:     1.752356, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-08-01 20:27:34,036 - INFO - joeynmt.training - Epoch  25, Step:    66400, Batch Loss:     1.979709, Tokens per Sec:    16274, Lr: 0.000300\n",
      "2021-08-01 20:28:00,441 - INFO - joeynmt.training - Epoch  25, Step:    66600, Batch Loss:     1.956674, Tokens per Sec:    16141, Lr: 0.000300\n",
      "2021-08-01 20:28:26,808 - INFO - joeynmt.training - Epoch  25, Step:    66800, Batch Loss:     2.090514, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-01 20:28:53,213 - INFO - joeynmt.training - Epoch  25, Step:    67000, Batch Loss:     1.993873, Tokens per Sec:    16322, Lr: 0.000300\n",
      "2021-08-01 20:29:19,905 - INFO - joeynmt.training - Epoch  25, Step:    67200, Batch Loss:     2.112533, Tokens per Sec:    16195, Lr: 0.000300\n",
      "2021-08-01 20:29:46,522 - INFO - joeynmt.training - Epoch  25, Step:    67400, Batch Loss:     1.459475, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-08-01 20:30:12,987 - INFO - joeynmt.training - Epoch  25, Step:    67600, Batch Loss:     2.170304, Tokens per Sec:    16070, Lr: 0.000300\n",
      "2021-08-01 20:30:39,670 - INFO - joeynmt.training - Epoch  25, Step:    67800, Batch Loss:     1.963935, Tokens per Sec:    16452, Lr: 0.000300\n",
      "2021-08-01 20:31:06,212 - INFO - joeynmt.training - Epoch  25, Step:    68000, Batch Loss:     2.088921, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-08-01 20:31:57,144 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:31:57,145 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:31:57,145 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:31:57,775 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:31:57,775 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:31:58,543 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:31:58,544 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:31:58,544 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:31:58,544 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:31:58,546 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:31:58,546 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:31:58,546 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:31:58,546 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-01 20:31:58,546 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:31:58,547 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:31:58,547 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:31:58,547 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have uncertain things ?\n",
      "2021-08-01 20:31:58,547 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:31:58,548 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:31:58,548 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:31:58,548 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:31:58,548 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    68000: bleu:  24.99, loss: 106843.2734, ppl:   5.9066, duration: 52.3355s\n",
      "2021-08-01 20:32:25,102 - INFO - joeynmt.training - Epoch  25, Step:    68200, Batch Loss:     1.990701, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-08-01 20:32:50,754 - INFO - joeynmt.training - Epoch  25: total training loss 5283.16\n",
      "2021-08-01 20:32:50,754 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-08-01 20:32:51,885 - INFO - joeynmt.training - Epoch  26, Step:    68400, Batch Loss:     2.153952, Tokens per Sec:    12320, Lr: 0.000300\n",
      "2021-08-01 20:33:18,623 - INFO - joeynmt.training - Epoch  26, Step:    68600, Batch Loss:     1.789477, Tokens per Sec:    16142, Lr: 0.000300\n",
      "2021-08-01 20:33:45,074 - INFO - joeynmt.training - Epoch  26, Step:    68800, Batch Loss:     2.049939, Tokens per Sec:    16346, Lr: 0.000300\n",
      "2021-08-01 20:34:11,815 - INFO - joeynmt.training - Epoch  26, Step:    69000, Batch Loss:     1.727012, Tokens per Sec:    16377, Lr: 0.000300\n",
      "2021-08-01 20:34:38,117 - INFO - joeynmt.training - Epoch  26, Step:    69200, Batch Loss:     1.858834, Tokens per Sec:    15891, Lr: 0.000300\n",
      "2021-08-01 20:35:04,759 - INFO - joeynmt.training - Epoch  26, Step:    69400, Batch Loss:     1.888526, Tokens per Sec:    16212, Lr: 0.000300\n",
      "2021-08-01 20:35:30,983 - INFO - joeynmt.training - Epoch  26, Step:    69600, Batch Loss:     1.789474, Tokens per Sec:    16333, Lr: 0.000300\n",
      "2021-08-01 20:35:57,716 - INFO - joeynmt.training - Epoch  26, Step:    69800, Batch Loss:     2.108758, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-01 20:36:24,361 - INFO - joeynmt.training - Epoch  26, Step:    70000, Batch Loss:     1.720348, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-08-01 20:37:16,595 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:37:16,595 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:37:16,595 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:37:17,188 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:37:17,188 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:37:18,243 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:37:18,244 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:37:18,244 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:37:18,244 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:37:18,244 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:37:18,245 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:37:18,245 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:37:18,245 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-01 20:37:18,245 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:37:18,246 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:37:18,246 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:37:18,246 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-01 20:37:18,246 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:37:18,247 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:37:18,247 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:37:18,247 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:37:18,247 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step    70000: bleu:  25.12, loss: 106771.4297, ppl:   5.8996, duration: 53.8857s\n",
      "2021-08-01 20:37:44,726 - INFO - joeynmt.training - Epoch  26, Step:    70200, Batch Loss:     2.036578, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-01 20:38:11,272 - INFO - joeynmt.training - Epoch  26, Step:    70400, Batch Loss:     2.065350, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-08-01 20:38:37,690 - INFO - joeynmt.training - Epoch  26, Step:    70600, Batch Loss:     1.923531, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-01 20:39:04,015 - INFO - joeynmt.training - Epoch  26, Step:    70800, Batch Loss:     2.090230, Tokens per Sec:    16189, Lr: 0.000300\n",
      "2021-08-01 20:39:30,600 - INFO - joeynmt.training - Epoch  26, Step:    71000, Batch Loss:     2.034437, Tokens per Sec:    16292, Lr: 0.000300\n",
      "2021-08-01 20:39:48,384 - INFO - joeynmt.training - Epoch  26: total training loss 5267.16\n",
      "2021-08-01 20:39:48,385 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-08-01 20:39:57,477 - INFO - joeynmt.training - Epoch  27, Step:    71200, Batch Loss:     1.942551, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-01 20:40:24,039 - INFO - joeynmt.training - Epoch  27, Step:    71400, Batch Loss:     1.940728, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-08-01 20:40:50,638 - INFO - joeynmt.training - Epoch  27, Step:    71600, Batch Loss:     2.136386, Tokens per Sec:    16316, Lr: 0.000300\n",
      "2021-08-01 20:41:17,222 - INFO - joeynmt.training - Epoch  27, Step:    71800, Batch Loss:     1.833012, Tokens per Sec:    16276, Lr: 0.000300\n",
      "2021-08-01 20:41:43,794 - INFO - joeynmt.training - Epoch  27, Step:    72000, Batch Loss:     1.973438, Tokens per Sec:    16360, Lr: 0.000300\n",
      "2021-08-01 20:42:36,483 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:42:36,484 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:42:36,484 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:42:37,117 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:42:37,117 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:42:37,826 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:42:37,827 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:42:37,828 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:42:37,828 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:42:37,828 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:42:37,829 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:42:37,829 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:42:37,829 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-01 20:42:37,829 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:42:37,830 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:42:37,830 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:42:37,830 - INFO - joeynmt.training - \tHypothesis: Why do false gods not exist ?\n",
      "2021-08-01 20:42:37,830 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:42:37,831 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:42:37,831 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:42:37,831 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:42:37,831 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    72000: bleu:  25.11, loss: 106725.8672, ppl:   5.8951, duration: 54.0371s\n",
      "2021-08-01 20:43:04,512 - INFO - joeynmt.training - Epoch  27, Step:    72200, Batch Loss:     1.736582, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-08-01 20:43:31,096 - INFO - joeynmt.training - Epoch  27, Step:    72400, Batch Loss:     2.007137, Tokens per Sec:    16390, Lr: 0.000300\n",
      "2021-08-01 20:43:57,430 - INFO - joeynmt.training - Epoch  27, Step:    72600, Batch Loss:     1.682969, Tokens per Sec:    16217, Lr: 0.000300\n",
      "2021-08-01 20:44:23,927 - INFO - joeynmt.training - Epoch  27, Step:    72800, Batch Loss:     1.986254, Tokens per Sec:    16350, Lr: 0.000300\n",
      "2021-08-01 20:44:50,357 - INFO - joeynmt.training - Epoch  27, Step:    73000, Batch Loss:     2.071044, Tokens per Sec:    16156, Lr: 0.000300\n",
      "2021-08-01 20:45:17,149 - INFO - joeynmt.training - Epoch  27, Step:    73200, Batch Loss:     1.908341, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-01 20:45:43,983 - INFO - joeynmt.training - Epoch  27, Step:    73400, Batch Loss:     2.066606, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-01 20:46:10,599 - INFO - joeynmt.training - Epoch  27, Step:    73600, Batch Loss:     1.802267, Tokens per Sec:    16347, Lr: 0.000300\n",
      "2021-08-01 20:46:36,912 - INFO - joeynmt.training - Epoch  27, Step:    73800, Batch Loss:     1.983882, Tokens per Sec:    16180, Lr: 0.000300\n",
      "2021-08-01 20:46:45,701 - INFO - joeynmt.training - Epoch  27: total training loss 5222.80\n",
      "2021-08-01 20:46:45,701 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-08-01 20:47:03,767 - INFO - joeynmt.training - Epoch  28, Step:    74000, Batch Loss:     1.951961, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-08-01 20:47:55,892 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:47:55,893 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:47:55,893 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:47:57,618 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:47:57,619 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:47:57,620 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:47:57,620 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:47:57,620 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:47:57,621 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:47:57,621 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:47:57,621 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-01 20:47:57,621 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:47:57,622 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:47:57,622 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:47:57,622 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unusual ?\n",
      "2021-08-01 20:47:57,622 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:47:57,623 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:47:57,623 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:47:57,623 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:47:57,623 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step    74000: bleu:  25.05, loss: 107125.7734, ppl:   5.9344, duration: 53.8560s\n",
      "2021-08-01 20:48:24,674 - INFO - joeynmt.training - Epoch  28, Step:    74200, Batch Loss:     1.925686, Tokens per Sec:    16070, Lr: 0.000300\n",
      "2021-08-01 20:48:51,300 - INFO - joeynmt.training - Epoch  28, Step:    74400, Batch Loss:     2.007663, Tokens per Sec:    16297, Lr: 0.000300\n",
      "2021-08-01 20:49:17,953 - INFO - joeynmt.training - Epoch  28, Step:    74600, Batch Loss:     1.812403, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-08-01 20:49:44,136 - INFO - joeynmt.training - Epoch  28, Step:    74800, Batch Loss:     2.047225, Tokens per Sec:    16269, Lr: 0.000300\n",
      "2021-08-01 20:50:10,686 - INFO - joeynmt.training - Epoch  28, Step:    75000, Batch Loss:     2.046589, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-01 20:50:37,108 - INFO - joeynmt.training - Epoch  28, Step:    75200, Batch Loss:     1.900916, Tokens per Sec:    16235, Lr: 0.000300\n",
      "2021-08-01 20:51:03,480 - INFO - joeynmt.training - Epoch  28, Step:    75400, Batch Loss:     1.759665, Tokens per Sec:    16346, Lr: 0.000300\n",
      "2021-08-01 20:51:30,274 - INFO - joeynmt.training - Epoch  28, Step:    75600, Batch Loss:     2.050005, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-01 20:51:57,131 - INFO - joeynmt.training - Epoch  28, Step:    75800, Batch Loss:     1.925406, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-08-01 20:52:23,669 - INFO - joeynmt.training - Epoch  28, Step:    76000, Batch Loss:     1.909851, Tokens per Sec:    16278, Lr: 0.000300\n",
      "2021-08-01 20:53:15,055 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:53:15,056 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:53:15,056 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:53:15,635 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:53:15,635 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:53:16,363 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:53:16,364 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:53:16,364 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:53:16,364 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:53:16,365 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:53:16,365 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:53:16,365 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:53:16,365 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-01 20:53:16,366 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:53:16,366 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:53:16,366 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:53:16,366 - INFO - joeynmt.training - \tHypothesis: Why are false gods uncertain ?\n",
      "2021-08-01 20:53:16,367 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:53:16,367 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:53:16,367 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:53:16,367 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:53:16,368 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step    76000: bleu:  25.47, loss: 105909.9062, ppl:   5.8157, duration: 52.6983s\n",
      "2021-08-01 20:53:43,172 - INFO - joeynmt.training - Epoch  28, Step:    76200, Batch Loss:     2.006179, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-01 20:54:09,908 - INFO - joeynmt.training - Epoch  28, Step:    76400, Batch Loss:     1.872630, Tokens per Sec:    15951, Lr: 0.000300\n",
      "2021-08-01 20:54:36,287 - INFO - joeynmt.training - Epoch  28, Step:    76600, Batch Loss:     1.966404, Tokens per Sec:    16473, Lr: 0.000300\n",
      "2021-08-01 20:54:36,297 - INFO - joeynmt.training - Epoch  28: total training loss 5200.10\n",
      "2021-08-01 20:54:36,297 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-08-01 20:55:03,234 - INFO - joeynmt.training - Epoch  29, Step:    76800, Batch Loss:     1.880306, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-08-01 20:55:29,752 - INFO - joeynmt.training - Epoch  29, Step:    77000, Batch Loss:     1.684737, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-01 20:55:56,323 - INFO - joeynmt.training - Epoch  29, Step:    77200, Batch Loss:     2.106439, Tokens per Sec:    16352, Lr: 0.000300\n",
      "2021-08-01 20:56:22,893 - INFO - joeynmt.training - Epoch  29, Step:    77400, Batch Loss:     2.497272, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-01 20:56:49,411 - INFO - joeynmt.training - Epoch  29, Step:    77600, Batch Loss:     1.774153, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-01 20:57:15,693 - INFO - joeynmt.training - Epoch  29, Step:    77800, Batch Loss:     1.903067, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-01 20:57:41,998 - INFO - joeynmt.training - Epoch  29, Step:    78000, Batch Loss:     1.863184, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-08-01 20:58:33,044 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 20:58:33,044 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 20:58:33,045 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 20:58:33,631 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 20:58:33,632 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 20:58:34,711 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 20:58:34,712 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 20:58:34,712 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 20:58:34,712 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 20:58:34,713 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 20:58:34,713 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 20:58:34,713 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 20:58:34,713 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible by preaching and studying the Scriptures as possible .\n",
      "2021-08-01 20:58:34,714 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 20:58:34,714 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 20:58:34,714 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 20:58:34,714 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-01 20:58:34,714 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 20:58:34,715 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 20:58:34,715 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 20:58:34,715 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 20:58:34,715 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    78000: bleu:  25.39, loss: 105521.0156, ppl:   5.7782, duration: 52.7170s\n",
      "2021-08-01 20:59:01,727 - INFO - joeynmt.training - Epoch  29, Step:    78200, Batch Loss:     2.084487, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-08-01 20:59:28,159 - INFO - joeynmt.training - Epoch  29, Step:    78400, Batch Loss:     1.960725, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-01 20:59:54,709 - INFO - joeynmt.training - Epoch  29, Step:    78600, Batch Loss:     1.931796, Tokens per Sec:    16477, Lr: 0.000300\n",
      "2021-08-01 21:00:21,275 - INFO - joeynmt.training - Epoch  29, Step:    78800, Batch Loss:     1.836222, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-01 21:00:47,607 - INFO - joeynmt.training - Epoch  29, Step:    79000, Batch Loss:     2.073796, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-08-01 21:01:14,276 - INFO - joeynmt.training - Epoch  29, Step:    79200, Batch Loss:     1.480512, Tokens per Sec:    16282, Lr: 0.000300\n",
      "2021-08-01 21:01:32,534 - INFO - joeynmt.training - Epoch  29: total training loss 5190.30\n",
      "2021-08-01 21:01:32,534 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-08-01 21:01:40,961 - INFO - joeynmt.training - Epoch  30, Step:    79400, Batch Loss:     1.876145, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-08-01 21:02:07,564 - INFO - joeynmt.training - Epoch  30, Step:    79600, Batch Loss:     1.907858, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-08-01 21:02:34,235 - INFO - joeynmt.training - Epoch  30, Step:    79800, Batch Loss:     1.747691, Tokens per Sec:    16218, Lr: 0.000300\n",
      "2021-08-01 21:03:01,291 - INFO - joeynmt.training - Epoch  30, Step:    80000, Batch Loss:     1.772973, Tokens per Sec:    16417, Lr: 0.000300\n",
      "2021-08-01 21:03:52,565 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 21:03:52,566 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 21:03:52,566 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 21:03:53,200 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 21:03:53,200 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 21:03:53,936 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 21:03:53,937 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 21:03:53,937 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 21:03:53,937 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 21:03:53,937 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 21:03:53,938 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 21:03:53,938 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 21:03:53,938 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-01 21:03:53,938 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 21:03:53,939 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 21:03:53,939 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 21:03:53,939 - INFO - joeynmt.training - \tHypothesis: Why do false gods not be uncertain ?\n",
      "2021-08-01 21:03:53,939 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 21:03:53,940 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 21:03:53,940 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 21:03:53,940 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-01 21:03:53,940 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    80000: bleu:  25.47, loss: 105212.8203, ppl:   5.7487, duration: 52.6487s\n",
      "2021-08-01 21:04:20,803 - INFO - joeynmt.training - Epoch  30, Step:    80200, Batch Loss:     1.868784, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-01 21:04:47,426 - INFO - joeynmt.training - Epoch  30, Step:    80400, Batch Loss:     1.894566, Tokens per Sec:    16278, Lr: 0.000300\n",
      "2021-08-01 21:05:14,139 - INFO - joeynmt.training - Epoch  30, Step:    80600, Batch Loss:     1.753883, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-01 21:05:40,693 - INFO - joeynmt.training - Epoch  30, Step:    80800, Batch Loss:     1.851079, Tokens per Sec:    16602, Lr: 0.000300\n",
      "2021-08-01 21:06:07,176 - INFO - joeynmt.training - Epoch  30, Step:    81000, Batch Loss:     1.901277, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-01 21:06:33,726 - INFO - joeynmt.training - Epoch  30, Step:    81200, Batch Loss:     1.998691, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-01 21:06:59,923 - INFO - joeynmt.training - Epoch  30, Step:    81400, Batch Loss:     2.021750, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-08-01 21:07:26,632 - INFO - joeynmt.training - Epoch  30, Step:    81600, Batch Loss:     1.912540, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-08-01 21:07:53,196 - INFO - joeynmt.training - Epoch  30, Step:    81800, Batch Loss:     1.917121, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-01 21:08:19,769 - INFO - joeynmt.training - Epoch  30, Step:    82000, Batch Loss:     2.156891, Tokens per Sec:    16271, Lr: 0.000300\n",
      "2021-08-01 21:09:12,372 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 21:09:12,372 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 21:09:12,373 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 21:09:12,953 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 21:09:12,953 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 21:09:14,058 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 21:09:14,059 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-01 21:09:14,059 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-01 21:09:14,059 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-01 21:09:14,059 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 21:09:14,060 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-01 21:09:14,060 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-01 21:09:14,060 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-01 21:09:14,060 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 21:09:14,060 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-01 21:09:14,061 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-01 21:09:14,061 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-01 21:09:14,061 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 21:09:14,061 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-01 21:09:14,061 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-01 21:09:14,062 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-01 21:09:14,062 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    82000: bleu:  25.81, loss: 104627.1328, ppl:   5.6930, duration: 54.2925s\n",
      "2021-08-01 21:09:23,077 - INFO - joeynmt.training - Epoch  30: total training loss 5153.35\n",
      "2021-08-01 21:09:23,078 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-08-01 21:09:23,078 - INFO - joeynmt.training - Best validation result (greedy) at step    82000:   5.69 ppl.\n",
      "2021-08-01 21:09:23,098 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
      "2021-08-01 21:09:23,432 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-01 21:09:23,618 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-01 21:09:23,679 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe.en)...\n",
      "2021-08-01 21:10:20,849 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 21:10:20,850 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 21:10:20,850 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 21:10:21,416 - INFO - joeynmt.prediction -  dev bleu[13a]:  26.27 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-01 21:10:21,421 - INFO - joeynmt.prediction - Translations saved to: models/lgen_reverse_transformer/00082000.hyps.dev\n",
      "2021-08-01 21:10:21,422 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe.en)...\n",
      "2021-08-01 21:11:20,098 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 21:11:20,098 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 21:11:20,098 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 21:11:20,673 - INFO - joeynmt.prediction - test bleu[13a]:  26.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-01 21:11:20,678 - INFO - joeynmt.prediction - Translations saved to: models/lgen_reverse_transformer/00082000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_lgen.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3uHyk_ECVtn",
    "outputId": "46655e3d-8df3-406c-b4f6-7e483881b71d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2000\tLoss: 225537.12500\tPPL: 42.48461\tbleu: 1.91223\tLR: 0.00030000\t*\n",
      "Steps: 4000\tLoss: 186507.21875\tPPL: 22.20552\tbleu: 6.78872\tLR: 0.00030000\t*\n",
      "Steps: 6000\tLoss: 169341.76562\tPPL: 16.69313\tbleu: 9.80236\tLR: 0.00030000\t*\n",
      "Steps: 8000\tLoss: 158586.51562\tPPL: 13.96020\tbleu: 11.66645\tLR: 0.00030000\t*\n",
      "Steps: 10000\tLoss: 150443.31250\tPPL: 12.19279\tbleu: 13.82781\tLR: 0.00030000\t*\n",
      "Steps: 12000\tLoss: 144066.60938\tPPL: 10.96648\tbleu: 15.39399\tLR: 0.00030000\t*\n",
      "Steps: 14000\tLoss: 139397.29688\tPPL: 10.14748\tbleu: 17.02612\tLR: 0.00030000\t*\n",
      "Steps: 16000\tLoss: 134982.57812\tPPL: 9.42946\tbleu: 18.16888\tLR: 0.00030000\t*\n",
      "Steps: 18000\tLoss: 132093.65625\tPPL: 8.98733\tbleu: 18.90839\tLR: 0.00030000\t*\n",
      "Steps: 20000\tLoss: 128846.18750\tPPL: 8.51502\tbleu: 19.27863\tLR: 0.00030000\t*\n",
      "Steps: 22000\tLoss: 126759.35938\tPPL: 8.22470\tbleu: 19.87707\tLR: 0.00030000\t*\n",
      "Steps: 24000\tLoss: 124511.36719\tPPL: 7.92303\tbleu: 20.25674\tLR: 0.00030000\t*\n",
      "Steps: 26000\tLoss: 122831.87500\tPPL: 7.70489\tbleu: 20.81013\tLR: 0.00030000\t*\n",
      "Steps: 28000\tLoss: 121200.50000\tPPL: 7.49875\tbleu: 21.37597\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 119474.71094\tPPL: 7.28668\tbleu: 21.73228\tLR: 0.00030000\t*\n",
      "Steps: 32000\tLoss: 118343.30469\tPPL: 7.15092\tbleu: 21.99319\tLR: 0.00030000\t*\n",
      "Steps: 34000\tLoss: 117510.20312\tPPL: 7.05257\tbleu: 22.07646\tLR: 0.00030000\t*\n",
      "Steps: 36000\tLoss: 116700.95312\tPPL: 6.95833\tbleu: 22.23505\tLR: 0.00030000\t*\n",
      "Steps: 38000\tLoss: 115166.03125\tPPL: 6.78303\tbleu: 22.65156\tLR: 0.00030000\t*\n",
      "Steps: 40000\tLoss: 114566.81250\tPPL: 6.71580\tbleu: 22.84238\tLR: 0.00030000\t*\n",
      "Steps: 42000\tLoss: 113640.10938\tPPL: 6.61314\tbleu: 23.14761\tLR: 0.00030000\t*\n",
      "Steps: 44000\tLoss: 113218.78125\tPPL: 6.56699\tbleu: 23.44004\tLR: 0.00030000\t*\n",
      "Steps: 46000\tLoss: 111947.19531\tPPL: 6.42963\tbleu: 23.63609\tLR: 0.00030000\t*\n",
      "Steps: 48000\tLoss: 111625.42969\tPPL: 6.39533\tbleu: 23.89640\tLR: 0.00030000\t*\n",
      "Steps: 50000\tLoss: 110886.50781\tPPL: 6.31726\tbleu: 24.04825\tLR: 0.00030000\t*\n",
      "Steps: 52000\tLoss: 110297.29688\tPPL: 6.25568\tbleu: 24.18926\tLR: 0.00030000\t*\n",
      "Steps: 54000\tLoss: 109763.03125\tPPL: 6.20037\tbleu: 23.92904\tLR: 0.00030000\t*\n",
      "Steps: 56000\tLoss: 109255.66406\tPPL: 6.14830\tbleu: 24.38228\tLR: 0.00030000\t*\n",
      "Steps: 58000\tLoss: 109463.93750\tPPL: 6.16962\tbleu: 24.32561\tLR: 0.00030000\t\n",
      "Steps: 60000\tLoss: 108825.46875\tPPL: 6.10449\tbleu: 24.35577\tLR: 0.00030000\t*\n",
      "Steps: 62000\tLoss: 108211.88281\tPPL: 6.04254\tbleu: 24.70058\tLR: 0.00030000\t*\n",
      "Steps: 64000\tLoss: 107587.03125\tPPL: 5.98010\tbleu: 24.72907\tLR: 0.00030000\t*\n",
      "Steps: 66000\tLoss: 107153.31250\tPPL: 5.93714\tbleu: 25.00507\tLR: 0.00030000\t*\n",
      "Steps: 68000\tLoss: 106843.27344\tPPL: 5.90662\tbleu: 24.98868\tLR: 0.00030000\t*\n",
      "Steps: 70000\tLoss: 106771.42969\tPPL: 5.89957\tbleu: 25.11845\tLR: 0.00030000\t*\n",
      "Steps: 72000\tLoss: 106725.86719\tPPL: 5.89510\tbleu: 25.10645\tLR: 0.00030000\t*\n",
      "Steps: 74000\tLoss: 107125.77344\tPPL: 5.93442\tbleu: 25.04759\tLR: 0.00030000\t\n",
      "Steps: 76000\tLoss: 105909.90625\tPPL: 5.81568\tbleu: 25.47043\tLR: 0.00030000\t*\n",
      "Steps: 78000\tLoss: 105521.01562\tPPL: 5.77821\tbleu: 25.39027\tLR: 0.00030000\t*\n",
      "Steps: 80000\tLoss: 105212.82031\tPPL: 5.74868\tbleu: 25.47405\tLR: 0.00030000\t*\n",
      "Steps: 82000\tLoss: 104627.13281\tPPL: 5.69298\tbleu: 25.81200\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lgen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVchRnb3pv5x"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 82000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"joeynmt/models/lgen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"{path}/joeynmt/models/{name}_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lgen_reverse_transformer\"', f'model_dir: \"models/lgen_reverse_transformer_continued\"')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "lIxS8WNsnRQy",
    "outputId": "c0a979a2-aee9-4d96-c232-a873094a488f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lgen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lg\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer/82000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lgen_reverse_transformer_continued\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_lgen_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LG-p3cDfsMTq",
    "outputId": "942b1ba5-af62-4969-ddd3-5d3e2132e8b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 20:37:04,486 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 20:37:04,511 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 20:37:11,086 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 20:37:11,924 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 20:37:13,023 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 20:37:13,997 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 20:37:13,997 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 20:37:14,252 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 20:37:14.433449: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 20:37:15,850 - INFO - joeynmt.training - Total params: 12151040\n",
      "2021-08-03 20:37:17,946 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer/82000.ckpt\n",
      "2021-08-03 20:37:32,041 - INFO - joeynmt.helpers - cfg.name                           : lgen_reverse_transformer\n",
      "2021-08-03 20:37:32,042 - INFO - joeynmt.helpers - cfg.data.src                       : lg\n",
      "2021-08-03 20:37:32,042 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 20:37:32,042 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\n",
      "2021-08-03 20:37:32,042 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\n",
      "2021-08-03 20:37:32,042 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\n",
      "2021-08-03 20:37:32,043 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 20:37:32,043 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 20:37:32,043 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 20:37:32,043 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-08-03 20:37:32,044 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-08-03 20:37:32,044 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 20:37:32,044 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 20:37:32,044 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer/82000.ckpt\n",
      "2021-08-03 20:37:32,045 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 20:37:32,045 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 20:37:32,045 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 20:37:32,045 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 20:37:32,046 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 20:37:32,046 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 20:37:32,046 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 20:37:32,046 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 20:37:32,046 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 20:37:32,047 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 20:37:32,047 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 20:37:32,047 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 20:37:32,047 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 20:37:32,048 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 20:37:32,048 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 20:37:32,048 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 20:37:32,048 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-03 20:37:32,049 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 20:37:32,049 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 20:37:32,049 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 20:37:32,049 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-03 20:37:32,050 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-03 20:37:32,050 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-03 20:37:32,050 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 20:37:32,050 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lgen_reverse_transformer_continued\n",
      "2021-08-03 20:37:32,050 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 20:37:32,051 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 20:37:32,051 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 20:37:32,051 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 20:37:32,051 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 20:37:32,052 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 20:37:32,052 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 20:37:32,052 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 20:37:32,052 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 20:37:32,053 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 20:37:32,053 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 20:37:32,053 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 20:37:32,053 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 20:37:32,054 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 20:37:32,054 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 20:37:32,054 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 20:37:32,054 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 20:37:32,055 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 20:37:32,055 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 20:37:32,055 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 20:37:32,055 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 20:37:32,056 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 20:37:32,056 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 20:37:32,056 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 20:37:32,056 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 20:37:32,057 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 20:37:32,057 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 20:37:32,057 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 20:37:32,057 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 20:37:32,058 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 20:37:32,058 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 20:37:32,058 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 222129,\n",
      "\tvalid 2270,\n",
      "\ttest 2270\n",
      "2021-08-03 20:37:32,058 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ itical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-03 20:37:32,059 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 20:37:32,059 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 20:37:32,059 - INFO - joeynmt.helpers - Number of Src words (types): 4261\n",
      "2021-08-03 20:37:32,059 - INFO - joeynmt.helpers - Number of Trg words (types): 4261\n",
      "2021-08-03 20:37:32,060 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4261),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4261))\n",
      "2021-08-03 20:37:32,072 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 20:37:32,073 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 20:37:52,890 - INFO - joeynmt.training - Epoch   1: total training loss 127.18\n",
      "2021-08-03 20:37:52,890 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 20:38:31,784 - INFO - joeynmt.training - Epoch   2, Step:    82200, Batch Loss:     1.882068, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-08-03 20:39:29,595 - INFO - joeynmt.training - Epoch   2, Step:    82400, Batch Loss:     2.003426, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-03 20:40:27,579 - INFO - joeynmt.training - Epoch   2, Step:    82600, Batch Loss:     1.833938, Tokens per Sec:     7350, Lr: 0.000300\n",
      "2021-08-03 20:41:25,553 - INFO - joeynmt.training - Epoch   2, Step:    82800, Batch Loss:     1.430353, Tokens per Sec:     7404, Lr: 0.000300\n",
      "2021-08-03 20:42:23,471 - INFO - joeynmt.training - Epoch   2, Step:    83000, Batch Loss:     1.534483, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-08-03 20:43:21,658 - INFO - joeynmt.training - Epoch   2, Step:    83200, Batch Loss:     2.040603, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-08-03 20:44:20,067 - INFO - joeynmt.training - Epoch   2, Step:    83400, Batch Loss:     1.980332, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-03 20:45:18,103 - INFO - joeynmt.training - Epoch   2, Step:    83600, Batch Loss:     1.636117, Tokens per Sec:     7426, Lr: 0.000300\n",
      "2021-08-03 20:46:16,288 - INFO - joeynmt.training - Epoch   2, Step:    83800, Batch Loss:     2.053876, Tokens per Sec:     7364, Lr: 0.000300\n",
      "2021-08-03 20:47:14,431 - INFO - joeynmt.training - Epoch   2, Step:    84000, Batch Loss:     2.048693, Tokens per Sec:     7332, Lr: 0.000300\n",
      "2021-08-03 20:48:54,132 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 20:48:54,133 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 20:48:54,133 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 20:48:54,831 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 20:48:54,832 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 20:48:55,615 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 20:48:55,617 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 20:48:55,618 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 20:48:55,618 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 20:48:55,618 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 20:48:55,619 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 20:48:55,619 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 20:48:55,620 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 20:48:55,620 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 20:48:55,621 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 20:48:55,621 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 20:48:55,621 - INFO - joeynmt.training - \tHypothesis: Why are false gods unununununcertain ?\n",
      "2021-08-03 20:48:55,622 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 20:48:55,622 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 20:48:55,622 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 20:48:55,623 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-03 20:48:55,623 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    84000: bleu:  25.74, loss: 104546.1719, ppl:   5.6853, duration: 101.1917s\n",
      "2021-08-03 20:49:54,220 - INFO - joeynmt.training - Epoch   2, Step:    84200, Batch Loss:     1.967893, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-08-03 20:50:51,813 - INFO - joeynmt.training - Epoch   2, Step:    84400, Batch Loss:     1.849602, Tokens per Sec:     7291, Lr: 0.000300\n",
      "2021-08-03 20:51:50,402 - INFO - joeynmt.training - Epoch   2, Step:    84600, Batch Loss:     1.794216, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-08-03 20:52:48,309 - INFO - joeynmt.training - Epoch   2, Step:    84800, Batch Loss:     1.582759, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-08-03 20:52:49,118 - INFO - joeynmt.training - Epoch   2: total training loss 5137.44\n",
      "2021-08-03 20:52:49,119 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 20:53:46,607 - INFO - joeynmt.training - Epoch   3, Step:    85000, Batch Loss:     1.836410, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-08-03 20:54:44,899 - INFO - joeynmt.training - Epoch   3, Step:    85200, Batch Loss:     1.871610, Tokens per Sec:     7548, Lr: 0.000300\n",
      "2021-08-03 20:55:42,959 - INFO - joeynmt.training - Epoch   3, Step:    85400, Batch Loss:     1.979342, Tokens per Sec:     7326, Lr: 0.000300\n",
      "2021-08-03 20:56:40,506 - INFO - joeynmt.training - Epoch   3, Step:    85600, Batch Loss:     1.880751, Tokens per Sec:     7275, Lr: 0.000300\n",
      "2021-08-03 20:57:38,522 - INFO - joeynmt.training - Epoch   3, Step:    85800, Batch Loss:     1.977204, Tokens per Sec:     7399, Lr: 0.000300\n",
      "2021-08-03 20:58:36,958 - INFO - joeynmt.training - Epoch   3, Step:    86000, Batch Loss:     2.003742, Tokens per Sec:     7402, Lr: 0.000300\n",
      "2021-08-03 21:00:12,217 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:00:12,217 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:00:12,217 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:00:14,145 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:00:14,151 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:00:14,151 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:00:14,152 - INFO - joeynmt.training - \tHypothesis: As we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:00:14,152 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:00:14,153 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:00:14,153 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:00:14,153 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-03 21:00:14,153 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:00:14,154 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:00:14,154 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:00:14,154 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a negative influence ?\n",
      "2021-08-03 21:00:14,155 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:00:14,155 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:00:14,156 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:00:14,156 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:00:14,156 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    86000: bleu:  25.67, loss: 104900.7188, ppl:   5.7189, duration: 97.1974s\n",
      "2021-08-03 21:01:12,147 - INFO - joeynmt.training - Epoch   3, Step:    86200, Batch Loss:     2.040614, Tokens per Sec:     7242, Lr: 0.000300\n",
      "2021-08-03 21:02:10,994 - INFO - joeynmt.training - Epoch   3, Step:    86400, Batch Loss:     1.657453, Tokens per Sec:     7448, Lr: 0.000300\n",
      "2021-08-03 21:03:09,597 - INFO - joeynmt.training - Epoch   3, Step:    86600, Batch Loss:     1.811370, Tokens per Sec:     7446, Lr: 0.000300\n",
      "2021-08-03 21:04:07,374 - INFO - joeynmt.training - Epoch   3, Step:    86800, Batch Loss:     1.965565, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-08-03 21:05:05,590 - INFO - joeynmt.training - Epoch   3, Step:    87000, Batch Loss:     2.005776, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-08-03 21:06:03,966 - INFO - joeynmt.training - Epoch   3, Step:    87200, Batch Loss:     1.859242, Tokens per Sec:     7338, Lr: 0.000300\n",
      "2021-08-03 21:07:02,060 - INFO - joeynmt.training - Epoch   3, Step:    87400, Batch Loss:     1.916857, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-08-03 21:07:43,304 - INFO - joeynmt.training - Epoch   3: total training loss 5127.51\n",
      "2021-08-03 21:07:43,305 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 21:08:00,414 - INFO - joeynmt.training - Epoch   4, Step:    87600, Batch Loss:     1.470506, Tokens per Sec:     7232, Lr: 0.000300\n",
      "2021-08-03 21:08:58,244 - INFO - joeynmt.training - Epoch   4, Step:    87800, Batch Loss:     1.963203, Tokens per Sec:     7340, Lr: 0.000300\n",
      "2021-08-03 21:09:56,619 - INFO - joeynmt.training - Epoch   4, Step:    88000, Batch Loss:     1.919940, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-08-03 21:11:32,488 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:11:32,489 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:11:32,489 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:11:33,224 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:11:33,225 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:11:34,119 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:11:34,120 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:11:34,120 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:11:34,120 - INFO - joeynmt.training - \tHypothesis: As we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:11:34,121 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:11:34,122 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:11:34,122 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:11:34,123 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to study the Scriptures if possible .\n",
      "2021-08-03 21:11:34,123 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:11:34,123 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:11:34,124 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:11:34,124 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-03 21:11:34,124 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:11:34,125 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:11:34,125 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:11:34,125 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-03 21:11:34,125 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    88000: bleu:  25.78, loss: 104344.7188, ppl:   5.6663, duration: 97.5061s\n",
      "2021-08-03 21:12:32,690 - INFO - joeynmt.training - Epoch   4, Step:    88200, Batch Loss:     2.014457, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-08-03 21:13:30,733 - INFO - joeynmt.training - Epoch   4, Step:    88400, Batch Loss:     1.685060, Tokens per Sec:     7319, Lr: 0.000300\n",
      "2021-08-03 21:14:29,134 - INFO - joeynmt.training - Epoch   4, Step:    88600, Batch Loss:     1.894943, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-08-03 21:15:27,211 - INFO - joeynmt.training - Epoch   4, Step:    88800, Batch Loss:     2.000748, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-08-03 21:16:25,288 - INFO - joeynmt.training - Epoch   4, Step:    89000, Batch Loss:     1.838194, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-08-03 21:17:23,743 - INFO - joeynmt.training - Epoch   4, Step:    89200, Batch Loss:     1.963397, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-08-03 21:18:21,619 - INFO - joeynmt.training - Epoch   4, Step:    89400, Batch Loss:     1.893996, Tokens per Sec:     7189, Lr: 0.000300\n",
      "2021-08-03 21:19:20,273 - INFO - joeynmt.training - Epoch   4, Step:    89600, Batch Loss:     1.744552, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-08-03 21:20:18,515 - INFO - joeynmt.training - Epoch   4, Step:    89800, Batch Loss:     1.862365, Tokens per Sec:     7403, Lr: 0.000300\n",
      "2021-08-03 21:21:16,013 - INFO - joeynmt.training - Epoch   4, Step:    90000, Batch Loss:     1.864294, Tokens per Sec:     7286, Lr: 0.000300\n",
      "2021-08-03 21:22:55,843 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:22:55,843 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:22:55,843 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:22:56,583 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:22:56,584 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:22:57,404 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:22:57,406 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:22:57,406 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:22:57,406 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:22:57,406 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:22:57,407 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:22:57,407 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:22:57,408 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-03 21:22:57,408 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:22:57,409 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:22:57,409 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:22:57,409 - INFO - joeynmt.training - \tHypothesis: Why are false gods ununusual ?\n",
      "2021-08-03 21:22:57,409 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:22:57,410 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:22:57,410 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:22:57,411 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:22:57,411 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    90000: bleu:  25.61, loss: 103869.9844, ppl:   5.6218, duration: 101.3975s\n",
      "2021-08-03 21:23:56,328 - INFO - joeynmt.training - Epoch   4, Step:    90200, Batch Loss:     1.901600, Tokens per Sec:     7431, Lr: 0.000300\n",
      "2021-08-03 21:24:21,469 - INFO - joeynmt.training - Epoch   4: total training loss 5121.97\n",
      "2021-08-03 21:24:21,469 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-03 21:24:54,784 - INFO - joeynmt.training - Epoch   5, Step:    90400, Batch Loss:     1.843011, Tokens per Sec:     7256, Lr: 0.000300\n",
      "2021-08-03 21:25:52,839 - INFO - joeynmt.training - Epoch   5, Step:    90600, Batch Loss:     2.033765, Tokens per Sec:     7349, Lr: 0.000300\n",
      "2021-08-03 21:26:50,392 - INFO - joeynmt.training - Epoch   5, Step:    90800, Batch Loss:     1.298231, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-08-03 21:27:48,686 - INFO - joeynmt.training - Epoch   5, Step:    91000, Batch Loss:     1.899546, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-03 21:28:46,865 - INFO - joeynmt.training - Epoch   5, Step:    91200, Batch Loss:     2.149942, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-08-03 21:29:44,825 - INFO - joeynmt.training - Epoch   5, Step:    91400, Batch Loss:     1.716919, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 21:30:43,126 - INFO - joeynmt.training - Epoch   5, Step:    91600, Batch Loss:     1.871467, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-03 21:31:41,748 - INFO - joeynmt.training - Epoch   5, Step:    91800, Batch Loss:     2.096408, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-08-03 21:32:39,879 - INFO - joeynmt.training - Epoch   5, Step:    92000, Batch Loss:     1.916529, Tokens per Sec:     7333, Lr: 0.000300\n",
      "2021-08-03 21:34:16,775 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:34:16,776 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:34:16,776 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:34:17,507 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:34:17,508 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:34:18,436 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:34:18,438 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:34:18,438 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:34:18,438 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:34:18,439 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:34:18,439 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:34:18,440 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:34:18,440 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible in our ministry and try to read the Scriptures if possible .\n",
      "2021-08-03 21:34:18,440 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:34:18,441 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:34:18,441 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:34:18,441 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have nothing ?\n",
      "2021-08-03 21:34:18,442 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:34:18,442 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:34:18,443 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:34:18,443 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:34:18,443 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    92000: bleu:  25.99, loss: 103786.9375, ppl:   5.6140, duration: 98.5637s\n",
      "2021-08-03 21:35:16,554 - INFO - joeynmt.training - Epoch   5, Step:    92200, Batch Loss:     0.861978, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-03 21:36:15,194 - INFO - joeynmt.training - Epoch   5, Step:    92400, Batch Loss:     1.916388, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 21:37:13,215 - INFO - joeynmt.training - Epoch   5, Step:    92600, Batch Loss:     1.899724, Tokens per Sec:     7342, Lr: 0.000300\n",
      "2021-08-03 21:38:11,378 - INFO - joeynmt.training - Epoch   5, Step:    92800, Batch Loss:     1.720037, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 21:39:09,579 - INFO - joeynmt.training - Epoch   5, Step:    93000, Batch Loss:     1.838408, Tokens per Sec:     7296, Lr: 0.000300\n",
      "2021-08-03 21:39:18,068 - INFO - joeynmt.training - Epoch   5: total training loss 5100.41\n",
      "2021-08-03 21:39:18,068 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-03 21:40:07,817 - INFO - joeynmt.training - Epoch   6, Step:    93200, Batch Loss:     2.017978, Tokens per Sec:     7360, Lr: 0.000300\n",
      "2021-08-03 21:41:06,066 - INFO - joeynmt.training - Epoch   6, Step:    93400, Batch Loss:     1.439444, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-03 21:42:04,387 - INFO - joeynmt.training - Epoch   6, Step:    93600, Batch Loss:     1.806603, Tokens per Sec:     7342, Lr: 0.000300\n",
      "2021-08-03 21:43:02,495 - INFO - joeynmt.training - Epoch   6, Step:    93800, Batch Loss:     1.981454, Tokens per Sec:     7316, Lr: 0.000300\n",
      "2021-08-03 21:44:00,606 - INFO - joeynmt.training - Epoch   6, Step:    94000, Batch Loss:     1.818661, Tokens per Sec:     7470, Lr: 0.000300\n",
      "2021-08-03 21:45:38,293 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:45:38,294 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:45:38,294 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:45:39,023 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:45:39,024 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:45:39,876 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:45:39,877 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:45:39,877 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:45:39,878 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he speaks to us .\n",
      "2021-08-03 21:45:39,878 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:45:39,879 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:45:39,879 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:45:39,879 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 21:45:39,879 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:45:39,880 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:45:39,880 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:45:39,880 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-03 21:45:39,881 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:45:39,882 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:45:39,882 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:45:39,882 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-03 21:45:39,883 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    94000: bleu:  25.86, loss: 103737.9297, ppl:   5.6095, duration: 99.2760s\n",
      "2021-08-03 21:46:38,192 - INFO - joeynmt.training - Epoch   6, Step:    94200, Batch Loss:     1.712371, Tokens per Sec:     7354, Lr: 0.000300\n",
      "2021-08-03 21:47:36,295 - INFO - joeynmt.training - Epoch   6, Step:    94400, Batch Loss:     1.896116, Tokens per Sec:     7353, Lr: 0.000300\n",
      "2021-08-03 21:48:34,224 - INFO - joeynmt.training - Epoch   6, Step:    94600, Batch Loss:     1.689305, Tokens per Sec:     7286, Lr: 0.000300\n",
      "2021-08-03 21:49:32,547 - INFO - joeynmt.training - Epoch   6, Step:    94800, Batch Loss:     1.909650, Tokens per Sec:     7448, Lr: 0.000300\n",
      "2021-08-03 21:50:30,961 - INFO - joeynmt.training - Epoch   6, Step:    95000, Batch Loss:     1.996264, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-08-03 21:51:29,084 - INFO - joeynmt.training - Epoch   6, Step:    95200, Batch Loss:     1.457787, Tokens per Sec:     7359, Lr: 0.000300\n",
      "2021-08-03 21:52:27,305 - INFO - joeynmt.training - Epoch   6, Step:    95400, Batch Loss:     1.531716, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-08-03 21:53:25,741 - INFO - joeynmt.training - Epoch   6, Step:    95600, Batch Loss:     1.746612, Tokens per Sec:     7330, Lr: 0.000300\n",
      "2021-08-03 21:54:15,433 - INFO - joeynmt.training - Epoch   6: total training loss 5082.14\n",
      "2021-08-03 21:54:15,433 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-03 21:54:24,694 - INFO - joeynmt.training - Epoch   7, Step:    95800, Batch Loss:     1.917283, Tokens per Sec:     7574, Lr: 0.000300\n",
      "2021-08-03 21:55:22,629 - INFO - joeynmt.training - Epoch   7, Step:    96000, Batch Loss:     1.867179, Tokens per Sec:     7332, Lr: 0.000300\n",
      "2021-08-03 21:56:59,296 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:56:59,296 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:56:59,297 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:57:00,007 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:57:00,008 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:57:00,841 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:57:00,842 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:57:00,842 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:57:00,842 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:57:00,843 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:57:00,843 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:57:00,843 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:57:00,844 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-03 21:57:00,844 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:57:00,845 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:57:00,845 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:57:00,845 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-03 21:57:00,845 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:57:00,846 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:57:00,846 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:57:00,847 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:57:00,847 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    96000: bleu:  26.13, loss: 103274.7188, ppl:   5.5664, duration: 98.2169s\n",
      "2021-08-03 21:57:59,603 - INFO - joeynmt.training - Epoch   7, Step:    96200, Batch Loss:     1.853973, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-08-03 21:58:57,830 - INFO - joeynmt.training - Epoch   7, Step:    96400, Batch Loss:     2.057150, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-08-03 21:59:55,361 - INFO - joeynmt.training - Epoch   7, Step:    96600, Batch Loss:     1.762405, Tokens per Sec:     7254, Lr: 0.000300\n",
      "2021-08-03 22:00:53,725 - INFO - joeynmt.training - Epoch   7, Step:    96800, Batch Loss:     1.996174, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-08-03 22:01:52,023 - INFO - joeynmt.training - Epoch   7, Step:    97000, Batch Loss:     2.045189, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-08-03 22:02:50,250 - INFO - joeynmt.training - Epoch   7, Step:    97200, Batch Loss:     1.875400, Tokens per Sec:     7447, Lr: 0.000300\n",
      "2021-08-03 22:03:48,296 - INFO - joeynmt.training - Epoch   7, Step:    97400, Batch Loss:     1.944964, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 22:04:46,462 - INFO - joeynmt.training - Epoch   7, Step:    97600, Batch Loss:     1.905852, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-08-03 22:05:43,906 - INFO - joeynmt.training - Epoch   7, Step:    97800, Batch Loss:     1.129681, Tokens per Sec:     7248, Lr: 0.000300\n",
      "2021-08-03 22:06:41,824 - INFO - joeynmt.training - Epoch   7, Step:    98000, Batch Loss:     1.823728, Tokens per Sec:     7356, Lr: 0.000300\n",
      "2021-08-03 22:08:18,342 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:08:18,343 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:08:18,343 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:08:19,054 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 22:08:19,054 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 22:08:19,874 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:08:19,875 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:08:19,876 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:08:19,876 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:08:19,876 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:08:19,877 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:08:19,877 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:08:19,877 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures as possible .\n",
      "2021-08-03 22:08:19,877 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:08:19,878 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:08:19,878 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:08:19,879 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-03 22:08:19,879 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:08:19,879 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:08:19,880 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:08:19,880 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:08:19,880 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    98000: bleu:  26.00, loss: 103066.2734, ppl:   5.5472, duration: 98.0558s\n",
      "2021-08-03 22:09:18,258 - INFO - joeynmt.training - Epoch   7, Step:    98200, Batch Loss:     1.964147, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-08-03 22:10:16,323 - INFO - joeynmt.training - Epoch   7, Step:    98400, Batch Loss:     1.573616, Tokens per Sec:     7335, Lr: 0.000300\n",
      "2021-08-03 22:10:48,746 - INFO - joeynmt.training - Epoch   7: total training loss 5064.67\n",
      "2021-08-03 22:10:48,746 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-03 22:11:15,044 - INFO - joeynmt.training - Epoch   8, Step:    98600, Batch Loss:     1.975872, Tokens per Sec:     7267, Lr: 0.000300\n",
      "2021-08-03 22:12:13,463 - INFO - joeynmt.training - Epoch   8, Step:    98800, Batch Loss:     1.831574, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-08-03 22:13:11,488 - INFO - joeynmt.training - Epoch   8, Step:    99000, Batch Loss:     1.943879, Tokens per Sec:     7328, Lr: 0.000300\n",
      "2021-08-03 22:14:09,599 - INFO - joeynmt.training - Epoch   8, Step:    99200, Batch Loss:     2.143200, Tokens per Sec:     7381, Lr: 0.000300\n",
      "2021-08-03 22:15:08,098 - INFO - joeynmt.training - Epoch   8, Step:    99400, Batch Loss:     1.675290, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-08-03 22:16:06,324 - INFO - joeynmt.training - Epoch   8, Step:    99600, Batch Loss:     1.926291, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-08-03 22:17:04,449 - INFO - joeynmt.training - Epoch   8, Step:    99800, Batch Loss:     1.849394, Tokens per Sec:     7459, Lr: 0.000300\n",
      "2021-08-03 22:18:02,592 - INFO - joeynmt.training - Epoch   8, Step:   100000, Batch Loss:     1.968302, Tokens per Sec:     7360, Lr: 0.000300\n",
      "2021-08-03 22:19:39,647 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:19:39,648 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:19:39,648 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:19:41,176 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:19:41,177 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:19:41,177 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:19:41,177 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:19:41,178 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:19:41,178 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:19:41,178 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:19:41,179 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the effort to read the Scriptures if possible .\n",
      "2021-08-03 22:19:41,179 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:19:41,180 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:19:41,180 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:19:41,180 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-03 22:19:41,180 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:19:41,181 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:19:41,181 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:19:41,181 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is a loving relationship with his people .\n",
      "2021-08-03 22:19:41,182 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   100000: bleu:  25.98, loss: 103366.4844, ppl:   5.5749, duration: 98.5889s\n",
      "2021-08-03 22:20:39,316 - INFO - joeynmt.training - Epoch   8, Step:   100200, Batch Loss:     1.865285, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-08-03 22:21:37,436 - INFO - joeynmt.training - Epoch   8, Step:   100400, Batch Loss:     1.730281, Tokens per Sec:     7346, Lr: 0.000300\n",
      "2021-08-03 22:22:35,474 - INFO - joeynmt.training - Epoch   8, Step:   100600, Batch Loss:     1.921106, Tokens per Sec:     7229, Lr: 0.000300\n",
      "2021-08-03 22:23:33,695 - INFO - joeynmt.training - Epoch   8, Step:   100800, Batch Loss:     1.567644, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-08-03 22:24:31,960 - INFO - joeynmt.training - Epoch   8, Step:   101000, Batch Loss:     2.003104, Tokens per Sec:     7450, Lr: 0.000300\n",
      "2021-08-03 22:25:30,068 - INFO - joeynmt.training - Epoch   8, Step:   101200, Batch Loss:     1.838221, Tokens per Sec:     7378, Lr: 0.000300\n",
      "2021-08-03 22:25:43,636 - INFO - joeynmt.training - Epoch   8: total training loss 5041.70\n",
      "2021-08-03 22:25:43,637 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-03 22:26:28,689 - INFO - joeynmt.training - Epoch   9, Step:   101400, Batch Loss:     1.964244, Tokens per Sec:     7394, Lr: 0.000300\n",
      "2021-08-03 22:27:26,262 - INFO - joeynmt.training - Epoch   9, Step:   101600, Batch Loss:     1.775508, Tokens per Sec:     7287, Lr: 0.000300\n",
      "2021-08-03 22:28:24,494 - INFO - joeynmt.training - Epoch   9, Step:   101800, Batch Loss:     1.828856, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-08-03 22:29:22,786 - INFO - joeynmt.training - Epoch   9, Step:   102000, Batch Loss:     2.008116, Tokens per Sec:     7453, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt train joeynmt/configs/transformer_reverse_lgen_reload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Eavvcr9DmPH3",
    "outputId": "67dfb2d0-7b28-4eed-9a5d-7fb14f001b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 84000\tLoss: 104546.17188\tPPL: 5.68533\tbleu: 25.74462\tLR: 0.00030000\t*\n",
      "Steps: 86000\tLoss: 104900.71875\tPPL: 5.71893\tbleu: 25.67198\tLR: 0.00030000\t\n",
      "Steps: 88000\tLoss: 104344.71875\tPPL: 5.66632\tbleu: 25.77683\tLR: 0.00030000\t*\n",
      "Steps: 90000\tLoss: 103869.98438\tPPL: 5.62178\tbleu: 25.60610\tLR: 0.00030000\t*\n",
      "Steps: 92000\tLoss: 103786.93750\tPPL: 5.61402\tbleu: 25.99209\tLR: 0.00030000\t*\n",
      "Steps: 94000\tLoss: 103737.92969\tPPL: 5.60945\tbleu: 25.85617\tLR: 0.00030000\t*\n",
      "Steps: 96000\tLoss: 103274.71875\tPPL: 5.56643\tbleu: 26.12597\tLR: 0.00030000\t*\n",
      "Steps: 98000\tLoss: 103066.27344\tPPL: 5.54717\tbleu: 26.00346\tLR: 0.00030000\t*\n",
      "Steps: 100000\tLoss: 103366.48438\tPPL: 5.57492\tbleu: 25.97731\tLR: 0.00030000\t\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lgen_reverse_transformer_continued/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gofufRYpKmUU"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 100000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"joeynmt/models/lgen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"{path}/joeynmt/models/{name}_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lgen_reverse_transformer\"', f'model_dir: \"models/lgen_reverse_transformer_continued2\"').replace(\n",
    "        f'epochs: 30', f'epochs: 22')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "FkT_V450SPTu",
    "outputId": "65dfeb88-1348-4c74-f6ee-7e30c7ad5649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lgen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lg\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer_continued/100000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 22                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lgen_reverse_transformer_continued2\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_lgen_reload2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNOuvwmxThOU",
    "outputId": "2ed9cdbb-aaf1-4e0e-f25b-510ed1548951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-04 00:01:28,130 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-04 00:01:28,206 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-04 00:01:33,001 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-04 00:01:33,640 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-04 00:01:34,362 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-04 00:01:34,988 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-04 00:01:34,989 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 00:01:35,201 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 00:01:35.459827: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 00:01:37,279 - INFO - joeynmt.training - Total params: 12151040\n",
      "2021-08-04 00:01:40,960 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer_continued/100000.ckpt\n",
      "2021-08-04 00:01:50,967 - INFO - joeynmt.helpers - cfg.name                           : lgen_reverse_transformer\n",
      "2021-08-04 00:01:50,967 - INFO - joeynmt.helpers - cfg.data.src                       : lg\n",
      "2021-08-04 00:01:50,967 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-04 00:01:50,968 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\n",
      "2021-08-04 00:01:50,968 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\n",
      "2021-08-04 00:01:50,968 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\n",
      "2021-08-04 00:01:50,968 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-04 00:01:50,968 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-04 00:01:50,968 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-04 00:01:50,968 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-08-04 00:01:50,969 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-08-04 00:01:50,969 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-04 00:01:50,969 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-04 00:01:50,969 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer_continued/100000.ckpt\n",
      "2021-08-04 00:01:50,969 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-04 00:01:50,970 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-04 00:01:50,970 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-04 00:01:50,970 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-04 00:01:50,970 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-04 00:01:50,970 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-04 00:01:50,971 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-04 00:01:50,971 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-04 00:01:50,971 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-04 00:01:50,971 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-04 00:01:50,971 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-04 00:01:50,971 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-04 00:01:50,972 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-04 00:01:50,972 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-04 00:01:50,972 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-04 00:01:50,972 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-04 00:01:50,972 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-04 00:01:50,973 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-04 00:01:50,973 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-04 00:01:50,973 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-04 00:01:50,973 - INFO - joeynmt.helpers - cfg.training.epochs                : 22\n",
      "2021-08-04 00:01:50,973 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-04 00:01:50,974 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-04 00:01:50,974 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-04 00:01:50,974 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lgen_reverse_transformer_continued2\n",
      "2021-08-04 00:01:50,974 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-04 00:01:50,974 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-04 00:01:50,974 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-04 00:01:50,975 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-04 00:01:50,975 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-04 00:01:50,975 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-04 00:01:50,975 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-04 00:01:50,975 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-04 00:01:50,975 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-04 00:01:50,975 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-04 00:01:50,976 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-04 00:01:50,976 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-04 00:01:50,976 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-04 00:01:50,976 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-04 00:01:50,976 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-04 00:01:50,976 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-04 00:01:50,977 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 00:01:50,977 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-04 00:01:50,977 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-04 00:01:50,977 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-04 00:01:50,977 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-04 00:01:50,977 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-04 00:01:50,977 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-04 00:01:50,978 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-04 00:01:50,978 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-04 00:01:50,978 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 00:01:50,978 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-04 00:01:50,978 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-04 00:01:50,978 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-04 00:01:50,978 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-04 00:01:50,979 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-04 00:01:50,979 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 222129,\n",
      "\tvalid 2270,\n",
      "\ttest 2270\n",
      "2021-08-04 00:01:50,979 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ itical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-04 00:01:50,979 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-04 00:01:50,979 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-04 00:01:50,980 - INFO - joeynmt.helpers - Number of Src words (types): 4261\n",
      "2021-08-04 00:01:50,980 - INFO - joeynmt.helpers - Number of Trg words (types): 4261\n",
      "2021-08-04 00:01:50,980 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4261),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4261))\n",
      "2021-08-04 00:01:50,991 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-04 00:01:50,991 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-04 00:02:16,534 - INFO - joeynmt.training - Epoch   1, Step:   100200, Batch Loss:     1.843266, Tokens per Sec:    16692, Lr: 0.000300\n",
      "2021-08-04 00:02:41,774 - INFO - joeynmt.training - Epoch   1, Step:   100400, Batch Loss:     1.727580, Tokens per Sec:    16916, Lr: 0.000300\n",
      "2021-08-04 00:03:07,619 - INFO - joeynmt.training - Epoch   1, Step:   100600, Batch Loss:     1.949139, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-08-04 00:03:33,827 - INFO - joeynmt.training - Epoch   1, Step:   100800, Batch Loss:     1.577351, Tokens per Sec:    16521, Lr: 0.000300\n",
      "2021-08-04 00:04:00,830 - INFO - joeynmt.training - Epoch   1, Step:   101000, Batch Loss:     2.006086, Tokens per Sec:    16076, Lr: 0.000300\n",
      "2021-08-04 00:04:27,775 - INFO - joeynmt.training - Epoch   1, Step:   101200, Batch Loss:     1.829333, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-08-04 00:04:33,973 - INFO - joeynmt.training - Epoch   1: total training loss 2301.10\n",
      "2021-08-04 00:04:33,973 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-04 00:04:54,724 - INFO - joeynmt.training - Epoch   2, Step:   101400, Batch Loss:     1.929011, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-04 00:05:21,353 - INFO - joeynmt.training - Epoch   2, Step:   101600, Batch Loss:     1.819875, Tokens per Sec:    15754, Lr: 0.000300\n",
      "2021-08-04 00:05:48,428 - INFO - joeynmt.training - Epoch   2, Step:   101800, Batch Loss:     1.832539, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-08-04 00:06:15,135 - INFO - joeynmt.training - Epoch   2, Step:   102000, Batch Loss:     1.997939, Tokens per Sec:    16268, Lr: 0.000300\n",
      "2021-08-04 00:07:07,915 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:07:07,916 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:07:07,916 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:07:08,521 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:07:08,521 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:07:09,590 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:07:09,591 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:07:09,591 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:07:09,592 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he speaks to us .\n",
      "2021-08-04 00:07:09,592 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:07:09,592 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:07:09,593 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:07:09,593 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible in our ministry and try to read the Scriptures when possible .\n",
      "2021-08-04 00:07:09,593 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:07:09,594 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:07:09,594 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:07:09,594 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-04 00:07:09,594 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:07:09,595 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:07:09,595 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:07:09,595 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 00:07:09,595 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   102000: bleu:  26.18, loss: 102950.8438, ppl:   5.5365, duration: 54.4596s\n",
      "2021-08-04 00:07:36,263 - INFO - joeynmt.training - Epoch   2, Step:   102200, Batch Loss:     1.669145, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-04 00:08:03,235 - INFO - joeynmt.training - Epoch   2, Step:   102400, Batch Loss:     1.847220, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-04 00:08:29,984 - INFO - joeynmt.training - Epoch   2, Step:   102600, Batch Loss:     1.834877, Tokens per Sec:    16220, Lr: 0.000300\n",
      "2021-08-04 00:08:56,608 - INFO - joeynmt.training - Epoch   2, Step:   102800, Batch Loss:     1.857871, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-08-04 00:09:23,488 - INFO - joeynmt.training - Epoch   2, Step:   103000, Batch Loss:     1.816054, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-04 00:09:49,984 - INFO - joeynmt.training - Epoch   2, Step:   103200, Batch Loss:     2.043293, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-04 00:10:16,887 - INFO - joeynmt.training - Epoch   2, Step:   103400, Batch Loss:     1.391618, Tokens per Sec:    15687, Lr: 0.000300\n",
      "2021-08-04 00:10:44,068 - INFO - joeynmt.training - Epoch   2, Step:   103600, Batch Loss:     1.910044, Tokens per Sec:    16291, Lr: 0.000300\n",
      "2021-08-04 00:11:10,677 - INFO - joeynmt.training - Epoch   2, Step:   103800, Batch Loss:     1.662574, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-04 00:11:35,821 - INFO - joeynmt.training - Epoch   2: total training loss 5037.53\n",
      "2021-08-04 00:11:35,822 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-04 00:11:37,910 - INFO - joeynmt.training - Epoch   3, Step:   104000, Batch Loss:     1.757082, Tokens per Sec:    13498, Lr: 0.000300\n",
      "2021-08-04 00:12:29,520 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:12:29,520 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:12:29,520 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:12:30,109 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:12:30,110 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:12:30,840 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:12:30,841 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:12:30,842 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:12:30,842 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:12:30,842 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:12:30,842 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:12:30,843 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:12:30,843 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-04 00:12:30,843 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:12:30,843 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:12:30,843 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:12:30,843 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-04 00:12:30,844 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:12:30,844 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:12:30,844 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:12:30,844 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 00:12:30,844 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   104000: bleu:  26.34, loss: 102431.8750, ppl:   5.4890, duration: 52.9343s\n",
      "2021-08-04 00:12:57,860 - INFO - joeynmt.training - Epoch   3, Step:   104200, Batch Loss:     1.800359, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-08-04 00:13:24,512 - INFO - joeynmt.training - Epoch   3, Step:   104400, Batch Loss:     2.074781, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-04 00:13:51,421 - INFO - joeynmt.training - Epoch   3, Step:   104600, Batch Loss:     1.755817, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-08-04 00:14:18,233 - INFO - joeynmt.training - Epoch   3, Step:   104800, Batch Loss:     1.910917, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-04 00:14:45,171 - INFO - joeynmt.training - Epoch   3, Step:   105000, Batch Loss:     1.841904, Tokens per Sec:    16365, Lr: 0.000300\n",
      "2021-08-04 00:15:11,916 - INFO - joeynmt.training - Epoch   3, Step:   105200, Batch Loss:     1.912949, Tokens per Sec:    15800, Lr: 0.000300\n",
      "2021-08-04 00:15:38,707 - INFO - joeynmt.training - Epoch   3, Step:   105400, Batch Loss:     1.897690, Tokens per Sec:    16246, Lr: 0.000300\n",
      "2021-08-04 00:16:05,707 - INFO - joeynmt.training - Epoch   3, Step:   105600, Batch Loss:     1.852067, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-04 00:16:32,561 - INFO - joeynmt.training - Epoch   3, Step:   105800, Batch Loss:     1.927872, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-08-04 00:16:59,257 - INFO - joeynmt.training - Epoch   3, Step:   106000, Batch Loss:     1.964335, Tokens per Sec:    15865, Lr: 0.000300\n",
      "2021-08-04 00:17:51,796 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:17:51,797 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:17:51,797 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:17:52,433 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:17:52,434 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:17:53,186 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:17:53,187 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:17:53,187 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:17:53,187 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:17:53,188 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:17:53,189 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:17:53,189 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:17:53,189 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures with the help of others .\n",
      "2021-08-04 00:17:53,189 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:17:53,189 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:17:53,190 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:17:53,190 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-04 00:17:53,190 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:17:53,190 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:17:53,191 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:17:53,191 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 00:17:53,191 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   106000: bleu:  26.34, loss: 102424.2109, ppl:   5.4883, duration: 53.9336s\n",
      "2021-08-04 00:18:19,909 - INFO - joeynmt.training - Epoch   3, Step:   106200, Batch Loss:     2.015122, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-08-04 00:18:46,712 - INFO - joeynmt.training - Epoch   3, Step:   106400, Batch Loss:     1.792284, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-04 00:19:13,722 - INFO - joeynmt.training - Epoch   3, Step:   106600, Batch Loss:     1.946104, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-04 00:19:30,232 - INFO - joeynmt.training - Epoch   3: total training loss 5015.53\n",
      "2021-08-04 00:19:30,233 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-04 00:19:40,412 - INFO - joeynmt.training - Epoch   4, Step:   106800, Batch Loss:     1.927608, Tokens per Sec:    15478, Lr: 0.000300\n",
      "2021-08-04 00:20:07,270 - INFO - joeynmt.training - Epoch   4, Step:   107000, Batch Loss:     1.835495, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-04 00:20:33,921 - INFO - joeynmt.training - Epoch   4, Step:   107200, Batch Loss:     1.857499, Tokens per Sec:    16114, Lr: 0.000300\n",
      "2021-08-04 00:21:00,688 - INFO - joeynmt.training - Epoch   4, Step:   107400, Batch Loss:     2.079315, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-08-04 00:21:27,541 - INFO - joeynmt.training - Epoch   4, Step:   107600, Batch Loss:     2.016387, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-08-04 00:21:54,421 - INFO - joeynmt.training - Epoch   4, Step:   107800, Batch Loss:     1.831279, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-08-04 00:22:21,051 - INFO - joeynmt.training - Epoch   4, Step:   108000, Batch Loss:     1.880800, Tokens per Sec:    16154, Lr: 0.000300\n",
      "2021-08-04 00:23:13,380 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:23:13,380 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:23:13,381 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:23:13,962 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:23:13,962 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:23:15,068 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:23:15,069 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:23:15,070 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:23:15,070 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:23:15,070 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:23:15,070 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:23:15,071 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:23:15,071 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to study the Scriptures if possible .\n",
      "2021-08-04 00:23:15,071 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:23:15,072 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:23:15,072 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:23:15,072 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-04 00:23:15,072 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:23:15,073 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:23:15,073 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:23:15,073 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is a loving relationship with his people .\n",
      "2021-08-04 00:23:15,073 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   108000: bleu:  26.42, loss: 102250.4531, ppl:   5.4725, duration: 54.0221s\n",
      "2021-08-04 00:23:41,932 - INFO - joeynmt.training - Epoch   4, Step:   108200, Batch Loss:     1.403658, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-08-04 00:24:08,886 - INFO - joeynmt.training - Epoch   4, Step:   108400, Batch Loss:     1.822072, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-04 00:24:35,401 - INFO - joeynmt.training - Epoch   4, Step:   108600, Batch Loss:     1.421437, Tokens per Sec:    16345, Lr: 0.000300\n",
      "2021-08-04 00:25:02,305 - INFO - joeynmt.training - Epoch   4, Step:   108800, Batch Loss:     1.885285, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-08-04 00:25:29,133 - INFO - joeynmt.training - Epoch   4, Step:   109000, Batch Loss:     1.984758, Tokens per Sec:    16366, Lr: 0.000300\n",
      "2021-08-04 00:25:55,924 - INFO - joeynmt.training - Epoch   4, Step:   109200, Batch Loss:     1.845949, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-08-04 00:26:22,841 - INFO - joeynmt.training - Epoch   4, Step:   109400, Batch Loss:     1.877516, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-04 00:26:30,467 - INFO - joeynmt.training - Epoch   4: total training loss 4993.35\n",
      "2021-08-04 00:26:30,467 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-04 00:26:49,658 - INFO - joeynmt.training - Epoch   5, Step:   109600, Batch Loss:     1.675827, Tokens per Sec:    15864, Lr: 0.000300\n",
      "2021-08-04 00:27:16,534 - INFO - joeynmt.training - Epoch   5, Step:   109800, Batch Loss:     1.485544, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-04 00:27:43,318 - INFO - joeynmt.training - Epoch   5, Step:   110000, Batch Loss:     1.661188, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-04 00:28:34,265 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:28:34,265 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:28:34,265 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:28:35,585 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:28:35,586 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:28:35,586 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:28:35,586 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:28:35,586 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:28:35,587 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:28:35,587 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:28:35,587 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 00:28:35,587 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:28:35,589 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:28:35,589 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:28:35,590 - INFO - joeynmt.training - \tHypothesis: Why do false gods not exist ?\n",
      "2021-08-04 00:28:35,590 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:28:35,590 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:28:35,590 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:28:35,590 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-04 00:28:35,591 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   110000: bleu:  26.23, loss: 102364.2500, ppl:   5.4828, duration: 52.2721s\n",
      "2021-08-04 00:29:02,740 - INFO - joeynmt.training - Epoch   5, Step:   110200, Batch Loss:     1.821986, Tokens per Sec:    16084, Lr: 0.000300\n",
      "2021-08-04 00:29:29,356 - INFO - joeynmt.training - Epoch   5, Step:   110400, Batch Loss:     1.806759, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-04 00:29:56,162 - INFO - joeynmt.training - Epoch   5, Step:   110600, Batch Loss:     2.022441, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-04 00:30:22,772 - INFO - joeynmt.training - Epoch   5, Step:   110800, Batch Loss:     1.872054, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-08-04 00:30:49,520 - INFO - joeynmt.training - Epoch   5, Step:   111000, Batch Loss:     1.894648, Tokens per Sec:    16169, Lr: 0.000300\n",
      "2021-08-04 00:31:16,416 - INFO - joeynmt.training - Epoch   5, Step:   111200, Batch Loss:     1.452162, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-08-04 00:31:42,888 - INFO - joeynmt.training - Epoch   5, Step:   111400, Batch Loss:     1.840162, Tokens per Sec:    16331, Lr: 0.000300\n",
      "2021-08-04 00:32:09,710 - INFO - joeynmt.training - Epoch   5, Step:   111600, Batch Loss:     1.787281, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-08-04 00:32:36,396 - INFO - joeynmt.training - Epoch   5, Step:   111800, Batch Loss:     1.845550, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-08-04 00:33:03,222 - INFO - joeynmt.training - Epoch   5, Step:   112000, Batch Loss:     1.873994, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-08-04 00:33:58,615 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:33:58,615 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:33:58,615 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:33:59,217 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:33:59,217 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:34:00,360 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:34:00,361 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:34:00,361 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:34:00,361 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:34:00,361 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:34:00,362 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:34:00,362 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:34:00,362 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-04 00:34:00,362 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:34:00,363 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:34:00,363 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:34:00,363 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-04 00:34:00,363 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:34:00,363 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:34:00,364 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:34:00,364 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 00:34:00,364 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   112000: bleu:  26.55, loss: 101759.6016, ppl:   5.4280, duration: 57.1417s\n",
      "2021-08-04 00:34:26,500 - INFO - joeynmt.training - Epoch   5: total training loss 4984.08\n",
      "2021-08-04 00:34:26,500 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-04 00:34:27,884 - INFO - joeynmt.training - Epoch   6, Step:   112200, Batch Loss:     1.782663, Tokens per Sec:    11946, Lr: 0.000300\n",
      "2021-08-04 00:34:55,175 - INFO - joeynmt.training - Epoch   6, Step:   112400, Batch Loss:     1.949844, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-08-04 00:35:22,281 - INFO - joeynmt.training - Epoch   6, Step:   112600, Batch Loss:     1.796893, Tokens per Sec:    16446, Lr: 0.000300\n",
      "2021-08-04 00:35:49,397 - INFO - joeynmt.training - Epoch   6, Step:   112800, Batch Loss:     2.064215, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-08-04 00:36:16,404 - INFO - joeynmt.training - Epoch   6, Step:   113000, Batch Loss:     1.932110, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-08-04 00:36:43,048 - INFO - joeynmt.training - Epoch   6, Step:   113200, Batch Loss:     1.741268, Tokens per Sec:    16141, Lr: 0.000300\n",
      "2021-08-04 00:37:09,998 - INFO - joeynmt.training - Epoch   6, Step:   113400, Batch Loss:     1.900889, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-04 00:37:36,608 - INFO - joeynmt.training - Epoch   6, Step:   113600, Batch Loss:     1.643876, Tokens per Sec:    15928, Lr: 0.000300\n",
      "2021-08-04 00:38:03,364 - INFO - joeynmt.training - Epoch   6, Step:   113800, Batch Loss:     1.812163, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-08-04 00:38:30,154 - INFO - joeynmt.training - Epoch   6, Step:   114000, Batch Loss:     1.776879, Tokens per Sec:    16308, Lr: 0.000300\n",
      "2021-08-04 00:39:22,299 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:39:22,300 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:39:22,300 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:39:22,881 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:39:22,881 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:39:23,960 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:39:23,961 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:39:23,961 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:39:23,961 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:39:23,961 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:39:23,962 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:39:23,962 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:39:23,962 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-04 00:39:23,962 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:39:23,962 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:39:23,963 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:39:23,963 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-04 00:39:23,963 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:39:23,963 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:39:23,963 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:39:23,964 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 00:39:23,964 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   114000: bleu:  26.52, loss: 101613.2188, ppl:   5.4148, duration: 53.8095s\n",
      "2021-08-04 00:39:51,057 - INFO - joeynmt.training - Epoch   6, Step:   114200, Batch Loss:     2.059537, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-08-04 00:40:17,635 - INFO - joeynmt.training - Epoch   6, Step:   114400, Batch Loss:     1.773827, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-04 00:40:44,295 - INFO - joeynmt.training - Epoch   6, Step:   114600, Batch Loss:     1.859448, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-04 00:41:11,125 - INFO - joeynmt.training - Epoch   6, Step:   114800, Batch Loss:     2.013108, Tokens per Sec:    15565, Lr: 0.000300\n",
      "2021-08-04 00:41:28,440 - INFO - joeynmt.training - Epoch   6: total training loss 4976.44\n",
      "2021-08-04 00:41:28,440 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-04 00:41:38,233 - INFO - joeynmt.training - Epoch   7, Step:   115000, Batch Loss:     2.058943, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-08-04 00:42:05,225 - INFO - joeynmt.training - Epoch   7, Step:   115200, Batch Loss:     1.910697, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-04 00:42:31,774 - INFO - joeynmt.training - Epoch   7, Step:   115400, Batch Loss:     1.812033, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-04 00:42:58,575 - INFO - joeynmt.training - Epoch   7, Step:   115600, Batch Loss:     1.633876, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-04 00:43:25,879 - INFO - joeynmt.training - Epoch   7, Step:   115800, Batch Loss:     1.917510, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-08-04 00:43:52,861 - INFO - joeynmt.training - Epoch   7, Step:   116000, Batch Loss:     1.846900, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-08-04 00:44:45,876 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:44:45,876 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:44:45,876 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:44:46,551 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:44:46,551 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:44:47,283 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:44:47,285 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:44:47,285 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:44:47,285 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:44:47,285 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:44:47,286 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:44:47,286 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:44:47,286 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-04 00:44:47,286 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:44:47,287 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:44:47,287 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:44:47,287 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-04 00:44:47,287 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:44:47,288 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:44:47,288 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:44:47,288 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 00:44:47,288 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   116000: bleu:  26.25, loss: 101536.2734, ppl:   5.4079, duration: 54.4268s\n",
      "2021-08-04 00:45:14,148 - INFO - joeynmt.training - Epoch   7, Step:   116200, Batch Loss:     1.778931, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-04 00:45:41,290 - INFO - joeynmt.training - Epoch   7, Step:   116400, Batch Loss:     1.899046, Tokens per Sec:    15912, Lr: 0.000300\n",
      "2021-08-04 00:46:08,162 - INFO - joeynmt.training - Epoch   7, Step:   116600, Batch Loss:     1.814634, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-04 00:46:34,866 - INFO - joeynmt.training - Epoch   7, Step:   116800, Batch Loss:     1.860511, Tokens per Sec:    16070, Lr: 0.000300\n",
      "2021-08-04 00:47:02,045 - INFO - joeynmt.training - Epoch   7, Step:   117000, Batch Loss:     1.866791, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-08-04 00:47:29,017 - INFO - joeynmt.training - Epoch   7, Step:   117200, Batch Loss:     1.962824, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-08-04 00:47:55,850 - INFO - joeynmt.training - Epoch   7, Step:   117400, Batch Loss:     1.957993, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-04 00:48:22,822 - INFO - joeynmt.training - Epoch   7, Step:   117600, Batch Loss:     1.858897, Tokens per Sec:    16072, Lr: 0.000300\n",
      "2021-08-04 00:48:30,976 - INFO - joeynmt.training - Epoch   7: total training loss 4951.75\n",
      "2021-08-04 00:48:30,976 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-04 00:48:49,973 - INFO - joeynmt.training - Epoch   8, Step:   117800, Batch Loss:     1.812473, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-04 00:49:16,754 - INFO - joeynmt.training - Epoch   8, Step:   118000, Batch Loss:     1.944929, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-04 00:50:10,131 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:50:10,131 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:50:10,131 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:50:10,735 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:50:10,735 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:50:11,446 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:50:11,446 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:50:11,447 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:50:11,447 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:50:11,447 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:50:11,447 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:50:11,448 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:50:11,448 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 00:50:11,448 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:50:11,448 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:50:11,448 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:50:11,449 - INFO - joeynmt.training - \tHypothesis: Why do false gods have nothing ?\n",
      "2021-08-04 00:50:11,449 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:50:11,449 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:50:11,450 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:50:11,450 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-04 00:50:11,450 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   118000: bleu:  26.53, loss: 101056.3750, ppl:   5.3649, duration: 54.6953s\n",
      "2021-08-04 00:50:38,426 - INFO - joeynmt.training - Epoch   8, Step:   118200, Batch Loss:     1.963706, Tokens per Sec:    15697, Lr: 0.000300\n",
      "2021-08-04 00:51:05,306 - INFO - joeynmt.training - Epoch   8, Step:   118400, Batch Loss:     1.690422, Tokens per Sec:    16209, Lr: 0.000300\n",
      "2021-08-04 00:51:32,214 - INFO - joeynmt.training - Epoch   8, Step:   118600, Batch Loss:     1.961959, Tokens per Sec:    16445, Lr: 0.000300\n",
      "2021-08-04 00:51:58,783 - INFO - joeynmt.training - Epoch   8, Step:   118800, Batch Loss:     1.929933, Tokens per Sec:    16035, Lr: 0.000300\n",
      "2021-08-04 00:52:25,660 - INFO - joeynmt.training - Epoch   8, Step:   119000, Batch Loss:     1.941071, Tokens per Sec:    16231, Lr: 0.000300\n",
      "2021-08-04 00:52:52,318 - INFO - joeynmt.training - Epoch   8, Step:   119200, Batch Loss:     2.127064, Tokens per Sec:    16224, Lr: 0.000300\n",
      "2021-08-04 00:53:18,837 - INFO - joeynmt.training - Epoch   8, Step:   119400, Batch Loss:     1.931935, Tokens per Sec:    15805, Lr: 0.000300\n",
      "2021-08-04 00:53:45,456 - INFO - joeynmt.training - Epoch   8, Step:   119600, Batch Loss:     1.846680, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-04 00:54:12,144 - INFO - joeynmt.training - Epoch   8, Step:   119800, Batch Loss:     1.068012, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-08-04 00:54:38,877 - INFO - joeynmt.training - Epoch   8, Step:   120000, Batch Loss:     1.732496, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-04 00:55:30,550 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:55:30,550 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:55:30,550 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:55:31,182 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:55:31,182 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:55:31,890 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:55:31,891 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 00:55:31,892 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 00:55:31,892 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-04 00:55:31,892 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:55:31,892 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 00:55:31,892 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 00:55:31,893 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-04 00:55:31,893 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:55:31,893 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 00:55:31,893 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 00:55:31,894 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 00:55:31,894 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:55:31,894 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 00:55:31,894 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 00:55:31,894 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-04 00:55:31,895 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   120000: bleu:  26.66, loss: 100925.5938, ppl:   5.3532, duration: 53.0171s\n",
      "2021-08-04 00:55:58,804 - INFO - joeynmt.training - Epoch   8, Step:   120200, Batch Loss:     1.927025, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-08-04 00:56:24,871 - INFO - joeynmt.training - Epoch   8: total training loss 4945.09\n",
      "2021-08-04 00:56:24,871 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-04 00:56:25,632 - INFO - joeynmt.training - Epoch   9, Step:   120400, Batch Loss:     1.551300, Tokens per Sec:    10999, Lr: 0.000300\n",
      "2021-08-04 00:56:52,256 - INFO - joeynmt.training - Epoch   9, Step:   120600, Batch Loss:     1.826414, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-04 00:57:19,114 - INFO - joeynmt.training - Epoch   9, Step:   120800, Batch Loss:     1.864799, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-08-04 00:57:45,852 - INFO - joeynmt.training - Epoch   9, Step:   121000, Batch Loss:     1.847239, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-04 00:58:12,820 - INFO - joeynmt.training - Epoch   9, Step:   121200, Batch Loss:     1.949217, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-08-04 00:58:39,423 - INFO - joeynmt.training - Epoch   9, Step:   121400, Batch Loss:     1.709957, Tokens per Sec:    16255, Lr: 0.000300\n",
      "2021-08-04 00:59:06,376 - INFO - joeynmt.training - Epoch   9, Step:   121600, Batch Loss:     1.689360, Tokens per Sec:    16318, Lr: 0.000300\n",
      "2021-08-04 00:59:32,918 - INFO - joeynmt.training - Epoch   9, Step:   121800, Batch Loss:     1.958827, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-04 00:59:59,674 - INFO - joeynmt.training - Epoch   9, Step:   122000, Batch Loss:     1.961520, Tokens per Sec:    16015, Lr: 0.000300\n",
      "2021-08-04 01:00:50,845 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:00:50,846 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:00:50,846 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:00:52,090 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:00:52,090 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:00:52,091 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:00:52,091 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:00:52,091 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:00:52,091 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:00:52,091 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:00:52,092 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-04 01:00:52,092 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:00:52,092 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:00:52,092 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:00:52,092 - INFO - joeynmt.training - \tHypothesis: Why do false gods not exist ?\n",
      "2021-08-04 01:00:52,092 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:00:52,093 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:00:52,093 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:00:52,093 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:00:52,093 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   122000: bleu:  26.46, loss: 101490.9531, ppl:   5.4038, duration: 52.4192s\n",
      "2021-08-04 01:01:18,729 - INFO - joeynmt.training - Epoch   9, Step:   122200, Batch Loss:     1.919346, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-08-04 01:01:45,419 - INFO - joeynmt.training - Epoch   9, Step:   122400, Batch Loss:     1.957556, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-04 01:02:11,927 - INFO - joeynmt.training - Epoch   9, Step:   122600, Batch Loss:     1.788478, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-08-04 01:02:38,977 - INFO - joeynmt.training - Epoch   9, Step:   122800, Batch Loss:     1.762039, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-08-04 01:03:05,961 - INFO - joeynmt.training - Epoch   9, Step:   123000, Batch Loss:     2.105173, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-04 01:03:22,950 - INFO - joeynmt.training - Epoch   9: total training loss 4931.40\n",
      "2021-08-04 01:03:22,950 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-04 01:03:32,854 - INFO - joeynmt.training - Epoch  10, Step:   123200, Batch Loss:     1.645640, Tokens per Sec:    15294, Lr: 0.000300\n",
      "2021-08-04 01:03:59,852 - INFO - joeynmt.training - Epoch  10, Step:   123400, Batch Loss:     1.838048, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-04 01:04:26,702 - INFO - joeynmt.training - Epoch  10, Step:   123600, Batch Loss:     1.931240, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-04 01:04:53,229 - INFO - joeynmt.training - Epoch  10, Step:   123800, Batch Loss:     2.002185, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-08-04 01:05:20,049 - INFO - joeynmt.training - Epoch  10, Step:   124000, Batch Loss:     1.938845, Tokens per Sec:    15956, Lr: 0.000300\n",
      "2021-08-04 01:06:16,222 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:06:16,223 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:06:16,223 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:06:17,529 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:06:17,530 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:06:17,531 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:06:17,531 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:06:17,531 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:06:17,532 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:06:17,532 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:06:17,532 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-04 01:06:17,532 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:06:17,533 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:06:17,533 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:06:17,533 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 01:06:17,533 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:06:17,534 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:06:17,534 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:06:17,534 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:06:17,534 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   124000: bleu:  26.84, loss: 101171.4531, ppl:   5.3752, duration: 57.4851s\n",
      "2021-08-04 01:06:44,667 - INFO - joeynmt.training - Epoch  10, Step:   124200, Batch Loss:     1.746586, Tokens per Sec:    15951, Lr: 0.000300\n",
      "2021-08-04 01:07:11,443 - INFO - joeynmt.training - Epoch  10, Step:   124400, Batch Loss:     1.820094, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-04 01:07:38,437 - INFO - joeynmt.training - Epoch  10, Step:   124600, Batch Loss:     1.806146, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-04 01:08:05,225 - INFO - joeynmt.training - Epoch  10, Step:   124800, Batch Loss:     1.848995, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-08-04 01:08:32,043 - INFO - joeynmt.training - Epoch  10, Step:   125000, Batch Loss:     1.796032, Tokens per Sec:    16311, Lr: 0.000300\n",
      "2021-08-04 01:08:59,007 - INFO - joeynmt.training - Epoch  10, Step:   125200, Batch Loss:     1.801357, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-04 01:09:25,691 - INFO - joeynmt.training - Epoch  10, Step:   125400, Batch Loss:     1.970816, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-08-04 01:09:52,518 - INFO - joeynmt.training - Epoch  10, Step:   125600, Batch Loss:     1.708703, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-08-04 01:10:19,295 - INFO - joeynmt.training - Epoch  10, Step:   125800, Batch Loss:     1.893635, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-08-04 01:10:27,468 - INFO - joeynmt.training - Epoch  10: total training loss 4923.36\n",
      "2021-08-04 01:10:27,468 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-04 01:10:46,365 - INFO - joeynmt.training - Epoch  11, Step:   126000, Batch Loss:     1.949439, Tokens per Sec:    15719, Lr: 0.000300\n",
      "2021-08-04 01:11:39,489 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:11:39,489 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:11:39,490 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:11:40,131 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 01:11:40,132 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 01:11:41,969 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:11:41,970 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:11:41,970 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:11:41,970 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:11:41,970 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:11:41,971 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:11:41,971 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:11:41,971 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible to preach and to study the Scriptures if possible .\n",
      "2021-08-04 01:11:41,971 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:11:41,972 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:11:41,972 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:11:41,972 - INFO - joeynmt.training - \tHypothesis: Why do false gods have nothing ?\n",
      "2021-08-04 01:11:41,972 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:11:41,973 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:11:41,973 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:11:41,973 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:11:41,973 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   126000: bleu:  26.66, loss: 100666.1797, ppl:   5.3302, duration: 55.6075s\n",
      "2021-08-04 01:12:09,071 - INFO - joeynmt.training - Epoch  11, Step:   126200, Batch Loss:     1.998146, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-08-04 01:12:36,014 - INFO - joeynmt.training - Epoch  11, Step:   126400, Batch Loss:     1.762188, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-04 01:13:02,731 - INFO - joeynmt.training - Epoch  11, Step:   126600, Batch Loss:     1.997225, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-08-04 01:13:29,304 - INFO - joeynmt.training - Epoch  11, Step:   126800, Batch Loss:     1.702228, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-08-04 01:13:55,929 - INFO - joeynmt.training - Epoch  11, Step:   127000, Batch Loss:     1.659440, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-04 01:14:22,930 - INFO - joeynmt.training - Epoch  11, Step:   127200, Batch Loss:     2.013161, Tokens per Sec:    16220, Lr: 0.000300\n",
      "2021-08-04 01:14:49,513 - INFO - joeynmt.training - Epoch  11, Step:   127400, Batch Loss:     1.726520, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-04 01:15:16,360 - INFO - joeynmt.training - Epoch  11, Step:   127600, Batch Loss:     1.935172, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-04 01:15:43,282 - INFO - joeynmt.training - Epoch  11, Step:   127800, Batch Loss:     1.557787, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-08-04 01:16:10,320 - INFO - joeynmt.training - Epoch  11, Step:   128000, Batch Loss:     1.831700, Tokens per Sec:    16275, Lr: 0.000300\n",
      "2021-08-04 01:17:03,100 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:17:03,100 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:17:03,100 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:17:03,702 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 01:17:03,702 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 01:17:04,388 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:17:04,389 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:17:04,389 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:17:04,389 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:17:04,389 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:17:04,391 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:17:04,391 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:17:04,391 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 01:17:04,393 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:17:04,393 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:17:04,393 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:17:04,393 - INFO - joeynmt.training - \tHypothesis: Why do false gods not exist ?\n",
      "2021-08-04 01:17:04,394 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:17:04,394 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:17:04,394 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:17:04,394 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-04 01:17:04,394 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   128000: bleu:  26.74, loss: 100640.1719, ppl:   5.3279, duration: 54.0736s\n",
      "2021-08-04 01:17:31,077 - INFO - joeynmt.training - Epoch  11, Step:   128200, Batch Loss:     1.991030, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-08-04 01:17:58,080 - INFO - joeynmt.training - Epoch  11, Step:   128400, Batch Loss:     1.722846, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-04 01:18:24,262 - INFO - joeynmt.training - Epoch  11: total training loss 4914.02\n",
      "2021-08-04 01:18:24,262 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-04 01:18:25,001 - INFO - joeynmt.training - Epoch  12, Step:   128600, Batch Loss:     1.898453, Tokens per Sec:    10023, Lr: 0.000300\n",
      "2021-08-04 01:18:51,845 - INFO - joeynmt.training - Epoch  12, Step:   128800, Batch Loss:     1.789172, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-08-04 01:19:18,431 - INFO - joeynmt.training - Epoch  12, Step:   129000, Batch Loss:     1.748772, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-08-04 01:19:45,074 - INFO - joeynmt.training - Epoch  12, Step:   129200, Batch Loss:     2.005618, Tokens per Sec:    16288, Lr: 0.000300\n",
      "2021-08-04 01:20:11,920 - INFO - joeynmt.training - Epoch  12, Step:   129400, Batch Loss:     1.697318, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-04 01:20:38,365 - INFO - joeynmt.training - Epoch  12, Step:   129600, Batch Loss:     1.267096, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-08-04 01:21:04,966 - INFO - joeynmt.training - Epoch  12, Step:   129800, Batch Loss:     1.749202, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-08-04 01:21:31,855 - INFO - joeynmt.training - Epoch  12, Step:   130000, Batch Loss:     1.911762, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-04 01:22:24,497 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:22:24,497 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:22:24,497 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:22:25,088 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 01:22:25,089 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 01:22:25,764 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:22:25,765 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:22:25,765 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:22:25,765 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:22:25,765 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:22:25,766 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:22:25,766 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:22:25,766 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to study the Scriptures if possible .\n",
      "2021-08-04 01:22:25,766 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:22:25,766 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:22:25,767 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:22:25,767 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 01:22:25,767 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:22:25,767 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:22:25,767 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:22:25,768 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:22:25,768 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   130000: bleu:  26.55, loss: 100602.2109, ppl:   5.3245, duration: 53.9122s\n",
      "2021-08-04 01:22:52,885 - INFO - joeynmt.training - Epoch  12, Step:   130200, Batch Loss:     1.922077, Tokens per Sec:    15780, Lr: 0.000300\n",
      "2021-08-04 01:23:19,850 - INFO - joeynmt.training - Epoch  12, Step:   130400, Batch Loss:     2.082067, Tokens per Sec:    16305, Lr: 0.000300\n",
      "2021-08-04 01:23:46,741 - INFO - joeynmt.training - Epoch  12, Step:   130600, Batch Loss:     1.650737, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-08-04 01:24:13,486 - INFO - joeynmt.training - Epoch  12, Step:   130800, Batch Loss:     1.605283, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-08-04 01:24:40,257 - INFO - joeynmt.training - Epoch  12, Step:   131000, Batch Loss:     1.899370, Tokens per Sec:    16084, Lr: 0.000300\n",
      "2021-08-04 01:25:07,269 - INFO - joeynmt.training - Epoch  12, Step:   131200, Batch Loss:     1.475408, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-04 01:25:25,288 - INFO - joeynmt.training - Epoch  12: total training loss 4917.22\n",
      "2021-08-04 01:25:25,289 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-04 01:25:33,933 - INFO - joeynmt.training - Epoch  13, Step:   131400, Batch Loss:     1.416104, Tokens per Sec:    15449, Lr: 0.000300\n",
      "2021-08-04 01:26:01,011 - INFO - joeynmt.training - Epoch  13, Step:   131600, Batch Loss:     1.839162, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-04 01:26:27,680 - INFO - joeynmt.training - Epoch  13, Step:   131800, Batch Loss:     1.770100, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-04 01:26:54,454 - INFO - joeynmt.training - Epoch  13, Step:   132000, Batch Loss:     1.684243, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-08-04 01:27:48,104 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:27:48,104 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:27:48,105 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:27:49,345 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:27:49,347 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:27:49,347 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:27:49,347 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:27:49,347 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:27:49,348 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:27:49,348 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:27:49,348 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-04 01:27:49,348 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:27:49,349 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:27:49,349 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:27:49,349 - INFO - joeynmt.training - \tHypothesis: Why do false gods not exist ?\n",
      "2021-08-04 01:27:49,349 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:27:49,350 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:27:49,350 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:27:49,350 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:27:49,350 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   132000: bleu:  26.86, loss: 100619.2500, ppl:   5.3261, duration: 54.8955s\n",
      "2021-08-04 01:28:16,669 - INFO - joeynmt.training - Epoch  13, Step:   132200, Batch Loss:     1.207365, Tokens per Sec:    15721, Lr: 0.000300\n",
      "2021-08-04 01:28:43,479 - INFO - joeynmt.training - Epoch  13, Step:   132400, Batch Loss:     1.966186, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-04 01:29:10,397 - INFO - joeynmt.training - Epoch  13, Step:   132600, Batch Loss:     1.874641, Tokens per Sec:    16118, Lr: 0.000300\n",
      "2021-08-04 01:29:36,827 - INFO - joeynmt.training - Epoch  13, Step:   132800, Batch Loss:     1.786980, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-04 01:30:03,853 - INFO - joeynmt.training - Epoch  13, Step:   133000, Batch Loss:     1.921725, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-04 01:30:30,398 - INFO - joeynmt.training - Epoch  13, Step:   133200, Batch Loss:     1.668836, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-08-04 01:30:57,255 - INFO - joeynmt.training - Epoch  13, Step:   133400, Batch Loss:     1.859165, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-04 01:31:23,998 - INFO - joeynmt.training - Epoch  13, Step:   133600, Batch Loss:     1.767785, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-04 01:31:50,769 - INFO - joeynmt.training - Epoch  13, Step:   133800, Batch Loss:     1.835553, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-04 01:32:17,588 - INFO - joeynmt.training - Epoch  13, Step:   134000, Batch Loss:     1.768070, Tokens per Sec:    16019, Lr: 0.000300\n",
      "2021-08-04 01:33:09,635 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:33:09,636 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:33:09,636 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:33:10,238 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 01:33:10,239 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 01:33:10,978 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:33:10,979 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:33:10,979 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:33:10,979 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:33:10,979 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:33:10,980 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:33:10,980 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:33:10,980 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 01:33:10,980 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:33:10,981 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:33:10,981 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:33:10,981 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 01:33:10,981 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:33:10,982 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:33:10,982 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:33:10,982 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:33:10,982 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   134000: bleu:  26.84, loss: 100105.9609, ppl:   5.2808, duration: 53.3942s\n",
      "2021-08-04 01:33:21,049 - INFO - joeynmt.training - Epoch  13: total training loss 4891.00\n",
      "2021-08-04 01:33:21,049 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-04 01:33:38,302 - INFO - joeynmt.training - Epoch  14, Step:   134200, Batch Loss:     1.650737, Tokens per Sec:    15587, Lr: 0.000300\n",
      "2021-08-04 01:34:05,326 - INFO - joeynmt.training - Epoch  14, Step:   134400, Batch Loss:     1.999831, Tokens per Sec:    16166, Lr: 0.000300\n",
      "2021-08-04 01:34:32,036 - INFO - joeynmt.training - Epoch  14, Step:   134600, Batch Loss:     1.733625, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-08-04 01:34:59,138 - INFO - joeynmt.training - Epoch  14, Step:   134800, Batch Loss:     1.776876, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-04 01:35:25,649 - INFO - joeynmt.training - Epoch  14, Step:   135000, Batch Loss:     1.677283, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-08-04 01:35:52,425 - INFO - joeynmt.training - Epoch  14, Step:   135200, Batch Loss:     1.897814, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-08-04 01:36:19,185 - INFO - joeynmt.training - Epoch  14, Step:   135400, Batch Loss:     1.956702, Tokens per Sec:    16072, Lr: 0.000300\n",
      "2021-08-04 01:36:45,758 - INFO - joeynmt.training - Epoch  14, Step:   135600, Batch Loss:     1.592725, Tokens per Sec:    16206, Lr: 0.000300\n",
      "2021-08-04 01:37:12,590 - INFO - joeynmt.training - Epoch  14, Step:   135800, Batch Loss:     1.646693, Tokens per Sec:    15846, Lr: 0.000300\n",
      "2021-08-04 01:37:39,172 - INFO - joeynmt.training - Epoch  14, Step:   136000, Batch Loss:     1.725790, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-08-04 01:38:32,667 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:38:32,668 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:38:32,668 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:38:33,313 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 01:38:33,313 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 01:38:34,035 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:38:34,035 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:38:34,035 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:38:34,036 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:38:34,036 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:38:34,036 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:38:34,036 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:38:34,036 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 01:38:34,038 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:38:34,039 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:38:34,039 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:38:34,039 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-04 01:38:34,040 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:38:34,040 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:38:34,040 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:38:34,041 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:38:34,041 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   136000: bleu:  27.11, loss: 99928.8125, ppl:   5.2653, duration: 54.8688s\n",
      "2021-08-04 01:39:01,127 - INFO - joeynmt.training - Epoch  14, Step:   136200, Batch Loss:     1.960826, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-04 01:39:27,917 - INFO - joeynmt.training - Epoch  14, Step:   136400, Batch Loss:     1.744707, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-04 01:39:54,728 - INFO - joeynmt.training - Epoch  14, Step:   136600, Batch Loss:     1.831281, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-08-04 01:40:21,322 - INFO - joeynmt.training - Epoch  14, Step:   136800, Batch Loss:     1.947079, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-08-04 01:40:22,690 - INFO - joeynmt.training - Epoch  14: total training loss 4887.47\n",
      "2021-08-04 01:40:22,690 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-04 01:40:48,482 - INFO - joeynmt.training - Epoch  15, Step:   137000, Batch Loss:     1.915649, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-08-04 01:41:15,170 - INFO - joeynmt.training - Epoch  15, Step:   137200, Batch Loss:     1.868905, Tokens per Sec:    16070, Lr: 0.000300\n",
      "2021-08-04 01:41:41,957 - INFO - joeynmt.training - Epoch  15, Step:   137400, Batch Loss:     1.798792, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-04 01:42:08,765 - INFO - joeynmt.training - Epoch  15, Step:   137600, Batch Loss:     1.670560, Tokens per Sec:    15933, Lr: 0.000300\n",
      "2021-08-04 01:42:35,314 - INFO - joeynmt.training - Epoch  15, Step:   137800, Batch Loss:     1.701654, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-08-04 01:43:02,035 - INFO - joeynmt.training - Epoch  15, Step:   138000, Batch Loss:     1.776350, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-04 01:43:54,807 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:43:54,807 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:43:54,807 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:43:56,054 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:43:56,055 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:43:56,055 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:43:56,055 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:43:56,055 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:43:56,055 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:43:56,055 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:43:56,056 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 01:43:56,056 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:43:56,056 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:43:56,056 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:43:56,056 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-04 01:43:56,057 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:43:56,057 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:43:56,057 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:43:56,058 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:43:56,059 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   138000: bleu:  26.80, loss: 100550.3359, ppl:   5.3200, duration: 54.0238s\n",
      "2021-08-04 01:44:23,181 - INFO - joeynmt.training - Epoch  15, Step:   138200, Batch Loss:     1.762915, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-04 01:44:50,244 - INFO - joeynmt.training - Epoch  15, Step:   138400, Batch Loss:     1.665645, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-04 01:45:16,876 - INFO - joeynmt.training - Epoch  15, Step:   138600, Batch Loss:     1.340395, Tokens per Sec:    16017, Lr: 0.000300\n",
      "2021-08-04 01:45:43,501 - INFO - joeynmt.training - Epoch  15, Step:   138800, Batch Loss:     1.764088, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-04 01:46:10,349 - INFO - joeynmt.training - Epoch  15, Step:   139000, Batch Loss:     1.819202, Tokens per Sec:    15942, Lr: 0.000300\n",
      "2021-08-04 01:46:37,075 - INFO - joeynmt.training - Epoch  15, Step:   139200, Batch Loss:     1.728005, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-08-04 01:47:04,062 - INFO - joeynmt.training - Epoch  15, Step:   139400, Batch Loss:     1.654048, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-08-04 01:47:23,468 - INFO - joeynmt.training - Epoch  15: total training loss 4875.23\n",
      "2021-08-04 01:47:23,469 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-04 01:47:30,979 - INFO - joeynmt.training - Epoch  16, Step:   139600, Batch Loss:     1.886906, Tokens per Sec:    15224, Lr: 0.000300\n",
      "2021-08-04 01:47:57,677 - INFO - joeynmt.training - Epoch  16, Step:   139800, Batch Loss:     1.773781, Tokens per Sec:    16136, Lr: 0.000300\n",
      "2021-08-04 01:48:24,552 - INFO - joeynmt.training - Epoch  16, Step:   140000, Batch Loss:     2.008669, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-04 01:49:16,794 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:49:16,794 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:49:16,794 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:49:18,094 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:49:18,095 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:49:18,095 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:49:18,095 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:49:18,095 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:49:18,096 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:49:18,096 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:49:18,096 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 01:49:18,096 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:49:18,097 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:49:18,097 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:49:18,097 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 01:49:18,097 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:49:18,098 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:49:18,098 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:49:18,098 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a loving relationship with his people .\n",
      "2021-08-04 01:49:18,098 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   140000: bleu:  26.82, loss: 100135.2656, ppl:   5.2834, duration: 53.5460s\n",
      "2021-08-04 01:49:45,286 - INFO - joeynmt.training - Epoch  16, Step:   140200, Batch Loss:     1.851489, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-04 01:50:11,908 - INFO - joeynmt.training - Epoch  16, Step:   140400, Batch Loss:     1.701225, Tokens per Sec:    15995, Lr: 0.000300\n",
      "2021-08-04 01:50:38,615 - INFO - joeynmt.training - Epoch  16, Step:   140600, Batch Loss:     1.607837, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-08-04 01:51:05,472 - INFO - joeynmt.training - Epoch  16, Step:   140800, Batch Loss:     1.739347, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-04 01:51:32,503 - INFO - joeynmt.training - Epoch  16, Step:   141000, Batch Loss:     1.776886, Tokens per Sec:    16300, Lr: 0.000300\n",
      "2021-08-04 01:51:59,423 - INFO - joeynmt.training - Epoch  16, Step:   141200, Batch Loss:     1.713793, Tokens per Sec:    15924, Lr: 0.000300\n",
      "2021-08-04 01:52:26,156 - INFO - joeynmt.training - Epoch  16, Step:   141400, Batch Loss:     1.751777, Tokens per Sec:    16243, Lr: 0.000300\n",
      "2021-08-04 01:52:52,798 - INFO - joeynmt.training - Epoch  16, Step:   141600, Batch Loss:     1.683271, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-08-04 01:53:19,689 - INFO - joeynmt.training - Epoch  16, Step:   141800, Batch Loss:     1.718271, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-08-04 01:53:46,511 - INFO - joeynmt.training - Epoch  16, Step:   142000, Batch Loss:     1.658787, Tokens per Sec:    16337, Lr: 0.000300\n",
      "2021-08-04 01:54:40,951 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 01:54:40,951 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 01:54:40,951 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 01:54:41,587 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 01:54:41,587 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 01:54:42,772 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 01:54:42,772 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 01:54:42,772 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 01:54:42,773 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 01:54:42,773 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 01:54:42,773 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 01:54:42,773 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 01:54:42,773 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-04 01:54:42,774 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 01:54:42,774 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 01:54:42,774 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 01:54:42,774 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 01:54:42,774 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 01:54:42,775 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 01:54:42,775 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 01:54:42,775 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 01:54:42,775 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   142000: bleu:  26.87, loss: 99654.1953, ppl:   5.2413, duration: 56.2639s\n",
      "2021-08-04 01:55:09,774 - INFO - joeynmt.training - Epoch  16, Step:   142200, Batch Loss:     1.844102, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-08-04 01:55:20,180 - INFO - joeynmt.training - Epoch  16: total training loss 4861.54\n",
      "2021-08-04 01:55:20,180 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-08-04 01:55:36,682 - INFO - joeynmt.training - Epoch  17, Step:   142400, Batch Loss:     1.749786, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-04 01:56:03,387 - INFO - joeynmt.training - Epoch  17, Step:   142600, Batch Loss:     1.811724, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-04 01:56:29,963 - INFO - joeynmt.training - Epoch  17, Step:   142800, Batch Loss:     1.888207, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-08-04 01:56:56,879 - INFO - joeynmt.training - Epoch  17, Step:   143000, Batch Loss:     1.868505, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-04 01:57:23,472 - INFO - joeynmt.training - Epoch  17, Step:   143200, Batch Loss:     1.685171, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-08-04 01:57:50,306 - INFO - joeynmt.training - Epoch  17, Step:   143400, Batch Loss:     1.697299, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-08-04 01:58:17,109 - INFO - joeynmt.training - Epoch  17, Step:   143600, Batch Loss:     1.683235, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-08-04 01:58:43,947 - INFO - joeynmt.training - Epoch  17, Step:   143800, Batch Loss:     1.673790, Tokens per Sec:    16287, Lr: 0.000300\n",
      "2021-08-04 01:59:11,000 - INFO - joeynmt.training - Epoch  17, Step:   144000, Batch Loss:     1.942937, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-04 02:00:04,584 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 02:00:04,585 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 02:00:04,585 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 02:00:05,833 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 02:00:05,835 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 02:00:05,835 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 02:00:05,835 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 02:00:05,835 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 02:00:05,835 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 02:00:05,836 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 02:00:05,836 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-04 02:00:05,836 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 02:00:05,836 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 02:00:05,836 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 02:00:05,837 - INFO - joeynmt.training - \tHypothesis: Why do false gods have nothing ?\n",
      "2021-08-04 02:00:05,837 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 02:00:05,837 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 02:00:05,837 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 02:00:05,837 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 02:00:05,838 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   144000: bleu:  27.48, loss: 99795.7500, ppl:   5.2536, duration: 54.8374s\n",
      "2021-08-04 02:00:32,589 - INFO - joeynmt.training - Epoch  17, Step:   144200, Batch Loss:     1.584437, Tokens per Sec:    15661, Lr: 0.000300\n",
      "2021-08-04 02:00:59,405 - INFO - joeynmt.training - Epoch  17, Step:   144400, Batch Loss:     1.940379, Tokens per Sec:    16019, Lr: 0.000300\n",
      "2021-08-04 02:01:26,143 - INFO - joeynmt.training - Epoch  17, Step:   144600, Batch Loss:     1.250979, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-04 02:01:53,000 - INFO - joeynmt.training - Epoch  17, Step:   144800, Batch Loss:     1.845052, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-04 02:02:19,802 - INFO - joeynmt.training - Epoch  17, Step:   145000, Batch Loss:     1.641769, Tokens per Sec:    16395, Lr: 0.000300\n",
      "2021-08-04 02:02:21,660 - INFO - joeynmt.training - Epoch  17: total training loss 4861.03\n",
      "2021-08-04 02:02:21,660 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-08-04 02:02:46,667 - INFO - joeynmt.training - Epoch  18, Step:   145200, Batch Loss:     1.709038, Tokens per Sec:    15675, Lr: 0.000300\n",
      "2021-08-04 02:03:13,326 - INFO - joeynmt.training - Epoch  18, Step:   145400, Batch Loss:     1.708619, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-04 02:03:39,935 - INFO - joeynmt.training - Epoch  18, Step:   145600, Batch Loss:     1.911861, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-08-04 02:04:07,064 - INFO - joeynmt.training - Epoch  18, Step:   145800, Batch Loss:     1.602694, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-08-04 02:04:33,933 - INFO - joeynmt.training - Epoch  18, Step:   146000, Batch Loss:     1.814497, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-04 02:05:26,198 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 02:05:26,198 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 02:05:26,198 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 02:05:26,828 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 02:05:26,829 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 02:05:27,569 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 02:05:27,570 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 02:05:27,571 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 02:05:27,571 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-04 02:05:27,571 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 02:05:27,572 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 02:05:27,572 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 02:05:27,572 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 02:05:27,572 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 02:05:27,573 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 02:05:27,573 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 02:05:27,573 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 02:05:27,573 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 02:05:27,574 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 02:05:27,574 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 02:05:27,574 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 02:05:27,574 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   146000: bleu:  27.33, loss: 99514.7891, ppl:   5.2292, duration: 53.6412s\n",
      "2021-08-04 02:05:54,451 - INFO - joeynmt.training - Epoch  18, Step:   146200, Batch Loss:     1.960776, Tokens per Sec:    15942, Lr: 0.000300\n",
      "2021-08-04 02:06:21,271 - INFO - joeynmt.training - Epoch  18, Step:   146400, Batch Loss:     1.830862, Tokens per Sec:    16424, Lr: 0.000300\n",
      "2021-08-04 02:06:48,161 - INFO - joeynmt.training - Epoch  18, Step:   146600, Batch Loss:     1.836448, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-04 02:07:14,824 - INFO - joeynmt.training - Epoch  18, Step:   146800, Batch Loss:     1.762957, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-04 02:07:41,426 - INFO - joeynmt.training - Epoch  18, Step:   147000, Batch Loss:     1.582843, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-04 02:08:08,258 - INFO - joeynmt.training - Epoch  18, Step:   147200, Batch Loss:     1.761383, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-04 02:08:35,086 - INFO - joeynmt.training - Epoch  18, Step:   147400, Batch Loss:     1.636855, Tokens per Sec:    16140, Lr: 0.000300\n",
      "2021-08-04 02:09:02,001 - INFO - joeynmt.training - Epoch  18, Step:   147600, Batch Loss:     1.822477, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-04 02:09:21,695 - INFO - joeynmt.training - Epoch  18: total training loss 4846.97\n",
      "2021-08-04 02:09:21,695 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-08-04 02:09:28,926 - INFO - joeynmt.training - Epoch  19, Step:   147800, Batch Loss:     1.851990, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-08-04 02:09:55,565 - INFO - joeynmt.training - Epoch  19, Step:   148000, Batch Loss:     1.769400, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-04 02:10:49,323 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 02:10:49,323 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 02:10:49,324 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 02:10:50,990 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 02:10:50,991 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 02:10:50,991 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 02:10:50,991 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-04 02:10:50,991 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 02:10:50,992 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 02:10:50,992 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 02:10:50,992 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to study the Scriptures if possible .\n",
      "2021-08-04 02:10:50,992 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 02:10:50,993 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 02:10:50,993 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 02:10:50,993 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 02:10:50,993 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 02:10:50,993 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 02:10:50,994 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 02:10:50,994 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 02:10:50,994 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   148000: bleu:  27.25, loss: 99645.7734, ppl:   5.2406, duration: 55.4281s\n",
      "2021-08-04 02:11:17,971 - INFO - joeynmt.training - Epoch  19, Step:   148200, Batch Loss:     1.953256, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-04 02:11:44,908 - INFO - joeynmt.training - Epoch  19, Step:   148400, Batch Loss:     1.994390, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-04 02:12:11,697 - INFO - joeynmt.training - Epoch  19, Step:   148600, Batch Loss:     1.737007, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-04 02:12:38,309 - INFO - joeynmt.training - Epoch  19, Step:   148800, Batch Loss:     1.793272, Tokens per Sec:    16109, Lr: 0.000300\n",
      "2021-08-04 02:13:05,124 - INFO - joeynmt.training - Epoch  19, Step:   149000, Batch Loss:     1.713796, Tokens per Sec:    15871, Lr: 0.000300\n",
      "2021-08-04 02:13:31,713 - INFO - joeynmt.training - Epoch  19, Step:   149200, Batch Loss:     1.764064, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-08-04 02:13:58,621 - INFO - joeynmt.training - Epoch  19, Step:   149400, Batch Loss:     1.839569, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-04 02:14:25,351 - INFO - joeynmt.training - Epoch  19, Step:   149600, Batch Loss:     1.829585, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-04 02:14:52,195 - INFO - joeynmt.training - Epoch  19, Step:   149800, Batch Loss:     1.996588, Tokens per Sec:    16017, Lr: 0.000300\n",
      "2021-08-04 02:15:19,000 - INFO - joeynmt.training - Epoch  19, Step:   150000, Batch Loss:     1.695410, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-04 02:16:11,545 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 02:16:11,546 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 02:16:11,546 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 02:16:12,184 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 02:16:12,184 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 02:16:12,934 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 02:16:12,935 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 02:16:12,935 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 02:16:12,935 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-04 02:16:12,936 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 02:16:12,936 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 02:16:12,937 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 02:16:12,937 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-04 02:16:12,937 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 02:16:12,938 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 02:16:12,938 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 02:16:12,938 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 02:16:12,938 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 02:16:12,939 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 02:16:12,939 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 02:16:12,939 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 02:16:12,939 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   150000: bleu:  27.28, loss: 99217.1797, ppl:   5.2034, duration: 53.9391s\n",
      "2021-08-04 02:16:39,908 - INFO - joeynmt.training - Epoch  19, Step:   150200, Batch Loss:     1.799444, Tokens per Sec:    15745, Lr: 0.000300\n",
      "2021-08-04 02:17:06,565 - INFO - joeynmt.training - Epoch  19, Step:   150400, Batch Loss:     2.036622, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-08-04 02:17:18,444 - INFO - joeynmt.training - Epoch  19: total training loss 4850.72\n",
      "2021-08-04 02:17:18,445 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-08-04 02:17:33,421 - INFO - joeynmt.training - Epoch  20, Step:   150600, Batch Loss:     1.805423, Tokens per Sec:    15520, Lr: 0.000300\n",
      "2021-08-04 02:18:00,304 - INFO - joeynmt.training - Epoch  20, Step:   150800, Batch Loss:     1.908063, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-08-04 02:18:26,711 - INFO - joeynmt.training - Epoch  20, Step:   151000, Batch Loss:     1.799115, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-08-04 02:18:53,663 - INFO - joeynmt.training - Epoch  20, Step:   151200, Batch Loss:     1.921112, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-04 02:19:20,499 - INFO - joeynmt.training - Epoch  20, Step:   151400, Batch Loss:     1.619262, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-04 02:19:47,408 - INFO - joeynmt.training - Epoch  20, Step:   151600, Batch Loss:     1.938608, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-08-04 02:20:14,386 - INFO - joeynmt.training - Epoch  20, Step:   151800, Batch Loss:     1.552969, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-08-04 02:20:40,929 - INFO - joeynmt.training - Epoch  20, Step:   152000, Batch Loss:     1.956824, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-08-04 02:21:34,701 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 02:21:34,701 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 02:21:34,702 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 02:21:36,053 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 02:21:36,054 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 02:21:36,054 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 02:21:36,054 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-04 02:21:36,055 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 02:21:36,055 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 02:21:36,055 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 02:21:36,055 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-04 02:21:36,056 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 02:21:36,056 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 02:21:36,057 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 02:21:36,057 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 02:21:36,057 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 02:21:36,057 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 02:21:36,058 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 02:21:36,058 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 02:21:36,058 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   152000: bleu:  27.42, loss: 99482.0547, ppl:   5.2263, duration: 55.1286s\n",
      "2021-08-04 02:22:02,850 - INFO - joeynmt.training - Epoch  20, Step:   152200, Batch Loss:     1.858703, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-04 02:22:29,678 - INFO - joeynmt.training - Epoch  20, Step:   152400, Batch Loss:     1.609208, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-04 02:22:56,607 - INFO - joeynmt.training - Epoch  20, Step:   152600, Batch Loss:     1.801550, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-04 02:23:22,996 - INFO - joeynmt.training - Epoch  20, Step:   152800, Batch Loss:     1.639556, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-04 02:23:49,943 - INFO - joeynmt.training - Epoch  20, Step:   153000, Batch Loss:     1.792635, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-04 02:24:16,803 - INFO - joeynmt.training - Epoch  20, Step:   153200, Batch Loss:     1.542770, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-04 02:24:20,874 - INFO - joeynmt.training - Epoch  20: total training loss 4841.92\n",
      "2021-08-04 02:24:20,874 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-08-04 02:24:43,854 - INFO - joeynmt.training - Epoch  21, Step:   153400, Batch Loss:     1.330502, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-08-04 02:25:10,749 - INFO - joeynmt.training - Epoch  21, Step:   153600, Batch Loss:     1.976066, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-08-04 02:25:37,514 - INFO - joeynmt.training - Epoch  21, Step:   153800, Batch Loss:     1.763542, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-04 02:26:04,119 - INFO - joeynmt.training - Epoch  21, Step:   154000, Batch Loss:     1.796824, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-04 02:26:58,277 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 02:26:58,277 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 02:26:58,277 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 02:26:59,644 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 02:26:59,645 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 02:26:59,645 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 02:26:59,645 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitating him , he is speaking to us .\n",
      "2021-08-04 02:26:59,645 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 02:26:59,646 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 02:26:59,646 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 02:26:59,647 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we should use the Bible to preach and to study the Scriptures if possible .\n",
      "2021-08-04 02:26:59,647 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 02:26:59,647 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 02:26:59,647 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 02:26:59,647 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 02:26:59,648 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 02:26:59,648 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 02:26:59,649 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 02:26:59,649 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 02:26:59,649 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step   154000: bleu:  27.28, loss: 99557.4688, ppl:   5.2329, duration: 55.5292s\n",
      "2021-08-04 02:27:26,529 - INFO - joeynmt.training - Epoch  21, Step:   154200, Batch Loss:     1.855793, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-04 02:27:53,457 - INFO - joeynmt.training - Epoch  21, Step:   154400, Batch Loss:     1.821902, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-08-04 02:28:20,237 - INFO - joeynmt.training - Epoch  21, Step:   154600, Batch Loss:     1.578848, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-04 02:28:47,110 - INFO - joeynmt.training - Epoch  21, Step:   154800, Batch Loss:     1.820368, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-08-04 02:29:13,711 - INFO - joeynmt.training - Epoch  21, Step:   155000, Batch Loss:     1.973243, Tokens per Sec:    16042, Lr: 0.000300\n",
      "2021-08-04 02:29:40,391 - INFO - joeynmt.training - Epoch  21, Step:   155200, Batch Loss:     1.767474, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-04 02:30:06,931 - INFO - joeynmt.training - Epoch  21, Step:   155400, Batch Loss:     1.703202, Tokens per Sec:    15756, Lr: 0.000300\n",
      "2021-08-04 02:30:33,533 - INFO - joeynmt.training - Epoch  21, Step:   155600, Batch Loss:     1.756852, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-04 02:31:00,495 - INFO - joeynmt.training - Epoch  21, Step:   155800, Batch Loss:     1.819454, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-08-04 02:31:23,717 - INFO - joeynmt.training - Epoch  21: total training loss 4845.36\n",
      "2021-08-04 02:31:23,717 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-08-04 02:31:27,230 - INFO - joeynmt.training - Epoch  22, Step:   156000, Batch Loss:     1.742757, Tokens per Sec:    14438, Lr: 0.000300\n",
      "2021-08-04 02:32:20,094 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 02:32:20,095 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 02:32:20,095 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 02:32:21,414 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 02:32:21,414 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 02:32:21,415 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 02:32:21,415 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-04 02:32:21,415 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 02:32:21,415 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 02:32:21,415 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 02:32:21,416 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to study the Scriptures if possible .\n",
      "2021-08-04 02:32:21,416 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 02:32:21,416 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 02:32:21,416 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 02:32:21,416 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 02:32:21,417 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 02:32:21,417 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 02:32:21,417 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 02:32:21,417 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 02:32:21,418 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step   156000: bleu:  27.21, loss: 99248.0469, ppl:   5.2060, duration: 54.1873s\n",
      "2021-08-04 02:32:48,338 - INFO - joeynmt.training - Epoch  22, Step:   156200, Batch Loss:     1.770433, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-08-04 02:33:14,898 - INFO - joeynmt.training - Epoch  22, Step:   156400, Batch Loss:     1.898980, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-04 02:33:41,547 - INFO - joeynmt.training - Epoch  22, Step:   156600, Batch Loss:     1.823564, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-08-04 02:34:08,376 - INFO - joeynmt.training - Epoch  22, Step:   156800, Batch Loss:     1.687812, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-08-04 02:34:35,147 - INFO - joeynmt.training - Epoch  22, Step:   157000, Batch Loss:     1.867013, Tokens per Sec:    16355, Lr: 0.000300\n",
      "2021-08-04 02:35:01,941 - INFO - joeynmt.training - Epoch  22, Step:   157200, Batch Loss:     1.768731, Tokens per Sec:    15815, Lr: 0.000300\n",
      "2021-08-04 02:35:28,408 - INFO - joeynmt.training - Epoch  22, Step:   157400, Batch Loss:     1.776369, Tokens per Sec:    16089, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_lgen_reload2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wg0n-qtm7P7t",
    "outputId": "675deada-8e9e-488b-fe6a-1378feba47bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shareddrives/NMT_for_African_Language/Luganda\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmcU9hMH-rMa",
    "outputId": "f7bb2be4-058b-46dd-ca10-06716475c742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 102000\tLoss: 102950.84375\tPPL: 5.53654\tbleu: 26.18486\tLR: 0.00030000\t*\n",
      "Steps: 104000\tLoss: 102431.87500\tPPL: 5.48898\tbleu: 26.33524\tLR: 0.00030000\t*\n",
      "Steps: 106000\tLoss: 102424.21094\tPPL: 5.48828\tbleu: 26.33946\tLR: 0.00030000\t*\n",
      "Steps: 108000\tLoss: 102250.45312\tPPL: 5.47245\tbleu: 26.42383\tLR: 0.00030000\t*\n",
      "Steps: 110000\tLoss: 102364.25000\tPPL: 5.48281\tbleu: 26.23428\tLR: 0.00030000\t\n",
      "Steps: 112000\tLoss: 101759.60156\tPPL: 5.42798\tbleu: 26.55033\tLR: 0.00030000\t*\n",
      "Steps: 114000\tLoss: 101613.21875\tPPL: 5.41479\tbleu: 26.52269\tLR: 0.00030000\t*\n",
      "Steps: 116000\tLoss: 101536.27344\tPPL: 5.40787\tbleu: 26.24736\tLR: 0.00030000\t*\n",
      "Steps: 118000\tLoss: 101056.37500\tPPL: 5.36490\tbleu: 26.52566\tLR: 0.00030000\t*\n",
      "Steps: 120000\tLoss: 100925.59375\tPPL: 5.35325\tbleu: 26.66043\tLR: 0.00030000\t*\n",
      "Steps: 122000\tLoss: 101490.95312\tPPL: 5.40379\tbleu: 26.45827\tLR: 0.00030000\t\n",
      "Steps: 124000\tLoss: 101171.45312\tPPL: 5.37517\tbleu: 26.83602\tLR: 0.00030000\t\n",
      "Steps: 126000\tLoss: 100666.17969\tPPL: 5.33021\tbleu: 26.66287\tLR: 0.00030000\t*\n",
      "Steps: 128000\tLoss: 100640.17188\tPPL: 5.32791\tbleu: 26.74245\tLR: 0.00030000\t*\n",
      "Steps: 130000\tLoss: 100602.21094\tPPL: 5.32455\tbleu: 26.55011\tLR: 0.00030000\t*\n",
      "Steps: 132000\tLoss: 100619.25000\tPPL: 5.32606\tbleu: 26.85878\tLR: 0.00030000\t\n",
      "Steps: 134000\tLoss: 100105.96094\tPPL: 5.28080\tbleu: 26.84202\tLR: 0.00030000\t*\n",
      "Steps: 136000\tLoss: 99928.81250\tPPL: 5.26528\tbleu: 27.10527\tLR: 0.00030000\t*\n",
      "Steps: 138000\tLoss: 100550.33594\tPPL: 5.31996\tbleu: 26.79949\tLR: 0.00030000\t\n",
      "Steps: 140000\tLoss: 100135.26562\tPPL: 5.28338\tbleu: 26.82159\tLR: 0.00030000\t\n",
      "Steps: 142000\tLoss: 99654.19531\tPPL: 5.24129\tbleu: 26.87426\tLR: 0.00030000\t*\n",
      "Steps: 144000\tLoss: 99795.75000\tPPL: 5.25364\tbleu: 27.47785\tLR: 0.00030000\t\n",
      "Steps: 146000\tLoss: 99514.78906\tPPL: 5.22916\tbleu: 27.33499\tLR: 0.00030000\t*\n",
      "Steps: 148000\tLoss: 99645.77344\tPPL: 5.24056\tbleu: 27.24984\tLR: 0.00030000\t\n",
      "Steps: 150000\tLoss: 99217.17969\tPPL: 5.20336\tbleu: 27.27722\tLR: 0.00030000\t*\n",
      "Steps: 152000\tLoss: 99482.05469\tPPL: 5.22632\tbleu: 27.41891\tLR: 0.00030000\t\n",
      "Steps: 154000\tLoss: 99557.46875\tPPL: 5.23287\tbleu: 27.28306\tLR: 0.00030000\t\n",
      "Steps: 156000\tLoss: 99248.04688\tPPL: 5.20603\tbleu: 27.20746\tLR: 0.00030000\t\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lgen_reverse_transformer_continued2/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6CA7SbWu7kKR"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 156000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"joeynmt/models/lgen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"{path}/joeynmt/models/{name}_reverse_transformer_continued2/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lgen_reverse_transformer\"', f'model_dir: \"models/lgen_reverse_transformer_continued3\"').replace(\n",
    "        f'epochs: 30', f'epochs: 1')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asSQAhDx7j-h",
    "outputId": "ac5690d8-78cb-4978-f7dd-13867da1183c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-04 06:53:01,932 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-04 06:53:02,009 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-04 06:53:08,691 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-04 06:53:09,634 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-04 06:53:11,069 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-04 06:53:12,966 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-04 06:53:12,967 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 06:53:13,361 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 06:53:13.609638: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 06:53:15,746 - INFO - joeynmt.training - Total params: 12151040\n",
      "2021-08-04 06:53:20,037 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer_continued2/156000.ckpt\n",
      "2021-08-04 06:53:20,610 - INFO - joeynmt.helpers - cfg.name                           : lgen_reverse_transformer\n",
      "2021-08-04 06:53:20,610 - INFO - joeynmt.helpers - cfg.data.src                       : lg\n",
      "2021-08-04 06:53:20,610 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-04 06:53:20,610 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\n",
      "2021-08-04 06:53:20,611 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\n",
      "2021-08-04 06:53:20,611 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\n",
      "2021-08-04 06:53:20,611 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-04 06:53:20,612 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-04 06:53:20,612 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-04 06:53:20,612 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-08-04 06:53:20,613 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-08-04 06:53:20,613 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-04 06:53:20,613 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-04 06:53:20,613 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer_continued2/156000.ckpt\n",
      "2021-08-04 06:53:20,614 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-04 06:53:20,614 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-04 06:53:20,614 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-04 06:53:20,614 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-04 06:53:20,615 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-04 06:53:20,615 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-04 06:53:20,615 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-04 06:53:20,615 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-04 06:53:20,616 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-04 06:53:20,616 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-04 06:53:20,616 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-04 06:53:20,616 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-04 06:53:20,617 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-04 06:53:20,617 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-04 06:53:20,617 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-04 06:53:20,617 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-04 06:53:20,618 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-04 06:53:20,618 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-04 06:53:20,618 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-04 06:53:20,618 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-04 06:53:20,619 - INFO - joeynmt.helpers - cfg.training.epochs                : 1\n",
      "2021-08-04 06:53:20,619 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-04 06:53:20,619 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-04 06:53:20,619 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-04 06:53:20,620 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lgen_reverse_transformer_continued3\n",
      "2021-08-04 06:53:20,620 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-04 06:53:20,620 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-04 06:53:20,620 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-04 06:53:20,621 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-04 06:53:20,621 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-04 06:53:20,621 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-04 06:53:20,621 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-04 06:53:20,622 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-04 06:53:20,622 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-04 06:53:20,622 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-04 06:53:20,622 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-04 06:53:20,623 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-04 06:53:20,623 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-04 06:53:20,623 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-04 06:53:20,623 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-04 06:53:20,624 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-04 06:53:20,624 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 06:53:20,624 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-04 06:53:20,624 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-04 06:53:20,625 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-04 06:53:20,625 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-04 06:53:20,625 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-04 06:53:20,625 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-04 06:53:20,626 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-04 06:53:20,626 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-04 06:53:20,626 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 06:53:20,626 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-04 06:53:20,627 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-04 06:53:20,627 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-04 06:53:20,627 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-04 06:53:20,627 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-04 06:53:20,628 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 222129,\n",
      "\tvalid 2270,\n",
      "\ttest 2270\n",
      "2021-08-04 06:53:20,628 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ itical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-04 06:53:20,628 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-04 06:53:20,628 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-04 06:53:20,629 - INFO - joeynmt.helpers - Number of Src words (types): 4261\n",
      "2021-08-04 06:53:20,629 - INFO - joeynmt.helpers - Number of Trg words (types): 4261\n",
      "2021-08-04 06:53:20,629 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4261),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4261))\n",
      "2021-08-04 06:53:20,643 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-04 06:53:20,643 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-04 06:54:17,853 - INFO - joeynmt.training - Epoch   1, Step:   156200, Batch Loss:     1.816447, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-08-04 06:55:14,529 - INFO - joeynmt.training - Epoch   1, Step:   156400, Batch Loss:     1.888282, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-08-04 06:56:11,201 - INFO - joeynmt.training - Epoch   1, Step:   156600, Batch Loss:     1.840011, Tokens per Sec:     7465, Lr: 0.000300\n",
      "2021-08-04 06:57:08,475 - INFO - joeynmt.training - Epoch   1, Step:   156800, Batch Loss:     1.663436, Tokens per Sec:     7573, Lr: 0.000300\n",
      "2021-08-04 06:58:05,793 - INFO - joeynmt.training - Epoch   1, Step:   157000, Batch Loss:     1.840648, Tokens per Sec:     7639, Lr: 0.000300\n",
      "2021-08-04 06:59:02,786 - INFO - joeynmt.training - Epoch   1, Step:   157200, Batch Loss:     1.769766, Tokens per Sec:     7435, Lr: 0.000300\n",
      "2021-08-04 06:59:59,454 - INFO - joeynmt.training - Epoch   1, Step:   157400, Batch Loss:     1.783713, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-08-04 07:00:56,894 - INFO - joeynmt.training - Epoch   1, Step:   157600, Batch Loss:     1.869902, Tokens per Sec:     7575, Lr: 0.000300\n",
      "2021-08-04 07:01:54,116 - INFO - joeynmt.training - Epoch   1, Step:   157800, Batch Loss:     1.506354, Tokens per Sec:     7584, Lr: 0.000300\n",
      "2021-08-04 07:02:50,874 - INFO - joeynmt.training - Epoch   1, Step:   158000, Batch Loss:     1.700680, Tokens per Sec:     7525, Lr: 0.000300\n",
      "2021-08-04 07:04:26,572 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 07:04:26,572 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 07:04:26,572 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 07:04:27,301 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:04:27,301 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:04:28,198 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:04:28,200 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-04 07:04:28,200 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-04 07:04:28,200 - INFO - joeynmt.training - \tHypothesis: By considering how he displayed faith and imitating him , he is speaking to us .\n",
      "2021-08-04 07:04:28,200 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:04:28,201 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-04 07:04:28,201 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-04 07:04:28,202 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-04 07:04:28,202 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:04:28,203 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-04 07:04:28,204 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-04 07:04:28,204 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-04 07:04:28,204 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:04:28,205 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-04 07:04:28,205 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-04 07:04:28,206 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-04 07:04:28,206 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step   158000: bleu:  27.12, loss: 98674.3438, ppl:   5.1566, duration: 97.3314s\n",
      "2021-08-04 07:05:25,701 - INFO - joeynmt.training - Epoch   1, Step:   158200, Batch Loss:     1.674631, Tokens per Sec:     7566, Lr: 0.000300\n",
      "2021-08-04 07:06:22,888 - INFO - joeynmt.training - Epoch   1, Step:   158400, Batch Loss:     1.666296, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-08-04 07:07:19,809 - INFO - joeynmt.training - Epoch   1, Step:   158600, Batch Loss:     1.770043, Tokens per Sec:     7521, Lr: 0.000300\n",
      "2021-08-04 07:07:52,907 - INFO - joeynmt.training - Epoch   1: total training loss 4786.22\n",
      "2021-08-04 07:07:52,908 - INFO - joeynmt.training - Training ended after   1 epochs.\n",
      "2021-08-04 07:07:52,908 - INFO - joeynmt.training - Best validation result (greedy) at step   158000:   5.16 ppl.\n",
      "2021-08-04 07:07:52,937 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
      "2021-08-04 07:07:53,417 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 07:07:53,664 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 07:07:53,741 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe.en)...\n",
      "2021-08-04 07:09:58,204 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 07:09:58,204 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 07:09:58,205 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 07:09:58,952 - INFO - joeynmt.prediction -  dev bleu[13a]:  27.76 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-04 07:09:58,959 - INFO - joeynmt.prediction - Translations saved to: models/lgen_reverse_transformer_continued3/00158000.hyps.dev\n",
      "2021-08-04 07:09:58,960 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe.en)...\n",
      "2021-08-04 07:12:06,839 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 07:12:06,839 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 07:12:06,840 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 07:12:07,595 - INFO - joeynmt.prediction - test bleu[13a]:  27.80 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-04 07:12:07,603 - INFO - joeynmt.prediction - Translations saved to: models/lgen_reverse_transformer_continued3/00158000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_lgen_reload3.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scaTiRnbgkbo"
   },
   "source": [
    "#### Sample translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGsaC4OpgNcT",
    "outputId": "494847a6-6c82-4b58-f8f2-76d54a0e50e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What question about the “ other sheep ” can we ask ourselves ?\n",
      "Because “ God is not partial , but in every nation a man fears him and works righteousness is acceptable to him . ” ​ — Acts 10 : 34 , 35 .\n",
      "Jesus will destroy Satan and eliminate all the problems Satan caused . ​ — Gen .\n",
      "Because David loved Jehovah .\n",
      "9 : 24 ; Luke 4 : 43 .\n",
      "Indeed , those who accept God’s word have been “ out of every tribe and tongue and nations . ”\n",
      "However , we know that “ the appointed times have given me . ”\n",
      "Nevertheless , people who seem to be unharmful are often hurt .\n",
      "“ The Endurance of Job ”\n",
      "But we may wonder : How can our conscience be trained to help us when\n"
     ]
    }
   ],
   "source": [
    "# Candidates\n",
    "! head \"joeynmt/models/lgen_reverse_transformer_continued3/00158000.hyps.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCBFB2QdgNcV",
    "outputId": "3d0b1b57-d9e1-4c0e-a1cd-972f61aac76e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What question about the “ other sheep ” now arises ?\n",
      "Because “ God is not partial , but in every nation the man that fears him and works righteousness is acceptable to him . ” ​ — Acts 10 : 34 , 35 .\n",
      "Jesus will crush the serpent’s head and erase from the universe all traces of Satan’s rebellion . ​ — Gen .\n",
      "Because David loved Jehovah .\n",
      "9 : 24 ; Luke 4 : 43 .\n",
      "Truly , those who have embraced God’s word have come from “ every tribe and tongue and people and nation . ”\n",
      "We do know , however , that “ the time left is reduced . ”\n",
      "Still , shocking deeds are often perpetrated by seemingly ordinary people in the neighborhood .\n",
      "“ The Endurance of Job ”\n",
      "However , we might ask : How can a well - trained conscience help us when we need to make decisions ?\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "! head \"test.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9FBxZW1gNcX",
    "outputId": "2388c9e4-2527-47ec-90b8-2cab4977ca5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kibuuzo ki ekikwata ku ‘ b’endiga endala ’ kye tuyinza okwebuuza ?\n",
      "Kubanga “ Katonda tasosola , naye mu buli ggwanga omuntu amutya n’akola eby’obutuukirivu amukkiriza . ” ​ — Bik . 10 : 34 , 35 .\n",
      "Yesu ajja kuzikiriza Sitaani era amalewo ebizibu byonna Sitaani bye yaleetawo . ​ — Lub .\n",
      "Kubanga Dawudi yali ayagala nnyo Yakuwa .\n",
      "9 : 24 ; Luk . 4 : 43 .\n",
      "Mazima ddala , abo abakkiriza ekigambo kya Katonda bavudde ‘ mu buli kika n’ennimi n’amawanga . ’\n",
      "Kyokka , tumanyi nti “ ebiro biyimpawadde . ”\n",
      "Wadde kiri kityo , abantu abalabika ng’abatalina mutawaana be batera okukola ebintu ebyesisiwaza ennyo .\n",
      "‘ Obugumiikiriza bwa Yobu ’\n",
      "Naye tuyinza okuba nga twebuuza : Omuntu waffe ow’omunda atendekeddwa obulungi ayinza atya okutuyamba nga tulina bye tusalawo ?\n"
     ]
    }
   ],
   "source": [
    "# Source\n",
    "! head \"test.lg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7UpBGEl9T44"
   },
   "source": [
    "## Kinyarwanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "eQTj3_hl1gZF"
   },
   "outputs": [],
   "source": [
    "# Changing to Kinyarwanda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "YY9Rini27wHX",
    "outputId": "ae40c587-276e-4946-b231-e1cc71bea0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 95 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 26.4 MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 41.3 MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.9.6-py3-none-any.whl (375 kB)\n",
      "\u001b[K     |████████████████████████████████| 375 kB 43.5 MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Collecting astroid<2.7,>=2.6.5\n",
      "  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 48.2 MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.9.3-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 45.7 MB/s \n",
      "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
      "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 43.0 MB/s \n",
      "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 3.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=d386fe214f306a191c1811c035ea3e977b62d9b3159a776198cbd69f04cba1ba\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ycqrw6gl/wheels/59/bd/a5/113ce66d51703aba488fda9b70982ce262e9b5fd115452af28\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68439 sha256=13c6faae1cf403d46c7e69240a0194db176bd2d2ca3667c156eee6fc23513626\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
      "Successfully built joeynmt wrapt\n",
      "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
      "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed astroid-2.6.5 isort-5.9.3 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.6 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "a7gEmbTc9bdI"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language2, source_language)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language2}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language2}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 2000\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer\"\n",
    "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda\", source_language=source_language, target_language2=target_language2)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meq21XqE9bdK",
    "outputId": "13298382-5ae4-4c38-a0e7-185221579fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-01 21:43:46,916 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-01 21:43:46,984 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-01 21:43:56,063 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-01 21:43:56,709 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-01 21:43:57,401 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-01 21:43:58,064 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-01 21:43:58,064 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-01 21:43:58,288 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-01 21:43:58.540201: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-01 21:44:01,479 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-08-01 21:44:05,154 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-08-01 21:44:05,154 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-08-01 21:44:05,154 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-01 21:44:05,154 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-08-01 21:44:05,154 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-08-01 21:44:05,154 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-08-01 21:44:05,155 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-01 21:44:05,155 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-01 21:44:05,155 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-01 21:44:05,155 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-08-01 21:44:05,155 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-08-01 21:44:05,155 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-01 21:44:05,155 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-01 21:44:05,155 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-01 21:44:05,156 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-01 21:44:05,156 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-01 21:44:05,156 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-01 21:44:05,156 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-01 21:44:05,156 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-01 21:44:05,156 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-01 21:44:05,156 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-01 21:44:05,156 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-01 21:44:05,157 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-01 21:44:05,157 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-01 21:44:05,157 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-01 21:44:05,157 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-01 21:44:05,157 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-01 21:44:05,157 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-01 21:44:05,157 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-01 21:44:05,157 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-01 21:44:05,158 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-01 21:44:05,158 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-01 21:44:05,158 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-01 21:44:05,158 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-01 21:44:05,158 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-01 21:44:05,158 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-08-01 21:44:05,158 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-01 21:44:05,158 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer\n",
      "2021-08-01 21:44:05,159 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
      "2021-08-01 21:44:05,159 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-01 21:44:05,159 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-01 21:44:05,159 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-01 21:44:05,159 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-01 21:44:05,159 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-01 21:44:05,160 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-01 21:44:05,160 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-01 21:44:05,160 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-01 21:44:05,160 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-01 21:44:05,160 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-01 21:44:05,160 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-01 21:44:05,160 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-01 21:44:05,161 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-01 21:44:05,161 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-01 21:44:05,161 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-01 21:44:05,161 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-01 21:44:05,161 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-01 21:44:05,161 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-01 21:44:05,161 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-01 21:44:05,161 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-01 21:44:05,162 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-01 21:44:05,162 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-01 21:44:05,162 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-01 21:44:05,162 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-01 21:44:05,162 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-01 21:44:05,162 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-01 21:44:05,162 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-01 21:44:05,163 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-01 21:44:05,163 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-01 21:44:05,163 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-01 21:44:05,163 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 426806,\n",
      "\tvalid 4368,\n",
      "\ttest 4368\n",
      "2021-08-01 21:44:05,163 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-01 21:44:05,163 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-01 21:44:05,164 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-01 21:44:05,164 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-08-01 21:44:05,164 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-08-01 21:44:05,164 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-08-01 21:44:05,174 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-01 21:44:05,175 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-01 21:44:18,494 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.565884, Tokens per Sec:    16488, Lr: 0.000300\n",
      "2021-08-01 21:44:31,169 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.438416, Tokens per Sec:    17244, Lr: 0.000300\n",
      "2021-08-01 21:44:43,886 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.367928, Tokens per Sec:    17012, Lr: 0.000300\n",
      "2021-08-01 21:44:57,044 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.018892, Tokens per Sec:    17142, Lr: 0.000300\n",
      "2021-08-01 21:45:10,189 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.892179, Tokens per Sec:    16411, Lr: 0.000300\n",
      "2021-08-01 21:45:23,422 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.710572, Tokens per Sec:    16606, Lr: 0.000300\n",
      "2021-08-01 21:45:36,812 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.762597, Tokens per Sec:    16408, Lr: 0.000300\n",
      "2021-08-01 21:45:50,243 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.493057, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-08-01 21:46:03,775 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.291920, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-01 21:46:17,540 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.284346, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-01 21:46:31,111 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     4.265389, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-08-01 21:46:44,769 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.314989, Tokens per Sec:    16363, Lr: 0.000300\n",
      "2021-08-01 21:46:58,272 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.459489, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-08-01 21:47:11,985 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.453602, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-01 21:47:25,547 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.467080, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-08-01 21:47:39,033 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.904145, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-08-01 21:47:52,670 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.186100, Tokens per Sec:    16308, Lr: 0.000300\n",
      "2021-08-01 21:48:06,377 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.818110, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-08-01 21:48:19,940 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.962349, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-01 21:48:33,797 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.115120, Tokens per Sec:    16250, Lr: 0.000300\n",
      "2021-08-01 21:48:47,436 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.983780, Tokens per Sec:    16506, Lr: 0.000300\n",
      "2021-08-01 21:49:01,016 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     4.125592, Tokens per Sec:    16455, Lr: 0.000300\n",
      "2021-08-01 21:49:14,557 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     4.101021, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-01 21:49:28,288 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.622675, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-01 21:49:41,937 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     3.349196, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-08-01 21:49:55,512 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.603637, Tokens per Sec:    15859, Lr: 0.000300\n",
      "2021-08-01 21:50:08,984 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.774602, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-08-01 21:50:22,541 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.342775, Tokens per Sec:    16129, Lr: 0.000300\n",
      "2021-08-01 21:50:36,051 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.746827, Tokens per Sec:    16277, Lr: 0.000300\n",
      "2021-08-01 21:50:49,689 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.626927, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-08-01 21:51:03,514 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.647932, Tokens per Sec:    15942, Lr: 0.000300\n",
      "2021-08-01 21:51:17,379 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.603436, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-01 21:51:31,079 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     3.472514, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-08-01 21:51:44,590 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.455549, Tokens per Sec:    16305, Lr: 0.000300\n",
      "2021-08-01 21:51:58,157 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     3.268437, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-01 21:52:11,979 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.511099, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-08-01 21:52:25,654 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     3.432998, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-08-01 21:52:39,069 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     3.353303, Tokens per Sec:    16303, Lr: 0.000300\n",
      "2021-08-01 21:52:52,462 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     3.457427, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-08-01 21:53:06,325 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     3.359060, Tokens per Sec:    16518, Lr: 0.000300\n",
      "2021-08-01 21:53:19,854 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     3.119673, Tokens per Sec:    15581, Lr: 0.000300\n",
      "2021-08-01 21:53:33,406 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     3.161037, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-01 21:53:47,129 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     3.543259, Tokens per Sec:    16157, Lr: 0.000300\n",
      "2021-08-01 21:54:00,732 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     3.243774, Tokens per Sec:    16235, Lr: 0.000300\n",
      "2021-08-01 21:54:14,377 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     3.665290, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-08-01 21:54:28,227 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     2.974811, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-08-01 21:54:41,657 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     3.272464, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-01 21:54:55,225 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     3.334963, Tokens per Sec:    15748, Lr: 0.000300\n",
      "2021-08-01 21:55:09,163 - INFO - joeynmt.training - Epoch   1, Step:     4900, Batch Loss:     3.085886, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-01 21:55:22,808 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     3.167774, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-01 21:57:21,333 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 21:57:21,333 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 21:57:21,333 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 21:57:22,571 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 21:57:22,571 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 21:57:23,252 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 21:57:23,253 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-01 21:57:23,253 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-01 21:57:23,253 - INFO - joeynmt.training - \tHypothesis: He said : “ I am not a God , but I am my God . ”\n",
      "2021-08-01 21:57:23,253 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 21:57:23,254 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-01 21:57:23,254 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-01 21:57:23,254 - INFO - joeynmt.training - \tHypothesis: King Solomon wrote : “ We have been a great way to Jehovah , and we are the things that we are in the way of Jehovah . ”\n",
      "2021-08-01 21:57:23,254 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 21:57:23,255 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-01 21:57:23,255 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-01 21:57:23,255 - INFO - joeynmt.training - \tHypothesis: Pharai : The book of Revelation is the book of Revelation , the book of God’s Kingdom .\n",
      "2021-08-01 21:57:23,255 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 21:57:23,256 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-01 21:57:23,256 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-01 21:57:23,256 - INFO - joeynmt.training - \tHypothesis: How does Christ “ keep the Christ ” and “ the sight of the things ” ?\n",
      "2021-08-01 21:57:23,256 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     5000: bleu:   6.43, loss: 369605.1562, ppl:  21.1739, duration: 120.4482s\n",
      "2021-08-01 21:57:36,914 - INFO - joeynmt.training - Epoch   1, Step:     5100, Batch Loss:     3.222271, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-08-01 21:57:50,423 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     3.230436, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-08-01 21:58:04,032 - INFO - joeynmt.training - Epoch   1, Step:     5300, Batch Loss:     3.155341, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-01 21:58:07,650 - INFO - joeynmt.training - Epoch   1: total training loss 20736.04\n",
      "2021-08-01 21:58:07,650 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-01 21:58:18,329 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     3.051142, Tokens per Sec:    15057, Lr: 0.000300\n",
      "2021-08-01 21:58:32,010 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     3.259073, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-08-01 21:58:45,543 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.880628, Tokens per Sec:    16286, Lr: 0.000300\n",
      "2021-08-01 21:58:59,201 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     2.890299, Tokens per Sec:    16449, Lr: 0.000300\n",
      "2021-08-01 21:59:12,841 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     3.601172, Tokens per Sec:    16155, Lr: 0.000300\n",
      "2021-08-01 21:59:26,624 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     3.004059, Tokens per Sec:    16398, Lr: 0.000300\n",
      "2021-08-01 21:59:40,193 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     2.799766, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-01 21:59:53,904 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     3.499376, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-01 22:00:07,744 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     3.138779, Tokens per Sec:    16416, Lr: 0.000300\n",
      "2021-08-01 22:00:21,329 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     3.105542, Tokens per Sec:    16427, Lr: 0.000300\n",
      "2021-08-01 22:00:34,808 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     2.835548, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-08-01 22:00:48,523 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.736385, Tokens per Sec:    16378, Lr: 0.000300\n",
      "2021-08-01 22:01:02,161 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     2.908800, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-01 22:01:15,751 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     2.970164, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-01 22:01:29,348 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     2.831478, Tokens per Sec:    16510, Lr: 0.000300\n",
      "2021-08-01 22:01:42,872 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     3.151037, Tokens per Sec:    16184, Lr: 0.000300\n",
      "2021-08-01 22:01:56,381 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     3.003474, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-08-01 22:02:10,075 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     2.858322, Tokens per Sec:    15958, Lr: 0.000300\n",
      "2021-08-01 22:02:23,650 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     3.141797, Tokens per Sec:    16400, Lr: 0.000300\n",
      "2021-08-01 22:02:37,270 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     2.887747, Tokens per Sec:    16499, Lr: 0.000300\n",
      "2021-08-01 22:02:50,754 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     3.230149, Tokens per Sec:    16386, Lr: 0.000300\n",
      "2021-08-01 22:03:04,288 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     2.837054, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-01 22:03:18,039 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     3.011582, Tokens per Sec:    16181, Lr: 0.000300\n",
      "2021-08-01 22:03:31,442 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     2.999291, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-08-01 22:03:45,063 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     2.952673, Tokens per Sec:    16585, Lr: 0.000300\n",
      "2021-08-01 22:03:58,604 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     2.634183, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-08-01 22:04:12,172 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     3.161128, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-01 22:04:25,878 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     2.752490, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-08-01 22:04:39,470 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     2.849482, Tokens per Sec:    16415, Lr: 0.000300\n",
      "2021-08-01 22:04:53,087 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     2.928237, Tokens per Sec:    16626, Lr: 0.000300\n",
      "2021-08-01 22:05:06,453 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     3.018109, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-01 22:05:20,107 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     3.095498, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-08-01 22:05:33,605 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     3.355749, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-01 22:05:47,011 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     2.779459, Tokens per Sec:    16178, Lr: 0.000300\n",
      "2021-08-01 22:06:00,768 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     2.946092, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-01 22:06:14,407 - INFO - joeynmt.training - Epoch   2, Step:     8900, Batch Loss:     2.966624, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-01 22:06:27,856 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     2.652592, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-01 22:06:41,507 - INFO - joeynmt.training - Epoch   2, Step:     9100, Batch Loss:     2.767262, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-08-01 22:06:55,330 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     2.754117, Tokens per Sec:    15898, Lr: 0.000300\n",
      "2021-08-01 22:07:09,118 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     3.206376, Tokens per Sec:    15942, Lr: 0.000300\n",
      "2021-08-01 22:07:22,459 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     2.957551, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-08-01 22:07:35,920 - INFO - joeynmt.training - Epoch   2, Step:     9500, Batch Loss:     3.675741, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-01 22:07:49,390 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     2.709157, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-08-01 22:08:02,890 - INFO - joeynmt.training - Epoch   2, Step:     9700, Batch Loss:     2.760036, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-01 22:08:16,406 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     2.709210, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-01 22:08:29,983 - INFO - joeynmt.training - Epoch   2, Step:     9900, Batch Loss:     2.546770, Tokens per Sec:    16304, Lr: 0.000300\n",
      "2021-08-01 22:08:43,421 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     2.608457, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-01 22:10:10,511 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 22:10:10,512 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 22:10:10,512 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 22:10:11,839 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 22:10:11,839 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 22:10:12,578 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 22:10:12,579 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-01 22:10:12,579 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-01 22:10:12,579 - INFO - joeynmt.training - \tHypothesis: He said : “ But the God is the One who is the God of me , and I am to be a good . ”\n",
      "2021-08-01 22:10:12,579 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 22:10:12,580 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-01 22:10:12,580 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-01 22:10:12,580 - INFO - joeynmt.training - \tHypothesis: King Solomon wrote : “ We are not the fear of Jehovah , and we are more important than the things of the things that are safe . ”\n",
      "2021-08-01 22:10:12,580 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 22:10:12,581 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-01 22:10:12,581 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-01 22:10:12,581 - INFO - joeynmt.training - \tHypothesis: Canius : The Gospel of Daniel is recorded in the book of Daniel about God’s Kingdom .\n",
      "2021-08-01 22:10:12,581 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 22:10:12,582 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-01 22:10:12,582 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-01 22:10:12,582 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on the heart ” and to be sick and to be sick ?\n",
      "2021-08-01 22:10:12,582 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    10000: bleu:  11.47, loss: 315656.0625, ppl:  13.5607, duration: 89.1608s\n",
      "2021-08-01 22:10:26,524 - INFO - joeynmt.training - Epoch   2, Step:    10100, Batch Loss:     2.702200, Tokens per Sec:    15602, Lr: 0.000300\n",
      "2021-08-01 22:10:40,185 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     3.187795, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-01 22:10:53,776 - INFO - joeynmt.training - Epoch   2, Step:    10300, Batch Loss:     3.112383, Tokens per Sec:    15762, Lr: 0.000300\n",
      "2021-08-01 22:11:07,476 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     3.099761, Tokens per Sec:    16166, Lr: 0.000300\n",
      "2021-08-01 22:11:21,219 - INFO - joeynmt.training - Epoch   2, Step:    10500, Batch Loss:     2.602605, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-08-01 22:11:34,703 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     2.755609, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-08-01 22:11:41,624 - INFO - joeynmt.training - Epoch   2: total training loss 15628.06\n",
      "2021-08-01 22:11:41,625 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-01 22:11:48,985 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     2.696851, Tokens per Sec:    15164, Lr: 0.000300\n",
      "2021-08-01 22:12:02,690 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     2.865384, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-01 22:12:16,376 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     2.719994, Tokens per Sec:    16364, Lr: 0.000300\n",
      "2021-08-01 22:12:29,917 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     2.573392, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-01 22:12:43,520 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     2.409816, Tokens per Sec:    16401, Lr: 0.000300\n",
      "2021-08-01 22:12:57,109 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     2.776703, Tokens per Sec:    16106, Lr: 0.000300\n",
      "2021-08-01 22:13:10,636 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     2.913113, Tokens per Sec:    15812, Lr: 0.000300\n",
      "2021-08-01 22:13:24,312 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     2.680282, Tokens per Sec:    16360, Lr: 0.000300\n",
      "2021-08-01 22:13:37,850 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     2.561074, Tokens per Sec:    16355, Lr: 0.000300\n",
      "2021-08-01 22:13:51,498 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     2.916020, Tokens per Sec:    16106, Lr: 0.000300\n",
      "2021-08-01 22:14:05,111 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     2.753025, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-01 22:14:18,714 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     2.577676, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-01 22:14:32,249 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     2.612067, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-01 22:14:45,745 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     3.144523, Tokens per Sec:    16327, Lr: 0.000300\n",
      "2021-08-01 22:14:59,352 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     2.802682, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-08-01 22:15:12,990 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     2.867098, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-08-01 22:15:26,469 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     2.641968, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-01 22:15:39,953 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     2.952091, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-01 22:15:53,344 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     2.835124, Tokens per Sec:    16549, Lr: 0.000300\n",
      "2021-08-01 22:16:06,777 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     2.461866, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-01 22:16:20,312 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     2.410890, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-08-01 22:16:33,973 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     2.793150, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-08-01 22:16:47,468 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     2.730109, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-01 22:17:01,110 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     2.794614, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-01 22:17:14,626 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     2.771093, Tokens per Sec:    16492, Lr: 0.000300\n",
      "2021-08-01 22:17:27,869 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     2.730776, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-08-01 22:17:41,284 - INFO - joeynmt.training - Epoch   3, Step:    13300, Batch Loss:     2.607793, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-08-01 22:17:54,688 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     2.942829, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-01 22:18:08,323 - INFO - joeynmt.training - Epoch   3, Step:    13500, Batch Loss:     2.643925, Tokens per Sec:    16155, Lr: 0.000300\n",
      "2021-08-01 22:18:21,911 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     2.376639, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-01 22:18:35,555 - INFO - joeynmt.training - Epoch   3, Step:    13700, Batch Loss:     2.611007, Tokens per Sec:    16306, Lr: 0.000300\n",
      "2021-08-01 22:18:49,219 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     2.381173, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-08-01 22:19:02,796 - INFO - joeynmt.training - Epoch   3, Step:    13900, Batch Loss:     2.573158, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-08-01 22:19:16,462 - INFO - joeynmt.training - Epoch   3, Step:    14000, Batch Loss:     2.620118, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-08-01 22:19:29,915 - INFO - joeynmt.training - Epoch   3, Step:    14100, Batch Loss:     2.456336, Tokens per Sec:    16274, Lr: 0.000300\n",
      "2021-08-01 22:19:43,483 - INFO - joeynmt.training - Epoch   3, Step:    14200, Batch Loss:     2.399810, Tokens per Sec:    16297, Lr: 0.000300\n",
      "2021-08-01 22:19:57,103 - INFO - joeynmt.training - Epoch   3, Step:    14300, Batch Loss:     2.564612, Tokens per Sec:    16518, Lr: 0.000300\n",
      "2021-08-01 22:20:10,882 - INFO - joeynmt.training - Epoch   3, Step:    14400, Batch Loss:     2.706959, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-01 22:20:24,466 - INFO - joeynmt.training - Epoch   3, Step:    14500, Batch Loss:     2.946449, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-01 22:20:37,898 - INFO - joeynmt.training - Epoch   3, Step:    14600, Batch Loss:     2.316525, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-01 22:20:51,362 - INFO - joeynmt.training - Epoch   3, Step:    14700, Batch Loss:     2.463533, Tokens per Sec:    16526, Lr: 0.000300\n",
      "2021-08-01 22:21:04,646 - INFO - joeynmt.training - Epoch   3, Step:    14800, Batch Loss:     2.575873, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-08-01 22:21:18,288 - INFO - joeynmt.training - Epoch   3, Step:    14900, Batch Loss:     2.774721, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-01 22:21:32,041 - INFO - joeynmt.training - Epoch   3, Step:    15000, Batch Loss:     2.732759, Tokens per Sec:    16142, Lr: 0.000300\n",
      "2021-08-01 22:23:14,366 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 22:23:14,367 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 22:23:14,367 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 22:23:15,608 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 22:23:15,609 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 22:23:16,347 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 22:23:16,348 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-01 22:23:16,349 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-01 22:23:16,349 - INFO - joeynmt.training - \tHypothesis: He said : “ I am dedicated to God , but I am to be my God . ”\n",
      "2021-08-01 22:23:16,349 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 22:23:16,349 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-01 22:23:16,349 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-01 22:23:16,350 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are not the one who is sorrow in Jehovah , the most powerful things that are saved . ”\n",
      "2021-08-01 22:23:16,351 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 22:23:16,351 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-01 22:23:16,352 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-01 22:23:16,352 - INFO - joeynmt.training - \tHypothesis: Canaanus : The young prophet Daniel is a book of Daniel to see what the Kingdom of God has been .\n",
      "2021-08-01 22:23:16,352 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 22:23:16,352 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-01 22:23:16,352 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-01 22:23:16,353 - INFO - joeynmt.training - \tHypothesis: How does Christ “ keep on the end ” and bring them to him ?\n",
      "2021-08-01 22:23:16,353 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    15000: bleu:  15.53, loss: 283178.8438, ppl:  10.3701, duration: 104.3112s\n",
      "2021-08-01 22:23:30,106 - INFO - joeynmt.training - Epoch   3, Step:    15100, Batch Loss:     2.598344, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-01 22:23:43,403 - INFO - joeynmt.training - Epoch   3, Step:    15200, Batch Loss:     2.512480, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-08-01 22:23:57,131 - INFO - joeynmt.training - Epoch   3, Step:    15300, Batch Loss:     2.592774, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-08-01 22:24:10,615 - INFO - joeynmt.training - Epoch   3, Step:    15400, Batch Loss:     2.513542, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-08-01 22:24:24,390 - INFO - joeynmt.training - Epoch   3, Step:    15500, Batch Loss:     2.414642, Tokens per Sec:    16615, Lr: 0.000300\n",
      "2021-08-01 22:24:38,024 - INFO - joeynmt.training - Epoch   3, Step:    15600, Batch Loss:     3.073452, Tokens per Sec:    16485, Lr: 0.000300\n",
      "2021-08-01 22:24:51,474 - INFO - joeynmt.training - Epoch   3, Step:    15700, Batch Loss:     2.540163, Tokens per Sec:    16237, Lr: 0.000300\n",
      "2021-08-01 22:25:04,997 - INFO - joeynmt.training - Epoch   3, Step:    15800, Batch Loss:     2.495703, Tokens per Sec:    16439, Lr: 0.000300\n",
      "2021-08-01 22:25:18,668 - INFO - joeynmt.training - Epoch   3, Step:    15900, Batch Loss:     2.979247, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-08-01 22:25:29,526 - INFO - joeynmt.training - Epoch   3: total training loss 13988.32\n",
      "2021-08-01 22:25:29,526 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-01 22:25:32,630 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     2.092979, Tokens per Sec:    13034, Lr: 0.000300\n",
      "2021-08-01 22:25:46,018 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     2.501781, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-08-01 22:25:59,671 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     2.845242, Tokens per Sec:    16252, Lr: 0.000300\n",
      "2021-08-01 22:26:13,086 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     2.429238, Tokens per Sec:    15928, Lr: 0.000300\n",
      "2021-08-01 22:26:26,757 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     2.492795, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-01 22:26:40,224 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     2.629764, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-01 22:26:53,732 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     2.454778, Tokens per Sec:    16154, Lr: 0.000300\n",
      "2021-08-01 22:27:07,172 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     2.539599, Tokens per Sec:    16235, Lr: 0.000300\n",
      "2021-08-01 22:27:20,875 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     2.558614, Tokens per Sec:    16366, Lr: 0.000300\n",
      "2021-08-01 22:27:34,612 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     2.172642, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-08-01 22:27:48,327 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     2.469423, Tokens per Sec:    16497, Lr: 0.000300\n",
      "2021-08-01 22:28:02,118 - INFO - joeynmt.training - Epoch   4, Step:    17100, Batch Loss:     2.239183, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-08-01 22:28:15,565 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     2.712141, Tokens per Sec:    16142, Lr: 0.000300\n",
      "2021-08-01 22:28:29,095 - INFO - joeynmt.training - Epoch   4, Step:    17300, Batch Loss:     2.181568, Tokens per Sec:    16361, Lr: 0.000300\n",
      "2021-08-01 22:28:42,484 - INFO - joeynmt.training - Epoch   4, Step:    17400, Batch Loss:     2.528276, Tokens per Sec:    16269, Lr: 0.000300\n",
      "2021-08-01 22:28:56,062 - INFO - joeynmt.training - Epoch   4, Step:    17500, Batch Loss:     2.492085, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-01 22:29:09,590 - INFO - joeynmt.training - Epoch   4, Step:    17600, Batch Loss:     2.225999, Tokens per Sec:    16147, Lr: 0.000300\n",
      "2021-08-01 22:29:22,981 - INFO - joeynmt.training - Epoch   4, Step:    17700, Batch Loss:     2.561279, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-01 22:29:36,264 - INFO - joeynmt.training - Epoch   4, Step:    17800, Batch Loss:     2.289659, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-08-01 22:29:49,599 - INFO - joeynmt.training - Epoch   4, Step:    17900, Batch Loss:     2.504128, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-08-01 22:30:03,231 - INFO - joeynmt.training - Epoch   4, Step:    18000, Batch Loss:     2.725129, Tokens per Sec:    16268, Lr: 0.000300\n",
      "2021-08-01 22:30:16,823 - INFO - joeynmt.training - Epoch   4, Step:    18100, Batch Loss:     2.316303, Tokens per Sec:    16248, Lr: 0.000300\n",
      "2021-08-01 22:30:30,410 - INFO - joeynmt.training - Epoch   4, Step:    18200, Batch Loss:     2.465699, Tokens per Sec:    16486, Lr: 0.000300\n",
      "2021-08-01 22:30:43,911 - INFO - joeynmt.training - Epoch   4, Step:    18300, Batch Loss:     2.441798, Tokens per Sec:    16466, Lr: 0.000300\n",
      "2021-08-01 22:30:57,431 - INFO - joeynmt.training - Epoch   4, Step:    18400, Batch Loss:     2.403108, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-08-01 22:31:11,177 - INFO - joeynmt.training - Epoch   4, Step:    18500, Batch Loss:     2.451602, Tokens per Sec:    16378, Lr: 0.000300\n",
      "2021-08-01 22:31:24,862 - INFO - joeynmt.training - Epoch   4, Step:    18600, Batch Loss:     2.393626, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-08-01 22:31:38,395 - INFO - joeynmt.training - Epoch   4, Step:    18700, Batch Loss:     2.519303, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-01 22:31:51,801 - INFO - joeynmt.training - Epoch   4, Step:    18800, Batch Loss:     2.425633, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-08-01 22:32:05,284 - INFO - joeynmt.training - Epoch   4, Step:    18900, Batch Loss:     2.553564, Tokens per Sec:    16425, Lr: 0.000300\n",
      "2021-08-01 22:32:19,025 - INFO - joeynmt.training - Epoch   4, Step:    19000, Batch Loss:     2.347545, Tokens per Sec:    16316, Lr: 0.000300\n",
      "2021-08-01 22:32:32,690 - INFO - joeynmt.training - Epoch   4, Step:    19100, Batch Loss:     2.335645, Tokens per Sec:    16310, Lr: 0.000300\n",
      "2021-08-01 22:32:46,235 - INFO - joeynmt.training - Epoch   4, Step:    19200, Batch Loss:     2.463034, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-01 22:32:59,914 - INFO - joeynmt.training - Epoch   4, Step:    19300, Batch Loss:     2.678273, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-01 22:33:13,373 - INFO - joeynmt.training - Epoch   4, Step:    19400, Batch Loss:     2.613602, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-08-01 22:33:26,897 - INFO - joeynmt.training - Epoch   4, Step:    19500, Batch Loss:     2.599417, Tokens per Sec:    16378, Lr: 0.000300\n",
      "2021-08-01 22:33:40,288 - INFO - joeynmt.training - Epoch   4, Step:    19600, Batch Loss:     2.586183, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-01 22:33:53,978 - INFO - joeynmt.training - Epoch   4, Step:    19700, Batch Loss:     2.753377, Tokens per Sec:    16603, Lr: 0.000300\n",
      "2021-08-01 22:34:07,612 - INFO - joeynmt.training - Epoch   4, Step:    19800, Batch Loss:     2.413830, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-08-01 22:34:21,228 - INFO - joeynmt.training - Epoch   4, Step:    19900, Batch Loss:     2.370069, Tokens per Sec:    16416, Lr: 0.000300\n",
      "2021-08-01 22:34:34,731 - INFO - joeynmt.training - Epoch   4, Step:    20000, Batch Loss:     2.430991, Tokens per Sec:    16386, Lr: 0.000300\n",
      "2021-08-01 22:36:11,413 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 22:36:11,414 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 22:36:11,414 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 22:36:12,712 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 22:36:12,713 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 22:36:13,811 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 22:36:13,812 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-01 22:36:13,812 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-01 22:36:13,813 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been dedicated to God , that I am to me . ”\n",
      "2021-08-01 22:36:13,813 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 22:36:13,813 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-01 22:36:13,814 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-01 22:36:13,814 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are not forgiving Jehovah , and we are more riches of riches . ”\n",
      "2021-08-01 22:36:13,814 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 22:36:13,814 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-01 22:36:13,814 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-01 22:36:13,815 - INFO - joeynmt.training - \tHypothesis: Canaanus : A new prophecy in the book of Daniel is represented on God’s Kingdom .\n",
      "2021-08-01 22:36:13,815 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 22:36:13,815 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-01 22:36:13,815 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-01 22:36:13,816 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep on walking ” and bring it ?\n",
      "2021-08-01 22:36:13,816 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    20000: bleu:  17.50, loss: 264551.6250, ppl:   8.8913, duration: 99.0848s\n",
      "2021-08-01 22:36:27,575 - INFO - joeynmt.training - Epoch   4, Step:    20100, Batch Loss:     2.406010, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-01 22:36:40,919 - INFO - joeynmt.training - Epoch   4, Step:    20200, Batch Loss:     2.447913, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-01 22:36:54,388 - INFO - joeynmt.training - Epoch   4, Step:    20300, Batch Loss:     2.389050, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-01 22:37:07,982 - INFO - joeynmt.training - Epoch   4, Step:    20400, Batch Loss:     2.408906, Tokens per Sec:    16553, Lr: 0.000300\n",
      "2021-08-01 22:37:21,551 - INFO - joeynmt.training - Epoch   4, Step:    20500, Batch Loss:     2.335275, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-08-01 22:37:35,057 - INFO - joeynmt.training - Epoch   4, Step:    20600, Batch Loss:     2.286759, Tokens per Sec:    16307, Lr: 0.000300\n",
      "2021-08-01 22:37:48,389 - INFO - joeynmt.training - Epoch   4, Step:    20700, Batch Loss:     2.271833, Tokens per Sec:    16416, Lr: 0.000300\n",
      "2021-08-01 22:38:01,791 - INFO - joeynmt.training - Epoch   4, Step:    20800, Batch Loss:     2.511788, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-08-01 22:38:15,587 - INFO - joeynmt.training - Epoch   4, Step:    20900, Batch Loss:     2.264782, Tokens per Sec:    16659, Lr: 0.000300\n",
      "2021-08-01 22:38:29,167 - INFO - joeynmt.training - Epoch   4, Step:    21000, Batch Loss:     2.300204, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-01 22:38:42,641 - INFO - joeynmt.training - Epoch   4, Step:    21100, Batch Loss:     2.557329, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-01 22:38:56,154 - INFO - joeynmt.training - Epoch   4, Step:    21200, Batch Loss:     2.561777, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-01 22:39:09,873 - INFO - joeynmt.training - Epoch   4, Step:    21300, Batch Loss:     2.642517, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-01 22:39:10,731 - INFO - joeynmt.training - Epoch   4: total training loss 12989.21\n",
      "2021-08-01 22:39:10,731 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-01 22:39:23,725 - INFO - joeynmt.training - Epoch   5, Step:    21400, Batch Loss:     2.280286, Tokens per Sec:    15355, Lr: 0.000300\n",
      "2021-08-01 22:39:37,282 - INFO - joeynmt.training - Epoch   5, Step:    21500, Batch Loss:     2.205127, Tokens per Sec:    16367, Lr: 0.000300\n",
      "2021-08-01 22:39:50,613 - INFO - joeynmt.training - Epoch   5, Step:    21600, Batch Loss:     2.301046, Tokens per Sec:    16220, Lr: 0.000300\n",
      "2021-08-01 22:40:04,113 - INFO - joeynmt.training - Epoch   5, Step:    21700, Batch Loss:     2.684275, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-08-01 22:40:17,666 - INFO - joeynmt.training - Epoch   5, Step:    21800, Batch Loss:     2.372855, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-08-01 22:40:31,106 - INFO - joeynmt.training - Epoch   5, Step:    21900, Batch Loss:     2.383106, Tokens per Sec:    16439, Lr: 0.000300\n",
      "2021-08-01 22:40:44,608 - INFO - joeynmt.training - Epoch   5, Step:    22000, Batch Loss:     2.345531, Tokens per Sec:    16497, Lr: 0.000300\n",
      "2021-08-01 22:40:58,099 - INFO - joeynmt.training - Epoch   5, Step:    22100, Batch Loss:     2.299749, Tokens per Sec:    15875, Lr: 0.000300\n",
      "2021-08-01 22:41:11,745 - INFO - joeynmt.training - Epoch   5, Step:    22200, Batch Loss:     2.220321, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-08-01 22:41:25,297 - INFO - joeynmt.training - Epoch   5, Step:    22300, Batch Loss:     2.207944, Tokens per Sec:    16350, Lr: 0.000300\n",
      "2021-08-01 22:41:38,906 - INFO - joeynmt.training - Epoch   5, Step:    22400, Batch Loss:     2.126358, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-08-01 22:41:52,311 - INFO - joeynmt.training - Epoch   5, Step:    22500, Batch Loss:     2.312515, Tokens per Sec:    16275, Lr: 0.000300\n",
      "2021-08-01 22:42:05,923 - INFO - joeynmt.training - Epoch   5, Step:    22600, Batch Loss:     2.190255, Tokens per Sec:    16391, Lr: 0.000300\n",
      "2021-08-01 22:42:19,539 - INFO - joeynmt.training - Epoch   5, Step:    22700, Batch Loss:     2.372079, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-01 22:42:33,057 - INFO - joeynmt.training - Epoch   5, Step:    22800, Batch Loss:     2.241135, Tokens per Sec:    16239, Lr: 0.000300\n",
      "2021-08-01 22:42:46,606 - INFO - joeynmt.training - Epoch   5, Step:    22900, Batch Loss:     2.434860, Tokens per Sec:    16344, Lr: 0.000300\n",
      "2021-08-01 22:43:00,044 - INFO - joeynmt.training - Epoch   5, Step:    23000, Batch Loss:     2.079063, Tokens per Sec:    16410, Lr: 0.000300\n",
      "2021-08-01 22:43:13,786 - INFO - joeynmt.training - Epoch   5, Step:    23100, Batch Loss:     2.475864, Tokens per Sec:    16498, Lr: 0.000300\n",
      "2021-08-01 22:43:27,431 - INFO - joeynmt.training - Epoch   5, Step:    23200, Batch Loss:     2.189241, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-01 22:43:41,257 - INFO - joeynmt.training - Epoch   5, Step:    23300, Batch Loss:     2.392144, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-08-01 22:43:54,866 - INFO - joeynmt.training - Epoch   5, Step:    23400, Batch Loss:     2.355270, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-08-01 22:44:08,456 - INFO - joeynmt.training - Epoch   5, Step:    23500, Batch Loss:     2.178488, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-01 22:44:22,061 - INFO - joeynmt.training - Epoch   5, Step:    23600, Batch Loss:     2.314741, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-08-01 22:44:35,598 - INFO - joeynmt.training - Epoch   5, Step:    23700, Batch Loss:     2.092823, Tokens per Sec:    16485, Lr: 0.000300\n",
      "2021-08-01 22:44:49,061 - INFO - joeynmt.training - Epoch   5, Step:    23800, Batch Loss:     2.639614, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-01 22:45:02,811 - INFO - joeynmt.training - Epoch   5, Step:    23900, Batch Loss:     2.011050, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-01 22:45:16,429 - INFO - joeynmt.training - Epoch   5, Step:    24000, Batch Loss:     2.488443, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-01 22:45:29,973 - INFO - joeynmt.training - Epoch   5, Step:    24100, Batch Loss:     2.265309, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-01 22:45:43,452 - INFO - joeynmt.training - Epoch   5, Step:    24200, Batch Loss:     2.325690, Tokens per Sec:    16267, Lr: 0.000300\n",
      "2021-08-01 22:45:57,090 - INFO - joeynmt.training - Epoch   5, Step:    24300, Batch Loss:     2.338303, Tokens per Sec:    16458, Lr: 0.000300\n",
      "2021-08-01 22:46:10,526 - INFO - joeynmt.training - Epoch   5, Step:    24400, Batch Loss:     2.398674, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-08-01 22:46:24,415 - INFO - joeynmt.training - Epoch   5, Step:    24500, Batch Loss:     2.225679, Tokens per Sec:    16420, Lr: 0.000300\n",
      "2021-08-01 22:46:37,858 - INFO - joeynmt.training - Epoch   5, Step:    24600, Batch Loss:     2.506919, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-08-01 22:46:51,512 - INFO - joeynmt.training - Epoch   5, Step:    24700, Batch Loss:     2.365000, Tokens per Sec:    16397, Lr: 0.000300\n",
      "2021-08-01 22:47:04,928 - INFO - joeynmt.training - Epoch   5, Step:    24800, Batch Loss:     2.326770, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-08-01 22:47:18,540 - INFO - joeynmt.training - Epoch   5, Step:    24900, Batch Loss:     2.215932, Tokens per Sec:    15717, Lr: 0.000300\n",
      "2021-08-01 22:47:32,112 - INFO - joeynmt.training - Epoch   5, Step:    25000, Batch Loss:     2.148871, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-01 22:49:05,611 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-01 22:49:05,612 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-01 22:49:05,612 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-01 22:49:06,813 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-01 22:49:06,813 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-01 22:49:07,511 - INFO - joeynmt.training - Example #0\n",
      "2021-08-01 22:49:07,512 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-01 22:49:07,512 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-01 22:49:07,512 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am dedicated to God , that you are to be on me . ”\n",
      "2021-08-01 22:49:07,512 - INFO - joeynmt.training - Example #1\n",
      "2021-08-01 22:49:07,513 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-01 22:49:07,513 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-01 22:49:07,513 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , and we are more important than many riches . ”\n",
      "2021-08-01 22:49:07,513 - INFO - joeynmt.training - Example #2\n",
      "2021-08-01 22:49:07,514 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-01 22:49:07,514 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-01 22:49:07,514 - INFO - joeynmt.training - \tHypothesis: Canaesar : Did the other prophecy in Daniel’s book reach what it says about God’s Kingdom .\n",
      "2021-08-01 22:49:07,514 - INFO - joeynmt.training - Example #3\n",
      "2021-08-01 22:49:07,515 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-01 22:49:07,515 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-01 22:49:07,515 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep on walking ” and bring it into a complete sense ?\n",
      "2021-08-01 22:49:07,515 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    25000: bleu:  19.33, loss: 252179.8281, ppl:   8.0276, duration: 95.4025s\n",
      "2021-08-01 22:49:21,221 - INFO - joeynmt.training - Epoch   5, Step:    25100, Batch Loss:     2.314698, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-01 22:49:34,891 - INFO - joeynmt.training - Epoch   5, Step:    25200, Batch Loss:     2.077306, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-08-01 22:49:48,437 - INFO - joeynmt.training - Epoch   5, Step:    25300, Batch Loss:     2.570048, Tokens per Sec:    16336, Lr: 0.000300\n",
      "2021-08-01 22:50:02,016 - INFO - joeynmt.training - Epoch   5, Step:    25400, Batch Loss:     2.451044, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-08-01 22:50:15,595 - INFO - joeynmt.training - Epoch   5, Step:    25500, Batch Loss:     2.412084, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-08-01 22:50:29,206 - INFO - joeynmt.training - Epoch   5, Step:    25600, Batch Loss:     2.190020, Tokens per Sec:    16523, Lr: 0.000300\n",
      "2021-08-01 22:50:42,672 - INFO - joeynmt.training - Epoch   5, Step:    25700, Batch Loss:     2.404507, Tokens per Sec:    16275, Lr: 0.000300\n",
      "2021-08-01 22:50:56,254 - INFO - joeynmt.training - Epoch   5, Step:    25800, Batch Loss:     1.999606, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-08-01 22:51:09,838 - INFO - joeynmt.training - Epoch   5, Step:    25900, Batch Loss:     2.300480, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-08-01 22:51:23,491 - INFO - joeynmt.training - Epoch   5, Step:    26000, Batch Loss:     2.238810, Tokens per Sec:    16623, Lr: 0.000300\n",
      "2021-08-01 22:51:37,078 - INFO - joeynmt.training - Epoch   5, Step:    26100, Batch Loss:     2.267513, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-08-01 22:51:50,454 - INFO - joeynmt.training - Epoch   5, Step:    26200, Batch Loss:     2.094682, Tokens per Sec:    16322, Lr: 0.000300\n",
      "2021-08-01 22:52:04,091 - INFO - joeynmt.training - Epoch   5, Step:    26300, Batch Loss:     2.286361, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-08-01 22:52:17,682 - INFO - joeynmt.training - Epoch   5, Step:    26400, Batch Loss:     2.220484, Tokens per Sec:    16277, Lr: 0.000300\n",
      "2021-08-01 22:52:31,186 - INFO - joeynmt.training - Epoch   5, Step:    26500, Batch Loss:     2.224251, Tokens per Sec:    16348, Lr: 0.000300\n",
      "2021-08-01 22:52:44,580 - INFO - joeynmt.training - Epoch   5, Step:    26600, Batch Loss:     2.489566, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-08-01 22:52:48,913 - INFO - joeynmt.training - Epoch   5: total training loss 12336.88\n",
      "2021-08-01 22:52:48,913 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-01 22:52:58,602 - INFO - joeynmt.training - Epoch   6, Step:    26700, Batch Loss:     2.120547, Tokens per Sec:    15467, Lr: 0.000300\n",
      "2021-08-01 22:53:12,268 - INFO - joeynmt.training - Epoch   6, Step:    26800, Batch Loss:     2.392249, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-01 22:53:25,733 - INFO - joeynmt.training - Epoch   6, Step:    26900, Batch Loss:     2.151417, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-01 22:53:39,272 - INFO - joeynmt.training - Epoch   6, Step:    27000, Batch Loss:     2.061590, Tokens per Sec:    16653, Lr: 0.000300\n",
      "2021-08-01 22:53:52,805 - INFO - joeynmt.training - Epoch   6, Step:    27100, Batch Loss:     2.217478, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-08-01 22:54:06,442 - INFO - joeynmt.training - Epoch   6, Step:    27200, Batch Loss:     2.317481, Tokens per Sec:    16406, Lr: 0.000300\n",
      "2021-08-01 22:54:20,001 - INFO - joeynmt.training - Epoch   6, Step:    27300, Batch Loss:     2.014642, Tokens per Sec:    16279, Lr: 0.000300\n",
      "2021-08-01 22:54:33,354 - INFO - joeynmt.training - Epoch   6, Step:    27400, Batch Loss:     2.527611, Tokens per Sec:    15666, Lr: 0.000300\n",
      "2021-08-01 22:54:46,784 - INFO - joeynmt.training - Epoch   6, Step:    27500, Batch Loss:     2.240025, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-08-01 22:55:00,293 - INFO - joeynmt.training - Epoch   6, Step:    27600, Batch Loss:     2.249320, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-01 22:55:13,862 - INFO - joeynmt.training - Epoch   6, Step:    27700, Batch Loss:     2.261283, Tokens per Sec:    16195, Lr: 0.000300\n",
      "2021-08-01 22:55:27,231 - INFO - joeynmt.training - Epoch   6, Step:    27800, Batch Loss:     2.298543, Tokens per Sec:    16355, Lr: 0.000300\n",
      "2021-08-01 22:55:40,868 - INFO - joeynmt.training - Epoch   6, Step:    27900, Batch Loss:     2.318638, Tokens per Sec:    16261, Lr: 0.000300\n",
      "2021-08-01 22:55:54,448 - INFO - joeynmt.training - Epoch   6, Step:    28000, Batch Loss:     2.299308, Tokens per Sec:    16515, Lr: 0.000300\n",
      "2021-08-01 22:56:07,925 - INFO - joeynmt.training - Epoch   6, Step:    28100, Batch Loss:     2.167055, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-01 22:56:21,343 - INFO - joeynmt.training - Epoch   6, Step:    28200, Batch Loss:     2.097845, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-08-01 22:56:34,724 - INFO - joeynmt.training - Epoch   6, Step:    28300, Batch Loss:     2.289965, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-01 22:56:48,387 - INFO - joeynmt.training - Epoch   6, Step:    28400, Batch Loss:     2.560438, Tokens per Sec:    16661, Lr: 0.000300\n",
      "2021-08-01 22:57:02,021 - INFO - joeynmt.training - Epoch   6, Step:    28500, Batch Loss:     2.054876, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-08-01 22:57:15,593 - INFO - joeynmt.training - Epoch   6, Step:    28600, Batch Loss:     2.067906, Tokens per Sec:    15837, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt2$src.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMgv7Y6_l9tU"
   },
   "source": [
    "5.5 epochs done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vG93x5HI-oA-",
    "outputId": "0bbbc8dc-66ac-4027-dcec-bfb8fa856cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5000\tLoss: 369605.15625\tPPL: 21.17395\tbleu: 6.42552\tLR: 0.00030000\t*\n",
      "Steps: 10000\tLoss: 315656.06250\tPPL: 13.56071\tbleu: 11.47374\tLR: 0.00030000\t*\n",
      "Steps: 15000\tLoss: 283178.84375\tPPL: 10.37013\tbleu: 15.52942\tLR: 0.00030000\t*\n",
      "Steps: 20000\tLoss: 264551.62500\tPPL: 8.89133\tbleu: 17.49981\tLR: 0.00030000\t*\n",
      "Steps: 25000\tLoss: 252179.82812\tPPL: 8.02765\tbleu: 19.33033\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 243752.06250\tPPL: 7.48785\tbleu: 20.43871\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/rwen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "5cWYmgZR2CEi"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 30000\n",
    "\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/models/rwen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rwen_reverse_transformer\"', f'model_dir: \"models/rwen_reverse_transformer_continued\"').replace(\n",
    "            f'epochs: 30', f'epochs: 25')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1ByOVePaozar",
    "outputId": "a04f3751-a55d-45bd-a08f-0400409072dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rwen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/30000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 2000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 25                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rwen_reverse_transformer_continued\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_{name}_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rxwmJOp5dsc",
    "outputId": "bf3fbb2d-0d87-49bd-c964-b446110d2858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-02 06:56:51,535 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-02 06:56:51,568 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-02 06:56:59,560 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-02 06:57:00,114 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-02 06:57:00,758 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-02 06:57:01,307 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-02 06:57:01,308 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-02 06:57:01,542 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-02 06:57:01.787151: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-02 06:57:04,913 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-08-02 06:57:08,725 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/30000.ckpt\n",
      "2021-08-02 06:57:09,173 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-08-02 06:57:09,174 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-08-02 06:57:09,174 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-02 06:57:09,174 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-08-02 06:57:09,174 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-08-02 06:57:09,175 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-08-02 06:57:09,176 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-02 06:57:09,178 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-02 06:57:09,178 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-02 06:57:09,178 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-08-02 06:57:09,178 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-08-02 06:57:09,179 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-02 06:57:09,179 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-02 06:57:09,179 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/30000.ckpt\n",
      "2021-08-02 06:57:09,179 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-02 06:57:09,179 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-02 06:57:09,179 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-02 06:57:09,180 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-02 06:57:09,180 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-02 06:57:09,180 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-02 06:57:09,180 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-02 06:57:09,180 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-02 06:57:09,181 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-02 06:57:09,181 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-02 06:57:09,181 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-02 06:57:09,181 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-02 06:57:09,181 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-02 06:57:09,181 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-02 06:57:09,182 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-02 06:57:09,182 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-02 06:57:09,182 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-02 06:57:09,182 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-02 06:57:09,182 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-02 06:57:09,182 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-02 06:57:09,182 - INFO - joeynmt.helpers - cfg.training.epochs                : 25\n",
      "2021-08-02 06:57:09,183 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-02 06:57:09,183 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-08-02 06:57:09,183 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-02 06:57:09,183 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer_continued\n",
      "2021-08-02 06:57:09,183 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-02 06:57:09,183 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-02 06:57:09,183 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-02 06:57:09,184 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-02 06:57:09,184 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-02 06:57:09,184 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-02 06:57:09,184 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-02 06:57:09,184 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-02 06:57:09,184 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-02 06:57:09,185 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-02 06:57:09,185 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-02 06:57:09,185 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-02 06:57:09,185 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-02 06:57:09,185 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-02 06:57:09,185 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-02 06:57:09,185 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-02 06:57:09,186 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-02 06:57:09,186 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-02 06:57:09,186 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-02 06:57:09,186 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-02 06:57:09,186 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-02 06:57:09,186 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-02 06:57:09,187 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-02 06:57:09,187 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-02 06:57:09,187 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-02 06:57:09,187 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-02 06:57:09,187 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-02 06:57:09,187 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-02 06:57:09,187 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-02 06:57:09,188 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-02 06:57:09,188 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-02 06:57:09,188 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 426806,\n",
      "\tvalid 4368,\n",
      "\ttest 4368\n",
      "2021-08-02 06:57:09,188 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-02 06:57:09,188 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-02 06:57:09,189 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-02 06:57:09,189 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-08-02 06:57:09,189 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-08-02 06:57:09,189 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-08-02 06:57:09,224 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-02 06:57:09,225 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-02 06:57:23,790 - INFO - joeynmt.training - Epoch   1, Step:    30100, Batch Loss:     2.150227, Tokens per Sec:    15038, Lr: 0.000300\n",
      "2021-08-02 06:57:36,656 - INFO - joeynmt.training - Epoch   1, Step:    30200, Batch Loss:     2.177351, Tokens per Sec:    17228, Lr: 0.000300\n",
      "2021-08-02 06:57:49,540 - INFO - joeynmt.training - Epoch   1, Step:    30300, Batch Loss:     2.216425, Tokens per Sec:    16752, Lr: 0.000300\n",
      "2021-08-02 06:58:02,551 - INFO - joeynmt.training - Epoch   1, Step:    30400, Batch Loss:     2.514416, Tokens per Sec:    16820, Lr: 0.000300\n",
      "2021-08-02 06:58:15,809 - INFO - joeynmt.training - Epoch   1, Step:    30500, Batch Loss:     2.186040, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-08-02 06:58:29,370 - INFO - joeynmt.training - Epoch   1, Step:    30600, Batch Loss:     2.108211, Tokens per Sec:    16725, Lr: 0.000300\n",
      "2021-08-02 06:58:42,880 - INFO - joeynmt.training - Epoch   1, Step:    30700, Batch Loss:     2.276277, Tokens per Sec:    16290, Lr: 0.000300\n",
      "2021-08-02 06:58:56,251 - INFO - joeynmt.training - Epoch   1, Step:    30800, Batch Loss:     2.009297, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-08-02 06:59:10,015 - INFO - joeynmt.training - Epoch   1, Step:    30900, Batch Loss:     2.097226, Tokens per Sec:    16216, Lr: 0.000300\n",
      "2021-08-02 06:59:24,200 - INFO - joeynmt.training - Epoch   1, Step:    31000, Batch Loss:     2.253149, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-08-02 06:59:38,432 - INFO - joeynmt.training - Epoch   1, Step:    31100, Batch Loss:     2.203321, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-08-02 06:59:52,533 - INFO - joeynmt.training - Epoch   1, Step:    31200, Batch Loss:     2.249092, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-08-02 07:00:06,494 - INFO - joeynmt.training - Epoch   1, Step:    31300, Batch Loss:     2.421996, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-08-02 07:00:20,075 - INFO - joeynmt.training - Epoch   1, Step:    31400, Batch Loss:     2.093547, Tokens per Sec:    15525, Lr: 0.000300\n",
      "2021-08-02 07:00:33,858 - INFO - joeynmt.training - Epoch   1, Step:    31500, Batch Loss:     1.987619, Tokens per Sec:    16401, Lr: 0.000300\n",
      "2021-08-02 07:00:47,494 - INFO - joeynmt.training - Epoch   1, Step:    31600, Batch Loss:     2.178073, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-08-02 07:01:01,536 - INFO - joeynmt.training - Epoch   1, Step:    31700, Batch Loss:     2.176339, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-02 07:01:15,523 - INFO - joeynmt.training - Epoch   1, Step:    31800, Batch Loss:     2.177482, Tokens per Sec:    15511, Lr: 0.000300\n",
      "2021-08-02 07:01:29,537 - INFO - joeynmt.training - Epoch   1, Step:    31900, Batch Loss:     2.198406, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-08-02 07:01:37,157 - INFO - joeynmt.training - Epoch   1: total training loss 4329.78\n",
      "2021-08-02 07:01:37,157 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-02 07:01:43,895 - INFO - joeynmt.training - Epoch   2, Step:    32000, Batch Loss:     2.184377, Tokens per Sec:    14310, Lr: 0.000300\n",
      "2021-08-02 07:01:57,768 - INFO - joeynmt.training - Epoch   2, Step:    32100, Batch Loss:     2.211880, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-02 07:02:11,855 - INFO - joeynmt.training - Epoch   2, Step:    32200, Batch Loss:     2.110691, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-08-02 07:02:25,692 - INFO - joeynmt.training - Epoch   2, Step:    32300, Batch Loss:     1.843911, Tokens per Sec:    15524, Lr: 0.000300\n",
      "2021-08-02 07:02:39,410 - INFO - joeynmt.training - Epoch   2, Step:    32400, Batch Loss:     2.082926, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-08-02 07:02:53,214 - INFO - joeynmt.training - Epoch   2, Step:    32500, Batch Loss:     2.144460, Tokens per Sec:    16133, Lr: 0.000300\n",
      "2021-08-02 07:03:07,065 - INFO - joeynmt.training - Epoch   2, Step:    32600, Batch Loss:     2.273538, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-08-02 07:03:21,048 - INFO - joeynmt.training - Epoch   2, Step:    32700, Batch Loss:     2.353073, Tokens per Sec:    15712, Lr: 0.000300\n",
      "2021-08-02 07:03:34,902 - INFO - joeynmt.training - Epoch   2, Step:    32800, Batch Loss:     2.082502, Tokens per Sec:    15639, Lr: 0.000300\n",
      "2021-08-02 07:03:48,717 - INFO - joeynmt.training - Epoch   2, Step:    32900, Batch Loss:     2.181321, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-02 07:04:02,626 - INFO - joeynmt.training - Epoch   2, Step:    33000, Batch Loss:     2.086925, Tokens per Sec:    16214, Lr: 0.000300\n",
      "2021-08-02 07:04:16,391 - INFO - joeynmt.training - Epoch   2, Step:    33100, Batch Loss:     2.259282, Tokens per Sec:    15588, Lr: 0.000300\n",
      "2021-08-02 07:04:30,356 - INFO - joeynmt.training - Epoch   2, Step:    33200, Batch Loss:     1.877378, Tokens per Sec:    15617, Lr: 0.000300\n",
      "2021-08-02 07:04:44,135 - INFO - joeynmt.training - Epoch   2, Step:    33300, Batch Loss:     2.223803, Tokens per Sec:    15695, Lr: 0.000300\n",
      "2021-08-02 07:04:57,982 - INFO - joeynmt.training - Epoch   2, Step:    33400, Batch Loss:     2.088736, Tokens per Sec:    15568, Lr: 0.000300\n",
      "2021-08-02 07:05:11,850 - INFO - joeynmt.training - Epoch   2, Step:    33500, Batch Loss:     2.189296, Tokens per Sec:    15919, Lr: 0.000300\n",
      "2021-08-02 07:05:25,626 - INFO - joeynmt.training - Epoch   2, Step:    33600, Batch Loss:     2.028005, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-08-02 07:05:39,454 - INFO - joeynmt.training - Epoch   2, Step:    33700, Batch Loss:     2.283878, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-08-02 07:05:53,075 - INFO - joeynmt.training - Epoch   2, Step:    33800, Batch Loss:     2.136696, Tokens per Sec:    15489, Lr: 0.000300\n",
      "2021-08-02 07:06:06,895 - INFO - joeynmt.training - Epoch   2, Step:    33900, Batch Loss:     2.188319, Tokens per Sec:    15661, Lr: 0.000300\n",
      "2021-08-02 07:06:20,703 - INFO - joeynmt.training - Epoch   2, Step:    34000, Batch Loss:     2.454590, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-08-02 07:06:34,436 - INFO - joeynmt.training - Epoch   2, Step:    34100, Batch Loss:     2.337651, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-08-02 07:06:48,223 - INFO - joeynmt.training - Epoch   2, Step:    34200, Batch Loss:     2.471221, Tokens per Sec:    16109, Lr: 0.000300\n",
      "2021-08-02 07:07:02,096 - INFO - joeynmt.training - Epoch   2, Step:    34300, Batch Loss:     2.488354, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-08-02 07:07:16,009 - INFO - joeynmt.training - Epoch   2, Step:    34400, Batch Loss:     2.206576, Tokens per Sec:    15557, Lr: 0.000300\n",
      "2021-08-02 07:07:29,856 - INFO - joeynmt.training - Epoch   2, Step:    34500, Batch Loss:     1.951650, Tokens per Sec:    15805, Lr: 0.000300\n",
      "2021-08-02 07:07:43,654 - INFO - joeynmt.training - Epoch   2, Step:    34600, Batch Loss:     2.195210, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-08-02 07:07:57,377 - INFO - joeynmt.training - Epoch   2, Step:    34700, Batch Loss:     2.157714, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-08-02 07:08:11,057 - INFO - joeynmt.training - Epoch   2, Step:    34800, Batch Loss:     2.167664, Tokens per Sec:    15341, Lr: 0.000300\n",
      "2021-08-02 07:08:24,999 - INFO - joeynmt.training - Epoch   2, Step:    34900, Batch Loss:     2.080340, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-08-02 07:08:38,785 - INFO - joeynmt.training - Epoch   2, Step:    35000, Batch Loss:     2.130337, Tokens per Sec:    15727, Lr: 0.000300\n",
      "2021-08-02 07:10:09,183 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 07:10:09,184 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 07:10:09,184 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 07:10:10,419 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 07:10:10,419 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 07:10:11,188 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 07:10:11,188 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 07:10:11,188 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 07:10:11,189 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been dedicated to God , that it is to be with me . ”\n",
      "2021-08-02 07:10:11,189 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 07:10:11,189 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 07:10:11,189 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 07:10:11,190 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel says : “ Let us be saved for Jehovah , we are more than many riches . ”\n",
      "2021-08-02 07:10:11,190 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 07:10:11,190 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 07:10:11,191 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 07:10:11,191 - INFO - joeynmt.training - \tHypothesis: Canaesar : A other prophecy in the book of Daniel is represented on God’s Kingdom .\n",
      "2021-08-02 07:10:11,191 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 07:10:11,191 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 07:10:11,191 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 07:10:11,192 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on conquering ” and bring it into a complete way ?\n",
      "2021-08-02 07:10:11,192 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    35000: bleu:  21.08, loss: 236982.7500, ppl:   7.0807, duration: 92.4065s\n",
      "2021-08-02 07:10:25,539 - INFO - joeynmt.training - Epoch   2, Step:    35100, Batch Loss:     2.073989, Tokens per Sec:    15549, Lr: 0.000300\n",
      "2021-08-02 07:10:39,178 - INFO - joeynmt.training - Epoch   2, Step:    35200, Batch Loss:     2.147533, Tokens per Sec:    15487, Lr: 0.000300\n",
      "2021-08-02 07:10:53,053 - INFO - joeynmt.training - Epoch   2, Step:    35300, Batch Loss:     2.019624, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-08-02 07:11:07,099 - INFO - joeynmt.training - Epoch   2, Step:    35400, Batch Loss:     2.189843, Tokens per Sec:    15563, Lr: 0.000300\n",
      "2021-08-02 07:11:20,844 - INFO - joeynmt.training - Epoch   2, Step:    35500, Batch Loss:     2.162641, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-08-02 07:11:34,683 - INFO - joeynmt.training - Epoch   2, Step:    35600, Batch Loss:     2.262287, Tokens per Sec:    16070, Lr: 0.000300\n",
      "2021-08-02 07:11:48,344 - INFO - joeynmt.training - Epoch   2, Step:    35700, Batch Loss:     2.202951, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-02 07:12:02,125 - INFO - joeynmt.training - Epoch   2, Step:    35800, Batch Loss:     2.223569, Tokens per Sec:    15536, Lr: 0.000300\n",
      "2021-08-02 07:12:16,028 - INFO - joeynmt.training - Epoch   2, Step:    35900, Batch Loss:     2.182081, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-08-02 07:12:29,765 - INFO - joeynmt.training - Epoch   2, Step:    36000, Batch Loss:     1.959648, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-08-02 07:12:43,520 - INFO - joeynmt.training - Epoch   2, Step:    36100, Batch Loss:     2.224959, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-02 07:12:57,171 - INFO - joeynmt.training - Epoch   2, Step:    36200, Batch Loss:     2.264861, Tokens per Sec:    15712, Lr: 0.000300\n",
      "2021-08-02 07:13:11,224 - INFO - joeynmt.training - Epoch   2, Step:    36300, Batch Loss:     1.989030, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-02 07:13:25,204 - INFO - joeynmt.training - Epoch   2, Step:    36400, Batch Loss:     2.133776, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-08-02 07:13:39,099 - INFO - joeynmt.training - Epoch   2, Step:    36500, Batch Loss:     2.175123, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-02 07:13:52,682 - INFO - joeynmt.training - Epoch   2, Step:    36600, Batch Loss:     2.050230, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-08-02 07:14:06,482 - INFO - joeynmt.training - Epoch   2, Step:    36700, Batch Loss:     2.088836, Tokens per Sec:    15576, Lr: 0.000300\n",
      "2021-08-02 07:14:20,184 - INFO - joeynmt.training - Epoch   2, Step:    36800, Batch Loss:     2.265575, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-08-02 07:14:34,171 - INFO - joeynmt.training - Epoch   2, Step:    36900, Batch Loss:     2.162426, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-08-02 07:14:47,926 - INFO - joeynmt.training - Epoch   2, Step:    37000, Batch Loss:     2.202134, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-02 07:15:01,857 - INFO - joeynmt.training - Epoch   2, Step:    37100, Batch Loss:     2.213283, Tokens per Sec:    16155, Lr: 0.000300\n",
      "2021-08-02 07:15:15,687 - INFO - joeynmt.training - Epoch   2, Step:    37200, Batch Loss:     2.085077, Tokens per Sec:    16108, Lr: 0.000300\n",
      "2021-08-02 07:15:29,672 - INFO - joeynmt.training - Epoch   2, Step:    37300, Batch Loss:     2.253771, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-08-02 07:15:29,845 - INFO - joeynmt.training - Epoch   2: total training loss 11577.81\n",
      "2021-08-02 07:15:29,845 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-02 07:15:44,263 - INFO - joeynmt.training - Epoch   3, Step:    37400, Batch Loss:     2.084147, Tokens per Sec:    15493, Lr: 0.000300\n",
      "2021-08-02 07:15:58,288 - INFO - joeynmt.training - Epoch   3, Step:    37500, Batch Loss:     2.051831, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-08-02 07:16:12,264 - INFO - joeynmt.training - Epoch   3, Step:    37600, Batch Loss:     2.069924, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-02 07:16:26,114 - INFO - joeynmt.training - Epoch   3, Step:    37700, Batch Loss:     2.157219, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-08-02 07:16:39,688 - INFO - joeynmt.training - Epoch   3, Step:    37800, Batch Loss:     2.010429, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-02 07:16:53,517 - INFO - joeynmt.training - Epoch   3, Step:    37900, Batch Loss:     2.184516, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-02 07:17:07,470 - INFO - joeynmt.training - Epoch   3, Step:    38000, Batch Loss:     2.125705, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-02 07:17:20,952 - INFO - joeynmt.training - Epoch   3, Step:    38100, Batch Loss:     2.374000, Tokens per Sec:    15591, Lr: 0.000300\n",
      "2021-08-02 07:17:34,743 - INFO - joeynmt.training - Epoch   3, Step:    38200, Batch Loss:     2.579994, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-02 07:17:48,322 - INFO - joeynmt.training - Epoch   3, Step:    38300, Batch Loss:     1.834435, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-02 07:18:02,225 - INFO - joeynmt.training - Epoch   3, Step:    38400, Batch Loss:     2.125169, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-08-02 07:18:16,026 - INFO - joeynmt.training - Epoch   3, Step:    38500, Batch Loss:     2.024383, Tokens per Sec:    15951, Lr: 0.000300\n",
      "2021-08-02 07:18:29,739 - INFO - joeynmt.training - Epoch   3, Step:    38600, Batch Loss:     2.123300, Tokens per Sec:    15907, Lr: 0.000300\n",
      "2021-08-02 07:18:43,585 - INFO - joeynmt.training - Epoch   3, Step:    38700, Batch Loss:     2.132586, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-02 07:18:57,283 - INFO - joeynmt.training - Epoch   3, Step:    38800, Batch Loss:     2.192229, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-08-02 07:19:11,073 - INFO - joeynmt.training - Epoch   3, Step:    38900, Batch Loss:     2.204057, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-08-02 07:19:25,062 - INFO - joeynmt.training - Epoch   3, Step:    39000, Batch Loss:     2.118951, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-02 07:19:38,920 - INFO - joeynmt.training - Epoch   3, Step:    39100, Batch Loss:     2.111310, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-02 07:19:52,727 - INFO - joeynmt.training - Epoch   3, Step:    39200, Batch Loss:     2.205980, Tokens per Sec:    15958, Lr: 0.000300\n",
      "2021-08-02 07:20:06,423 - INFO - joeynmt.training - Epoch   3, Step:    39300, Batch Loss:     2.021249, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-02 07:20:20,359 - INFO - joeynmt.training - Epoch   3, Step:    39400, Batch Loss:     2.194276, Tokens per Sec:    15795, Lr: 0.000300\n",
      "2021-08-02 07:20:34,164 - INFO - joeynmt.training - Epoch   3, Step:    39500, Batch Loss:     1.993277, Tokens per Sec:    15704, Lr: 0.000300\n",
      "2021-08-02 07:20:47,912 - INFO - joeynmt.training - Epoch   3, Step:    39600, Batch Loss:     2.180529, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-02 07:21:01,789 - INFO - joeynmt.training - Epoch   3, Step:    39700, Batch Loss:     1.951113, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-02 07:21:15,541 - INFO - joeynmt.training - Epoch   3, Step:    39800, Batch Loss:     2.299165, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-08-02 07:21:29,446 - INFO - joeynmt.training - Epoch   3, Step:    39900, Batch Loss:     2.372559, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-08-02 07:21:43,116 - INFO - joeynmt.training - Epoch   3, Step:    40000, Batch Loss:     2.001075, Tokens per Sec:    15712, Lr: 0.000300\n",
      "2021-08-02 07:23:17,146 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 07:23:17,147 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 07:23:17,147 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 07:23:18,400 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 07:23:18,400 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 07:23:19,169 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 07:23:19,171 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 07:23:19,171 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 07:23:19,171 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , that he is to be able to me . ”\n",
      "2021-08-02 07:23:19,172 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 07:23:19,172 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 07:23:19,172 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 07:23:19,172 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Remember , we are giving Jehovah , the more riches are still alive . ”\n",
      "2021-08-02 07:23:19,173 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 07:23:19,173 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 07:23:19,173 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 07:23:19,173 - INFO - joeynmt.training - \tHypothesis: Canadiastes : Look ! The prophecy of Daniel is replaced on God’s Kingdom .\n",
      "2021-08-02 07:23:19,174 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 07:23:19,174 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 07:23:19,174 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 07:23:19,174 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on conquering ” and bring lasting lasting conquences ?\n",
      "2021-08-02 07:23:19,176 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    40000: bleu:  21.92, loss: 231373.4219, ppl:   6.7601, duration: 96.0593s\n",
      "2021-08-02 07:23:33,190 - INFO - joeynmt.training - Epoch   3, Step:    40100, Batch Loss:     1.971839, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-08-02 07:23:47,156 - INFO - joeynmt.training - Epoch   3, Step:    40200, Batch Loss:     2.178880, Tokens per Sec:    16181, Lr: 0.000300\n",
      "2021-08-02 07:24:01,018 - INFO - joeynmt.training - Epoch   3, Step:    40300, Batch Loss:     2.212201, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-02 07:24:14,852 - INFO - joeynmt.training - Epoch   3, Step:    40400, Batch Loss:     2.154576, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-08-02 07:24:28,740 - INFO - joeynmt.training - Epoch   3, Step:    40500, Batch Loss:     1.987540, Tokens per Sec:    16149, Lr: 0.000300\n",
      "2021-08-02 07:24:42,573 - INFO - joeynmt.training - Epoch   3, Step:    40600, Batch Loss:     2.041609, Tokens per Sec:    15732, Lr: 0.000300\n",
      "2021-08-02 07:24:56,287 - INFO - joeynmt.training - Epoch   3, Step:    40700, Batch Loss:     2.156737, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-08-02 07:25:09,949 - INFO - joeynmt.training - Epoch   3, Step:    40800, Batch Loss:     1.965121, Tokens per Sec:    15611, Lr: 0.000300\n",
      "2021-08-02 07:25:23,672 - INFO - joeynmt.training - Epoch   3, Step:    40900, Batch Loss:     2.359153, Tokens per Sec:    15738, Lr: 0.000300\n",
      "2021-08-02 07:25:37,454 - INFO - joeynmt.training - Epoch   3, Step:    41000, Batch Loss:     1.885151, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-08-02 07:25:51,211 - INFO - joeynmt.training - Epoch   3, Step:    41100, Batch Loss:     2.152901, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-08-02 07:26:05,003 - INFO - joeynmt.training - Epoch   3, Step:    41200, Batch Loss:     2.378101, Tokens per Sec:    16056, Lr: 0.000300\n",
      "2021-08-02 07:26:18,716 - INFO - joeynmt.training - Epoch   3, Step:    41300, Batch Loss:     2.072200, Tokens per Sec:    15717, Lr: 0.000300\n",
      "2021-08-02 07:26:32,679 - INFO - joeynmt.training - Epoch   3, Step:    41400, Batch Loss:     1.976533, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-08-02 07:26:46,504 - INFO - joeynmt.training - Epoch   3, Step:    41500, Batch Loss:     2.153373, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-02 07:27:00,326 - INFO - joeynmt.training - Epoch   3, Step:    41600, Batch Loss:     2.082606, Tokens per Sec:    15734, Lr: 0.000300\n",
      "2021-08-02 07:27:14,165 - INFO - joeynmt.training - Epoch   3, Step:    41700, Batch Loss:     2.083826, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-08-02 07:27:27,763 - INFO - joeynmt.training - Epoch   3, Step:    41800, Batch Loss:     2.052801, Tokens per Sec:    15782, Lr: 0.000300\n",
      "2021-08-02 07:27:41,616 - INFO - joeynmt.training - Epoch   3, Step:    41900, Batch Loss:     1.927931, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-02 07:27:55,390 - INFO - joeynmt.training - Epoch   3, Step:    42000, Batch Loss:     2.034056, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-08-02 07:28:09,441 - INFO - joeynmt.training - Epoch   3, Step:    42100, Batch Loss:     1.864072, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-02 07:28:23,174 - INFO - joeynmt.training - Epoch   3, Step:    42200, Batch Loss:     2.185555, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-02 07:28:36,801 - INFO - joeynmt.training - Epoch   3, Step:    42300, Batch Loss:     2.056420, Tokens per Sec:    15780, Lr: 0.000300\n",
      "2021-08-02 07:28:50,661 - INFO - joeynmt.training - Epoch   3, Step:    42400, Batch Loss:     2.068436, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-08-02 07:29:04,367 - INFO - joeynmt.training - Epoch   3, Step:    42500, Batch Loss:     2.022650, Tokens per Sec:    15374, Lr: 0.000300\n",
      "2021-08-02 07:29:18,368 - INFO - joeynmt.training - Epoch   3, Step:    42600, Batch Loss:     2.098187, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-08-02 07:29:22,944 - INFO - joeynmt.training - Epoch   3: total training loss 11269.00\n",
      "2021-08-02 07:29:22,945 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-02 07:29:32,973 - INFO - joeynmt.training - Epoch   4, Step:    42700, Batch Loss:     2.118691, Tokens per Sec:    14909, Lr: 0.000300\n",
      "2021-08-02 07:29:46,687 - INFO - joeynmt.training - Epoch   4, Step:    42800, Batch Loss:     2.235322, Tokens per Sec:    16261, Lr: 0.000300\n",
      "2021-08-02 07:30:00,488 - INFO - joeynmt.training - Epoch   4, Step:    42900, Batch Loss:     2.044833, Tokens per Sec:    16188, Lr: 0.000300\n",
      "2021-08-02 07:30:14,715 - INFO - joeynmt.training - Epoch   4, Step:    43000, Batch Loss:     1.999741, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-02 07:30:28,780 - INFO - joeynmt.training - Epoch   4, Step:    43100, Batch Loss:     1.865153, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-02 07:30:42,336 - INFO - joeynmt.training - Epoch   4, Step:    43200, Batch Loss:     2.134005, Tokens per Sec:    15631, Lr: 0.000300\n",
      "2021-08-02 07:30:56,300 - INFO - joeynmt.training - Epoch   4, Step:    43300, Batch Loss:     1.919498, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-02 07:31:10,142 - INFO - joeynmt.training - Epoch   4, Step:    43400, Batch Loss:     2.162209, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-02 07:31:23,952 - INFO - joeynmt.training - Epoch   4, Step:    43500, Batch Loss:     2.164979, Tokens per Sec:    15742, Lr: 0.000300\n",
      "2021-08-02 07:31:37,943 - INFO - joeynmt.training - Epoch   4, Step:    43600, Batch Loss:     2.053454, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-08-02 07:31:51,539 - INFO - joeynmt.training - Epoch   4, Step:    43700, Batch Loss:     2.036796, Tokens per Sec:    15439, Lr: 0.000300\n",
      "2021-08-02 07:32:05,187 - INFO - joeynmt.training - Epoch   4, Step:    43800, Batch Loss:     1.827861, Tokens per Sec:    15899, Lr: 0.000300\n",
      "2021-08-02 07:32:18,996 - INFO - joeynmt.training - Epoch   4, Step:    43900, Batch Loss:     2.234420, Tokens per Sec:    15742, Lr: 0.000300\n",
      "2021-08-02 07:32:32,661 - INFO - joeynmt.training - Epoch   4, Step:    44000, Batch Loss:     2.157421, Tokens per Sec:    15405, Lr: 0.000300\n",
      "2021-08-02 07:32:46,527 - INFO - joeynmt.training - Epoch   4, Step:    44100, Batch Loss:     2.015147, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-02 07:33:00,533 - INFO - joeynmt.training - Epoch   4, Step:    44200, Batch Loss:     2.035499, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-08-02 07:33:14,306 - INFO - joeynmt.training - Epoch   4, Step:    44300, Batch Loss:     2.118388, Tokens per Sec:    15471, Lr: 0.000300\n",
      "2021-08-02 07:33:28,305 - INFO - joeynmt.training - Epoch   4, Step:    44400, Batch Loss:     1.967578, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-08-02 07:33:41,971 - INFO - joeynmt.training - Epoch   4, Step:    44500, Batch Loss:     1.892123, Tokens per Sec:    15705, Lr: 0.000300\n",
      "2021-08-02 07:33:55,698 - INFO - joeynmt.training - Epoch   4, Step:    44600, Batch Loss:     1.959728, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-08-02 07:34:09,844 - INFO - joeynmt.training - Epoch   4, Step:    44700, Batch Loss:     2.253614, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-02 07:34:23,518 - INFO - joeynmt.training - Epoch   4, Step:    44800, Batch Loss:     2.051479, Tokens per Sec:    15649, Lr: 0.000300\n",
      "2021-08-02 07:34:37,269 - INFO - joeynmt.training - Epoch   4, Step:    44900, Batch Loss:     1.870294, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-02 07:34:50,946 - INFO - joeynmt.training - Epoch   4, Step:    45000, Batch Loss:     2.234180, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-08-02 07:36:24,029 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 07:36:24,030 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 07:36:24,030 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 07:36:25,375 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 07:36:25,376 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 07:36:26,192 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 07:36:26,193 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 07:36:26,193 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 07:36:26,193 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , that is good for me . ”\n",
      "2021-08-02 07:36:26,193 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 07:36:26,194 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 07:36:26,194 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 07:36:26,194 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The rest of Jehovah is greater than many riches that are standing . ”\n",
      "2021-08-02 07:36:26,194 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 07:36:26,195 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 07:36:26,195 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 07:36:26,195 - INFO - joeynmt.training - \tHypothesis: Canaius : Look ! The prophecy of Daniel is a remarkable report about God’s Kingdom .\n",
      "2021-08-02 07:36:26,195 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 07:36:26,196 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 07:36:26,196 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 07:36:26,196 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep on conquering ” and bring it completely ?\n",
      "2021-08-02 07:36:26,196 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    45000: bleu:  22.44, loss: 226930.5312, ppl:   6.5165, duration: 95.2497s\n",
      "2021-08-02 07:36:40,151 - INFO - joeynmt.training - Epoch   4, Step:    45100, Batch Loss:     1.883641, Tokens per Sec:    16122, Lr: 0.000300\n",
      "2021-08-02 07:36:54,034 - INFO - joeynmt.training - Epoch   4, Step:    45200, Batch Loss:     2.160032, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-02 07:37:07,906 - INFO - joeynmt.training - Epoch   4, Step:    45300, Batch Loss:     2.118015, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-02 07:37:21,747 - INFO - joeynmt.training - Epoch   4, Step:    45400, Batch Loss:     2.115315, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-08-02 07:37:35,473 - INFO - joeynmt.training - Epoch   4, Step:    45500, Batch Loss:     2.038537, Tokens per Sec:    15434, Lr: 0.000300\n",
      "2021-08-02 07:37:49,547 - INFO - joeynmt.training - Epoch   4, Step:    45600, Batch Loss:     1.958870, Tokens per Sec:    16179, Lr: 0.000300\n",
      "2021-08-02 07:38:03,321 - INFO - joeynmt.training - Epoch   4, Step:    45700, Batch Loss:     1.969300, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-02 07:38:17,227 - INFO - joeynmt.training - Epoch   4, Step:    45800, Batch Loss:     1.843709, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-02 07:38:30,978 - INFO - joeynmt.training - Epoch   4, Step:    45900, Batch Loss:     2.048093, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-08-02 07:38:44,808 - INFO - joeynmt.training - Epoch   4, Step:    46000, Batch Loss:     2.257385, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-02 07:38:58,673 - INFO - joeynmt.training - Epoch   4, Step:    46100, Batch Loss:     2.127251, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-02 07:39:12,515 - INFO - joeynmt.training - Epoch   4, Step:    46200, Batch Loss:     2.042510, Tokens per Sec:    15728, Lr: 0.000300\n",
      "2021-08-02 07:39:26,334 - INFO - joeynmt.training - Epoch   4, Step:    46300, Batch Loss:     2.122251, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-08-02 07:39:40,024 - INFO - joeynmt.training - Epoch   4, Step:    46400, Batch Loss:     2.156102, Tokens per Sec:    15838, Lr: 0.000300\n",
      "2021-08-02 07:39:53,815 - INFO - joeynmt.training - Epoch   4, Step:    46500, Batch Loss:     2.053857, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-02 07:40:07,633 - INFO - joeynmt.training - Epoch   4, Step:    46600, Batch Loss:     2.049626, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-02 07:40:21,459 - INFO - joeynmt.training - Epoch   4, Step:    46700, Batch Loss:     2.046854, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-08-02 07:40:35,201 - INFO - joeynmt.training - Epoch   4, Step:    46800, Batch Loss:     2.127419, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-02 07:40:49,009 - INFO - joeynmt.training - Epoch   4, Step:    46900, Batch Loss:     2.198154, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-02 07:41:02,788 - INFO - joeynmt.training - Epoch   4, Step:    47000, Batch Loss:     1.938005, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-08-02 07:41:16,727 - INFO - joeynmt.training - Epoch   4, Step:    47100, Batch Loss:     2.337774, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-08-02 07:41:30,623 - INFO - joeynmt.training - Epoch   4, Step:    47200, Batch Loss:     1.960949, Tokens per Sec:    15745, Lr: 0.000300\n",
      "2021-08-02 07:41:44,176 - INFO - joeynmt.training - Epoch   4, Step:    47300, Batch Loss:     2.140410, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-08-02 07:41:57,795 - INFO - joeynmt.training - Epoch   4, Step:    47400, Batch Loss:     1.935007, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-02 07:42:11,665 - INFO - joeynmt.training - Epoch   4, Step:    47500, Batch Loss:     1.902693, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-02 07:42:25,368 - INFO - joeynmt.training - Epoch   4, Step:    47600, Batch Loss:     2.101035, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-02 07:42:39,385 - INFO - joeynmt.training - Epoch   4, Step:    47700, Batch Loss:     2.330878, Tokens per Sec:    16053, Lr: 0.000300\n",
      "2021-08-02 07:42:53,263 - INFO - joeynmt.training - Epoch   4, Step:    47800, Batch Loss:     2.184233, Tokens per Sec:    16130, Lr: 0.000300\n",
      "2021-08-02 07:43:06,733 - INFO - joeynmt.training - Epoch   4, Step:    47900, Batch Loss:     2.107682, Tokens per Sec:    15596, Lr: 0.000300\n",
      "2021-08-02 07:43:14,340 - INFO - joeynmt.training - Epoch   4: total training loss 11031.24\n",
      "2021-08-02 07:43:14,340 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-02 07:43:21,223 - INFO - joeynmt.training - Epoch   5, Step:    48000, Batch Loss:     1.936976, Tokens per Sec:    14863, Lr: 0.000300\n",
      "2021-08-02 07:43:35,051 - INFO - joeynmt.training - Epoch   5, Step:    48100, Batch Loss:     1.957173, Tokens per Sec:    15420, Lr: 0.000300\n",
      "2021-08-02 07:43:48,729 - INFO - joeynmt.training - Epoch   5, Step:    48200, Batch Loss:     2.123202, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-02 07:44:02,714 - INFO - joeynmt.training - Epoch   5, Step:    48300, Batch Loss:     1.968339, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-02 07:44:16,599 - INFO - joeynmt.training - Epoch   5, Step:    48400, Batch Loss:     1.995867, Tokens per Sec:    15818, Lr: 0.000300\n",
      "2021-08-02 07:44:30,203 - INFO - joeynmt.training - Epoch   5, Step:    48500, Batch Loss:     2.029536, Tokens per Sec:    15792, Lr: 0.000300\n",
      "2021-08-02 07:44:44,059 - INFO - joeynmt.training - Epoch   5, Step:    48600, Batch Loss:     2.144659, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-02 07:44:57,852 - INFO - joeynmt.training - Epoch   5, Step:    48700, Batch Loss:     2.050761, Tokens per Sec:    16139, Lr: 0.000300\n",
      "2021-08-02 07:45:11,639 - INFO - joeynmt.training - Epoch   5, Step:    48800, Batch Loss:     1.789538, Tokens per Sec:    15515, Lr: 0.000300\n",
      "2021-08-02 07:45:25,460 - INFO - joeynmt.training - Epoch   5, Step:    48900, Batch Loss:     2.266388, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-08-02 07:45:39,181 - INFO - joeynmt.training - Epoch   5, Step:    49000, Batch Loss:     1.916063, Tokens per Sec:    15852, Lr: 0.000300\n",
      "2021-08-02 07:45:53,069 - INFO - joeynmt.training - Epoch   5, Step:    49100, Batch Loss:     2.130673, Tokens per Sec:    16350, Lr: 0.000300\n",
      "2021-08-02 07:46:06,735 - INFO - joeynmt.training - Epoch   5, Step:    49200, Batch Loss:     2.063258, Tokens per Sec:    15939, Lr: 0.000300\n",
      "2021-08-02 07:46:20,776 - INFO - joeynmt.training - Epoch   5, Step:    49300, Batch Loss:     1.968219, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-02 07:46:34,378 - INFO - joeynmt.training - Epoch   5, Step:    49400, Batch Loss:     2.005571, Tokens per Sec:    15838, Lr: 0.000300\n",
      "2021-08-02 07:46:47,954 - INFO - joeynmt.training - Epoch   5, Step:    49500, Batch Loss:     1.961385, Tokens per Sec:    15649, Lr: 0.000300\n",
      "2021-08-02 07:47:01,609 - INFO - joeynmt.training - Epoch   5, Step:    49600, Batch Loss:     1.926725, Tokens per Sec:    16139, Lr: 0.000300\n",
      "2021-08-02 07:47:15,556 - INFO - joeynmt.training - Epoch   5, Step:    49700, Batch Loss:     2.104177, Tokens per Sec:    15817, Lr: 0.000300\n",
      "2021-08-02 07:47:29,624 - INFO - joeynmt.training - Epoch   5, Step:    49800, Batch Loss:     2.338879, Tokens per Sec:    16366, Lr: 0.000300\n",
      "2021-08-02 07:47:43,363 - INFO - joeynmt.training - Epoch   5, Step:    49900, Batch Loss:     2.056667, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-08-02 07:47:57,405 - INFO - joeynmt.training - Epoch   5, Step:    50000, Batch Loss:     2.108612, Tokens per Sec:    16244, Lr: 0.000300\n",
      "2021-08-02 07:49:34,116 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 07:49:34,117 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 07:49:34,117 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 07:49:35,371 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 07:49:35,372 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 07:49:36,197 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 07:49:36,198 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 07:49:36,198 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 07:49:36,198 - INFO - joeynmt.training - \tHypothesis: He sang : “ I have been approaching God , for I have come to me . ”\n",
      "2021-08-02 07:49:36,198 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 07:49:36,199 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 07:49:36,199 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 07:49:36,199 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are living in fear of Jehovah , the most precious things are representing . ”\n",
      "2021-08-02 07:49:36,199 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 07:49:36,200 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 07:49:36,200 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 07:49:36,202 - INFO - joeynmt.training - \tHypothesis: Canaius : Look ! The prophecy of the book of Daniel reports about God’s Kingdom .\n",
      "2021-08-02 07:49:36,203 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 07:49:36,203 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 07:49:36,203 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 07:49:36,203 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and bring it completely ?\n",
      "2021-08-02 07:49:36,204 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    50000: bleu:  22.82, loss: 224104.5000, ppl:   6.3662, duration: 98.7981s\n",
      "2021-08-02 07:49:50,075 - INFO - joeynmt.training - Epoch   5, Step:    50100, Batch Loss:     2.074527, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-08-02 07:50:03,781 - INFO - joeynmt.training - Epoch   5, Step:    50200, Batch Loss:     2.238218, Tokens per Sec:    15698, Lr: 0.000300\n",
      "2021-08-02 07:50:17,696 - INFO - joeynmt.training - Epoch   5, Step:    50300, Batch Loss:     2.227992, Tokens per Sec:    15587, Lr: 0.000300\n",
      "2021-08-02 07:50:31,538 - INFO - joeynmt.training - Epoch   5, Step:    50400, Batch Loss:     2.114619, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-02 07:50:45,238 - INFO - joeynmt.training - Epoch   5, Step:    50500, Batch Loss:     1.842590, Tokens per Sec:    15725, Lr: 0.000300\n",
      "2021-08-02 07:50:58,955 - INFO - joeynmt.training - Epoch   5, Step:    50600, Batch Loss:     2.270215, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-02 07:51:12,852 - INFO - joeynmt.training - Epoch   5, Step:    50700, Batch Loss:     1.857486, Tokens per Sec:    15609, Lr: 0.000300\n",
      "2021-08-02 07:51:26,711 - INFO - joeynmt.training - Epoch   5, Step:    50800, Batch Loss:     1.826242, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-02 07:51:40,536 - INFO - joeynmt.training - Epoch   5, Step:    50900, Batch Loss:     1.836715, Tokens per Sec:    16027, Lr: 0.000300\n",
      "2021-08-02 07:51:54,210 - INFO - joeynmt.training - Epoch   5, Step:    51000, Batch Loss:     2.107700, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-02 07:52:07,924 - INFO - joeynmt.training - Epoch   5, Step:    51100, Batch Loss:     1.977484, Tokens per Sec:    15644, Lr: 0.000300\n",
      "2021-08-02 07:52:21,821 - INFO - joeynmt.training - Epoch   5, Step:    51200, Batch Loss:     1.766633, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-02 07:52:35,618 - INFO - joeynmt.training - Epoch   5, Step:    51300, Batch Loss:     2.070300, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-08-02 07:52:49,453 - INFO - joeynmt.training - Epoch   5, Step:    51400, Batch Loss:     2.196099, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-02 07:53:03,136 - INFO - joeynmt.training - Epoch   5, Step:    51500, Batch Loss:     2.230702, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-02 07:53:17,091 - INFO - joeynmt.training - Epoch   5, Step:    51600, Batch Loss:     1.976737, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-02 07:53:30,912 - INFO - joeynmt.training - Epoch   5, Step:    51700, Batch Loss:     2.127987, Tokens per Sec:    15906, Lr: 0.000300\n",
      "2021-08-02 07:53:44,641 - INFO - joeynmt.training - Epoch   5, Step:    51800, Batch Loss:     2.143948, Tokens per Sec:    15606, Lr: 0.000300\n",
      "2021-08-02 07:53:58,605 - INFO - joeynmt.training - Epoch   5, Step:    51900, Batch Loss:     2.102267, Tokens per Sec:    16281, Lr: 0.000300\n",
      "2021-08-02 07:54:12,305 - INFO - joeynmt.training - Epoch   5, Step:    52000, Batch Loss:     2.103310, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-08-02 07:54:26,331 - INFO - joeynmt.training - Epoch   5, Step:    52100, Batch Loss:     2.346738, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-02 07:54:40,330 - INFO - joeynmt.training - Epoch   5, Step:    52200, Batch Loss:     2.158877, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-02 07:54:54,001 - INFO - joeynmt.training - Epoch   5, Step:    52300, Batch Loss:     2.058857, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-02 07:55:08,020 - INFO - joeynmt.training - Epoch   5, Step:    52400, Batch Loss:     2.227106, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-08-02 07:55:21,490 - INFO - joeynmt.training - Epoch   5, Step:    52500, Batch Loss:     2.533094, Tokens per Sec:    15599, Lr: 0.000300\n",
      "2021-08-02 07:55:35,307 - INFO - joeynmt.training - Epoch   5, Step:    52600, Batch Loss:     1.977334, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-08-02 07:55:49,062 - INFO - joeynmt.training - Epoch   5, Step:    52700, Batch Loss:     2.172513, Tokens per Sec:    15931, Lr: 0.000300\n",
      "2021-08-02 07:56:02,846 - INFO - joeynmt.training - Epoch   5, Step:    52800, Batch Loss:     1.947802, Tokens per Sec:    15766, Lr: 0.000300\n",
      "2021-08-02 07:56:16,961 - INFO - joeynmt.training - Epoch   5, Step:    52900, Batch Loss:     2.139954, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-02 07:56:30,816 - INFO - joeynmt.training - Epoch   5, Step:    53000, Batch Loss:     2.201399, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-08-02 07:56:44,580 - INFO - joeynmt.training - Epoch   5, Step:    53100, Batch Loss:     2.177248, Tokens per Sec:    16224, Lr: 0.000300\n",
      "2021-08-02 07:56:58,402 - INFO - joeynmt.training - Epoch   5, Step:    53200, Batch Loss:     1.849425, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-08-02 07:57:08,915 - INFO - joeynmt.training - Epoch   5: total training loss 10839.13\n",
      "2021-08-02 07:57:08,916 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-02 07:57:12,852 - INFO - joeynmt.training - Epoch   6, Step:    53300, Batch Loss:     2.073765, Tokens per Sec:    13587, Lr: 0.000300\n",
      "2021-08-02 07:57:26,963 - INFO - joeynmt.training - Epoch   6, Step:    53400, Batch Loss:     2.024522, Tokens per Sec:    16101, Lr: 0.000300\n",
      "2021-08-02 07:57:40,758 - INFO - joeynmt.training - Epoch   6, Step:    53500, Batch Loss:     2.030396, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-08-02 07:57:54,534 - INFO - joeynmt.training - Epoch   6, Step:    53600, Batch Loss:     2.012753, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-02 07:58:08,134 - INFO - joeynmt.training - Epoch   6, Step:    53700, Batch Loss:     2.023965, Tokens per Sec:    15634, Lr: 0.000300\n",
      "2021-08-02 07:58:22,101 - INFO - joeynmt.training - Epoch   6, Step:    53800, Batch Loss:     1.926139, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-08-02 07:58:36,078 - INFO - joeynmt.training - Epoch   6, Step:    53900, Batch Loss:     1.897683, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-02 07:58:49,500 - INFO - joeynmt.training - Epoch   6, Step:    54000, Batch Loss:     1.979060, Tokens per Sec:    15427, Lr: 0.000300\n",
      "2021-08-02 07:59:03,265 - INFO - joeynmt.training - Epoch   6, Step:    54100, Batch Loss:     1.791415, Tokens per Sec:    16305, Lr: 0.000300\n",
      "2021-08-02 07:59:17,141 - INFO - joeynmt.training - Epoch   6, Step:    54200, Batch Loss:     2.203611, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-08-02 07:59:31,041 - INFO - joeynmt.training - Epoch   6, Step:    54300, Batch Loss:     2.060418, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-08-02 07:59:44,788 - INFO - joeynmt.training - Epoch   6, Step:    54400, Batch Loss:     2.010362, Tokens per Sec:    15912, Lr: 0.000300\n",
      "2021-08-02 07:59:58,817 - INFO - joeynmt.training - Epoch   6, Step:    54500, Batch Loss:     2.121958, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-08-02 08:00:12,700 - INFO - joeynmt.training - Epoch   6, Step:    54600, Batch Loss:     1.775786, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-08-02 08:00:26,306 - INFO - joeynmt.training - Epoch   6, Step:    54700, Batch Loss:     1.892711, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-02 08:00:40,138 - INFO - joeynmt.training - Epoch   6, Step:    54800, Batch Loss:     2.041293, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-08-02 08:00:53,895 - INFO - joeynmt.training - Epoch   6, Step:    54900, Batch Loss:     1.794242, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-02 08:01:07,741 - INFO - joeynmt.training - Epoch   6, Step:    55000, Batch Loss:     2.035764, Tokens per Sec:    15732, Lr: 0.000300\n",
      "2021-08-02 08:02:44,187 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:02:44,188 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:02:44,188 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:02:45,426 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:02:45,426 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:02:46,148 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:02:46,148 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 08:02:46,148 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 08:02:46,148 - INFO - joeynmt.training - \tHypothesis: He sang : “ I have been approaching God , for I am saving me . ”\n",
      "2021-08-02 08:02:46,149 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:02:46,151 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 08:02:46,151 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 08:02:46,151 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident is a fear of Jehovah , we are more than many riches are standing . ”\n",
      "2021-08-02 08:02:46,151 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:02:46,152 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 08:02:46,152 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 08:02:46,152 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel is discussed about God’s Kingdom .\n",
      "2021-08-02 08:02:46,152 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:02:46,152 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 08:02:46,153 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 08:02:46,153 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 08:02:46,153 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    55000: bleu:  23.03, loss: 220764.6406, ppl:   6.1930, duration: 98.4112s\n",
      "2021-08-02 08:02:59,987 - INFO - joeynmt.training - Epoch   6, Step:    55100, Batch Loss:     2.149095, Tokens per Sec:    15524, Lr: 0.000300\n",
      "2021-08-02 08:03:13,977 - INFO - joeynmt.training - Epoch   6, Step:    55200, Batch Loss:     1.833255, Tokens per Sec:    16056, Lr: 0.000300\n",
      "2021-08-02 08:03:27,811 - INFO - joeynmt.training - Epoch   6, Step:    55300, Batch Loss:     2.030869, Tokens per Sec:    15473, Lr: 0.000300\n",
      "2021-08-02 08:03:41,710 - INFO - joeynmt.training - Epoch   6, Step:    55400, Batch Loss:     2.008504, Tokens per Sec:    15853, Lr: 0.000300\n",
      "2021-08-02 08:03:55,369 - INFO - joeynmt.training - Epoch   6, Step:    55500, Batch Loss:     2.112078, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-08-02 08:04:09,098 - INFO - joeynmt.training - Epoch   6, Step:    55600, Batch Loss:     2.545226, Tokens per Sec:    16019, Lr: 0.000300\n",
      "2021-08-02 08:04:22,913 - INFO - joeynmt.training - Epoch   6, Step:    55700, Batch Loss:     1.833764, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-08-02 08:04:36,786 - INFO - joeynmt.training - Epoch   6, Step:    55800, Batch Loss:     2.013978, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-08-02 08:04:50,648 - INFO - joeynmt.training - Epoch   6, Step:    55900, Batch Loss:     2.189852, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-02 08:05:04,663 - INFO - joeynmt.training - Epoch   6, Step:    56000, Batch Loss:     1.941041, Tokens per Sec:    15790, Lr: 0.000300\n",
      "2021-08-02 08:05:18,524 - INFO - joeynmt.training - Epoch   6, Step:    56100, Batch Loss:     2.056927, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-02 08:05:32,263 - INFO - joeynmt.training - Epoch   6, Step:    56200, Batch Loss:     1.977821, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-02 08:05:46,076 - INFO - joeynmt.training - Epoch   6, Step:    56300, Batch Loss:     2.030319, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-02 08:06:00,218 - INFO - joeynmt.training - Epoch   6, Step:    56400, Batch Loss:     1.753851, Tokens per Sec:    16185, Lr: 0.000300\n",
      "2021-08-02 08:06:14,120 - INFO - joeynmt.training - Epoch   6, Step:    56500, Batch Loss:     2.263349, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-08-02 08:06:27,774 - INFO - joeynmt.training - Epoch   6, Step:    56600, Batch Loss:     1.932720, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-08-02 08:06:41,718 - INFO - joeynmt.training - Epoch   6, Step:    56700, Batch Loss:     2.191304, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-02 08:06:55,397 - INFO - joeynmt.training - Epoch   6, Step:    56800, Batch Loss:     2.133225, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-08-02 08:07:09,259 - INFO - joeynmt.training - Epoch   6, Step:    56900, Batch Loss:     1.986738, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-08-02 08:07:23,124 - INFO - joeynmt.training - Epoch   6, Step:    57000, Batch Loss:     1.960403, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-08-02 08:07:37,062 - INFO - joeynmt.training - Epoch   6, Step:    57100, Batch Loss:     1.790457, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-02 08:07:50,676 - INFO - joeynmt.training - Epoch   6, Step:    57200, Batch Loss:     1.785238, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-08-02 08:08:04,623 - INFO - joeynmt.training - Epoch   6, Step:    57300, Batch Loss:     2.032877, Tokens per Sec:    16208, Lr: 0.000300\n",
      "2021-08-02 08:08:18,385 - INFO - joeynmt.training - Epoch   6, Step:    57400, Batch Loss:     2.174837, Tokens per Sec:    15561, Lr: 0.000300\n",
      "2021-08-02 08:08:32,043 - INFO - joeynmt.training - Epoch   6, Step:    57500, Batch Loss:     1.877604, Tokens per Sec:    15485, Lr: 0.000300\n",
      "2021-08-02 08:08:45,603 - INFO - joeynmt.training - Epoch   6, Step:    57600, Batch Loss:     2.002766, Tokens per Sec:    15789, Lr: 0.000300\n",
      "2021-08-02 08:08:59,411 - INFO - joeynmt.training - Epoch   6, Step:    57700, Batch Loss:     2.401077, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-02 08:09:13,297 - INFO - joeynmt.training - Epoch   6, Step:    57800, Batch Loss:     2.040045, Tokens per Sec:    15898, Lr: 0.000300\n",
      "2021-08-02 08:09:27,319 - INFO - joeynmt.training - Epoch   6, Step:    57900, Batch Loss:     2.064432, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-02 08:09:41,028 - INFO - joeynmt.training - Epoch   6, Step:    58000, Batch Loss:     1.684517, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-08-02 08:09:54,875 - INFO - joeynmt.training - Epoch   6, Step:    58100, Batch Loss:     1.894164, Tokens per Sec:    16149, Lr: 0.000300\n",
      "2021-08-02 08:10:08,554 - INFO - joeynmt.training - Epoch   6, Step:    58200, Batch Loss:     2.144884, Tokens per Sec:    15780, Lr: 0.000300\n",
      "2021-08-02 08:10:22,577 - INFO - joeynmt.training - Epoch   6, Step:    58300, Batch Loss:     1.952547, Tokens per Sec:    15579, Lr: 0.000300\n",
      "2021-08-02 08:10:36,493 - INFO - joeynmt.training - Epoch   6, Step:    58400, Batch Loss:     2.026289, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-08-02 08:10:50,461 - INFO - joeynmt.training - Epoch   6, Step:    58500, Batch Loss:     1.997382, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-02 08:11:04,009 - INFO - joeynmt.training - Epoch   6, Step:    58600, Batch Loss:     1.950717, Tokens per Sec:    15516, Lr: 0.000300\n",
      "2021-08-02 08:11:04,311 - INFO - joeynmt.training - Epoch   6: total training loss 10691.27\n",
      "2021-08-02 08:11:04,311 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-02 08:11:18,507 - INFO - joeynmt.training - Epoch   7, Step:    58700, Batch Loss:     1.907299, Tokens per Sec:    15265, Lr: 0.000300\n",
      "2021-08-02 08:11:32,154 - INFO - joeynmt.training - Epoch   7, Step:    58800, Batch Loss:     1.925388, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-08-02 08:11:45,946 - INFO - joeynmt.training - Epoch   7, Step:    58900, Batch Loss:     2.166777, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-08-02 08:11:59,598 - INFO - joeynmt.training - Epoch   7, Step:    59000, Batch Loss:     1.720651, Tokens per Sec:    15669, Lr: 0.000300\n",
      "2021-08-02 08:12:13,610 - INFO - joeynmt.training - Epoch   7, Step:    59100, Batch Loss:     2.051275, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-08-02 08:12:27,455 - INFO - joeynmt.training - Epoch   7, Step:    59200, Batch Loss:     1.846913, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-08-02 08:12:41,405 - INFO - joeynmt.training - Epoch   7, Step:    59300, Batch Loss:     1.749682, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-02 08:12:55,062 - INFO - joeynmt.training - Epoch   7, Step:    59400, Batch Loss:     2.098799, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-08-02 08:13:08,735 - INFO - joeynmt.training - Epoch   7, Step:    59500, Batch Loss:     1.918808, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-08-02 08:13:22,680 - INFO - joeynmt.training - Epoch   7, Step:    59600, Batch Loss:     2.185962, Tokens per Sec:    15601, Lr: 0.000300\n",
      "2021-08-02 08:13:36,409 - INFO - joeynmt.training - Epoch   7, Step:    59700, Batch Loss:     2.196739, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-08-02 08:13:50,372 - INFO - joeynmt.training - Epoch   7, Step:    59800, Batch Loss:     2.226791, Tokens per Sec:    16106, Lr: 0.000300\n",
      "2021-08-02 08:14:04,223 - INFO - joeynmt.training - Epoch   7, Step:    59900, Batch Loss:     2.111602, Tokens per Sec:    16139, Lr: 0.000300\n",
      "2021-08-02 08:14:18,283 - INFO - joeynmt.training - Epoch   7, Step:    60000, Batch Loss:     2.002884, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-08-02 08:15:57,595 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:15:57,596 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:15:57,596 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:15:58,936 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:15:58,936 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:15:59,675 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:15:59,676 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 08:15:59,676 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 08:15:59,676 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , that is good for me . ”\n",
      "2021-08-02 08:15:59,677 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:15:59,677 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 08:15:59,677 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 08:15:59,677 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are living in fear of Jehovah , greater riches are now representing . ”\n",
      "2021-08-02 08:15:59,678 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:15:59,678 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 08:15:59,678 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 08:15:59,678 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel reports about God’s Kingdom .\n",
      "2021-08-02 08:15:59,678 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:15:59,679 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 08:15:59,679 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 08:15:59,679 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 08:15:59,679 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    60000: bleu:  23.58, loss: 218070.6094, ppl:   6.0567, duration: 101.3964s\n",
      "2021-08-02 08:16:13,551 - INFO - joeynmt.training - Epoch   7, Step:    60100, Batch Loss:     2.087825, Tokens per Sec:    15290, Lr: 0.000300\n",
      "2021-08-02 08:16:27,467 - INFO - joeynmt.training - Epoch   7, Step:    60200, Batch Loss:     2.021393, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-08-02 08:16:41,201 - INFO - joeynmt.training - Epoch   7, Step:    60300, Batch Loss:     1.991228, Tokens per Sec:    15853, Lr: 0.000300\n",
      "2021-08-02 08:16:54,885 - INFO - joeynmt.training - Epoch   7, Step:    60400, Batch Loss:     1.950432, Tokens per Sec:    15699, Lr: 0.000300\n",
      "2021-08-02 08:17:08,811 - INFO - joeynmt.training - Epoch   7, Step:    60500, Batch Loss:     1.920901, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-02 08:17:22,458 - INFO - joeynmt.training - Epoch   7, Step:    60600, Batch Loss:     2.076017, Tokens per Sec:    15737, Lr: 0.000300\n",
      "2021-08-02 08:17:36,162 - INFO - joeynmt.training - Epoch   7, Step:    60700, Batch Loss:     2.095959, Tokens per Sec:    16106, Lr: 0.000300\n",
      "2021-08-02 08:17:50,064 - INFO - joeynmt.training - Epoch   7, Step:    60800, Batch Loss:     1.947044, Tokens per Sec:    16373, Lr: 0.000300\n",
      "2021-08-02 08:18:04,145 - INFO - joeynmt.training - Epoch   7, Step:    60900, Batch Loss:     2.169148, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-08-02 08:18:18,110 - INFO - joeynmt.training - Epoch   7, Step:    61000, Batch Loss:     2.222500, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-02 08:18:31,956 - INFO - joeynmt.training - Epoch   7, Step:    61100, Batch Loss:     1.807299, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-02 08:18:45,645 - INFO - joeynmt.training - Epoch   7, Step:    61200, Batch Loss:     2.010695, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-08-02 08:18:59,446 - INFO - joeynmt.training - Epoch   7, Step:    61300, Batch Loss:     1.942307, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-08-02 08:19:13,098 - INFO - joeynmt.training - Epoch   7, Step:    61400, Batch Loss:     2.047955, Tokens per Sec:    15450, Lr: 0.000300\n",
      "2021-08-02 08:19:27,177 - INFO - joeynmt.training - Epoch   7, Step:    61500, Batch Loss:     1.989532, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-08-02 08:19:40,856 - INFO - joeynmt.training - Epoch   7, Step:    61600, Batch Loss:     1.957345, Tokens per Sec:    15875, Lr: 0.000300\n",
      "2021-08-02 08:19:54,604 - INFO - joeynmt.training - Epoch   7, Step:    61700, Batch Loss:     1.800230, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-08-02 08:20:08,430 - INFO - joeynmt.training - Epoch   7, Step:    61800, Batch Loss:     1.802993, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-02 08:20:22,243 - INFO - joeynmt.training - Epoch   7, Step:    61900, Batch Loss:     2.001062, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-08-02 08:20:36,058 - INFO - joeynmt.training - Epoch   7, Step:    62000, Batch Loss:     2.086481, Tokens per Sec:    15753, Lr: 0.000300\n",
      "2021-08-02 08:20:49,874 - INFO - joeynmt.training - Epoch   7, Step:    62100, Batch Loss:     1.687694, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-08-02 08:21:03,548 - INFO - joeynmt.training - Epoch   7, Step:    62200, Batch Loss:     1.938286, Tokens per Sec:    15853, Lr: 0.000300\n",
      "2021-08-02 08:21:17,299 - INFO - joeynmt.training - Epoch   7, Step:    62300, Batch Loss:     1.845365, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-08-02 08:21:31,180 - INFO - joeynmt.training - Epoch   7, Step:    62400, Batch Loss:     1.824684, Tokens per Sec:    15886, Lr: 0.000300\n",
      "2021-08-02 08:21:44,944 - INFO - joeynmt.training - Epoch   7, Step:    62500, Batch Loss:     1.954705, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-02 08:21:58,749 - INFO - joeynmt.training - Epoch   7, Step:    62600, Batch Loss:     2.023649, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-08-02 08:22:12,725 - INFO - joeynmt.training - Epoch   7, Step:    62700, Batch Loss:     1.677996, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-08-02 08:22:26,481 - INFO - joeynmt.training - Epoch   7, Step:    62800, Batch Loss:     2.009434, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-08-02 08:22:40,264 - INFO - joeynmt.training - Epoch   7, Step:    62900, Batch Loss:     1.909936, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-08-02 08:22:53,828 - INFO - joeynmt.training - Epoch   7, Step:    63000, Batch Loss:     1.680275, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-08-02 08:23:07,489 - INFO - joeynmt.training - Epoch   7, Step:    63100, Batch Loss:     1.742711, Tokens per Sec:    15418, Lr: 0.000300\n",
      "2021-08-02 08:23:21,388 - INFO - joeynmt.training - Epoch   7, Step:    63200, Batch Loss:     2.037270, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-02 08:23:35,083 - INFO - joeynmt.training - Epoch   7, Step:    63300, Batch Loss:     1.831929, Tokens per Sec:    15632, Lr: 0.000300\n",
      "2021-08-02 08:23:49,047 - INFO - joeynmt.training - Epoch   7, Step:    63400, Batch Loss:     1.941013, Tokens per Sec:    16598, Lr: 0.000300\n",
      "2021-08-02 08:24:02,628 - INFO - joeynmt.training - Epoch   7, Step:    63500, Batch Loss:     1.689538, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-08-02 08:24:16,467 - INFO - joeynmt.training - Epoch   7, Step:    63600, Batch Loss:     1.980909, Tokens per Sec:    15507, Lr: 0.000300\n",
      "2021-08-02 08:24:30,195 - INFO - joeynmt.training - Epoch   7, Step:    63700, Batch Loss:     2.058961, Tokens per Sec:    15889, Lr: 0.000300\n",
      "2021-08-02 08:24:43,966 - INFO - joeynmt.training - Epoch   7, Step:    63800, Batch Loss:     2.194520, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-02 08:24:57,708 - INFO - joeynmt.training - Epoch   7, Step:    63900, Batch Loss:     1.899889, Tokens per Sec:    16042, Lr: 0.000300\n",
      "2021-08-02 08:25:02,544 - INFO - joeynmt.training - Epoch   7: total training loss 10580.00\n",
      "2021-08-02 08:25:02,545 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-02 08:25:11,966 - INFO - joeynmt.training - Epoch   8, Step:    64000, Batch Loss:     1.819884, Tokens per Sec:    14602, Lr: 0.000300\n",
      "2021-08-02 08:25:25,845 - INFO - joeynmt.training - Epoch   8, Step:    64100, Batch Loss:     1.969962, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-08-02 08:25:39,522 - INFO - joeynmt.training - Epoch   8, Step:    64200, Batch Loss:     1.925536, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-08-02 08:25:53,254 - INFO - joeynmt.training - Epoch   8, Step:    64300, Batch Loss:     2.083013, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-08-02 08:26:06,935 - INFO - joeynmt.training - Epoch   8, Step:    64400, Batch Loss:     2.000086, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-08-02 08:26:20,795 - INFO - joeynmt.training - Epoch   8, Step:    64500, Batch Loss:     1.884944, Tokens per Sec:    15801, Lr: 0.000300\n",
      "2021-08-02 08:26:34,701 - INFO - joeynmt.training - Epoch   8, Step:    64600, Batch Loss:     1.780974, Tokens per Sec:    15852, Lr: 0.000300\n",
      "2021-08-02 08:26:48,460 - INFO - joeynmt.training - Epoch   8, Step:    64700, Batch Loss:     2.071167, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-08-02 08:27:02,305 - INFO - joeynmt.training - Epoch   8, Step:    64800, Batch Loss:     2.202930, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-08-02 08:27:16,323 - INFO - joeynmt.training - Epoch   8, Step:    64900, Batch Loss:     1.898111, Tokens per Sec:    16289, Lr: 0.000300\n",
      "2021-08-02 08:27:30,028 - INFO - joeynmt.training - Epoch   8, Step:    65000, Batch Loss:     1.712824, Tokens per Sec:    15889, Lr: 0.000300\n",
      "2021-08-02 08:29:02,844 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:29:02,844 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:29:02,844 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:29:04,154 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:29:04,155 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:29:04,929 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:29:04,929 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 08:29:04,929 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 08:29:04,930 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , that is good for me . ”\n",
      "2021-08-02 08:29:04,930 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:29:04,930 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 08:29:04,930 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 08:29:04,931 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are living in fear of Jehovah , and we are greater than many wealth . ”\n",
      "2021-08-02 08:29:04,931 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:29:04,931 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 08:29:04,931 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 08:29:04,932 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel is about God’s Kingdom .\n",
      "2021-08-02 08:29:04,932 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:29:04,932 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 08:29:04,932 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 08:29:04,932 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 08:29:04,933 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    65000: bleu:  23.98, loss: 215245.0625, ppl:   5.9170, duration: 94.9044s\n",
      "2021-08-02 08:29:19,319 - INFO - joeynmt.training - Epoch   8, Step:    65100, Batch Loss:     1.955389, Tokens per Sec:    15337, Lr: 0.000300\n",
      "2021-08-02 08:29:33,079 - INFO - joeynmt.training - Epoch   8, Step:    65200, Batch Loss:     1.836491, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-02 08:29:46,760 - INFO - joeynmt.training - Epoch   8, Step:    65300, Batch Loss:     2.027085, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-02 08:30:00,380 - INFO - joeynmt.training - Epoch   8, Step:    65400, Batch Loss:     1.969102, Tokens per Sec:    15803, Lr: 0.000300\n",
      "2021-08-02 08:30:14,236 - INFO - joeynmt.training - Epoch   8, Step:    65500, Batch Loss:     1.904074, Tokens per Sec:    15701, Lr: 0.000300\n",
      "2021-08-02 08:30:28,038 - INFO - joeynmt.training - Epoch   8, Step:    65600, Batch Loss:     1.962816, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-08-02 08:30:41,866 - INFO - joeynmt.training - Epoch   8, Step:    65700, Batch Loss:     1.952641, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-08-02 08:30:55,543 - INFO - joeynmt.training - Epoch   8, Step:    65800, Batch Loss:     1.806460, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-08-02 08:31:09,311 - INFO - joeynmt.training - Epoch   8, Step:    65900, Batch Loss:     1.737023, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-08-02 08:31:23,346 - INFO - joeynmt.training - Epoch   8, Step:    66000, Batch Loss:     1.894762, Tokens per Sec:    15891, Lr: 0.000300\n",
      "2021-08-02 08:31:37,275 - INFO - joeynmt.training - Epoch   8, Step:    66100, Batch Loss:     2.015627, Tokens per Sec:    15929, Lr: 0.000300\n",
      "2021-08-02 08:31:51,178 - INFO - joeynmt.training - Epoch   8, Step:    66200, Batch Loss:     2.065010, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-08-02 08:32:04,922 - INFO - joeynmt.training - Epoch   8, Step:    66300, Batch Loss:     1.869846, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-08-02 08:32:18,497 - INFO - joeynmt.training - Epoch   8, Step:    66400, Batch Loss:     1.710999, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-02 08:32:32,559 - INFO - joeynmt.training - Epoch   8, Step:    66500, Batch Loss:     1.717859, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-02 08:32:46,195 - INFO - joeynmt.training - Epoch   8, Step:    66600, Batch Loss:     2.070637, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-08-02 08:33:00,078 - INFO - joeynmt.training - Epoch   8, Step:    66700, Batch Loss:     2.181863, Tokens per Sec:    15737, Lr: 0.000300\n",
      "2021-08-02 08:33:13,947 - INFO - joeynmt.training - Epoch   8, Step:    66800, Batch Loss:     2.068330, Tokens per Sec:    15833, Lr: 0.000300\n",
      "2021-08-02 08:33:27,577 - INFO - joeynmt.training - Epoch   8, Step:    66900, Batch Loss:     1.921475, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-08-02 08:33:41,332 - INFO - joeynmt.training - Epoch   8, Step:    67000, Batch Loss:     1.864318, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-02 08:33:55,051 - INFO - joeynmt.training - Epoch   8, Step:    67100, Batch Loss:     1.838102, Tokens per Sec:    15834, Lr: 0.000300\n",
      "2021-08-02 08:34:09,121 - INFO - joeynmt.training - Epoch   8, Step:    67200, Batch Loss:     1.954402, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-02 08:34:22,967 - INFO - joeynmt.training - Epoch   8, Step:    67300, Batch Loss:     2.064454, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-02 08:34:36,754 - INFO - joeynmt.training - Epoch   8, Step:    67400, Batch Loss:     2.064595, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-08-02 08:34:50,506 - INFO - joeynmt.training - Epoch   8, Step:    67500, Batch Loss:     1.934669, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-08-02 08:35:04,357 - INFO - joeynmt.training - Epoch   8, Step:    67600, Batch Loss:     1.892070, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-02 08:35:18,494 - INFO - joeynmt.training - Epoch   8, Step:    67700, Batch Loss:     1.857736, Tokens per Sec:    16185, Lr: 0.000300\n",
      "2021-08-02 08:35:31,986 - INFO - joeynmt.training - Epoch   8, Step:    67800, Batch Loss:     1.841575, Tokens per Sec:    15472, Lr: 0.000300\n",
      "2021-08-02 08:35:45,703 - INFO - joeynmt.training - Epoch   8, Step:    67900, Batch Loss:     2.040801, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-08-02 08:35:59,454 - INFO - joeynmt.training - Epoch   8, Step:    68000, Batch Loss:     2.267864, Tokens per Sec:    16404, Lr: 0.000300\n",
      "2021-08-02 08:36:13,337 - INFO - joeynmt.training - Epoch   8, Step:    68100, Batch Loss:     2.061175, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-08-02 08:36:27,480 - INFO - joeynmt.training - Epoch   8, Step:    68200, Batch Loss:     1.921490, Tokens per Sec:    16220, Lr: 0.000300\n",
      "2021-08-02 08:36:41,204 - INFO - joeynmt.training - Epoch   8, Step:    68300, Batch Loss:     2.033051, Tokens per Sec:    15861, Lr: 0.000300\n",
      "2021-08-02 08:36:54,894 - INFO - joeynmt.training - Epoch   8, Step:    68400, Batch Loss:     1.825423, Tokens per Sec:    15609, Lr: 0.000300\n",
      "2021-08-02 08:37:08,667 - INFO - joeynmt.training - Epoch   8, Step:    68500, Batch Loss:     2.051089, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-02 08:37:22,523 - INFO - joeynmt.training - Epoch   8, Step:    68600, Batch Loss:     1.883837, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-02 08:37:36,553 - INFO - joeynmt.training - Epoch   8, Step:    68700, Batch Loss:     1.979388, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-08-02 08:37:50,412 - INFO - joeynmt.training - Epoch   8, Step:    68800, Batch Loss:     1.941944, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-08-02 08:38:04,233 - INFO - joeynmt.training - Epoch   8, Step:    68900, Batch Loss:     1.751417, Tokens per Sec:    15698, Lr: 0.000300\n",
      "2021-08-02 08:38:18,047 - INFO - joeynmt.training - Epoch   8, Step:    69000, Batch Loss:     1.931904, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-02 08:38:31,770 - INFO - joeynmt.training - Epoch   8, Step:    69100, Batch Loss:     2.259213, Tokens per Sec:    15765, Lr: 0.000300\n",
      "2021-08-02 08:38:45,371 - INFO - joeynmt.training - Epoch   8, Step:    69200, Batch Loss:     1.807521, Tokens per Sec:    15810, Lr: 0.000300\n",
      "2021-08-02 08:38:54,201 - INFO - joeynmt.training - Epoch   8: total training loss 10444.48\n",
      "2021-08-02 08:38:54,202 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-02 08:38:59,798 - INFO - joeynmt.training - Epoch   9, Step:    69300, Batch Loss:     1.640808, Tokens per Sec:    14141, Lr: 0.000300\n",
      "2021-08-02 08:39:13,653 - INFO - joeynmt.training - Epoch   9, Step:    69400, Batch Loss:     2.048735, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-08-02 08:39:27,510 - INFO - joeynmt.training - Epoch   9, Step:    69500, Batch Loss:     1.953607, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-08-02 08:39:40,957 - INFO - joeynmt.training - Epoch   9, Step:    69600, Batch Loss:     1.794912, Tokens per Sec:    15629, Lr: 0.000300\n",
      "2021-08-02 08:39:54,910 - INFO - joeynmt.training - Epoch   9, Step:    69700, Batch Loss:     2.069067, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-08-02 08:40:08,882 - INFO - joeynmt.training - Epoch   9, Step:    69800, Batch Loss:     2.057445, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-08-02 08:40:22,787 - INFO - joeynmt.training - Epoch   9, Step:    69900, Batch Loss:     1.785360, Tokens per Sec:    16021, Lr: 0.000300\n",
      "2021-08-02 08:40:36,655 - INFO - joeynmt.training - Epoch   9, Step:    70000, Batch Loss:     1.808221, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-08-02 08:42:05,054 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:42:05,054 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:42:05,054 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:42:06,279 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:42:06,279 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:42:07,334 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:42:07,335 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 08:42:07,335 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 08:42:07,336 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , for good for me . ”\n",
      "2021-08-02 08:42:07,336 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:42:07,336 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 08:42:07,336 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 08:42:07,337 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our rest is in fear of Jehovah , we are more than the most wealth of the rest of the representation . ”\n",
      "2021-08-02 08:42:07,337 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:42:07,337 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 08:42:07,337 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 08:42:07,338 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel is a detailed reality about God’s Kingdom .\n",
      "2021-08-02 08:42:07,338 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:42:07,338 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 08:42:07,338 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 08:42:07,338 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on conquering ” and completely conquering it ?\n",
      "2021-08-02 08:42:07,339 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    70000: bleu:  24.09, loss: 213543.6562, ppl:   5.8344, duration: 90.6834s\n",
      "2021-08-02 08:42:21,286 - INFO - joeynmt.training - Epoch   9, Step:    70100, Batch Loss:     2.314959, Tokens per Sec:    15465, Lr: 0.000300\n",
      "2021-08-02 08:42:35,148 - INFO - joeynmt.training - Epoch   9, Step:    70200, Batch Loss:     1.905727, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-08-02 08:42:48,858 - INFO - joeynmt.training - Epoch   9, Step:    70300, Batch Loss:     1.911969, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-08-02 08:43:02,630 - INFO - joeynmt.training - Epoch   9, Step:    70400, Batch Loss:     1.933966, Tokens per Sec:    15675, Lr: 0.000300\n",
      "2021-08-02 08:43:16,391 - INFO - joeynmt.training - Epoch   9, Step:    70500, Batch Loss:     1.955982, Tokens per Sec:    15931, Lr: 0.000300\n",
      "2021-08-02 08:43:30,407 - INFO - joeynmt.training - Epoch   9, Step:    70600, Batch Loss:     2.127478, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-02 08:43:44,231 - INFO - joeynmt.training - Epoch   9, Step:    70700, Batch Loss:     1.895243, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-08-02 08:43:57,948 - INFO - joeynmt.training - Epoch   9, Step:    70800, Batch Loss:     1.874543, Tokens per Sec:    15461, Lr: 0.000300\n",
      "2021-08-02 08:44:11,903 - INFO - joeynmt.training - Epoch   9, Step:    70900, Batch Loss:     2.085236, Tokens per Sec:    15871, Lr: 0.000300\n",
      "2021-08-02 08:44:25,578 - INFO - joeynmt.training - Epoch   9, Step:    71000, Batch Loss:     1.931026, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-02 08:44:39,400 - INFO - joeynmt.training - Epoch   9, Step:    71100, Batch Loss:     1.917181, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-02 08:44:53,106 - INFO - joeynmt.training - Epoch   9, Step:    71200, Batch Loss:     2.678321, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-08-02 08:45:06,863 - INFO - joeynmt.training - Epoch   9, Step:    71300, Batch Loss:     2.050807, Tokens per Sec:    15621, Lr: 0.000300\n",
      "2021-08-02 08:45:20,877 - INFO - joeynmt.training - Epoch   9, Step:    71400, Batch Loss:     2.042032, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-08-02 08:45:34,484 - INFO - joeynmt.training - Epoch   9, Step:    71500, Batch Loss:     1.838174, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-08-02 08:45:48,150 - INFO - joeynmt.training - Epoch   9, Step:    71600, Batch Loss:     1.952582, Tokens per Sec:    15723, Lr: 0.000300\n",
      "2021-08-02 08:46:02,003 - INFO - joeynmt.training - Epoch   9, Step:    71700, Batch Loss:     1.892908, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-02 08:46:15,896 - INFO - joeynmt.training - Epoch   9, Step:    71800, Batch Loss:     1.981816, Tokens per Sec:    15610, Lr: 0.000300\n",
      "2021-08-02 08:46:29,664 - INFO - joeynmt.training - Epoch   9, Step:    71900, Batch Loss:     1.735968, Tokens per Sec:    15708, Lr: 0.000300\n",
      "2021-08-02 08:46:43,407 - INFO - joeynmt.training - Epoch   9, Step:    72000, Batch Loss:     1.925730, Tokens per Sec:    15926, Lr: 0.000300\n",
      "2021-08-02 08:46:57,150 - INFO - joeynmt.training - Epoch   9, Step:    72100, Batch Loss:     1.808461, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-02 08:47:10,805 - INFO - joeynmt.training - Epoch   9, Step:    72200, Batch Loss:     1.665145, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-02 08:47:24,732 - INFO - joeynmt.training - Epoch   9, Step:    72300, Batch Loss:     1.988382, Tokens per Sec:    15704, Lr: 0.000300\n",
      "2021-08-02 08:47:38,540 - INFO - joeynmt.training - Epoch   9, Step:    72400, Batch Loss:     1.918537, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-08-02 08:47:52,516 - INFO - joeynmt.training - Epoch   9, Step:    72500, Batch Loss:     2.001242, Tokens per Sec:    16216, Lr: 0.000300\n",
      "2021-08-02 08:48:06,293 - INFO - joeynmt.training - Epoch   9, Step:    72600, Batch Loss:     1.794121, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-02 08:48:20,193 - INFO - joeynmt.training - Epoch   9, Step:    72700, Batch Loss:     1.638649, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-08-02 08:48:34,132 - INFO - joeynmt.training - Epoch   9, Step:    72800, Batch Loss:     1.983192, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-02 08:48:47,965 - INFO - joeynmt.training - Epoch   9, Step:    72900, Batch Loss:     2.105857, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-08-02 08:49:01,910 - INFO - joeynmt.training - Epoch   9, Step:    73000, Batch Loss:     2.002392, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-08-02 08:49:15,814 - INFO - joeynmt.training - Epoch   9, Step:    73100, Batch Loss:     2.152449, Tokens per Sec:    15901, Lr: 0.000300\n",
      "2021-08-02 08:49:29,623 - INFO - joeynmt.training - Epoch   9, Step:    73200, Batch Loss:     2.302351, Tokens per Sec:    16269, Lr: 0.000300\n",
      "2021-08-02 08:49:43,349 - INFO - joeynmt.training - Epoch   9, Step:    73300, Batch Loss:     2.133169, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-02 08:49:57,091 - INFO - joeynmt.training - Epoch   9, Step:    73400, Batch Loss:     1.873554, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-08-02 08:50:11,096 - INFO - joeynmt.training - Epoch   9, Step:    73500, Batch Loss:     1.625280, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-02 08:50:24,842 - INFO - joeynmt.training - Epoch   9, Step:    73600, Batch Loss:     1.829820, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-02 08:50:38,560 - INFO - joeynmt.training - Epoch   9, Step:    73700, Batch Loss:     2.014319, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-02 08:50:52,287 - INFO - joeynmt.training - Epoch   9, Step:    73800, Batch Loss:     1.966089, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-08-02 08:51:06,064 - INFO - joeynmt.training - Epoch   9, Step:    73900, Batch Loss:     1.922359, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-08-02 08:51:19,868 - INFO - joeynmt.training - Epoch   9, Step:    74000, Batch Loss:     1.785568, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-08-02 08:51:33,663 - INFO - joeynmt.training - Epoch   9, Step:    74100, Batch Loss:     1.921908, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-02 08:51:47,268 - INFO - joeynmt.training - Epoch   9, Step:    74200, Batch Loss:     2.021281, Tokens per Sec:    15794, Lr: 0.000300\n",
      "2021-08-02 08:52:00,908 - INFO - joeynmt.training - Epoch   9, Step:    74300, Batch Loss:     2.123537, Tokens per Sec:    15863, Lr: 0.000300\n",
      "2021-08-02 08:52:14,811 - INFO - joeynmt.training - Epoch   9, Step:    74400, Batch Loss:     1.574778, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-02 08:52:28,508 - INFO - joeynmt.training - Epoch   9, Step:    74500, Batch Loss:     1.763964, Tokens per Sec:    15676, Lr: 0.000300\n",
      "2021-08-02 08:52:41,124 - INFO - joeynmt.training - Epoch   9: total training loss 10339.30\n",
      "2021-08-02 08:52:41,124 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-02 08:52:42,790 - INFO - joeynmt.training - Epoch  10, Step:    74600, Batch Loss:     1.774610, Tokens per Sec:    10095, Lr: 0.000300\n",
      "2021-08-02 08:52:56,591 - INFO - joeynmt.training - Epoch  10, Step:    74700, Batch Loss:     1.878679, Tokens per Sec:    16155, Lr: 0.000300\n",
      "2021-08-02 08:53:10,387 - INFO - joeynmt.training - Epoch  10, Step:    74800, Batch Loss:     2.025409, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-08-02 08:53:24,287 - INFO - joeynmt.training - Epoch  10, Step:    74900, Batch Loss:     2.114367, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-08-02 08:53:37,951 - INFO - joeynmt.training - Epoch  10, Step:    75000, Batch Loss:     1.865201, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-08-02 08:55:11,691 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:55:11,691 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:55:11,692 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:55:12,999 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:55:13,000 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:55:14,068 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:55:14,069 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 08:55:14,069 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 08:55:14,069 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , that is good for me . ”\n",
      "2021-08-02 08:55:14,069 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:55:14,070 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 08:55:14,070 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 08:55:14,070 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident we are living in fear of Jehovah , we are more than many riches that are standing . ”\n",
      "2021-08-02 08:55:14,070 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:55:14,070 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 08:55:14,071 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 08:55:14,071 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel is about God’s Kingdom .\n",
      "2021-08-02 08:55:14,071 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:55:14,071 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 08:55:14,071 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 08:55:14,072 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 08:55:14,072 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    75000: bleu:  24.60, loss: 211926.6094, ppl:   5.7570, duration: 96.1204s\n",
      "2021-08-02 08:55:28,115 - INFO - joeynmt.training - Epoch  10, Step:    75100, Batch Loss:     1.872232, Tokens per Sec:    15997, Lr: 0.000300\n",
      "2021-08-02 08:55:41,804 - INFO - joeynmt.training - Epoch  10, Step:    75200, Batch Loss:     1.879312, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-08-02 08:55:55,659 - INFO - joeynmt.training - Epoch  10, Step:    75300, Batch Loss:     1.844979, Tokens per Sec:    15782, Lr: 0.000300\n",
      "2021-08-02 08:56:09,290 - INFO - joeynmt.training - Epoch  10, Step:    75400, Batch Loss:     1.891335, Tokens per Sec:    15543, Lr: 0.000300\n",
      "2021-08-02 08:56:23,097 - INFO - joeynmt.training - Epoch  10, Step:    75500, Batch Loss:     2.057741, Tokens per Sec:    15484, Lr: 0.000300\n",
      "2021-08-02 08:56:37,016 - INFO - joeynmt.training - Epoch  10, Step:    75600, Batch Loss:     1.959982, Tokens per Sec:    16100, Lr: 0.000300\n",
      "2021-08-02 08:56:50,517 - INFO - joeynmt.training - Epoch  10, Step:    75700, Batch Loss:     2.247575, Tokens per Sec:    15681, Lr: 0.000300\n",
      "2021-08-02 08:57:04,400 - INFO - joeynmt.training - Epoch  10, Step:    75800, Batch Loss:     1.648740, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-08-02 08:57:18,382 - INFO - joeynmt.training - Epoch  10, Step:    75900, Batch Loss:     2.249743, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-08-02 08:57:32,314 - INFO - joeynmt.training - Epoch  10, Step:    76000, Batch Loss:     1.832720, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-02 08:57:46,054 - INFO - joeynmt.training - Epoch  10, Step:    76100, Batch Loss:     2.180179, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-08-02 08:57:59,833 - INFO - joeynmt.training - Epoch  10, Step:    76200, Batch Loss:     1.814496, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-02 08:58:13,702 - INFO - joeynmt.training - Epoch  10, Step:    76300, Batch Loss:     1.695504, Tokens per Sec:    15838, Lr: 0.000300\n",
      "2021-08-02 08:58:27,553 - INFO - joeynmt.training - Epoch  10, Step:    76400, Batch Loss:     1.788044, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-02 08:58:41,208 - INFO - joeynmt.training - Epoch  10, Step:    76500, Batch Loss:     2.083662, Tokens per Sec:    15626, Lr: 0.000300\n",
      "2021-08-02 08:58:54,935 - INFO - joeynmt.training - Epoch  10, Step:    76600, Batch Loss:     1.916787, Tokens per Sec:    16139, Lr: 0.000300\n",
      "2021-08-02 08:59:08,776 - INFO - joeynmt.training - Epoch  10, Step:    76700, Batch Loss:     1.899140, Tokens per Sec:    16386, Lr: 0.000300\n",
      "2021-08-02 08:59:22,703 - INFO - joeynmt.training - Epoch  10, Step:    76800, Batch Loss:     1.939822, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-08-02 08:59:36,605 - INFO - joeynmt.training - Epoch  10, Step:    76900, Batch Loss:     1.999777, Tokens per Sec:    15659, Lr: 0.000300\n",
      "2021-08-02 08:59:50,545 - INFO - joeynmt.training - Epoch  10, Step:    77000, Batch Loss:     1.849456, Tokens per Sec:    15673, Lr: 0.000300\n",
      "2021-08-02 09:00:04,450 - INFO - joeynmt.training - Epoch  10, Step:    77100, Batch Loss:     2.128878, Tokens per Sec:    15674, Lr: 0.000300\n",
      "2021-08-02 09:00:18,138 - INFO - joeynmt.training - Epoch  10, Step:    77200, Batch Loss:     1.828272, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-08-02 09:00:32,018 - INFO - joeynmt.training - Epoch  10, Step:    77300, Batch Loss:     1.813609, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-02 09:00:45,670 - INFO - joeynmt.training - Epoch  10, Step:    77400, Batch Loss:     1.893664, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-02 09:00:59,668 - INFO - joeynmt.training - Epoch  10, Step:    77500, Batch Loss:     1.813942, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-08-02 09:01:13,577 - INFO - joeynmt.training - Epoch  10, Step:    77600, Batch Loss:     1.868157, Tokens per Sec:    15750, Lr: 0.000300\n",
      "2021-08-02 09:01:27,284 - INFO - joeynmt.training - Epoch  10, Step:    77700, Batch Loss:     1.964992, Tokens per Sec:    15682, Lr: 0.000300\n",
      "2021-08-02 09:01:41,079 - INFO - joeynmt.training - Epoch  10, Step:    77800, Batch Loss:     1.687138, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-08-02 09:01:54,753 - INFO - joeynmt.training - Epoch  10, Step:    77900, Batch Loss:     1.937053, Tokens per Sec:    15732, Lr: 0.000300\n",
      "2021-08-02 09:02:08,472 - INFO - joeynmt.training - Epoch  10, Step:    78000, Batch Loss:     1.991446, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-08-02 09:02:22,419 - INFO - joeynmt.training - Epoch  10, Step:    78100, Batch Loss:     1.971604, Tokens per Sec:    15750, Lr: 0.000300\n",
      "2021-08-02 09:02:36,343 - INFO - joeynmt.training - Epoch  10, Step:    78200, Batch Loss:     1.823598, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-02 09:02:50,003 - INFO - joeynmt.training - Epoch  10, Step:    78300, Batch Loss:     1.862389, Tokens per Sec:    15905, Lr: 0.000300\n",
      "2021-08-02 09:03:03,744 - INFO - joeynmt.training - Epoch  10, Step:    78400, Batch Loss:     1.930383, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-08-02 09:03:17,730 - INFO - joeynmt.training - Epoch  10, Step:    78500, Batch Loss:     1.967435, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-08-02 09:03:31,516 - INFO - joeynmt.training - Epoch  10, Step:    78600, Batch Loss:     2.013729, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-02 09:03:45,267 - INFO - joeynmt.training - Epoch  10, Step:    78700, Batch Loss:     1.946264, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-02 09:03:59,021 - INFO - joeynmt.training - Epoch  10, Step:    78800, Batch Loss:     2.238948, Tokens per Sec:    15901, Lr: 0.000300\n",
      "2021-08-02 09:04:12,744 - INFO - joeynmt.training - Epoch  10, Step:    78900, Batch Loss:     1.695104, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-02 09:04:26,846 - INFO - joeynmt.training - Epoch  10, Step:    79000, Batch Loss:     1.804175, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-08-02 09:04:40,756 - INFO - joeynmt.training - Epoch  10, Step:    79100, Batch Loss:     2.002529, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-02 09:04:54,716 - INFO - joeynmt.training - Epoch  10, Step:    79200, Batch Loss:     1.881877, Tokens per Sec:    15647, Lr: 0.000300\n",
      "2021-08-02 09:05:08,737 - INFO - joeynmt.training - Epoch  10, Step:    79300, Batch Loss:     1.869329, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-08-02 09:05:22,439 - INFO - joeynmt.training - Epoch  10, Step:    79400, Batch Loss:     1.948304, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-08-02 09:05:36,295 - INFO - joeynmt.training - Epoch  10, Step:    79500, Batch Loss:     1.749646, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-02 09:05:50,062 - INFO - joeynmt.training - Epoch  10, Step:    79600, Batch Loss:     2.092028, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-02 09:06:03,815 - INFO - joeynmt.training - Epoch  10, Step:    79700, Batch Loss:     2.412047, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-08-02 09:06:17,719 - INFO - joeynmt.training - Epoch  10, Step:    79800, Batch Loss:     1.894473, Tokens per Sec:    16155, Lr: 0.000300\n",
      "2021-08-02 09:06:31,421 - INFO - joeynmt.training - Epoch  10, Step:    79900, Batch Loss:     1.802729, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-08-02 09:06:33,779 - INFO - joeynmt.training - Epoch  10: total training loss 10240.53\n",
      "2021-08-02 09:06:33,779 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-02 09:06:45,563 - INFO - joeynmt.training - Epoch  11, Step:    80000, Batch Loss:     1.834275, Tokens per Sec:    15171, Lr: 0.000300\n",
      "2021-08-02 09:08:20,048 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:08:20,049 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:08:20,049 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:08:21,359 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:08:21,360 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:08:22,470 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:08:22,471 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:08:22,471 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:08:22,471 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have come to draw close to God , the good news to me . ”\n",
      "2021-08-02 09:08:22,472 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:08:22,472 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:08:22,472 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:08:22,472 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel says : “ We are living in fear of Jehovah , greater than the riches that are standing . ”\n",
      "2021-08-02 09:08:22,473 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:08:22,473 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:08:22,473 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:08:22,473 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel about God’s Kingdom .\n",
      "2021-08-02 09:08:22,474 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:08:22,474 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:08:22,474 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:08:22,474 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to overcome ” and completely conquer ?\n",
      "2021-08-02 09:08:22,475 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    80000: bleu:  24.83, loss: 209693.7188, ppl:   5.6518, duration: 96.9113s\n",
      "2021-08-02 09:08:36,338 - INFO - joeynmt.training - Epoch  11, Step:    80100, Batch Loss:     1.880262, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-08-02 09:08:50,075 - INFO - joeynmt.training - Epoch  11, Step:    80200, Batch Loss:     2.107301, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-08-02 09:09:03,818 - INFO - joeynmt.training - Epoch  11, Step:    80300, Batch Loss:     2.004448, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-02 09:09:17,726 - INFO - joeynmt.training - Epoch  11, Step:    80400, Batch Loss:     2.015807, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-02 09:09:31,616 - INFO - joeynmt.training - Epoch  11, Step:    80500, Batch Loss:     2.118113, Tokens per Sec:    15799, Lr: 0.000300\n",
      "2021-08-02 09:09:45,408 - INFO - joeynmt.training - Epoch  11, Step:    80600, Batch Loss:     1.814281, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-02 09:09:59,200 - INFO - joeynmt.training - Epoch  11, Step:    80700, Batch Loss:     2.021647, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-02 09:10:12,818 - INFO - joeynmt.training - Epoch  11, Step:    80800, Batch Loss:     2.045400, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-08-02 09:10:26,663 - INFO - joeynmt.training - Epoch  11, Step:    80900, Batch Loss:     1.943716, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-02 09:10:40,592 - INFO - joeynmt.training - Epoch  11, Step:    81000, Batch Loss:     1.866602, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-02 09:10:54,298 - INFO - joeynmt.training - Epoch  11, Step:    81100, Batch Loss:     1.806060, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-02 09:11:08,000 - INFO - joeynmt.training - Epoch  11, Step:    81200, Batch Loss:     2.102654, Tokens per Sec:    15649, Lr: 0.000300\n",
      "2021-08-02 09:11:21,760 - INFO - joeynmt.training - Epoch  11, Step:    81300, Batch Loss:     1.692203, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-08-02 09:11:35,579 - INFO - joeynmt.training - Epoch  11, Step:    81400, Batch Loss:     2.165015, Tokens per Sec:    16252, Lr: 0.000300\n",
      "2021-08-02 09:11:49,134 - INFO - joeynmt.training - Epoch  11, Step:    81500, Batch Loss:     2.003428, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-08-02 09:12:03,018 - INFO - joeynmt.training - Epoch  11, Step:    81600, Batch Loss:     1.683094, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-02 09:12:16,779 - INFO - joeynmt.training - Epoch  11, Step:    81700, Batch Loss:     1.832361, Tokens per Sec:    15434, Lr: 0.000300\n",
      "2021-08-02 09:12:30,636 - INFO - joeynmt.training - Epoch  11, Step:    81800, Batch Loss:     1.981879, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-08-02 09:12:44,225 - INFO - joeynmt.training - Epoch  11, Step:    81900, Batch Loss:     2.281191, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-08-02 09:12:57,972 - INFO - joeynmt.training - Epoch  11, Step:    82000, Batch Loss:     1.809208, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-02 09:13:11,813 - INFO - joeynmt.training - Epoch  11, Step:    82100, Batch Loss:     1.779719, Tokens per Sec:    15679, Lr: 0.000300\n",
      "2021-08-02 09:13:25,757 - INFO - joeynmt.training - Epoch  11, Step:    82200, Batch Loss:     1.864138, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-08-02 09:13:39,554 - INFO - joeynmt.training - Epoch  11, Step:    82300, Batch Loss:     1.881378, Tokens per Sec:    16244, Lr: 0.000300\n",
      "2021-08-02 09:13:53,167 - INFO - joeynmt.training - Epoch  11, Step:    82400, Batch Loss:     1.889906, Tokens per Sec:    15687, Lr: 0.000300\n",
      "2021-08-02 09:14:07,019 - INFO - joeynmt.training - Epoch  11, Step:    82500, Batch Loss:     2.047028, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-08-02 09:14:20,946 - INFO - joeynmt.training - Epoch  11, Step:    82600, Batch Loss:     1.848142, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-02 09:14:34,809 - INFO - joeynmt.training - Epoch  11, Step:    82700, Batch Loss:     1.882349, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-02 09:14:48,577 - INFO - joeynmt.training - Epoch  11, Step:    82800, Batch Loss:     1.924094, Tokens per Sec:    15865, Lr: 0.000300\n",
      "2021-08-02 09:15:02,213 - INFO - joeynmt.training - Epoch  11, Step:    82900, Batch Loss:     1.852876, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-08-02 09:15:16,188 - INFO - joeynmt.training - Epoch  11, Step:    83000, Batch Loss:     1.911312, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-08-02 09:15:29,952 - INFO - joeynmt.training - Epoch  11, Step:    83100, Batch Loss:     1.876340, Tokens per Sec:    15646, Lr: 0.000300\n",
      "2021-08-02 09:15:43,778 - INFO - joeynmt.training - Epoch  11, Step:    83200, Batch Loss:     1.952914, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-02 09:15:57,829 - INFO - joeynmt.training - Epoch  11, Step:    83300, Batch Loss:     2.041278, Tokens per Sec:    15752, Lr: 0.000300\n",
      "2021-08-02 09:16:11,662 - INFO - joeynmt.training - Epoch  11, Step:    83400, Batch Loss:     2.082371, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-02 09:16:25,354 - INFO - joeynmt.training - Epoch  11, Step:    83500, Batch Loss:     1.920282, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-08-02 09:16:39,080 - INFO - joeynmt.training - Epoch  11, Step:    83600, Batch Loss:     1.765193, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-08-02 09:16:52,972 - INFO - joeynmt.training - Epoch  11, Step:    83700, Batch Loss:     1.836015, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-08-02 09:17:07,085 - INFO - joeynmt.training - Epoch  11, Step:    83800, Batch Loss:     2.009048, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-02 09:17:21,015 - INFO - joeynmt.training - Epoch  11, Step:    83900, Batch Loss:     1.936182, Tokens per Sec:    16129, Lr: 0.000300\n",
      "2021-08-02 09:17:34,726 - INFO - joeynmt.training - Epoch  11, Step:    84000, Batch Loss:     2.188417, Tokens per Sec:    15653, Lr: 0.000300\n",
      "2021-08-02 09:17:48,485 - INFO - joeynmt.training - Epoch  11, Step:    84100, Batch Loss:     1.730030, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-08-02 09:18:02,355 - INFO - joeynmt.training - Epoch  11, Step:    84200, Batch Loss:     1.990294, Tokens per Sec:    15792, Lr: 0.000300\n",
      "2021-08-02 09:18:15,995 - INFO - joeynmt.training - Epoch  11, Step:    84300, Batch Loss:     1.719746, Tokens per Sec:    15349, Lr: 0.000300\n",
      "2021-08-02 09:18:29,883 - INFO - joeynmt.training - Epoch  11, Step:    84400, Batch Loss:     1.723514, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-02 09:18:43,633 - INFO - joeynmt.training - Epoch  11, Step:    84500, Batch Loss:     1.778733, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-02 09:18:57,237 - INFO - joeynmt.training - Epoch  11, Step:    84600, Batch Loss:     1.993591, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-08-02 09:19:11,140 - INFO - joeynmt.training - Epoch  11, Step:    84700, Batch Loss:     2.068536, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-08-02 09:19:25,218 - INFO - joeynmt.training - Epoch  11, Step:    84800, Batch Loss:     1.818958, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-02 09:19:38,840 - INFO - joeynmt.training - Epoch  11, Step:    84900, Batch Loss:     1.973385, Tokens per Sec:    15705, Lr: 0.000300\n",
      "2021-08-02 09:19:52,498 - INFO - joeynmt.training - Epoch  11, Step:    85000, Batch Loss:     1.879818, Tokens per Sec:    15863, Lr: 0.000300\n",
      "2021-08-02 09:21:25,192 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:21:25,192 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:21:25,193 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:21:26,515 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:21:26,515 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:21:27,563 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:21:27,565 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:21:27,565 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:21:27,565 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so I am saving for me . ”\n",
      "2021-08-02 09:21:27,565 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:21:27,566 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:21:27,566 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:21:27,566 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident is godly devotion , O Jehovah , greater than the riches that are standing . ”\n",
      "2021-08-02 09:21:27,566 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:21:27,567 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:21:27,567 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:21:27,567 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The book of Daniel tells us what is said about God’s Kingdom .\n",
      "2021-08-02 09:21:27,567 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:21:27,568 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:21:27,568 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:21:27,568 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 09:21:27,568 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    85000: bleu:  24.83, loss: 207986.2969, ppl:   5.5727, duration: 95.0702s\n",
      "2021-08-02 09:21:41,381 - INFO - joeynmt.training - Epoch  11, Step:    85100, Batch Loss:     1.945250, Tokens per Sec:    15440, Lr: 0.000300\n",
      "2021-08-02 09:21:55,185 - INFO - joeynmt.training - Epoch  11, Step:    85200, Batch Loss:     1.878245, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-02 09:22:02,672 - INFO - joeynmt.training - Epoch  11: total training loss 10176.00\n",
      "2021-08-02 09:22:02,673 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-02 09:22:09,625 - INFO - joeynmt.training - Epoch  12, Step:    85300, Batch Loss:     1.899591, Tokens per Sec:    14587, Lr: 0.000300\n",
      "2021-08-02 09:22:23,220 - INFO - joeynmt.training - Epoch  12, Step:    85400, Batch Loss:     2.179757, Tokens per Sec:    15799, Lr: 0.000300\n",
      "2021-08-02 09:22:37,072 - INFO - joeynmt.training - Epoch  12, Step:    85500, Batch Loss:     1.856531, Tokens per Sec:    16216, Lr: 0.000300\n",
      "2021-08-02 09:22:50,882 - INFO - joeynmt.training - Epoch  12, Step:    85600, Batch Loss:     1.950718, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-08-02 09:23:04,765 - INFO - joeynmt.training - Epoch  12, Step:    85700, Batch Loss:     1.850551, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-08-02 09:23:18,613 - INFO - joeynmt.training - Epoch  12, Step:    85800, Batch Loss:     1.865136, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-08-02 09:23:32,336 - INFO - joeynmt.training - Epoch  12, Step:    85900, Batch Loss:     1.812999, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-02 09:23:46,166 - INFO - joeynmt.training - Epoch  12, Step:    86000, Batch Loss:     1.905673, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-08-02 09:24:00,265 - INFO - joeynmt.training - Epoch  12, Step:    86100, Batch Loss:     1.847789, Tokens per Sec:    16267, Lr: 0.000300\n",
      "2021-08-02 09:24:14,056 - INFO - joeynmt.training - Epoch  12, Step:    86200, Batch Loss:     1.891930, Tokens per Sec:    15750, Lr: 0.000300\n",
      "2021-08-02 09:24:27,976 - INFO - joeynmt.training - Epoch  12, Step:    86300, Batch Loss:     2.141521, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-02 09:24:41,637 - INFO - joeynmt.training - Epoch  12, Step:    86400, Batch Loss:     1.959961, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-08-02 09:24:55,255 - INFO - joeynmt.training - Epoch  12, Step:    86500, Batch Loss:     1.537565, Tokens per Sec:    15340, Lr: 0.000300\n",
      "2021-08-02 09:25:09,011 - INFO - joeynmt.training - Epoch  12, Step:    86600, Batch Loss:     1.767584, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-08-02 09:25:22,931 - INFO - joeynmt.training - Epoch  12, Step:    86700, Batch Loss:     2.021808, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-02 09:25:36,877 - INFO - joeynmt.training - Epoch  12, Step:    86800, Batch Loss:     1.870791, Tokens per Sec:    16223, Lr: 0.000300\n",
      "2021-08-02 09:25:50,637 - INFO - joeynmt.training - Epoch  12, Step:    86900, Batch Loss:     2.028607, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-08-02 09:26:04,475 - INFO - joeynmt.training - Epoch  12, Step:    87000, Batch Loss:     1.785470, Tokens per Sec:    15997, Lr: 0.000300\n",
      "2021-08-02 09:26:18,323 - INFO - joeynmt.training - Epoch  12, Step:    87100, Batch Loss:     1.821433, Tokens per Sec:    15815, Lr: 0.000300\n",
      "2021-08-02 09:26:32,190 - INFO - joeynmt.training - Epoch  12, Step:    87200, Batch Loss:     1.980956, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-08-02 09:26:46,006 - INFO - joeynmt.training - Epoch  12, Step:    87300, Batch Loss:     1.508772, Tokens per Sec:    15523, Lr: 0.000300\n",
      "2021-08-02 09:26:59,752 - INFO - joeynmt.training - Epoch  12, Step:    87400, Batch Loss:     1.728631, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-08-02 09:27:13,627 - INFO - joeynmt.training - Epoch  12, Step:    87500, Batch Loss:     1.951860, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-08-02 09:27:27,487 - INFO - joeynmt.training - Epoch  12, Step:    87600, Batch Loss:     1.867073, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-08-02 09:27:41,248 - INFO - joeynmt.training - Epoch  12, Step:    87700, Batch Loss:     1.918967, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-02 09:27:55,129 - INFO - joeynmt.training - Epoch  12, Step:    87800, Batch Loss:     1.937665, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-02 09:28:09,060 - INFO - joeynmt.training - Epoch  12, Step:    87900, Batch Loss:     1.778459, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-08-02 09:28:22,968 - INFO - joeynmt.training - Epoch  12, Step:    88000, Batch Loss:     1.768794, Tokens per Sec:    15905, Lr: 0.000300\n",
      "2021-08-02 09:28:36,697 - INFO - joeynmt.training - Epoch  12, Step:    88100, Batch Loss:     1.988500, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-08-02 09:28:50,358 - INFO - joeynmt.training - Epoch  12, Step:    88200, Batch Loss:     1.743573, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-08-02 09:29:04,473 - INFO - joeynmt.training - Epoch  12, Step:    88300, Batch Loss:     2.027280, Tokens per Sec:    16236, Lr: 0.000300\n",
      "2021-08-02 09:29:18,211 - INFO - joeynmt.training - Epoch  12, Step:    88400, Batch Loss:     1.988974, Tokens per Sec:    15595, Lr: 0.000300\n",
      "2021-08-02 09:29:31,964 - INFO - joeynmt.training - Epoch  12, Step:    88500, Batch Loss:     1.794329, Tokens per Sec:    15700, Lr: 0.000300\n",
      "2021-08-02 09:29:45,894 - INFO - joeynmt.training - Epoch  12, Step:    88600, Batch Loss:     2.171667, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-08-02 09:29:59,466 - INFO - joeynmt.training - Epoch  12, Step:    88700, Batch Loss:     1.805967, Tokens per Sec:    15899, Lr: 0.000300\n",
      "2021-08-02 09:30:13,247 - INFO - joeynmt.training - Epoch  12, Step:    88800, Batch Loss:     2.177971, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-02 09:30:27,172 - INFO - joeynmt.training - Epoch  12, Step:    88900, Batch Loss:     1.841862, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-08-02 09:30:40,801 - INFO - joeynmt.training - Epoch  12, Step:    89000, Batch Loss:     1.976462, Tokens per Sec:    15474, Lr: 0.000300\n",
      "2021-08-02 09:30:54,540 - INFO - joeynmt.training - Epoch  12, Step:    89100, Batch Loss:     1.818056, Tokens per Sec:    15905, Lr: 0.000300\n",
      "2021-08-02 09:31:08,120 - INFO - joeynmt.training - Epoch  12, Step:    89200, Batch Loss:     1.556422, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-02 09:31:22,092 - INFO - joeynmt.training - Epoch  12, Step:    89300, Batch Loss:     1.998068, Tokens per Sec:    15958, Lr: 0.000300\n",
      "2021-08-02 09:31:36,001 - INFO - joeynmt.training - Epoch  12, Step:    89400, Batch Loss:     1.905509, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-02 09:31:49,864 - INFO - joeynmt.training - Epoch  12, Step:    89500, Batch Loss:     1.858664, Tokens per Sec:    16122, Lr: 0.000300\n",
      "2021-08-02 09:32:03,653 - INFO - joeynmt.training - Epoch  12, Step:    89600, Batch Loss:     2.001063, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-02 09:32:17,680 - INFO - joeynmt.training - Epoch  12, Step:    89700, Batch Loss:     1.894146, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-08-02 09:32:31,382 - INFO - joeynmt.training - Epoch  12, Step:    89800, Batch Loss:     1.853097, Tokens per Sec:    15805, Lr: 0.000300\n",
      "2021-08-02 09:32:45,180 - INFO - joeynmt.training - Epoch  12, Step:    89900, Batch Loss:     1.969542, Tokens per Sec:    15756, Lr: 0.000300\n",
      "2021-08-02 09:32:58,929 - INFO - joeynmt.training - Epoch  12, Step:    90000, Batch Loss:     2.125252, Tokens per Sec:    15530, Lr: 0.000300\n",
      "2021-08-02 09:34:35,147 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:34:35,148 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:34:35,148 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:34:36,379 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:34:36,380 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:34:37,129 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:34:37,130 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:34:37,130 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:34:37,131 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am drawing close to God , so I am saving me . ”\n",
      "2021-08-02 09:34:37,131 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:34:37,131 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:34:37,131 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:34:37,131 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are living in fear of Jehovah , greater riches are standing . ”\n",
      "2021-08-02 09:34:37,132 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:34:37,132 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:34:37,132 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:34:37,132 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has something about God’s Kingdom .\n",
      "2021-08-02 09:34:37,133 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:34:37,133 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:34:37,133 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:34:37,133 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 09:34:37,134 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    90000: bleu:  24.99, loss: 206906.2031, ppl:   5.5232, duration: 98.2047s\n",
      "2021-08-02 09:34:51,186 - INFO - joeynmt.training - Epoch  12, Step:    90100, Batch Loss:     1.732204, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-02 09:35:04,991 - INFO - joeynmt.training - Epoch  12, Step:    90200, Batch Loss:     1.887295, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-02 09:35:18,929 - INFO - joeynmt.training - Epoch  12, Step:    90300, Batch Loss:     1.799751, Tokens per Sec:    16021, Lr: 0.000300\n",
      "2021-08-02 09:35:32,565 - INFO - joeynmt.training - Epoch  12, Step:    90400, Batch Loss:     2.016567, Tokens per Sec:    15699, Lr: 0.000300\n",
      "2021-08-02 09:35:46,463 - INFO - joeynmt.training - Epoch  12, Step:    90500, Batch Loss:     1.960310, Tokens per Sec:    16353, Lr: 0.000300\n",
      "2021-08-02 09:35:56,842 - INFO - joeynmt.training - Epoch  12: total training loss 10065.56\n",
      "2021-08-02 09:35:56,842 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-02 09:36:00,699 - INFO - joeynmt.training - Epoch  13, Step:    90600, Batch Loss:     1.922197, Tokens per Sec:    14161, Lr: 0.000300\n",
      "2021-08-02 09:36:14,553 - INFO - joeynmt.training - Epoch  13, Step:    90700, Batch Loss:     1.738532, Tokens per Sec:    16181, Lr: 0.000300\n",
      "2021-08-02 09:36:28,555 - INFO - joeynmt.training - Epoch  13, Step:    90800, Batch Loss:     1.929865, Tokens per Sec:    15580, Lr: 0.000300\n",
      "2021-08-02 09:36:42,397 - INFO - joeynmt.training - Epoch  13, Step:    90900, Batch Loss:     2.018367, Tokens per Sec:    16244, Lr: 0.000300\n",
      "2021-08-02 09:36:56,009 - INFO - joeynmt.training - Epoch  13, Step:    91000, Batch Loss:     1.926998, Tokens per Sec:    15872, Lr: 0.000300\n",
      "2021-08-02 09:37:09,795 - INFO - joeynmt.training - Epoch  13, Step:    91100, Batch Loss:     1.712586, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-02 09:37:23,550 - INFO - joeynmt.training - Epoch  13, Step:    91200, Batch Loss:     1.943400, Tokens per Sec:    15433, Lr: 0.000300\n",
      "2021-08-02 09:37:37,526 - INFO - joeynmt.training - Epoch  13, Step:    91300, Batch Loss:     1.990819, Tokens per Sec:    16008, Lr: 0.000300\n",
      "2021-08-02 09:37:51,300 - INFO - joeynmt.training - Epoch  13, Step:    91400, Batch Loss:     1.795502, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-02 09:38:05,127 - INFO - joeynmt.training - Epoch  13, Step:    91500, Batch Loss:     2.213545, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-02 09:38:19,009 - INFO - joeynmt.training - Epoch  13, Step:    91600, Batch Loss:     1.777313, Tokens per Sec:    16129, Lr: 0.000300\n",
      "2021-08-02 09:38:32,783 - INFO - joeynmt.training - Epoch  13, Step:    91700, Batch Loss:     2.009605, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-02 09:38:46,502 - INFO - joeynmt.training - Epoch  13, Step:    91800, Batch Loss:     2.010282, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-08-02 09:39:00,346 - INFO - joeynmt.training - Epoch  13, Step:    91900, Batch Loss:     1.883047, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-08-02 09:39:14,202 - INFO - joeynmt.training - Epoch  13, Step:    92000, Batch Loss:     1.773906, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-08-02 09:39:28,137 - INFO - joeynmt.training - Epoch  13, Step:    92100, Batch Loss:     1.906314, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-02 09:39:41,938 - INFO - joeynmt.training - Epoch  13, Step:    92200, Batch Loss:     1.979639, Tokens per Sec:    16017, Lr: 0.000300\n",
      "2021-08-02 09:39:55,685 - INFO - joeynmt.training - Epoch  13, Step:    92300, Batch Loss:     1.784697, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-08-02 09:40:09,560 - INFO - joeynmt.training - Epoch  13, Step:    92400, Batch Loss:     1.964482, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-02 09:40:23,374 - INFO - joeynmt.training - Epoch  13, Step:    92500, Batch Loss:     1.804028, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-08-02 09:40:37,200 - INFO - joeynmt.training - Epoch  13, Step:    92600, Batch Loss:     1.824473, Tokens per Sec:    16241, Lr: 0.000300\n",
      "2021-08-02 09:40:50,904 - INFO - joeynmt.training - Epoch  13, Step:    92700, Batch Loss:     1.946885, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-08-02 09:41:04,489 - INFO - joeynmt.training - Epoch  13, Step:    92800, Batch Loss:     1.998578, Tokens per Sec:    15818, Lr: 0.000300\n",
      "2021-08-02 09:41:18,295 - INFO - joeynmt.training - Epoch  13, Step:    92900, Batch Loss:     2.065674, Tokens per Sec:    15368, Lr: 0.000300\n",
      "2021-08-02 09:41:32,194 - INFO - joeynmt.training - Epoch  13, Step:    93000, Batch Loss:     2.105031, Tokens per Sec:    15865, Lr: 0.000300\n",
      "2021-08-02 09:41:45,731 - INFO - joeynmt.training - Epoch  13, Step:    93100, Batch Loss:     2.038840, Tokens per Sec:    15681, Lr: 0.000300\n",
      "2021-08-02 09:41:59,542 - INFO - joeynmt.training - Epoch  13, Step:    93200, Batch Loss:     1.842063, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-02 09:42:13,345 - INFO - joeynmt.training - Epoch  13, Step:    93300, Batch Loss:     1.911776, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-02 09:42:27,501 - INFO - joeynmt.training - Epoch  13, Step:    93400, Batch Loss:     1.948531, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-02 09:42:41,185 - INFO - joeynmt.training - Epoch  13, Step:    93500, Batch Loss:     1.722760, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-08-02 09:42:54,937 - INFO - joeynmt.training - Epoch  13, Step:    93600, Batch Loss:     2.020329, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-02 09:43:08,580 - INFO - joeynmt.training - Epoch  13, Step:    93700, Batch Loss:     1.840262, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-08-02 09:43:22,299 - INFO - joeynmt.training - Epoch  13, Step:    93800, Batch Loss:     1.856072, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-02 09:43:35,902 - INFO - joeynmt.training - Epoch  13, Step:    93900, Batch Loss:     1.945032, Tokens per Sec:    15505, Lr: 0.000300\n",
      "2021-08-02 09:43:49,621 - INFO - joeynmt.training - Epoch  13, Step:    94000, Batch Loss:     2.096736, Tokens per Sec:    15579, Lr: 0.000300\n",
      "2021-08-02 09:44:03,401 - INFO - joeynmt.training - Epoch  13, Step:    94100, Batch Loss:     2.193971, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-08-02 09:44:17,246 - INFO - joeynmt.training - Epoch  13, Step:    94200, Batch Loss:     1.930974, Tokens per Sec:    16265, Lr: 0.000300\n",
      "2021-08-02 09:44:31,100 - INFO - joeynmt.training - Epoch  13, Step:    94300, Batch Loss:     1.843922, Tokens per Sec:    16042, Lr: 0.000300\n",
      "2021-08-02 09:44:44,769 - INFO - joeynmt.training - Epoch  13, Step:    94400, Batch Loss:     2.104566, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-02 09:44:58,610 - INFO - joeynmt.training - Epoch  13, Step:    94500, Batch Loss:     1.859069, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-02 09:45:12,498 - INFO - joeynmt.training - Epoch  13, Step:    94600, Batch Loss:     2.056871, Tokens per Sec:    15803, Lr: 0.000300\n",
      "2021-08-02 09:45:26,488 - INFO - joeynmt.training - Epoch  13, Step:    94700, Batch Loss:     1.821777, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-08-02 09:45:40,209 - INFO - joeynmt.training - Epoch  13, Step:    94800, Batch Loss:     1.643115, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-02 09:45:54,002 - INFO - joeynmt.training - Epoch  13, Step:    94900, Batch Loss:     1.866939, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-08-02 09:46:07,982 - INFO - joeynmt.training - Epoch  13, Step:    95000, Batch Loss:     1.859533, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-02 09:47:43,496 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:47:43,498 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:47:43,498 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:47:44,716 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:47:44,717 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:47:45,832 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:47:45,832 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:47:45,832 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:47:45,833 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , that is good for me . ”\n",
      "2021-08-02 09:47:45,833 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:47:45,833 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:47:45,834 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:47:45,834 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our resident is godly devotion , greater than the riches that are standing . ”\n",
      "2021-08-02 09:47:45,834 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:47:45,834 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:47:45,834 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:47:45,835 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel tells us what it says about God’s Kingdom .\n",
      "2021-08-02 09:47:45,835 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:47:45,835 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:47:45,835 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:47:45,835 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering it ?\n",
      "2021-08-02 09:47:45,836 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    95000: bleu:  25.04, loss: 204909.0625, ppl:   5.4328, duration: 97.8535s\n",
      "2021-08-02 09:47:59,529 - INFO - joeynmt.training - Epoch  13, Step:    95100, Batch Loss:     1.845378, Tokens per Sec:    15574, Lr: 0.000300\n",
      "2021-08-02 09:48:13,199 - INFO - joeynmt.training - Epoch  13, Step:    95200, Batch Loss:     1.841596, Tokens per Sec:    15729, Lr: 0.000300\n",
      "2021-08-02 09:48:27,168 - INFO - joeynmt.training - Epoch  13, Step:    95300, Batch Loss:     2.158899, Tokens per Sec:    15638, Lr: 0.000300\n",
      "2021-08-02 09:48:40,971 - INFO - joeynmt.training - Epoch  13, Step:    95400, Batch Loss:     2.108254, Tokens per Sec:    15548, Lr: 0.000300\n",
      "2021-08-02 09:48:54,867 - INFO - joeynmt.training - Epoch  13, Step:    95500, Batch Loss:     1.865520, Tokens per Sec:    15818, Lr: 0.000300\n",
      "2021-08-02 09:49:08,584 - INFO - joeynmt.training - Epoch  13, Step:    95600, Batch Loss:     1.992403, Tokens per Sec:    15618, Lr: 0.000300\n",
      "2021-08-02 09:49:22,217 - INFO - joeynmt.training - Epoch  13, Step:    95700, Batch Loss:     1.860002, Tokens per Sec:    15763, Lr: 0.000300\n",
      "2021-08-02 09:49:35,993 - INFO - joeynmt.training - Epoch  13, Step:    95800, Batch Loss:     1.708913, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-02 09:49:49,848 - INFO - joeynmt.training - Epoch  13, Step:    95900, Batch Loss:     1.860040, Tokens per Sec:    16493, Lr: 0.000300\n",
      "2021-08-02 09:49:51,657 - INFO - joeynmt.training - Epoch  13: total training loss 10030.13\n",
      "2021-08-02 09:49:51,657 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-02 09:50:04,299 - INFO - joeynmt.training - Epoch  14, Step:    96000, Batch Loss:     1.888526, Tokens per Sec:    15135, Lr: 0.000300\n",
      "2021-08-02 09:50:18,210 - INFO - joeynmt.training - Epoch  14, Step:    96100, Batch Loss:     1.958676, Tokens per Sec:    15761, Lr: 0.000300\n",
      "2021-08-02 09:50:32,015 - INFO - joeynmt.training - Epoch  14, Step:    96200, Batch Loss:     1.892600, Tokens per Sec:    16108, Lr: 0.000300\n",
      "2021-08-02 09:50:45,593 - INFO - joeynmt.training - Epoch  14, Step:    96300, Batch Loss:     1.969102, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-02 09:50:59,495 - INFO - joeynmt.training - Epoch  14, Step:    96400, Batch Loss:     1.808863, Tokens per Sec:    16376, Lr: 0.000300\n",
      "2021-08-02 09:51:13,313 - INFO - joeynmt.training - Epoch  14, Step:    96500, Batch Loss:     1.970552, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-08-02 09:51:27,069 - INFO - joeynmt.training - Epoch  14, Step:    96600, Batch Loss:     1.841423, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-08-02 09:51:40,745 - INFO - joeynmt.training - Epoch  14, Step:    96700, Batch Loss:     1.786762, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-08-02 09:51:54,585 - INFO - joeynmt.training - Epoch  14, Step:    96800, Batch Loss:     2.169115, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-02 09:52:08,436 - INFO - joeynmt.training - Epoch  14, Step:    96900, Batch Loss:     2.025217, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-08-02 09:52:22,386 - INFO - joeynmt.training - Epoch  14, Step:    97000, Batch Loss:     1.889146, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-02 09:52:36,397 - INFO - joeynmt.training - Epoch  14, Step:    97100, Batch Loss:     1.793140, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-02 09:52:50,094 - INFO - joeynmt.training - Epoch  14, Step:    97200, Batch Loss:     1.810060, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-08-02 09:53:03,906 - INFO - joeynmt.training - Epoch  14, Step:    97300, Batch Loss:     1.639772, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-08-02 09:53:17,734 - INFO - joeynmt.training - Epoch  14, Step:    97400, Batch Loss:     1.755886, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-02 09:53:31,403 - INFO - joeynmt.training - Epoch  14, Step:    97500, Batch Loss:     1.405077, Tokens per Sec:    15498, Lr: 0.000300\n",
      "2021-08-02 09:53:45,106 - INFO - joeynmt.training - Epoch  14, Step:    97600, Batch Loss:     1.731742, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-08-02 09:53:58,839 - INFO - joeynmt.training - Epoch  14, Step:    97700, Batch Loss:     1.883624, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-08-02 09:54:12,631 - INFO - joeynmt.training - Epoch  14, Step:    97800, Batch Loss:     2.027856, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-08-02 09:54:26,565 - INFO - joeynmt.training - Epoch  14, Step:    97900, Batch Loss:     2.212236, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-02 09:54:40,496 - INFO - joeynmt.training - Epoch  14, Step:    98000, Batch Loss:     1.752414, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-02 09:54:54,385 - INFO - joeynmt.training - Epoch  14, Step:    98100, Batch Loss:     1.929369, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-08-02 09:55:08,274 - INFO - joeynmt.training - Epoch  14, Step:    98200, Batch Loss:     1.893347, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-02 09:55:22,151 - INFO - joeynmt.training - Epoch  14, Step:    98300, Batch Loss:     1.955224, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-02 09:55:35,980 - INFO - joeynmt.training - Epoch  14, Step:    98400, Batch Loss:     1.805118, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-08-02 09:55:49,660 - INFO - joeynmt.training - Epoch  14, Step:    98500, Batch Loss:     2.135423, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-08-02 09:56:03,517 - INFO - joeynmt.training - Epoch  14, Step:    98600, Batch Loss:     1.729959, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-08-02 09:56:17,349 - INFO - joeynmt.training - Epoch  14, Step:    98700, Batch Loss:     1.849684, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-08-02 09:56:31,138 - INFO - joeynmt.training - Epoch  14, Step:    98800, Batch Loss:     1.823702, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-08-02 09:56:44,934 - INFO - joeynmt.training - Epoch  14, Step:    98900, Batch Loss:     1.982970, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-02 09:56:58,597 - INFO - joeynmt.training - Epoch  14, Step:    99000, Batch Loss:     1.800900, Tokens per Sec:    15833, Lr: 0.000300\n",
      "2021-08-02 09:57:12,672 - INFO - joeynmt.training - Epoch  14, Step:    99100, Batch Loss:     1.835766, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-02 09:57:26,490 - INFO - joeynmt.training - Epoch  14, Step:    99200, Batch Loss:     1.843161, Tokens per Sec:    15762, Lr: 0.000300\n",
      "2021-08-02 09:57:40,320 - INFO - joeynmt.training - Epoch  14, Step:    99300, Batch Loss:     1.751397, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-02 09:57:53,926 - INFO - joeynmt.training - Epoch  14, Step:    99400, Batch Loss:     1.929797, Tokens per Sec:    16154, Lr: 0.000300\n",
      "2021-08-02 09:58:07,744 - INFO - joeynmt.training - Epoch  14, Step:    99500, Batch Loss:     1.890846, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-02 09:58:21,674 - INFO - joeynmt.training - Epoch  14, Step:    99600, Batch Loss:     1.812043, Tokens per Sec:    15641, Lr: 0.000300\n",
      "2021-08-02 09:58:35,572 - INFO - joeynmt.training - Epoch  14, Step:    99700, Batch Loss:     1.781298, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-02 09:58:49,336 - INFO - joeynmt.training - Epoch  14, Step:    99800, Batch Loss:     1.856978, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-08-02 09:59:02,980 - INFO - joeynmt.training - Epoch  14, Step:    99900, Batch Loss:     1.609779, Tokens per Sec:    15939, Lr: 0.000300\n",
      "2021-08-02 09:59:16,801 - INFO - joeynmt.training - Epoch  14, Step:   100000, Batch Loss:     1.919683, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-02 10:00:49,530 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:00:49,531 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:00:49,531 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:00:50,794 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:00:50,795 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:00:51,565 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:00:51,566 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:00:51,566 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:00:51,566 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , I have drawn close to God , good for me . ”\n",
      "2021-08-02 10:00:51,566 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:00:51,567 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:00:51,567 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:00:51,567 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our resident is godly devotion , greater than the riches that are standing . ”\n",
      "2021-08-02 10:00:51,567 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:00:51,568 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:00:51,568 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:00:51,568 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has something about God’s Kingdom .\n",
      "2021-08-02 10:00:51,568 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:00:51,568 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:00:51,569 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:00:51,569 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on conquering ” and completely conquering it ?\n",
      "2021-08-02 10:00:51,569 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   100000: bleu:  25.24, loss: 204008.1562, ppl:   5.3925, duration: 94.7677s\n",
      "2021-08-02 10:01:05,640 - INFO - joeynmt.training - Epoch  14, Step:   100100, Batch Loss:     2.052758, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-08-02 10:01:19,510 - INFO - joeynmt.training - Epoch  14, Step:   100200, Batch Loss:     1.755924, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-02 10:01:33,274 - INFO - joeynmt.training - Epoch  14, Step:   100300, Batch Loss:     1.807743, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-02 10:01:46,883 - INFO - joeynmt.training - Epoch  14, Step:   100400, Batch Loss:     1.967062, Tokens per Sec:    15846, Lr: 0.000300\n",
      "2021-08-02 10:02:00,580 - INFO - joeynmt.training - Epoch  14, Step:   100500, Batch Loss:     1.969683, Tokens per Sec:    15602, Lr: 0.000300\n",
      "2021-08-02 10:02:14,453 - INFO - joeynmt.training - Epoch  14, Step:   100600, Batch Loss:     1.601519, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-08-02 10:02:28,171 - INFO - joeynmt.training - Epoch  14, Step:   100700, Batch Loss:     1.927552, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-08-02 10:02:41,799 - INFO - joeynmt.training - Epoch  14, Step:   100800, Batch Loss:     1.937229, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-02 10:02:55,581 - INFO - joeynmt.training - Epoch  14, Step:   100900, Batch Loss:     2.010184, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-08-02 10:03:09,507 - INFO - joeynmt.training - Epoch  14, Step:   101000, Batch Loss:     1.712081, Tokens per Sec:    15796, Lr: 0.000300\n",
      "2021-08-02 10:03:23,451 - INFO - joeynmt.training - Epoch  14, Step:   101100, Batch Loss:     1.809541, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-08-02 10:03:37,086 - INFO - joeynmt.training - Epoch  14, Step:   101200, Batch Loss:     1.763306, Tokens per Sec:    15794, Lr: 0.000300\n",
      "2021-08-02 10:03:41,945 - INFO - joeynmt.training - Epoch  14: total training loss 9935.19\n",
      "2021-08-02 10:03:41,946 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-02 10:03:51,360 - INFO - joeynmt.training - Epoch  15, Step:   101300, Batch Loss:     1.923606, Tokens per Sec:    14861, Lr: 0.000300\n",
      "2021-08-02 10:04:05,314 - INFO - joeynmt.training - Epoch  15, Step:   101400, Batch Loss:     2.004806, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-02 10:04:19,218 - INFO - joeynmt.training - Epoch  15, Step:   101500, Batch Loss:     1.905341, Tokens per Sec:    16154, Lr: 0.000300\n",
      "2021-08-02 10:04:33,253 - INFO - joeynmt.training - Epoch  15, Step:   101600, Batch Loss:     1.556061, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-02 10:04:46,955 - INFO - joeynmt.training - Epoch  15, Step:   101700, Batch Loss:     1.643111, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-02 10:05:00,721 - INFO - joeynmt.training - Epoch  15, Step:   101800, Batch Loss:     1.890626, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-08-02 10:05:14,504 - INFO - joeynmt.training - Epoch  15, Step:   101900, Batch Loss:     1.910907, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-02 10:05:28,503 - INFO - joeynmt.training - Epoch  15, Step:   102000, Batch Loss:     1.749470, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-08-02 10:05:42,562 - INFO - joeynmt.training - Epoch  15, Step:   102100, Batch Loss:     1.725150, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-02 10:05:56,323 - INFO - joeynmt.training - Epoch  15, Step:   102200, Batch Loss:     1.957063, Tokens per Sec:    15734, Lr: 0.000300\n",
      "2021-08-02 10:06:10,301 - INFO - joeynmt.training - Epoch  15, Step:   102300, Batch Loss:     1.854064, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-08-02 10:06:23,933 - INFO - joeynmt.training - Epoch  15, Step:   102400, Batch Loss:     1.691380, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-08-02 10:06:37,679 - INFO - joeynmt.training - Epoch  15, Step:   102500, Batch Loss:     1.946681, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-08-02 10:06:51,484 - INFO - joeynmt.training - Epoch  15, Step:   102600, Batch Loss:     1.990704, Tokens per Sec:    16276, Lr: 0.000300\n",
      "2021-08-02 10:07:05,019 - INFO - joeynmt.training - Epoch  15, Step:   102700, Batch Loss:     1.987250, Tokens per Sec:    15667, Lr: 0.000300\n",
      "2021-08-02 10:07:19,047 - INFO - joeynmt.training - Epoch  15, Step:   102800, Batch Loss:     2.098119, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-02 10:07:32,885 - INFO - joeynmt.training - Epoch  15, Step:   102900, Batch Loss:     1.970638, Tokens per Sec:    16327, Lr: 0.000300\n",
      "2021-08-02 10:07:46,658 - INFO - joeynmt.training - Epoch  15, Step:   103000, Batch Loss:     1.660925, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-02 10:08:00,354 - INFO - joeynmt.training - Epoch  15, Step:   103100, Batch Loss:     1.768960, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-08-02 10:08:14,198 - INFO - joeynmt.training - Epoch  15, Step:   103200, Batch Loss:     1.774764, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-08-02 10:08:28,064 - INFO - joeynmt.training - Epoch  15, Step:   103300, Batch Loss:     1.980047, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-08-02 10:08:41,858 - INFO - joeynmt.training - Epoch  15, Step:   103400, Batch Loss:     1.732993, Tokens per Sec:    16223, Lr: 0.000300\n",
      "2021-08-02 10:08:55,589 - INFO - joeynmt.training - Epoch  15, Step:   103500, Batch Loss:     1.749961, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-08-02 10:09:09,411 - INFO - joeynmt.training - Epoch  15, Step:   103600, Batch Loss:     1.954612, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-08-02 10:09:23,410 - INFO - joeynmt.training - Epoch  15, Step:   103700, Batch Loss:     1.830525, Tokens per Sec:    15761, Lr: 0.000300\n",
      "2021-08-02 10:09:37,167 - INFO - joeynmt.training - Epoch  15, Step:   103800, Batch Loss:     1.827871, Tokens per Sec:    15700, Lr: 0.000300\n",
      "2021-08-02 10:09:50,945 - INFO - joeynmt.training - Epoch  15, Step:   103900, Batch Loss:     1.821643, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-08-02 10:10:04,744 - INFO - joeynmt.training - Epoch  15, Step:   104000, Batch Loss:     1.816052, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-02 10:10:18,576 - INFO - joeynmt.training - Epoch  15, Step:   104100, Batch Loss:     1.823914, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-08-02 10:10:32,530 - INFO - joeynmt.training - Epoch  15, Step:   104200, Batch Loss:     1.796078, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-02 10:10:46,215 - INFO - joeynmt.training - Epoch  15, Step:   104300, Batch Loss:     1.741142, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-02 10:11:00,002 - INFO - joeynmt.training - Epoch  15, Step:   104400, Batch Loss:     1.962915, Tokens per Sec:    15555, Lr: 0.000300\n",
      "2021-08-02 10:11:13,915 - INFO - joeynmt.training - Epoch  15, Step:   104500, Batch Loss:     1.643268, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-08-02 10:11:27,670 - INFO - joeynmt.training - Epoch  15, Step:   104600, Batch Loss:     1.899674, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-08-02 10:11:41,260 - INFO - joeynmt.training - Epoch  15, Step:   104700, Batch Loss:     1.791023, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-02 10:11:54,895 - INFO - joeynmt.training - Epoch  15, Step:   104800, Batch Loss:     1.994079, Tokens per Sec:    16143, Lr: 0.000300\n",
      "2021-08-02 10:12:08,929 - INFO - joeynmt.training - Epoch  15, Step:   104900, Batch Loss:     1.875878, Tokens per Sec:    15752, Lr: 0.000300\n",
      "2021-08-02 10:12:22,825 - INFO - joeynmt.training - Epoch  15, Step:   105000, Batch Loss:     2.030416, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-08-02 10:13:55,542 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:13:55,542 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:13:55,542 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:13:56,765 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:13:56,765 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:13:57,804 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:13:57,804 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:13:57,804 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:13:57,804 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , the good news for me . ”\n",
      "2021-08-02 10:13:57,805 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:13:57,805 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:13:57,805 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:13:57,806 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident we are godly devotion , greater than the riches that are standing . ”\n",
      "2021-08-02 10:13:57,806 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:13:57,806 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:13:57,806 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:13:57,806 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel says about God’s Kingdom .\n",
      "2021-08-02 10:13:57,806 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:13:57,807 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:13:57,807 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:13:57,807 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep on conquering ” and completely conquering it ?\n",
      "2021-08-02 10:13:57,807 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   105000: bleu:  25.31, loss: 203301.4375, ppl:   5.3611, duration: 94.9820s\n",
      "2021-08-02 10:14:11,896 - INFO - joeynmt.training - Epoch  15, Step:   105100, Batch Loss:     1.724616, Tokens per Sec:    15598, Lr: 0.000300\n",
      "2021-08-02 10:14:25,718 - INFO - joeynmt.training - Epoch  15, Step:   105200, Batch Loss:     1.875331, Tokens per Sec:    15574, Lr: 0.000300\n",
      "2021-08-02 10:14:39,515 - INFO - joeynmt.training - Epoch  15, Step:   105300, Batch Loss:     1.939101, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-02 10:14:53,240 - INFO - joeynmt.training - Epoch  15, Step:   105400, Batch Loss:     1.950668, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-02 10:15:06,820 - INFO - joeynmt.training - Epoch  15, Step:   105500, Batch Loss:     1.788887, Tokens per Sec:    15628, Lr: 0.000300\n",
      "2021-08-02 10:15:20,541 - INFO - joeynmt.training - Epoch  15, Step:   105600, Batch Loss:     1.920666, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-08-02 10:15:34,357 - INFO - joeynmt.training - Epoch  15, Step:   105700, Batch Loss:     1.795138, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-08-02 10:15:48,215 - INFO - joeynmt.training - Epoch  15, Step:   105800, Batch Loss:     1.993977, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-02 10:16:02,241 - INFO - joeynmt.training - Epoch  15, Step:   105900, Batch Loss:     1.879453, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-08-02 10:16:15,875 - INFO - joeynmt.training - Epoch  15, Step:   106000, Batch Loss:     1.953271, Tokens per Sec:    15719, Lr: 0.000300\n",
      "2021-08-02 10:16:29,492 - INFO - joeynmt.training - Epoch  15, Step:   106100, Batch Loss:     1.840477, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-02 10:16:43,197 - INFO - joeynmt.training - Epoch  15, Step:   106200, Batch Loss:     1.944813, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-02 10:16:56,846 - INFO - joeynmt.training - Epoch  15, Step:   106300, Batch Loss:     1.852866, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-02 10:17:10,795 - INFO - joeynmt.training - Epoch  15, Step:   106400, Batch Loss:     1.579783, Tokens per Sec:    15907, Lr: 0.000300\n",
      "2021-08-02 10:17:24,696 - INFO - joeynmt.training - Epoch  15, Step:   106500, Batch Loss:     1.844109, Tokens per Sec:    16323, Lr: 0.000300\n",
      "2021-08-02 10:17:33,357 - INFO - joeynmt.training - Epoch  15: total training loss 9883.56\n",
      "2021-08-02 10:17:33,361 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-02 10:17:38,943 - INFO - joeynmt.training - Epoch  16, Step:   106600, Batch Loss:     1.819970, Tokens per Sec:    14540, Lr: 0.000300\n",
      "2021-08-02 10:17:52,688 - INFO - joeynmt.training - Epoch  16, Step:   106700, Batch Loss:     1.878528, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-08-02 10:18:06,439 - INFO - joeynmt.training - Epoch  16, Step:   106800, Batch Loss:     1.780964, Tokens per Sec:    15571, Lr: 0.000300\n",
      "2021-08-02 10:18:20,372 - INFO - joeynmt.training - Epoch  16, Step:   106900, Batch Loss:     1.781591, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-02 10:18:34,097 - INFO - joeynmt.training - Epoch  16, Step:   107000, Batch Loss:     1.741541, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-08-02 10:18:47,817 - INFO - joeynmt.training - Epoch  16, Step:   107100, Batch Loss:     1.745616, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-08-02 10:19:01,577 - INFO - joeynmt.training - Epoch  16, Step:   107200, Batch Loss:     2.134184, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-02 10:19:15,558 - INFO - joeynmt.training - Epoch  16, Step:   107300, Batch Loss:     1.845832, Tokens per Sec:    15737, Lr: 0.000300\n",
      "2021-08-02 10:19:29,350 - INFO - joeynmt.training - Epoch  16, Step:   107400, Batch Loss:     1.940084, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-08-02 10:19:43,131 - INFO - joeynmt.training - Epoch  16, Step:   107500, Batch Loss:     1.803276, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-08-02 10:19:56,735 - INFO - joeynmt.training - Epoch  16, Step:   107600, Batch Loss:     1.870016, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-02 10:20:10,704 - INFO - joeynmt.training - Epoch  16, Step:   107700, Batch Loss:     2.066620, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-02 10:20:24,673 - INFO - joeynmt.training - Epoch  16, Step:   107800, Batch Loss:     1.793524, Tokens per Sec:    15951, Lr: 0.000300\n",
      "2021-08-02 10:20:38,475 - INFO - joeynmt.training - Epoch  16, Step:   107900, Batch Loss:     1.984189, Tokens per Sec:    15852, Lr: 0.000300\n",
      "2021-08-02 10:20:52,225 - INFO - joeynmt.training - Epoch  16, Step:   108000, Batch Loss:     1.696222, Tokens per Sec:    16274, Lr: 0.000300\n",
      "2021-08-02 10:21:06,153 - INFO - joeynmt.training - Epoch  16, Step:   108100, Batch Loss:     1.848364, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-02 10:21:19,949 - INFO - joeynmt.training - Epoch  16, Step:   108200, Batch Loss:     1.974384, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-08-02 10:21:33,824 - INFO - joeynmt.training - Epoch  16, Step:   108300, Batch Loss:     1.879011, Tokens per Sec:    15586, Lr: 0.000300\n",
      "2021-08-02 10:21:47,561 - INFO - joeynmt.training - Epoch  16, Step:   108400, Batch Loss:     1.638007, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-08-02 10:22:01,512 - INFO - joeynmt.training - Epoch  16, Step:   108500, Batch Loss:     1.866744, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-08-02 10:22:15,292 - INFO - joeynmt.training - Epoch  16, Step:   108600, Batch Loss:     1.977554, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-08-02 10:22:29,089 - INFO - joeynmt.training - Epoch  16, Step:   108700, Batch Loss:     1.839565, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-02 10:22:42,832 - INFO - joeynmt.training - Epoch  16, Step:   108800, Batch Loss:     1.960875, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-08-02 10:22:56,788 - INFO - joeynmt.training - Epoch  16, Step:   108900, Batch Loss:     1.759936, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-08-02 10:23:10,708 - INFO - joeynmt.training - Epoch  16, Step:   109000, Batch Loss:     1.770785, Tokens per Sec:    15697, Lr: 0.000300\n",
      "2021-08-02 10:23:24,455 - INFO - joeynmt.training - Epoch  16, Step:   109100, Batch Loss:     1.779140, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-08-02 10:23:38,038 - INFO - joeynmt.training - Epoch  16, Step:   109200, Batch Loss:     1.704539, Tokens per Sec:    15622, Lr: 0.000300\n",
      "2021-08-02 10:23:51,659 - INFO - joeynmt.training - Epoch  16, Step:   109300, Batch Loss:     1.824130, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-02 10:24:05,598 - INFO - joeynmt.training - Epoch  16, Step:   109400, Batch Loss:     1.880754, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-08-02 10:24:19,502 - INFO - joeynmt.training - Epoch  16, Step:   109500, Batch Loss:     2.043597, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-08-02 10:24:33,156 - INFO - joeynmt.training - Epoch  16, Step:   109600, Batch Loss:     1.923853, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-08-02 10:24:46,945 - INFO - joeynmt.training - Epoch  16, Step:   109700, Batch Loss:     1.863927, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-08-02 10:25:00,732 - INFO - joeynmt.training - Epoch  16, Step:   109800, Batch Loss:     2.001525, Tokens per Sec:    16227, Lr: 0.000300\n",
      "2021-08-02 10:25:14,449 - INFO - joeynmt.training - Epoch  16, Step:   109900, Batch Loss:     2.079700, Tokens per Sec:    15726, Lr: 0.000300\n",
      "2021-08-02 10:25:28,135 - INFO - joeynmt.training - Epoch  16, Step:   110000, Batch Loss:     1.803487, Tokens per Sec:    15525, Lr: 0.000300\n",
      "2021-08-02 10:26:59,501 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:26:59,501 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:26:59,502 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:27:00,814 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:27:00,814 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:27:01,584 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:27:01,584 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:27:01,584 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:27:01,585 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so good for me . ”\n",
      "2021-08-02 10:27:01,585 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:27:01,585 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:27:01,585 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:27:01,586 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are living in fear of Jehovah is greater than many riches are standing . ”\n",
      "2021-08-02 10:27:01,586 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:27:01,586 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:27:01,586 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:27:01,587 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel tells us about God’s Kingdom .\n",
      "2021-08-02 10:27:01,587 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:27:01,587 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:27:01,587 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:27:01,588 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 10:27:01,588 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   110000: bleu:  25.40, loss: 202475.5156, ppl:   5.3247, duration: 93.4529s\n",
      "2021-08-02 10:27:15,665 - INFO - joeynmt.training - Epoch  16, Step:   110100, Batch Loss:     1.919261, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-08-02 10:27:29,240 - INFO - joeynmt.training - Epoch  16, Step:   110200, Batch Loss:     1.652340, Tokens per Sec:    15568, Lr: 0.000300\n",
      "2021-08-02 10:27:43,055 - INFO - joeynmt.training - Epoch  16, Step:   110300, Batch Loss:     1.755627, Tokens per Sec:    16206, Lr: 0.000300\n",
      "2021-08-02 10:27:56,961 - INFO - joeynmt.training - Epoch  16, Step:   110400, Batch Loss:     1.958619, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-08-02 10:28:10,759 - INFO - joeynmt.training - Epoch  16, Step:   110500, Batch Loss:     1.928332, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-08-02 10:28:24,432 - INFO - joeynmt.training - Epoch  16, Step:   110600, Batch Loss:     1.952278, Tokens per Sec:    15739, Lr: 0.000300\n",
      "2021-08-02 10:28:38,078 - INFO - joeynmt.training - Epoch  16, Step:   110700, Batch Loss:     1.850003, Tokens per Sec:    15898, Lr: 0.000300\n",
      "2021-08-02 10:28:51,728 - INFO - joeynmt.training - Epoch  16, Step:   110800, Batch Loss:     1.994242, Tokens per Sec:    15614, Lr: 0.000300\n",
      "2021-08-02 10:29:05,749 - INFO - joeynmt.training - Epoch  16, Step:   110900, Batch Loss:     1.691676, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-02 10:29:19,824 - INFO - joeynmt.training - Epoch  16, Step:   111000, Batch Loss:     1.896980, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-08-02 10:29:33,489 - INFO - joeynmt.training - Epoch  16, Step:   111100, Batch Loss:     1.839230, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-08-02 10:29:47,115 - INFO - joeynmt.training - Epoch  16, Step:   111200, Batch Loss:     1.737456, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-08-02 10:30:01,003 - INFO - joeynmt.training - Epoch  16, Step:   111300, Batch Loss:     1.868998, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-08-02 10:30:14,990 - INFO - joeynmt.training - Epoch  16, Step:   111400, Batch Loss:     1.719064, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-02 10:30:28,815 - INFO - joeynmt.training - Epoch  16, Step:   111500, Batch Loss:     2.007936, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-02 10:30:42,671 - INFO - joeynmt.training - Epoch  16, Step:   111600, Batch Loss:     1.738858, Tokens per Sec:    16030, Lr: 0.000300\n",
      "2021-08-02 10:30:56,669 - INFO - joeynmt.training - Epoch  16, Step:   111700, Batch Loss:     1.769813, Tokens per Sec:    16196, Lr: 0.000300\n",
      "2021-08-02 10:31:10,311 - INFO - joeynmt.training - Epoch  16, Step:   111800, Batch Loss:     1.918305, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-08-02 10:31:22,615 - INFO - joeynmt.training - Epoch  16: total training loss 9822.17\n",
      "2021-08-02 10:31:22,616 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-08-02 10:31:24,885 - INFO - joeynmt.training - Epoch  17, Step:   111900, Batch Loss:     1.798445, Tokens per Sec:    11363, Lr: 0.000300\n",
      "2021-08-02 10:31:38,724 - INFO - joeynmt.training - Epoch  17, Step:   112000, Batch Loss:     1.933193, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-02 10:31:52,356 - INFO - joeynmt.training - Epoch  17, Step:   112100, Batch Loss:     1.886206, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-02 10:32:06,181 - INFO - joeynmt.training - Epoch  17, Step:   112200, Batch Loss:     1.935445, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-02 10:32:19,925 - INFO - joeynmt.training - Epoch  17, Step:   112300, Batch Loss:     1.920040, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-02 10:32:33,814 - INFO - joeynmt.training - Epoch  17, Step:   112400, Batch Loss:     1.566614, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-08-02 10:32:47,619 - INFO - joeynmt.training - Epoch  17, Step:   112500, Batch Loss:     1.763188, Tokens per Sec:    15707, Lr: 0.000300\n",
      "2021-08-02 10:33:01,484 - INFO - joeynmt.training - Epoch  17, Step:   112600, Batch Loss:     1.700167, Tokens per Sec:    15695, Lr: 0.000300\n",
      "2021-08-02 10:33:15,173 - INFO - joeynmt.training - Epoch  17, Step:   112700, Batch Loss:     1.696986, Tokens per Sec:    15815, Lr: 0.000300\n",
      "2021-08-02 10:33:28,920 - INFO - joeynmt.training - Epoch  17, Step:   112800, Batch Loss:     1.616531, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-08-02 10:33:42,698 - INFO - joeynmt.training - Epoch  17, Step:   112900, Batch Loss:     1.889048, Tokens per Sec:    16299, Lr: 0.000300\n",
      "2021-08-02 10:33:56,559 - INFO - joeynmt.training - Epoch  17, Step:   113000, Batch Loss:     1.801159, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-08-02 10:34:10,396 - INFO - joeynmt.training - Epoch  17, Step:   113100, Batch Loss:     1.818431, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-02 10:34:24,086 - INFO - joeynmt.training - Epoch  17, Step:   113200, Batch Loss:     2.042249, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-08-02 10:34:37,948 - INFO - joeynmt.training - Epoch  17, Step:   113300, Batch Loss:     1.748319, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-08-02 10:34:51,746 - INFO - joeynmt.training - Epoch  17, Step:   113400, Batch Loss:     1.511151, Tokens per Sec:    16072, Lr: 0.000300\n",
      "2021-08-02 10:35:05,726 - INFO - joeynmt.training - Epoch  17, Step:   113500, Batch Loss:     1.837239, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-02 10:35:19,570 - INFO - joeynmt.training - Epoch  17, Step:   113600, Batch Loss:     1.663737, Tokens per Sec:    15748, Lr: 0.000300\n",
      "2021-08-02 10:35:33,341 - INFO - joeynmt.training - Epoch  17, Step:   113700, Batch Loss:     1.752353, Tokens per Sec:    15700, Lr: 0.000300\n",
      "2021-08-02 10:35:47,235 - INFO - joeynmt.training - Epoch  17, Step:   113800, Batch Loss:     1.817648, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-02 10:36:01,034 - INFO - joeynmt.training - Epoch  17, Step:   113900, Batch Loss:     1.896452, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-08-02 10:36:14,923 - INFO - joeynmt.training - Epoch  17, Step:   114000, Batch Loss:     1.882582, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-08-02 10:36:28,488 - INFO - joeynmt.training - Epoch  17, Step:   114100, Batch Loss:     1.869685, Tokens per Sec:    15675, Lr: 0.000300\n",
      "2021-08-02 10:36:42,467 - INFO - joeynmt.training - Epoch  17, Step:   114200, Batch Loss:     1.766196, Tokens per Sec:    16279, Lr: 0.000300\n",
      "2021-08-02 10:36:56,185 - INFO - joeynmt.training - Epoch  17, Step:   114300, Batch Loss:     1.858046, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-02 10:37:10,013 - INFO - joeynmt.training - Epoch  17, Step:   114400, Batch Loss:     1.600450, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-08-02 10:37:23,927 - INFO - joeynmt.training - Epoch  17, Step:   114500, Batch Loss:     1.737632, Tokens per Sec:    15995, Lr: 0.000300\n",
      "2021-08-02 10:37:37,691 - INFO - joeynmt.training - Epoch  17, Step:   114600, Batch Loss:     1.840358, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-08-02 10:37:51,656 - INFO - joeynmt.training - Epoch  17, Step:   114700, Batch Loss:     1.798405, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-08-02 10:38:05,381 - INFO - joeynmt.training - Epoch  17, Step:   114800, Batch Loss:     1.757863, Tokens per Sec:    15713, Lr: 0.000300\n",
      "2021-08-02 10:38:19,099 - INFO - joeynmt.training - Epoch  17, Step:   114900, Batch Loss:     1.835739, Tokens per Sec:    15691, Lr: 0.000300\n",
      "2021-08-02 10:38:32,771 - INFO - joeynmt.training - Epoch  17, Step:   115000, Batch Loss:     1.895160, Tokens per Sec:    15838, Lr: 0.000300\n",
      "2021-08-02 10:40:08,520 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:40:08,521 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:40:08,521 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:40:09,856 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:40:09,856 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:40:10,594 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:40:10,594 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:40:10,595 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:40:10,595 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , so good for me . ”\n",
      "2021-08-02 10:40:10,595 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:40:10,595 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:40:10,596 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:40:10,596 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our resident is godly devotion to Jehovah , we are more than the riches that are standing . ”\n",
      "2021-08-02 10:40:10,596 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:40:10,596 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:40:10,597 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:40:10,597 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has a detailed discussion of God’s Kingdom .\n",
      "2021-08-02 10:40:10,597 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:40:10,597 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:40:10,597 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:40:10,598 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to overcome ” and completely conquer ?\n",
      "2021-08-02 10:40:10,598 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   115000: bleu:  25.65, loss: 200951.4688, ppl:   5.2581, duration: 97.8262s\n",
      "2021-08-02 10:40:24,585 - INFO - joeynmt.training - Epoch  17, Step:   115100, Batch Loss:     1.690365, Tokens per Sec:    15491, Lr: 0.000300\n",
      "2021-08-02 10:40:38,479 - INFO - joeynmt.training - Epoch  17, Step:   115200, Batch Loss:     1.911546, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-02 10:40:52,126 - INFO - joeynmt.training - Epoch  17, Step:   115300, Batch Loss:     1.830594, Tokens per Sec:    15815, Lr: 0.000300\n",
      "2021-08-02 10:41:05,697 - INFO - joeynmt.training - Epoch  17, Step:   115400, Batch Loss:     1.788405, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-08-02 10:41:19,632 - INFO - joeynmt.training - Epoch  17, Step:   115500, Batch Loss:     1.692151, Tokens per Sec:    15659, Lr: 0.000300\n",
      "2021-08-02 10:41:33,485 - INFO - joeynmt.training - Epoch  17, Step:   115600, Batch Loss:     1.888547, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-08-02 10:41:47,259 - INFO - joeynmt.training - Epoch  17, Step:   115700, Batch Loss:     1.879950, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-08-02 10:42:01,036 - INFO - joeynmt.training - Epoch  17, Step:   115800, Batch Loss:     1.924341, Tokens per Sec:    16301, Lr: 0.000300\n",
      "2021-08-02 10:42:14,695 - INFO - joeynmt.training - Epoch  17, Step:   115900, Batch Loss:     1.793256, Tokens per Sec:    15766, Lr: 0.000300\n",
      "2021-08-02 10:42:28,635 - INFO - joeynmt.training - Epoch  17, Step:   116000, Batch Loss:     1.864396, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-08-02 10:42:42,674 - INFO - joeynmt.training - Epoch  17, Step:   116100, Batch Loss:     1.819703, Tokens per Sec:    16288, Lr: 0.000300\n",
      "2021-08-02 10:42:56,477 - INFO - joeynmt.training - Epoch  17, Step:   116200, Batch Loss:     1.950997, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-02 10:43:10,114 - INFO - joeynmt.training - Epoch  17, Step:   116300, Batch Loss:     1.688026, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-08-02 10:43:24,107 - INFO - joeynmt.training - Epoch  17, Step:   116400, Batch Loss:     1.801811, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-08-02 10:43:38,321 - INFO - joeynmt.training - Epoch  17, Step:   116500, Batch Loss:     1.512362, Tokens per Sec:    15692, Lr: 0.000300\n",
      "2021-08-02 10:43:52,157 - INFO - joeynmt.training - Epoch  17, Step:   116600, Batch Loss:     1.675473, Tokens per Sec:    15598, Lr: 0.000300\n",
      "2021-08-02 10:44:06,073 - INFO - joeynmt.training - Epoch  17, Step:   116700, Batch Loss:     1.705067, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-02 10:44:20,287 - INFO - joeynmt.training - Epoch  17, Step:   116800, Batch Loss:     1.810365, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-08-02 10:44:33,959 - INFO - joeynmt.training - Epoch  17, Step:   116900, Batch Loss:     2.126014, Tokens per Sec:    15794, Lr: 0.000300\n",
      "2021-08-02 10:44:47,598 - INFO - joeynmt.training - Epoch  17, Step:   117000, Batch Loss:     1.771905, Tokens per Sec:    15613, Lr: 0.000300\n",
      "2021-08-02 10:45:01,413 - INFO - joeynmt.training - Epoch  17, Step:   117100, Batch Loss:     1.834902, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-08-02 10:45:15,301 - INFO - joeynmt.training - Epoch  17, Step:   117200, Batch Loss:     1.925889, Tokens per Sec:    15634, Lr: 0.000300\n",
      "2021-08-02 10:45:18,426 - INFO - joeynmt.training - Epoch  17: total training loss 9794.90\n",
      "2021-08-02 10:45:18,426 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-08-02 10:45:29,641 - INFO - joeynmt.training - Epoch  18, Step:   117300, Batch Loss:     1.682404, Tokens per Sec:    14943, Lr: 0.000300\n",
      "2021-08-02 10:45:43,525 - INFO - joeynmt.training - Epoch  18, Step:   117400, Batch Loss:     1.711436, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-08-02 10:45:57,157 - INFO - joeynmt.training - Epoch  18, Step:   117500, Batch Loss:     1.660856, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-02 10:46:11,140 - INFO - joeynmt.training - Epoch  18, Step:   117600, Batch Loss:     1.735854, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-02 10:46:24,969 - INFO - joeynmt.training - Epoch  18, Step:   117700, Batch Loss:     1.812678, Tokens per Sec:    15672, Lr: 0.000300\n",
      "2021-08-02 10:46:38,606 - INFO - joeynmt.training - Epoch  18, Step:   117800, Batch Loss:     1.793957, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-08-02 10:46:52,246 - INFO - joeynmt.training - Epoch  18, Step:   117900, Batch Loss:     1.674689, Tokens per Sec:    15676, Lr: 0.000300\n",
      "2021-08-02 10:47:06,135 - INFO - joeynmt.training - Epoch  18, Step:   118000, Batch Loss:     1.804683, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-08-02 10:47:20,137 - INFO - joeynmt.training - Epoch  18, Step:   118100, Batch Loss:     1.855352, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-02 10:47:34,055 - INFO - joeynmt.training - Epoch  18, Step:   118200, Batch Loss:     1.899425, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-08-02 10:47:47,820 - INFO - joeynmt.training - Epoch  18, Step:   118300, Batch Loss:     2.044580, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-08-02 10:48:01,489 - INFO - joeynmt.training - Epoch  18, Step:   118400, Batch Loss:     1.929109, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-08-02 10:48:15,209 - INFO - joeynmt.training - Epoch  18, Step:   118500, Batch Loss:     2.050534, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-08-02 10:48:28,910 - INFO - joeynmt.training - Epoch  18, Step:   118600, Batch Loss:     1.850878, Tokens per Sec:    15575, Lr: 0.000300\n",
      "2021-08-02 10:48:42,582 - INFO - joeynmt.training - Epoch  18, Step:   118700, Batch Loss:     1.945318, Tokens per Sec:    15600, Lr: 0.000300\n",
      "2021-08-02 10:48:56,426 - INFO - joeynmt.training - Epoch  18, Step:   118800, Batch Loss:     1.746696, Tokens per Sec:    15796, Lr: 0.000300\n",
      "2021-08-02 10:49:10,351 - INFO - joeynmt.training - Epoch  18, Step:   118900, Batch Loss:     1.850508, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-08-02 10:49:24,037 - INFO - joeynmt.training - Epoch  18, Step:   119000, Batch Loss:     1.823848, Tokens per Sec:    15833, Lr: 0.000300\n",
      "2021-08-02 10:49:37,892 - INFO - joeynmt.training - Epoch  18, Step:   119100, Batch Loss:     1.961314, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-08-02 10:49:51,652 - INFO - joeynmt.training - Epoch  18, Step:   119200, Batch Loss:     1.864292, Tokens per Sec:    15939, Lr: 0.000300\n",
      "2021-08-02 10:50:05,583 - INFO - joeynmt.training - Epoch  18, Step:   119300, Batch Loss:     1.838043, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-02 10:50:19,443 - INFO - joeynmt.training - Epoch  18, Step:   119400, Batch Loss:     1.783437, Tokens per Sec:    15760, Lr: 0.000300\n",
      "2021-08-02 10:50:33,387 - INFO - joeynmt.training - Epoch  18, Step:   119500, Batch Loss:     1.805363, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-02 10:50:47,188 - INFO - joeynmt.training - Epoch  18, Step:   119600, Batch Loss:     1.615349, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-02 10:51:00,972 - INFO - joeynmt.training - Epoch  18, Step:   119700, Batch Loss:     1.808099, Tokens per Sec:    16194, Lr: 0.000300\n",
      "2021-08-02 10:51:14,913 - INFO - joeynmt.training - Epoch  18, Step:   119800, Batch Loss:     1.813513, Tokens per Sec:    15715, Lr: 0.000300\n",
      "2021-08-02 10:51:28,587 - INFO - joeynmt.training - Epoch  18, Step:   119900, Batch Loss:     1.764353, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-08-02 10:51:42,225 - INFO - joeynmt.training - Epoch  18, Step:   120000, Batch Loss:     1.769690, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-08-02 10:53:16,848 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:53:16,848 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:53:16,848 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:53:18,175 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:53:18,176 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:53:18,929 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:53:18,929 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:53:18,930 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:53:18,930 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , so good for me . ”\n",
      "2021-08-02 10:53:18,930 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:53:18,930 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:53:18,931 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:53:18,931 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion is greater than the riches that are standing . ”\n",
      "2021-08-02 10:53:18,931 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:53:18,931 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:53:18,932 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:53:18,932 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of the book of Daniel tells us about God’s Kingdom .\n",
      "2021-08-02 10:53:18,932 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:53:18,932 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:53:18,933 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:53:18,933 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-02 10:53:18,933 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   120000: bleu:  25.61, loss: 199999.8438, ppl:   5.2169, duration: 96.7076s\n",
      "2021-08-02 10:53:32,956 - INFO - joeynmt.training - Epoch  18, Step:   120100, Batch Loss:     1.928824, Tokens per Sec:    15750, Lr: 0.000300\n",
      "2021-08-02 10:53:46,598 - INFO - joeynmt.training - Epoch  18, Step:   120200, Batch Loss:     1.360553, Tokens per Sec:    15671, Lr: 0.000300\n",
      "2021-08-02 10:54:00,288 - INFO - joeynmt.training - Epoch  18, Step:   120300, Batch Loss:     1.874830, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-08-02 10:54:14,000 - INFO - joeynmt.training - Epoch  18, Step:   120400, Batch Loss:     1.776939, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-08-02 10:54:27,738 - INFO - joeynmt.training - Epoch  18, Step:   120500, Batch Loss:     1.739997, Tokens per Sec:    15609, Lr: 0.000300\n",
      "2021-08-02 10:54:41,569 - INFO - joeynmt.training - Epoch  18, Step:   120600, Batch Loss:     1.974393, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-02 10:54:55,270 - INFO - joeynmt.training - Epoch  18, Step:   120700, Batch Loss:     1.417089, Tokens per Sec:    16035, Lr: 0.000300\n",
      "2021-08-02 10:55:09,149 - INFO - joeynmt.training - Epoch  18, Step:   120800, Batch Loss:     1.921870, Tokens per Sec:    15608, Lr: 0.000300\n",
      "2021-08-02 10:55:22,780 - INFO - joeynmt.training - Epoch  18, Step:   120900, Batch Loss:     1.938171, Tokens per Sec:    15708, Lr: 0.000300\n",
      "2021-08-02 10:55:36,710 - INFO - joeynmt.training - Epoch  18, Step:   121000, Batch Loss:     1.684592, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-08-02 10:55:50,421 - INFO - joeynmt.training - Epoch  18, Step:   121100, Batch Loss:     1.755302, Tokens per Sec:    15685, Lr: 0.000300\n",
      "2021-08-02 10:56:04,355 - INFO - joeynmt.training - Epoch  18, Step:   121200, Batch Loss:     1.859793, Tokens per Sec:    16042, Lr: 0.000300\n",
      "2021-08-02 10:56:18,246 - INFO - joeynmt.training - Epoch  18, Step:   121300, Batch Loss:     1.786962, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-08-02 10:56:31,848 - INFO - joeynmt.training - Epoch  18, Step:   121400, Batch Loss:     1.811140, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-08-02 10:56:45,691 - INFO - joeynmt.training - Epoch  18, Step:   121500, Batch Loss:     1.774511, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-08-02 10:56:59,421 - INFO - joeynmt.training - Epoch  18, Step:   121600, Batch Loss:     1.796400, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-02 10:57:13,296 - INFO - joeynmt.training - Epoch  18, Step:   121700, Batch Loss:     1.697595, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-02 10:57:27,098 - INFO - joeynmt.training - Epoch  18, Step:   121800, Batch Loss:     1.702815, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-08-02 10:57:40,695 - INFO - joeynmt.training - Epoch  18, Step:   121900, Batch Loss:     1.769932, Tokens per Sec:    15633, Lr: 0.000300\n",
      "2021-08-02 10:57:54,586 - INFO - joeynmt.training - Epoch  18, Step:   122000, Batch Loss:     1.851419, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-08-02 10:58:08,419 - INFO - joeynmt.training - Epoch  18, Step:   122100, Batch Loss:     1.769130, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-02 10:58:22,322 - INFO - joeynmt.training - Epoch  18, Step:   122200, Batch Loss:     1.809465, Tokens per Sec:    15578, Lr: 0.000300\n",
      "2021-08-02 10:58:36,195 - INFO - joeynmt.training - Epoch  18, Step:   122300, Batch Loss:     1.800169, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-02 10:58:49,866 - INFO - joeynmt.training - Epoch  18, Step:   122400, Batch Loss:     1.645133, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-08-02 10:59:03,678 - INFO - joeynmt.training - Epoch  18, Step:   122500, Batch Loss:     1.699652, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-02 10:59:11,639 - INFO - joeynmt.training - Epoch  18: total training loss 9736.48\n",
      "2021-08-02 10:59:11,639 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-08-02 10:59:18,235 - INFO - joeynmt.training - Epoch  19, Step:   122600, Batch Loss:     1.902839, Tokens per Sec:    14767, Lr: 0.000300\n",
      "2021-08-02 10:59:31,929 - INFO - joeynmt.training - Epoch  19, Step:   122700, Batch Loss:     2.083964, Tokens per Sec:    15596, Lr: 0.000300\n",
      "2021-08-02 10:59:45,687 - INFO - joeynmt.training - Epoch  19, Step:   122800, Batch Loss:     1.938440, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-08-02 10:59:59,506 - INFO - joeynmt.training - Epoch  19, Step:   122900, Batch Loss:     1.887803, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-08-02 11:00:13,261 - INFO - joeynmt.training - Epoch  19, Step:   123000, Batch Loss:     1.896864, Tokens per Sec:    15801, Lr: 0.000300\n",
      "2021-08-02 11:00:27,105 - INFO - joeynmt.training - Epoch  19, Step:   123100, Batch Loss:     1.880892, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-02 11:00:40,781 - INFO - joeynmt.training - Epoch  19, Step:   123200, Batch Loss:     1.683864, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-08-02 11:00:54,634 - INFO - joeynmt.training - Epoch  19, Step:   123300, Batch Loss:     1.894741, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-02 11:01:08,626 - INFO - joeynmt.training - Epoch  19, Step:   123400, Batch Loss:     1.859876, Tokens per Sec:    15799, Lr: 0.000300\n",
      "2021-08-02 11:01:22,496 - INFO - joeynmt.training - Epoch  19, Step:   123500, Batch Loss:     1.714138, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-08-02 11:01:36,238 - INFO - joeynmt.training - Epoch  19, Step:   123600, Batch Loss:     1.686544, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-02 11:01:49,972 - INFO - joeynmt.training - Epoch  19, Step:   123700, Batch Loss:     1.670591, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-02 11:02:03,693 - INFO - joeynmt.training - Epoch  19, Step:   123800, Batch Loss:     1.863275, Tokens per Sec:    15774, Lr: 0.000300\n",
      "2021-08-02 11:02:17,522 - INFO - joeynmt.training - Epoch  19, Step:   123900, Batch Loss:     1.605303, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-08-02 11:02:31,617 - INFO - joeynmt.training - Epoch  19, Step:   124000, Batch Loss:     1.652780, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-02 11:02:45,264 - INFO - joeynmt.training - Epoch  19, Step:   124100, Batch Loss:     1.782134, Tokens per Sec:    15846, Lr: 0.000300\n",
      "2021-08-02 11:02:59,029 - INFO - joeynmt.training - Epoch  19, Step:   124200, Batch Loss:     1.738230, Tokens per Sec:    15852, Lr: 0.000300\n",
      "2021-08-02 11:03:12,997 - INFO - joeynmt.training - Epoch  19, Step:   124300, Batch Loss:     1.791493, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-02 11:03:26,721 - INFO - joeynmt.training - Epoch  19, Step:   124400, Batch Loss:     1.761546, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-08-02 11:03:40,340 - INFO - joeynmt.training - Epoch  19, Step:   124500, Batch Loss:     1.846601, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-02 11:03:53,968 - INFO - joeynmt.training - Epoch  19, Step:   124600, Batch Loss:     1.650643, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-02 11:04:07,757 - INFO - joeynmt.training - Epoch  19, Step:   124700, Batch Loss:     1.870138, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-08-02 11:04:21,572 - INFO - joeynmt.training - Epoch  19, Step:   124800, Batch Loss:     1.998714, Tokens per Sec:    15676, Lr: 0.000300\n",
      "2021-08-02 11:04:35,346 - INFO - joeynmt.training - Epoch  19, Step:   124900, Batch Loss:     1.739862, Tokens per Sec:    15581, Lr: 0.000300\n",
      "2021-08-02 11:04:49,056 - INFO - joeynmt.training - Epoch  19, Step:   125000, Batch Loss:     1.702640, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-08-02 11:06:23,272 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:06:23,273 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:06:23,273 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:06:24,497 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:06:24,497 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:06:25,231 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:06:25,232 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:06:25,232 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:06:25,232 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am drawing close to God , so good for me . ”\n",
      "2021-08-02 11:06:25,232 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:06:25,233 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:06:25,233 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:06:25,233 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion is greater than the riches that are standing . ”\n",
      "2021-08-02 11:06:25,234 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:06:25,234 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:06:25,234 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:06:25,234 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel says about God’s Kingdom .\n",
      "2021-08-02 11:06:25,234 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:06:25,235 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:06:25,235 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:06:25,235 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to overcome ” and completely conquer ?\n",
      "2021-08-02 11:06:25,235 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   125000: bleu:  25.84, loss: 199043.4219, ppl:   5.1759, duration: 96.1794s\n",
      "2021-08-02 11:06:39,163 - INFO - joeynmt.training - Epoch  19, Step:   125100, Batch Loss:     1.879210, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-08-02 11:06:53,087 - INFO - joeynmt.training - Epoch  19, Step:   125200, Batch Loss:     1.863992, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-08-02 11:07:06,967 - INFO - joeynmt.training - Epoch  19, Step:   125300, Batch Loss:     2.030277, Tokens per Sec:    15731, Lr: 0.000300\n",
      "2021-08-02 11:07:20,776 - INFO - joeynmt.training - Epoch  19, Step:   125400, Batch Loss:     1.854258, Tokens per Sec:    15882, Lr: 0.000300\n",
      "2021-08-02 11:07:34,457 - INFO - joeynmt.training - Epoch  19, Step:   125500, Batch Loss:     1.945013, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-02 11:07:48,232 - INFO - joeynmt.training - Epoch  19, Step:   125600, Batch Loss:     1.644624, Tokens per Sec:    15872, Lr: 0.000300\n",
      "2021-08-02 11:08:01,895 - INFO - joeynmt.training - Epoch  19, Step:   125700, Batch Loss:     1.812433, Tokens per Sec:    15619, Lr: 0.000300\n",
      "2021-08-02 11:08:15,701 - INFO - joeynmt.training - Epoch  19, Step:   125800, Batch Loss:     1.791521, Tokens per Sec:    15678, Lr: 0.000300\n",
      "2021-08-02 11:08:29,620 - INFO - joeynmt.training - Epoch  19, Step:   125900, Batch Loss:     1.581317, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-02 11:08:43,469 - INFO - joeynmt.training - Epoch  19, Step:   126000, Batch Loss:     1.929834, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-02 11:08:57,270 - INFO - joeynmt.training - Epoch  19, Step:   126100, Batch Loss:     1.653325, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-08-02 11:09:11,219 - INFO - joeynmt.training - Epoch  19, Step:   126200, Batch Loss:     2.051229, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-08-02 11:09:24,780 - INFO - joeynmt.training - Epoch  19, Step:   126300, Batch Loss:     1.754723, Tokens per Sec:    15595, Lr: 0.000300\n",
      "2021-08-02 11:09:38,646 - INFO - joeynmt.training - Epoch  19, Step:   126400, Batch Loss:     1.869629, Tokens per Sec:    16321, Lr: 0.000300\n",
      "2021-08-02 11:09:52,217 - INFO - joeynmt.training - Epoch  19, Step:   126500, Batch Loss:     1.715234, Tokens per Sec:    15928, Lr: 0.000300\n",
      "2021-08-02 11:10:06,087 - INFO - joeynmt.training - Epoch  19, Step:   126600, Batch Loss:     1.699256, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-02 11:10:20,058 - INFO - joeynmt.training - Epoch  19, Step:   126700, Batch Loss:     1.721153, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-08-02 11:10:33,884 - INFO - joeynmt.training - Epoch  19, Step:   126800, Batch Loss:     1.995893, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-02 11:10:47,754 - INFO - joeynmt.training - Epoch  19, Step:   126900, Batch Loss:     1.746836, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-02 11:11:01,580 - INFO - joeynmt.training - Epoch  19, Step:   127000, Batch Loss:     1.817692, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-08-02 11:11:15,181 - INFO - joeynmt.training - Epoch  19, Step:   127100, Batch Loss:     1.946948, Tokens per Sec:    15325, Lr: 0.000300\n",
      "2021-08-02 11:11:29,093 - INFO - joeynmt.training - Epoch  19, Step:   127200, Batch Loss:     1.661268, Tokens per Sec:    16015, Lr: 0.000300\n",
      "2021-08-02 11:11:42,883 - INFO - joeynmt.training - Epoch  19, Step:   127300, Batch Loss:     1.771828, Tokens per Sec:    16209, Lr: 0.000300\n",
      "2021-08-02 11:11:56,419 - INFO - joeynmt.training - Epoch  19, Step:   127400, Batch Loss:     1.800304, Tokens per Sec:    15721, Lr: 0.000300\n",
      "2021-08-02 11:12:10,410 - INFO - joeynmt.training - Epoch  19, Step:   127500, Batch Loss:     1.679616, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-02 11:12:24,350 - INFO - joeynmt.training - Epoch  19, Step:   127600, Batch Loss:     1.838138, Tokens per Sec:    16182, Lr: 0.000300\n",
      "2021-08-02 11:12:38,167 - INFO - joeynmt.training - Epoch  19, Step:   127700, Batch Loss:     2.104684, Tokens per Sec:    16195, Lr: 0.000300\n",
      "2021-08-02 11:12:51,959 - INFO - joeynmt.training - Epoch  19, Step:   127800, Batch Loss:     1.688087, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-02 11:13:04,208 - INFO - joeynmt.training - Epoch  19: total training loss 9697.42\n",
      "2021-08-02 11:13:04,209 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-08-02 11:13:06,303 - INFO - joeynmt.training - Epoch  20, Step:   127900, Batch Loss:     1.801612, Tokens per Sec:    11439, Lr: 0.000300\n",
      "2021-08-02 11:13:20,225 - INFO - joeynmt.training - Epoch  20, Step:   128000, Batch Loss:     1.873495, Tokens per Sec:    15850, Lr: 0.000300\n",
      "2021-08-02 11:13:33,948 - INFO - joeynmt.training - Epoch  20, Step:   128100, Batch Loss:     1.661246, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-08-02 11:13:47,684 - INFO - joeynmt.training - Epoch  20, Step:   128200, Batch Loss:     1.910753, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-02 11:14:01,342 - INFO - joeynmt.training - Epoch  20, Step:   128300, Batch Loss:     1.577446, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-08-02 11:14:15,394 - INFO - joeynmt.training - Epoch  20, Step:   128400, Batch Loss:     1.886997, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-02 11:14:29,243 - INFO - joeynmt.training - Epoch  20, Step:   128500, Batch Loss:     1.892642, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-08-02 11:14:42,924 - INFO - joeynmt.training - Epoch  20, Step:   128600, Batch Loss:     1.574578, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-02 11:14:56,701 - INFO - joeynmt.training - Epoch  20, Step:   128700, Batch Loss:     1.646614, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-02 11:15:10,294 - INFO - joeynmt.training - Epoch  20, Step:   128800, Batch Loss:     1.805354, Tokens per Sec:    15928, Lr: 0.000300\n",
      "2021-08-02 11:15:24,117 - INFO - joeynmt.training - Epoch  20, Step:   128900, Batch Loss:     1.820135, Tokens per Sec:    15656, Lr: 0.000300\n",
      "2021-08-02 11:15:38,059 - INFO - joeynmt.training - Epoch  20, Step:   129000, Batch Loss:     1.795005, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-08-02 11:15:51,740 - INFO - joeynmt.training - Epoch  20, Step:   129100, Batch Loss:     1.826807, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-08-02 11:16:05,510 - INFO - joeynmt.training - Epoch  20, Step:   129200, Batch Loss:     1.666956, Tokens per Sec:    16030, Lr: 0.000300\n",
      "2021-08-02 11:16:19,245 - INFO - joeynmt.training - Epoch  20, Step:   129300, Batch Loss:     1.860147, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-08-02 11:16:33,050 - INFO - joeynmt.training - Epoch  20, Step:   129400, Batch Loss:     1.839885, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-08-02 11:16:46,993 - INFO - joeynmt.training - Epoch  20, Step:   129500, Batch Loss:     1.748588, Tokens per Sec:    15995, Lr: 0.000300\n",
      "2021-08-02 11:17:01,049 - INFO - joeynmt.training - Epoch  20, Step:   129600, Batch Loss:     1.797882, Tokens per Sec:    16188, Lr: 0.000300\n",
      "2021-08-02 11:17:14,971 - INFO - joeynmt.training - Epoch  20, Step:   129700, Batch Loss:     1.843058, Tokens per Sec:    15801, Lr: 0.000300\n",
      "2021-08-02 11:17:28,745 - INFO - joeynmt.training - Epoch  20, Step:   129800, Batch Loss:     2.060636, Tokens per Sec:    16218, Lr: 0.000300\n",
      "2021-08-02 11:17:42,498 - INFO - joeynmt.training - Epoch  20, Step:   129900, Batch Loss:     1.703284, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-02 11:17:55,982 - INFO - joeynmt.training - Epoch  20, Step:   130000, Batch Loss:     1.872231, Tokens per Sec:    15745, Lr: 0.000300\n",
      "2021-08-02 11:19:32,768 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:19:32,768 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:19:32,769 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:19:34,715 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:19:34,716 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:19:34,716 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:19:34,717 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so good for me . ”\n",
      "2021-08-02 11:19:34,717 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:19:34,717 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:19:34,717 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:19:34,718 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion to Jehovah is greater than many riches are standing . ”\n",
      "2021-08-02 11:19:34,718 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:19:34,718 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:19:34,718 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:19:34,718 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has something about God’s Kingdom .\n",
      "2021-08-02 11:19:34,719 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:19:34,719 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:19:34,720 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:19:34,720 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-02 11:19:34,720 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   130000: bleu:  25.84, loss: 199432.6875, ppl:   5.1925, duration: 98.7380s\n",
      "2021-08-02 11:19:48,757 - INFO - joeynmt.training - Epoch  20, Step:   130100, Batch Loss:     1.643846, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-08-02 11:20:02,490 - INFO - joeynmt.training - Epoch  20, Step:   130200, Batch Loss:     1.794998, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-02 11:20:16,417 - INFO - joeynmt.training - Epoch  20, Step:   130300, Batch Loss:     1.721327, Tokens per Sec:    15926, Lr: 0.000300\n",
      "2021-08-02 11:20:30,144 - INFO - joeynmt.training - Epoch  20, Step:   130400, Batch Loss:     1.699091, Tokens per Sec:    15846, Lr: 0.000300\n",
      "2021-08-02 11:20:43,915 - INFO - joeynmt.training - Epoch  20, Step:   130500, Batch Loss:     1.744844, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-02 11:20:57,571 - INFO - joeynmt.training - Epoch  20, Step:   130600, Batch Loss:     1.781331, Tokens per Sec:    15716, Lr: 0.000300\n",
      "2021-08-02 11:21:11,538 - INFO - joeynmt.training - Epoch  20, Step:   130700, Batch Loss:     1.690917, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-02 11:21:25,154 - INFO - joeynmt.training - Epoch  20, Step:   130800, Batch Loss:     1.729916, Tokens per Sec:    15525, Lr: 0.000300\n",
      "2021-08-02 11:21:39,209 - INFO - joeynmt.training - Epoch  20, Step:   130900, Batch Loss:     2.050433, Tokens per Sec:    15919, Lr: 0.000300\n",
      "2021-08-02 11:21:52,979 - INFO - joeynmt.training - Epoch  20, Step:   131000, Batch Loss:     1.830805, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-08-02 11:22:06,948 - INFO - joeynmt.training - Epoch  20, Step:   131100, Batch Loss:     1.771074, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-02 11:22:20,841 - INFO - joeynmt.training - Epoch  20, Step:   131200, Batch Loss:     1.771604, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-08-02 11:22:34,671 - INFO - joeynmt.training - Epoch  20, Step:   131300, Batch Loss:     1.874957, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-02 11:22:48,435 - INFO - joeynmt.training - Epoch  20, Step:   131400, Batch Loss:     1.848011, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-08-02 11:23:02,247 - INFO - joeynmt.training - Epoch  20, Step:   131500, Batch Loss:     1.790552, Tokens per Sec:    15515, Lr: 0.000300\n",
      "2021-08-02 11:23:16,241 - INFO - joeynmt.training - Epoch  20, Step:   131600, Batch Loss:     1.876296, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-02 11:23:29,895 - INFO - joeynmt.training - Epoch  20, Step:   131700, Batch Loss:     1.702736, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-02 11:23:43,428 - INFO - joeynmt.training - Epoch  20, Step:   131800, Batch Loss:     1.636269, Tokens per Sec:    15707, Lr: 0.000300\n",
      "2021-08-02 11:23:56,857 - INFO - joeynmt.training - Epoch  20, Step:   131900, Batch Loss:     1.683465, Tokens per Sec:    15553, Lr: 0.000300\n",
      "2021-08-02 11:24:10,683 - INFO - joeynmt.training - Epoch  20, Step:   132000, Batch Loss:     1.825906, Tokens per Sec:    15641, Lr: 0.000300\n",
      "2021-08-02 11:24:24,477 - INFO - joeynmt.training - Epoch  20, Step:   132100, Batch Loss:     1.721696, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-08-02 11:24:38,215 - INFO - joeynmt.training - Epoch  20, Step:   132200, Batch Loss:     1.766721, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-02 11:24:52,030 - INFO - joeynmt.training - Epoch  20, Step:   132300, Batch Loss:     1.869022, Tokens per Sec:    16218, Lr: 0.000300\n",
      "2021-08-02 11:25:05,800 - INFO - joeynmt.training - Epoch  20, Step:   132400, Batch Loss:     1.801499, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-02 11:25:19,694 - INFO - joeynmt.training - Epoch  20, Step:   132500, Batch Loss:     1.832592, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-08-02 11:25:33,508 - INFO - joeynmt.training - Epoch  20, Step:   132600, Batch Loss:     1.936091, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-02 11:25:47,296 - INFO - joeynmt.training - Epoch  20, Step:   132700, Batch Loss:     1.824718, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-08-02 11:26:01,138 - INFO - joeynmt.training - Epoch  20, Step:   132800, Batch Loss:     1.821615, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-08-02 11:26:15,048 - INFO - joeynmt.training - Epoch  20, Step:   132900, Batch Loss:     1.835683, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-02 11:26:28,873 - INFO - joeynmt.training - Epoch  20, Step:   133000, Batch Loss:     1.579520, Tokens per Sec:    15566, Lr: 0.000300\n",
      "2021-08-02 11:26:42,477 - INFO - joeynmt.training - Epoch  20, Step:   133100, Batch Loss:     1.950513, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-08-02 11:26:56,459 - INFO - joeynmt.training - Epoch  20, Step:   133200, Batch Loss:     1.931749, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-02 11:26:59,371 - INFO - joeynmt.training - Epoch  20: total training loss 9648.96\n",
      "2021-08-02 11:26:59,371 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-08-02 11:27:10,922 - INFO - joeynmt.training - Epoch  21, Step:   133300, Batch Loss:     1.472495, Tokens per Sec:    15179, Lr: 0.000300\n",
      "2021-08-02 11:27:24,603 - INFO - joeynmt.training - Epoch  21, Step:   133400, Batch Loss:     1.633447, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-08-02 11:27:38,360 - INFO - joeynmt.training - Epoch  21, Step:   133500, Batch Loss:     1.552137, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-08-02 11:27:51,950 - INFO - joeynmt.training - Epoch  21, Step:   133600, Batch Loss:     1.799390, Tokens per Sec:    15773, Lr: 0.000300\n",
      "2021-08-02 11:28:05,793 - INFO - joeynmt.training - Epoch  21, Step:   133700, Batch Loss:     2.081025, Tokens per Sec:    15726, Lr: 0.000300\n",
      "2021-08-02 11:28:19,434 - INFO - joeynmt.training - Epoch  21, Step:   133800, Batch Loss:     1.745502, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-02 11:28:33,213 - INFO - joeynmt.training - Epoch  21, Step:   133900, Batch Loss:     1.769294, Tokens per Sec:    15760, Lr: 0.000300\n",
      "2021-08-02 11:28:46,689 - INFO - joeynmt.training - Epoch  21, Step:   134000, Batch Loss:     1.778151, Tokens per Sec:    15722, Lr: 0.000300\n",
      "2021-08-02 11:29:00,525 - INFO - joeynmt.training - Epoch  21, Step:   134100, Batch Loss:     1.772819, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-02 11:29:14,406 - INFO - joeynmt.training - Epoch  21, Step:   134200, Batch Loss:     1.739000, Tokens per Sec:    15782, Lr: 0.000300\n",
      "2021-08-02 11:29:27,977 - INFO - joeynmt.training - Epoch  21, Step:   134300, Batch Loss:     1.710407, Tokens per Sec:    15662, Lr: 0.000300\n",
      "2021-08-02 11:29:41,859 - INFO - joeynmt.training - Epoch  21, Step:   134400, Batch Loss:     1.831071, Tokens per Sec:    16132, Lr: 0.000300\n",
      "2021-08-02 11:29:55,711 - INFO - joeynmt.training - Epoch  21, Step:   134500, Batch Loss:     2.015543, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-08-02 11:30:09,496 - INFO - joeynmt.training - Epoch  21, Step:   134600, Batch Loss:     1.768948, Tokens per Sec:    15925, Lr: 0.000300\n",
      "2021-08-02 11:30:23,462 - INFO - joeynmt.training - Epoch  21, Step:   134700, Batch Loss:     1.604202, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-08-02 11:30:37,353 - INFO - joeynmt.training - Epoch  21, Step:   134800, Batch Loss:     1.964034, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-02 11:30:51,082 - INFO - joeynmt.training - Epoch  21, Step:   134900, Batch Loss:     1.655295, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-08-02 11:31:04,769 - INFO - joeynmt.training - Epoch  21, Step:   135000, Batch Loss:     1.630983, Tokens per Sec:    16122, Lr: 0.000300\n",
      "2021-08-02 11:32:39,697 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:32:39,698 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:32:39,698 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:32:40,933 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:32:40,933 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:32:42,075 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:32:42,076 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:32:42,076 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:32:42,076 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am drawing close to God , so good for me . ”\n",
      "2021-08-02 11:32:42,077 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:32:42,077 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:32:42,077 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:32:42,077 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion is greater than many riches that are standing . ”\n",
      "2021-08-02 11:32:42,077 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:32:42,078 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:32:42,078 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:32:42,078 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has something about God’s Kingdom .\n",
      "2021-08-02 11:32:42,078 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:32:42,079 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:32:42,079 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:32:42,079 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 11:32:42,079 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step   135000: bleu:  26.16, loss: 197800.0156, ppl:   5.1230, duration: 97.3101s\n",
      "2021-08-02 11:32:56,145 - INFO - joeynmt.training - Epoch  21, Step:   135100, Batch Loss:     1.728984, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-08-02 11:33:10,013 - INFO - joeynmt.training - Epoch  21, Step:   135200, Batch Loss:     1.880602, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-08-02 11:33:23,863 - INFO - joeynmt.training - Epoch  21, Step:   135300, Batch Loss:     1.762644, Tokens per Sec:    15668, Lr: 0.000300\n",
      "2021-08-02 11:33:37,595 - INFO - joeynmt.training - Epoch  21, Step:   135400, Batch Loss:     1.720319, Tokens per Sec:    15933, Lr: 0.000300\n",
      "2021-08-02 11:33:51,274 - INFO - joeynmt.training - Epoch  21, Step:   135500, Batch Loss:     1.632856, Tokens per Sec:    15939, Lr: 0.000300\n",
      "2021-08-02 11:34:05,118 - INFO - joeynmt.training - Epoch  21, Step:   135600, Batch Loss:     1.853092, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-02 11:34:18,792 - INFO - joeynmt.training - Epoch  21, Step:   135700, Batch Loss:     1.842448, Tokens per Sec:    15555, Lr: 0.000300\n",
      "2021-08-02 11:34:32,730 - INFO - joeynmt.training - Epoch  21, Step:   135800, Batch Loss:     1.816291, Tokens per Sec:    16232, Lr: 0.000300\n",
      "2021-08-02 11:34:46,509 - INFO - joeynmt.training - Epoch  21, Step:   135900, Batch Loss:     1.841825, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-08-02 11:35:00,300 - INFO - joeynmt.training - Epoch  21, Step:   136000, Batch Loss:     1.787560, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-08-02 11:35:14,267 - INFO - joeynmt.training - Epoch  21, Step:   136100, Batch Loss:     1.848717, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-08-02 11:35:27,979 - INFO - joeynmt.training - Epoch  21, Step:   136200, Batch Loss:     1.888838, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-02 11:35:41,883 - INFO - joeynmt.training - Epoch  21, Step:   136300, Batch Loss:     1.842502, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-02 11:35:55,514 - INFO - joeynmt.training - Epoch  21, Step:   136400, Batch Loss:     1.670000, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-02 11:36:09,208 - INFO - joeynmt.training - Epoch  21, Step:   136500, Batch Loss:     1.622810, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-08-02 11:36:23,050 - INFO - joeynmt.training - Epoch  21, Step:   136600, Batch Loss:     1.867409, Tokens per Sec:    15696, Lr: 0.000300\n",
      "2021-08-02 11:36:36,944 - INFO - joeynmt.training - Epoch  21, Step:   136700, Batch Loss:     1.851540, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-08-02 11:36:50,561 - INFO - joeynmt.training - Epoch  21, Step:   136800, Batch Loss:     1.847990, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-08-02 11:37:04,374 - INFO - joeynmt.training - Epoch  21, Step:   136900, Batch Loss:     1.886444, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-02 11:37:17,993 - INFO - joeynmt.training - Epoch  21, Step:   137000, Batch Loss:     1.892060, Tokens per Sec:    15606, Lr: 0.000300\n",
      "2021-08-02 11:37:32,083 - INFO - joeynmt.training - Epoch  21, Step:   137100, Batch Loss:     2.046225, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-02 11:37:45,787 - INFO - joeynmt.training - Epoch  21, Step:   137200, Batch Loss:     1.957957, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-08-02 11:37:59,872 - INFO - joeynmt.training - Epoch  21, Step:   137300, Batch Loss:     1.761292, Tokens per Sec:    16248, Lr: 0.000300\n",
      "2021-08-02 11:38:13,667 - INFO - joeynmt.training - Epoch  21, Step:   137400, Batch Loss:     1.804839, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-02 11:38:27,507 - INFO - joeynmt.training - Epoch  21, Step:   137500, Batch Loss:     1.871217, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-02 11:38:41,328 - INFO - joeynmt.training - Epoch  21, Step:   137600, Batch Loss:     1.713596, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-08-02 11:38:54,992 - INFO - joeynmt.training - Epoch  21, Step:   137700, Batch Loss:     1.907849, Tokens per Sec:    15728, Lr: 0.000300\n",
      "2021-08-02 11:39:09,024 - INFO - joeynmt.training - Epoch  21, Step:   137800, Batch Loss:     1.682396, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-08-02 11:39:22,767 - INFO - joeynmt.training - Epoch  21, Step:   137900, Batch Loss:     1.689301, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-08-02 11:39:36,529 - INFO - joeynmt.training - Epoch  21, Step:   138000, Batch Loss:     1.806179, Tokens per Sec:    16118, Lr: 0.000300\n",
      "2021-08-02 11:39:50,228 - INFO - joeynmt.training - Epoch  21, Step:   138100, Batch Loss:     1.782997, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-08-02 11:40:04,261 - INFO - joeynmt.training - Epoch  21, Step:   138200, Batch Loss:     1.892489, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-02 11:40:18,242 - INFO - joeynmt.training - Epoch  21, Step:   138300, Batch Loss:     1.767959, Tokens per Sec:    15814, Lr: 0.000300\n",
      "2021-08-02 11:40:31,963 - INFO - joeynmt.training - Epoch  21, Step:   138400, Batch Loss:     1.875506, Tokens per Sec:    16084, Lr: 0.000300\n",
      "2021-08-02 11:40:45,803 - INFO - joeynmt.training - Epoch  21, Step:   138500, Batch Loss:     1.658883, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-08-02 11:40:52,298 - INFO - joeynmt.training - Epoch  21: total training loss 9605.29\n",
      "2021-08-02 11:40:52,298 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-08-02 11:40:59,969 - INFO - joeynmt.training - Epoch  22, Step:   138600, Batch Loss:     1.772933, Tokens per Sec:    15123, Lr: 0.000300\n",
      "2021-08-02 11:41:13,861 - INFO - joeynmt.training - Epoch  22, Step:   138700, Batch Loss:     1.575279, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-02 11:41:27,717 - INFO - joeynmt.training - Epoch  22, Step:   138800, Batch Loss:     1.808631, Tokens per Sec:    15912, Lr: 0.000300\n",
      "2021-08-02 11:41:41,548 - INFO - joeynmt.training - Epoch  22, Step:   138900, Batch Loss:     1.929432, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-02 11:41:55,073 - INFO - joeynmt.training - Epoch  22, Step:   139000, Batch Loss:     1.840389, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-02 11:42:08,854 - INFO - joeynmt.training - Epoch  22, Step:   139100, Batch Loss:     1.788112, Tokens per Sec:    15931, Lr: 0.000300\n",
      "2021-08-02 11:42:22,877 - INFO - joeynmt.training - Epoch  22, Step:   139200, Batch Loss:     1.788148, Tokens per Sec:    16210, Lr: 0.000300\n",
      "2021-08-02 11:42:36,802 - INFO - joeynmt.training - Epoch  22, Step:   139300, Batch Loss:     1.786277, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-08-02 11:42:50,557 - INFO - joeynmt.training - Epoch  22, Step:   139400, Batch Loss:     1.783599, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-08-02 11:43:04,501 - INFO - joeynmt.training - Epoch  22, Step:   139500, Batch Loss:     1.859653, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-02 11:43:18,271 - INFO - joeynmt.training - Epoch  22, Step:   139600, Batch Loss:     1.816577, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-08-02 11:43:32,120 - INFO - joeynmt.training - Epoch  22, Step:   139700, Batch Loss:     2.093313, Tokens per Sec:    15600, Lr: 0.000300\n",
      "2021-08-02 11:43:45,941 - INFO - joeynmt.training - Epoch  22, Step:   139800, Batch Loss:     1.678455, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-02 11:43:59,851 - INFO - joeynmt.training - Epoch  22, Step:   139900, Batch Loss:     1.897405, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-02 11:44:13,555 - INFO - joeynmt.training - Epoch  22, Step:   140000, Batch Loss:     1.581132, Tokens per Sec:    15810, Lr: 0.000300\n",
      "2021-08-02 11:45:45,289 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:45:45,290 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:45:45,290 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:45:46,510 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:45:46,511 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:45:47,522 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:45:47,523 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:45:47,523 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:45:47,523 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so good for me . ”\n",
      "2021-08-02 11:45:47,523 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:45:47,523 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:45:47,524 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:45:47,524 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our resident is godly devotion to Jehovah , we are more than many riches that are standing . ”\n",
      "2021-08-02 11:45:47,524 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:45:47,524 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:45:47,524 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:45:47,525 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has a detailed discussion of God’s Kingdom .\n",
      "2021-08-02 11:45:47,525 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:45:47,525 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:45:47,525 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:45:47,525 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 11:45:47,526 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step   140000: bleu:  26.22, loss: 197283.3750, ppl:   5.1012, duration: 93.9701s\n",
      "2021-08-02 11:46:01,418 - INFO - joeynmt.training - Epoch  22, Step:   140100, Batch Loss:     1.728971, Tokens per Sec:    15631, Lr: 0.000300\n",
      "2021-08-02 11:46:15,352 - INFO - joeynmt.training - Epoch  22, Step:   140200, Batch Loss:     1.718151, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-02 11:46:29,143 - INFO - joeynmt.training - Epoch  22, Step:   140300, Batch Loss:     1.619694, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-02 11:46:43,110 - INFO - joeynmt.training - Epoch  22, Step:   140400, Batch Loss:     1.848209, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-08-02 11:46:56,769 - INFO - joeynmt.training - Epoch  22, Step:   140500, Batch Loss:     1.892213, Tokens per Sec:    15737, Lr: 0.000300\n",
      "2021-08-02 11:47:10,604 - INFO - joeynmt.training - Epoch  22, Step:   140600, Batch Loss:     1.947530, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-02 11:47:24,257 - INFO - joeynmt.training - Epoch  22, Step:   140700, Batch Loss:     1.878741, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-08-02 11:47:37,988 - INFO - joeynmt.training - Epoch  22, Step:   140800, Batch Loss:     1.652124, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-08-02 11:47:51,691 - INFO - joeynmt.training - Epoch  22, Step:   140900, Batch Loss:     1.725879, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-02 11:48:05,375 - INFO - joeynmt.training - Epoch  22, Step:   141000, Batch Loss:     1.632458, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-08-02 11:48:19,238 - INFO - joeynmt.training - Epoch  22, Step:   141100, Batch Loss:     1.821647, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-08-02 11:48:32,958 - INFO - joeynmt.training - Epoch  22, Step:   141200, Batch Loss:     1.639606, Tokens per Sec:    15538, Lr: 0.000300\n",
      "2021-08-02 11:48:46,641 - INFO - joeynmt.training - Epoch  22, Step:   141300, Batch Loss:     1.934986, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-08-02 11:49:00,521 - INFO - joeynmt.training - Epoch  22, Step:   141400, Batch Loss:     1.753994, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-08-02 11:49:14,383 - INFO - joeynmt.training - Epoch  22, Step:   141500, Batch Loss:     1.831120, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-08-02 11:49:27,933 - INFO - joeynmt.training - Epoch  22, Step:   141600, Batch Loss:     1.865898, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-08-02 11:49:41,628 - INFO - joeynmt.training - Epoch  22, Step:   141700, Batch Loss:     1.998385, Tokens per Sec:    15882, Lr: 0.000300\n",
      "2021-08-02 11:49:55,293 - INFO - joeynmt.training - Epoch  22, Step:   141800, Batch Loss:     2.049790, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-02 11:50:09,215 - INFO - joeynmt.training - Epoch  22, Step:   141900, Batch Loss:     1.842990, Tokens per Sec:    15744, Lr: 0.000300\n",
      "2021-08-02 11:50:23,105 - INFO - joeynmt.training - Epoch  22, Step:   142000, Batch Loss:     1.663994, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-02 11:50:36,919 - INFO - joeynmt.training - Epoch  22, Step:   142100, Batch Loss:     1.770478, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-02 11:50:50,781 - INFO - joeynmt.training - Epoch  22, Step:   142200, Batch Loss:     1.853920, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-08-02 11:51:04,633 - INFO - joeynmt.training - Epoch  22, Step:   142300, Batch Loss:     1.805001, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-08-02 11:51:18,662 - INFO - joeynmt.training - Epoch  22, Step:   142400, Batch Loss:     1.724102, Tokens per Sec:    15931, Lr: 0.000300\n",
      "2021-08-02 11:51:32,383 - INFO - joeynmt.training - Epoch  22, Step:   142500, Batch Loss:     1.825518, Tokens per Sec:    15852, Lr: 0.000300\n",
      "2021-08-02 11:51:46,218 - INFO - joeynmt.training - Epoch  22, Step:   142600, Batch Loss:     1.665678, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-08-02 11:51:59,878 - INFO - joeynmt.training - Epoch  22, Step:   142700, Batch Loss:     1.549382, Tokens per Sec:    15725, Lr: 0.000300\n",
      "2021-08-02 11:52:13,665 - INFO - joeynmt.training - Epoch  22, Step:   142800, Batch Loss:     1.642055, Tokens per Sec:    15437, Lr: 0.000300\n",
      "2021-08-02 11:52:27,614 - INFO - joeynmt.training - Epoch  22, Step:   142900, Batch Loss:     1.769029, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-02 11:52:41,352 - INFO - joeynmt.training - Epoch  22, Step:   143000, Batch Loss:     1.832110, Tokens per Sec:    15986, Lr: 0.000300\n",
      "2021-08-02 11:52:55,113 - INFO - joeynmt.training - Epoch  22, Step:   143100, Batch Loss:     1.586621, Tokens per Sec:    16155, Lr: 0.000300\n",
      "2021-08-02 11:53:08,972 - INFO - joeynmt.training - Epoch  22, Step:   143200, Batch Loss:     1.770707, Tokens per Sec:    16157, Lr: 0.000300\n",
      "2021-08-02 11:53:22,856 - INFO - joeynmt.training - Epoch  22, Step:   143300, Batch Loss:     1.886620, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-08-02 11:53:36,785 - INFO - joeynmt.training - Epoch  22, Step:   143400, Batch Loss:     2.162490, Tokens per Sec:    15663, Lr: 0.000300\n",
      "2021-08-02 11:53:50,449 - INFO - joeynmt.training - Epoch  22, Step:   143500, Batch Loss:     1.644425, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-08-02 11:54:03,973 - INFO - joeynmt.training - Epoch  22, Step:   143600, Batch Loss:     1.561563, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-08-02 11:54:17,894 - INFO - joeynmt.training - Epoch  22, Step:   143700, Batch Loss:     1.873919, Tokens per Sec:    16227, Lr: 0.000300\n",
      "2021-08-02 11:54:31,703 - INFO - joeynmt.training - Epoch  22, Step:   143800, Batch Loss:     1.586205, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-02 11:54:42,663 - INFO - joeynmt.training - Epoch  22: total training loss 9579.64\n",
      "2021-08-02 11:54:42,663 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-08-02 11:54:46,211 - INFO - joeynmt.training - Epoch  23, Step:   143900, Batch Loss:     1.435143, Tokens per Sec:    12730, Lr: 0.000300\n",
      "2021-08-02 11:55:00,227 - INFO - joeynmt.training - Epoch  23, Step:   144000, Batch Loss:     1.891713, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-08-02 11:55:14,165 - INFO - joeynmt.training - Epoch  23, Step:   144100, Batch Loss:     1.860659, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-02 11:55:27,626 - INFO - joeynmt.training - Epoch  23, Step:   144200, Batch Loss:     1.770133, Tokens per Sec:    15633, Lr: 0.000300\n",
      "2021-08-02 11:55:41,283 - INFO - joeynmt.training - Epoch  23, Step:   144300, Batch Loss:     1.561667, Tokens per Sec:    15789, Lr: 0.000300\n",
      "2021-08-02 11:55:55,213 - INFO - joeynmt.training - Epoch  23, Step:   144400, Batch Loss:     1.732202, Tokens per Sec:    16243, Lr: 0.000300\n",
      "2021-08-02 11:56:09,214 - INFO - joeynmt.training - Epoch  23, Step:   144500, Batch Loss:     1.628577, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-08-02 11:56:23,083 - INFO - joeynmt.training - Epoch  23, Step:   144600, Batch Loss:     1.676716, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-08-02 11:56:36,702 - INFO - joeynmt.training - Epoch  23, Step:   144700, Batch Loss:     1.824170, Tokens per Sec:    15622, Lr: 0.000300\n",
      "2021-08-02 11:56:50,427 - INFO - joeynmt.training - Epoch  23, Step:   144800, Batch Loss:     1.729285, Tokens per Sec:    16176, Lr: 0.000300\n",
      "2021-08-02 11:57:04,230 - INFO - joeynmt.training - Epoch  23, Step:   144900, Batch Loss:     1.622650, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-08-02 11:57:18,034 - INFO - joeynmt.training - Epoch  23, Step:   145000, Batch Loss:     1.741183, Tokens per Sec:    15926, Lr: 0.000300\n",
      "2021-08-02 11:58:50,346 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:58:50,346 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:58:50,346 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:58:52,448 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:58:52,449 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:58:52,449 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:58:52,449 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am draw close to God , good for me . ”\n",
      "2021-08-02 11:58:52,449 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:58:52,450 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:58:52,450 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:58:52,450 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we live in fear of Jehovah is greater than the riches that are standing . ”\n",
      "2021-08-02 11:58:52,450 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:58:52,451 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:58:52,451 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:58:52,451 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of the book of Daniel says about God’s Kingdom .\n",
      "2021-08-02 11:58:52,451 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:58:52,452 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:58:52,452 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:58:52,452 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-02 11:58:52,452 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step   145000: bleu:  26.19, loss: 197286.3750, ppl:   5.1013, duration: 94.4177s\n",
      "2021-08-02 11:59:06,297 - INFO - joeynmt.training - Epoch  23, Step:   145100, Batch Loss:     1.849639, Tokens per Sec:    15647, Lr: 0.000300\n",
      "2021-08-02 11:59:20,344 - INFO - joeynmt.training - Epoch  23, Step:   145200, Batch Loss:     1.713600, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-08-02 11:59:34,323 - INFO - joeynmt.training - Epoch  23, Step:   145300, Batch Loss:     2.105083, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-02 11:59:48,155 - INFO - joeynmt.training - Epoch  23, Step:   145400, Batch Loss:     1.736517, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-08-02 12:00:02,064 - INFO - joeynmt.training - Epoch  23, Step:   145500, Batch Loss:     1.705833, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-02 12:00:15,795 - INFO - joeynmt.training - Epoch  23, Step:   145600, Batch Loss:     1.777618, Tokens per Sec:    15931, Lr: 0.000300\n",
      "2021-08-02 12:00:29,417 - INFO - joeynmt.training - Epoch  23, Step:   145700, Batch Loss:     1.699157, Tokens per Sec:    15906, Lr: 0.000300\n",
      "2021-08-02 12:00:42,986 - INFO - joeynmt.training - Epoch  23, Step:   145800, Batch Loss:     1.730919, Tokens per Sec:    15656, Lr: 0.000300\n",
      "2021-08-02 12:00:56,692 - INFO - joeynmt.training - Epoch  23, Step:   145900, Batch Loss:     1.841860, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-08-02 12:01:10,793 - INFO - joeynmt.training - Epoch  23, Step:   146000, Batch Loss:     1.877096, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-02 12:01:24,492 - INFO - joeynmt.training - Epoch  23, Step:   146100, Batch Loss:     1.938952, Tokens per Sec:    15810, Lr: 0.000300\n",
      "2021-08-02 12:01:38,117 - INFO - joeynmt.training - Epoch  23, Step:   146200, Batch Loss:     1.755647, Tokens per Sec:    15701, Lr: 0.000300\n",
      "2021-08-02 12:01:51,958 - INFO - joeynmt.training - Epoch  23, Step:   146300, Batch Loss:     1.779358, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-02 12:02:05,735 - INFO - joeynmt.training - Epoch  23, Step:   146400, Batch Loss:     1.919664, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-02 12:02:19,487 - INFO - joeynmt.training - Epoch  23, Step:   146500, Batch Loss:     1.750441, Tokens per Sec:    15875, Lr: 0.000300\n",
      "2021-08-02 12:02:33,402 - INFO - joeynmt.training - Epoch  23, Step:   146600, Batch Loss:     1.846688, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-08-02 12:02:47,235 - INFO - joeynmt.training - Epoch  23, Step:   146700, Batch Loss:     1.824170, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-08-02 12:03:00,885 - INFO - joeynmt.training - Epoch  23, Step:   146800, Batch Loss:     1.740795, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-02 12:03:15,013 - INFO - joeynmt.training - Epoch  23, Step:   146900, Batch Loss:     1.796177, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-08-02 12:03:28,917 - INFO - joeynmt.training - Epoch  23, Step:   147000, Batch Loss:     1.711916, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-02 12:03:42,651 - INFO - joeynmt.training - Epoch  23, Step:   147100, Batch Loss:     1.725166, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-08-02 12:03:56,413 - INFO - joeynmt.training - Epoch  23, Step:   147200, Batch Loss:     2.049295, Tokens per Sec:    15989, Lr: 0.000300\n",
      "2021-08-02 12:04:10,164 - INFO - joeynmt.training - Epoch  23, Step:   147300, Batch Loss:     1.860196, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-02 12:04:24,003 - INFO - joeynmt.training - Epoch  23, Step:   147400, Batch Loss:     1.802718, Tokens per Sec:    15658, Lr: 0.000300\n",
      "2021-08-02 12:04:37,734 - INFO - joeynmt.training - Epoch  23, Step:   147500, Batch Loss:     1.968662, Tokens per Sec:    15664, Lr: 0.000300\n",
      "2021-08-02 12:04:51,618 - INFO - joeynmt.training - Epoch  23, Step:   147600, Batch Loss:     1.794152, Tokens per Sec:    16035, Lr: 0.000300\n",
      "2021-08-02 12:05:05,286 - INFO - joeynmt.training - Epoch  23, Step:   147700, Batch Loss:     1.759532, Tokens per Sec:    15951, Lr: 0.000300\n",
      "2021-08-02 12:05:19,133 - INFO - joeynmt.training - Epoch  23, Step:   147800, Batch Loss:     1.667046, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-08-02 12:05:33,178 - INFO - joeynmt.training - Epoch  23, Step:   147900, Batch Loss:     1.915051, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-08-02 12:05:46,962 - INFO - joeynmt.training - Epoch  23, Step:   148000, Batch Loss:     1.874967, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-02 12:06:00,990 - INFO - joeynmt.training - Epoch  23, Step:   148100, Batch Loss:     1.947070, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-02 12:06:14,850 - INFO - joeynmt.training - Epoch  23, Step:   148200, Batch Loss:     1.582117, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-02 12:06:28,842 - INFO - joeynmt.training - Epoch  23, Step:   148300, Batch Loss:     1.945036, Tokens per Sec:    16251, Lr: 0.000300\n",
      "2021-08-02 12:06:42,430 - INFO - joeynmt.training - Epoch  23, Step:   148400, Batch Loss:     1.629937, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-02 12:06:56,375 - INFO - joeynmt.training - Epoch  23, Step:   148500, Batch Loss:     1.545342, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-08-02 12:07:10,242 - INFO - joeynmt.training - Epoch  23, Step:   148600, Batch Loss:     1.838954, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-08-02 12:07:23,927 - INFO - joeynmt.training - Epoch  23, Step:   148700, Batch Loss:     1.816625, Tokens per Sec:    15594, Lr: 0.000300\n",
      "2021-08-02 12:07:37,652 - INFO - joeynmt.training - Epoch  23, Step:   148800, Batch Loss:     1.738386, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-08-02 12:07:51,234 - INFO - joeynmt.training - Epoch  23, Step:   148900, Batch Loss:     1.856843, Tokens per Sec:    15792, Lr: 0.000300\n",
      "2021-08-02 12:08:04,796 - INFO - joeynmt.training - Epoch  23, Step:   149000, Batch Loss:     1.807934, Tokens per Sec:    15756, Lr: 0.000300\n",
      "2021-08-02 12:08:18,697 - INFO - joeynmt.training - Epoch  23, Step:   149100, Batch Loss:     1.677553, Tokens per Sec:    15594, Lr: 0.000300\n",
      "2021-08-02 12:08:32,455 - INFO - joeynmt.training - Epoch  23, Step:   149200, Batch Loss:     1.908800, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-08-02 12:08:34,241 - INFO - joeynmt.training - Epoch  23: total training loss 9546.46\n",
      "2021-08-02 12:08:34,241 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-08-02 12:08:46,806 - INFO - joeynmt.training - Epoch  24, Step:   149300, Batch Loss:     1.791220, Tokens per Sec:    15230, Lr: 0.000300\n",
      "2021-08-02 12:09:00,384 - INFO - joeynmt.training - Epoch  24, Step:   149400, Batch Loss:     1.968126, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-02 12:09:14,241 - INFO - joeynmt.training - Epoch  24, Step:   149500, Batch Loss:     1.843229, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-02 12:09:28,321 - INFO - joeynmt.training - Epoch  24, Step:   149600, Batch Loss:     1.600090, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-02 12:09:42,109 - INFO - joeynmt.training - Epoch  24, Step:   149700, Batch Loss:     1.736422, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-08-02 12:09:55,743 - INFO - joeynmt.training - Epoch  24, Step:   149800, Batch Loss:     2.205630, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-02 12:10:09,504 - INFO - joeynmt.training - Epoch  24, Step:   149900, Batch Loss:     1.886774, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-02 12:10:23,244 - INFO - joeynmt.training - Epoch  24, Step:   150000, Batch Loss:     1.749970, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-08-02 12:11:58,318 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 12:11:58,318 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 12:11:58,318 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 12:11:59,633 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 12:11:59,633 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 12:12:00,714 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 12:12:00,715 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 12:12:00,715 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 12:12:00,715 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am draw close to God , so good to me . ”\n",
      "2021-08-02 12:12:00,716 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 12:12:00,716 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 12:12:00,716 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 12:12:00,717 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion to Jehovah is greater than the riches that are standing . ”\n",
      "2021-08-02 12:12:00,717 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 12:12:00,717 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 12:12:00,717 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 12:12:00,717 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has something more about God’s Kingdom .\n",
      "2021-08-02 12:12:00,718 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 12:12:00,718 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 12:12:00,718 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 12:12:00,718 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely overcome ?\n",
      "2021-08-02 12:12:00,719 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step   150000: bleu:  26.15, loss: 196379.3438, ppl:   5.0632, duration: 97.4738s\n",
      "2021-08-02 12:12:14,884 - INFO - joeynmt.training - Epoch  24, Step:   150100, Batch Loss:     1.870644, Tokens per Sec:    15499, Lr: 0.000300\n",
      "2021-08-02 12:12:28,700 - INFO - joeynmt.training - Epoch  24, Step:   150200, Batch Loss:     1.827913, Tokens per Sec:    15989, Lr: 0.000300\n",
      "2021-08-02 12:12:42,572 - INFO - joeynmt.training - Epoch  24, Step:   150300, Batch Loss:     1.780813, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-08-02 12:12:56,145 - INFO - joeynmt.training - Epoch  24, Step:   150400, Batch Loss:     1.956132, Tokens per Sec:    16132, Lr: 0.000300\n",
      "2021-08-02 12:13:10,073 - INFO - joeynmt.training - Epoch  24, Step:   150500, Batch Loss:     1.824591, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-02 12:13:23,737 - INFO - joeynmt.training - Epoch  24, Step:   150600, Batch Loss:     1.751410, Tokens per Sec:    15662, Lr: 0.000300\n",
      "2021-08-02 12:13:37,492 - INFO - joeynmt.training - Epoch  24, Step:   150700, Batch Loss:     1.704486, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-02 12:13:51,214 - INFO - joeynmt.training - Epoch  24, Step:   150800, Batch Loss:     1.745564, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-08-02 12:14:05,096 - INFO - joeynmt.training - Epoch  24, Step:   150900, Batch Loss:     1.786449, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-02 12:14:19,053 - INFO - joeynmt.training - Epoch  24, Step:   151000, Batch Loss:     1.830445, Tokens per Sec:    15507, Lr: 0.000300\n",
      "2021-08-02 12:14:32,622 - INFO - joeynmt.training - Epoch  24, Step:   151100, Batch Loss:     1.846546, Tokens per Sec:    15919, Lr: 0.000300\n",
      "2021-08-02 12:14:46,488 - INFO - joeynmt.training - Epoch  24, Step:   151200, Batch Loss:     1.820760, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-02 12:15:00,344 - INFO - joeynmt.training - Epoch  24, Step:   151300, Batch Loss:     1.822252, Tokens per Sec:    16332, Lr: 0.000300\n",
      "2021-08-02 12:15:13,916 - INFO - joeynmt.training - Epoch  24, Step:   151400, Batch Loss:     1.928944, Tokens per Sec:    15765, Lr: 0.000300\n",
      "2021-08-02 12:15:27,900 - INFO - joeynmt.training - Epoch  24, Step:   151500, Batch Loss:     1.918980, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-08-02 12:15:41,751 - INFO - joeynmt.training - Epoch  24, Step:   151600, Batch Loss:     1.654712, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-02 12:15:55,290 - INFO - joeynmt.training - Epoch  24, Step:   151700, Batch Loss:     1.795088, Tokens per Sec:    15572, Lr: 0.000300\n",
      "2021-08-02 12:16:09,040 - INFO - joeynmt.training - Epoch  24, Step:   151800, Batch Loss:     1.817155, Tokens per Sec:    16405, Lr: 0.000300\n",
      "2021-08-02 12:16:22,783 - INFO - joeynmt.training - Epoch  24, Step:   151900, Batch Loss:     1.847246, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-08-02 12:16:36,731 - INFO - joeynmt.training - Epoch  24, Step:   152000, Batch Loss:     1.715235, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-02 12:16:50,366 - INFO - joeynmt.training - Epoch  24, Step:   152100, Batch Loss:     1.756700, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-08-02 12:17:04,151 - INFO - joeynmt.training - Epoch  24, Step:   152200, Batch Loss:     1.811890, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-08-02 12:17:18,054 - INFO - joeynmt.training - Epoch  24, Step:   152300, Batch Loss:     1.789675, Tokens per Sec:    15956, Lr: 0.000300\n",
      "2021-08-02 12:17:31,953 - INFO - joeynmt.training - Epoch  24, Step:   152400, Batch Loss:     1.832453, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-08-02 12:17:45,659 - INFO - joeynmt.training - Epoch  24, Step:   152500, Batch Loss:     1.851694, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-08-02 12:17:59,650 - INFO - joeynmt.training - Epoch  24, Step:   152600, Batch Loss:     1.847391, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-08-02 12:18:13,485 - INFO - joeynmt.training - Epoch  24, Step:   152700, Batch Loss:     1.710949, Tokens per Sec:    15683, Lr: 0.000300\n",
      "2021-08-02 12:18:27,372 - INFO - joeynmt.training - Epoch  24, Step:   152800, Batch Loss:     1.973352, Tokens per Sec:    16285, Lr: 0.000300\n",
      "2021-08-02 12:18:41,055 - INFO - joeynmt.training - Epoch  24, Step:   152900, Batch Loss:     1.709432, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-02 12:18:54,775 - INFO - joeynmt.training - Epoch  24, Step:   153000, Batch Loss:     1.838989, Tokens per Sec:    15624, Lr: 0.000300\n",
      "2021-08-02 12:19:08,493 - INFO - joeynmt.training - Epoch  24, Step:   153100, Batch Loss:     1.776345, Tokens per Sec:    15560, Lr: 0.000300\n",
      "2021-08-02 12:19:22,428 - INFO - joeynmt.training - Epoch  24, Step:   153200, Batch Loss:     1.737143, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-08-02 12:19:36,199 - INFO - joeynmt.training - Epoch  24, Step:   153300, Batch Loss:     1.688216, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-02 12:19:50,077 - INFO - joeynmt.training - Epoch  24, Step:   153400, Batch Loss:     2.041876, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-02 12:20:03,686 - INFO - joeynmt.training - Epoch  24, Step:   153500, Batch Loss:     1.959837, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-08-02 12:20:17,442 - INFO - joeynmt.training - Epoch  24, Step:   153600, Batch Loss:     1.872809, Tokens per Sec:    15628, Lr: 0.000300\n",
      "2021-08-02 12:20:31,445 - INFO - joeynmt.training - Epoch  24, Step:   153700, Batch Loss:     1.829119, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-08-02 12:20:45,140 - INFO - joeynmt.training - Epoch  24, Step:   153800, Batch Loss:     1.834684, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-02 12:20:58,988 - INFO - joeynmt.training - Epoch  24, Step:   153900, Batch Loss:     1.758012, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-08-02 12:21:12,649 - INFO - joeynmt.training - Epoch  24, Step:   154000, Batch Loss:     2.055477, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-02 12:21:26,591 - INFO - joeynmt.training - Epoch  24, Step:   154100, Batch Loss:     1.472422, Tokens per Sec:    15864, Lr: 0.000300\n",
      "2021-08-02 12:21:40,336 - INFO - joeynmt.training - Epoch  24, Step:   154200, Batch Loss:     1.973168, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-08-02 12:21:54,058 - INFO - joeynmt.training - Epoch  24, Step:   154300, Batch Loss:     1.955947, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-02 12:22:07,693 - INFO - joeynmt.training - Epoch  24, Step:   154400, Batch Loss:     1.683453, Tokens per Sec:    15616, Lr: 0.000300\n",
      "2021-08-02 12:22:21,387 - INFO - joeynmt.training - Epoch  24, Step:   154500, Batch Loss:     1.829687, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-08-02 12:22:27,484 - INFO - joeynmt.training - Epoch  24: total training loss 9510.78\n",
      "2021-08-02 12:22:27,484 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-08-02 12:22:35,587 - INFO - joeynmt.training - Epoch  25, Step:   154600, Batch Loss:     2.151754, Tokens per Sec:    15164, Lr: 0.000300\n",
      "2021-08-02 12:22:49,541 - INFO - joeynmt.training - Epoch  25, Step:   154700, Batch Loss:     1.552398, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-08-02 12:23:03,491 - INFO - joeynmt.training - Epoch  25, Step:   154800, Batch Loss:     1.667448, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-08-02 12:23:17,412 - INFO - joeynmt.training - Epoch  25, Step:   154900, Batch Loss:     1.788362, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-08-02 12:23:31,246 - INFO - joeynmt.training - Epoch  25, Step:   155000, Batch Loss:     1.870341, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-08-02 12:25:04,299 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 12:25:04,299 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 12:25:04,300 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 12:25:05,606 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 12:25:05,607 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 12:25:06,709 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 12:25:06,710 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 12:25:06,711 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 12:25:06,711 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am draw close to God , so good for me . ”\n",
      "2021-08-02 12:25:06,711 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 12:25:06,711 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 12:25:06,712 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 12:25:06,712 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion is greater than many riches that are standing . ”\n",
      "2021-08-02 12:25:06,712 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 12:25:06,712 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 12:25:06,712 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 12:25:06,713 - INFO - joeynmt.training - \tHypothesis: Cameron : Note another prophecy in the book of Daniel tells us about God’s Kingdom .\n",
      "2021-08-02 12:25:06,713 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 12:25:06,713 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 12:25:06,713 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 12:25:06,713 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering it ?\n",
      "2021-08-02 12:25:06,714 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step   155000: bleu:  26.05, loss: 195225.0156, ppl:   5.0152, duration: 95.4667s\n",
      "2021-08-02 12:25:20,795 - INFO - joeynmt.training - Epoch  25, Step:   155100, Batch Loss:     1.936985, Tokens per Sec:    15387, Lr: 0.000300\n",
      "2021-08-02 12:25:34,553 - INFO - joeynmt.training - Epoch  25, Step:   155200, Batch Loss:     1.972349, Tokens per Sec:    15853, Lr: 0.000300\n",
      "2021-08-02 12:25:48,310 - INFO - joeynmt.training - Epoch  25, Step:   155300, Batch Loss:     1.625595, Tokens per Sec:    15701, Lr: 0.000300\n",
      "2021-08-02 12:26:02,010 - INFO - joeynmt.training - Epoch  25, Step:   155400, Batch Loss:     1.798719, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-08-02 12:26:15,928 - INFO - joeynmt.training - Epoch  25, Step:   155500, Batch Loss:     1.776087, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-02 12:26:29,760 - INFO - joeynmt.training - Epoch  25, Step:   155600, Batch Loss:     1.709169, Tokens per Sec:    15803, Lr: 0.000300\n",
      "2021-08-02 12:26:43,551 - INFO - joeynmt.training - Epoch  25, Step:   155700, Batch Loss:     2.384390, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-08-02 12:26:57,285 - INFO - joeynmt.training - Epoch  25, Step:   155800, Batch Loss:     1.745384, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-08-02 12:27:11,042 - INFO - joeynmt.training - Epoch  25, Step:   155900, Batch Loss:     1.766742, Tokens per Sec:    16231, Lr: 0.000300\n",
      "2021-08-02 12:27:24,682 - INFO - joeynmt.training - Epoch  25, Step:   156000, Batch Loss:     1.795295, Tokens per Sec:    15549, Lr: 0.000300\n",
      "2021-08-02 12:27:38,680 - INFO - joeynmt.training - Epoch  25, Step:   156100, Batch Loss:     1.846151, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-08-02 12:27:52,374 - INFO - joeynmt.training - Epoch  25, Step:   156200, Batch Loss:     1.773759, Tokens per Sec:    15752, Lr: 0.000300\n",
      "2021-08-02 12:28:06,162 - INFO - joeynmt.training - Epoch  25, Step:   156300, Batch Loss:     2.067361, Tokens per Sec:    15594, Lr: 0.000300\n",
      "2021-08-02 12:28:20,038 - INFO - joeynmt.training - Epoch  25, Step:   156400, Batch Loss:     1.779973, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-08-02 12:28:33,714 - INFO - joeynmt.training - Epoch  25, Step:   156500, Batch Loss:     1.683362, Tokens per Sec:    16035, Lr: 0.000300\n",
      "2021-08-02 12:28:47,494 - INFO - joeynmt.training - Epoch  25, Step:   156600, Batch Loss:     1.624686, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-02 12:29:01,382 - INFO - joeynmt.training - Epoch  25, Step:   156700, Batch Loss:     1.632725, Tokens per Sec:    15989, Lr: 0.000300\n",
      "2021-08-02 12:29:15,175 - INFO - joeynmt.training - Epoch  25, Step:   156800, Batch Loss:     1.540795, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-08-02 12:29:28,972 - INFO - joeynmt.training - Epoch  25, Step:   156900, Batch Loss:     1.796169, Tokens per Sec:    15792, Lr: 0.000300\n",
      "2021-08-02 12:29:42,728 - INFO - joeynmt.training - Epoch  25, Step:   157000, Batch Loss:     1.858322, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-08-02 12:29:56,314 - INFO - joeynmt.training - Epoch  25, Step:   157100, Batch Loss:     1.696610, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-08-02 12:30:10,281 - INFO - joeynmt.training - Epoch  25, Step:   157200, Batch Loss:     1.941786, Tokens per Sec:    16050, Lr: 0.000300\n",
      "2021-08-02 12:30:24,163 - INFO - joeynmt.training - Epoch  25, Step:   157300, Batch Loss:     1.790132, Tokens per Sec:    15739, Lr: 0.000300\n",
      "2021-08-02 12:30:38,086 - INFO - joeynmt.training - Epoch  25, Step:   157400, Batch Loss:     1.834860, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-08-02 12:30:51,797 - INFO - joeynmt.training - Epoch  25, Step:   157500, Batch Loss:     1.643208, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-08-02 12:31:05,565 - INFO - joeynmt.training - Epoch  25, Step:   157600, Batch Loss:     1.716255, Tokens per Sec:    16233, Lr: 0.000300\n",
      "2021-08-02 12:31:19,305 - INFO - joeynmt.training - Epoch  25, Step:   157700, Batch Loss:     1.657501, Tokens per Sec:    15638, Lr: 0.000300\n",
      "2021-08-02 12:31:33,356 - INFO - joeynmt.training - Epoch  25, Step:   157800, Batch Loss:     1.551370, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-08-02 12:31:47,181 - INFO - joeynmt.training - Epoch  25, Step:   157900, Batch Loss:     1.594750, Tokens per Sec:    16021, Lr: 0.000300\n",
      "2021-08-02 12:32:01,014 - INFO - joeynmt.training - Epoch  25, Step:   158000, Batch Loss:     1.682739, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-02 12:32:14,766 - INFO - joeynmt.training - Epoch  25, Step:   158100, Batch Loss:     1.836800, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-02 12:32:28,703 - INFO - joeynmt.training - Epoch  25, Step:   158200, Batch Loss:     1.881320, Tokens per Sec:    15675, Lr: 0.000300\n",
      "2021-08-02 12:32:42,652 - INFO - joeynmt.training - Epoch  25, Step:   158300, Batch Loss:     1.904287, Tokens per Sec:    16241, Lr: 0.000300\n",
      "2021-08-02 12:32:56,528 - INFO - joeynmt.training - Epoch  25, Step:   158400, Batch Loss:     1.839511, Tokens per Sec:    15663, Lr: 0.000300\n",
      "2021-08-02 12:33:10,457 - INFO - joeynmt.training - Epoch  25, Step:   158500, Batch Loss:     1.740193, Tokens per Sec:    16176, Lr: 0.000300\n",
      "2021-08-02 12:33:24,116 - INFO - joeynmt.training - Epoch  25, Step:   158600, Batch Loss:     1.553176, Tokens per Sec:    16027, Lr: 0.000300\n",
      "2021-08-02 12:33:37,845 - INFO - joeynmt.training - Epoch  25, Step:   158700, Batch Loss:     1.843670, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-08-02 12:33:51,550 - INFO - joeynmt.training - Epoch  25, Step:   158800, Batch Loss:     1.755813, Tokens per Sec:    15818, Lr: 0.000300\n",
      "2021-08-02 12:34:05,462 - INFO - joeynmt.training - Epoch  25, Step:   158900, Batch Loss:     1.592770, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-08-02 12:34:19,260 - INFO - joeynmt.training - Epoch  25, Step:   159000, Batch Loss:     1.874111, Tokens per Sec:    15517, Lr: 0.000300\n",
      "2021-08-02 12:34:32,960 - INFO - joeynmt.training - Epoch  25, Step:   159100, Batch Loss:     1.800469, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-02 12:34:46,850 - INFO - joeynmt.training - Epoch  25, Step:   159200, Batch Loss:     1.697042, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-02 12:35:00,600 - INFO - joeynmt.training - Epoch  25, Step:   159300, Batch Loss:     1.778606, Tokens per Sec:    15705, Lr: 0.000300\n",
      "2021-08-02 12:35:14,658 - INFO - joeynmt.training - Epoch  25, Step:   159400, Batch Loss:     1.769116, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-02 12:35:28,598 - INFO - joeynmt.training - Epoch  25, Step:   159500, Batch Loss:     1.763185, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-02 12:35:42,449 - INFO - joeynmt.training - Epoch  25, Step:   159600, Batch Loss:     1.810132, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-02 12:35:56,030 - INFO - joeynmt.training - Epoch  25, Step:   159700, Batch Loss:     1.762913, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-08-02 12:36:09,721 - INFO - joeynmt.training - Epoch  25, Step:   159800, Batch Loss:     1.963837, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-02 12:36:19,255 - INFO - joeynmt.training - Epoch  25: total training loss 9462.79\n",
      "2021-08-02 12:36:19,255 - INFO - joeynmt.training - Training ended after  25 epochs.\n",
      "2021-08-02 12:36:19,256 - INFO - joeynmt.training - Best validation result (greedy) at step   155000:   5.02 ppl.\n",
      "2021-08-02 12:36:19,277 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 10000 (with beam_size)\n",
      "2021-08-02 12:36:19,643 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-02 12:36:19,842 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-02 12:36:19,906 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe.en)...\n",
      "2021-08-02 12:38:13,193 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 12:38:13,193 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 12:38:13,194 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 12:38:14,384 - INFO - joeynmt.prediction -  dev bleu[13a]:  26.55 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-02 12:38:14,390 - INFO - joeynmt.prediction - Translations saved to: models/rwen_reverse_transformer_continued/00155000.hyps.dev\n",
      "2021-08-02 12:38:14,391 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe.en)...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
      "    main()\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/training.py\", line 822, in train\n",
      "    datasets=datasets_to_test)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/prediction.py\", line 335, in test\n",
      "    bpe_type=bpe_type, sacrebleu=sacrebleu, n_gpu=n_gpu)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/prediction.py\", line 104, in validate_on_data\n",
      "    for valid_batch in iter(valid_iter):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\", line 160, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\", line 230, in process\n",
      "    padded = self.pad(batch)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\", line 248, in pad\n",
      "    max_len = max(len(x) for x in minibatch)\n",
      "ValueError: max() arg is an empty sequence\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_rwen_reload.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiqxC-T0mNe3"
   },
   "source": [
    "25.5 epochs done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75jtnPa_5qSx",
    "outputId": "d0931782-923e-46d4-b134-eec4ebb3625f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 35000\tLoss: 236982.75000\tPPL: 7.08068\tbleu: 21.07577\tLR: 0.00030000\t*\n",
      "Steps: 40000\tLoss: 231373.42188\tPPL: 6.76011\tbleu: 21.92058\tLR: 0.00030000\t*\n",
      "Steps: 45000\tLoss: 226930.53125\tPPL: 6.51654\tbleu: 22.44196\tLR: 0.00030000\t*\n",
      "Steps: 50000\tLoss: 224104.50000\tPPL: 6.36619\tbleu: 22.81634\tLR: 0.00030000\t*\n",
      "Steps: 55000\tLoss: 220764.64062\tPPL: 6.19298\tbleu: 23.03003\tLR: 0.00030000\t*\n",
      "Steps: 60000\tLoss: 218070.60938\tPPL: 6.05670\tbleu: 23.57888\tLR: 0.00030000\t*\n",
      "Steps: 65000\tLoss: 215245.06250\tPPL: 5.91698\tbleu: 23.98163\tLR: 0.00030000\t*\n",
      "Steps: 70000\tLoss: 213543.65625\tPPL: 5.83441\tbleu: 24.09171\tLR: 0.00030000\t*\n",
      "Steps: 75000\tLoss: 211926.60938\tPPL: 5.75701\tbleu: 24.59532\tLR: 0.00030000\t*\n",
      "Steps: 80000\tLoss: 209693.71875\tPPL: 5.65181\tbleu: 24.83026\tLR: 0.00030000\t*\n",
      "Steps: 85000\tLoss: 207986.29688\tPPL: 5.57266\tbleu: 24.82739\tLR: 0.00030000\t*\n",
      "Steps: 90000\tLoss: 206906.20312\tPPL: 5.52317\tbleu: 24.99484\tLR: 0.00030000\t*\n",
      "Steps: 95000\tLoss: 204909.06250\tPPL: 5.43281\tbleu: 25.03757\tLR: 0.00030000\t*\n",
      "Steps: 100000\tLoss: 204008.15625\tPPL: 5.39253\tbleu: 25.23743\tLR: 0.00030000\t*\n",
      "Steps: 105000\tLoss: 203301.43750\tPPL: 5.36115\tbleu: 25.30802\tLR: 0.00030000\t*\n",
      "Steps: 110000\tLoss: 202475.51562\tPPL: 5.32470\tbleu: 25.39948\tLR: 0.00030000\t*\n",
      "Steps: 115000\tLoss: 200951.46875\tPPL: 5.25809\tbleu: 25.65472\tLR: 0.00030000\t*\n",
      "Steps: 120000\tLoss: 199999.84375\tPPL: 5.21693\tbleu: 25.61110\tLR: 0.00030000\t*\n",
      "Steps: 125000\tLoss: 199043.42188\tPPL: 5.17588\tbleu: 25.84482\tLR: 0.00030000\t*\n",
      "Steps: 130000\tLoss: 199432.68750\tPPL: 5.19254\tbleu: 25.83968\tLR: 0.00030000\t\n",
      "Steps: 135000\tLoss: 197800.01562\tPPL: 5.12299\tbleu: 26.16172\tLR: 0.00030000\t*\n",
      "Steps: 140000\tLoss: 197283.37500\tPPL: 5.10118\tbleu: 26.22191\tLR: 0.00030000\t*\n",
      "Steps: 145000\tLoss: 197286.37500\tPPL: 5.10130\tbleu: 26.18850\tLR: 0.00030000\t\n",
      "Steps: 150000\tLoss: 196379.34375\tPPL: 5.06323\tbleu: 26.14740\tLR: 0.00030000\t*\n",
      "Steps: 155000\tLoss: 195225.01562\tPPL: 5.01519\tbleu: 26.04868\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/rwen_reverse_transformer_continued/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxDnInOKB35g",
    "outputId": "53fa619c-1de2-448c-86d2-1904837c5372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 07:02:21,339 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 07:02:26,033 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 07:02:26,258 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/rwen_reverse_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe.rw\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/translation.bpe.rw_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKOjJX4h1YM-",
    "outputId": "1e807880-4a0a-4198-e4d4-8a48287ee4d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
      "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 25.8 59.3/34.4/23.0/16.3 (BP = 0.870 ratio = 0.878 hyp_len = 74798 ref_len = 85182)\n"
     ]
    }
   ],
   "source": [
    "!cat \"translation.bpe.rw_en\" | sacrebleu \"test.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj7qLvxQtjcJ"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 155000\n",
    "\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/models/rwen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rwen_reverse_transformer2\"', f'model_dir: \"models/rwen_reverse_transformer2_continued\"')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "h_Vzbp17tjEn",
    "outputId": "1ce3fb57-fbb4-4d76-c690-f2efb20e34e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rwen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer_continued/155000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 2000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rwen_reverse_transformer\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_{name}_reload2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aAoPluHyMQV",
    "outputId": "aa253ca0-aa77-4346-a278-bb2d85bfaa26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 07:13:43,869 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 07:13:43,906 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 07:13:54,094 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 07:13:54,380 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 07:13:55,487 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 07:13:56,313 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 07:13:56,313 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 07:13:56,513 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 07:13:56.783953: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 07:13:58,343 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-08-03 07:14:01,747 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer_continued/155000.ckpt\n",
      "2021-08-03 07:14:02,149 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-08-03 07:14:02,149 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-08-03 07:14:02,150 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 07:14:02,150 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-08-03 07:14:02,150 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-08-03 07:14:02,150 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-08-03 07:14:02,150 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 07:14:02,150 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 07:14:02,150 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 07:14:02,150 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer_continued/155000.ckpt\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 07:14:02,151 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 07:14:02,152 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 07:14:02,152 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 07:14:02,152 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 07:14:02,152 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 07:14:02,152 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 07:14:02,152 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 07:14:02,153 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 07:14:02,154 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 07:14:02,155 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 07:14:02,155 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 07:14:02,155 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 07:14:02,155 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-03 07:14:02,155 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 07:14:02,155 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 07:14:02,155 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 07:14:02,156 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-03 07:14:02,156 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-03 07:14:02,156 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-08-03 07:14:02,156 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 07:14:02,156 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer\n",
      "2021-08-03 07:14:02,156 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 07:14:02,156 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 07:14:02,156 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 07:14:02,157 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 07:14:02,157 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 07:14:02,157 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 07:14:02,157 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 07:14:02,157 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 07:14:02,157 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 07:14:02,157 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 07:14:02,158 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 07:14:02,158 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 07:14:02,158 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 07:14:02,158 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 07:14:02,158 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 07:14:02,158 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 07:14:02,159 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 07:14:02,159 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 07:14:02,159 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 07:14:02,159 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 07:14:02,159 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 07:14:02,159 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 07:14:02,159 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 07:14:02,160 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 07:14:02,160 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 07:14:02,160 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 07:14:02,160 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 07:14:02,160 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 07:14:02,160 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 07:14:02,160 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 07:14:02,160 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 07:14:02,161 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 426806,\n",
      "\tvalid 4368,\n",
      "\ttest 4368\n",
      "2021-08-03 07:14:02,161 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-03 07:14:02,161 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 07:14:02,161 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 07:14:02,161 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-08-03 07:14:02,161 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-08-03 07:14:02,162 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-08-03 07:14:02,171 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 07:14:02,172 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 07:14:15,545 - INFO - joeynmt.training - Epoch   1, Step:   155100, Batch Loss:     1.926072, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-03 07:14:28,178 - INFO - joeynmt.training - Epoch   1, Step:   155200, Batch Loss:     1.989935, Tokens per Sec:    17266, Lr: 0.000300\n",
      "2021-08-03 07:14:41,114 - INFO - joeynmt.training - Epoch   1, Step:   155300, Batch Loss:     1.645131, Tokens per Sec:    16697, Lr: 0.000300\n",
      "2021-08-03 07:14:54,069 - INFO - joeynmt.training - Epoch   1, Step:   155400, Batch Loss:     1.823497, Tokens per Sec:    16726, Lr: 0.000300\n",
      "2021-08-03 07:15:07,110 - INFO - joeynmt.training - Epoch   1, Step:   155500, Batch Loss:     1.810458, Tokens per Sec:    17059, Lr: 0.000300\n",
      "2021-08-03 07:15:20,067 - INFO - joeynmt.training - Epoch   1, Step:   155600, Batch Loss:     1.678601, Tokens per Sec:    16870, Lr: 0.000300\n",
      "2021-08-03 07:15:33,212 - INFO - joeynmt.training - Epoch   1, Step:   155700, Batch Loss:     2.433964, Tokens per Sec:    16752, Lr: 0.000300\n",
      "2021-08-03 07:15:46,561 - INFO - joeynmt.training - Epoch   1, Step:   155800, Batch Loss:     1.759296, Tokens per Sec:    16486, Lr: 0.000300\n",
      "2021-08-03 07:15:59,980 - INFO - joeynmt.training - Epoch   1, Step:   155900, Batch Loss:     1.759322, Tokens per Sec:    16639, Lr: 0.000300\n",
      "2021-08-03 07:16:13,097 - INFO - joeynmt.training - Epoch   1, Step:   156000, Batch Loss:     1.807590, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-08-03 07:16:26,632 - INFO - joeynmt.training - Epoch   1, Step:   156100, Batch Loss:     1.851287, Tokens per Sec:    16443, Lr: 0.000300\n",
      "2021-08-03 07:16:39,957 - INFO - joeynmt.training - Epoch   1, Step:   156200, Batch Loss:     1.758914, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-03 07:16:53,213 - INFO - joeynmt.training - Epoch   1, Step:   156300, Batch Loss:     2.097206, Tokens per Sec:    16218, Lr: 0.000300\n",
      "2021-08-03 07:17:06,687 - INFO - joeynmt.training - Epoch   1, Step:   156400, Batch Loss:     1.799421, Tokens per Sec:    16493, Lr: 0.000300\n",
      "2021-08-03 07:17:20,178 - INFO - joeynmt.training - Epoch   1, Step:   156500, Batch Loss:     1.692519, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-03 07:17:33,855 - INFO - joeynmt.training - Epoch   1, Step:   156600, Batch Loss:     1.583270, Tokens per Sec:    16143, Lr: 0.000300\n",
      "2021-08-03 07:17:47,508 - INFO - joeynmt.training - Epoch   1, Step:   156700, Batch Loss:     1.576422, Tokens per Sec:    16265, Lr: 0.000300\n",
      "2021-08-03 07:18:00,974 - INFO - joeynmt.training - Epoch   1, Step:   156800, Batch Loss:     1.591168, Tokens per Sec:    16180, Lr: 0.000300\n",
      "2021-08-03 07:18:14,529 - INFO - joeynmt.training - Epoch   1, Step:   156900, Batch Loss:     1.789325, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-03 07:18:28,278 - INFO - joeynmt.training - Epoch   1, Step:   157000, Batch Loss:     1.850348, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-03 07:18:41,889 - INFO - joeynmt.training - Epoch   1, Step:   157100, Batch Loss:     1.635236, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-08-03 07:18:55,696 - INFO - joeynmt.training - Epoch   1, Step:   157200, Batch Loss:     1.923367, Tokens per Sec:    16237, Lr: 0.000300\n",
      "2021-08-03 07:19:09,452 - INFO - joeynmt.training - Epoch   1, Step:   157300, Batch Loss:     1.768901, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-08-03 07:19:23,306 - INFO - joeynmt.training - Epoch   1, Step:   157400, Batch Loss:     1.839825, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-03 07:19:37,181 - INFO - joeynmt.training - Epoch   1, Step:   157500, Batch Loss:     1.608399, Tokens per Sec:    15691, Lr: 0.000300\n",
      "2021-08-03 07:19:51,177 - INFO - joeynmt.training - Epoch   1, Step:   157600, Batch Loss:     1.696648, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-08-03 07:20:04,801 - INFO - joeynmt.training - Epoch   1, Step:   157700, Batch Loss:     1.674169, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-08-03 07:20:18,802 - INFO - joeynmt.training - Epoch   1, Step:   157800, Batch Loss:     1.582897, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-08-03 07:20:32,621 - INFO - joeynmt.training - Epoch   1, Step:   157900, Batch Loss:     1.581442, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-08-03 07:20:46,607 - INFO - joeynmt.training - Epoch   1, Step:   158000, Batch Loss:     1.693668, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-03 07:21:00,433 - INFO - joeynmt.training - Epoch   1, Step:   158100, Batch Loss:     1.823455, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-08-03 07:21:14,178 - INFO - joeynmt.training - Epoch   1, Step:   158200, Batch Loss:     1.893127, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-08-03 07:21:28,013 - INFO - joeynmt.training - Epoch   1, Step:   158300, Batch Loss:     1.938199, Tokens per Sec:    16376, Lr: 0.000300\n",
      "2021-08-03 07:21:41,815 - INFO - joeynmt.training - Epoch   1, Step:   158400, Batch Loss:     1.856164, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-08-03 07:21:55,755 - INFO - joeynmt.training - Epoch   1, Step:   158500, Batch Loss:     1.727776, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-03 07:22:09,476 - INFO - joeynmt.training - Epoch   1, Step:   158600, Batch Loss:     1.576349, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-03 07:22:23,309 - INFO - joeynmt.training - Epoch   1, Step:   158700, Batch Loss:     1.852538, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-08-03 07:22:37,142 - INFO - joeynmt.training - Epoch   1, Step:   158800, Batch Loss:     1.725824, Tokens per Sec:    15671, Lr: 0.000300\n",
      "2021-08-03 07:22:50,828 - INFO - joeynmt.training - Epoch   1, Step:   158900, Batch Loss:     1.572023, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-03 07:23:04,466 - INFO - joeynmt.training - Epoch   1, Step:   159000, Batch Loss:     1.867381, Tokens per Sec:    15698, Lr: 0.000300\n",
      "2021-08-03 07:23:18,191 - INFO - joeynmt.training - Epoch   1, Step:   159100, Batch Loss:     1.775879, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-08-03 07:23:32,269 - INFO - joeynmt.training - Epoch   1, Step:   159200, Batch Loss:     1.670082, Tokens per Sec:    15956, Lr: 0.000300\n",
      "2021-08-03 07:23:46,040 - INFO - joeynmt.training - Epoch   1, Step:   159300, Batch Loss:     1.785938, Tokens per Sec:    15681, Lr: 0.000300\n",
      "2021-08-03 07:23:59,971 - INFO - joeynmt.training - Epoch   1, Step:   159400, Batch Loss:     1.774193, Tokens per Sec:    16236, Lr: 0.000300\n",
      "2021-08-03 07:24:13,771 - INFO - joeynmt.training - Epoch   1, Step:   159500, Batch Loss:     1.787036, Tokens per Sec:    16316, Lr: 0.000300\n",
      "2021-08-03 07:24:27,675 - INFO - joeynmt.training - Epoch   1, Step:   159600, Batch Loss:     1.798519, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-03 07:24:41,404 - INFO - joeynmt.training - Epoch   1, Step:   159700, Batch Loss:     1.779277, Tokens per Sec:    15672, Lr: 0.000300\n",
      "2021-08-03 07:24:55,168 - INFO - joeynmt.training - Epoch   1, Step:   159800, Batch Loss:     1.989402, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-08-03 07:25:04,592 - INFO - joeynmt.training - Epoch   1: total training loss 8657.20\n",
      "2021-08-03 07:25:04,592 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 07:25:09,512 - INFO - joeynmt.training - Epoch   2, Step:   159900, Batch Loss:     1.634810, Tokens per Sec:    14195, Lr: 0.000300\n",
      "2021-08-03 07:25:23,223 - INFO - joeynmt.training - Epoch   2, Step:   160000, Batch Loss:     1.753061, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-03 07:26:55,984 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 07:26:55,984 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 07:26:55,985 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 07:26:57,274 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 07:26:57,274 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 07:26:58,047 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 07:26:58,047 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 07:26:58,047 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 07:26:58,048 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , I am draw close to God , that is good for me . ”\n",
      "2021-08-03 07:26:58,048 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 07:26:58,048 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 07:26:58,048 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 07:26:58,049 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our resident is godly devotion to Jehovah , and we are far more than many treasures that are standing . ”\n",
      "2021-08-03 07:26:58,049 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 07:26:58,049 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 07:26:58,049 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 07:26:58,050 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! A further prophecy in the book of Daniel has something about God’s Kingdom .\n",
      "2021-08-03 07:26:58,050 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 07:26:58,051 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 07:26:58,051 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 07:26:58,051 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 07:26:58,051 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   160000: bleu:  26.36, loss: 194533.7344, ppl:   4.9866, duration: 94.8277s\n",
      "2021-08-03 07:27:12,089 - INFO - joeynmt.training - Epoch   2, Step:   160100, Batch Loss:     1.867508, Tokens per Sec:    16056, Lr: 0.000300\n",
      "2021-08-03 07:27:26,017 - INFO - joeynmt.training - Epoch   2, Step:   160200, Batch Loss:     1.449795, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-08-03 07:27:39,907 - INFO - joeynmt.training - Epoch   2, Step:   160300, Batch Loss:     1.914316, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-08-03 07:27:53,635 - INFO - joeynmt.training - Epoch   2, Step:   160400, Batch Loss:     1.739870, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-03 07:28:07,202 - INFO - joeynmt.training - Epoch   2, Step:   160500, Batch Loss:     2.001676, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-08-03 07:28:21,049 - INFO - joeynmt.training - Epoch   2, Step:   160600, Batch Loss:     1.777953, Tokens per Sec:    16027, Lr: 0.000300\n",
      "2021-08-03 07:28:34,942 - INFO - joeynmt.training - Epoch   2, Step:   160700, Batch Loss:     1.570183, Tokens per Sec:    15678, Lr: 0.000300\n",
      "2021-08-03 07:28:48,802 - INFO - joeynmt.training - Epoch   2, Step:   160800, Batch Loss:     1.690037, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-08-03 07:29:02,571 - INFO - joeynmt.training - Epoch   2, Step:   160900, Batch Loss:     1.785334, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-03 07:29:16,477 - INFO - joeynmt.training - Epoch   2, Step:   161000, Batch Loss:     1.985400, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-08-03 07:29:30,147 - INFO - joeynmt.training - Epoch   2, Step:   161100, Batch Loss:     2.139230, Tokens per Sec:    15518, Lr: 0.000300\n",
      "2021-08-03 07:29:44,134 - INFO - joeynmt.training - Epoch   2, Step:   161200, Batch Loss:     1.799249, Tokens per Sec:    15582, Lr: 0.000300\n",
      "2021-08-03 07:29:57,773 - INFO - joeynmt.training - Epoch   2, Step:   161300, Batch Loss:     1.614059, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-03 07:30:11,628 - INFO - joeynmt.training - Epoch   2, Step:   161400, Batch Loss:     1.571384, Tokens per Sec:    15834, Lr: 0.000300\n",
      "2021-08-03 07:30:25,606 - INFO - joeynmt.training - Epoch   2, Step:   161500, Batch Loss:     1.885762, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-03 07:30:39,392 - INFO - joeynmt.training - Epoch   2, Step:   161600, Batch Loss:     2.044951, Tokens per Sec:    15479, Lr: 0.000300\n",
      "2021-08-03 07:30:53,196 - INFO - joeynmt.training - Epoch   2, Step:   161700, Batch Loss:     1.814633, Tokens per Sec:    15561, Lr: 0.000300\n",
      "2021-08-03 07:31:07,071 - INFO - joeynmt.training - Epoch   2, Step:   161800, Batch Loss:     1.794834, Tokens per Sec:    16176, Lr: 0.000300\n",
      "2021-08-03 07:31:20,706 - INFO - joeynmt.training - Epoch   2, Step:   161900, Batch Loss:     1.754869, Tokens per Sec:    15863, Lr: 0.000300\n",
      "2021-08-03 07:31:34,320 - INFO - joeynmt.training - Epoch   2, Step:   162000, Batch Loss:     1.710555, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-03 07:31:48,195 - INFO - joeynmt.training - Epoch   2, Step:   162100, Batch Loss:     1.962702, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-03 07:32:02,094 - INFO - joeynmt.training - Epoch   2, Step:   162200, Batch Loss:     1.694626, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-03 07:32:15,634 - INFO - joeynmt.training - Epoch   2, Step:   162300, Batch Loss:     1.716703, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-08-03 07:32:29,239 - INFO - joeynmt.training - Epoch   2, Step:   162400, Batch Loss:     1.622274, Tokens per Sec:    15691, Lr: 0.000300\n",
      "2021-08-03 07:32:42,838 - INFO - joeynmt.training - Epoch   2, Step:   162500, Batch Loss:     1.887489, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-03 07:32:56,861 - INFO - joeynmt.training - Epoch   2, Step:   162600, Batch Loss:     1.940786, Tokens per Sec:    16117, Lr: 0.000300\n",
      "2021-08-03 07:33:10,775 - INFO - joeynmt.training - Epoch   2, Step:   162700, Batch Loss:     1.814905, Tokens per Sec:    16217, Lr: 0.000300\n",
      "2021-08-03 07:33:24,684 - INFO - joeynmt.training - Epoch   2, Step:   162800, Batch Loss:     1.782607, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-03 07:33:38,808 - INFO - joeynmt.training - Epoch   2, Step:   162900, Batch Loss:     1.712621, Tokens per Sec:    15907, Lr: 0.000300\n",
      "2021-08-03 07:33:52,591 - INFO - joeynmt.training - Epoch   2, Step:   163000, Batch Loss:     1.883840, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-08-03 07:34:06,370 - INFO - joeynmt.training - Epoch   2, Step:   163100, Batch Loss:     1.929250, Tokens per Sec:    15919, Lr: 0.000300\n",
      "2021-08-03 07:34:20,225 - INFO - joeynmt.training - Epoch   2, Step:   163200, Batch Loss:     1.705535, Tokens per Sec:    15989, Lr: 0.000300\n",
      "2021-08-03 07:34:33,989 - INFO - joeynmt.training - Epoch   2, Step:   163300, Batch Loss:     1.540576, Tokens per Sec:    15506, Lr: 0.000300\n",
      "2021-08-03 07:34:47,944 - INFO - joeynmt.training - Epoch   2, Step:   163400, Batch Loss:     1.722738, Tokens per Sec:    15925, Lr: 0.000300\n",
      "2021-08-03 07:35:01,650 - INFO - joeynmt.training - Epoch   2, Step:   163500, Batch Loss:     1.881198, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-03 07:35:15,417 - INFO - joeynmt.training - Epoch   2, Step:   163600, Batch Loss:     1.901025, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-03 07:35:29,224 - INFO - joeynmt.training - Epoch   2, Step:   163700, Batch Loss:     1.694666, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-03 07:35:43,038 - INFO - joeynmt.training - Epoch   2, Step:   163800, Batch Loss:     1.661893, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-08-03 07:35:56,792 - INFO - joeynmt.training - Epoch   2, Step:   163900, Batch Loss:     1.649346, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-08-03 07:36:10,683 - INFO - joeynmt.training - Epoch   2, Step:   164000, Batch Loss:     1.614342, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-03 07:36:24,446 - INFO - joeynmt.training - Epoch   2, Step:   164100, Batch Loss:     1.516176, Tokens per Sec:    16155, Lr: 0.000300\n",
      "2021-08-03 07:36:38,131 - INFO - joeynmt.training - Epoch   2, Step:   164200, Batch Loss:     1.933735, Tokens per Sec:    15665, Lr: 0.000300\n",
      "2021-08-03 07:36:52,015 - INFO - joeynmt.training - Epoch   2, Step:   164300, Batch Loss:     1.712555, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-08-03 07:37:05,814 - INFO - joeynmt.training - Epoch   2, Step:   164400, Batch Loss:     1.850860, Tokens per Sec:    15925, Lr: 0.000300\n",
      "2021-08-03 07:37:19,714 - INFO - joeynmt.training - Epoch   2, Step:   164500, Batch Loss:     1.753876, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-03 07:37:33,347 - INFO - joeynmt.training - Epoch   2, Step:   164600, Batch Loss:     1.870617, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-08-03 07:37:47,094 - INFO - joeynmt.training - Epoch   2, Step:   164700, Batch Loss:     1.912948, Tokens per Sec:    15561, Lr: 0.000300\n",
      "2021-08-03 07:38:01,093 - INFO - joeynmt.training - Epoch   2, Step:   164800, Batch Loss:     1.680977, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-03 07:38:14,858 - INFO - joeynmt.training - Epoch   2, Step:   164900, Batch Loss:     1.721947, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-08-03 07:38:28,630 - INFO - joeynmt.training - Epoch   2, Step:   165000, Batch Loss:     1.623882, Tokens per Sec:    15683, Lr: 0.000300\n",
      "2021-08-03 07:40:02,793 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 07:40:02,793 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 07:40:02,793 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 07:40:03,990 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 07:40:03,991 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 07:40:04,849 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 07:40:04,850 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 07:40:04,851 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 07:40:04,851 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am draw close to God , so good for me . ”\n",
      "2021-08-03 07:40:04,851 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 07:40:04,851 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 07:40:04,852 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 07:40:04,852 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we live in godly devotion is greater than the many riches that are standing . ”\n",
      "2021-08-03 07:40:04,852 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 07:40:04,852 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 07:40:04,852 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 07:40:04,853 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The other prophecies of Daniel have a detailed point on God’s Kingdom .\n",
      "2021-08-03 07:40:04,853 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 07:40:04,853 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 07:40:04,853 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 07:40:04,853 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-03 07:40:04,854 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   165000: bleu:  26.37, loss: 193757.0156, ppl:   4.9547, duration: 96.2233s\n",
      "2021-08-03 07:40:18,916 - INFO - joeynmt.training - Epoch   2, Step:   165100, Batch Loss:     1.779432, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-08-03 07:40:32,774 - INFO - joeynmt.training - Epoch   2, Step:   165200, Batch Loss:     1.804491, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-08-03 07:40:33,078 - INFO - joeynmt.training - Epoch   2: total training loss 9443.36\n",
      "2021-08-03 07:40:33,079 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 07:40:47,243 - INFO - joeynmt.training - Epoch   3, Step:   165300, Batch Loss:     1.800134, Tokens per Sec:    14927, Lr: 0.000300\n",
      "2021-08-03 07:41:01,263 - INFO - joeynmt.training - Epoch   3, Step:   165400, Batch Loss:     1.520257, Tokens per Sec:    16308, Lr: 0.000300\n",
      "2021-08-03 07:41:14,827 - INFO - joeynmt.training - Epoch   3, Step:   165500, Batch Loss:     1.794048, Tokens per Sec:    15456, Lr: 0.000300\n",
      "2021-08-03 07:41:28,767 - INFO - joeynmt.training - Epoch   3, Step:   165600, Batch Loss:     1.824334, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-03 07:41:42,555 - INFO - joeynmt.training - Epoch   3, Step:   165700, Batch Loss:     1.697542, Tokens per Sec:    15591, Lr: 0.000300\n",
      "2021-08-03 07:41:55,939 - INFO - joeynmt.training - Epoch   3, Step:   165800, Batch Loss:     1.875858, Tokens per Sec:    15169, Lr: 0.000300\n",
      "2021-08-03 07:42:09,794 - INFO - joeynmt.training - Epoch   3, Step:   165900, Batch Loss:     1.916020, Tokens per Sec:    16206, Lr: 0.000300\n",
      "2021-08-03 07:42:23,610 - INFO - joeynmt.training - Epoch   3, Step:   166000, Batch Loss:     1.839056, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-03 07:42:37,513 - INFO - joeynmt.training - Epoch   3, Step:   166100, Batch Loss:     1.594059, Tokens per Sec:    15906, Lr: 0.000300\n",
      "2021-08-03 07:42:51,471 - INFO - joeynmt.training - Epoch   3, Step:   166200, Batch Loss:     1.862682, Tokens per Sec:    15803, Lr: 0.000300\n",
      "2021-08-03 07:43:05,152 - INFO - joeynmt.training - Epoch   3, Step:   166300, Batch Loss:     1.752350, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-08-03 07:43:18,930 - INFO - joeynmt.training - Epoch   3, Step:   166400, Batch Loss:     1.832993, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-03 07:43:32,390 - INFO - joeynmt.training - Epoch   3, Step:   166500, Batch Loss:     1.850465, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-08-03 07:43:45,997 - INFO - joeynmt.training - Epoch   3, Step:   166600, Batch Loss:     1.815934, Tokens per Sec:    15661, Lr: 0.000300\n",
      "2021-08-03 07:43:59,898 - INFO - joeynmt.training - Epoch   3, Step:   166700, Batch Loss:     1.954551, Tokens per Sec:    15809, Lr: 0.000300\n",
      "2021-08-03 07:44:13,733 - INFO - joeynmt.training - Epoch   3, Step:   166800, Batch Loss:     1.731832, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-08-03 07:44:27,742 - INFO - joeynmt.training - Epoch   3, Step:   166900, Batch Loss:     1.687245, Tokens per Sec:    15636, Lr: 0.000300\n",
      "2021-08-03 07:44:41,515 - INFO - joeynmt.training - Epoch   3, Step:   167000, Batch Loss:     1.826807, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-08-03 07:44:55,173 - INFO - joeynmt.training - Epoch   3, Step:   167100, Batch Loss:     1.676582, Tokens per Sec:    16239, Lr: 0.000300\n",
      "2021-08-03 07:45:08,777 - INFO - joeynmt.training - Epoch   3, Step:   167200, Batch Loss:     1.919222, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-03 07:45:22,687 - INFO - joeynmt.training - Epoch   3, Step:   167300, Batch Loss:     1.795009, Tokens per Sec:    16361, Lr: 0.000300\n",
      "2021-08-03 07:45:36,292 - INFO - joeynmt.training - Epoch   3, Step:   167400, Batch Loss:     1.784619, Tokens per Sec:    15423, Lr: 0.000300\n",
      "2021-08-03 07:45:50,247 - INFO - joeynmt.training - Epoch   3, Step:   167500, Batch Loss:     1.776325, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-03 07:46:03,967 - INFO - joeynmt.training - Epoch   3, Step:   167600, Batch Loss:     1.918239, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-03 07:46:17,660 - INFO - joeynmt.training - Epoch   3, Step:   167700, Batch Loss:     1.867140, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-03 07:46:31,422 - INFO - joeynmt.training - Epoch   3, Step:   167800, Batch Loss:     2.015460, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-03 07:46:45,256 - INFO - joeynmt.training - Epoch   3, Step:   167900, Batch Loss:     1.649896, Tokens per Sec:    15656, Lr: 0.000300\n",
      "2021-08-03 07:46:59,079 - INFO - joeynmt.training - Epoch   3, Step:   168000, Batch Loss:     1.779893, Tokens per Sec:    16269, Lr: 0.000300\n",
      "2021-08-03 07:47:12,796 - INFO - joeynmt.training - Epoch   3, Step:   168100, Batch Loss:     1.857513, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-08-03 07:47:26,622 - INFO - joeynmt.training - Epoch   3, Step:   168200, Batch Loss:     1.504772, Tokens per Sec:    16278, Lr: 0.000300\n",
      "2021-08-03 07:47:40,427 - INFO - joeynmt.training - Epoch   3, Step:   168300, Batch Loss:     1.827977, Tokens per Sec:    15859, Lr: 0.000300\n",
      "2021-08-03 07:47:54,253 - INFO - joeynmt.training - Epoch   3, Step:   168400, Batch Loss:     1.806236, Tokens per Sec:    15607, Lr: 0.000300\n",
      "2021-08-03 07:48:08,130 - INFO - joeynmt.training - Epoch   3, Step:   168500, Batch Loss:     1.925423, Tokens per Sec:    16362, Lr: 0.000300\n",
      "2021-08-03 07:48:21,876 - INFO - joeynmt.training - Epoch   3, Step:   168600, Batch Loss:     1.873325, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-08-03 07:48:35,570 - INFO - joeynmt.training - Epoch   3, Step:   168700, Batch Loss:     1.817653, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-03 07:48:49,448 - INFO - joeynmt.training - Epoch   3, Step:   168800, Batch Loss:     1.619448, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-08-03 07:49:03,039 - INFO - joeynmt.training - Epoch   3, Step:   168900, Batch Loss:     1.772504, Tokens per Sec:    15501, Lr: 0.000300\n",
      "2021-08-03 07:49:16,952 - INFO - joeynmt.training - Epoch   3, Step:   169000, Batch Loss:     1.713114, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-08-03 07:49:30,699 - INFO - joeynmt.training - Epoch   3, Step:   169100, Batch Loss:     1.844602, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-08-03 07:49:44,395 - INFO - joeynmt.training - Epoch   3, Step:   169200, Batch Loss:     1.797411, Tokens per Sec:    15624, Lr: 0.000300\n",
      "2021-08-03 07:49:58,250 - INFO - joeynmt.training - Epoch   3, Step:   169300, Batch Loss:     1.817199, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-08-03 07:50:11,926 - INFO - joeynmt.training - Epoch   3, Step:   169400, Batch Loss:     1.659185, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-03 07:50:25,522 - INFO - joeynmt.training - Epoch   3, Step:   169500, Batch Loss:     1.906285, Tokens per Sec:    15730, Lr: 0.000300\n",
      "2021-08-03 07:50:39,455 - INFO - joeynmt.training - Epoch   3, Step:   169600, Batch Loss:     1.780875, Tokens per Sec:    15730, Lr: 0.000300\n",
      "2021-08-03 07:50:53,328 - INFO - joeynmt.training - Epoch   3, Step:   169700, Batch Loss:     1.678092, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-03 07:51:07,178 - INFO - joeynmt.training - Epoch   3, Step:   169800, Batch Loss:     1.714326, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-08-03 07:51:20,838 - INFO - joeynmt.training - Epoch   3, Step:   169900, Batch Loss:     1.898538, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-08-03 07:51:34,801 - INFO - joeynmt.training - Epoch   3, Step:   170000, Batch Loss:     1.665265, Tokens per Sec:    16288, Lr: 0.000300\n",
      "2021-08-03 07:53:09,815 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 07:53:09,815 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 07:53:09,815 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 07:53:11,009 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 07:53:11,009 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 07:53:11,769 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 07:53:11,769 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 07:53:11,770 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 07:53:11,770 - INFO - joeynmt.training - \tHypothesis: He sang : “ But as for me , I draw close to God , good for me . ”\n",
      "2021-08-03 07:53:11,770 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 07:53:11,770 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 07:53:11,770 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 07:53:11,771 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion to Jehovah is greater than many riches are standing . ”\n",
      "2021-08-03 07:53:11,771 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 07:53:11,771 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 07:53:11,771 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 07:53:11,771 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The other prophecies of Daniel have a detailed meaning on God’s Kingdom .\n",
      "2021-08-03 07:53:11,772 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 07:53:11,772 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 07:53:11,772 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 07:53:11,772 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-03 07:53:11,772 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   170000: bleu:  26.53, loss: 193487.0781, ppl:   4.9437, duration: 96.9714s\n",
      "2021-08-03 07:53:25,778 - INFO - joeynmt.training - Epoch   3, Step:   170100, Batch Loss:     1.667416, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-03 07:53:39,654 - INFO - joeynmt.training - Epoch   3, Step:   170200, Batch Loss:     1.740514, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-03 07:53:53,449 - INFO - joeynmt.training - Epoch   3, Step:   170300, Batch Loss:     1.877884, Tokens per Sec:    15726, Lr: 0.000300\n",
      "2021-08-03 07:54:07,217 - INFO - joeynmt.training - Epoch   3, Step:   170400, Batch Loss:     1.700178, Tokens per Sec:    15741, Lr: 0.000300\n",
      "2021-08-03 07:54:21,180 - INFO - joeynmt.training - Epoch   3, Step:   170500, Batch Loss:     1.826785, Tokens per Sec:    16070, Lr: 0.000300\n",
      "2021-08-03 07:54:26,545 - INFO - joeynmt.training - Epoch   3: total training loss 9422.22\n",
      "2021-08-03 07:54:26,546 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 07:54:35,653 - INFO - joeynmt.training - Epoch   4, Step:   170600, Batch Loss:     1.643528, Tokens per Sec:    14772, Lr: 0.000300\n",
      "2021-08-03 07:54:49,437 - INFO - joeynmt.training - Epoch   4, Step:   170700, Batch Loss:     1.768041, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-03 07:55:03,019 - INFO - joeynmt.training - Epoch   4, Step:   170800, Batch Loss:     2.193409, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-03 07:55:16,685 - INFO - joeynmt.training - Epoch   4, Step:   170900, Batch Loss:     1.657536, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-08-03 07:55:30,553 - INFO - joeynmt.training - Epoch   4, Step:   171000, Batch Loss:     1.656390, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-08-03 07:55:44,361 - INFO - joeynmt.training - Epoch   4, Step:   171100, Batch Loss:     1.773473, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-03 07:55:57,985 - INFO - joeynmt.training - Epoch   4, Step:   171200, Batch Loss:     1.572512, Tokens per Sec:    15659, Lr: 0.000300\n",
      "2021-08-03 07:56:11,763 - INFO - joeynmt.training - Epoch   4, Step:   171300, Batch Loss:     1.788510, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-03 07:56:25,511 - INFO - joeynmt.training - Epoch   4, Step:   171400, Batch Loss:     1.827784, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-08-03 07:56:39,179 - INFO - joeynmt.training - Epoch   4, Step:   171500, Batch Loss:     1.814182, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-03 07:56:52,775 - INFO - joeynmt.training - Epoch   4, Step:   171600, Batch Loss:     1.770109, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-08-03 07:57:06,293 - INFO - joeynmt.training - Epoch   4, Step:   171700, Batch Loss:     1.686969, Tokens per Sec:    15722, Lr: 0.000300\n",
      "2021-08-03 07:57:20,217 - INFO - joeynmt.training - Epoch   4, Step:   171800, Batch Loss:     1.912859, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-08-03 07:57:33,865 - INFO - joeynmt.training - Epoch   4, Step:   171900, Batch Loss:     1.684288, Tokens per Sec:    15559, Lr: 0.000300\n",
      "2021-08-03 07:57:47,659 - INFO - joeynmt.training - Epoch   4, Step:   172000, Batch Loss:     1.924717, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-08-03 07:58:01,383 - INFO - joeynmt.training - Epoch   4, Step:   172100, Batch Loss:     1.715611, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-03 07:58:15,341 - INFO - joeynmt.training - Epoch   4, Step:   172200, Batch Loss:     1.843288, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-03 07:58:29,053 - INFO - joeynmt.training - Epoch   4, Step:   172300, Batch Loss:     1.483725, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-03 07:58:42,825 - INFO - joeynmt.training - Epoch   4, Step:   172400, Batch Loss:     1.710112, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-03 07:58:56,681 - INFO - joeynmt.training - Epoch   4, Step:   172500, Batch Loss:     1.436275, Tokens per Sec:    15672, Lr: 0.000300\n",
      "2021-08-03 07:59:10,652 - INFO - joeynmt.training - Epoch   4, Step:   172600, Batch Loss:     1.885412, Tokens per Sec:    15962, Lr: 0.000300\n",
      "2021-08-03 07:59:24,598 - INFO - joeynmt.training - Epoch   4, Step:   172700, Batch Loss:     1.678052, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-08-03 07:59:38,520 - INFO - joeynmt.training - Epoch   4, Step:   172800, Batch Loss:     1.848248, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-08-03 07:59:52,575 - INFO - joeynmt.training - Epoch   4, Step:   172900, Batch Loss:     1.572484, Tokens per Sec:    15669, Lr: 0.000300\n",
      "2021-08-03 08:00:06,699 - INFO - joeynmt.training - Epoch   4, Step:   173000, Batch Loss:     1.658121, Tokens per Sec:    15698, Lr: 0.000300\n",
      "2021-08-03 08:00:20,625 - INFO - joeynmt.training - Epoch   4, Step:   173100, Batch Loss:     1.721617, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-08-03 08:00:34,613 - INFO - joeynmt.training - Epoch   4, Step:   173200, Batch Loss:     2.035606, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-08-03 08:00:48,359 - INFO - joeynmt.training - Epoch   4, Step:   173300, Batch Loss:     1.936406, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-03 08:01:02,106 - INFO - joeynmt.training - Epoch   4, Step:   173400, Batch Loss:     1.820527, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-03 08:01:15,967 - INFO - joeynmt.training - Epoch   4, Step:   173500, Batch Loss:     1.731731, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-08-03 08:01:30,132 - INFO - joeynmt.training - Epoch   4, Step:   173600, Batch Loss:     1.651024, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-03 08:01:43,935 - INFO - joeynmt.training - Epoch   4, Step:   173700, Batch Loss:     1.696003, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-08-03 08:01:57,407 - INFO - joeynmt.training - Epoch   4, Step:   173800, Batch Loss:     2.048925, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-08-03 08:02:11,069 - INFO - joeynmt.training - Epoch   4, Step:   173900, Batch Loss:     1.746615, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-08-03 08:02:24,841 - INFO - joeynmt.training - Epoch   4, Step:   174000, Batch Loss:     1.719168, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-03 08:02:38,868 - INFO - joeynmt.training - Epoch   4, Step:   174100, Batch Loss:     1.707914, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-08-03 08:02:52,625 - INFO - joeynmt.training - Epoch   4, Step:   174200, Batch Loss:     1.760244, Tokens per Sec:    15426, Lr: 0.000300\n",
      "2021-08-03 08:03:06,463 - INFO - joeynmt.training - Epoch   4, Step:   174300, Batch Loss:     1.906199, Tokens per Sec:    15649, Lr: 0.000300\n",
      "2021-08-03 08:03:20,354 - INFO - joeynmt.training - Epoch   4, Step:   174400, Batch Loss:     1.677570, Tokens per Sec:    15744, Lr: 0.000300\n",
      "2021-08-03 08:03:34,062 - INFO - joeynmt.training - Epoch   4, Step:   174500, Batch Loss:     1.712410, Tokens per Sec:    15571, Lr: 0.000300\n",
      "2021-08-03 08:03:47,937 - INFO - joeynmt.training - Epoch   4, Step:   174600, Batch Loss:     1.767642, Tokens per Sec:    15609, Lr: 0.000300\n",
      "2021-08-03 08:04:01,656 - INFO - joeynmt.training - Epoch   4, Step:   174700, Batch Loss:     1.913692, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-08-03 08:04:15,539 - INFO - joeynmt.training - Epoch   4, Step:   174800, Batch Loss:     1.715590, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-08-03 08:04:29,144 - INFO - joeynmt.training - Epoch   4, Step:   174900, Batch Loss:     1.692742, Tokens per Sec:    15596, Lr: 0.000300\n",
      "2021-08-03 08:04:43,067 - INFO - joeynmt.training - Epoch   4, Step:   175000, Batch Loss:     1.681661, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-08-03 08:06:15,985 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:06:15,986 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:06:15,986 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:06:17,273 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:06:17,273 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:06:18,037 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:06:18,038 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:06:18,038 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:06:18,038 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am drawing close to God , so good for me . ”\n",
      "2021-08-03 08:06:18,038 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:06:18,039 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:06:18,039 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:06:18,039 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion is greater than the riches that are standing . ”\n",
      "2021-08-03 08:06:18,039 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:06:18,040 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:06:18,040 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:06:18,040 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The book of Daniel has a detailed point on God’s Kingdom .\n",
      "2021-08-03 08:06:18,040 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:06:18,041 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:06:18,041 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:06:18,041 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-03 08:06:18,041 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   175000: bleu:  26.41, loss: 192673.1406, ppl:   4.9106, duration: 94.9741s\n",
      "2021-08-03 08:06:32,323 - INFO - joeynmt.training - Epoch   4, Step:   175100, Batch Loss:     1.735421, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-08-03 08:06:46,066 - INFO - joeynmt.training - Epoch   4, Step:   175200, Batch Loss:     1.793159, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-08-03 08:06:59,816 - INFO - joeynmt.training - Epoch   4, Step:   175300, Batch Loss:     1.973680, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-08-03 08:07:13,604 - INFO - joeynmt.training - Epoch   4, Step:   175400, Batch Loss:     1.849423, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-08-03 08:07:27,375 - INFO - joeynmt.training - Epoch   4, Step:   175500, Batch Loss:     1.769118, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-03 08:07:41,114 - INFO - joeynmt.training - Epoch   4, Step:   175600, Batch Loss:     1.850172, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-08-03 08:07:54,712 - INFO - joeynmt.training - Epoch   4, Step:   175700, Batch Loss:     1.574839, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-08-03 08:08:08,477 - INFO - joeynmt.training - Epoch   4, Step:   175800, Batch Loss:     2.306640, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-03 08:08:18,472 - INFO - joeynmt.training - Epoch   4: total training loss 9396.97\n",
      "2021-08-03 08:08:18,473 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-03 08:08:22,918 - INFO - joeynmt.training - Epoch   5, Step:   175900, Batch Loss:     1.550743, Tokens per Sec:    14565, Lr: 0.000300\n",
      "2021-08-03 08:08:36,666 - INFO - joeynmt.training - Epoch   5, Step:   176000, Batch Loss:     1.833275, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-08-03 08:08:50,716 - INFO - joeynmt.training - Epoch   5, Step:   176100, Batch Loss:     1.897434, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-08-03 08:09:04,542 - INFO - joeynmt.training - Epoch   5, Step:   176200, Batch Loss:     1.616352, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-03 08:09:18,094 - INFO - joeynmt.training - Epoch   5, Step:   176300, Batch Loss:     1.940920, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-08-03 08:09:31,905 - INFO - joeynmt.training - Epoch   5, Step:   176400, Batch Loss:     1.904275, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-08-03 08:09:45,713 - INFO - joeynmt.training - Epoch   5, Step:   176500, Batch Loss:     1.925852, Tokens per Sec:    15607, Lr: 0.000300\n",
      "2021-08-03 08:09:59,581 - INFO - joeynmt.training - Epoch   5, Step:   176600, Batch Loss:     1.605059, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-08-03 08:10:13,303 - INFO - joeynmt.training - Epoch   5, Step:   176700, Batch Loss:     1.723500, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-03 08:10:27,238 - INFO - joeynmt.training - Epoch   5, Step:   176800, Batch Loss:     2.054222, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-03 08:10:40,936 - INFO - joeynmt.training - Epoch   5, Step:   176900, Batch Loss:     1.785625, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-08-03 08:10:54,864 - INFO - joeynmt.training - Epoch   5, Step:   177000, Batch Loss:     1.675332, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-08-03 08:11:08,766 - INFO - joeynmt.training - Epoch   5, Step:   177100, Batch Loss:     1.633561, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-03 08:11:22,771 - INFO - joeynmt.training - Epoch   5, Step:   177200, Batch Loss:     1.813427, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-08-03 08:11:36,446 - INFO - joeynmt.training - Epoch   5, Step:   177300, Batch Loss:     1.709363, Tokens per Sec:    15583, Lr: 0.000300\n",
      "2021-08-03 08:11:50,218 - INFO - joeynmt.training - Epoch   5, Step:   177400, Batch Loss:     1.630548, Tokens per Sec:    16108, Lr: 0.000300\n",
      "2021-08-03 08:12:03,926 - INFO - joeynmt.training - Epoch   5, Step:   177500, Batch Loss:     1.768715, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-03 08:12:17,665 - INFO - joeynmt.training - Epoch   5, Step:   177600, Batch Loss:     1.595856, Tokens per Sec:    15942, Lr: 0.000300\n",
      "2021-08-03 08:12:31,583 - INFO - joeynmt.training - Epoch   5, Step:   177700, Batch Loss:     1.855605, Tokens per Sec:    15600, Lr: 0.000300\n",
      "2021-08-03 08:12:45,449 - INFO - joeynmt.training - Epoch   5, Step:   177800, Batch Loss:     1.747905, Tokens per Sec:    15635, Lr: 0.000300\n",
      "2021-08-03 08:12:59,262 - INFO - joeynmt.training - Epoch   5, Step:   177900, Batch Loss:     1.659274, Tokens per Sec:    15604, Lr: 0.000300\n",
      "2021-08-03 08:13:13,248 - INFO - joeynmt.training - Epoch   5, Step:   178000, Batch Loss:     1.794486, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-08-03 08:13:26,978 - INFO - joeynmt.training - Epoch   5, Step:   178100, Batch Loss:     1.788967, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-08-03 08:13:41,150 - INFO - joeynmt.training - Epoch   5, Step:   178200, Batch Loss:     1.933767, Tokens per Sec:    16118, Lr: 0.000300\n",
      "2021-08-03 08:13:54,781 - INFO - joeynmt.training - Epoch   5, Step:   178300, Batch Loss:     1.418264, Tokens per Sec:    15630, Lr: 0.000300\n",
      "2021-08-03 08:14:08,583 - INFO - joeynmt.training - Epoch   5, Step:   178400, Batch Loss:     1.838245, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-08-03 08:14:22,255 - INFO - joeynmt.training - Epoch   5, Step:   178500, Batch Loss:     1.585271, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-03 08:14:36,269 - INFO - joeynmt.training - Epoch   5, Step:   178600, Batch Loss:     1.708797, Tokens per Sec:    15939, Lr: 0.000300\n",
      "2021-08-03 08:14:50,161 - INFO - joeynmt.training - Epoch   5, Step:   178700, Batch Loss:     1.795661, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-03 08:15:04,049 - INFO - joeynmt.training - Epoch   5, Step:   178800, Batch Loss:     1.949717, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-03 08:15:17,860 - INFO - joeynmt.training - Epoch   5, Step:   178900, Batch Loss:     1.750904, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-03 08:15:31,504 - INFO - joeynmt.training - Epoch   5, Step:   179000, Batch Loss:     1.729735, Tokens per Sec:    15636, Lr: 0.000300\n",
      "2021-08-03 08:15:45,439 - INFO - joeynmt.training - Epoch   5, Step:   179100, Batch Loss:     1.599889, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-03 08:15:59,569 - INFO - joeynmt.training - Epoch   5, Step:   179200, Batch Loss:     1.752309, Tokens per Sec:    15800, Lr: 0.000300\n",
      "2021-08-03 08:16:13,384 - INFO - joeynmt.training - Epoch   5, Step:   179300, Batch Loss:     1.815836, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-03 08:16:27,278 - INFO - joeynmt.training - Epoch   5, Step:   179400, Batch Loss:     1.734876, Tokens per Sec:    15734, Lr: 0.000300\n",
      "2021-08-03 08:16:41,339 - INFO - joeynmt.training - Epoch   5, Step:   179500, Batch Loss:     1.636216, Tokens per Sec:    15921, Lr: 0.000300\n",
      "2021-08-03 08:16:55,219 - INFO - joeynmt.training - Epoch   5, Step:   179600, Batch Loss:     1.855579, Tokens per Sec:    15768, Lr: 0.000300\n",
      "2021-08-03 08:17:08,917 - INFO - joeynmt.training - Epoch   5, Step:   179700, Batch Loss:     1.620602, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-08-03 08:17:23,014 - INFO - joeynmt.training - Epoch   5, Step:   179800, Batch Loss:     1.664960, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-08-03 08:17:37,033 - INFO - joeynmt.training - Epoch   5, Step:   179900, Batch Loss:     1.838911, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-08-03 08:17:51,054 - INFO - joeynmt.training - Epoch   5, Step:   180000, Batch Loss:     1.788961, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-08-03 08:19:26,166 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:19:26,167 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:19:26,167 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:19:27,469 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:19:27,469 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:19:28,208 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:19:28,209 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:19:28,209 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:19:28,209 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , I draw close to God , so good for me . ”\n",
      "2021-08-03 08:19:28,210 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:19:28,210 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:19:28,211 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:19:28,211 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident of our fear of Jehovah is greater than the abundance of riches that are standing . ”\n",
      "2021-08-03 08:19:28,211 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:19:28,211 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:19:28,212 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:19:28,212 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The book of Daniel has a detailed effect on God’s Kingdom .\n",
      "2021-08-03 08:19:28,212 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:19:28,213 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:19:28,213 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:19:28,213 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 08:19:28,213 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   180000: bleu:  26.91, loss: 192016.7656, ppl:   4.8840, duration: 97.1589s\n",
      "2021-08-03 08:19:42,328 - INFO - joeynmt.training - Epoch   5, Step:   180100, Batch Loss:     1.774421, Tokens per Sec:    15354, Lr: 0.000300\n",
      "2021-08-03 08:19:56,292 - INFO - joeynmt.training - Epoch   5, Step:   180200, Batch Loss:     1.859836, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-08-03 08:20:10,265 - INFO - joeynmt.training - Epoch   5, Step:   180300, Batch Loss:     1.681119, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-03 08:20:24,134 - INFO - joeynmt.training - Epoch   5, Step:   180400, Batch Loss:     1.754267, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-03 08:20:38,005 - INFO - joeynmt.training - Epoch   5, Step:   180500, Batch Loss:     1.906924, Tokens per Sec:    15762, Lr: 0.000300\n",
      "2021-08-03 08:20:52,036 - INFO - joeynmt.training - Epoch   5, Step:   180600, Batch Loss:     1.657133, Tokens per Sec:    15782, Lr: 0.000300\n",
      "2021-08-03 08:21:05,812 - INFO - joeynmt.training - Epoch   5, Step:   180700, Batch Loss:     1.635009, Tokens per Sec:    15790, Lr: 0.000300\n",
      "2021-08-03 08:21:19,560 - INFO - joeynmt.training - Epoch   5, Step:   180800, Batch Loss:     1.788648, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-03 08:21:33,411 - INFO - joeynmt.training - Epoch   5, Step:   180900, Batch Loss:     1.687748, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-03 08:21:47,317 - INFO - joeynmt.training - Epoch   5, Step:   181000, Batch Loss:     1.686798, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-03 08:22:01,377 - INFO - joeynmt.training - Epoch   5, Step:   181100, Batch Loss:     1.661021, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-08-03 08:22:14,380 - INFO - joeynmt.training - Epoch   5: total training loss 9348.89\n",
      "2021-08-03 08:22:14,380 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-03 08:22:15,645 - INFO - joeynmt.training - Epoch   6, Step:   181200, Batch Loss:     1.724020, Tokens per Sec:     7536, Lr: 0.000300\n",
      "2021-08-03 08:22:29,608 - INFO - joeynmt.training - Epoch   6, Step:   181300, Batch Loss:     1.612668, Tokens per Sec:    15729, Lr: 0.000300\n",
      "2021-08-03 08:22:43,436 - INFO - joeynmt.training - Epoch   6, Step:   181400, Batch Loss:     1.801482, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-08-03 08:22:57,236 - INFO - joeynmt.training - Epoch   6, Step:   181500, Batch Loss:     1.619906, Tokens per Sec:    15956, Lr: 0.000300\n",
      "2021-08-03 08:23:10,985 - INFO - joeynmt.training - Epoch   6, Step:   181600, Batch Loss:     1.712218, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-03 08:23:24,892 - INFO - joeynmt.training - Epoch   6, Step:   181700, Batch Loss:     1.814456, Tokens per Sec:    16021, Lr: 0.000300\n",
      "2021-08-03 08:23:38,826 - INFO - joeynmt.training - Epoch   6, Step:   181800, Batch Loss:     1.813496, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-03 08:23:52,522 - INFO - joeynmt.training - Epoch   6, Step:   181900, Batch Loss:     1.677242, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-03 08:24:06,436 - INFO - joeynmt.training - Epoch   6, Step:   182000, Batch Loss:     1.627643, Tokens per Sec:    16058, Lr: 0.000300\n",
      "2021-08-03 08:24:20,034 - INFO - joeynmt.training - Epoch   6, Step:   182100, Batch Loss:     1.716016, Tokens per Sec:    15622, Lr: 0.000300\n",
      "2021-08-03 08:24:33,853 - INFO - joeynmt.training - Epoch   6, Step:   182200, Batch Loss:     1.727843, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-08-03 08:24:47,669 - INFO - joeynmt.training - Epoch   6, Step:   182300, Batch Loss:     2.059395, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-03 08:25:01,360 - INFO - joeynmt.training - Epoch   6, Step:   182400, Batch Loss:     1.782383, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-08-03 08:25:15,064 - INFO - joeynmt.training - Epoch   6, Step:   182500, Batch Loss:     1.677355, Tokens per Sec:    15675, Lr: 0.000300\n",
      "2021-08-03 08:25:28,939 - INFO - joeynmt.training - Epoch   6, Step:   182600, Batch Loss:     1.546994, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-08-03 08:25:42,903 - INFO - joeynmt.training - Epoch   6, Step:   182700, Batch Loss:     1.553925, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-03 08:25:56,868 - INFO - joeynmt.training - Epoch   6, Step:   182800, Batch Loss:     1.569968, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-08-03 08:26:10,681 - INFO - joeynmt.training - Epoch   6, Step:   182900, Batch Loss:     1.640944, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-03 08:26:24,342 - INFO - joeynmt.training - Epoch   6, Step:   183000, Batch Loss:     1.720242, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-03 08:26:37,972 - INFO - joeynmt.training - Epoch   6, Step:   183100, Batch Loss:     1.708139, Tokens per Sec:    15796, Lr: 0.000300\n",
      "2021-08-03 08:26:51,968 - INFO - joeynmt.training - Epoch   6, Step:   183200, Batch Loss:     1.721840, Tokens per Sec:    15664, Lr: 0.000300\n",
      "2021-08-03 08:27:05,919 - INFO - joeynmt.training - Epoch   6, Step:   183300, Batch Loss:     1.704368, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-08-03 08:27:19,816 - INFO - joeynmt.training - Epoch   6, Step:   183400, Batch Loss:     1.740753, Tokens per Sec:    15605, Lr: 0.000300\n",
      "2021-08-03 08:27:33,706 - INFO - joeynmt.training - Epoch   6, Step:   183500, Batch Loss:     1.822774, Tokens per Sec:    15678, Lr: 0.000300\n",
      "2021-08-03 08:27:47,373 - INFO - joeynmt.training - Epoch   6, Step:   183600, Batch Loss:     1.699879, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-08-03 08:28:01,072 - INFO - joeynmt.training - Epoch   6, Step:   183700, Batch Loss:     1.755155, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-03 08:28:15,018 - INFO - joeynmt.training - Epoch   6, Step:   183800, Batch Loss:     1.781382, Tokens per Sec:    16232, Lr: 0.000300\n",
      "2021-08-03 08:28:28,991 - INFO - joeynmt.training - Epoch   6, Step:   183900, Batch Loss:     1.972451, Tokens per Sec:    15498, Lr: 0.000300\n",
      "2021-08-03 08:28:43,084 - INFO - joeynmt.training - Epoch   6, Step:   184000, Batch Loss:     1.713813, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-03 08:28:56,983 - INFO - joeynmt.training - Epoch   6, Step:   184100, Batch Loss:     1.680041, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-08-03 08:29:10,673 - INFO - joeynmt.training - Epoch   6, Step:   184200, Batch Loss:     1.705900, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-03 08:29:24,476 - INFO - joeynmt.training - Epoch   6, Step:   184300, Batch Loss:     1.893668, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-08-03 08:29:38,362 - INFO - joeynmt.training - Epoch   6, Step:   184400, Batch Loss:     1.574377, Tokens per Sec:    15667, Lr: 0.000300\n",
      "2021-08-03 08:29:52,430 - INFO - joeynmt.training - Epoch   6, Step:   184500, Batch Loss:     1.703409, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-03 08:30:06,500 - INFO - joeynmt.training - Epoch   6, Step:   184600, Batch Loss:     1.963059, Tokens per Sec:    16283, Lr: 0.000300\n",
      "2021-08-03 08:30:20,273 - INFO - joeynmt.training - Epoch   6, Step:   184700, Batch Loss:     1.538433, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-03 08:30:34,290 - INFO - joeynmt.training - Epoch   6, Step:   184800, Batch Loss:     1.606881, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-08-03 08:30:48,145 - INFO - joeynmt.training - Epoch   6, Step:   184900, Batch Loss:     1.957719, Tokens per Sec:    15557, Lr: 0.000300\n",
      "2021-08-03 08:31:01,905 - INFO - joeynmt.training - Epoch   6, Step:   185000, Batch Loss:     1.519927, Tokens per Sec:    15576, Lr: 0.000300\n",
      "2021-08-03 08:32:35,960 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:32:35,960 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:32:35,960 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:32:37,869 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:32:37,870 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:32:37,870 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:32:37,870 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am drawing close to God , so good for me . ”\n",
      "2021-08-03 08:32:37,870 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:32:37,871 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:32:37,871 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:32:37,871 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident is godly devotion to Jehovah , we are far more than the abundance of riches that are standing . ”\n",
      "2021-08-03 08:32:37,871 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:32:37,872 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:32:37,872 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:32:37,872 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has something about God’s Kingdom .\n",
      "2021-08-03 08:32:37,872 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:32:37,873 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:32:37,873 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:32:37,873 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 08:32:37,873 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   185000: bleu:  26.86, loss: 192402.7656, ppl:   4.8996, duration: 95.9682s\n",
      "2021-08-03 08:32:52,051 - INFO - joeynmt.training - Epoch   6, Step:   185100, Batch Loss:     1.586402, Tokens per Sec:    15591, Lr: 0.000300\n",
      "2021-08-03 08:33:05,933 - INFO - joeynmt.training - Epoch   6, Step:   185200, Batch Loss:     1.907828, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-08-03 08:33:19,931 - INFO - joeynmt.training - Epoch   6, Step:   185300, Batch Loss:     1.722411, Tokens per Sec:    15768, Lr: 0.000300\n",
      "2021-08-03 08:33:33,860 - INFO - joeynmt.training - Epoch   6, Step:   185400, Batch Loss:     1.659487, Tokens per Sec:    15521, Lr: 0.000300\n",
      "2021-08-03 08:33:47,883 - INFO - joeynmt.training - Epoch   6, Step:   185500, Batch Loss:     1.751256, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-03 08:34:01,658 - INFO - joeynmt.training - Epoch   6, Step:   185600, Batch Loss:     1.775326, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-08-03 08:34:15,349 - INFO - joeynmt.training - Epoch   6, Step:   185700, Batch Loss:     1.547792, Tokens per Sec:    15775, Lr: 0.000300\n",
      "2021-08-03 08:34:29,296 - INFO - joeynmt.training - Epoch   6, Step:   185800, Batch Loss:     1.710366, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-08-03 08:34:43,139 - INFO - joeynmt.training - Epoch   6, Step:   185900, Batch Loss:     1.866827, Tokens per Sec:    15859, Lr: 0.000300\n",
      "2021-08-03 08:34:57,035 - INFO - joeynmt.training - Epoch   6, Step:   186000, Batch Loss:     1.795229, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-03 08:35:10,927 - INFO - joeynmt.training - Epoch   6, Step:   186100, Batch Loss:     1.872666, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-08-03 08:35:24,630 - INFO - joeynmt.training - Epoch   6, Step:   186200, Batch Loss:     1.643976, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-08-03 08:35:38,634 - INFO - joeynmt.training - Epoch   6, Step:   186300, Batch Loss:     1.857468, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-03 08:35:52,379 - INFO - joeynmt.training - Epoch   6, Step:   186400, Batch Loss:     1.652121, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-08-03 08:36:06,260 - INFO - joeynmt.training - Epoch   6, Step:   186500, Batch Loss:     1.887553, Tokens per Sec:    15924, Lr: 0.000300\n",
      "2021-08-03 08:36:09,457 - INFO - joeynmt.training - Epoch   6: total training loss 9331.66\n",
      "2021-08-03 08:36:09,457 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-03 08:36:20,584 - INFO - joeynmt.training - Epoch   7, Step:   186600, Batch Loss:     1.799796, Tokens per Sec:    15148, Lr: 0.000300\n",
      "2021-08-03 08:36:34,429 - INFO - joeynmt.training - Epoch   7, Step:   186700, Batch Loss:     1.646137, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-08-03 08:36:48,354 - INFO - joeynmt.training - Epoch   7, Step:   186800, Batch Loss:     1.786064, Tokens per Sec:    15719, Lr: 0.000300\n",
      "2021-08-03 08:37:02,177 - INFO - joeynmt.training - Epoch   7, Step:   186900, Batch Loss:     1.783352, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-08-03 08:37:15,838 - INFO - joeynmt.training - Epoch   7, Step:   187000, Batch Loss:     1.653054, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-08-03 08:37:29,540 - INFO - joeynmt.training - Epoch   7, Step:   187100, Batch Loss:     1.628743, Tokens per Sec:    15730, Lr: 0.000300\n",
      "2021-08-03 08:37:43,427 - INFO - joeynmt.training - Epoch   7, Step:   187200, Batch Loss:     1.720001, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-08-03 08:37:57,436 - INFO - joeynmt.training - Epoch   7, Step:   187300, Batch Loss:     1.771353, Tokens per Sec:    15699, Lr: 0.000300\n",
      "2021-08-03 08:38:11,220 - INFO - joeynmt.training - Epoch   7, Step:   187400, Batch Loss:     1.805436, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-08-03 08:38:25,078 - INFO - joeynmt.training - Epoch   7, Step:   187500, Batch Loss:     1.448090, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-08-03 08:38:38,865 - INFO - joeynmt.training - Epoch   7, Step:   187600, Batch Loss:     1.800697, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-03 08:38:52,669 - INFO - joeynmt.training - Epoch   7, Step:   187700, Batch Loss:     1.728390, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-08-03 08:39:06,351 - INFO - joeynmt.training - Epoch   7, Step:   187800, Batch Loss:     1.626985, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-08-03 08:39:20,306 - INFO - joeynmt.training - Epoch   7, Step:   187900, Batch Loss:     2.061786, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-03 08:39:34,193 - INFO - joeynmt.training - Epoch   7, Step:   188000, Batch Loss:     1.674742, Tokens per Sec:    15535, Lr: 0.000300\n",
      "2021-08-03 08:39:48,073 - INFO - joeynmt.training - Epoch   7, Step:   188100, Batch Loss:     1.649401, Tokens per Sec:    16008, Lr: 0.000300\n",
      "2021-08-03 08:40:01,942 - INFO - joeynmt.training - Epoch   7, Step:   188200, Batch Loss:     1.870967, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-03 08:40:15,533 - INFO - joeynmt.training - Epoch   7, Step:   188300, Batch Loss:     1.847695, Tokens per Sec:    15463, Lr: 0.000300\n",
      "2021-08-03 08:40:29,338 - INFO - joeynmt.training - Epoch   7, Step:   188400, Batch Loss:     1.580896, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-08-03 08:40:43,333 - INFO - joeynmt.training - Epoch   7, Step:   188500, Batch Loss:     1.791460, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-08-03 08:40:56,941 - INFO - joeynmt.training - Epoch   7, Step:   188600, Batch Loss:     1.447956, Tokens per Sec:    15818, Lr: 0.000300\n",
      "2021-08-03 08:41:10,604 - INFO - joeynmt.training - Epoch   7, Step:   188700, Batch Loss:     2.093695, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-08-03 08:41:24,429 - INFO - joeynmt.training - Epoch   7, Step:   188800, Batch Loss:     1.733176, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-08-03 08:41:38,160 - INFO - joeynmt.training - Epoch   7, Step:   188900, Batch Loss:     1.983209, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-08-03 08:41:52,337 - INFO - joeynmt.training - Epoch   7, Step:   189000, Batch Loss:     1.682247, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-03 08:42:05,968 - INFO - joeynmt.training - Epoch   7, Step:   189100, Batch Loss:     1.902985, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-03 08:42:19,793 - INFO - joeynmt.training - Epoch   7, Step:   189200, Batch Loss:     1.734474, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-03 08:42:33,512 - INFO - joeynmt.training - Epoch   7, Step:   189300, Batch Loss:     2.298936, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-08-03 08:42:47,365 - INFO - joeynmt.training - Epoch   7, Step:   189400, Batch Loss:     1.874057, Tokens per Sec:    15691, Lr: 0.000300\n",
      "2021-08-03 08:43:01,402 - INFO - joeynmt.training - Epoch   7, Step:   189500, Batch Loss:     1.603151, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-08-03 08:43:14,996 - INFO - joeynmt.training - Epoch   7, Step:   189600, Batch Loss:     1.613186, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-03 08:43:28,587 - INFO - joeynmt.training - Epoch   7, Step:   189700, Batch Loss:     1.815138, Tokens per Sec:    15819, Lr: 0.000300\n",
      "2021-08-03 08:43:42,392 - INFO - joeynmt.training - Epoch   7, Step:   189800, Batch Loss:     1.952123, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-03 08:43:56,228 - INFO - joeynmt.training - Epoch   7, Step:   189900, Batch Loss:     1.673029, Tokens per Sec:    15761, Lr: 0.000300\n",
      "2021-08-03 08:44:10,099 - INFO - joeynmt.training - Epoch   7, Step:   190000, Batch Loss:     1.653923, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-03 08:45:46,491 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:45:46,491 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:45:46,491 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:45:47,701 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:45:47,701 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:45:48,781 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:45:48,782 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:45:48,782 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:45:48,782 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , you draw close to God , good for me . ”\n",
      "2021-08-03 08:45:48,783 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:45:48,783 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:45:48,783 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:45:48,783 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are fear of Jehovah is greater than the riches that are standing . ”\n",
      "2021-08-03 08:45:48,783 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:45:48,784 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:45:48,784 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:45:48,784 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has something about God’s Kingdom .\n",
      "2021-08-03 08:45:48,784 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:45:48,785 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:45:48,785 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:45:48,785 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 08:45:48,785 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   190000: bleu:  27.05, loss: 191457.7969, ppl:   4.8615, duration: 98.6864s\n",
      "2021-08-03 08:46:02,626 - INFO - joeynmt.training - Epoch   7, Step:   190100, Batch Loss:     1.744133, Tokens per Sec:    15919, Lr: 0.000300\n",
      "2021-08-03 08:46:16,574 - INFO - joeynmt.training - Epoch   7, Step:   190200, Batch Loss:     1.784345, Tokens per Sec:    16313, Lr: 0.000300\n",
      "2021-08-03 08:46:30,424 - INFO - joeynmt.training - Epoch   7, Step:   190300, Batch Loss:     1.937689, Tokens per Sec:    15886, Lr: 0.000300\n",
      "2021-08-03 08:46:44,300 - INFO - joeynmt.training - Epoch   7, Step:   190400, Batch Loss:     1.784784, Tokens per Sec:    15871, Lr: 0.000300\n",
      "2021-08-03 08:46:58,043 - INFO - joeynmt.training - Epoch   7, Step:   190500, Batch Loss:     1.794451, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-03 08:47:11,804 - INFO - joeynmt.training - Epoch   7, Step:   190600, Batch Loss:     1.419331, Tokens per Sec:    16100, Lr: 0.000300\n",
      "2021-08-03 08:47:25,453 - INFO - joeynmt.training - Epoch   7, Step:   190700, Batch Loss:     1.777611, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-03 08:47:39,399 - INFO - joeynmt.training - Epoch   7, Step:   190800, Batch Loss:     1.642254, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-03 08:47:53,205 - INFO - joeynmt.training - Epoch   7, Step:   190900, Batch Loss:     1.826206, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-03 08:48:07,086 - INFO - joeynmt.training - Epoch   7, Step:   191000, Batch Loss:     1.667206, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-03 08:48:20,766 - INFO - joeynmt.training - Epoch   7, Step:   191100, Batch Loss:     1.646362, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-08-03 08:48:34,668 - INFO - joeynmt.training - Epoch   7, Step:   191200, Batch Loss:     1.883349, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-08-03 08:48:48,524 - INFO - joeynmt.training - Epoch   7, Step:   191300, Batch Loss:     1.638363, Tokens per Sec:    15789, Lr: 0.000300\n",
      "2021-08-03 08:49:02,585 - INFO - joeynmt.training - Epoch   7, Step:   191400, Batch Loss:     1.665578, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-03 08:49:16,301 - INFO - joeynmt.training - Epoch   7, Step:   191500, Batch Loss:     1.656795, Tokens per Sec:    15496, Lr: 0.000300\n",
      "2021-08-03 08:49:30,255 - INFO - joeynmt.training - Epoch   7, Step:   191600, Batch Loss:     1.776717, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-08-03 08:49:43,957 - INFO - joeynmt.training - Epoch   7, Step:   191700, Batch Loss:     1.603140, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-03 08:49:57,785 - INFO - joeynmt.training - Epoch   7, Step:   191800, Batch Loss:     1.601788, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-08-03 08:50:05,306 - INFO - joeynmt.training - Epoch   7: total training loss 9317.04\n",
      "2021-08-03 08:50:05,307 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-03 08:50:12,032 - INFO - joeynmt.training - Epoch   8, Step:   191900, Batch Loss:     1.751228, Tokens per Sec:    14645, Lr: 0.000300\n",
      "2021-08-03 08:50:25,907 - INFO - joeynmt.training - Epoch   8, Step:   192000, Batch Loss:     1.918687, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-08-03 08:50:39,731 - INFO - joeynmt.training - Epoch   8, Step:   192100, Batch Loss:     1.919008, Tokens per Sec:    15800, Lr: 0.000300\n",
      "2021-08-03 08:50:53,536 - INFO - joeynmt.training - Epoch   8, Step:   192200, Batch Loss:     1.626812, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-03 08:51:07,402 - INFO - joeynmt.training - Epoch   8, Step:   192300, Batch Loss:     1.593747, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-08-03 08:51:21,125 - INFO - joeynmt.training - Epoch   8, Step:   192400, Batch Loss:     1.826419, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-03 08:51:34,897 - INFO - joeynmt.training - Epoch   8, Step:   192500, Batch Loss:     1.886692, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-03 08:51:48,786 - INFO - joeynmt.training - Epoch   8, Step:   192600, Batch Loss:     1.603883, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-03 08:52:02,326 - INFO - joeynmt.training - Epoch   8, Step:   192700, Batch Loss:     1.804961, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-08-03 08:52:16,202 - INFO - joeynmt.training - Epoch   8, Step:   192800, Batch Loss:     1.816558, Tokens per Sec:    15942, Lr: 0.000300\n",
      "2021-08-03 08:52:29,913 - INFO - joeynmt.training - Epoch   8, Step:   192900, Batch Loss:     1.739109, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-08-03 08:52:43,682 - INFO - joeynmt.training - Epoch   8, Step:   193000, Batch Loss:     1.932256, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-08-03 08:52:57,758 - INFO - joeynmt.training - Epoch   8, Step:   193100, Batch Loss:     1.687136, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-08-03 08:53:11,626 - INFO - joeynmt.training - Epoch   8, Step:   193200, Batch Loss:     1.725392, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-08-03 08:53:25,326 - INFO - joeynmt.training - Epoch   8, Step:   193300, Batch Loss:     1.772967, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-03 08:53:38,994 - INFO - joeynmt.training - Epoch   8, Step:   193400, Batch Loss:     1.722078, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-08-03 08:53:52,779 - INFO - joeynmt.training - Epoch   8, Step:   193500, Batch Loss:     1.855184, Tokens per Sec:    15679, Lr: 0.000300\n",
      "2021-08-03 08:54:06,663 - INFO - joeynmt.training - Epoch   8, Step:   193600, Batch Loss:     1.818509, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-08-03 08:54:20,311 - INFO - joeynmt.training - Epoch   8, Step:   193700, Batch Loss:     1.756094, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-08-03 08:54:34,172 - INFO - joeynmt.training - Epoch   8, Step:   193800, Batch Loss:     1.859280, Tokens per Sec:    16280, Lr: 0.000300\n",
      "2021-08-03 08:54:47,814 - INFO - joeynmt.training - Epoch   8, Step:   193900, Batch Loss:     1.860592, Tokens per Sec:    15697, Lr: 0.000300\n",
      "2021-08-03 08:55:01,691 - INFO - joeynmt.training - Epoch   8, Step:   194000, Batch Loss:     1.700529, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-08-03 08:55:15,582 - INFO - joeynmt.training - Epoch   8, Step:   194100, Batch Loss:     1.911524, Tokens per Sec:    15597, Lr: 0.000300\n",
      "2021-08-03 08:55:29,197 - INFO - joeynmt.training - Epoch   8, Step:   194200, Batch Loss:     1.453607, Tokens per Sec:    15563, Lr: 0.000300\n",
      "2021-08-03 08:55:43,050 - INFO - joeynmt.training - Epoch   8, Step:   194300, Batch Loss:     1.543020, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-08-03 08:55:56,643 - INFO - joeynmt.training - Epoch   8, Step:   194400, Batch Loss:     1.791455, Tokens per Sec:    15727, Lr: 0.000300\n",
      "2021-08-03 08:56:10,487 - INFO - joeynmt.training - Epoch   8, Step:   194500, Batch Loss:     1.653413, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-08-03 08:56:24,208 - INFO - joeynmt.training - Epoch   8, Step:   194600, Batch Loss:     1.749675, Tokens per Sec:    15713, Lr: 0.000300\n",
      "2021-08-03 08:56:38,052 - INFO - joeynmt.training - Epoch   8, Step:   194700, Batch Loss:     1.754240, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-03 08:56:51,961 - INFO - joeynmt.training - Epoch   8, Step:   194800, Batch Loss:     1.738240, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-03 08:57:05,810 - INFO - joeynmt.training - Epoch   8, Step:   194900, Batch Loss:     1.800327, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-03 08:57:19,675 - INFO - joeynmt.training - Epoch   8, Step:   195000, Batch Loss:     1.838674, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-03 08:58:52,476 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:58:52,476 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:58:52,477 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:58:53,768 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:58:53,768 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:58:54,875 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:58:54,876 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:58:54,876 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:58:54,876 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so good for me . ”\n",
      "2021-08-03 08:58:54,876 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:58:54,877 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:58:54,877 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:58:54,877 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident residents are godly devotion to Jehovah , we are far more than the riches that are standing . ”\n",
      "2021-08-03 08:58:54,877 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:58:54,877 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:58:54,878 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:58:54,878 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has something about God’s Kingdom .\n",
      "2021-08-03 08:58:54,878 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:58:54,878 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:58:54,879 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:58:54,879 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue to conquer ” and complete conquer ?\n",
      "2021-08-03 08:58:54,879 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   195000: bleu:  26.96, loss: 190923.7500, ppl:   4.8401, duration: 95.2040s\n",
      "2021-08-03 08:59:08,889 - INFO - joeynmt.training - Epoch   8, Step:   195100, Batch Loss:     1.801977, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-08-03 08:59:22,685 - INFO - joeynmt.training - Epoch   8, Step:   195200, Batch Loss:     1.721074, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-03 08:59:36,404 - INFO - joeynmt.training - Epoch   8, Step:   195300, Batch Loss:     1.689379, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-03 08:59:50,439 - INFO - joeynmt.training - Epoch   8, Step:   195400, Batch Loss:     1.969334, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-08-03 09:00:04,218 - INFO - joeynmt.training - Epoch   8, Step:   195500, Batch Loss:     1.646075, Tokens per Sec:    15567, Lr: 0.000300\n",
      "2021-08-03 09:00:18,037 - INFO - joeynmt.training - Epoch   8, Step:   195600, Batch Loss:     1.668542, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-08-03 09:00:31,765 - INFO - joeynmt.training - Epoch   8, Step:   195700, Batch Loss:     1.754647, Tokens per Sec:    15614, Lr: 0.000300\n",
      "2021-08-03 09:00:45,435 - INFO - joeynmt.training - Epoch   8, Step:   195800, Batch Loss:     1.670441, Tokens per Sec:    15901, Lr: 0.000300\n",
      "2021-08-03 09:00:59,185 - INFO - joeynmt.training - Epoch   8, Step:   195900, Batch Loss:     1.767852, Tokens per Sec:    16140, Lr: 0.000300\n",
      "2021-08-03 09:01:12,918 - INFO - joeynmt.training - Epoch   8, Step:   196000, Batch Loss:     1.856573, Tokens per Sec:    16204, Lr: 0.000300\n",
      "2021-08-03 09:01:26,983 - INFO - joeynmt.training - Epoch   8, Step:   196100, Batch Loss:     1.868212, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-03 09:01:40,883 - INFO - joeynmt.training - Epoch   8, Step:   196200, Batch Loss:     1.805728, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-03 09:01:54,683 - INFO - joeynmt.training - Epoch   8, Step:   196300, Batch Loss:     1.651695, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-03 09:02:08,495 - INFO - joeynmt.training - Epoch   8, Step:   196400, Batch Loss:     1.639662, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-03 09:02:22,414 - INFO - joeynmt.training - Epoch   8, Step:   196500, Batch Loss:     1.699967, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-03 09:02:36,174 - INFO - joeynmt.training - Epoch   8, Step:   196600, Batch Loss:     1.488231, Tokens per Sec:    15610, Lr: 0.000300\n",
      "2021-08-03 09:02:50,045 - INFO - joeynmt.training - Epoch   8, Step:   196700, Batch Loss:     1.843351, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-08-03 09:03:03,803 - INFO - joeynmt.training - Epoch   8, Step:   196800, Batch Loss:     1.834451, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-08-03 09:03:17,571 - INFO - joeynmt.training - Epoch   8, Step:   196900, Batch Loss:     1.783971, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-08-03 09:03:31,379 - INFO - joeynmt.training - Epoch   8, Step:   197000, Batch Loss:     1.746657, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-08-03 09:03:45,260 - INFO - joeynmt.training - Epoch   8, Step:   197100, Batch Loss:     1.739111, Tokens per Sec:    15580, Lr: 0.000300\n",
      "2021-08-03 09:03:57,006 - INFO - joeynmt.training - Epoch   8: total training loss 9292.39\n",
      "2021-08-03 09:03:57,006 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-03 09:03:59,818 - INFO - joeynmt.training - Epoch   9, Step:   197200, Batch Loss:     1.915759, Tokens per Sec:    13429, Lr: 0.000300\n",
      "2021-08-03 09:04:13,623 - INFO - joeynmt.training - Epoch   9, Step:   197300, Batch Loss:     1.979352, Tokens per Sec:    15773, Lr: 0.000300\n",
      "2021-08-03 09:04:27,316 - INFO - joeynmt.training - Epoch   9, Step:   197400, Batch Loss:     1.637068, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-08-03 09:04:41,142 - INFO - joeynmt.training - Epoch   9, Step:   197500, Batch Loss:     1.842030, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-03 09:04:54,983 - INFO - joeynmt.training - Epoch   9, Step:   197600, Batch Loss:     1.742963, Tokens per Sec:    15543, Lr: 0.000300\n",
      "2021-08-03 09:05:08,618 - INFO - joeynmt.training - Epoch   9, Step:   197700, Batch Loss:     1.799646, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-08-03 09:05:22,403 - INFO - joeynmt.training - Epoch   9, Step:   197800, Batch Loss:     1.556398, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-03 09:05:36,185 - INFO - joeynmt.training - Epoch   9, Step:   197900, Batch Loss:     1.739252, Tokens per Sec:    15899, Lr: 0.000300\n",
      "2021-08-03 09:05:49,871 - INFO - joeynmt.training - Epoch   9, Step:   198000, Batch Loss:     1.739454, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-08-03 09:06:03,724 - INFO - joeynmt.training - Epoch   9, Step:   198100, Batch Loss:     1.551789, Tokens per Sec:    16204, Lr: 0.000300\n",
      "2021-08-03 09:06:17,584 - INFO - joeynmt.training - Epoch   9, Step:   198200, Batch Loss:     1.744603, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-03 09:06:31,491 - INFO - joeynmt.training - Epoch   9, Step:   198300, Batch Loss:     1.801873, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-08-03 09:06:45,301 - INFO - joeynmt.training - Epoch   9, Step:   198400, Batch Loss:     1.870696, Tokens per Sec:    15748, Lr: 0.000300\n",
      "2021-08-03 09:06:59,002 - INFO - joeynmt.training - Epoch   9, Step:   198500, Batch Loss:     1.767259, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-08-03 09:07:12,789 - INFO - joeynmt.training - Epoch   9, Step:   198600, Batch Loss:     1.714382, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-03 09:07:26,500 - INFO - joeynmt.training - Epoch   9, Step:   198700, Batch Loss:     1.838275, Tokens per Sec:    15499, Lr: 0.000300\n",
      "2021-08-03 09:07:40,465 - INFO - joeynmt.training - Epoch   9, Step:   198800, Batch Loss:     1.634492, Tokens per Sec:    15846, Lr: 0.000300\n",
      "2021-08-03 09:07:54,248 - INFO - joeynmt.training - Epoch   9, Step:   198900, Batch Loss:     1.634965, Tokens per Sec:    16302, Lr: 0.000300\n",
      "2021-08-03 09:08:08,065 - INFO - joeynmt.training - Epoch   9, Step:   199000, Batch Loss:     1.733926, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-08-03 09:08:21,783 - INFO - joeynmt.training - Epoch   9, Step:   199100, Batch Loss:     1.497579, Tokens per Sec:    15864, Lr: 0.000300\n",
      "2021-08-03 09:08:35,607 - INFO - joeynmt.training - Epoch   9, Step:   199200, Batch Loss:     1.910535, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-08-03 09:08:49,445 - INFO - joeynmt.training - Epoch   9, Step:   199300, Batch Loss:     1.791114, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-08-03 09:09:03,442 - INFO - joeynmt.training - Epoch   9, Step:   199400, Batch Loss:     1.784888, Tokens per Sec:    16403, Lr: 0.000300\n",
      "2021-08-03 09:09:17,313 - INFO - joeynmt.training - Epoch   9, Step:   199500, Batch Loss:     1.831450, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-08-03 09:09:30,950 - INFO - joeynmt.training - Epoch   9, Step:   199600, Batch Loss:     1.739935, Tokens per Sec:    15687, Lr: 0.000300\n",
      "2021-08-03 09:09:44,673 - INFO - joeynmt.training - Epoch   9, Step:   199700, Batch Loss:     1.510594, Tokens per Sec:    15642, Lr: 0.000300\n",
      "2021-08-03 09:09:58,681 - INFO - joeynmt.training - Epoch   9, Step:   199800, Batch Loss:     1.952155, Tokens per Sec:    16042, Lr: 0.000300\n",
      "2021-08-03 09:10:12,421 - INFO - joeynmt.training - Epoch   9, Step:   199900, Batch Loss:     1.845052, Tokens per Sec:    16169, Lr: 0.000300\n",
      "2021-08-03 09:10:26,206 - INFO - joeynmt.training - Epoch   9, Step:   200000, Batch Loss:     1.891671, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-03 09:12:00,496 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:12:00,497 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:12:00,497 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:12:02,356 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:12:02,359 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:12:02,359 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:12:02,359 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , I draw close to God , so good for me . ”\n",
      "2021-08-03 09:12:02,359 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:12:02,360 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:12:02,360 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:12:02,360 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are fear Jehovah is greater than many riches are standing . ”\n",
      "2021-08-03 09:12:02,360 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:12:02,361 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:12:02,361 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:12:02,361 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has something about God’s Kingdom .\n",
      "2021-08-03 09:12:02,361 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:12:02,362 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:12:02,362 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:12:02,362 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 09:12:02,362 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   200000: bleu:  26.89, loss: 191517.0000, ppl:   4.8639, duration: 96.1553s\n",
      "2021-08-03 09:12:16,086 - INFO - joeynmt.training - Epoch   9, Step:   200100, Batch Loss:     1.804020, Tokens per Sec:    15440, Lr: 0.000300\n",
      "2021-08-03 09:12:29,881 - INFO - joeynmt.training - Epoch   9, Step:   200200, Batch Loss:     1.665913, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-03 09:12:43,713 - INFO - joeynmt.training - Epoch   9, Step:   200300, Batch Loss:     1.881611, Tokens per Sec:    15600, Lr: 0.000300\n",
      "2021-08-03 09:12:57,522 - INFO - joeynmt.training - Epoch   9, Step:   200400, Batch Loss:     1.865374, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-03 09:13:11,154 - INFO - joeynmt.training - Epoch   9, Step:   200500, Batch Loss:     1.695670, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-08-03 09:13:25,066 - INFO - joeynmt.training - Epoch   9, Step:   200600, Batch Loss:     1.719435, Tokens per Sec:    16021, Lr: 0.000300\n",
      "2021-08-03 09:13:38,938 - INFO - joeynmt.training - Epoch   9, Step:   200700, Batch Loss:     1.819478, Tokens per Sec:    16208, Lr: 0.000300\n",
      "2021-08-03 09:13:52,825 - INFO - joeynmt.training - Epoch   9, Step:   200800, Batch Loss:     1.880115, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-08-03 09:14:06,686 - INFO - joeynmt.training - Epoch   9, Step:   200900, Batch Loss:     1.556607, Tokens per Sec:    16359, Lr: 0.000300\n",
      "2021-08-03 09:14:20,408 - INFO - joeynmt.training - Epoch   9, Step:   201000, Batch Loss:     1.563089, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-03 09:14:34,195 - INFO - joeynmt.training - Epoch   9, Step:   201100, Batch Loss:     1.805098, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-08-03 09:14:48,035 - INFO - joeynmt.training - Epoch   9, Step:   201200, Batch Loss:     1.723909, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-03 09:15:01,959 - INFO - joeynmt.training - Epoch   9, Step:   201300, Batch Loss:     1.477550, Tokens per Sec:    16391, Lr: 0.000300\n",
      "2021-08-03 09:15:15,635 - INFO - joeynmt.training - Epoch   9, Step:   201400, Batch Loss:     1.795606, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-03 09:15:29,315 - INFO - joeynmt.training - Epoch   9, Step:   201500, Batch Loss:     1.734605, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-08-03 09:15:43,052 - INFO - joeynmt.training - Epoch   9, Step:   201600, Batch Loss:     1.699853, Tokens per Sec:    15803, Lr: 0.000300\n",
      "2021-08-03 09:15:56,810 - INFO - joeynmt.training - Epoch   9, Step:   201700, Batch Loss:     1.748903, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-03 09:16:10,476 - INFO - joeynmt.training - Epoch   9, Step:   201800, Batch Loss:     1.728244, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-03 09:16:24,309 - INFO - joeynmt.training - Epoch   9, Step:   201900, Batch Loss:     1.852155, Tokens per Sec:    15695, Lr: 0.000300\n",
      "2021-08-03 09:16:38,079 - INFO - joeynmt.training - Epoch   9, Step:   202000, Batch Loss:     1.808802, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-08-03 09:16:51,794 - INFO - joeynmt.training - Epoch   9, Step:   202100, Batch Loss:     1.823840, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-08-03 09:17:05,282 - INFO - joeynmt.training - Epoch   9, Step:   202200, Batch Loss:     1.834281, Tokens per Sec:    15602, Lr: 0.000300\n",
      "2021-08-03 09:17:19,091 - INFO - joeynmt.training - Epoch   9, Step:   202300, Batch Loss:     1.795235, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-03 09:17:32,978 - INFO - joeynmt.training - Epoch   9, Step:   202400, Batch Loss:     1.610451, Tokens per Sec:    15650, Lr: 0.000300\n",
      "2021-08-03 09:17:46,826 - INFO - joeynmt.training - Epoch   9, Step:   202500, Batch Loss:     1.622270, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-03 09:17:49,089 - INFO - joeynmt.training - Epoch   9: total training loss 9279.93\n",
      "2021-08-03 09:17:49,089 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-03 09:18:00,995 - INFO - joeynmt.training - Epoch  10, Step:   202600, Batch Loss:     1.709582, Tokens per Sec:    15346, Lr: 0.000300\n",
      "2021-08-03 09:18:14,750 - INFO - joeynmt.training - Epoch  10, Step:   202700, Batch Loss:     1.581548, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-03 09:18:28,295 - INFO - joeynmt.training - Epoch  10, Step:   202800, Batch Loss:     1.682126, Tokens per Sec:    15708, Lr: 0.000300\n",
      "2021-08-03 09:18:42,248 - INFO - joeynmt.training - Epoch  10, Step:   202900, Batch Loss:     1.710257, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-03 09:18:55,977 - INFO - joeynmt.training - Epoch  10, Step:   203000, Batch Loss:     1.465939, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-08-03 09:19:09,662 - INFO - joeynmt.training - Epoch  10, Step:   203100, Batch Loss:     1.915943, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-08-03 09:19:22,966 - INFO - joeynmt.training - Epoch  10, Step:   203200, Batch Loss:     1.940560, Tokens per Sec:    15602, Lr: 0.000300\n",
      "2021-08-03 09:19:36,983 - INFO - joeynmt.training - Epoch  10, Step:   203300, Batch Loss:     1.707903, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-03 09:19:50,914 - INFO - joeynmt.training - Epoch  10, Step:   203400, Batch Loss:     1.688982, Tokens per Sec:    15926, Lr: 0.000300\n",
      "2021-08-03 09:20:04,478 - INFO - joeynmt.training - Epoch  10, Step:   203500, Batch Loss:     1.931551, Tokens per Sec:    15765, Lr: 0.000300\n",
      "2021-08-03 09:20:18,320 - INFO - joeynmt.training - Epoch  10, Step:   203600, Batch Loss:     1.634905, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-03 09:20:32,306 - INFO - joeynmt.training - Epoch  10, Step:   203700, Batch Loss:     1.578101, Tokens per Sec:    16568, Lr: 0.000300\n",
      "2021-08-03 09:20:46,191 - INFO - joeynmt.training - Epoch  10, Step:   203800, Batch Loss:     1.805171, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-03 09:21:00,109 - INFO - joeynmt.training - Epoch  10, Step:   203900, Batch Loss:     1.607639, Tokens per Sec:    15799, Lr: 0.000300\n",
      "2021-08-03 09:21:13,826 - INFO - joeynmt.training - Epoch  10, Step:   204000, Batch Loss:     1.704974, Tokens per Sec:    15564, Lr: 0.000300\n",
      "2021-08-03 09:21:27,650 - INFO - joeynmt.training - Epoch  10, Step:   204100, Batch Loss:     1.935639, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-08-03 09:21:41,492 - INFO - joeynmt.training - Epoch  10, Step:   204200, Batch Loss:     1.703472, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-03 09:21:55,208 - INFO - joeynmt.training - Epoch  10, Step:   204300, Batch Loss:     1.831919, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-08-03 09:22:08,961 - INFO - joeynmt.training - Epoch  10, Step:   204400, Batch Loss:     1.567830, Tokens per Sec:    15684, Lr: 0.000300\n",
      "2021-08-03 09:22:22,696 - INFO - joeynmt.training - Epoch  10, Step:   204500, Batch Loss:     1.809196, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-03 09:22:36,587 - INFO - joeynmt.training - Epoch  10, Step:   204600, Batch Loss:     1.830716, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-03 09:22:50,319 - INFO - joeynmt.training - Epoch  10, Step:   204700, Batch Loss:     2.091949, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-08-03 09:23:04,176 - INFO - joeynmt.training - Epoch  10, Step:   204800, Batch Loss:     1.625039, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-03 09:23:18,030 - INFO - joeynmt.training - Epoch  10, Step:   204900, Batch Loss:     1.685499, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-08-03 09:23:31,880 - INFO - joeynmt.training - Epoch  10, Step:   205000, Batch Loss:     1.744740, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-03 09:25:03,039 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:25:03,039 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:25:03,039 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:25:04,217 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:25:04,218 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:25:04,941 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:25:04,942 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:25:04,942 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:25:04,943 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , drawing close to God , the good thing is for me . ”\n",
      "2021-08-03 09:25:04,943 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:25:04,944 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:25:04,945 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:25:04,945 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are fearing Jehovah is greater than the riches that are standing . ”\n",
      "2021-08-03 09:25:04,945 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:25:04,946 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:25:04,946 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:25:04,946 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has something about God’s Kingdom .\n",
      "2021-08-03 09:25:04,946 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:25:04,946 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:25:04,947 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:25:04,947 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 09:25:04,947 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   205000: bleu:  26.97, loss: 190386.0625, ppl:   4.8187, duration: 93.0667s\n",
      "2021-08-03 09:25:18,754 - INFO - joeynmt.training - Epoch  10, Step:   205100, Batch Loss:     1.729468, Tokens per Sec:    15584, Lr: 0.000300\n",
      "2021-08-03 09:25:32,499 - INFO - joeynmt.training - Epoch  10, Step:   205200, Batch Loss:     1.747059, Tokens per Sec:    15566, Lr: 0.000300\n",
      "2021-08-03 09:25:46,485 - INFO - joeynmt.training - Epoch  10, Step:   205300, Batch Loss:     1.839192, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-03 09:26:00,464 - INFO - joeynmt.training - Epoch  10, Step:   205400, Batch Loss:     1.711781, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-08-03 09:26:14,091 - INFO - joeynmt.training - Epoch  10, Step:   205500, Batch Loss:     1.720116, Tokens per Sec:    15720, Lr: 0.000300\n",
      "2021-08-03 09:26:27,966 - INFO - joeynmt.training - Epoch  10, Step:   205600, Batch Loss:     2.004331, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-03 09:26:41,742 - INFO - joeynmt.training - Epoch  10, Step:   205700, Batch Loss:     1.680036, Tokens per Sec:    15717, Lr: 0.000300\n",
      "2021-08-03 09:26:55,646 - INFO - joeynmt.training - Epoch  10, Step:   205800, Batch Loss:     1.973128, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-08-03 09:27:09,512 - INFO - joeynmt.training - Epoch  10, Step:   205900, Batch Loss:     1.720897, Tokens per Sec:    15889, Lr: 0.000300\n",
      "2021-08-03 09:27:23,282 - INFO - joeynmt.training - Epoch  10, Step:   206000, Batch Loss:     1.568497, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-08-03 09:27:37,133 - INFO - joeynmt.training - Epoch  10, Step:   206100, Batch Loss:     1.754225, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-03 09:27:50,931 - INFO - joeynmt.training - Epoch  10, Step:   206200, Batch Loss:     1.794000, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-08-03 09:28:04,798 - INFO - joeynmt.training - Epoch  10, Step:   206300, Batch Loss:     1.702950, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-08-03 09:28:18,558 - INFO - joeynmt.training - Epoch  10, Step:   206400, Batch Loss:     1.858732, Tokens per Sec:    15732, Lr: 0.000300\n",
      "2021-08-03 09:28:32,380 - INFO - joeynmt.training - Epoch  10, Step:   206500, Batch Loss:     1.503264, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-08-03 09:28:46,214 - INFO - joeynmt.training - Epoch  10, Step:   206600, Batch Loss:     1.627860, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-03 09:29:00,016 - INFO - joeynmt.training - Epoch  10, Step:   206700, Batch Loss:     1.632355, Tokens per Sec:    16019, Lr: 0.000300\n",
      "2021-08-03 09:29:13,898 - INFO - joeynmt.training - Epoch  10, Step:   206800, Batch Loss:     1.615679, Tokens per Sec:    16166, Lr: 0.000300\n",
      "2021-08-03 09:29:27,808 - INFO - joeynmt.training - Epoch  10, Step:   206900, Batch Loss:     1.625560, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-08-03 09:29:41,840 - INFO - joeynmt.training - Epoch  10, Step:   207000, Batch Loss:     1.795141, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-08-03 09:29:55,728 - INFO - joeynmt.training - Epoch  10, Step:   207100, Batch Loss:     1.671687, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-08-03 09:30:09,669 - INFO - joeynmt.training - Epoch  10, Step:   207200, Batch Loss:     1.689558, Tokens per Sec:    16188, Lr: 0.000300\n",
      "2021-08-03 09:30:23,285 - INFO - joeynmt.training - Epoch  10, Step:   207300, Batch Loss:     1.711425, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-08-03 09:30:37,037 - INFO - joeynmt.training - Epoch  10, Step:   207400, Batch Loss:     1.803116, Tokens per Sec:    15569, Lr: 0.000300\n",
      "2021-08-03 09:30:50,951 - INFO - joeynmt.training - Epoch  10, Step:   207500, Batch Loss:     1.734985, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-08-03 09:31:04,723 - INFO - joeynmt.training - Epoch  10, Step:   207600, Batch Loss:     1.560019, Tokens per Sec:    16335, Lr: 0.000300\n",
      "2021-08-03 09:31:18,453 - INFO - joeynmt.training - Epoch  10, Step:   207700, Batch Loss:     1.728578, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-08-03 09:31:31,979 - INFO - joeynmt.training - Epoch  10, Step:   207800, Batch Loss:     1.534637, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-03 09:31:38,654 - INFO - joeynmt.training - Epoch  10: total training loss 9245.05\n",
      "2021-08-03 09:31:38,654 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-03 09:31:46,527 - INFO - joeynmt.training - Epoch  11, Step:   207900, Batch Loss:     1.622719, Tokens per Sec:    14640, Lr: 0.000300\n",
      "2021-08-03 09:32:00,283 - INFO - joeynmt.training - Epoch  11, Step:   208000, Batch Loss:     1.685598, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-08-03 09:32:13,947 - INFO - joeynmt.training - Epoch  11, Step:   208100, Batch Loss:     1.997439, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-03 09:32:27,484 - INFO - joeynmt.training - Epoch  11, Step:   208200, Batch Loss:     1.566936, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-08-03 09:32:41,238 - INFO - joeynmt.training - Epoch  11, Step:   208300, Batch Loss:     1.857858, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-08-03 09:32:55,091 - INFO - joeynmt.training - Epoch  11, Step:   208400, Batch Loss:     1.737836, Tokens per Sec:    15691, Lr: 0.000300\n",
      "2021-08-03 09:33:08,800 - INFO - joeynmt.training - Epoch  11, Step:   208500, Batch Loss:     1.649042, Tokens per Sec:    15769, Lr: 0.000300\n",
      "2021-08-03 09:33:22,756 - INFO - joeynmt.training - Epoch  11, Step:   208600, Batch Loss:     1.571428, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-08-03 09:33:36,824 - INFO - joeynmt.training - Epoch  11, Step:   208700, Batch Loss:     1.828440, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-08-03 09:33:50,406 - INFO - joeynmt.training - Epoch  11, Step:   208800, Batch Loss:     1.692294, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-03 09:34:04,079 - INFO - joeynmt.training - Epoch  11, Step:   208900, Batch Loss:     1.744098, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-03 09:34:17,767 - INFO - joeynmt.training - Epoch  11, Step:   209000, Batch Loss:     1.733617, Tokens per Sec:    15598, Lr: 0.000300\n",
      "2021-08-03 09:34:31,762 - INFO - joeynmt.training - Epoch  11, Step:   209100, Batch Loss:     1.832072, Tokens per Sec:    16252, Lr: 0.000300\n",
      "2021-08-03 09:34:45,632 - INFO - joeynmt.training - Epoch  11, Step:   209200, Batch Loss:     1.465027, Tokens per Sec:    15677, Lr: 0.000300\n",
      "2021-08-03 09:34:59,154 - INFO - joeynmt.training - Epoch  11, Step:   209300, Batch Loss:     1.629021, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-08-03 09:35:12,956 - INFO - joeynmt.training - Epoch  11, Step:   209400, Batch Loss:     1.620809, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-03 09:35:26,843 - INFO - joeynmt.training - Epoch  11, Step:   209500, Batch Loss:     1.657842, Tokens per Sec:    16147, Lr: 0.000300\n",
      "2021-08-03 09:35:40,577 - INFO - joeynmt.training - Epoch  11, Step:   209600, Batch Loss:     1.739691, Tokens per Sec:    15645, Lr: 0.000300\n",
      "2021-08-03 09:35:54,516 - INFO - joeynmt.training - Epoch  11, Step:   209700, Batch Loss:     1.864273, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-08-03 09:36:08,210 - INFO - joeynmt.training - Epoch  11, Step:   209800, Batch Loss:     1.772191, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-08-03 09:36:21,864 - INFO - joeynmt.training - Epoch  11, Step:   209900, Batch Loss:     1.816188, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-08-03 09:36:35,541 - INFO - joeynmt.training - Epoch  11, Step:   210000, Batch Loss:     1.512557, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-08-03 09:38:07,836 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:38:07,836 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:38:07,836 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:38:09,101 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:38:09,102 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:38:09,840 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:38:09,841 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:38:09,841 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:38:09,842 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , I draw close to God , so good for me . ”\n",
      "2021-08-03 09:38:09,842 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:38:09,842 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:38:09,842 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:38:09,844 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are godly devotion to Jehovah is greater than many riches that are standing . ”\n",
      "2021-08-03 09:38:09,845 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:38:09,845 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:38:09,845 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:38:09,846 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The other prophecies in the book of Daniel have a detailed point about God’s Kingdom .\n",
      "2021-08-03 09:38:09,846 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:38:09,846 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:38:09,847 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:38:09,847 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-03 09:38:09,847 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   210000: bleu:  26.85, loss: 190259.4844, ppl:   4.8137, duration: 94.3057s\n",
      "2021-08-03 09:38:23,973 - INFO - joeynmt.training - Epoch  11, Step:   210100, Batch Loss:     1.895013, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-03 09:38:37,643 - INFO - joeynmt.training - Epoch  11, Step:   210200, Batch Loss:     1.914678, Tokens per Sec:    15667, Lr: 0.000300\n",
      "2021-08-03 09:38:51,615 - INFO - joeynmt.training - Epoch  11, Step:   210300, Batch Loss:     1.879131, Tokens per Sec:    16338, Lr: 0.000300\n",
      "2021-08-03 09:39:05,605 - INFO - joeynmt.training - Epoch  11, Step:   210400, Batch Loss:     1.776819, Tokens per Sec:    16536, Lr: 0.000300\n",
      "2021-08-03 09:39:19,274 - INFO - joeynmt.training - Epoch  11, Step:   210500, Batch Loss:     1.752150, Tokens per Sec:    15640, Lr: 0.000300\n",
      "2021-08-03 09:39:33,185 - INFO - joeynmt.training - Epoch  11, Step:   210600, Batch Loss:     1.640273, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-03 09:39:47,102 - INFO - joeynmt.training - Epoch  11, Step:   210700, Batch Loss:     1.753169, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-08-03 09:40:00,967 - INFO - joeynmt.training - Epoch  11, Step:   210800, Batch Loss:     1.766153, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-08-03 09:40:14,756 - INFO - joeynmt.training - Epoch  11, Step:   210900, Batch Loss:     1.704342, Tokens per Sec:    16318, Lr: 0.000300\n",
      "2021-08-03 09:40:28,453 - INFO - joeynmt.training - Epoch  11, Step:   211000, Batch Loss:     1.736239, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-08-03 09:40:42,310 - INFO - joeynmt.training - Epoch  11, Step:   211100, Batch Loss:     1.819947, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-03 09:40:56,022 - INFO - joeynmt.training - Epoch  11, Step:   211200, Batch Loss:     1.712894, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-08-03 09:41:09,744 - INFO - joeynmt.training - Epoch  11, Step:   211300, Batch Loss:     1.726903, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-08-03 09:41:23,484 - INFO - joeynmt.training - Epoch  11, Step:   211400, Batch Loss:     1.794039, Tokens per Sec:    16109, Lr: 0.000300\n",
      "2021-08-03 09:41:37,355 - INFO - joeynmt.training - Epoch  11, Step:   211500, Batch Loss:     1.550653, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-08-03 09:41:51,268 - INFO - joeynmt.training - Epoch  11, Step:   211600, Batch Loss:     1.764581, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-08-03 09:42:04,643 - INFO - joeynmt.training - Epoch  11, Step:   211700, Batch Loss:     1.919660, Tokens per Sec:    15597, Lr: 0.000300\n",
      "2021-08-03 09:42:18,209 - INFO - joeynmt.training - Epoch  11, Step:   211800, Batch Loss:     1.477425, Tokens per Sec:    15761, Lr: 0.000300\n",
      "2021-08-03 09:42:31,945 - INFO - joeynmt.training - Epoch  11, Step:   211900, Batch Loss:     1.612625, Tokens per Sec:    15939, Lr: 0.000300\n",
      "2021-08-03 09:42:45,830 - INFO - joeynmt.training - Epoch  11, Step:   212000, Batch Loss:     1.463823, Tokens per Sec:    15754, Lr: 0.000300\n",
      "2021-08-03 09:42:59,527 - INFO - joeynmt.training - Epoch  11, Step:   212100, Batch Loss:     1.785000, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-03 09:43:13,188 - INFO - joeynmt.training - Epoch  11, Step:   212200, Batch Loss:     1.687629, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-03 09:43:26,803 - INFO - joeynmt.training - Epoch  11, Step:   212300, Batch Loss:     1.636550, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-03 09:43:40,562 - INFO - joeynmt.training - Epoch  11, Step:   212400, Batch Loss:     1.820697, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-03 09:43:54,479 - INFO - joeynmt.training - Epoch  11, Step:   212500, Batch Loss:     1.862420, Tokens per Sec:    15790, Lr: 0.000300\n",
      "2021-08-03 09:44:08,238 - INFO - joeynmt.training - Epoch  11, Step:   212600, Batch Loss:     1.723514, Tokens per Sec:    15772, Lr: 0.000300\n",
      "2021-08-03 09:44:22,073 - INFO - joeynmt.training - Epoch  11, Step:   212700, Batch Loss:     1.781463, Tokens per Sec:    15814, Lr: 0.000300\n",
      "2021-08-03 09:44:35,770 - INFO - joeynmt.training - Epoch  11, Step:   212800, Batch Loss:     1.795909, Tokens per Sec:    15509, Lr: 0.000300\n",
      "2021-08-03 09:44:49,496 - INFO - joeynmt.training - Epoch  11, Step:   212900, Batch Loss:     1.808754, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-03 09:45:03,266 - INFO - joeynmt.training - Epoch  11, Step:   213000, Batch Loss:     1.631025, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-03 09:45:17,010 - INFO - joeynmt.training - Epoch  11, Step:   213100, Batch Loss:     1.824219, Tokens per Sec:    16140, Lr: 0.000300\n",
      "2021-08-03 09:45:28,806 - INFO - joeynmt.training - Epoch  11: total training loss 9248.46\n",
      "2021-08-03 09:45:28,806 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-03 09:45:31,411 - INFO - joeynmt.training - Epoch  12, Step:   213200, Batch Loss:     1.522902, Tokens per Sec:    12621, Lr: 0.000300\n",
      "2021-08-03 09:45:45,394 - INFO - joeynmt.training - Epoch  12, Step:   213300, Batch Loss:     1.679667, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-03 09:45:59,102 - INFO - joeynmt.training - Epoch  12, Step:   213400, Batch Loss:     1.645402, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-08-03 09:46:12,908 - INFO - joeynmt.training - Epoch  12, Step:   213500, Batch Loss:     1.616654, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-03 09:46:26,522 - INFO - joeynmt.training - Epoch  12, Step:   213600, Batch Loss:     1.647804, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-08-03 09:46:40,465 - INFO - joeynmt.training - Epoch  12, Step:   213700, Batch Loss:     1.795841, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-03 09:46:54,333 - INFO - joeynmt.training - Epoch  12, Step:   213800, Batch Loss:     1.581929, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-03 09:47:08,104 - INFO - joeynmt.training - Epoch  12, Step:   213900, Batch Loss:     1.770630, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-03 09:47:21,836 - INFO - joeynmt.training - Epoch  12, Step:   214000, Batch Loss:     1.700921, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-03 09:47:35,486 - INFO - joeynmt.training - Epoch  12, Step:   214100, Batch Loss:     1.695952, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-03 09:47:49,505 - INFO - joeynmt.training - Epoch  12, Step:   214200, Batch Loss:     1.732208, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-08-03 09:48:03,376 - INFO - joeynmt.training - Epoch  12, Step:   214300, Batch Loss:     1.835765, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-08-03 09:48:16,959 - INFO - joeynmt.training - Epoch  12, Step:   214400, Batch Loss:     1.785142, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-08-03 09:48:30,509 - INFO - joeynmt.training - Epoch  12, Step:   214500, Batch Loss:     1.788000, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-03 09:48:44,402 - INFO - joeynmt.training - Epoch  12, Step:   214600, Batch Loss:     1.631400, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-08-03 09:48:58,211 - INFO - joeynmt.training - Epoch  12, Step:   214700, Batch Loss:     1.689500, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-08-03 09:49:11,927 - INFO - joeynmt.training - Epoch  12, Step:   214800, Batch Loss:     1.826588, Tokens per Sec:    15645, Lr: 0.000300\n",
      "2021-08-03 09:49:25,806 - INFO - joeynmt.training - Epoch  12, Step:   214900, Batch Loss:     1.892474, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-08-03 09:49:39,554 - INFO - joeynmt.training - Epoch  12, Step:   215000, Batch Loss:     1.678606, Tokens per Sec:    15642, Lr: 0.000300\n",
      "2021-08-03 09:51:11,656 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:51:11,657 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:51:11,657 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:51:12,888 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:51:12,888 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:51:13,604 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:51:13,605 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:51:13,605 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:51:13,605 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am drawing close to God , so good for me . ”\n",
      "2021-08-03 09:51:13,605 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:51:13,605 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:51:13,606 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:51:13,606 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are fearing Jehovah is greater than the riches that are standing . ”\n",
      "2021-08-03 09:51:13,606 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:51:13,606 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:51:13,607 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:51:13,607 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has a meaningful discussion of God’s Kingdom .\n",
      "2021-08-03 09:51:13,607 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:51:13,607 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:51:13,607 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:51:13,608 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 09:51:13,608 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   215000: bleu:  27.05, loss: 190039.7500, ppl:   4.8049, duration: 94.0530s\n",
      "2021-08-03 09:51:27,615 - INFO - joeynmt.training - Epoch  12, Step:   215100, Batch Loss:     1.877002, Tokens per Sec:    15575, Lr: 0.000300\n",
      "2021-08-03 09:51:41,639 - INFO - joeynmt.training - Epoch  12, Step:   215200, Batch Loss:     1.708887, Tokens per Sec:    16334, Lr: 0.000300\n",
      "2021-08-03 09:51:55,632 - INFO - joeynmt.training - Epoch  12, Step:   215300, Batch Loss:     1.742350, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-08-03 09:52:09,374 - INFO - joeynmt.training - Epoch  12, Step:   215400, Batch Loss:     1.720621, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-03 09:52:23,142 - INFO - joeynmt.training - Epoch  12, Step:   215500, Batch Loss:     1.717223, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-03 09:52:36,775 - INFO - joeynmt.training - Epoch  12, Step:   215600, Batch Loss:     1.671924, Tokens per Sec:    15766, Lr: 0.000300\n",
      "2021-08-03 09:52:50,724 - INFO - joeynmt.training - Epoch  12, Step:   215700, Batch Loss:     1.597514, Tokens per Sec:    16286, Lr: 0.000300\n",
      "2021-08-03 09:53:04,189 - INFO - joeynmt.training - Epoch  12, Step:   215800, Batch Loss:     1.788805, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-03 09:53:17,859 - INFO - joeynmt.training - Epoch  12, Step:   215900, Batch Loss:     1.720577, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-03 09:53:31,484 - INFO - joeynmt.training - Epoch  12, Step:   216000, Batch Loss:     1.713005, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-03 09:53:45,346 - INFO - joeynmt.training - Epoch  12, Step:   216100, Batch Loss:     1.932020, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-08-03 09:53:58,964 - INFO - joeynmt.training - Epoch  12, Step:   216200, Batch Loss:     1.600762, Tokens per Sec:    15405, Lr: 0.000300\n",
      "2021-08-03 09:54:12,727 - INFO - joeynmt.training - Epoch  12, Step:   216300, Batch Loss:     1.798725, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-08-03 09:54:26,373 - INFO - joeynmt.training - Epoch  12, Step:   216400, Batch Loss:     1.451042, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-08-03 09:54:40,356 - INFO - joeynmt.training - Epoch  12, Step:   216500, Batch Loss:     1.461743, Tokens per Sec:    16361, Lr: 0.000300\n",
      "2021-08-03 09:54:54,520 - INFO - joeynmt.training - Epoch  12, Step:   216600, Batch Loss:     1.646740, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-03 09:55:08,387 - INFO - joeynmt.training - Epoch  12, Step:   216700, Batch Loss:     1.701517, Tokens per Sec:    15865, Lr: 0.000300\n",
      "2021-08-03 09:55:22,212 - INFO - joeynmt.training - Epoch  12, Step:   216800, Batch Loss:     1.861988, Tokens per Sec:    15512, Lr: 0.000300\n",
      "2021-08-03 09:55:36,186 - INFO - joeynmt.training - Epoch  12, Step:   216900, Batch Loss:     1.985178, Tokens per Sec:    15414, Lr: 0.000300\n",
      "2021-08-03 09:55:49,985 - INFO - joeynmt.training - Epoch  12, Step:   217000, Batch Loss:     1.601434, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-03 09:56:03,729 - INFO - joeynmt.training - Epoch  12, Step:   217100, Batch Loss:     1.969334, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-03 09:56:17,216 - INFO - joeynmt.training - Epoch  12, Step:   217200, Batch Loss:     1.704163, Tokens per Sec:    15638, Lr: 0.000300\n",
      "2021-08-03 09:56:31,271 - INFO - joeynmt.training - Epoch  12, Step:   217300, Batch Loss:     1.717389, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-08-03 09:56:45,061 - INFO - joeynmt.training - Epoch  12, Step:   217400, Batch Loss:     1.771941, Tokens per Sec:    15542, Lr: 0.000300\n",
      "2021-08-03 09:56:58,801 - INFO - joeynmt.training - Epoch  12, Step:   217500, Batch Loss:     1.723688, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-03 09:57:12,454 - INFO - joeynmt.training - Epoch  12, Step:   217600, Batch Loss:     2.005419, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-08-03 09:57:26,349 - INFO - joeynmt.training - Epoch  12, Step:   217700, Batch Loss:     1.773578, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-03 09:57:40,226 - INFO - joeynmt.training - Epoch  12, Step:   217800, Batch Loss:     1.516769, Tokens per Sec:    15678, Lr: 0.000300\n",
      "2021-08-03 09:57:54,405 - INFO - joeynmt.training - Epoch  12, Step:   217900, Batch Loss:     1.670261, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-08-03 09:58:08,291 - INFO - joeynmt.training - Epoch  12, Step:   218000, Batch Loss:     1.818088, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-08-03 09:58:22,087 - INFO - joeynmt.training - Epoch  12, Step:   218100, Batch Loss:     1.666892, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-03 09:58:36,179 - INFO - joeynmt.training - Epoch  12, Step:   218200, Batch Loss:     1.562081, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-08-03 09:58:49,858 - INFO - joeynmt.training - Epoch  12, Step:   218300, Batch Loss:     1.740438, Tokens per Sec:    15285, Lr: 0.000300\n",
      "2021-08-03 09:59:03,759 - INFO - joeynmt.training - Epoch  12, Step:   218400, Batch Loss:     1.806225, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-08-03 09:59:17,731 - INFO - joeynmt.training - Epoch  12, Step:   218500, Batch Loss:     1.737739, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-08-03 09:59:18,872 - INFO - joeynmt.training - Epoch  12: total training loss 9200.24\n",
      "2021-08-03 09:59:18,872 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-03 09:59:32,261 - INFO - joeynmt.training - Epoch  13, Step:   218600, Batch Loss:     1.591002, Tokens per Sec:    15644, Lr: 0.000300\n",
      "2021-08-03 09:59:46,261 - INFO - joeynmt.training - Epoch  13, Step:   218700, Batch Loss:     1.686941, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-08-03 10:00:00,307 - INFO - joeynmt.training - Epoch  13, Step:   218800, Batch Loss:     1.784822, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-08-03 10:00:14,165 - INFO - joeynmt.training - Epoch  13, Step:   218900, Batch Loss:     1.814438, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-08-03 10:00:28,105 - INFO - joeynmt.training - Epoch  13, Step:   219000, Batch Loss:     1.590054, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-03 10:00:41,893 - INFO - joeynmt.training - Epoch  13, Step:   219100, Batch Loss:     1.808914, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-03 10:00:55,821 - INFO - joeynmt.training - Epoch  13, Step:   219200, Batch Loss:     1.524309, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-03 10:01:09,575 - INFO - joeynmt.training - Epoch  13, Step:   219300, Batch Loss:     1.546704, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-08-03 10:01:23,513 - INFO - joeynmt.training - Epoch  13, Step:   219400, Batch Loss:     1.922028, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-08-03 10:01:37,534 - INFO - joeynmt.training - Epoch  13, Step:   219500, Batch Loss:     1.710065, Tokens per Sec:    15463, Lr: 0.000300\n",
      "2021-08-03 10:01:51,392 - INFO - joeynmt.training - Epoch  13, Step:   219600, Batch Loss:     1.826344, Tokens per Sec:    15589, Lr: 0.000300\n",
      "2021-08-03 10:02:05,210 - INFO - joeynmt.training - Epoch  13, Step:   219700, Batch Loss:     1.928062, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-08-03 10:02:18,961 - INFO - joeynmt.training - Epoch  13, Step:   219800, Batch Loss:     1.802023, Tokens per Sec:    15774, Lr: 0.000300\n",
      "2021-08-03 10:02:32,909 - INFO - joeynmt.training - Epoch  13, Step:   219900, Batch Loss:     1.828602, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-08-03 10:02:46,767 - INFO - joeynmt.training - Epoch  13, Step:   220000, Batch Loss:     1.653666, Tokens per Sec:    15590, Lr: 0.000300\n",
      "2021-08-03 10:04:18,418 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:04:18,419 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:04:18,419 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:04:20,290 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:04:20,291 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:04:20,291 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:04:20,291 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , so good for me . ”\n",
      "2021-08-03 10:04:20,292 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:04:20,293 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:04:20,293 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:04:20,294 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we live in fear of Jehovah is greater than many treasures that are standing . ”\n",
      "2021-08-03 10:04:20,294 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:04:20,294 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:04:20,294 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:04:20,295 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The book of Daniel has a detailed meaning about God’s Kingdom .\n",
      "2021-08-03 10:04:20,295 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:04:20,295 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:04:20,295 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:04:20,295 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 10:04:20,296 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   220000: bleu:  27.15, loss: 190176.1250, ppl:   4.8103, duration: 93.5285s\n",
      "2021-08-03 10:04:34,013 - INFO - joeynmt.training - Epoch  13, Step:   220100, Batch Loss:     1.756785, Tokens per Sec:    15511, Lr: 0.000300\n",
      "2021-08-03 10:04:48,164 - INFO - joeynmt.training - Epoch  13, Step:   220200, Batch Loss:     1.776584, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-03 10:05:02,014 - INFO - joeynmt.training - Epoch  13, Step:   220300, Batch Loss:     1.425639, Tokens per Sec:    15731, Lr: 0.000300\n",
      "2021-08-03 10:05:15,917 - INFO - joeynmt.training - Epoch  13, Step:   220400, Batch Loss:     1.724109, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-08-03 10:05:29,972 - INFO - joeynmt.training - Epoch  13, Step:   220500, Batch Loss:     1.666087, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-03 10:05:43,796 - INFO - joeynmt.training - Epoch  13, Step:   220600, Batch Loss:     1.712899, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-03 10:05:57,668 - INFO - joeynmt.training - Epoch  13, Step:   220700, Batch Loss:     1.722976, Tokens per Sec:    15924, Lr: 0.000300\n",
      "2021-08-03 10:06:11,423 - INFO - joeynmt.training - Epoch  13, Step:   220800, Batch Loss:     1.823830, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-08-03 10:06:25,182 - INFO - joeynmt.training - Epoch  13, Step:   220900, Batch Loss:     1.890619, Tokens per Sec:    15652, Lr: 0.000300\n",
      "2021-08-03 10:06:39,191 - INFO - joeynmt.training - Epoch  13, Step:   221000, Batch Loss:     1.511886, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-08-03 10:06:52,888 - INFO - joeynmt.training - Epoch  13, Step:   221100, Batch Loss:     1.778780, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-08-03 10:07:06,647 - INFO - joeynmt.training - Epoch  13, Step:   221200, Batch Loss:     1.774817, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-03 10:07:20,412 - INFO - joeynmt.training - Epoch  13, Step:   221300, Batch Loss:     1.819497, Tokens per Sec:    15653, Lr: 0.000300\n",
      "2021-08-03 10:07:34,302 - INFO - joeynmt.training - Epoch  13, Step:   221400, Batch Loss:     1.788180, Tokens per Sec:    15637, Lr: 0.000300\n",
      "2021-08-03 10:07:48,208 - INFO - joeynmt.training - Epoch  13, Step:   221500, Batch Loss:     1.338680, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-03 10:08:02,014 - INFO - joeynmt.training - Epoch  13, Step:   221600, Batch Loss:     1.671932, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-08-03 10:08:15,728 - INFO - joeynmt.training - Epoch  13, Step:   221700, Batch Loss:     1.817653, Tokens per Sec:    15787, Lr: 0.000300\n",
      "2021-08-03 10:08:29,816 - INFO - joeynmt.training - Epoch  13, Step:   221800, Batch Loss:     1.713317, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-03 10:08:43,441 - INFO - joeynmt.training - Epoch  13, Step:   221900, Batch Loss:     1.823194, Tokens per Sec:    15402, Lr: 0.000300\n",
      "2021-08-03 10:08:57,425 - INFO - joeynmt.training - Epoch  13, Step:   222000, Batch Loss:     1.670382, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-03 10:09:11,176 - INFO - joeynmt.training - Epoch  13, Step:   222100, Batch Loss:     1.800204, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-08-03 10:09:24,796 - INFO - joeynmt.training - Epoch  13, Step:   222200, Batch Loss:     1.728346, Tokens per Sec:    15646, Lr: 0.000300\n",
      "2021-08-03 10:09:38,722 - INFO - joeynmt.training - Epoch  13, Step:   222300, Batch Loss:     1.685537, Tokens per Sec:    15555, Lr: 0.000300\n",
      "2021-08-03 10:09:52,746 - INFO - joeynmt.training - Epoch  13, Step:   222400, Batch Loss:     1.747837, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-08-03 10:10:06,747 - INFO - joeynmt.training - Epoch  13, Step:   222500, Batch Loss:     1.555191, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-03 10:10:20,698 - INFO - joeynmt.training - Epoch  13, Step:   222600, Batch Loss:     1.840537, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-08-03 10:10:34,405 - INFO - joeynmt.training - Epoch  13, Step:   222700, Batch Loss:     1.788299, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-03 10:10:48,427 - INFO - joeynmt.training - Epoch  13, Step:   222800, Batch Loss:     1.819746, Tokens per Sec:    15799, Lr: 0.000300\n",
      "2021-08-03 10:11:02,319 - INFO - joeynmt.training - Epoch  13, Step:   222900, Batch Loss:     1.517547, Tokens per Sec:    15834, Lr: 0.000300\n",
      "2021-08-03 10:11:16,089 - INFO - joeynmt.training - Epoch  13, Step:   223000, Batch Loss:     1.680122, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-08-03 10:11:29,997 - INFO - joeynmt.training - Epoch  13, Step:   223100, Batch Loss:     1.633449, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-08-03 10:11:43,881 - INFO - joeynmt.training - Epoch  13, Step:   223200, Batch Loss:     1.990533, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-03 10:11:57,584 - INFO - joeynmt.training - Epoch  13, Step:   223300, Batch Loss:     1.601668, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-03 10:12:11,239 - INFO - joeynmt.training - Epoch  13, Step:   223400, Batch Loss:     1.744745, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-03 10:12:25,065 - INFO - joeynmt.training - Epoch  13, Step:   223500, Batch Loss:     1.657497, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-03 10:12:38,854 - INFO - joeynmt.training - Epoch  13, Step:   223600, Batch Loss:     1.687771, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-08-03 10:12:52,645 - INFO - joeynmt.training - Epoch  13, Step:   223700, Batch Loss:     1.713898, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-03 10:13:06,460 - INFO - joeynmt.training - Epoch  13, Step:   223800, Batch Loss:     1.559240, Tokens per Sec:    16164, Lr: 0.000300\n",
      "2021-08-03 10:13:11,020 - INFO - joeynmt.training - Epoch  13: total training loss 9180.38\n",
      "2021-08-03 10:13:11,021 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-03 10:13:20,811 - INFO - joeynmt.training - Epoch  14, Step:   223900, Batch Loss:     1.806798, Tokens per Sec:    15027, Lr: 0.000300\n",
      "2021-08-03 10:13:34,744 - INFO - joeynmt.training - Epoch  14, Step:   224000, Batch Loss:     1.982456, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-08-03 10:13:48,611 - INFO - joeynmt.training - Epoch  14, Step:   224100, Batch Loss:     1.705692, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-03 10:14:02,125 - INFO - joeynmt.training - Epoch  14, Step:   224200, Batch Loss:     1.770795, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-08-03 10:14:15,935 - INFO - joeynmt.training - Epoch  14, Step:   224300, Batch Loss:     1.603063, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-03 10:14:29,804 - INFO - joeynmt.training - Epoch  14, Step:   224400, Batch Loss:     1.591711, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-03 10:14:43,564 - INFO - joeynmt.training - Epoch  14, Step:   224500, Batch Loss:     1.747510, Tokens per Sec:    15634, Lr: 0.000300\n",
      "2021-08-03 10:14:57,455 - INFO - joeynmt.training - Epoch  14, Step:   224600, Batch Loss:     1.502152, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-03 10:15:11,095 - INFO - joeynmt.training - Epoch  14, Step:   224700, Batch Loss:     1.809970, Tokens per Sec:    15729, Lr: 0.000300\n",
      "2021-08-03 10:15:24,961 - INFO - joeynmt.training - Epoch  14, Step:   224800, Batch Loss:     2.045245, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-03 10:15:38,928 - INFO - joeynmt.training - Epoch  14, Step:   224900, Batch Loss:     1.679940, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-03 10:15:53,056 - INFO - joeynmt.training - Epoch  14, Step:   225000, Batch Loss:     1.671082, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-03 10:17:25,872 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:17:25,872 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:17:25,872 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:17:27,154 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:17:27,154 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:17:27,943 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:17:27,944 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:17:27,944 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:17:27,944 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , so is good for me . ”\n",
      "2021-08-03 10:17:27,944 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:17:27,945 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:17:27,945 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:17:27,945 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident of our fear of Jehovah is greater than the riches that are standing . ”\n",
      "2021-08-03 10:17:27,945 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:17:27,946 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:17:27,946 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:17:27,946 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The book of Daniel has a details about God’s Kingdom .\n",
      "2021-08-03 10:17:27,946 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:17:27,947 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:17:27,947 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:17:27,947 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 10:17:27,947 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   225000: bleu:  26.99, loss: 189220.0000, ppl:   4.7725, duration: 94.8902s\n",
      "2021-08-03 10:17:42,010 - INFO - joeynmt.training - Epoch  14, Step:   225100, Batch Loss:     2.296950, Tokens per Sec:    15692, Lr: 0.000300\n",
      "2021-08-03 10:17:55,801 - INFO - joeynmt.training - Epoch  14, Step:   225200, Batch Loss:     1.699103, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-03 10:18:09,641 - INFO - joeynmt.training - Epoch  14, Step:   225300, Batch Loss:     2.011980, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-03 10:18:23,370 - INFO - joeynmt.training - Epoch  14, Step:   225400, Batch Loss:     1.820017, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-03 10:18:37,193 - INFO - joeynmt.training - Epoch  14, Step:   225500, Batch Loss:     1.627857, Tokens per Sec:    15677, Lr: 0.000300\n",
      "2021-08-03 10:18:51,166 - INFO - joeynmt.training - Epoch  14, Step:   225600, Batch Loss:     1.657315, Tokens per Sec:    16324, Lr: 0.000300\n",
      "2021-08-03 10:19:04,843 - INFO - joeynmt.training - Epoch  14, Step:   225700, Batch Loss:     1.875792, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-03 10:19:18,625 - INFO - joeynmt.training - Epoch  14, Step:   225800, Batch Loss:     1.734450, Tokens per Sec:    15819, Lr: 0.000300\n",
      "2021-08-03 10:19:32,624 - INFO - joeynmt.training - Epoch  14, Step:   225900, Batch Loss:     1.646327, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-03 10:19:46,572 - INFO - joeynmt.training - Epoch  14, Step:   226000, Batch Loss:     1.677227, Tokens per Sec:    15501, Lr: 0.000300\n",
      "2021-08-03 10:20:00,338 - INFO - joeynmt.training - Epoch  14, Step:   226100, Batch Loss:     1.744217, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-08-03 10:20:14,284 - INFO - joeynmt.training - Epoch  14, Step:   226200, Batch Loss:     1.639700, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-03 10:20:27,994 - INFO - joeynmt.training - Epoch  14, Step:   226300, Batch Loss:     1.611426, Tokens per Sec:    16141, Lr: 0.000300\n",
      "2021-08-03 10:20:41,867 - INFO - joeynmt.training - Epoch  14, Step:   226400, Batch Loss:     1.690576, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-08-03 10:20:55,692 - INFO - joeynmt.training - Epoch  14, Step:   226500, Batch Loss:     1.764763, Tokens per Sec:    15484, Lr: 0.000300\n",
      "2021-08-03 10:21:09,405 - INFO - joeynmt.training - Epoch  14, Step:   226600, Batch Loss:     1.708909, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-03 10:21:23,122 - INFO - joeynmt.training - Epoch  14, Step:   226700, Batch Loss:     1.622608, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-03 10:21:36,741 - INFO - joeynmt.training - Epoch  14, Step:   226800, Batch Loss:     1.779618, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-08-03 10:21:50,723 - INFO - joeynmt.training - Epoch  14, Step:   226900, Batch Loss:     1.631982, Tokens per Sec:    15701, Lr: 0.000300\n",
      "2021-08-03 10:22:04,665 - INFO - joeynmt.training - Epoch  14, Step:   227000, Batch Loss:     1.918305, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-03 10:22:18,537 - INFO - joeynmt.training - Epoch  14, Step:   227100, Batch Loss:     1.772555, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-08-03 10:22:32,280 - INFO - joeynmt.training - Epoch  14, Step:   227200, Batch Loss:     1.759833, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-08-03 10:22:45,782 - INFO - joeynmt.training - Epoch  14, Step:   227300, Batch Loss:     1.892146, Tokens per Sec:    15644, Lr: 0.000300\n",
      "2021-08-03 10:22:59,597 - INFO - joeynmt.training - Epoch  14, Step:   227400, Batch Loss:     1.935637, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-08-03 10:23:13,451 - INFO - joeynmt.training - Epoch  14, Step:   227500, Batch Loss:     1.861092, Tokens per Sec:    16209, Lr: 0.000300\n",
      "2021-08-03 10:23:27,227 - INFO - joeynmt.training - Epoch  14, Step:   227600, Batch Loss:     2.011987, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-08-03 10:23:41,129 - INFO - joeynmt.training - Epoch  14, Step:   227700, Batch Loss:     1.831435, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-08-03 10:23:54,821 - INFO - joeynmt.training - Epoch  14, Step:   227800, Batch Loss:     1.597035, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-08-03 10:24:08,569 - INFO - joeynmt.training - Epoch  14, Step:   227900, Batch Loss:     1.944884, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-03 10:24:22,303 - INFO - joeynmt.training - Epoch  14, Step:   228000, Batch Loss:     1.679882, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-08-03 10:24:36,203 - INFO - joeynmt.training - Epoch  14, Step:   228100, Batch Loss:     1.433371, Tokens per Sec:    15683, Lr: 0.000300\n",
      "2021-08-03 10:24:50,101 - INFO - joeynmt.training - Epoch  14, Step:   228200, Batch Loss:     1.700184, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-03 10:25:03,822 - INFO - joeynmt.training - Epoch  14, Step:   228300, Batch Loss:     1.777196, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-08-03 10:25:17,376 - INFO - joeynmt.training - Epoch  14, Step:   228400, Batch Loss:     1.714698, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-03 10:25:31,177 - INFO - joeynmt.training - Epoch  14, Step:   228500, Batch Loss:     1.631209, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-08-03 10:25:45,023 - INFO - joeynmt.training - Epoch  14, Step:   228600, Batch Loss:     1.638614, Tokens per Sec:    15803, Lr: 0.000300\n",
      "2021-08-03 10:25:58,937 - INFO - joeynmt.training - Epoch  14, Step:   228700, Batch Loss:     1.699703, Tokens per Sec:    15662, Lr: 0.000300\n",
      "2021-08-03 10:26:12,780 - INFO - joeynmt.training - Epoch  14, Step:   228800, Batch Loss:     1.773401, Tokens per Sec:    16143, Lr: 0.000300\n",
      "2021-08-03 10:26:26,323 - INFO - joeynmt.training - Epoch  14, Step:   228900, Batch Loss:     2.357378, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-08-03 10:26:40,190 - INFO - joeynmt.training - Epoch  14, Step:   229000, Batch Loss:     1.509113, Tokens per Sec:    16295, Lr: 0.000300\n",
      "2021-08-03 10:26:53,962 - INFO - joeynmt.training - Epoch  14, Step:   229100, Batch Loss:     1.806859, Tokens per Sec:    15654, Lr: 0.000300\n",
      "2021-08-03 10:27:02,752 - INFO - joeynmt.training - Epoch  14: total training loss 9178.80\n",
      "2021-08-03 10:27:02,752 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-03 10:27:08,374 - INFO - joeynmt.training - Epoch  15, Step:   229200, Batch Loss:     1.710095, Tokens per Sec:    14186, Lr: 0.000300\n",
      "2021-08-03 10:27:22,496 - INFO - joeynmt.training - Epoch  15, Step:   229300, Batch Loss:     1.581327, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-03 10:27:36,357 - INFO - joeynmt.training - Epoch  15, Step:   229400, Batch Loss:     1.686740, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-03 10:27:50,025 - INFO - joeynmt.training - Epoch  15, Step:   229500, Batch Loss:     1.688929, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-03 10:28:03,818 - INFO - joeynmt.training - Epoch  15, Step:   229600, Batch Loss:     1.649302, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-08-03 10:28:17,637 - INFO - joeynmt.training - Epoch  15, Step:   229700, Batch Loss:     1.716074, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-03 10:28:31,369 - INFO - joeynmt.training - Epoch  15, Step:   229800, Batch Loss:     1.472401, Tokens per Sec:    15488, Lr: 0.000300\n",
      "2021-08-03 10:28:45,149 - INFO - joeynmt.training - Epoch  15, Step:   229900, Batch Loss:     1.716275, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-08-03 10:28:58,825 - INFO - joeynmt.training - Epoch  15, Step:   230000, Batch Loss:     1.561523, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-08-03 10:30:32,492 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:30:32,492 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:30:32,492 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:30:34,521 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:30:34,522 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:30:34,522 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:30:34,522 - INFO - joeynmt.training - \tHypothesis: He sang : “ But as for me , drawing close to God , so good for me . ”\n",
      "2021-08-03 10:30:34,523 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:30:34,523 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:30:34,524 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:30:34,524 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we live in fear of Jehovah is greater than the riches that are standing . ”\n",
      "2021-08-03 10:30:34,525 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:30:34,526 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:30:34,526 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:30:34,526 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The book of Daniel has another prophecy about God’s Kingdom .\n",
      "2021-08-03 10:30:34,527 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:30:34,527 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:30:34,527 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:30:34,527 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely victory ?\n",
      "2021-08-03 10:30:34,528 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   230000: bleu:  27.45, loss: 189277.5469, ppl:   4.7748, duration: 95.7019s\n",
      "2021-08-03 10:30:48,816 - INFO - joeynmt.training - Epoch  15, Step:   230100, Batch Loss:     1.598766, Tokens per Sec:    15308, Lr: 0.000300\n",
      "2021-08-03 10:31:02,672 - INFO - joeynmt.training - Epoch  15, Step:   230200, Batch Loss:     1.767330, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-08-03 10:31:16,329 - INFO - joeynmt.training - Epoch  15, Step:   230300, Batch Loss:     1.651015, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-08-03 10:31:30,027 - INFO - joeynmt.training - Epoch  15, Step:   230400, Batch Loss:     1.755231, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-03 10:31:43,826 - INFO - joeynmt.training - Epoch  15, Step:   230500, Batch Loss:     1.830556, Tokens per Sec:    15891, Lr: 0.000300\n",
      "2021-08-03 10:31:57,592 - INFO - joeynmt.training - Epoch  15, Step:   230600, Batch Loss:     1.694480, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-03 10:32:11,243 - INFO - joeynmt.training - Epoch  15, Step:   230700, Batch Loss:     2.016489, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-03 10:32:24,730 - INFO - joeynmt.training - Epoch  15, Step:   230800, Batch Loss:     1.622820, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-03 10:32:38,451 - INFO - joeynmt.training - Epoch  15, Step:   230900, Batch Loss:     1.455108, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-08-03 10:32:52,386 - INFO - joeynmt.training - Epoch  15, Step:   231000, Batch Loss:     1.990032, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-03 10:33:06,234 - INFO - joeynmt.training - Epoch  15, Step:   231100, Batch Loss:     1.580685, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-08-03 10:33:20,040 - INFO - joeynmt.training - Epoch  15, Step:   231200, Batch Loss:     1.695369, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-08-03 10:33:33,779 - INFO - joeynmt.training - Epoch  15, Step:   231300, Batch Loss:     1.687288, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-08-03 10:33:47,675 - INFO - joeynmt.training - Epoch  15, Step:   231400, Batch Loss:     1.822632, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-08-03 10:34:01,554 - INFO - joeynmt.training - Epoch  15, Step:   231500, Batch Loss:     1.708526, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-03 10:34:15,332 - INFO - joeynmt.training - Epoch  15, Step:   231600, Batch Loss:     1.650842, Tokens per Sec:    15933, Lr: 0.000300\n",
      "2021-08-03 10:34:29,343 - INFO - joeynmt.training - Epoch  15, Step:   231700, Batch Loss:     1.617026, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-03 10:34:43,202 - INFO - joeynmt.training - Epoch  15, Step:   231800, Batch Loss:     1.626067, Tokens per Sec:    15710, Lr: 0.000300\n",
      "2021-08-03 10:34:57,048 - INFO - joeynmt.training - Epoch  15, Step:   231900, Batch Loss:     1.711240, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-08-03 10:35:10,558 - INFO - joeynmt.training - Epoch  15, Step:   232000, Batch Loss:     1.760559, Tokens per Sec:    15742, Lr: 0.000300\n",
      "2021-08-03 10:35:24,171 - INFO - joeynmt.training - Epoch  15, Step:   232100, Batch Loss:     1.733562, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-08-03 10:35:37,860 - INFO - joeynmt.training - Epoch  15, Step:   232200, Batch Loss:     1.698707, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-08-03 10:35:51,651 - INFO - joeynmt.training - Epoch  15, Step:   232300, Batch Loss:     2.094091, Tokens per Sec:    15748, Lr: 0.000300\n",
      "2021-08-03 10:36:05,422 - INFO - joeynmt.training - Epoch  15, Step:   232400, Batch Loss:     1.536755, Tokens per Sec:    16304, Lr: 0.000300\n",
      "2021-08-03 10:36:18,997 - INFO - joeynmt.training - Epoch  15, Step:   232500, Batch Loss:     1.665264, Tokens per Sec:    15901, Lr: 0.000300\n",
      "2021-08-03 10:36:32,683 - INFO - joeynmt.training - Epoch  15, Step:   232600, Batch Loss:     1.528831, Tokens per Sec:    15742, Lr: 0.000300\n",
      "2021-08-03 10:36:46,713 - INFO - joeynmt.training - Epoch  15, Step:   232700, Batch Loss:     1.736322, Tokens per Sec:    15928, Lr: 0.000300\n",
      "2021-08-03 10:37:00,648 - INFO - joeynmt.training - Epoch  15, Step:   232800, Batch Loss:     1.893096, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-08-03 10:37:14,399 - INFO - joeynmt.training - Epoch  15, Step:   232900, Batch Loss:     1.797608, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-08-03 10:37:28,174 - INFO - joeynmt.training - Epoch  15, Step:   233000, Batch Loss:     1.661247, Tokens per Sec:    15761, Lr: 0.000300\n",
      "2021-08-03 10:37:42,060 - INFO - joeynmt.training - Epoch  15, Step:   233100, Batch Loss:     1.640205, Tokens per Sec:    15810, Lr: 0.000300\n",
      "2021-08-03 10:37:55,973 - INFO - joeynmt.training - Epoch  15, Step:   233200, Batch Loss:     1.699901, Tokens per Sec:    15682, Lr: 0.000300\n",
      "2021-08-03 10:38:09,886 - INFO - joeynmt.training - Epoch  15, Step:   233300, Batch Loss:     1.671433, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-03 10:38:23,742 - INFO - joeynmt.training - Epoch  15, Step:   233400, Batch Loss:     1.538590, Tokens per Sec:    15669, Lr: 0.000300\n",
      "2021-08-03 10:38:37,532 - INFO - joeynmt.training - Epoch  15, Step:   233500, Batch Loss:     1.679067, Tokens per Sec:    15463, Lr: 0.000300\n",
      "2021-08-03 10:38:51,033 - INFO - joeynmt.training - Epoch  15, Step:   233600, Batch Loss:     1.614315, Tokens per Sec:    15730, Lr: 0.000300\n",
      "2021-08-03 10:39:04,707 - INFO - joeynmt.training - Epoch  15, Step:   233700, Batch Loss:     1.662330, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-03 10:39:18,442 - INFO - joeynmt.training - Epoch  15, Step:   233800, Batch Loss:     1.651761, Tokens per Sec:    16261, Lr: 0.000300\n",
      "2021-08-03 10:39:32,286 - INFO - joeynmt.training - Epoch  15, Step:   233900, Batch Loss:     1.573710, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-03 10:39:46,038 - INFO - joeynmt.training - Epoch  15, Step:   234000, Batch Loss:     1.563135, Tokens per Sec:    15850, Lr: 0.000300\n",
      "2021-08-03 10:39:59,766 - INFO - joeynmt.training - Epoch  15, Step:   234100, Batch Loss:     1.769757, Tokens per Sec:    15611, Lr: 0.000300\n",
      "2021-08-03 10:40:13,570 - INFO - joeynmt.training - Epoch  15, Step:   234200, Batch Loss:     1.749902, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-03 10:40:27,514 - INFO - joeynmt.training - Epoch  15, Step:   234300, Batch Loss:     1.759198, Tokens per Sec:    16422, Lr: 0.000300\n",
      "2021-08-03 10:40:41,539 - INFO - joeynmt.training - Epoch  15, Step:   234400, Batch Loss:     1.786035, Tokens per Sec:    15761, Lr: 0.000300\n",
      "2021-08-03 10:40:55,324 - INFO - joeynmt.training - Epoch  15, Step:   234500, Batch Loss:     1.619415, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-08-03 10:40:55,643 - INFO - joeynmt.training - Epoch  15: total training loss 9175.78\n",
      "2021-08-03 10:40:55,644 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-03 10:41:09,814 - INFO - joeynmt.training - Epoch  16, Step:   234600, Batch Loss:     1.768860, Tokens per Sec:    15506, Lr: 0.000300\n",
      "2021-08-03 10:41:23,447 - INFO - joeynmt.training - Epoch  16, Step:   234700, Batch Loss:     1.583865, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-03 10:41:37,478 - INFO - joeynmt.training - Epoch  16, Step:   234800, Batch Loss:     1.783582, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-08-03 10:41:51,462 - INFO - joeynmt.training - Epoch  16, Step:   234900, Batch Loss:     1.702838, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-03 10:42:05,295 - INFO - joeynmt.training - Epoch  16, Step:   235000, Batch Loss:     1.721774, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-08-03 10:43:37,265 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:43:37,266 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:43:37,266 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:43:38,545 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:43:38,545 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:43:39,629 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:43:39,630 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:43:39,630 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:43:39,630 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , you draw close to God , good for me . ”\n",
      "2021-08-03 10:43:39,630 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:43:39,631 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:43:39,631 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:43:39,631 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we live in fear of Jehovah is greater than many riches there is standing . ”\n",
      "2021-08-03 10:43:39,632 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:43:39,632 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:43:39,632 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:43:39,632 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The other prophecies in the book of Daniel have a detail about God’s Kingdom .\n",
      "2021-08-03 10:43:39,632 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:43:39,633 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:43:39,633 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:43:39,634 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 10:43:39,634 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   235000: bleu:  27.26, loss: 188398.6094, ppl:   4.7402, duration: 94.3380s\n",
      "2021-08-03 10:43:53,672 - INFO - joeynmt.training - Epoch  16, Step:   235100, Batch Loss:     1.767335, Tokens per Sec:    15599, Lr: 0.000300\n",
      "2021-08-03 10:44:07,410 - INFO - joeynmt.training - Epoch  16, Step:   235200, Batch Loss:     1.790473, Tokens per Sec:    15669, Lr: 0.000300\n",
      "2021-08-03 10:44:21,232 - INFO - joeynmt.training - Epoch  16, Step:   235300, Batch Loss:     1.900251, Tokens per Sec:    15662, Lr: 0.000300\n",
      "2021-08-03 10:44:35,155 - INFO - joeynmt.training - Epoch  16, Step:   235400, Batch Loss:     1.831843, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-03 10:44:48,974 - INFO - joeynmt.training - Epoch  16, Step:   235500, Batch Loss:     1.598363, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-08-03 10:45:02,636 - INFO - joeynmt.training - Epoch  16, Step:   235600, Batch Loss:     1.788930, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-08-03 10:45:16,314 - INFO - joeynmt.training - Epoch  16, Step:   235700, Batch Loss:     1.745375, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-08-03 10:45:30,121 - INFO - joeynmt.training - Epoch  16, Step:   235800, Batch Loss:     1.794595, Tokens per Sec:    15584, Lr: 0.000300\n",
      "2021-08-03 10:45:43,786 - INFO - joeynmt.training - Epoch  16, Step:   235900, Batch Loss:     1.467796, Tokens per Sec:    15596, Lr: 0.000300\n",
      "2021-08-03 10:45:57,853 - INFO - joeynmt.training - Epoch  16, Step:   236000, Batch Loss:     1.975411, Tokens per Sec:    16376, Lr: 0.000300\n",
      "2021-08-03 10:46:11,553 - INFO - joeynmt.training - Epoch  16, Step:   236100, Batch Loss:     1.744325, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-03 10:46:25,244 - INFO - joeynmt.training - Epoch  16, Step:   236200, Batch Loss:     1.649834, Tokens per Sec:    15659, Lr: 0.000300\n",
      "2021-08-03 10:46:39,125 - INFO - joeynmt.training - Epoch  16, Step:   236300, Batch Loss:     1.647634, Tokens per Sec:    15796, Lr: 0.000300\n",
      "2021-08-03 10:46:53,054 - INFO - joeynmt.training - Epoch  16, Step:   236400, Batch Loss:     1.514978, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-08-03 10:47:06,506 - INFO - joeynmt.training - Epoch  16, Step:   236500, Batch Loss:     1.597133, Tokens per Sec:    15484, Lr: 0.000300\n",
      "2021-08-03 10:47:20,375 - INFO - joeynmt.training - Epoch  16, Step:   236600, Batch Loss:     1.792452, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-08-03 10:47:34,322 - INFO - joeynmt.training - Epoch  16, Step:   236700, Batch Loss:     1.652602, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-08-03 10:47:48,264 - INFO - joeynmt.training - Epoch  16, Step:   236800, Batch Loss:     1.756254, Tokens per Sec:    15692, Lr: 0.000300\n",
      "2021-08-03 10:48:02,071 - INFO - joeynmt.training - Epoch  16, Step:   236900, Batch Loss:     1.775604, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-08-03 10:48:15,783 - INFO - joeynmt.training - Epoch  16, Step:   237000, Batch Loss:     1.712805, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-08-03 10:48:29,505 - INFO - joeynmt.training - Epoch  16, Step:   237100, Batch Loss:     1.713675, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-08-03 10:48:43,409 - INFO - joeynmt.training - Epoch  16, Step:   237200, Batch Loss:     1.819544, Tokens per Sec:    15848, Lr: 0.000300\n",
      "2021-08-03 10:48:57,325 - INFO - joeynmt.training - Epoch  16, Step:   237300, Batch Loss:     1.714754, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-03 10:49:11,128 - INFO - joeynmt.training - Epoch  16, Step:   237400, Batch Loss:     1.578558, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-03 10:49:25,098 - INFO - joeynmt.training - Epoch  16, Step:   237500, Batch Loss:     1.400770, Tokens per Sec:    15929, Lr: 0.000300\n",
      "2021-08-03 10:49:39,202 - INFO - joeynmt.training - Epoch  16, Step:   237600, Batch Loss:     1.805264, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-03 10:49:53,034 - INFO - joeynmt.training - Epoch  16, Step:   237700, Batch Loss:     1.682629, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-03 10:50:06,851 - INFO - joeynmt.training - Epoch  16, Step:   237800, Batch Loss:     1.825239, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-03 10:50:20,494 - INFO - joeynmt.training - Epoch  16, Step:   237900, Batch Loss:     1.517452, Tokens per Sec:    15650, Lr: 0.000300\n",
      "2021-08-03 10:50:34,527 - INFO - joeynmt.training - Epoch  16, Step:   238000, Batch Loss:     1.747458, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-03 10:50:48,382 - INFO - joeynmt.training - Epoch  16, Step:   238100, Batch Loss:     1.859714, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-08-03 10:51:02,073 - INFO - joeynmt.training - Epoch  16, Step:   238200, Batch Loss:     1.735853, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-08-03 10:51:15,958 - INFO - joeynmt.training - Epoch  16, Step:   238300, Batch Loss:     1.530601, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-08-03 10:51:29,912 - INFO - joeynmt.training - Epoch  16, Step:   238400, Batch Loss:     1.903489, Tokens per Sec:    15637, Lr: 0.000300\n",
      "2021-08-03 10:51:43,563 - INFO - joeynmt.training - Epoch  16, Step:   238500, Batch Loss:     1.644905, Tokens per Sec:    15552, Lr: 0.000300\n",
      "2021-08-03 10:51:57,547 - INFO - joeynmt.training - Epoch  16, Step:   238600, Batch Loss:     1.771389, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-08-03 10:52:11,379 - INFO - joeynmt.training - Epoch  16, Step:   238700, Batch Loss:     1.929728, Tokens per Sec:    16084, Lr: 0.000300\n",
      "2021-08-03 10:52:25,209 - INFO - joeynmt.training - Epoch  16, Step:   238800, Batch Loss:     1.517755, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-03 10:52:39,050 - INFO - joeynmt.training - Epoch  16, Step:   238900, Batch Loss:     1.741326, Tokens per Sec:    15519, Lr: 0.000300\n",
      "2021-08-03 10:52:52,926 - INFO - joeynmt.training - Epoch  16, Step:   239000, Batch Loss:     1.828024, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-08-03 10:53:06,628 - INFO - joeynmt.training - Epoch  16, Step:   239100, Batch Loss:     1.574014, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-08-03 10:53:20,359 - INFO - joeynmt.training - Epoch  16, Step:   239200, Batch Loss:     1.964136, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-03 10:53:34,132 - INFO - joeynmt.training - Epoch  16, Step:   239300, Batch Loss:     1.699253, Tokens per Sec:    15647, Lr: 0.000300\n",
      "2021-08-03 10:53:48,033 - INFO - joeynmt.training - Epoch  16, Step:   239400, Batch Loss:     1.754949, Tokens per Sec:    15725, Lr: 0.000300\n",
      "2021-08-03 10:54:02,037 - INFO - joeynmt.training - Epoch  16, Step:   239500, Batch Loss:     1.828376, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-03 10:54:16,037 - INFO - joeynmt.training - Epoch  16, Step:   239600, Batch Loss:     1.751169, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-08-03 10:54:29,861 - INFO - joeynmt.training - Epoch  16, Step:   239700, Batch Loss:     1.642479, Tokens per Sec:    15814, Lr: 0.000300\n",
      "2021-08-03 10:54:43,643 - INFO - joeynmt.training - Epoch  16, Step:   239800, Batch Loss:     1.789416, Tokens per Sec:    15733, Lr: 0.000300\n",
      "2021-08-03 10:54:49,037 - INFO - joeynmt.training - Epoch  16: total training loss 9151.64\n",
      "2021-08-03 10:54:49,037 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-08-03 10:54:58,124 - INFO - joeynmt.training - Epoch  17, Step:   239900, Batch Loss:     1.630810, Tokens per Sec:    14753, Lr: 0.000300\n",
      "2021-08-03 10:55:11,994 - INFO - joeynmt.training - Epoch  17, Step:   240000, Batch Loss:     1.782539, Tokens per Sec:    15617, Lr: 0.000300\n",
      "2021-08-03 10:56:46,170 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:56:46,170 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:56:46,171 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:56:48,106 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:56:48,107 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:56:48,107 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:56:48,107 - INFO - joeynmt.training - \tHypothesis: He sang : “ But as for me , draw close to God , good for me . ”\n",
      "2021-08-03 10:56:48,108 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:56:48,108 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:56:48,108 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:56:48,109 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The dread of Jehovah is greater than many wealth are standing . ”\n",
      "2021-08-03 10:56:48,109 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:56:48,109 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:56:48,109 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:56:48,110 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has a detailed point to God’s Kingdom .\n",
      "2021-08-03 10:56:48,110 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:56:48,110 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:56:48,110 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:56:48,111 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 10:56:48,111 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   240000: bleu:  27.35, loss: 188411.3438, ppl:   4.7407, duration: 96.1162s\n",
      "2021-08-03 10:57:02,090 - INFO - joeynmt.training - Epoch  17, Step:   240100, Batch Loss:     1.736144, Tokens per Sec:    15498, Lr: 0.000300\n",
      "2021-08-03 10:57:16,049 - INFO - joeynmt.training - Epoch  17, Step:   240200, Batch Loss:     1.511543, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-03 10:57:30,205 - INFO - joeynmt.training - Epoch  17, Step:   240300, Batch Loss:     1.714198, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-08-03 10:57:44,216 - INFO - joeynmt.training - Epoch  17, Step:   240400, Batch Loss:     1.610844, Tokens per Sec:    15772, Lr: 0.000300\n",
      "2021-08-03 10:57:58,088 - INFO - joeynmt.training - Epoch  17, Step:   240500, Batch Loss:     1.461916, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-08-03 10:58:11,965 - INFO - joeynmt.training - Epoch  17, Step:   240600, Batch Loss:     1.800671, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-08-03 10:58:25,770 - INFO - joeynmt.training - Epoch  17, Step:   240700, Batch Loss:     1.629308, Tokens per Sec:    16101, Lr: 0.000300\n",
      "2021-08-03 10:58:39,493 - INFO - joeynmt.training - Epoch  17, Step:   240800, Batch Loss:     1.754895, Tokens per Sec:    15899, Lr: 0.000300\n",
      "2021-08-03 10:58:53,292 - INFO - joeynmt.training - Epoch  17, Step:   240900, Batch Loss:     1.477940, Tokens per Sec:    15733, Lr: 0.000300\n",
      "2021-08-03 10:59:07,048 - INFO - joeynmt.training - Epoch  17, Step:   241000, Batch Loss:     1.552665, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-08-03 10:59:20,773 - INFO - joeynmt.training - Epoch  17, Step:   241100, Batch Loss:     1.773096, Tokens per Sec:    15519, Lr: 0.000300\n",
      "2021-08-03 10:59:34,545 - INFO - joeynmt.training - Epoch  17, Step:   241200, Batch Loss:     1.646608, Tokens per Sec:    15850, Lr: 0.000300\n",
      "2021-08-03 10:59:48,550 - INFO - joeynmt.training - Epoch  17, Step:   241300, Batch Loss:     1.821200, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-03 11:00:02,484 - INFO - joeynmt.training - Epoch  17, Step:   241400, Batch Loss:     1.662700, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-08-03 11:00:16,478 - INFO - joeynmt.training - Epoch  17, Step:   241500, Batch Loss:     1.622269, Tokens per Sec:    15803, Lr: 0.000300\n",
      "2021-08-03 11:00:30,414 - INFO - joeynmt.training - Epoch  17, Step:   241600, Batch Loss:     1.622872, Tokens per Sec:    15664, Lr: 0.000300\n",
      "2021-08-03 11:00:44,157 - INFO - joeynmt.training - Epoch  17, Step:   241700, Batch Loss:     1.684988, Tokens per Sec:    16276, Lr: 0.000300\n",
      "2021-08-03 11:00:57,826 - INFO - joeynmt.training - Epoch  17, Step:   241800, Batch Loss:     1.622907, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-03 11:01:11,558 - INFO - joeynmt.training - Epoch  17, Step:   241900, Batch Loss:     1.514397, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-08-03 11:01:25,527 - INFO - joeynmt.training - Epoch  17, Step:   242000, Batch Loss:     1.542985, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-08-03 11:01:39,395 - INFO - joeynmt.training - Epoch  17, Step:   242100, Batch Loss:     1.828077, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-08-03 11:01:53,099 - INFO - joeynmt.training - Epoch  17, Step:   242200, Batch Loss:     1.603154, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-08-03 11:02:06,868 - INFO - joeynmt.training - Epoch  17, Step:   242300, Batch Loss:     1.581945, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-08-03 11:02:20,391 - INFO - joeynmt.training - Epoch  17, Step:   242400, Batch Loss:     1.807155, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-08-03 11:02:34,229 - INFO - joeynmt.training - Epoch  17, Step:   242500, Batch Loss:     1.709347, Tokens per Sec:    15495, Lr: 0.000300\n",
      "2021-08-03 11:02:48,057 - INFO - joeynmt.training - Epoch  17, Step:   242600, Batch Loss:     1.764770, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-03 11:03:01,879 - INFO - joeynmt.training - Epoch  17, Step:   242700, Batch Loss:     1.638610, Tokens per Sec:    16213, Lr: 0.000300\n",
      "2021-08-03 11:03:15,654 - INFO - joeynmt.training - Epoch  17, Step:   242800, Batch Loss:     1.586360, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-08-03 11:03:29,487 - INFO - joeynmt.training - Epoch  17, Step:   242900, Batch Loss:     1.921516, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-08-03 11:03:43,309 - INFO - joeynmt.training - Epoch  17, Step:   243000, Batch Loss:     1.694294, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-08-03 11:03:57,009 - INFO - joeynmt.training - Epoch  17, Step:   243100, Batch Loss:     1.797725, Tokens per Sec:    15632, Lr: 0.000300\n",
      "2021-08-03 11:04:10,793 - INFO - joeynmt.training - Epoch  17, Step:   243200, Batch Loss:     1.700957, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-03 11:04:24,275 - INFO - joeynmt.training - Epoch  17, Step:   243300, Batch Loss:     1.580422, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-03 11:04:37,834 - INFO - joeynmt.training - Epoch  17, Step:   243400, Batch Loss:     2.048083, Tokens per Sec:    15589, Lr: 0.000300\n",
      "2021-08-03 11:04:51,655 - INFO - joeynmt.training - Epoch  17, Step:   243500, Batch Loss:     1.759802, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-08-03 11:05:05,585 - INFO - joeynmt.training - Epoch  17, Step:   243600, Batch Loss:     1.807227, Tokens per Sec:    16101, Lr: 0.000300\n",
      "2021-08-03 11:05:19,480 - INFO - joeynmt.training - Epoch  17, Step:   243700, Batch Loss:     1.575286, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-03 11:05:33,396 - INFO - joeynmt.training - Epoch  17, Step:   243800, Batch Loss:     1.645385, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-08-03 11:05:47,202 - INFO - joeynmt.training - Epoch  17, Step:   243900, Batch Loss:     1.666949, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-08-03 11:06:01,062 - INFO - joeynmt.training - Epoch  17, Step:   244000, Batch Loss:     1.829075, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-08-03 11:06:14,766 - INFO - joeynmt.training - Epoch  17, Step:   244100, Batch Loss:     1.790127, Tokens per Sec:    16179, Lr: 0.000300\n",
      "2021-08-03 11:06:28,693 - INFO - joeynmt.training - Epoch  17, Step:   244200, Batch Loss:     1.542938, Tokens per Sec:    15661, Lr: 0.000300\n",
      "2021-08-03 11:06:42,317 - INFO - joeynmt.training - Epoch  17, Step:   244300, Batch Loss:     1.663055, Tokens per Sec:    15532, Lr: 0.000300\n",
      "2021-08-03 11:06:55,951 - INFO - joeynmt.training - Epoch  17, Step:   244400, Batch Loss:     1.716253, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-08-03 11:07:09,745 - INFO - joeynmt.training - Epoch  17, Step:   244500, Batch Loss:     1.485121, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-08-03 11:07:23,551 - INFO - joeynmt.training - Epoch  17, Step:   244600, Batch Loss:     1.773500, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-03 11:07:37,453 - INFO - joeynmt.training - Epoch  17, Step:   244700, Batch Loss:     1.566449, Tokens per Sec:    15762, Lr: 0.000300\n",
      "2021-08-03 11:07:51,364 - INFO - joeynmt.training - Epoch  17, Step:   244800, Batch Loss:     1.763806, Tokens per Sec:    15939, Lr: 0.000300\n",
      "2021-08-03 11:08:05,101 - INFO - joeynmt.training - Epoch  17, Step:   244900, Batch Loss:     1.691269, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-08-03 11:08:18,990 - INFO - joeynmt.training - Epoch  17, Step:   245000, Batch Loss:     1.660406, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-03 11:09:53,858 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:09:53,858 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:09:53,859 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:09:55,149 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 11:09:55,149 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 11:09:55,908 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:09:55,909 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 11:09:55,909 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 11:09:55,909 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , so good for me . ”\n",
      "2021-08-03 11:09:55,909 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:09:55,910 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 11:09:55,910 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 11:09:55,910 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are fearing Jehovah is greater than the abundance of riches are standing . ”\n",
      "2021-08-03 11:09:55,910 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:09:55,911 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 11:09:55,911 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 11:09:55,911 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The other prophecy in the book of Daniel has a detailed point of God’s Kingdom .\n",
      "2021-08-03 11:09:55,911 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:09:55,912 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 11:09:55,912 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 11:09:55,912 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 11:09:55,912 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   245000: bleu:  27.41, loss: 188212.8750, ppl:   4.7330, duration: 96.9223s\n",
      "2021-08-03 11:10:09,810 - INFO - joeynmt.training - Epoch  17, Step:   245100, Batch Loss:     1.848842, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-08-03 11:10:19,020 - INFO - joeynmt.training - Epoch  17: total training loss 9125.08\n",
      "2021-08-03 11:10:19,021 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-08-03 11:10:24,195 - INFO - joeynmt.training - Epoch  18, Step:   245200, Batch Loss:     1.625782, Tokens per Sec:    14598, Lr: 0.000300\n",
      "2021-08-03 11:10:37,769 - INFO - joeynmt.training - Epoch  18, Step:   245300, Batch Loss:     1.703929, Tokens per Sec:    15719, Lr: 0.000300\n",
      "2021-08-03 11:10:51,825 - INFO - joeynmt.training - Epoch  18, Step:   245400, Batch Loss:     1.804406, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-03 11:11:05,416 - INFO - joeynmt.training - Epoch  18, Step:   245500, Batch Loss:     1.797425, Tokens per Sec:    15696, Lr: 0.000300\n",
      "2021-08-03 11:11:19,116 - INFO - joeynmt.training - Epoch  18, Step:   245600, Batch Loss:     1.834712, Tokens per Sec:    15712, Lr: 0.000300\n",
      "2021-08-03 11:11:32,989 - INFO - joeynmt.training - Epoch  18, Step:   245700, Batch Loss:     1.777910, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-08-03 11:11:46,809 - INFO - joeynmt.training - Epoch  18, Step:   245800, Batch Loss:     1.555885, Tokens per Sec:    15886, Lr: 0.000300\n",
      "2021-08-03 11:12:00,351 - INFO - joeynmt.training - Epoch  18, Step:   245900, Batch Loss:     1.599637, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-08-03 11:12:13,996 - INFO - joeynmt.training - Epoch  18, Step:   246000, Batch Loss:     1.855321, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-08-03 11:12:27,797 - INFO - joeynmt.training - Epoch  18, Step:   246100, Batch Loss:     1.849554, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-03 11:12:41,558 - INFO - joeynmt.training - Epoch  18, Step:   246200, Batch Loss:     1.726247, Tokens per Sec:    15929, Lr: 0.000300\n",
      "2021-08-03 11:12:55,310 - INFO - joeynmt.training - Epoch  18, Step:   246300, Batch Loss:     1.681214, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-08-03 11:13:09,205 - INFO - joeynmt.training - Epoch  18, Step:   246400, Batch Loss:     1.688245, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-03 11:13:22,820 - INFO - joeynmt.training - Epoch  18, Step:   246500, Batch Loss:     1.536944, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-08-03 11:13:36,642 - INFO - joeynmt.training - Epoch  18, Step:   246600, Batch Loss:     1.553043, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-03 11:13:50,525 - INFO - joeynmt.training - Epoch  18, Step:   246700, Batch Loss:     1.822668, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-03 11:14:04,089 - INFO - joeynmt.training - Epoch  18, Step:   246800, Batch Loss:     1.711258, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-03 11:14:17,882 - INFO - joeynmt.training - Epoch  18, Step:   246900, Batch Loss:     1.701587, Tokens per Sec:    16302, Lr: 0.000300\n",
      "2021-08-03 11:14:31,491 - INFO - joeynmt.training - Epoch  18, Step:   247000, Batch Loss:     1.706742, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-08-03 11:14:45,381 - INFO - joeynmt.training - Epoch  18, Step:   247100, Batch Loss:     1.582074, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-08-03 11:14:59,179 - INFO - joeynmt.training - Epoch  18, Step:   247200, Batch Loss:     1.778051, Tokens per Sec:    15817, Lr: 0.000300\n",
      "2021-08-03 11:15:12,989 - INFO - joeynmt.training - Epoch  18, Step:   247300, Batch Loss:     1.841193, Tokens per Sec:    16286, Lr: 0.000300\n",
      "2021-08-03 11:15:26,740 - INFO - joeynmt.training - Epoch  18, Step:   247400, Batch Loss:     1.708824, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-03 11:15:40,674 - INFO - joeynmt.training - Epoch  18, Step:   247500, Batch Loss:     1.690307, Tokens per Sec:    16303, Lr: 0.000300\n",
      "2021-08-03 11:15:54,375 - INFO - joeynmt.training - Epoch  18, Step:   247600, Batch Loss:     1.814395, Tokens per Sec:    15590, Lr: 0.000300\n",
      "2021-08-03 11:16:08,101 - INFO - joeynmt.training - Epoch  18, Step:   247700, Batch Loss:     1.698845, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-08-03 11:16:22,142 - INFO - joeynmt.training - Epoch  18, Step:   247800, Batch Loss:     1.807959, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-03 11:16:35,879 - INFO - joeynmt.training - Epoch  18, Step:   247900, Batch Loss:     1.778971, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-08-03 11:16:49,602 - INFO - joeynmt.training - Epoch  18, Step:   248000, Batch Loss:     1.845330, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-03 11:17:03,473 - INFO - joeynmt.training - Epoch  18, Step:   248100, Batch Loss:     1.721130, Tokens per Sec:    16285, Lr: 0.000300\n",
      "2021-08-03 11:17:17,421 - INFO - joeynmt.training - Epoch  18, Step:   248200, Batch Loss:     1.710734, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-03 11:17:31,294 - INFO - joeynmt.training - Epoch  18, Step:   248300, Batch Loss:     1.869087, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-08-03 11:17:44,947 - INFO - joeynmt.training - Epoch  18, Step:   248400, Batch Loss:     1.680890, Tokens per Sec:    15610, Lr: 0.000300\n",
      "2021-08-03 11:17:58,702 - INFO - joeynmt.training - Epoch  18, Step:   248500, Batch Loss:     1.831685, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-03 11:18:12,509 - INFO - joeynmt.training - Epoch  18, Step:   248600, Batch Loss:     1.578225, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-08-03 11:18:26,345 - INFO - joeynmt.training - Epoch  18, Step:   248700, Batch Loss:     1.693130, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-03 11:18:40,253 - INFO - joeynmt.training - Epoch  18, Step:   248800, Batch Loss:     1.723925, Tokens per Sec:    15673, Lr: 0.000300\n",
      "2021-08-03 11:18:53,970 - INFO - joeynmt.training - Epoch  18, Step:   248900, Batch Loss:     1.915347, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-03 11:19:07,760 - INFO - joeynmt.training - Epoch  18, Step:   249000, Batch Loss:     1.795134, Tokens per Sec:    16139, Lr: 0.000300\n",
      "2021-08-03 11:19:21,155 - INFO - joeynmt.training - Epoch  18, Step:   249100, Batch Loss:     1.830885, Tokens per Sec:    15666, Lr: 0.000300\n",
      "2021-08-03 11:19:34,958 - INFO - joeynmt.training - Epoch  18, Step:   249200, Batch Loss:     1.768633, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-03 11:19:48,717 - INFO - joeynmt.training - Epoch  18, Step:   249300, Batch Loss:     1.330721, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-08-03 11:20:02,538 - INFO - joeynmt.training - Epoch  18, Step:   249400, Batch Loss:     1.597250, Tokens per Sec:    16227, Lr: 0.000300\n",
      "2021-08-03 11:20:16,289 - INFO - joeynmt.training - Epoch  18, Step:   249500, Batch Loss:     1.903771, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-03 11:20:30,057 - INFO - joeynmt.training - Epoch  18, Step:   249600, Batch Loss:     1.439418, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-03 11:20:43,892 - INFO - joeynmt.training - Epoch  18, Step:   249700, Batch Loss:     1.604681, Tokens per Sec:    15864, Lr: 0.000300\n",
      "2021-08-03 11:20:57,586 - INFO - joeynmt.training - Epoch  18, Step:   249800, Batch Loss:     1.750638, Tokens per Sec:    15575, Lr: 0.000300\n",
      "2021-08-03 11:21:11,216 - INFO - joeynmt.training - Epoch  18, Step:   249900, Batch Loss:     1.678594, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-08-03 11:21:24,820 - INFO - joeynmt.training - Epoch  18, Step:   250000, Batch Loss:     1.906061, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-03 11:22:58,401 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:22:58,401 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:22:58,402 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:22:59,585 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 11:22:59,585 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 11:23:00,296 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:23:00,297 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 11:23:00,297 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 11:23:00,297 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , draw close to God , so good for me . ”\n",
      "2021-08-03 11:23:00,297 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:23:00,298 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 11:23:00,298 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 11:23:00,298 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The dwelling of the fear of Jehovah is greater than many riches are standing . ”\n",
      "2021-08-03 11:23:00,298 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:23:00,299 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 11:23:00,299 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 11:23:00,299 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has a detailed point of God’s Kingdom .\n",
      "2021-08-03 11:23:00,299 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:23:00,300 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 11:23:00,300 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 11:23:00,300 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-03 11:23:00,300 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   250000: bleu:  27.39, loss: 187954.9062, ppl:   4.7229, duration: 95.4793s\n",
      "2021-08-03 11:23:14,109 - INFO - joeynmt.training - Epoch  18, Step:   250100, Batch Loss:     1.859201, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-03 11:23:27,965 - INFO - joeynmt.training - Epoch  18, Step:   250200, Batch Loss:     2.080667, Tokens per Sec:    15928, Lr: 0.000300\n",
      "2021-08-03 11:23:41,871 - INFO - joeynmt.training - Epoch  18, Step:   250300, Batch Loss:     1.791391, Tokens per Sec:    15608, Lr: 0.000300\n",
      "2021-08-03 11:23:55,647 - INFO - joeynmt.training - Epoch  18, Step:   250400, Batch Loss:     1.950670, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-03 11:24:09,242 - INFO - joeynmt.training - Epoch  18, Step:   250500, Batch Loss:     1.606808, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-08-03 11:24:09,678 - INFO - joeynmt.training - Epoch  18: total training loss 9119.87\n",
      "2021-08-03 11:24:09,679 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-08-03 11:24:23,397 - INFO - joeynmt.training - Epoch  19, Step:   250600, Batch Loss:     1.582009, Tokens per Sec:    15287, Lr: 0.000300\n",
      "2021-08-03 11:24:37,237 - INFO - joeynmt.training - Epoch  19, Step:   250700, Batch Loss:     1.559664, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-03 11:24:50,985 - INFO - joeynmt.training - Epoch  19, Step:   250800, Batch Loss:     1.555938, Tokens per Sec:    15756, Lr: 0.000300\n",
      "2021-08-03 11:25:04,967 - INFO - joeynmt.training - Epoch  19, Step:   250900, Batch Loss:     1.886333, Tokens per Sec:    16189, Lr: 0.000300\n",
      "2021-08-03 11:25:18,790 - INFO - joeynmt.training - Epoch  19, Step:   251000, Batch Loss:     1.611808, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-08-03 11:25:32,521 - INFO - joeynmt.training - Epoch  19, Step:   251100, Batch Loss:     1.640588, Tokens per Sec:    15886, Lr: 0.000300\n",
      "2021-08-03 11:25:46,323 - INFO - joeynmt.training - Epoch  19, Step:   251200, Batch Loss:     1.692814, Tokens per Sec:    15589, Lr: 0.000300\n",
      "2021-08-03 11:26:00,261 - INFO - joeynmt.training - Epoch  19, Step:   251300, Batch Loss:     1.729046, Tokens per Sec:    16132, Lr: 0.000300\n",
      "2021-08-03 11:26:14,095 - INFO - joeynmt.training - Epoch  19, Step:   251400, Batch Loss:     1.743418, Tokens per Sec:    16118, Lr: 0.000300\n",
      "2021-08-03 11:26:27,775 - INFO - joeynmt.training - Epoch  19, Step:   251500, Batch Loss:     1.617678, Tokens per Sec:    15774, Lr: 0.000300\n",
      "2021-08-03 11:26:41,690 - INFO - joeynmt.training - Epoch  19, Step:   251600, Batch Loss:     1.744889, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-03 11:26:55,525 - INFO - joeynmt.training - Epoch  19, Step:   251700, Batch Loss:     1.750731, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-08-03 11:27:09,406 - INFO - joeynmt.training - Epoch  19, Step:   251800, Batch Loss:     1.668769, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-08-03 11:27:23,327 - INFO - joeynmt.training - Epoch  19, Step:   251900, Batch Loss:     1.789296, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-03 11:27:37,322 - INFO - joeynmt.training - Epoch  19, Step:   252000, Batch Loss:     1.708199, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-08-03 11:27:51,132 - INFO - joeynmt.training - Epoch  19, Step:   252100, Batch Loss:     1.547876, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-08-03 11:28:05,115 - INFO - joeynmt.training - Epoch  19, Step:   252200, Batch Loss:     1.720705, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-08-03 11:28:18,872 - INFO - joeynmt.training - Epoch  19, Step:   252300, Batch Loss:     1.659260, Tokens per Sec:    15794, Lr: 0.000300\n",
      "2021-08-03 11:28:32,655 - INFO - joeynmt.training - Epoch  19, Step:   252400, Batch Loss:     1.858938, Tokens per Sec:    15649, Lr: 0.000300\n",
      "2021-08-03 11:28:46,269 - INFO - joeynmt.training - Epoch  19, Step:   252500, Batch Loss:     1.636341, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-03 11:29:00,214 - INFO - joeynmt.training - Epoch  19, Step:   252600, Batch Loss:     1.724861, Tokens per Sec:    16308, Lr: 0.000300\n",
      "2021-08-03 11:29:14,046 - INFO - joeynmt.training - Epoch  19, Step:   252700, Batch Loss:     1.643220, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-08-03 11:29:27,958 - INFO - joeynmt.training - Epoch  19, Step:   252800, Batch Loss:     1.764177, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-08-03 11:29:42,044 - INFO - joeynmt.training - Epoch  19, Step:   252900, Batch Loss:     1.567545, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-08-03 11:29:55,772 - INFO - joeynmt.training - Epoch  19, Step:   253000, Batch Loss:     1.733889, Tokens per Sec:    15774, Lr: 0.000300\n",
      "2021-08-03 11:30:09,605 - INFO - joeynmt.training - Epoch  19, Step:   253100, Batch Loss:     1.551381, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-03 11:30:23,383 - INFO - joeynmt.training - Epoch  19, Step:   253200, Batch Loss:     1.938215, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-08-03 11:30:37,254 - INFO - joeynmt.training - Epoch  19, Step:   253300, Batch Loss:     1.657209, Tokens per Sec:    15515, Lr: 0.000300\n",
      "2021-08-03 11:30:51,087 - INFO - joeynmt.training - Epoch  19, Step:   253400, Batch Loss:     1.721599, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-08-03 11:31:04,955 - INFO - joeynmt.training - Epoch  19, Step:   253500, Batch Loss:     1.370061, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-08-03 11:31:18,846 - INFO - joeynmt.training - Epoch  19, Step:   253600, Batch Loss:     1.843198, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-08-03 11:31:32,600 - INFO - joeynmt.training - Epoch  19, Step:   253700, Batch Loss:     1.571812, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-08-03 11:31:46,549 - INFO - joeynmt.training - Epoch  19, Step:   253800, Batch Loss:     1.908782, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-08-03 11:32:00,329 - INFO - joeynmt.training - Epoch  19, Step:   253900, Batch Loss:     1.624624, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-08-03 11:32:14,299 - INFO - joeynmt.training - Epoch  19, Step:   254000, Batch Loss:     1.789278, Tokens per Sec:    16236, Lr: 0.000300\n",
      "2021-08-03 11:32:28,023 - INFO - joeynmt.training - Epoch  19, Step:   254100, Batch Loss:     1.760247, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-03 11:32:41,951 - INFO - joeynmt.training - Epoch  19, Step:   254200, Batch Loss:     1.677833, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-08-03 11:32:56,012 - INFO - joeynmt.training - Epoch  19, Step:   254300, Batch Loss:     1.715919, Tokens per Sec:    15795, Lr: 0.000300\n",
      "2021-08-03 11:33:09,900 - INFO - joeynmt.training - Epoch  19, Step:   254400, Batch Loss:     1.836960, Tokens per Sec:    15626, Lr: 0.000300\n",
      "2021-08-03 11:33:23,778 - INFO - joeynmt.training - Epoch  19, Step:   254500, Batch Loss:     1.619971, Tokens per Sec:    15469, Lr: 0.000300\n",
      "2021-08-03 11:33:37,812 - INFO - joeynmt.training - Epoch  19, Step:   254600, Batch Loss:     1.660317, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-08-03 11:33:51,556 - INFO - joeynmt.training - Epoch  19, Step:   254700, Batch Loss:     1.766341, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-03 11:34:05,224 - INFO - joeynmt.training - Epoch  19, Step:   254800, Batch Loss:     1.581708, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-03 11:34:19,153 - INFO - joeynmt.training - Epoch  19, Step:   254900, Batch Loss:     1.695137, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-08-03 11:34:33,001 - INFO - joeynmt.training - Epoch  19, Step:   255000, Batch Loss:     1.437521, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-03 11:36:05,877 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:36:05,877 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:36:05,878 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:36:07,069 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 11:36:07,069 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 11:36:07,792 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:36:07,793 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 11:36:07,793 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 11:36:07,794 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , drawing close to God , so good for me . ”\n",
      "2021-08-03 11:36:07,794 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:36:07,794 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 11:36:07,794 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 11:36:07,795 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The dwelling of the dwelling of Jehovah is greater than the abundance of riches that are standing . ”\n",
      "2021-08-03 11:36:07,795 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:36:07,795 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 11:36:07,795 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 11:36:07,795 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has a detailed meaning on God’s Kingdom .\n",
      "2021-08-03 11:36:07,796 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:36:07,796 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 11:36:07,796 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 11:36:07,796 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 11:36:07,796 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   255000: bleu:  27.36, loss: 187299.9531, ppl:   4.6974, duration: 94.7945s\n",
      "2021-08-03 11:36:21,610 - INFO - joeynmt.training - Epoch  19, Step:   255100, Batch Loss:     1.526318, Tokens per Sec:    15852, Lr: 0.000300\n",
      "2021-08-03 11:36:35,401 - INFO - joeynmt.training - Epoch  19, Step:   255200, Batch Loss:     1.913425, Tokens per Sec:    15653, Lr: 0.000300\n",
      "2021-08-03 11:36:49,329 - INFO - joeynmt.training - Epoch  19, Step:   255300, Batch Loss:     1.727427, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-03 11:37:03,323 - INFO - joeynmt.training - Epoch  19, Step:   255400, Batch Loss:     1.671732, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-08-03 11:37:17,124 - INFO - joeynmt.training - Epoch  19, Step:   255500, Batch Loss:     1.732100, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-08-03 11:37:30,938 - INFO - joeynmt.training - Epoch  19, Step:   255600, Batch Loss:     1.714841, Tokens per Sec:    16269, Lr: 0.000300\n",
      "2021-08-03 11:37:44,815 - INFO - joeynmt.training - Epoch  19, Step:   255700, Batch Loss:     1.541486, Tokens per Sec:    15722, Lr: 0.000300\n",
      "2021-08-03 11:37:58,760 - INFO - joeynmt.training - Epoch  19, Step:   255800, Batch Loss:     1.575098, Tokens per Sec:    15912, Lr: 0.000300\n",
      "2021-08-03 11:38:01,130 - INFO - joeynmt.training - Epoch  19: total training loss 9067.36\n",
      "2021-08-03 11:38:01,130 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-08-03 11:38:13,172 - INFO - joeynmt.training - Epoch  20, Step:   255900, Batch Loss:     1.975393, Tokens per Sec:    15117, Lr: 0.000300\n",
      "2021-08-03 11:38:27,099 - INFO - joeynmt.training - Epoch  20, Step:   256000, Batch Loss:     1.827760, Tokens per Sec:    15697, Lr: 0.000300\n",
      "2021-08-03 11:38:40,718 - INFO - joeynmt.training - Epoch  20, Step:   256100, Batch Loss:     1.689200, Tokens per Sec:    15625, Lr: 0.000300\n",
      "2021-08-03 11:38:54,366 - INFO - joeynmt.training - Epoch  20, Step:   256200, Batch Loss:     1.539036, Tokens per Sec:    15790, Lr: 0.000300\n",
      "2021-08-03 11:39:08,265 - INFO - joeynmt.training - Epoch  20, Step:   256300, Batch Loss:     1.556414, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-03 11:39:22,136 - INFO - joeynmt.training - Epoch  20, Step:   256400, Batch Loss:     1.566009, Tokens per Sec:    16136, Lr: 0.000300\n",
      "2021-08-03 11:39:35,862 - INFO - joeynmt.training - Epoch  20, Step:   256500, Batch Loss:     1.798326, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-08-03 11:39:49,692 - INFO - joeynmt.training - Epoch  20, Step:   256600, Batch Loss:     1.494559, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-08-03 11:40:03,347 - INFO - joeynmt.training - Epoch  20, Step:   256700, Batch Loss:     1.692779, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-08-03 11:40:16,961 - INFO - joeynmt.training - Epoch  20, Step:   256800, Batch Loss:     1.568543, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-03 11:40:31,019 - INFO - joeynmt.training - Epoch  20, Step:   256900, Batch Loss:     1.681791, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-03 11:40:44,681 - INFO - joeynmt.training - Epoch  20, Step:   257000, Batch Loss:     1.844772, Tokens per Sec:    15601, Lr: 0.000300\n",
      "2021-08-03 11:40:58,547 - INFO - joeynmt.training - Epoch  20, Step:   257100, Batch Loss:     1.817179, Tokens per Sec:    16282, Lr: 0.000300\n",
      "2021-08-03 11:41:12,457 - INFO - joeynmt.training - Epoch  20, Step:   257200, Batch Loss:     1.768863, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-03 11:41:26,223 - INFO - joeynmt.training - Epoch  20, Step:   257300, Batch Loss:     1.709975, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-03 11:41:40,190 - INFO - joeynmt.training - Epoch  20, Step:   257400, Batch Loss:     1.771783, Tokens per Sec:    15704, Lr: 0.000300\n",
      "2021-08-03 11:41:54,238 - INFO - joeynmt.training - Epoch  20, Step:   257500, Batch Loss:     1.627429, Tokens per Sec:    16287, Lr: 0.000300\n",
      "2021-08-03 11:42:07,822 - INFO - joeynmt.training - Epoch  20, Step:   257600, Batch Loss:     1.557900, Tokens per Sec:    15741, Lr: 0.000300\n",
      "2021-08-03 11:42:21,614 - INFO - joeynmt.training - Epoch  20, Step:   257700, Batch Loss:     1.681656, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-08-03 11:42:35,526 - INFO - joeynmt.training - Epoch  20, Step:   257800, Batch Loss:     1.648675, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-03 11:42:49,308 - INFO - joeynmt.training - Epoch  20, Step:   257900, Batch Loss:     1.632753, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-08-03 11:43:03,127 - INFO - joeynmt.training - Epoch  20, Step:   258000, Batch Loss:     2.149632, Tokens per Sec:    15639, Lr: 0.000300\n",
      "2021-08-03 11:43:16,995 - INFO - joeynmt.training - Epoch  20, Step:   258100, Batch Loss:     1.664029, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-08-03 11:43:30,644 - INFO - joeynmt.training - Epoch  20, Step:   258200, Batch Loss:     1.651103, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-08-03 11:43:44,513 - INFO - joeynmt.training - Epoch  20, Step:   258300, Batch Loss:     1.626310, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-08-03 11:43:58,463 - INFO - joeynmt.training - Epoch  20, Step:   258400, Batch Loss:     1.708821, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-03 11:44:12,376 - INFO - joeynmt.training - Epoch  20, Step:   258500, Batch Loss:     1.539845, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-03 11:44:26,279 - INFO - joeynmt.training - Epoch  20, Step:   258600, Batch Loss:     1.726167, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-03 11:44:40,212 - INFO - joeynmt.training - Epoch  20, Step:   258700, Batch Loss:     1.765059, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-03 11:44:54,122 - INFO - joeynmt.training - Epoch  20, Step:   258800, Batch Loss:     1.674782, Tokens per Sec:    16310, Lr: 0.000300\n",
      "2021-08-03 11:45:07,737 - INFO - joeynmt.training - Epoch  20, Step:   258900, Batch Loss:     1.839493, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-03 11:45:21,445 - INFO - joeynmt.training - Epoch  20, Step:   259000, Batch Loss:     1.772708, Tokens per Sec:    15538, Lr: 0.000300\n",
      "2021-08-03 11:45:35,430 - INFO - joeynmt.training - Epoch  20, Step:   259100, Batch Loss:     1.720206, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-03 11:45:49,146 - INFO - joeynmt.training - Epoch  20, Step:   259200, Batch Loss:     1.686185, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-03 11:46:02,849 - INFO - joeynmt.training - Epoch  20, Step:   259300, Batch Loss:     1.710035, Tokens per Sec:    15650, Lr: 0.000300\n",
      "2021-08-03 11:46:16,551 - INFO - joeynmt.training - Epoch  20, Step:   259400, Batch Loss:     1.743361, Tokens per Sec:    16129, Lr: 0.000300\n",
      "2021-08-03 11:46:30,473 - INFO - joeynmt.training - Epoch  20, Step:   259500, Batch Loss:     1.616384, Tokens per Sec:    16021, Lr: 0.000300\n",
      "2021-08-03 11:46:44,421 - INFO - joeynmt.training - Epoch  20, Step:   259600, Batch Loss:     1.798172, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-08-03 11:46:58,164 - INFO - joeynmt.training - Epoch  20, Step:   259700, Batch Loss:     1.515318, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-08-03 11:47:11,867 - INFO - joeynmt.training - Epoch  20, Step:   259800, Batch Loss:     1.530138, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-03 11:47:25,702 - INFO - joeynmt.training - Epoch  20, Step:   259900, Batch Loss:     1.720691, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-03 11:47:39,541 - INFO - joeynmt.training - Epoch  20, Step:   260000, Batch Loss:     1.696959, Tokens per Sec:    15658, Lr: 0.000300\n",
      "2021-08-03 11:49:11,967 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:49:11,968 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:49:11,968 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:49:13,274 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 11:49:13,275 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 11:49:14,048 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:49:14,049 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 11:49:14,049 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 11:49:14,049 - INFO - joeynmt.training - \tHypothesis: He sang : “ But as for me , drawing close to God , so good for me . ”\n",
      "2021-08-03 11:49:14,050 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:49:14,050 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 11:49:14,050 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 11:49:14,050 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The residents are fearing Jehovah are greater than many riches are standing . ”\n",
      "2021-08-03 11:49:14,051 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:49:14,051 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 11:49:14,051 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 11:49:14,051 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has a detailed point on God’s Kingdom .\n",
      "2021-08-03 11:49:14,051 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:49:14,052 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 11:49:14,052 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 11:49:14,052 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 11:49:14,052 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   260000: bleu:  27.55, loss: 186705.0156, ppl:   4.6744, duration: 94.5110s\n",
      "2021-08-03 11:49:28,143 - INFO - joeynmt.training - Epoch  20, Step:   260100, Batch Loss:     1.763696, Tokens per Sec:    15429, Lr: 0.000300\n",
      "2021-08-03 11:49:41,958 - INFO - joeynmt.training - Epoch  20, Step:   260200, Batch Loss:     1.750449, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-08-03 11:49:55,833 - INFO - joeynmt.training - Epoch  20, Step:   260300, Batch Loss:     1.697195, Tokens per Sec:    15912, Lr: 0.000300\n",
      "2021-08-03 11:50:09,625 - INFO - joeynmt.training - Epoch  20, Step:   260400, Batch Loss:     1.688824, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-03 11:50:23,375 - INFO - joeynmt.training - Epoch  20, Step:   260500, Batch Loss:     1.496941, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-08-03 11:50:37,310 - INFO - joeynmt.training - Epoch  20, Step:   260600, Batch Loss:     1.606200, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-03 11:50:51,019 - INFO - joeynmt.training - Epoch  20, Step:   260700, Batch Loss:     1.747222, Tokens per Sec:    15905, Lr: 0.000300\n",
      "2021-08-03 11:51:04,707 - INFO - joeynmt.training - Epoch  20, Step:   260800, Batch Loss:     1.660919, Tokens per Sec:    15882, Lr: 0.000300\n",
      "2021-08-03 11:51:18,407 - INFO - joeynmt.training - Epoch  20, Step:   260900, Batch Loss:     1.623707, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-03 11:51:32,198 - INFO - joeynmt.training - Epoch  20, Step:   261000, Batch Loss:     2.114972, Tokens per Sec:    15477, Lr: 0.000300\n",
      "2021-08-03 11:51:46,004 - INFO - joeynmt.training - Epoch  20, Step:   261100, Batch Loss:     1.754408, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-08-03 11:51:51,668 - INFO - joeynmt.training - Epoch  20: total training loss 9067.13\n",
      "2021-08-03 11:51:51,669 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-08-03 11:52:00,375 - INFO - joeynmt.training - Epoch  21, Step:   261200, Batch Loss:     1.723011, Tokens per Sec:    14723, Lr: 0.000300\n",
      "2021-08-03 11:52:14,359 - INFO - joeynmt.training - Epoch  21, Step:   261300, Batch Loss:     1.616064, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-03 11:52:28,236 - INFO - joeynmt.training - Epoch  21, Step:   261400, Batch Loss:     1.652709, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-03 11:52:42,180 - INFO - joeynmt.training - Epoch  21, Step:   261500, Batch Loss:     1.767495, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-08-03 11:52:56,116 - INFO - joeynmt.training - Epoch  21, Step:   261600, Batch Loss:     1.863228, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-03 11:53:09,891 - INFO - joeynmt.training - Epoch  21, Step:   261700, Batch Loss:     1.763363, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-03 11:53:23,554 - INFO - joeynmt.training - Epoch  21, Step:   261800, Batch Loss:     1.695867, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-08-03 11:53:37,489 - INFO - joeynmt.training - Epoch  21, Step:   261900, Batch Loss:     1.640186, Tokens per Sec:    15882, Lr: 0.000300\n",
      "2021-08-03 11:53:51,487 - INFO - joeynmt.training - Epoch  21, Step:   262000, Batch Loss:     1.818178, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-03 11:54:05,380 - INFO - joeynmt.training - Epoch  21, Step:   262100, Batch Loss:     1.493274, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-03 11:54:19,035 - INFO - joeynmt.training - Epoch  21, Step:   262200, Batch Loss:     1.680270, Tokens per Sec:    15772, Lr: 0.000300\n",
      "2021-08-03 11:54:32,777 - INFO - joeynmt.training - Epoch  21, Step:   262300, Batch Loss:     1.964257, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-08-03 11:54:46,526 - INFO - joeynmt.training - Epoch  21, Step:   262400, Batch Loss:     1.796292, Tokens per Sec:    15682, Lr: 0.000300\n",
      "2021-08-03 11:55:00,217 - INFO - joeynmt.training - Epoch  21, Step:   262500, Batch Loss:     1.755199, Tokens per Sec:    15613, Lr: 0.000300\n",
      "2021-08-03 11:55:14,198 - INFO - joeynmt.training - Epoch  21, Step:   262600, Batch Loss:     1.652011, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-03 11:55:28,224 - INFO - joeynmt.training - Epoch  21, Step:   262700, Batch Loss:     1.677459, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-08-03 11:55:42,140 - INFO - joeynmt.training - Epoch  21, Step:   262800, Batch Loss:     1.597607, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-03 11:55:55,796 - INFO - joeynmt.training - Epoch  21, Step:   262900, Batch Loss:     1.893282, Tokens per Sec:    15906, Lr: 0.000300\n",
      "2021-08-03 11:56:09,492 - INFO - joeynmt.training - Epoch  21, Step:   263000, Batch Loss:     1.783788, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-08-03 11:56:23,333 - INFO - joeynmt.training - Epoch  21, Step:   263100, Batch Loss:     1.939671, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-08-03 11:56:37,216 - INFO - joeynmt.training - Epoch  21, Step:   263200, Batch Loss:     1.900676, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-03 11:56:51,127 - INFO - joeynmt.training - Epoch  21, Step:   263300, Batch Loss:     1.706245, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-08-03 11:57:04,913 - INFO - joeynmt.training - Epoch  21, Step:   263400, Batch Loss:     1.730707, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-03 11:57:18,682 - INFO - joeynmt.training - Epoch  21, Step:   263500, Batch Loss:     1.643186, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-03 11:57:32,528 - INFO - joeynmt.training - Epoch  21, Step:   263600, Batch Loss:     1.616437, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-03 11:57:46,563 - INFO - joeynmt.training - Epoch  21, Step:   263700, Batch Loss:     1.944044, Tokens per Sec:    15999, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_rwen_reload2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_pgYC3FmiWN"
   },
   "source": [
    "20.5 epochs done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmA0g27Lfkz5",
    "outputId": "8b934613-646c-4c63-d70d-6c9693b147f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 160000\tLoss: 194533.73438\tPPL: 4.98663\tbleu: 26.36352\tLR: 0.00030000\t*\n",
      "Steps: 165000\tLoss: 193757.01562\tPPL: 4.95474\tbleu: 26.37000\tLR: 0.00030000\t*\n",
      "Steps: 170000\tLoss: 193487.07812\tPPL: 4.94371\tbleu: 26.52795\tLR: 0.00030000\t*\n",
      "Steps: 175000\tLoss: 192673.14062\tPPL: 4.91058\tbleu: 26.41083\tLR: 0.00030000\t*\n",
      "Steps: 180000\tLoss: 192016.76562\tPPL: 4.88403\tbleu: 26.90800\tLR: 0.00030000\t*\n",
      "Steps: 185000\tLoss: 192402.76562\tPPL: 4.89963\tbleu: 26.86276\tLR: 0.00030000\t\n",
      "Steps: 190000\tLoss: 191457.79688\tPPL: 4.86154\tbleu: 27.04945\tLR: 0.00030000\t*\n",
      "Steps: 195000\tLoss: 190923.75000\tPPL: 4.84014\tbleu: 26.96196\tLR: 0.00030000\t*\n",
      "Steps: 200000\tLoss: 191517.00000\tPPL: 4.86392\tbleu: 26.88936\tLR: 0.00030000\t\n",
      "Steps: 205000\tLoss: 190386.06250\tPPL: 4.81869\tbleu: 26.96544\tLR: 0.00030000\t*\n",
      "Steps: 210000\tLoss: 190259.48438\tPPL: 4.81366\tbleu: 26.85295\tLR: 0.00030000\t*\n",
      "Steps: 215000\tLoss: 190039.75000\tPPL: 4.80493\tbleu: 27.04768\tLR: 0.00030000\t*\n",
      "Steps: 220000\tLoss: 190176.12500\tPPL: 4.81035\tbleu: 27.14828\tLR: 0.00030000\t\n",
      "Steps: 225000\tLoss: 189220.00000\tPPL: 4.77251\tbleu: 26.98863\tLR: 0.00030000\t*\n",
      "Steps: 230000\tLoss: 189277.54688\tPPL: 4.77478\tbleu: 27.44829\tLR: 0.00030000\t\n",
      "Steps: 235000\tLoss: 188398.60938\tPPL: 4.74024\tbleu: 27.26051\tLR: 0.00030000\t*\n",
      "Steps: 240000\tLoss: 188411.34375\tPPL: 4.74074\tbleu: 27.34967\tLR: 0.00030000\t\n",
      "Steps: 245000\tLoss: 188212.87500\tPPL: 4.73297\tbleu: 27.40826\tLR: 0.00030000\t*\n",
      "Steps: 250000\tLoss: 187954.90625\tPPL: 4.72290\tbleu: 27.38998\tLR: 0.00030000\t*\n",
      "Steps: 255000\tLoss: 187299.95312\tPPL: 4.69742\tbleu: 27.36378\tLR: 0.00030000\t*\n",
      "Steps: 260000\tLoss: 186705.01562\tPPL: 4.67439\tbleu: 27.54954\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/rwen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UpiMPsdyM2F"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 260000\n",
    "\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/models/rwen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rwen_reverse_transformer\"', f'model_dir: \"models/rwen_reverse_transformer_continued2\"').replace(\n",
    "            f'epochs: 30', f'epochs: 10')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "W6yMelLex79k",
    "outputId": "8a9bc413-5ee1-4b1b-9f83-9ea320a3d657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rwen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/260000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 2000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 10                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rwen_reverse_transformer_continued2\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_{name}_reload3.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxeCIKiKx7yz",
    "outputId": "389aab5d-e08b-4e56-b335-3ae708186459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 15:10:15,726 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 15:10:15,813 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 15:10:26,650 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 15:10:27,478 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 15:10:28,651 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 15:10:29,787 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 15:10:29,788 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 15:10:30,058 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 15:10:30.320043: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 15:10:33,965 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-08-03 15:10:36,208 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/260000.ckpt\n",
      "2021-08-03 15:10:36,762 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-08-03 15:10:36,762 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-08-03 15:10:36,763 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 15:10:36,763 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-08-03 15:10:36,763 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-08-03 15:10:36,763 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-08-03 15:10:36,764 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 15:10:36,764 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 15:10:36,764 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 15:10:36,764 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-08-03 15:10:36,765 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-08-03 15:10:36,765 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 15:10:36,765 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 15:10:36,765 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/260000.ckpt\n",
      "2021-08-03 15:10:36,765 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 15:10:36,766 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 15:10:36,766 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 15:10:36,766 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 15:10:36,766 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 15:10:36,767 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 15:10:36,767 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 15:10:36,767 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 15:10:36,767 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 15:10:36,768 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 15:10:36,768 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 15:10:36,768 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 15:10:36,768 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 15:10:36,769 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 15:10:36,770 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 15:10:36,770 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 15:10:36,771 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-03 15:10:36,771 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 15:10:36,771 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 15:10:36,771 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 15:10:36,772 - INFO - joeynmt.helpers - cfg.training.epochs                : 10\n",
      "2021-08-03 15:10:36,772 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-03 15:10:36,772 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-08-03 15:10:36,772 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 15:10:36,773 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer_continued2\n",
      "2021-08-03 15:10:36,773 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 15:10:36,773 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 15:10:36,773 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 15:10:36,774 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 15:10:36,774 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 15:10:36,774 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 15:10:36,774 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 15:10:36,775 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 15:10:36,775 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 15:10:36,775 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 15:10:36,775 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 15:10:36,776 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 15:10:36,776 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 15:10:36,776 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 15:10:36,776 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 15:10:36,777 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 15:10:36,777 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 15:10:36,777 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 15:10:36,777 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 15:10:36,777 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 15:10:36,778 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 15:10:36,778 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 15:10:36,778 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 15:10:36,778 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 15:10:36,779 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 15:10:36,779 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 15:10:36,779 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 15:10:36,779 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 15:10:36,780 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 15:10:36,780 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 15:10:36,780 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 15:10:36,780 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 426806,\n",
      "\tvalid 4368,\n",
      "\ttest 4368\n",
      "2021-08-03 15:10:36,781 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-03 15:10:36,781 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 15:10:36,781 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 15:10:36,782 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-08-03 15:10:36,782 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-08-03 15:10:36,782 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-08-03 15:10:36,804 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 15:10:36,805 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 15:11:08,364 - INFO - joeynmt.training - Epoch   1, Step:   260100, Batch Loss:     1.729964, Tokens per Sec:     6889, Lr: 0.000300\n",
      "2021-08-03 15:11:38,073 - INFO - joeynmt.training - Epoch   1, Step:   260200, Batch Loss:     1.746420, Tokens per Sec:     7486, Lr: 0.000300\n",
      "2021-08-03 15:12:07,801 - INFO - joeynmt.training - Epoch   1, Step:   260300, Batch Loss:     1.708281, Tokens per Sec:     7426, Lr: 0.000300\n",
      "2021-08-03 15:12:37,366 - INFO - joeynmt.training - Epoch   1, Step:   260400, Batch Loss:     1.708240, Tokens per Sec:     7578, Lr: 0.000300\n",
      "2021-08-03 15:13:06,846 - INFO - joeynmt.training - Epoch   1, Step:   260500, Batch Loss:     1.524931, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-08-03 15:13:36,485 - INFO - joeynmt.training - Epoch   1, Step:   260600, Batch Loss:     1.548037, Tokens per Sec:     7528, Lr: 0.000300\n",
      "2021-08-03 15:14:05,834 - INFO - joeynmt.training - Epoch   1, Step:   260700, Batch Loss:     1.770050, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-03 15:14:35,396 - INFO - joeynmt.training - Epoch   1, Step:   260800, Batch Loss:     1.665614, Tokens per Sec:     7354, Lr: 0.000300\n",
      "2021-08-03 15:15:04,850 - INFO - joeynmt.training - Epoch   1, Step:   260900, Batch Loss:     1.612775, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-08-03 15:15:34,122 - INFO - joeynmt.training - Epoch   1, Step:   261000, Batch Loss:     2.107085, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-08-03 15:16:03,555 - INFO - joeynmt.training - Epoch   1, Step:   261100, Batch Loss:     1.784288, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-08-03 15:16:15,736 - INFO - joeynmt.training - Epoch   1: total training loss 1947.11\n",
      "2021-08-03 15:16:15,737 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 15:16:33,863 - INFO - joeynmt.training - Epoch   2, Step:   261200, Batch Loss:     1.720723, Tokens per Sec:     7071, Lr: 0.000300\n",
      "2021-08-03 15:17:03,843 - INFO - joeynmt.training - Epoch   2, Step:   261300, Batch Loss:     1.628665, Tokens per Sec:     7522, Lr: 0.000300\n",
      "2021-08-03 15:17:33,781 - INFO - joeynmt.training - Epoch   2, Step:   261400, Batch Loss:     1.642833, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-08-03 15:18:03,432 - INFO - joeynmt.training - Epoch   2, Step:   261500, Batch Loss:     1.760080, Tokens per Sec:     7462, Lr: 0.000300\n",
      "2021-08-03 15:18:33,132 - INFO - joeynmt.training - Epoch   2, Step:   261600, Batch Loss:     1.896631, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-08-03 15:19:02,765 - INFO - joeynmt.training - Epoch   2, Step:   261700, Batch Loss:     1.761341, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-08-03 15:19:32,053 - INFO - joeynmt.training - Epoch   2, Step:   261800, Batch Loss:     1.679073, Tokens per Sec:     7374, Lr: 0.000300\n",
      "2021-08-03 15:20:01,760 - INFO - joeynmt.training - Epoch   2, Step:   261900, Batch Loss:     1.654193, Tokens per Sec:     7450, Lr: 0.000300\n",
      "2021-08-03 15:20:31,498 - INFO - joeynmt.training - Epoch   2, Step:   262000, Batch Loss:     1.829759, Tokens per Sec:     7498, Lr: 0.000300\n",
      "2021-08-03 15:21:00,994 - INFO - joeynmt.training - Epoch   2, Step:   262100, Batch Loss:     1.507122, Tokens per Sec:     7534, Lr: 0.000300\n",
      "2021-08-03 15:21:30,397 - INFO - joeynmt.training - Epoch   2, Step:   262200, Batch Loss:     1.691651, Tokens per Sec:     7324, Lr: 0.000300\n",
      "2021-08-03 15:21:59,977 - INFO - joeynmt.training - Epoch   2, Step:   262300, Batch Loss:     1.981854, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-03 15:22:29,274 - INFO - joeynmt.training - Epoch   2, Step:   262400, Batch Loss:     1.798520, Tokens per Sec:     7360, Lr: 0.000300\n",
      "2021-08-03 15:22:58,289 - INFO - joeynmt.training - Epoch   2, Step:   262500, Batch Loss:     1.762622, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-08-03 15:23:28,209 - INFO - joeynmt.training - Epoch   2, Step:   262600, Batch Loss:     1.648590, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-08-03 15:23:57,925 - INFO - joeynmt.training - Epoch   2, Step:   262700, Batch Loss:     1.684113, Tokens per Sec:     7595, Lr: 0.000300\n",
      "2021-08-03 15:24:27,605 - INFO - joeynmt.training - Epoch   2, Step:   262800, Batch Loss:     1.638403, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-08-03 15:24:56,925 - INFO - joeynmt.training - Epoch   2, Step:   262900, Batch Loss:     1.895353, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-08-03 15:25:26,346 - INFO - joeynmt.training - Epoch   2, Step:   263000, Batch Loss:     1.818989, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-08-03 15:25:55,904 - INFO - joeynmt.training - Epoch   2, Step:   263100, Batch Loss:     1.903851, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-08-03 15:26:25,500 - INFO - joeynmt.training - Epoch   2, Step:   263200, Batch Loss:     1.889108, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-08-03 15:26:55,156 - INFO - joeynmt.training - Epoch   2, Step:   263300, Batch Loss:     1.704701, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-08-03 15:27:24,674 - INFO - joeynmt.training - Epoch   2, Step:   263400, Batch Loss:     1.750622, Tokens per Sec:     7465, Lr: 0.000300\n",
      "2021-08-03 15:27:54,229 - INFO - joeynmt.training - Epoch   2, Step:   263500, Batch Loss:     1.662880, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-08-03 15:28:23,632 - INFO - joeynmt.training - Epoch   2, Step:   263600, Batch Loss:     1.659449, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-08-03 15:28:53,427 - INFO - joeynmt.training - Epoch   2, Step:   263700, Batch Loss:     1.920319, Tokens per Sec:     7537, Lr: 0.000300\n",
      "2021-08-03 15:29:22,916 - INFO - joeynmt.training - Epoch   2, Step:   263800, Batch Loss:     1.910491, Tokens per Sec:     7459, Lr: 0.000300\n",
      "2021-08-03 15:29:52,592 - INFO - joeynmt.training - Epoch   2, Step:   263900, Batch Loss:     1.729691, Tokens per Sec:     7412, Lr: 0.000300\n",
      "2021-08-03 15:30:22,031 - INFO - joeynmt.training - Epoch   2, Step:   264000, Batch Loss:     1.724669, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-08-03 15:30:51,509 - INFO - joeynmt.training - Epoch   2, Step:   264100, Batch Loss:     1.770322, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-08-03 15:31:21,159 - INFO - joeynmt.training - Epoch   2, Step:   264200, Batch Loss:     1.708549, Tokens per Sec:     7469, Lr: 0.000300\n",
      "2021-08-03 15:31:50,661 - INFO - joeynmt.training - Epoch   2, Step:   264300, Batch Loss:     1.664628, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-03 15:32:20,490 - INFO - joeynmt.training - Epoch   2, Step:   264400, Batch Loss:     1.676258, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-08-03 15:32:50,349 - INFO - joeynmt.training - Epoch   2, Step:   264500, Batch Loss:     1.683499, Tokens per Sec:     7589, Lr: 0.000300\n",
      "2021-08-03 15:33:19,801 - INFO - joeynmt.training - Epoch   2, Step:   264600, Batch Loss:     1.641007, Tokens per Sec:     7502, Lr: 0.000300\n",
      "2021-08-03 15:33:49,342 - INFO - joeynmt.training - Epoch   2, Step:   264700, Batch Loss:     1.629022, Tokens per Sec:     7278, Lr: 0.000300\n",
      "2021-08-03 15:34:18,699 - INFO - joeynmt.training - Epoch   2, Step:   264800, Batch Loss:     1.602364, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-08-03 15:34:48,170 - INFO - joeynmt.training - Epoch   2, Step:   264900, Batch Loss:     1.776635, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-03 15:35:17,524 - INFO - joeynmt.training - Epoch   2, Step:   265000, Batch Loss:     1.619444, Tokens per Sec:     7302, Lr: 0.000300\n",
      "2021-08-03 15:38:57,419 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 15:38:57,419 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 15:38:57,420 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 15:38:58,971 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 15:38:58,971 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 15:38:59,861 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 15:38:59,862 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 15:38:59,862 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 15:38:59,863 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , draw close to God , so good for me . ”\n",
      "2021-08-03 15:38:59,863 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 15:38:59,864 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 15:38:59,864 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 15:38:59,864 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are fearing Jehovah is greater than the riches that are standing . ”\n",
      "2021-08-03 15:38:59,864 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 15:38:59,865 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 15:38:59,865 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 15:38:59,865 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has a detail about God’s Kingdom .\n",
      "2021-08-03 15:38:59,866 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 15:38:59,866 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 15:38:59,866 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 15:38:59,867 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 15:38:59,868 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   265000: bleu:  27.63, loss: 186668.7812, ppl:   4.6730, duration: 222.3438s\n",
      "2021-08-03 15:39:29,610 - INFO - joeynmt.training - Epoch   2, Step:   265100, Batch Loss:     1.679032, Tokens per Sec:     7339, Lr: 0.000300\n",
      "2021-08-03 15:39:58,883 - INFO - joeynmt.training - Epoch   2, Step:   265200, Batch Loss:     1.449007, Tokens per Sec:     7331, Lr: 0.000300\n",
      "2021-08-03 15:40:28,428 - INFO - joeynmt.training - Epoch   2, Step:   265300, Batch Loss:     1.668124, Tokens per Sec:     7533, Lr: 0.000300\n",
      "2021-08-03 15:40:58,077 - INFO - joeynmt.training - Epoch   2, Step:   265400, Batch Loss:     1.480364, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-08-03 15:41:27,558 - INFO - joeynmt.training - Epoch   2, Step:   265500, Batch Loss:     1.408826, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-08-03 15:41:56,964 - INFO - joeynmt.training - Epoch   2, Step:   265600, Batch Loss:     1.691399, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-08-03 15:42:26,590 - INFO - joeynmt.training - Epoch   2, Step:   265700, Batch Loss:     1.599358, Tokens per Sec:     7516, Lr: 0.000300\n",
      "2021-08-03 15:42:56,314 - INFO - joeynmt.training - Epoch   2, Step:   265800, Batch Loss:     1.603909, Tokens per Sec:     7525, Lr: 0.000300\n",
      "2021-08-03 15:43:25,626 - INFO - joeynmt.training - Epoch   2, Step:   265900, Batch Loss:     1.748295, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-08-03 15:43:55,311 - INFO - joeynmt.training - Epoch   2, Step:   266000, Batch Loss:     1.752524, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-08-03 15:44:24,584 - INFO - joeynmt.training - Epoch   2, Step:   266100, Batch Loss:     1.848341, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-08-03 15:44:53,768 - INFO - joeynmt.training - Epoch   2, Step:   266200, Batch Loss:     1.715279, Tokens per Sec:     7359, Lr: 0.000300\n",
      "2021-08-03 15:45:22,983 - INFO - joeynmt.training - Epoch   2, Step:   266300, Batch Loss:     2.013148, Tokens per Sec:     7255, Lr: 0.000300\n",
      "2021-08-03 15:45:52,620 - INFO - joeynmt.training - Epoch   2, Step:   266400, Batch Loss:     1.641537, Tokens per Sec:     7558, Lr: 0.000300\n",
      "2021-08-03 15:46:11,182 - INFO - joeynmt.training - Epoch   2: total training loss 9052.75\n",
      "2021-08-03 15:46:11,183 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 15:46:23,330 - INFO - joeynmt.training - Epoch   3, Step:   266500, Batch Loss:     1.789802, Tokens per Sec:     7021, Lr: 0.000300\n",
      "2021-08-03 15:46:52,765 - INFO - joeynmt.training - Epoch   3, Step:   266600, Batch Loss:     1.650881, Tokens per Sec:     7368, Lr: 0.000300\n",
      "2021-08-03 15:47:22,548 - INFO - joeynmt.training - Epoch   3, Step:   266700, Batch Loss:     1.693097, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-08-03 15:47:52,016 - INFO - joeynmt.training - Epoch   3, Step:   266800, Batch Loss:     1.743291, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-08-03 15:48:21,360 - INFO - joeynmt.training - Epoch   3, Step:   266900, Batch Loss:     1.560624, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-08-03 15:48:50,966 - INFO - joeynmt.training - Epoch   3, Step:   267000, Batch Loss:     1.710554, Tokens per Sec:     7567, Lr: 0.000300\n",
      "2021-08-03 15:49:20,473 - INFO - joeynmt.training - Epoch   3, Step:   267100, Batch Loss:     2.069402, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-03 15:49:50,052 - INFO - joeynmt.training - Epoch   3, Step:   267200, Batch Loss:     1.570096, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-08-03 15:50:19,792 - INFO - joeynmt.training - Epoch   3, Step:   267300, Batch Loss:     1.770577, Tokens per Sec:     7446, Lr: 0.000300\n",
      "2021-08-03 15:50:49,187 - INFO - joeynmt.training - Epoch   3, Step:   267400, Batch Loss:     1.836818, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-08-03 15:51:18,599 - INFO - joeynmt.training - Epoch   3, Step:   267500, Batch Loss:     1.588799, Tokens per Sec:     7237, Lr: 0.000300\n",
      "2021-08-03 15:51:48,143 - INFO - joeynmt.training - Epoch   3, Step:   267600, Batch Loss:     1.671833, Tokens per Sec:     7486, Lr: 0.000300\n",
      "2021-08-03 15:52:17,887 - INFO - joeynmt.training - Epoch   3, Step:   267700, Batch Loss:     1.792866, Tokens per Sec:     7553, Lr: 0.000300\n",
      "2021-08-03 15:52:47,342 - INFO - joeynmt.training - Epoch   3, Step:   267800, Batch Loss:     1.674604, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 15:53:16,770 - INFO - joeynmt.training - Epoch   3, Step:   267900, Batch Loss:     1.624441, Tokens per Sec:     7430, Lr: 0.000300\n",
      "2021-08-03 15:53:46,486 - INFO - joeynmt.training - Epoch   3, Step:   268000, Batch Loss:     1.670346, Tokens per Sec:     7543, Lr: 0.000300\n",
      "2021-08-03 15:54:16,037 - INFO - joeynmt.training - Epoch   3, Step:   268100, Batch Loss:     1.655377, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-08-03 15:54:45,178 - INFO - joeynmt.training - Epoch   3, Step:   268200, Batch Loss:     1.703615, Tokens per Sec:     7286, Lr: 0.000300\n",
      "2021-08-03 15:55:14,568 - INFO - joeynmt.training - Epoch   3, Step:   268300, Batch Loss:     1.718928, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-08-03 15:55:44,262 - INFO - joeynmt.training - Epoch   3, Step:   268400, Batch Loss:     1.609769, Tokens per Sec:     7550, Lr: 0.000300\n",
      "2021-08-03 15:56:13,743 - INFO - joeynmt.training - Epoch   3, Step:   268500, Batch Loss:     1.877572, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-08-03 15:56:43,269 - INFO - joeynmt.training - Epoch   3, Step:   268600, Batch Loss:     1.451113, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-08-03 15:57:13,078 - INFO - joeynmt.training - Epoch   3, Step:   268700, Batch Loss:     1.774380, Tokens per Sec:     7634, Lr: 0.000300\n",
      "2021-08-03 15:57:42,661 - INFO - joeynmt.training - Epoch   3, Step:   268800, Batch Loss:     1.502723, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-03 15:58:12,329 - INFO - joeynmt.training - Epoch   3, Step:   268900, Batch Loss:     1.638612, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-08-03 15:58:41,577 - INFO - joeynmt.training - Epoch   3, Step:   269000, Batch Loss:     1.675561, Tokens per Sec:     7399, Lr: 0.000300\n",
      "2021-08-03 15:59:10,946 - INFO - joeynmt.training - Epoch   3, Step:   269100, Batch Loss:     1.641649, Tokens per Sec:     7435, Lr: 0.000300\n",
      "2021-08-03 15:59:40,345 - INFO - joeynmt.training - Epoch   3, Step:   269200, Batch Loss:     1.609659, Tokens per Sec:     7488, Lr: 0.000300\n",
      "2021-08-03 16:00:09,880 - INFO - joeynmt.training - Epoch   3, Step:   269300, Batch Loss:     1.878479, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-08-03 16:00:39,355 - INFO - joeynmt.training - Epoch   3, Step:   269400, Batch Loss:     1.625623, Tokens per Sec:     7450, Lr: 0.000300\n",
      "2021-08-03 16:01:08,620 - INFO - joeynmt.training - Epoch   3, Step:   269500, Batch Loss:     1.539757, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-08-03 16:01:37,720 - INFO - joeynmt.training - Epoch   3, Step:   269600, Batch Loss:     1.629317, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-08-03 16:02:07,292 - INFO - joeynmt.training - Epoch   3, Step:   269700, Batch Loss:     1.600468, Tokens per Sec:     7599, Lr: 0.000300\n",
      "2021-08-03 16:02:36,605 - INFO - joeynmt.training - Epoch   3, Step:   269800, Batch Loss:     1.737861, Tokens per Sec:     7375, Lr: 0.000300\n",
      "2021-08-03 16:03:05,818 - INFO - joeynmt.training - Epoch   3, Step:   269900, Batch Loss:     1.771727, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-08-03 16:03:35,373 - INFO - joeynmt.training - Epoch   3, Step:   270000, Batch Loss:     1.438596, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-08-03 16:07:08,683 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 16:07:08,684 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 16:07:08,684 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 16:07:11,154 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 16:07:11,155 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 16:07:11,155 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 16:07:11,155 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , draw close to God , so good for me . ”\n",
      "2021-08-03 16:07:11,155 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 16:07:11,156 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 16:07:11,156 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 16:07:11,156 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The dwelling of the fear of Jehovah is greater than many riches are standing . ”\n",
      "2021-08-03 16:07:11,157 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 16:07:11,158 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 16:07:11,158 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 16:07:11,158 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has a detail about God’s Kingdom .\n",
      "2021-08-03 16:07:11,159 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 16:07:11,159 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 16:07:11,159 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 16:07:11,160 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue to conquer ” and conquer completely ?\n",
      "2021-08-03 16:07:11,160 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   270000: bleu:  27.46, loss: 186920.0938, ppl:   4.6827, duration: 215.7863s\n",
      "2021-08-03 16:07:41,607 - INFO - joeynmt.training - Epoch   3, Step:   270100, Batch Loss:     1.557124, Tokens per Sec:     7250, Lr: 0.000300\n",
      "2021-08-03 16:08:11,364 - INFO - joeynmt.training - Epoch   3, Step:   270200, Batch Loss:     1.876526, Tokens per Sec:     7479, Lr: 0.000300\n",
      "2021-08-03 16:08:40,861 - INFO - joeynmt.training - Epoch   3, Step:   270300, Batch Loss:     1.778620, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-08-03 16:09:09,729 - INFO - joeynmt.training - Epoch   3, Step:   270400, Batch Loss:     1.625101, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-08-03 16:09:39,120 - INFO - joeynmt.training - Epoch   3, Step:   270500, Batch Loss:     1.721246, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-08-03 16:10:08,811 - INFO - joeynmt.training - Epoch   3, Step:   270600, Batch Loss:     1.900366, Tokens per Sec:     7433, Lr: 0.000300\n",
      "2021-08-03 16:10:37,936 - INFO - joeynmt.training - Epoch   3, Step:   270700, Batch Loss:     1.667076, Tokens per Sec:     7274, Lr: 0.000300\n",
      "2021-08-03 16:11:07,792 - INFO - joeynmt.training - Epoch   3, Step:   270800, Batch Loss:     1.621400, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-08-03 16:11:37,764 - INFO - joeynmt.training - Epoch   3, Step:   270900, Batch Loss:     1.690141, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-08-03 16:12:07,006 - INFO - joeynmt.training - Epoch   3, Step:   271000, Batch Loss:     1.678374, Tokens per Sec:     7307, Lr: 0.000300\n",
      "2021-08-03 16:12:36,478 - INFO - joeynmt.training - Epoch   3, Step:   271100, Batch Loss:     1.670015, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-08-03 16:13:05,924 - INFO - joeynmt.training - Epoch   3, Step:   271200, Batch Loss:     1.628958, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 16:13:35,103 - INFO - joeynmt.training - Epoch   3, Step:   271300, Batch Loss:     1.785143, Tokens per Sec:     7345, Lr: 0.000300\n",
      "2021-08-03 16:14:04,755 - INFO - joeynmt.training - Epoch   3, Step:   271400, Batch Loss:     1.648351, Tokens per Sec:     7494, Lr: 0.000300\n",
      "2021-08-03 16:14:34,162 - INFO - joeynmt.training - Epoch   3, Step:   271500, Batch Loss:     1.477301, Tokens per Sec:     7346, Lr: 0.000300\n",
      "2021-08-03 16:15:03,894 - INFO - joeynmt.training - Epoch   3, Step:   271600, Batch Loss:     1.543120, Tokens per Sec:     7598, Lr: 0.000300\n",
      "2021-08-03 16:15:33,057 - INFO - joeynmt.training - Epoch   3, Step:   271700, Batch Loss:     1.664350, Tokens per Sec:     7403, Lr: 0.000300\n",
      "2021-08-03 16:16:02,312 - INFO - joeynmt.training - Epoch   3, Step:   271800, Batch Loss:     1.707834, Tokens per Sec:     7353, Lr: 0.000300\n",
      "2021-08-03 16:16:02,332 - INFO - joeynmt.training - Epoch   3: total training loss 9063.33\n",
      "2021-08-03 16:16:02,333 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 16:16:32,440 - INFO - joeynmt.training - Epoch   4, Step:   271900, Batch Loss:     1.573291, Tokens per Sec:     7325, Lr: 0.000300\n",
      "2021-08-03 16:17:01,823 - INFO - joeynmt.training - Epoch   4, Step:   272000, Batch Loss:     1.633823, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-08-03 16:17:31,257 - INFO - joeynmt.training - Epoch   4, Step:   272100, Batch Loss:     1.699338, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-08-03 16:18:00,803 - INFO - joeynmt.training - Epoch   4, Step:   272200, Batch Loss:     1.735165, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-08-03 16:18:30,521 - INFO - joeynmt.training - Epoch   4, Step:   272300, Batch Loss:     1.734016, Tokens per Sec:     7498, Lr: 0.000300\n",
      "2021-08-03 16:18:59,728 - INFO - joeynmt.training - Epoch   4, Step:   272400, Batch Loss:     1.901175, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-03 16:19:28,912 - INFO - joeynmt.training - Epoch   4, Step:   272500, Batch Loss:     1.886500, Tokens per Sec:     7330, Lr: 0.000300\n",
      "2021-08-03 16:19:58,521 - INFO - joeynmt.training - Epoch   4, Step:   272600, Batch Loss:     1.537317, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-08-03 16:20:28,026 - INFO - joeynmt.training - Epoch   4, Step:   272700, Batch Loss:     1.700427, Tokens per Sec:     7485, Lr: 0.000300\n",
      "2021-08-03 16:20:57,696 - INFO - joeynmt.training - Epoch   4, Step:   272800, Batch Loss:     1.572971, Tokens per Sec:     7446, Lr: 0.000300\n",
      "2021-08-03 16:21:26,896 - INFO - joeynmt.training - Epoch   4, Step:   272900, Batch Loss:     1.647483, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-08-03 16:21:56,255 - INFO - joeynmt.training - Epoch   4, Step:   273000, Batch Loss:     1.580494, Tokens per Sec:     7366, Lr: 0.000300\n",
      "2021-08-03 16:22:25,531 - INFO - joeynmt.training - Epoch   4, Step:   273100, Batch Loss:     1.626355, Tokens per Sec:     7319, Lr: 0.000300\n",
      "2021-08-03 16:22:55,130 - INFO - joeynmt.training - Epoch   4, Step:   273200, Batch Loss:     1.891069, Tokens per Sec:     7539, Lr: 0.000300\n",
      "2021-08-03 16:23:24,181 - INFO - joeynmt.training - Epoch   4, Step:   273300, Batch Loss:     1.605421, Tokens per Sec:     7269, Lr: 0.000300\n",
      "2021-08-03 16:23:54,026 - INFO - joeynmt.training - Epoch   4, Step:   273400, Batch Loss:     1.797164, Tokens per Sec:     7537, Lr: 0.000300\n",
      "2021-08-03 16:24:23,607 - INFO - joeynmt.training - Epoch   4, Step:   273500, Batch Loss:     1.486345, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-03 16:24:53,543 - INFO - joeynmt.training - Epoch   4, Step:   273600, Batch Loss:     1.723746, Tokens per Sec:     7573, Lr: 0.000300\n",
      "2021-08-03 16:25:22,858 - INFO - joeynmt.training - Epoch   4, Step:   273700, Batch Loss:     1.767341, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-08-03 16:25:52,457 - INFO - joeynmt.training - Epoch   4, Step:   273800, Batch Loss:     1.687148, Tokens per Sec:     7481, Lr: 0.000300\n",
      "2021-08-03 16:26:21,865 - INFO - joeynmt.training - Epoch   4, Step:   273900, Batch Loss:     1.682536, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-08-03 16:26:51,661 - INFO - joeynmt.training - Epoch   4, Step:   274000, Batch Loss:     1.673460, Tokens per Sec:     7519, Lr: 0.000300\n",
      "2021-08-03 16:27:21,174 - INFO - joeynmt.training - Epoch   4, Step:   274100, Batch Loss:     1.490898, Tokens per Sec:     7544, Lr: 0.000300\n",
      "2021-08-03 16:27:50,712 - INFO - joeynmt.training - Epoch   4, Step:   274200, Batch Loss:     1.841426, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-08-03 16:28:19,821 - INFO - joeynmt.training - Epoch   4, Step:   274300, Batch Loss:     1.869958, Tokens per Sec:     7327, Lr: 0.000300\n",
      "2021-08-03 16:28:49,181 - INFO - joeynmt.training - Epoch   4, Step:   274400, Batch Loss:     1.587655, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-08-03 16:29:18,946 - INFO - joeynmt.training - Epoch   4, Step:   274500, Batch Loss:     1.576192, Tokens per Sec:     7529, Lr: 0.000300\n",
      "2021-08-03 16:29:48,492 - INFO - joeynmt.training - Epoch   4, Step:   274600, Batch Loss:     1.871104, Tokens per Sec:     7545, Lr: 0.000300\n",
      "2021-08-03 16:30:17,924 - INFO - joeynmt.training - Epoch   4, Step:   274700, Batch Loss:     1.600300, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-08-03 16:30:47,442 - INFO - joeynmt.training - Epoch   4, Step:   274800, Batch Loss:     1.766387, Tokens per Sec:     7479, Lr: 0.000300\n",
      "2021-08-03 16:31:17,022 - INFO - joeynmt.training - Epoch   4, Step:   274900, Batch Loss:     1.875135, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-08-03 16:31:46,340 - INFO - joeynmt.training - Epoch   4, Step:   275000, Batch Loss:     1.782200, Tokens per Sec:     7352, Lr: 0.000300\n",
      "2021-08-03 16:35:15,674 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 16:35:15,675 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 16:35:15,675 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 16:35:17,187 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 16:35:17,188 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 16:35:18,135 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 16:35:18,136 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 16:35:18,136 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 16:35:18,136 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , drawing close to God , so good for me . ”\n",
      "2021-08-03 16:35:18,137 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 16:35:18,137 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 16:35:18,137 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 16:35:18,138 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are fearing Jehovah is more than many riches are standing . ”\n",
      "2021-08-03 16:35:18,138 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 16:35:18,138 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 16:35:18,139 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 16:35:18,139 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel tells us about God’s Kingdom .\n",
      "2021-08-03 16:35:18,139 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 16:35:18,140 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 16:35:18,140 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 16:35:18,140 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 16:35:18,141 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   275000: bleu:  27.48, loss: 186200.2500, ppl:   4.6549, duration: 211.8004s\n",
      "2021-08-03 16:35:47,440 - INFO - joeynmt.training - Epoch   4, Step:   275100, Batch Loss:     1.682913, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-08-03 16:36:17,055 - INFO - joeynmt.training - Epoch   4, Step:   275200, Batch Loss:     1.567392, Tokens per Sec:     7340, Lr: 0.000300\n",
      "2021-08-03 16:36:46,854 - INFO - joeynmt.training - Epoch   4, Step:   275300, Batch Loss:     1.760794, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-08-03 16:37:16,298 - INFO - joeynmt.training - Epoch   4, Step:   275400, Batch Loss:     1.805786, Tokens per Sec:     7336, Lr: 0.000300\n",
      "2021-08-03 16:37:45,427 - INFO - joeynmt.training - Epoch   4, Step:   275500, Batch Loss:     1.662037, Tokens per Sec:     7340, Lr: 0.000300\n",
      "2021-08-03 16:38:15,114 - INFO - joeynmt.training - Epoch   4, Step:   275600, Batch Loss:     1.422041, Tokens per Sec:     7485, Lr: 0.000300\n",
      "2021-08-03 16:38:44,387 - INFO - joeynmt.training - Epoch   4, Step:   275700, Batch Loss:     1.632214, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-08-03 16:39:13,750 - INFO - joeynmt.training - Epoch   4, Step:   275800, Batch Loss:     1.903679, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-08-03 16:39:43,483 - INFO - joeynmt.training - Epoch   4, Step:   275900, Batch Loss:     1.961016, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-08-03 16:40:13,123 - INFO - joeynmt.training - Epoch   4, Step:   276000, Batch Loss:     1.664681, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-08-03 16:40:42,763 - INFO - joeynmt.training - Epoch   4, Step:   276100, Batch Loss:     1.626050, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-08-03 16:41:12,170 - INFO - joeynmt.training - Epoch   4, Step:   276200, Batch Loss:     1.683435, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-08-03 16:41:42,120 - INFO - joeynmt.training - Epoch   4, Step:   276300, Batch Loss:     1.381965, Tokens per Sec:     7578, Lr: 0.000300\n",
      "2021-08-03 16:42:11,086 - INFO - joeynmt.training - Epoch   4, Step:   276400, Batch Loss:     1.914363, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-08-03 16:42:40,615 - INFO - joeynmt.training - Epoch   4, Step:   276500, Batch Loss:     1.681015, Tokens per Sec:     7450, Lr: 0.000300\n",
      "2021-08-03 16:43:10,073 - INFO - joeynmt.training - Epoch   4, Step:   276600, Batch Loss:     1.704524, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-08-03 16:43:39,494 - INFO - joeynmt.training - Epoch   4, Step:   276700, Batch Loss:     1.718560, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-08-03 16:44:09,010 - INFO - joeynmt.training - Epoch   4, Step:   276800, Batch Loss:     1.739627, Tokens per Sec:     7414, Lr: 0.000300\n",
      "2021-08-03 16:44:38,325 - INFO - joeynmt.training - Epoch   4, Step:   276900, Batch Loss:     1.745488, Tokens per Sec:     7378, Lr: 0.000300\n",
      "2021-08-03 16:45:07,865 - INFO - joeynmt.training - Epoch   4, Step:   277000, Batch Loss:     1.599636, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-08-03 16:45:37,810 - INFO - joeynmt.training - Epoch   4, Step:   277100, Batch Loss:     1.583647, Tokens per Sec:     7569, Lr: 0.000300\n",
      "2021-08-03 16:45:46,852 - INFO - joeynmt.training - Epoch   4: total training loss 9041.27\n",
      "2021-08-03 16:45:46,852 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-03 16:46:08,442 - INFO - joeynmt.training - Epoch   5, Step:   277200, Batch Loss:     1.591396, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-08-03 16:46:37,896 - INFO - joeynmt.training - Epoch   5, Step:   277300, Batch Loss:     1.679926, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-03 16:47:07,129 - INFO - joeynmt.training - Epoch   5, Step:   277400, Batch Loss:     1.617809, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-08-03 16:47:36,799 - INFO - joeynmt.training - Epoch   5, Step:   277500, Batch Loss:     1.759102, Tokens per Sec:     7394, Lr: 0.000300\n",
      "2021-08-03 16:48:06,334 - INFO - joeynmt.training - Epoch   5, Step:   277600, Batch Loss:     1.626175, Tokens per Sec:     7324, Lr: 0.000300\n",
      "2021-08-03 16:48:35,922 - INFO - joeynmt.training - Epoch   5, Step:   277700, Batch Loss:     1.620108, Tokens per Sec:     7554, Lr: 0.000300\n",
      "2021-08-03 16:49:05,324 - INFO - joeynmt.training - Epoch   5, Step:   277800, Batch Loss:     1.795377, Tokens per Sec:     7263, Lr: 0.000300\n",
      "2021-08-03 16:49:34,708 - INFO - joeynmt.training - Epoch   5, Step:   277900, Batch Loss:     1.580621, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-08-03 16:50:04,386 - INFO - joeynmt.training - Epoch   5, Step:   278000, Batch Loss:     1.686573, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-08-03 16:50:33,913 - INFO - joeynmt.training - Epoch   5, Step:   278100, Batch Loss:     1.756719, Tokens per Sec:     7490, Lr: 0.000300\n",
      "2021-08-03 16:51:03,341 - INFO - joeynmt.training - Epoch   5, Step:   278200, Batch Loss:     1.570478, Tokens per Sec:     7477, Lr: 0.000300\n",
      "2021-08-03 16:51:32,866 - INFO - joeynmt.training - Epoch   5, Step:   278300, Batch Loss:     1.668512, Tokens per Sec:     7568, Lr: 0.000300\n",
      "2021-08-03 16:52:02,464 - INFO - joeynmt.training - Epoch   5, Step:   278400, Batch Loss:     1.670676, Tokens per Sec:     7550, Lr: 0.000300\n",
      "2021-08-03 16:52:31,921 - INFO - joeynmt.training - Epoch   5, Step:   278500, Batch Loss:     1.562294, Tokens per Sec:     7325, Lr: 0.000300\n",
      "2021-08-03 16:53:00,791 - INFO - joeynmt.training - Epoch   5, Step:   278600, Batch Loss:     1.796202, Tokens per Sec:     7314, Lr: 0.000300\n",
      "2021-08-03 16:53:30,265 - INFO - joeynmt.training - Epoch   5, Step:   278700, Batch Loss:     1.596977, Tokens per Sec:     7378, Lr: 0.000300\n",
      "2021-08-03 16:54:00,264 - INFO - joeynmt.training - Epoch   5, Step:   278800, Batch Loss:     1.512031, Tokens per Sec:     7540, Lr: 0.000300\n",
      "2021-08-03 16:54:29,657 - INFO - joeynmt.training - Epoch   5, Step:   278900, Batch Loss:     1.605029, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-08-03 16:54:58,921 - INFO - joeynmt.training - Epoch   5, Step:   279000, Batch Loss:     1.585773, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-08-03 16:55:28,121 - INFO - joeynmt.training - Epoch   5, Step:   279100, Batch Loss:     1.890275, Tokens per Sec:     7418, Lr: 0.000300\n",
      "2021-08-03 16:55:57,581 - INFO - joeynmt.training - Epoch   5, Step:   279200, Batch Loss:     1.701105, Tokens per Sec:     7486, Lr: 0.000300\n",
      "2021-08-03 16:56:27,308 - INFO - joeynmt.training - Epoch   5, Step:   279300, Batch Loss:     1.635574, Tokens per Sec:     7387, Lr: 0.000300\n",
      "2021-08-03 16:56:56,621 - INFO - joeynmt.training - Epoch   5, Step:   279400, Batch Loss:     1.676114, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-08-03 16:57:25,704 - INFO - joeynmt.training - Epoch   5, Step:   279500, Batch Loss:     1.707854, Tokens per Sec:     7175, Lr: 0.000300\n",
      "2021-08-03 16:57:55,065 - INFO - joeynmt.training - Epoch   5, Step:   279600, Batch Loss:     1.627389, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 16:58:24,648 - INFO - joeynmt.training - Epoch   5, Step:   279700, Batch Loss:     1.825785, Tokens per Sec:     7521, Lr: 0.000300\n",
      "2021-08-03 16:58:54,345 - INFO - joeynmt.training - Epoch   5, Step:   279800, Batch Loss:     1.804505, Tokens per Sec:     7326, Lr: 0.000300\n",
      "2021-08-03 16:59:23,922 - INFO - joeynmt.training - Epoch   5, Step:   279900, Batch Loss:     1.794295, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-08-03 16:59:53,470 - INFO - joeynmt.training - Epoch   5, Step:   280000, Batch Loss:     1.834949, Tokens per Sec:     7408, Lr: 0.000300\n",
      "2021-08-03 17:03:24,911 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 17:03:24,912 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 17:03:24,912 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 17:03:26,433 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 17:03:26,433 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 17:03:27,219 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 17:03:27,220 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 17:03:27,221 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 17:03:27,221 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , draw close to God , so good for me . ”\n",
      "2021-08-03 17:03:27,221 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 17:03:27,222 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 17:03:27,222 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 17:03:27,222 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The residents are in fear of Jehovah , greater than many riches are standing . ”\n",
      "2021-08-03 17:03:27,222 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 17:03:27,223 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 17:03:27,223 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 17:03:27,223 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The other prophecy in the book of Daniel has a meaning on God’s Kingdom .\n",
      "2021-08-03 17:03:27,224 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 17:03:27,224 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 17:03:27,225 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 17:03:27,225 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue conquering ” and completely conquer ?\n",
      "2021-08-03 17:03:27,225 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   280000: bleu:  27.39, loss: 186035.5469, ppl:   4.6486, duration: 213.7545s\n",
      "2021-08-03 17:03:56,742 - INFO - joeynmt.training - Epoch   5, Step:   280100, Batch Loss:     1.545242, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-03 17:04:26,820 - INFO - joeynmt.training - Epoch   5, Step:   280200, Batch Loss:     1.709319, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-08-03 17:04:56,488 - INFO - joeynmt.training - Epoch   5, Step:   280300, Batch Loss:     1.603376, Tokens per Sec:     7476, Lr: 0.000300\n",
      "2021-08-03 17:05:25,888 - INFO - joeynmt.training - Epoch   5, Step:   280400, Batch Loss:     1.886643, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-08-03 17:05:55,226 - INFO - joeynmt.training - Epoch   5, Step:   280500, Batch Loss:     1.468518, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-08-03 17:06:24,759 - INFO - joeynmt.training - Epoch   5, Step:   280600, Batch Loss:     1.888782, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-08-03 17:06:54,206 - INFO - joeynmt.training - Epoch   5, Step:   280700, Batch Loss:     1.700227, Tokens per Sec:     7304, Lr: 0.000300\n",
      "2021-08-03 17:07:24,141 - INFO - joeynmt.training - Epoch   5, Step:   280800, Batch Loss:     1.651007, Tokens per Sec:     7648, Lr: 0.000300\n",
      "2021-08-03 17:07:53,891 - INFO - joeynmt.training - Epoch   5, Step:   280900, Batch Loss:     1.591761, Tokens per Sec:     7635, Lr: 0.000300\n",
      "2021-08-03 17:08:23,342 - INFO - joeynmt.training - Epoch   5, Step:   281000, Batch Loss:     1.791029, Tokens per Sec:     7408, Lr: 0.000300\n",
      "2021-08-03 17:08:52,666 - INFO - joeynmt.training - Epoch   5, Step:   281100, Batch Loss:     1.673225, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-08-03 17:09:22,124 - INFO - joeynmt.training - Epoch   5, Step:   281200, Batch Loss:     1.681782, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-08-03 17:09:51,347 - INFO - joeynmt.training - Epoch   5, Step:   281300, Batch Loss:     1.890762, Tokens per Sec:     7458, Lr: 0.000300\n",
      "2021-08-03 17:10:20,524 - INFO - joeynmt.training - Epoch   5, Step:   281400, Batch Loss:     1.838818, Tokens per Sec:     7327, Lr: 0.000300\n",
      "2021-08-03 17:10:50,165 - INFO - joeynmt.training - Epoch   5, Step:   281500, Batch Loss:     1.784276, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-08-03 17:11:20,091 - INFO - joeynmt.training - Epoch   5, Step:   281600, Batch Loss:     1.854712, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-08-03 17:11:49,850 - INFO - joeynmt.training - Epoch   5, Step:   281700, Batch Loss:     1.722079, Tokens per Sec:     7649, Lr: 0.000300\n",
      "2021-08-03 17:12:19,273 - INFO - joeynmt.training - Epoch   5, Step:   281800, Batch Loss:     1.841088, Tokens per Sec:     7399, Lr: 0.000300\n",
      "2021-08-03 17:12:48,959 - INFO - joeynmt.training - Epoch   5, Step:   281900, Batch Loss:     1.520779, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-03 17:13:17,977 - INFO - joeynmt.training - Epoch   5, Step:   282000, Batch Loss:     1.611735, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-08-03 17:13:47,526 - INFO - joeynmt.training - Epoch   5, Step:   282100, Batch Loss:     1.706330, Tokens per Sec:     7447, Lr: 0.000300\n",
      "2021-08-03 17:14:16,974 - INFO - joeynmt.training - Epoch   5, Step:   282200, Batch Loss:     1.739923, Tokens per Sec:     7517, Lr: 0.000300\n",
      "2021-08-03 17:14:46,190 - INFO - joeynmt.training - Epoch   5, Step:   282300, Batch Loss:     1.718333, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-08-03 17:15:15,949 - INFO - joeynmt.training - Epoch   5, Step:   282400, Batch Loss:     1.787925, Tokens per Sec:     7585, Lr: 0.000300\n",
      "2021-08-03 17:15:33,573 - INFO - joeynmt.training - Epoch   5: total training loss 9019.71\n",
      "2021-08-03 17:15:33,573 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-03 17:15:46,210 - INFO - joeynmt.training - Epoch   6, Step:   282500, Batch Loss:     1.713863, Tokens per Sec:     6849, Lr: 0.000300\n",
      "2021-08-03 17:16:15,729 - INFO - joeynmt.training - Epoch   6, Step:   282600, Batch Loss:     1.652472, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-08-03 17:16:45,301 - INFO - joeynmt.training - Epoch   6, Step:   282700, Batch Loss:     1.603545, Tokens per Sec:     7553, Lr: 0.000300\n",
      "2021-08-03 17:17:14,917 - INFO - joeynmt.training - Epoch   6, Step:   282800, Batch Loss:     1.599258, Tokens per Sec:     7541, Lr: 0.000300\n",
      "2021-08-03 17:17:44,664 - INFO - joeynmt.training - Epoch   6, Step:   282900, Batch Loss:     1.714401, Tokens per Sec:     7385, Lr: 0.000300\n",
      "2021-08-03 17:18:14,406 - INFO - joeynmt.training - Epoch   6, Step:   283000, Batch Loss:     1.629794, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-08-03 17:18:43,386 - INFO - joeynmt.training - Epoch   6, Step:   283100, Batch Loss:     1.690362, Tokens per Sec:     7354, Lr: 0.000300\n",
      "2021-08-03 17:19:12,620 - INFO - joeynmt.training - Epoch   6, Step:   283200, Batch Loss:     1.846230, Tokens per Sec:     7260, Lr: 0.000300\n",
      "2021-08-03 17:19:42,079 - INFO - joeynmt.training - Epoch   6, Step:   283300, Batch Loss:     1.831668, Tokens per Sec:     7477, Lr: 0.000300\n",
      "2021-08-03 17:20:11,135 - INFO - joeynmt.training - Epoch   6, Step:   283400, Batch Loss:     1.444632, Tokens per Sec:     7399, Lr: 0.000300\n",
      "2021-08-03 17:20:40,615 - INFO - joeynmt.training - Epoch   6, Step:   283500, Batch Loss:     1.349881, Tokens per Sec:     7392, Lr: 0.000300\n",
      "2021-08-03 17:21:10,176 - INFO - joeynmt.training - Epoch   6, Step:   283600, Batch Loss:     1.723234, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-08-03 17:21:39,739 - INFO - joeynmt.training - Epoch   6, Step:   283700, Batch Loss:     1.620790, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-08-03 17:22:09,477 - INFO - joeynmt.training - Epoch   6, Step:   283800, Batch Loss:     1.806265, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-08-03 17:22:38,774 - INFO - joeynmt.training - Epoch   6, Step:   283900, Batch Loss:     1.714543, Tokens per Sec:     7365, Lr: 0.000300\n",
      "2021-08-03 17:23:08,057 - INFO - joeynmt.training - Epoch   6, Step:   284000, Batch Loss:     1.609233, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-08-03 17:23:37,426 - INFO - joeynmt.training - Epoch   6, Step:   284100, Batch Loss:     1.754920, Tokens per Sec:     7290, Lr: 0.000300\n",
      "2021-08-03 17:24:07,056 - INFO - joeynmt.training - Epoch   6, Step:   284200, Batch Loss:     1.904477, Tokens per Sec:     7497, Lr: 0.000300\n",
      "2021-08-03 17:24:36,247 - INFO - joeynmt.training - Epoch   6, Step:   284300, Batch Loss:     1.829623, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-08-03 17:25:05,637 - INFO - joeynmt.training - Epoch   6, Step:   284400, Batch Loss:     1.507459, Tokens per Sec:     7465, Lr: 0.000300\n",
      "2021-08-03 17:25:35,195 - INFO - joeynmt.training - Epoch   6, Step:   284500, Batch Loss:     1.702481, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-08-03 17:26:04,538 - INFO - joeynmt.training - Epoch   6, Step:   284600, Batch Loss:     1.580270, Tokens per Sec:     7471, Lr: 0.000300\n",
      "2021-08-03 17:26:34,118 - INFO - joeynmt.training - Epoch   6, Step:   284700, Batch Loss:     1.792695, Tokens per Sec:     7476, Lr: 0.000300\n",
      "2021-08-03 17:27:03,669 - INFO - joeynmt.training - Epoch   6, Step:   284800, Batch Loss:     1.644935, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-03 17:27:33,219 - INFO - joeynmt.training - Epoch   6, Step:   284900, Batch Loss:     1.780550, Tokens per Sec:     7582, Lr: 0.000300\n",
      "2021-08-03 17:28:02,242 - INFO - joeynmt.training - Epoch   6, Step:   285000, Batch Loss:     1.770401, Tokens per Sec:     7301, Lr: 0.000300\n",
      "2021-08-03 17:31:31,295 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 17:31:31,295 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 17:31:31,295 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 17:31:33,766 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 17:31:33,767 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 17:31:33,767 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 17:31:33,767 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , drawing close to God , so is good for me . ”\n",
      "2021-08-03 17:31:33,767 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 17:31:33,768 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 17:31:33,768 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 17:31:33,769 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The resident that we are fearing Jehovah is greater than many riches are standing . ”\n",
      "2021-08-03 17:31:33,769 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 17:31:33,770 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 17:31:33,770 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 17:31:33,770 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has a detailed point on God’s Kingdom .\n",
      "2021-08-03 17:31:33,771 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 17:31:33,773 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 17:31:33,775 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 17:31:33,776 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 17:31:33,776 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   285000: bleu:  27.76, loss: 186202.7188, ppl:   4.6550, duration: 211.5339s\n",
      "2021-08-03 17:32:03,975 - INFO - joeynmt.training - Epoch   6, Step:   285100, Batch Loss:     1.831942, Tokens per Sec:     7188, Lr: 0.000300\n",
      "2021-08-03 17:32:33,469 - INFO - joeynmt.training - Epoch   6, Step:   285200, Batch Loss:     1.558597, Tokens per Sec:     7548, Lr: 0.000300\n",
      "2021-08-03 17:33:02,761 - INFO - joeynmt.training - Epoch   6, Step:   285300, Batch Loss:     1.445597, Tokens per Sec:     7295, Lr: 0.000300\n",
      "2021-08-03 17:33:32,225 - INFO - joeynmt.training - Epoch   6, Step:   285400, Batch Loss:     1.789983, Tokens per Sec:     7535, Lr: 0.000300\n",
      "2021-08-03 17:34:01,879 - INFO - joeynmt.training - Epoch   6, Step:   285500, Batch Loss:     1.784414, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-08-03 17:34:31,534 - INFO - joeynmt.training - Epoch   6, Step:   285600, Batch Loss:     1.675250, Tokens per Sec:     7580, Lr: 0.000300\n",
      "2021-08-03 17:35:00,943 - INFO - joeynmt.training - Epoch   6, Step:   285700, Batch Loss:     1.766476, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 17:35:30,717 - INFO - joeynmt.training - Epoch   6, Step:   285800, Batch Loss:     1.903565, Tokens per Sec:     7488, Lr: 0.000300\n",
      "2021-08-03 17:35:59,948 - INFO - joeynmt.training - Epoch   6, Step:   285900, Batch Loss:     1.832197, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-08-03 17:36:29,752 - INFO - joeynmt.training - Epoch   6, Step:   286000, Batch Loss:     1.575611, Tokens per Sec:     7481, Lr: 0.000300\n",
      "2021-08-03 17:36:58,897 - INFO - joeynmt.training - Epoch   6, Step:   286100, Batch Loss:     1.848016, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-08-03 17:37:28,447 - INFO - joeynmt.training - Epoch   6, Step:   286200, Batch Loss:     1.887881, Tokens per Sec:     7459, Lr: 0.000300\n",
      "2021-08-03 17:37:57,871 - INFO - joeynmt.training - Epoch   6, Step:   286300, Batch Loss:     1.700365, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-03 17:38:27,526 - INFO - joeynmt.training - Epoch   6, Step:   286400, Batch Loss:     1.640253, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-03 17:38:57,154 - INFO - joeynmt.training - Epoch   6, Step:   286500, Batch Loss:     1.669991, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-08-03 17:39:26,684 - INFO - joeynmt.training - Epoch   6, Step:   286600, Batch Loss:     1.714645, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-08-03 17:39:56,088 - INFO - joeynmt.training - Epoch   6, Step:   286700, Batch Loss:     1.776938, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-08-03 17:40:25,353 - INFO - joeynmt.training - Epoch   6, Step:   286800, Batch Loss:     1.649163, Tokens per Sec:     7361, Lr: 0.000300\n",
      "2021-08-03 17:40:55,092 - INFO - joeynmt.training - Epoch   6, Step:   286900, Batch Loss:     1.802109, Tokens per Sec:     7616, Lr: 0.000300\n",
      "2021-08-03 17:41:24,792 - INFO - joeynmt.training - Epoch   6, Step:   287000, Batch Loss:     1.674556, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-08-03 17:41:54,124 - INFO - joeynmt.training - Epoch   6, Step:   287100, Batch Loss:     1.898438, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-08-03 17:42:23,692 - INFO - joeynmt.training - Epoch   6, Step:   287200, Batch Loss:     1.475523, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-08-03 17:42:52,789 - INFO - joeynmt.training - Epoch   6, Step:   287300, Batch Loss:     1.654595, Tokens per Sec:     7329, Lr: 0.000300\n",
      "2021-08-03 17:43:22,397 - INFO - joeynmt.training - Epoch   6, Step:   287400, Batch Loss:     1.710397, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-08-03 17:43:51,913 - INFO - joeynmt.training - Epoch   6, Step:   287500, Batch Loss:     1.490307, Tokens per Sec:     7448, Lr: 0.000300\n",
      "2021-08-03 17:44:21,524 - INFO - joeynmt.training - Epoch   6, Step:   287600, Batch Loss:     1.725045, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-08-03 17:44:51,295 - INFO - joeynmt.training - Epoch   6, Step:   287700, Batch Loss:     1.739994, Tokens per Sec:     7614, Lr: 0.000300\n",
      "2021-08-03 17:45:18,808 - INFO - joeynmt.training - Epoch   6: total training loss 9019.23\n",
      "2021-08-03 17:45:18,808 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-03 17:45:21,677 - INFO - joeynmt.training - Epoch   7, Step:   287800, Batch Loss:     1.766620, Tokens per Sec:     5768, Lr: 0.000300\n",
      "2021-08-03 17:45:51,274 - INFO - joeynmt.training - Epoch   7, Step:   287900, Batch Loss:     1.696995, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-08-03 17:46:20,884 - INFO - joeynmt.training - Epoch   7, Step:   288000, Batch Loss:     1.757664, Tokens per Sec:     7338, Lr: 0.000300\n",
      "2021-08-03 17:46:50,620 - INFO - joeynmt.training - Epoch   7, Step:   288100, Batch Loss:     1.772571, Tokens per Sec:     7272, Lr: 0.000300\n",
      "2021-08-03 17:47:20,517 - INFO - joeynmt.training - Epoch   7, Step:   288200, Batch Loss:     1.845101, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-08-03 17:47:49,934 - INFO - joeynmt.training - Epoch   7, Step:   288300, Batch Loss:     1.513820, Tokens per Sec:     7331, Lr: 0.000300\n",
      "2021-08-03 17:48:19,964 - INFO - joeynmt.training - Epoch   7, Step:   288400, Batch Loss:     1.573023, Tokens per Sec:     7609, Lr: 0.000300\n",
      "2021-08-03 17:48:49,305 - INFO - joeynmt.training - Epoch   7, Step:   288500, Batch Loss:     1.561190, Tokens per Sec:     7430, Lr: 0.000300\n",
      "2021-08-03 17:49:18,681 - INFO - joeynmt.training - Epoch   7, Step:   288600, Batch Loss:     1.573063, Tokens per Sec:     7469, Lr: 0.000300\n",
      "2021-08-03 17:49:48,405 - INFO - joeynmt.training - Epoch   7, Step:   288700, Batch Loss:     1.748713, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-08-03 17:50:18,184 - INFO - joeynmt.training - Epoch   7, Step:   288800, Batch Loss:     1.554580, Tokens per Sec:     7532, Lr: 0.000300\n",
      "2021-08-03 17:50:47,375 - INFO - joeynmt.training - Epoch   7, Step:   288900, Batch Loss:     1.995352, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-08-03 17:51:17,168 - INFO - joeynmt.training - Epoch   7, Step:   289000, Batch Loss:     1.791641, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-08-03 17:51:46,858 - INFO - joeynmt.training - Epoch   7, Step:   289100, Batch Loss:     1.805074, Tokens per Sec:     7487, Lr: 0.000300\n",
      "2021-08-03 17:52:16,636 - INFO - joeynmt.training - Epoch   7, Step:   289200, Batch Loss:     1.652008, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-08-03 17:52:46,084 - INFO - joeynmt.training - Epoch   7, Step:   289300, Batch Loss:     1.549890, Tokens per Sec:     7361, Lr: 0.000300\n",
      "2021-08-03 17:53:15,840 - INFO - joeynmt.training - Epoch   7, Step:   289400, Batch Loss:     1.650830, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-08-03 17:53:45,496 - INFO - joeynmt.training - Epoch   7, Step:   289500, Batch Loss:     1.652708, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-08-03 17:54:15,162 - INFO - joeynmt.training - Epoch   7, Step:   289600, Batch Loss:     1.511646, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-08-03 17:54:44,642 - INFO - joeynmt.training - Epoch   7, Step:   289700, Batch Loss:     1.591080, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-08-03 17:55:14,244 - INFO - joeynmt.training - Epoch   7, Step:   289800, Batch Loss:     1.631917, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-08-03 17:55:43,299 - INFO - joeynmt.training - Epoch   7, Step:   289900, Batch Loss:     1.490682, Tokens per Sec:     7301, Lr: 0.000300\n",
      "2021-08-03 17:56:12,851 - INFO - joeynmt.training - Epoch   7, Step:   290000, Batch Loss:     1.647938, Tokens per Sec:     7364, Lr: 0.000300\n",
      "2021-08-03 17:59:47,020 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 17:59:47,021 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 17:59:47,021 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 17:59:48,552 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 17:59:48,552 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 17:59:49,512 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 17:59:49,512 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 17:59:49,513 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 17:59:49,513 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am drawing close to God , so good for me . ”\n",
      "2021-08-03 17:59:49,513 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 17:59:49,514 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 17:59:49,514 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 17:59:49,514 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The dwelling of the earth is to fear Jehovah , greater than many riches are standing . ”\n",
      "2021-08-03 17:59:49,515 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 17:59:49,515 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 17:59:49,515 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 17:59:49,516 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The book of Daniel has a detail on God’s Kingdom .\n",
      "2021-08-03 17:59:49,516 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 17:59:49,517 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 17:59:49,517 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 17:59:49,517 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue conquering ” and completely conquer ?\n",
      "2021-08-03 17:59:49,517 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   290000: bleu:  27.74, loss: 185566.2188, ppl:   4.6306, duration: 216.6661s\n",
      "2021-08-03 18:00:19,246 - INFO - joeynmt.training - Epoch   7, Step:   290100, Batch Loss:     1.807703, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-08-03 18:00:48,827 - INFO - joeynmt.training - Epoch   7, Step:   290200, Batch Loss:     1.560460, Tokens per Sec:     7397, Lr: 0.000300\n",
      "2021-08-03 18:01:18,701 - INFO - joeynmt.training - Epoch   7, Step:   290300, Batch Loss:     1.576181, Tokens per Sec:     7531, Lr: 0.000300\n",
      "2021-08-03 18:01:48,295 - INFO - joeynmt.training - Epoch   7, Step:   290400, Batch Loss:     1.778439, Tokens per Sec:     7420, Lr: 0.000300\n",
      "2021-08-03 18:02:17,753 - INFO - joeynmt.training - Epoch   7, Step:   290500, Batch Loss:     1.741848, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-03 18:02:47,022 - INFO - joeynmt.training - Epoch   7, Step:   290600, Batch Loss:     2.035940, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-08-03 18:03:16,545 - INFO - joeynmt.training - Epoch   7, Step:   290700, Batch Loss:     1.794311, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-08-03 18:03:46,115 - INFO - joeynmt.training - Epoch   7, Step:   290800, Batch Loss:     1.460523, Tokens per Sec:     7618, Lr: 0.000300\n",
      "2021-08-03 18:04:15,724 - INFO - joeynmt.training - Epoch   7, Step:   290900, Batch Loss:     1.841126, Tokens per Sec:     7363, Lr: 0.000300\n",
      "2021-08-03 18:04:45,133 - INFO - joeynmt.training - Epoch   7, Step:   291000, Batch Loss:     1.699840, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-08-03 18:05:14,604 - INFO - joeynmt.training - Epoch   7, Step:   291100, Batch Loss:     1.815575, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-08-03 18:05:44,162 - INFO - joeynmt.training - Epoch   7, Step:   291200, Batch Loss:     1.843639, Tokens per Sec:     7506, Lr: 0.000300\n",
      "2021-08-03 18:06:13,459 - INFO - joeynmt.training - Epoch   7, Step:   291300, Batch Loss:     1.736304, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-03 18:06:43,113 - INFO - joeynmt.training - Epoch   7, Step:   291400, Batch Loss:     1.698710, Tokens per Sec:     7584, Lr: 0.000300\n",
      "2021-08-03 18:07:12,266 - INFO - joeynmt.training - Epoch   7, Step:   291500, Batch Loss:     1.681706, Tokens per Sec:     7268, Lr: 0.000300\n",
      "2021-08-03 18:07:41,277 - INFO - joeynmt.training - Epoch   7, Step:   291600, Batch Loss:     1.766310, Tokens per Sec:     7301, Lr: 0.000300\n",
      "2021-08-03 18:08:10,488 - INFO - joeynmt.training - Epoch   7, Step:   291700, Batch Loss:     1.658809, Tokens per Sec:     7369, Lr: 0.000300\n",
      "2021-08-03 18:08:39,930 - INFO - joeynmt.training - Epoch   7, Step:   291800, Batch Loss:     1.719395, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-08-03 18:09:09,302 - INFO - joeynmt.training - Epoch   7, Step:   291900, Batch Loss:     1.842863, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-08-03 18:09:38,564 - INFO - joeynmt.training - Epoch   7, Step:   292000, Batch Loss:     1.578718, Tokens per Sec:     7374, Lr: 0.000300\n",
      "2021-08-03 18:10:07,938 - INFO - joeynmt.training - Epoch   7, Step:   292100, Batch Loss:     1.665679, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-08-03 18:10:37,388 - INFO - joeynmt.training - Epoch   7, Step:   292200, Batch Loss:     1.695545, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-08-03 18:11:06,779 - INFO - joeynmt.training - Epoch   7, Step:   292300, Batch Loss:     1.661783, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-08-03 18:11:36,052 - INFO - joeynmt.training - Epoch   7, Step:   292400, Batch Loss:     1.718693, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-08-03 18:12:05,540 - INFO - joeynmt.training - Epoch   7, Step:   292500, Batch Loss:     1.802703, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-08-03 18:12:34,857 - INFO - joeynmt.training - Epoch   7, Step:   292600, Batch Loss:     2.015547, Tokens per Sec:     7515, Lr: 0.000300\n",
      "2021-08-03 18:13:04,426 - INFO - joeynmt.training - Epoch   7, Step:   292700, Batch Loss:     1.735694, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-08-03 18:13:33,644 - INFO - joeynmt.training - Epoch   7, Step:   292800, Batch Loss:     1.952694, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-08-03 18:14:02,981 - INFO - joeynmt.training - Epoch   7, Step:   292900, Batch Loss:     1.639654, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-08-03 18:14:32,611 - INFO - joeynmt.training - Epoch   7, Step:   293000, Batch Loss:     1.659712, Tokens per Sec:     7581, Lr: 0.000300\n",
      "2021-08-03 18:15:02,257 - INFO - joeynmt.training - Epoch   7, Step:   293100, Batch Loss:     1.832865, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-08-03 18:15:08,646 - INFO - joeynmt.training - Epoch   7: total training loss 8995.13\n",
      "2021-08-03 18:15:08,646 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-03 18:15:32,045 - INFO - joeynmt.training - Epoch   8, Step:   293200, Batch Loss:     1.676566, Tokens per Sec:     7064, Lr: 0.000300\n",
      "2021-08-03 18:16:01,044 - INFO - joeynmt.training - Epoch   8, Step:   293300, Batch Loss:     1.746205, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-08-03 18:16:30,238 - INFO - joeynmt.training - Epoch   8, Step:   293400, Batch Loss:     1.526678, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-08-03 18:16:59,605 - INFO - joeynmt.training - Epoch   8, Step:   293500, Batch Loss:     1.575613, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-08-03 18:17:28,935 - INFO - joeynmt.training - Epoch   8, Step:   293600, Batch Loss:     1.865599, Tokens per Sec:     7464, Lr: 0.000300\n",
      "2021-08-03 18:17:58,261 - INFO - joeynmt.training - Epoch   8, Step:   293700, Batch Loss:     1.746565, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-08-03 18:18:27,893 - INFO - joeynmt.training - Epoch   8, Step:   293800, Batch Loss:     1.546173, Tokens per Sec:     7412, Lr: 0.000300\n",
      "2021-08-03 18:18:57,666 - INFO - joeynmt.training - Epoch   8, Step:   293900, Batch Loss:     1.668637, Tokens per Sec:     7596, Lr: 0.000300\n",
      "2021-08-03 18:19:27,220 - INFO - joeynmt.training - Epoch   8, Step:   294000, Batch Loss:     1.647633, Tokens per Sec:     7501, Lr: 0.000300\n",
      "2021-08-03 18:19:56,999 - INFO - joeynmt.training - Epoch   8, Step:   294100, Batch Loss:     1.840571, Tokens per Sec:     7536, Lr: 0.000300\n",
      "2021-08-03 18:20:26,502 - INFO - joeynmt.training - Epoch   8, Step:   294200, Batch Loss:     1.577417, Tokens per Sec:     7524, Lr: 0.000300\n",
      "2021-08-03 18:20:56,047 - INFO - joeynmt.training - Epoch   8, Step:   294300, Batch Loss:     1.707314, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-08-03 18:21:25,760 - INFO - joeynmt.training - Epoch   8, Step:   294400, Batch Loss:     1.675329, Tokens per Sec:     7470, Lr: 0.000300\n",
      "2021-08-03 18:21:55,111 - INFO - joeynmt.training - Epoch   8, Step:   294500, Batch Loss:     1.454950, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-08-03 18:22:24,786 - INFO - joeynmt.training - Epoch   8, Step:   294600, Batch Loss:     1.645701, Tokens per Sec:     7580, Lr: 0.000300\n",
      "2021-08-03 18:22:54,447 - INFO - joeynmt.training - Epoch   8, Step:   294700, Batch Loss:     1.654126, Tokens per Sec:     7546, Lr: 0.000300\n",
      "2021-08-03 18:23:23,881 - INFO - joeynmt.training - Epoch   8, Step:   294800, Batch Loss:     1.775523, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-08-03 18:23:53,680 - INFO - joeynmt.training - Epoch   8, Step:   294900, Batch Loss:     1.734761, Tokens per Sec:     7486, Lr: 0.000300\n",
      "2021-08-03 18:24:23,284 - INFO - joeynmt.training - Epoch   8, Step:   295000, Batch Loss:     1.801131, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-08-03 18:27:54,546 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 18:27:54,546 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 18:27:54,546 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 18:27:56,074 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 18:27:56,075 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 18:27:57,018 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 18:27:57,019 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 18:27:57,019 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 18:27:57,019 - INFO - joeynmt.training - \tHypothesis: He sang : “ But as for me , drawing close to God , so good for me . ”\n",
      "2021-08-03 18:27:57,019 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 18:27:57,020 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 18:27:57,020 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 18:27:57,020 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The dwelling of the fear of Jehovah is greater than many riches that are standing . ”\n",
      "2021-08-03 18:27:57,021 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 18:27:57,021 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 18:27:57,022 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 18:27:57,022 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel has a detail about God’s Kingdom .\n",
      "2021-08-03 18:27:57,022 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 18:27:57,023 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 18:27:57,023 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 18:27:57,023 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 18:27:57,023 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   295000: bleu:  27.67, loss: 185271.3594, ppl:   4.6194, duration: 213.7389s\n",
      "2021-08-03 18:28:26,474 - INFO - joeynmt.training - Epoch   8, Step:   295100, Batch Loss:     1.674638, Tokens per Sec:     7311, Lr: 0.000300\n",
      "2021-08-03 18:28:55,975 - INFO - joeynmt.training - Epoch   8, Step:   295200, Batch Loss:     1.650426, Tokens per Sec:     7489, Lr: 0.000300\n",
      "2021-08-03 18:29:25,545 - INFO - joeynmt.training - Epoch   8, Step:   295300, Batch Loss:     1.644859, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-08-03 18:29:54,934 - INFO - joeynmt.training - Epoch   8, Step:   295400, Batch Loss:     1.609877, Tokens per Sec:     7433, Lr: 0.000300\n",
      "2021-08-03 18:30:24,339 - INFO - joeynmt.training - Epoch   8, Step:   295500, Batch Loss:     1.683714, Tokens per Sec:     7306, Lr: 0.000300\n",
      "2021-08-03 18:30:54,033 - INFO - joeynmt.training - Epoch   8, Step:   295600, Batch Loss:     1.756736, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-03 18:31:23,192 - INFO - joeynmt.training - Epoch   8, Step:   295700, Batch Loss:     1.638571, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 18:31:53,060 - INFO - joeynmt.training - Epoch   8, Step:   295800, Batch Loss:     1.513260, Tokens per Sec:     7404, Lr: 0.000300\n",
      "2021-08-03 18:32:22,736 - INFO - joeynmt.training - Epoch   8, Step:   295900, Batch Loss:     1.535805, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-08-03 18:32:52,225 - INFO - joeynmt.training - Epoch   8, Step:   296000, Batch Loss:     1.839841, Tokens per Sec:     7509, Lr: 0.000300\n",
      "2021-08-03 18:33:21,590 - INFO - joeynmt.training - Epoch   8, Step:   296100, Batch Loss:     1.770688, Tokens per Sec:     7206, Lr: 0.000300\n",
      "2021-08-03 18:33:51,413 - INFO - joeynmt.training - Epoch   8, Step:   296200, Batch Loss:     1.789000, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-08-03 18:34:21,110 - INFO - joeynmt.training - Epoch   8, Step:   296300, Batch Loss:     2.161913, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-08-03 18:34:50,421 - INFO - joeynmt.training - Epoch   8, Step:   296400, Batch Loss:     1.452422, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-08-03 18:35:20,229 - INFO - joeynmt.training - Epoch   8, Step:   296500, Batch Loss:     1.566479, Tokens per Sec:     7526, Lr: 0.000300\n",
      "2021-08-03 18:35:49,573 - INFO - joeynmt.training - Epoch   8, Step:   296600, Batch Loss:     1.608341, Tokens per Sec:     7336, Lr: 0.000300\n",
      "2021-08-03 18:36:19,165 - INFO - joeynmt.training - Epoch   8, Step:   296700, Batch Loss:     1.602398, Tokens per Sec:     7446, Lr: 0.000300\n",
      "2021-08-03 18:36:49,542 - INFO - joeynmt.training - Epoch   8, Step:   296800, Batch Loss:     1.746918, Tokens per Sec:     7630, Lr: 0.000300\n",
      "2021-08-03 18:37:18,888 - INFO - joeynmt.training - Epoch   8, Step:   296900, Batch Loss:     1.673429, Tokens per Sec:     7418, Lr: 0.000300\n",
      "2021-08-03 18:37:48,328 - INFO - joeynmt.training - Epoch   8, Step:   297000, Batch Loss:     1.593694, Tokens per Sec:     7490, Lr: 0.000300\n",
      "2021-08-03 18:38:17,984 - INFO - joeynmt.training - Epoch   8, Step:   297100, Batch Loss:     1.743860, Tokens per Sec:     7586, Lr: 0.000300\n",
      "2021-08-03 18:38:47,366 - INFO - joeynmt.training - Epoch   8, Step:   297200, Batch Loss:     1.680065, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-08-03 18:39:17,101 - INFO - joeynmt.training - Epoch   8, Step:   297300, Batch Loss:     1.636553, Tokens per Sec:     7488, Lr: 0.000300\n",
      "2021-08-03 18:39:46,635 - INFO - joeynmt.training - Epoch   8, Step:   297400, Batch Loss:     2.067863, Tokens per Sec:     7586, Lr: 0.000300\n",
      "2021-08-03 18:40:15,925 - INFO - joeynmt.training - Epoch   8, Step:   297500, Batch Loss:     1.749218, Tokens per Sec:     7305, Lr: 0.000300\n",
      "2021-08-03 18:40:45,504 - INFO - joeynmt.training - Epoch   8, Step:   297600, Batch Loss:     1.822387, Tokens per Sec:     7487, Lr: 0.000300\n",
      "2021-08-03 18:41:14,882 - INFO - joeynmt.training - Epoch   8, Step:   297700, Batch Loss:     1.542453, Tokens per Sec:     7430, Lr: 0.000300\n",
      "2021-08-03 18:41:43,971 - INFO - joeynmt.training - Epoch   8, Step:   297800, Batch Loss:     1.561062, Tokens per Sec:     7321, Lr: 0.000300\n",
      "2021-08-03 18:42:13,021 - INFO - joeynmt.training - Epoch   8, Step:   297900, Batch Loss:     1.768747, Tokens per Sec:     7402, Lr: 0.000300\n",
      "2021-08-03 18:42:42,720 - INFO - joeynmt.training - Epoch   8, Step:   298000, Batch Loss:     1.740003, Tokens per Sec:     7569, Lr: 0.000300\n",
      "2021-08-03 18:43:12,036 - INFO - joeynmt.training - Epoch   8, Step:   298100, Batch Loss:     1.815120, Tokens per Sec:     7335, Lr: 0.000300\n",
      "2021-08-03 18:43:41,173 - INFO - joeynmt.training - Epoch   8, Step:   298200, Batch Loss:     1.787132, Tokens per Sec:     7523, Lr: 0.000300\n",
      "2021-08-03 18:44:10,644 - INFO - joeynmt.training - Epoch   8, Step:   298300, Batch Loss:     1.523945, Tokens per Sec:     7414, Lr: 0.000300\n",
      "2021-08-03 18:44:40,195 - INFO - joeynmt.training - Epoch   8, Step:   298400, Batch Loss:     1.615423, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-08-03 18:44:55,168 - INFO - joeynmt.training - Epoch   8: total training loss 8979.51\n",
      "2021-08-03 18:44:55,169 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-03 18:45:10,185 - INFO - joeynmt.training - Epoch   9, Step:   298500, Batch Loss:     1.472866, Tokens per Sec:     7045, Lr: 0.000300\n",
      "2021-08-03 18:45:39,862 - INFO - joeynmt.training - Epoch   9, Step:   298600, Batch Loss:     1.865122, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-08-03 18:46:09,328 - INFO - joeynmt.training - Epoch   9, Step:   298700, Batch Loss:     1.697512, Tokens per Sec:     7474, Lr: 0.000300\n",
      "2021-08-03 18:46:38,739 - INFO - joeynmt.training - Epoch   9, Step:   298800, Batch Loss:     1.691672, Tokens per Sec:     7459, Lr: 0.000300\n",
      "2021-08-03 18:47:08,272 - INFO - joeynmt.training - Epoch   9, Step:   298900, Batch Loss:     1.609475, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-08-03 18:47:37,445 - INFO - joeynmt.training - Epoch   9, Step:   299000, Batch Loss:     1.916550, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-08-03 18:48:06,645 - INFO - joeynmt.training - Epoch   9, Step:   299100, Batch Loss:     1.801230, Tokens per Sec:     7326, Lr: 0.000300\n",
      "2021-08-03 18:48:35,824 - INFO - joeynmt.training - Epoch   9, Step:   299200, Batch Loss:     1.688799, Tokens per Sec:     7533, Lr: 0.000300\n",
      "2021-08-03 18:49:06,060 - INFO - joeynmt.training - Epoch   9, Step:   299300, Batch Loss:     1.669609, Tokens per Sec:     7690, Lr: 0.000300\n",
      "2021-08-03 18:49:35,527 - INFO - joeynmt.training - Epoch   9, Step:   299400, Batch Loss:     1.532491, Tokens per Sec:     7530, Lr: 0.000300\n",
      "2021-08-03 18:50:05,071 - INFO - joeynmt.training - Epoch   9, Step:   299500, Batch Loss:     1.540294, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-08-03 18:50:34,709 - INFO - joeynmt.training - Epoch   9, Step:   299600, Batch Loss:     1.654845, Tokens per Sec:     7536, Lr: 0.000300\n",
      "2021-08-03 18:51:03,796 - INFO - joeynmt.training - Epoch   9, Step:   299700, Batch Loss:     1.600522, Tokens per Sec:     7404, Lr: 0.000300\n",
      "2021-08-03 18:51:33,426 - INFO - joeynmt.training - Epoch   9, Step:   299800, Batch Loss:     1.656525, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-08-03 18:52:03,104 - INFO - joeynmt.training - Epoch   9, Step:   299900, Batch Loss:     1.801242, Tokens per Sec:     7548, Lr: 0.000300\n",
      "2021-08-03 18:52:32,730 - INFO - joeynmt.training - Epoch   9, Step:   300000, Batch Loss:     1.747504, Tokens per Sec:     7420, Lr: 0.000300\n",
      "2021-08-03 18:56:04,651 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 18:56:04,652 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 18:56:04,652 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 18:56:06,178 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 18:56:06,178 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 18:56:07,408 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 18:56:07,410 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 18:56:07,410 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 18:56:07,411 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , draw close to God , so good for me . ”\n",
      "2021-08-03 18:56:07,411 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 18:56:07,412 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 18:56:07,412 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 18:56:07,412 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The dwelling of the fear of Jehovah is greater than many riches that are standing . ”\n",
      "2021-08-03 18:56:07,412 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 18:56:07,413 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 18:56:07,413 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 18:56:07,413 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The other prophecy in the book of Daniel has a detailed meaning on God’s Kingdom .\n",
      "2021-08-03 18:56:07,414 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 18:56:07,414 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 18:56:07,415 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 18:56:07,415 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue conquering ” and completely conquer ?\n",
      "2021-08-03 18:56:07,415 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   300000: bleu:  27.81, loss: 184974.0156, ppl:   4.6080, duration: 214.6843s\n",
      "2021-08-03 18:56:36,736 - INFO - joeynmt.training - Epoch   9, Step:   300100, Batch Loss:     1.701424, Tokens per Sec:     7332, Lr: 0.000300\n",
      "2021-08-03 18:57:06,194 - INFO - joeynmt.training - Epoch   9, Step:   300200, Batch Loss:     1.462619, Tokens per Sec:     7374, Lr: 0.000300\n",
      "2021-08-03 18:57:35,689 - INFO - joeynmt.training - Epoch   9, Step:   300300, Batch Loss:     1.747637, Tokens per Sec:     7526, Lr: 0.000300\n",
      "2021-08-03 18:58:05,103 - INFO - joeynmt.training - Epoch   9, Step:   300400, Batch Loss:     1.777579, Tokens per Sec:     7375, Lr: 0.000300\n",
      "2021-08-03 18:58:34,536 - INFO - joeynmt.training - Epoch   9, Step:   300500, Batch Loss:     1.582051, Tokens per Sec:     7551, Lr: 0.000300\n",
      "2021-08-03 18:59:04,325 - INFO - joeynmt.training - Epoch   9, Step:   300600, Batch Loss:     1.627030, Tokens per Sec:     7547, Lr: 0.000300\n",
      "2021-08-03 18:59:33,912 - INFO - joeynmt.training - Epoch   9, Step:   300700, Batch Loss:     1.682850, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-03 19:00:03,354 - INFO - joeynmt.training - Epoch   9, Step:   300800, Batch Loss:     1.483195, Tokens per Sec:     7290, Lr: 0.000300\n",
      "2021-08-03 19:00:32,696 - INFO - joeynmt.training - Epoch   9, Step:   300900, Batch Loss:     1.411785, Tokens per Sec:     7497, Lr: 0.000300\n",
      "2021-08-03 19:01:02,252 - INFO - joeynmt.training - Epoch   9, Step:   301000, Batch Loss:     1.670876, Tokens per Sec:     7497, Lr: 0.000300\n",
      "2021-08-03 19:01:31,731 - INFO - joeynmt.training - Epoch   9, Step:   301100, Batch Loss:     1.602405, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-08-03 19:02:01,120 - INFO - joeynmt.training - Epoch   9, Step:   301200, Batch Loss:     1.744650, Tokens per Sec:     7353, Lr: 0.000300\n",
      "2021-08-03 19:02:30,698 - INFO - joeynmt.training - Epoch   9, Step:   301300, Batch Loss:     1.722326, Tokens per Sec:     7519, Lr: 0.000300\n",
      "2021-08-03 19:03:00,148 - INFO - joeynmt.training - Epoch   9, Step:   301400, Batch Loss:     1.600553, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-08-03 19:03:29,829 - INFO - joeynmt.training - Epoch   9, Step:   301500, Batch Loss:     1.746968, Tokens per Sec:     7467, Lr: 0.000300\n",
      "2021-08-03 19:03:59,136 - INFO - joeynmt.training - Epoch   9, Step:   301600, Batch Loss:     1.601176, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-08-03 19:04:28,224 - INFO - joeynmt.training - Epoch   9, Step:   301700, Batch Loss:     1.577880, Tokens per Sec:     7286, Lr: 0.000300\n",
      "2021-08-03 19:04:57,642 - INFO - joeynmt.training - Epoch   9, Step:   301800, Batch Loss:     1.657196, Tokens per Sec:     7516, Lr: 0.000300\n",
      "2021-08-03 19:05:26,959 - INFO - joeynmt.training - Epoch   9, Step:   301900, Batch Loss:     1.640012, Tokens per Sec:     7553, Lr: 0.000300\n",
      "2021-08-03 19:05:56,681 - INFO - joeynmt.training - Epoch   9, Step:   302000, Batch Loss:     1.783040, Tokens per Sec:     7547, Lr: 0.000300\n",
      "2021-08-03 19:06:26,190 - INFO - joeynmt.training - Epoch   9, Step:   302100, Batch Loss:     1.532220, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-03 19:06:55,781 - INFO - joeynmt.training - Epoch   9, Step:   302200, Batch Loss:     1.663091, Tokens per Sec:     7664, Lr: 0.000300\n",
      "2021-08-03 19:07:25,162 - INFO - joeynmt.training - Epoch   9, Step:   302300, Batch Loss:     1.743113, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-08-03 19:07:55,036 - INFO - joeynmt.training - Epoch   9, Step:   302400, Batch Loss:     1.685832, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-08-03 19:08:24,874 - INFO - joeynmt.training - Epoch   9, Step:   302500, Batch Loss:     1.710634, Tokens per Sec:     7686, Lr: 0.000300\n",
      "2021-08-03 19:08:53,989 - INFO - joeynmt.training - Epoch   9, Step:   302600, Batch Loss:     1.631456, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-08-03 19:09:23,211 - INFO - joeynmt.training - Epoch   9, Step:   302700, Batch Loss:     1.601663, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-08-03 19:09:52,743 - INFO - joeynmt.training - Epoch   9, Step:   302800, Batch Loss:     1.842467, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-08-03 19:10:22,255 - INFO - joeynmt.training - Epoch   9, Step:   302900, Batch Loss:     1.644725, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-08-03 19:10:51,876 - INFO - joeynmt.training - Epoch   9, Step:   303000, Batch Loss:     1.680648, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-08-03 19:11:21,470 - INFO - joeynmt.training - Epoch   9, Step:   303100, Batch Loss:     1.667691, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-08-03 19:11:50,989 - INFO - joeynmt.training - Epoch   9, Step:   303200, Batch Loss:     2.050119, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-08-03 19:12:20,605 - INFO - joeynmt.training - Epoch   9, Step:   303300, Batch Loss:     1.584781, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 19:12:50,011 - INFO - joeynmt.training - Epoch   9, Step:   303400, Batch Loss:     1.737413, Tokens per Sec:     7519, Lr: 0.000300\n",
      "2021-08-03 19:13:19,418 - INFO - joeynmt.training - Epoch   9, Step:   303500, Batch Loss:     1.613527, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-08-03 19:13:48,888 - INFO - joeynmt.training - Epoch   9, Step:   303600, Batch Loss:     1.646108, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-08-03 19:14:18,525 - INFO - joeynmt.training - Epoch   9, Step:   303700, Batch Loss:     1.619425, Tokens per Sec:     7727, Lr: 0.000300\n",
      "2021-08-03 19:14:38,381 - INFO - joeynmt.training - Epoch   9: total training loss 8952.29\n",
      "2021-08-03 19:14:38,381 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-03 19:14:48,204 - INFO - joeynmt.training - Epoch  10, Step:   303800, Batch Loss:     1.766381, Tokens per Sec:     7014, Lr: 0.000300\n",
      "2021-08-03 19:15:17,603 - INFO - joeynmt.training - Epoch  10, Step:   303900, Batch Loss:     1.741718, Tokens per Sec:     7500, Lr: 0.000300\n",
      "2021-08-03 19:15:46,730 - INFO - joeynmt.training - Epoch  10, Step:   304000, Batch Loss:     1.755381, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 19:16:16,082 - INFO - joeynmt.training - Epoch  10, Step:   304100, Batch Loss:     1.735648, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-08-03 19:16:45,557 - INFO - joeynmt.training - Epoch  10, Step:   304200, Batch Loss:     1.680372, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-08-03 19:17:15,016 - INFO - joeynmt.training - Epoch  10, Step:   304300, Batch Loss:     1.678607, Tokens per Sec:     7567, Lr: 0.000300\n",
      "2021-08-03 19:17:44,317 - INFO - joeynmt.training - Epoch  10, Step:   304400, Batch Loss:     1.609201, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-08-03 19:18:13,919 - INFO - joeynmt.training - Epoch  10, Step:   304500, Batch Loss:     1.503449, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-08-03 19:18:43,111 - INFO - joeynmt.training - Epoch  10, Step:   304600, Batch Loss:     1.667230, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-08-03 19:19:12,606 - INFO - joeynmt.training - Epoch  10, Step:   304700, Batch Loss:     1.703602, Tokens per Sec:     7471, Lr: 0.000300\n",
      "2021-08-03 19:19:42,504 - INFO - joeynmt.training - Epoch  10, Step:   304800, Batch Loss:     1.716941, Tokens per Sec:     7639, Lr: 0.000300\n",
      "2021-08-03 19:20:11,906 - INFO - joeynmt.training - Epoch  10, Step:   304900, Batch Loss:     1.575287, Tokens per Sec:     7459, Lr: 0.000300\n",
      "2021-08-03 19:20:41,120 - INFO - joeynmt.training - Epoch  10, Step:   305000, Batch Loss:     1.656186, Tokens per Sec:     7500, Lr: 0.000300\n",
      "2021-08-03 19:24:12,136 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 19:24:12,136 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 19:24:12,136 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 19:24:13,625 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 19:24:13,625 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 19:24:14,841 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 19:24:14,842 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 19:24:14,842 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 19:24:14,842 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , so is good for me . ”\n",
      "2021-08-03 19:24:14,843 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 19:24:14,843 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 19:24:14,844 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 19:24:14,844 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ The residents are in fear of Jehovah are greater than many riches that are standing . ”\n",
      "2021-08-03 19:24:14,844 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 19:24:14,845 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 19:24:14,845 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 19:24:14,845 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy in the book of Daniel says about God’s Kingdom .\n",
      "2021-08-03 19:24:14,846 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 19:24:14,846 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 19:24:14,847 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 19:24:14,847 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 19:24:14,847 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   305000: bleu:  27.79, loss: 184685.5156, ppl:   4.5971, duration: 213.7270s\n",
      "2021-08-03 19:24:44,490 - INFO - joeynmt.training - Epoch  10, Step:   305100, Batch Loss:     1.734777, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-03 19:25:13,756 - INFO - joeynmt.training - Epoch  10, Step:   305200, Batch Loss:     1.773782, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-08-03 19:25:43,062 - INFO - joeynmt.training - Epoch  10, Step:   305300, Batch Loss:     1.512102, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-03 19:26:12,697 - INFO - joeynmt.training - Epoch  10, Step:   305400, Batch Loss:     1.911816, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-08-03 19:26:41,963 - INFO - joeynmt.training - Epoch  10, Step:   305500, Batch Loss:     1.472854, Tokens per Sec:     7385, Lr: 0.000300\n",
      "2021-08-03 19:27:11,481 - INFO - joeynmt.training - Epoch  10, Step:   305600, Batch Loss:     1.729824, Tokens per Sec:     7560, Lr: 0.000300\n",
      "2021-08-03 19:27:41,156 - INFO - joeynmt.training - Epoch  10, Step:   305700, Batch Loss:     1.655619, Tokens per Sec:     7565, Lr: 0.000300\n",
      "2021-08-03 19:28:10,629 - INFO - joeynmt.training - Epoch  10, Step:   305800, Batch Loss:     1.671886, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-08-03 19:28:40,052 - INFO - joeynmt.training - Epoch  10, Step:   305900, Batch Loss:     1.716835, Tokens per Sec:     7402, Lr: 0.000300\n",
      "2021-08-03 19:29:09,697 - INFO - joeynmt.training - Epoch  10, Step:   306000, Batch Loss:     1.684952, Tokens per Sec:     7583, Lr: 0.000300\n",
      "2021-08-03 19:29:39,243 - INFO - joeynmt.training - Epoch  10, Step:   306100, Batch Loss:     1.588946, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-08-03 19:30:08,501 - INFO - joeynmt.training - Epoch  10, Step:   306200, Batch Loss:     1.789979, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-08-03 19:30:38,039 - INFO - joeynmt.training - Epoch  10, Step:   306300, Batch Loss:     1.613347, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-08-03 19:31:07,496 - INFO - joeynmt.training - Epoch  10, Step:   306400, Batch Loss:     1.759409, Tokens per Sec:     7412, Lr: 0.000300\n",
      "2021-08-03 19:31:36,922 - INFO - joeynmt.training - Epoch  10, Step:   306500, Batch Loss:     1.805532, Tokens per Sec:     7343, Lr: 0.000300\n",
      "2021-08-03 19:32:06,268 - INFO - joeynmt.training - Epoch  10, Step:   306600, Batch Loss:     1.712940, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-08-03 19:32:35,893 - INFO - joeynmt.training - Epoch  10, Step:   306700, Batch Loss:     1.768331, Tokens per Sec:     7532, Lr: 0.000300\n",
      "2021-08-03 19:33:05,425 - INFO - joeynmt.training - Epoch  10, Step:   306800, Batch Loss:     1.779556, Tokens per Sec:     7479, Lr: 0.000300\n",
      "2021-08-03 19:33:35,137 - INFO - joeynmt.training - Epoch  10, Step:   306900, Batch Loss:     1.743544, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-08-03 19:34:04,802 - INFO - joeynmt.training - Epoch  10, Step:   307000, Batch Loss:     1.532754, Tokens per Sec:     7426, Lr: 0.000300\n",
      "2021-08-03 19:34:34,473 - INFO - joeynmt.training - Epoch  10, Step:   307100, Batch Loss:     1.860423, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 19:35:04,149 - INFO - joeynmt.training - Epoch  10, Step:   307200, Batch Loss:     1.713427, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-08-03 19:35:33,378 - INFO - joeynmt.training - Epoch  10, Step:   307300, Batch Loss:     1.586667, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-08-03 19:36:02,564 - INFO - joeynmt.training - Epoch  10, Step:   307400, Batch Loss:     1.582100, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-03 19:36:31,920 - INFO - joeynmt.training - Epoch  10, Step:   307500, Batch Loss:     1.737799, Tokens per Sec:     7528, Lr: 0.000300\n",
      "2021-08-03 19:37:01,556 - INFO - joeynmt.training - Epoch  10, Step:   307600, Batch Loss:     1.875362, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-08-03 19:37:31,032 - INFO - joeynmt.training - Epoch  10, Step:   307700, Batch Loss:     1.661517, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-08-03 19:37:59,749 - INFO - joeynmt.training - Epoch  10, Step:   307800, Batch Loss:     1.768682, Tokens per Sec:     7364, Lr: 0.000300\n",
      "2021-08-03 19:38:29,348 - INFO - joeynmt.training - Epoch  10, Step:   307900, Batch Loss:     1.661621, Tokens per Sec:     7487, Lr: 0.000300\n",
      "2021-08-03 19:38:58,655 - INFO - joeynmt.training - Epoch  10, Step:   308000, Batch Loss:     1.591361, Tokens per Sec:     7312, Lr: 0.000300\n",
      "2021-08-03 19:39:28,064 - INFO - joeynmt.training - Epoch  10, Step:   308100, Batch Loss:     1.712105, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-08-03 19:39:57,718 - INFO - joeynmt.training - Epoch  10, Step:   308200, Batch Loss:     1.642132, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-08-03 19:40:27,102 - INFO - joeynmt.training - Epoch  10, Step:   308300, Batch Loss:     1.743259, Tokens per Sec:     7346, Lr: 0.000300\n",
      "2021-08-03 19:40:56,676 - INFO - joeynmt.training - Epoch  10, Step:   308400, Batch Loss:     1.861765, Tokens per Sec:     7332, Lr: 0.000300\n",
      "2021-08-03 19:41:25,838 - INFO - joeynmt.training - Epoch  10, Step:   308500, Batch Loss:     1.763378, Tokens per Sec:     7351, Lr: 0.000300\n",
      "2021-08-03 19:41:55,257 - INFO - joeynmt.training - Epoch  10, Step:   308600, Batch Loss:     1.582859, Tokens per Sec:     7340, Lr: 0.000300\n",
      "2021-08-03 19:42:25,000 - INFO - joeynmt.training - Epoch  10, Step:   308700, Batch Loss:     1.684258, Tokens per Sec:     7562, Lr: 0.000300\n",
      "2021-08-03 19:42:54,524 - INFO - joeynmt.training - Epoch  10, Step:   308800, Batch Loss:     1.868556, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-08-03 19:43:24,295 - INFO - joeynmt.training - Epoch  10, Step:   308900, Batch Loss:     1.851663, Tokens per Sec:     7500, Lr: 0.000300\n",
      "2021-08-03 19:43:53,667 - INFO - joeynmt.training - Epoch  10, Step:   309000, Batch Loss:     1.620757, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-08-03 19:44:23,181 - INFO - joeynmt.training - Epoch  10, Step:   309100, Batch Loss:     1.819586, Tokens per Sec:     7533, Lr: 0.000300\n",
      "2021-08-03 19:44:24,649 - INFO - joeynmt.training - Epoch  10: total training loss 8971.00\n",
      "2021-08-03 19:44:24,649 - INFO - joeynmt.training - Training ended after  10 epochs.\n",
      "2021-08-03 19:44:24,649 - INFO - joeynmt.training - Best validation result (greedy) at step   305000:   4.60 ppl.\n",
      "2021-08-03 19:44:24,679 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 10000 (with beam_size)\n",
      "2021-08-03 19:44:25,126 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 19:44:25,378 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 19:44:25,451 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe.en)...\n",
      "2021-08-03 19:48:43,267 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 19:48:43,268 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 19:48:43,268 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 19:48:44,797 - INFO - joeynmt.prediction -  dev bleu[13a]:  28.24 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-03 19:48:44,806 - INFO - joeynmt.prediction - Translations saved to: models/rwen_reverse_transformer_continued2/00305000.hyps.dev\n",
      "2021-08-03 19:48:44,807 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe.en)...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
      "    main()\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/training.py\", line 822, in train\n",
      "    datasets=datasets_to_test)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/prediction.py\", line 335, in test\n",
      "    bpe_type=bpe_type, sacrebleu=sacrebleu, n_gpu=n_gpu)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/prediction.py\", line 104, in validate_on_data\n",
      "    for valid_batch in iter(valid_iter):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/iterator.py\", line 160, in __iter__\n",
      "    yield Batch(minibatch, self.dataset, self.device)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/batch.py\", line 34, in __init__\n",
      "    setattr(self, name, field.process(batch, device=device))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\", line 230, in process\n",
      "    padded = self.pad(batch)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\", line 248, in pad\n",
      "    max_len = max(len(x) for x in minibatch)\n",
      "ValueError: max() arg is an empty sequence\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_rwen_reload3.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNm-DzIqm4Is"
   },
   "source": [
    "9.5 epochs done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCCsxotPkwab",
    "outputId": "a2b725b5-28e2-48a0-ed65-06650b1e592a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 265000\tLoss: 186668.78125\tPPL: 4.67299\tbleu: 27.62741\tLR: 0.00030000\t*\n",
      "Steps: 270000\tLoss: 186920.09375\tPPL: 4.68270\tbleu: 27.46330\tLR: 0.00030000\t\n",
      "Steps: 275000\tLoss: 186200.25000\tPPL: 4.65494\tbleu: 27.48177\tLR: 0.00030000\t*\n",
      "Steps: 280000\tLoss: 186035.54688\tPPL: 4.64862\tbleu: 27.39439\tLR: 0.00030000\t*\n",
      "Steps: 285000\tLoss: 186202.71875\tPPL: 4.65504\tbleu: 27.76276\tLR: 0.00030000\t\n",
      "Steps: 290000\tLoss: 185566.21875\tPPL: 4.63063\tbleu: 27.73543\tLR: 0.00030000\t*\n",
      "Steps: 295000\tLoss: 185271.35938\tPPL: 4.61937\tbleu: 27.66582\tLR: 0.00030000\t*\n",
      "Steps: 300000\tLoss: 184974.01562\tPPL: 4.60804\tbleu: 27.81265\tLR: 0.00030000\t*\n",
      "Steps: 305000\tLoss: 184685.51562\tPPL: 4.59707\tbleu: 27.79174\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/rwen_reverse_transformer_continued2/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tUjx8ykkwae",
    "outputId": "634cc4f1-b663-408f-8403-2dbc7d2a625d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 19:54:02,056 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 19:54:04,931 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 19:54:05,188 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/rwen_reverse_transformer_continued2/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe.rw\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/translation2.bpe.rw_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzQunCmNkwaf",
    "outputId": "b2f7d940-7ecb-452b-ef0a-ff0fd411a131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
      "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 27.1 60.2/35.7/24.4/17.6 (BP = 0.875 ratio = 0.883 hyp_len = 75182 ref_len = 85182)\n"
     ]
    }
   ],
   "source": [
    "!cat \"translation2.bpe.rw_en\" | sacrebleu \"test.en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA82URW4gDfT"
   },
   "source": [
    "#### Sample translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWP2Lk8dfcXk",
    "outputId": "c9ff705f-0dba-49f2-bb81-5cf96116a69c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although my parents did not become baptized Witnesses , they soon concluded that the Catholic teachings were not in harmony with the Bible .\n",
      "Why is it important to seek time for communication ?\n",
      "Would God resolve this problem in that Adam would continue to live in pleasure and goodness ?\n",
      "Yoonhee : I learned that I was pregnant !\n",
      "; Degandt , B .\n",
      "However , I feel comforted when I meditate on all the things we have done for 45 years .\n",
      "They stood , and they suffered in the face .\n",
      "The End of the Christian Congregation\n",
      "What counsel may we have been given when we were about to make decisions ?\n",
      "These crafty tactics that Satan used were really a thief .\n"
     ]
    }
   ],
   "source": [
    "# Candidates\n",
    "! head \"translation2.bpe.rw_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdQCQ2uZfcXm",
    "outputId": "6f6f0548-0c68-4dc7-f945-fdc01ad49ffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Though my parents never became baptized Witnesses , they soon concluded that the teachings of the Catholic Church were not in harmony with the Bible .\n",
      "Why is it important to make time for communication ?\n",
      "Could God solve this problem in such a way as to ensure Adam’s continued happiness and welfare ?\n",
      "Yoonhee : I was devastated ​ — and scared !\n",
      "; Degandt , B .\n",
      "I take considerable comfort , though , in what was accomplished in the 45 years we were together .\n",
      "And they stood still with sad faces .\n",
      "Christian Congregation Affected\n",
      "We likely have received what advice when we faced a decision ?\n",
      "This sly approach exposed Satan for what he really is ​ — a devious intruder .\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "! head \"test.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EJ5OW0afcXn",
    "outputId": "e941d746-3ba6-46de-e999-a2953d5b7867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N’ubwo ababyeyi banjye batigeze baba Abahamya babatijwe , bidatinze bageze ku mwanzuro w’uko inyigisho za kiliziya Gatolika zitari zihuje na Bibiliya .\n",
      "Kuki gushaka igihe cyo gushyikirana ari iby’ingenzi ?\n",
      "Mbese , Imana yashoboraga gukemura icyo kibazo mu buryo bw’uko Adamu yari gukomeza kubaho mu munezero no kugubwa neza ?\n",
      "Yoonhee : Maze kumenya ko ntwite nagize ubwoba !\n",
      "; Degandt , B .\n",
      "Icyakora , iyo ntekereje ibintu byose twakoranye mu myaka 45 , numva mpumurijwe .\n",
      "Nuko barahagarara , bafite umubabaro mu maso .\n",
      "Ingaruka ku Itorero rya Gikristo\n",
      "Ni iyihe nama dushobora kuba twarahawe igihe twari tugiye gufata umwanzuro ?\n",
      "Ayo mayeri Satani yakoresheje , yagaragaje uwo ari we by’ukuri , ni umujura wuzuye ubucakura .\n"
     ]
    }
   ],
   "source": [
    "# Source\n",
    "! head \"test.rw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbCNBp8BdCpd"
   },
   "source": [
    "## Luhyia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rJddfbVID9rw"
   },
   "outputs": [],
   "source": [
    "# Changing to Luhyia directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pg9VrSuzBGRT",
    "outputId": "ca97c62f-f133-4e60-be7b-ce0ba4ef7503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shareddrives/NMT_for_African_Language/Luhya\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kE77vmoBEJXK",
    "outputId": "24ec30e8-b78d-4f9a-ce65-41c867b7a6e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 97 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 19.2 MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 34.3 MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.9.6-py3-none-any.whl (375 kB)\n",
      "\u001b[K     |████████████████████████████████| 375 kB 65.1 MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Collecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.9.3-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 19.5 MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting astroid<2.7,>=2.6.5\n",
      "  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 74.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 4.0 MB/s \n",
      "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
      "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 44.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=9649301e69fa1f3cdd15a3e47eb15e8ed54ab93ca052a191f26a2093816c9572\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gqxv9vwy/wheels/c3/5f/f1/40cf058695468dfc7487d8583ffb64a17b7e0407d7a8d362aa\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68431 sha256=bcb602570472caa76e9f050b7cc436e6c083e636bdf5a1ad4891191fa2e08833\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
      "Successfully built joeynmt wrapt\n",
      "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
      "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed astroid-2.6.5 isort-5.9.3 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.6 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "code",
    "id": "ZAHPSbe89GrT"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language3, source_language)\n",
    "path = \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya\"\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language3}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language3}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"{path}/train.bpe\"\n",
    "    dev:   \"{path}/dev.bpe\"\n",
    "    test:  \"{path}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"{path}/vocab.txt\"\n",
    "    trg_vocab: \"{path}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"          \n",
    "    patience: 5                     \n",
    "    learning_rate_factor: 0.5       \n",
    "    learning_rate_warmup: 1000      \n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 1096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 1600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  \n",
    "    validation_freq: 200         # Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer\"\n",
    "    overwrite: True             # Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             \n",
    "        embeddings:\n",
    "            embedding_dim: 256   \n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         \n",
    "        ff_size: 1024            \n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              \n",
    "        embeddings:\n",
    "            embedding_dim: 256    \n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         \n",
    "        ff_size: 1024            \n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, path=path, source_language=source_language, target_language3=target_language3)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESL-qhTaelt0",
    "outputId": "6dd33e61-058f-455a-e8d5-abd2c806d363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-04 07:16:17,936 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-04 07:16:18,542 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-04 07:16:21,126 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-04 07:16:22,152 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-04 07:16:23,540 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-04 07:16:24,855 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-04 07:16:24,855 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 07:16:25,116 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 07:16:25.302320: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 07:16:27,056 - INFO - joeynmt.training - Total params: 12099840\n",
      "2021-08-04 07:16:29,276 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-08-04 07:16:29,276 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-08-04 07:16:29,276 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-04 07:16:29,277 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/train.bpe\n",
      "2021-08-04 07:16:29,277 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/dev.bpe\n",
      "2021-08-04 07:16:29,277 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/test.bpe\n",
      "2021-08-04 07:16:29,277 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-04 07:16:29,278 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-04 07:16:29,278 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-04 07:16:29,278 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/vocab.txt\n",
      "2021-08-04 07:16:29,278 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/vocab.txt\n",
      "2021-08-04 07:16:29,279 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-04 07:16:29,279 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-04 07:16:29,279 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-04 07:16:29,279 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-04 07:16:29,280 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-04 07:16:29,280 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-04 07:16:29,280 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-04 07:16:29,280 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-04 07:16:29,281 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-04 07:16:29,281 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-04 07:16:29,281 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-04 07:16:29,281 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-04 07:16:29,282 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-04 07:16:29,282 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-04 07:16:29,282 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-04 07:16:29,282 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-04 07:16:29,283 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1096\n",
      "2021-08-04 07:16:29,283 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-04 07:16:29,283 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-08-04 07:16:29,283 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-04 07:16:29,284 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-04 07:16:29,284 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-04 07:16:29,284 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-04 07:16:29,284 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 200\n",
      "2021-08-04 07:16:29,285 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-08-04 07:16:29,285 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-04 07:16:29,285 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lhen_reverse_transformer\n",
      "2021-08-04 07:16:29,285 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-04 07:16:29,286 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-04 07:16:29,286 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-04 07:16:29,286 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-04 07:16:29,286 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-04 07:16:29,287 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-04 07:16:29,287 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-04 07:16:29,287 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-04 07:16:29,287 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-04 07:16:29,288 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-04 07:16:29,288 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-04 07:16:29,288 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-04 07:16:29,288 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-04 07:16:29,289 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-04 07:16:29,289 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-04 07:16:29,289 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-04 07:16:29,289 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 07:16:29,290 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-04 07:16:29,290 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-04 07:16:29,290 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-04 07:16:29,290 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-04 07:16:29,291 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-04 07:16:29,291 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-04 07:16:29,291 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-04 07:16:29,291 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-04 07:16:29,292 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 07:16:29,292 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-04 07:16:29,292 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-04 07:16:29,292 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-04 07:16:29,292 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-04 07:16:29,293 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-04 07:16:29,293 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 7745,\n",
      "\tvalid 79,\n",
      "\ttest 79\n",
      "2021-08-04 07:16:29,293 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-08-04 07:16:29,294 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-08-04 07:16:29,294 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-08-04 07:16:29,294 - INFO - joeynmt.helpers - Number of Src words (types): 4061\n",
      "2021-08-04 07:16:29,295 - INFO - joeynmt.helpers - Number of Trg words (types): 4061\n",
      "2021-08-04 07:16:29,295 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4061),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4061))\n",
      "2021-08-04 07:16:29,308 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 1096\n",
      "\ttotal batch size (w. parallel & accumulation): 1096\n",
      "2021-08-04 07:16:29,309 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-04 07:16:42,620 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.182063, Tokens per Sec:     5360, Lr: 0.000300\n",
      "2021-08-04 07:16:55,778 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.054873, Tokens per Sec:     5426, Lr: 0.000300\n",
      "2021-08-04 07:17:04,762 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:17:04,763 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:17:05,552 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:17:05,553 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:17:05,553 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:17:05,554 - INFO - joeynmt.training - \tHypothesis: And the the , “ and the , “ and and and and and and and the , “ “ “ “ “ and and and and the , “ and the , “ and the , “ and and and and the , “ and and and the , and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "2021-08-04 07:17:05,554 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:17:05,555 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:17:05,555 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:17:05,555 - INFO - joeynmt.training - \tHypothesis: And the the , and the , and the , and and and and and and and and and and and and and and and and and and the , and the , and the , and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "2021-08-04 07:17:05,555 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:17:05,556 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:17:05,556 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:17:05,556 - INFO - joeynmt.training - \tHypothesis: And the the , “ and the , “ and and and and and and and the , “ and and and and and and and and and the , “ and the , “ and the , and and and and the , “ and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "2021-08-04 07:17:05,557 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:17:05,557 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:17:05,558 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:17:05,558 - INFO - joeynmt.training - \tHypothesis: And the the , and the , and the , and and and and and and and and and and and and and and and and and and the , and the , and the , and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and\n",
      "2021-08-04 07:17:05,558 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      200: bleu:   0.12, loss: 13427.6797, ppl: 147.4603, duration: 9.7800s\n",
      "2021-08-04 07:17:19,153 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     4.819888, Tokens per Sec:     5311, Lr: 0.000300\n",
      "2021-08-04 07:17:25,891 - INFO - joeynmt.training - Epoch   1: total training loss 1822.97\n",
      "2021-08-04 07:17:25,891 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-04 07:17:32,139 - INFO - joeynmt.training - Epoch   2, Step:      400, Batch Loss:     4.523382, Tokens per Sec:     5371, Lr: 0.000300\n",
      "2021-08-04 07:17:41,172 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:17:41,173 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:17:42,488 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:17:42,489 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:17:42,490 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:17:42,490 - INFO - joeynmt.training - \tHypothesis: And He He He He He He He He He He He He He He He He He He He He He He the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2021-08-04 07:17:42,490 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:17:42,491 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:17:42,491 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:17:42,491 - INFO - joeynmt.training - \tHypothesis: And He the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2021-08-04 07:17:42,491 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:17:42,492 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:17:42,492 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:17:42,493 - INFO - joeynmt.training - \tHypothesis: And He He He He He He He He He He He He He He He He He He He He He He He He He He He He He He He He He the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2021-08-04 07:17:42,493 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:17:42,494 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:17:42,494 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:17:42,494 - INFO - joeynmt.training - \tHypothesis: And He the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2021-08-04 07:17:42,495 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      400: bleu:   0.03, loss: 12647.7109, ppl: 110.3326, duration: 10.3545s\n",
      "2021-08-04 07:17:56,019 - INFO - joeynmt.training - Epoch   2, Step:      500, Batch Loss:     4.388593, Tokens per Sec:     5242, Lr: 0.000300\n",
      "2021-08-04 07:18:09,223 - INFO - joeynmt.training - Epoch   2, Step:      600, Batch Loss:     4.620733, Tokens per Sec:     5495, Lr: 0.000300\n",
      "2021-08-04 07:18:18,278 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:18:18,279 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:18:19,125 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:18:19,127 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:18:19,127 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:18:19,127 - INFO - joeynmt.training - \tHypothesis: And He had had had had had had had had had had not not not not not not not not not not not not not not not not not not not not not not be things .\n",
      "2021-08-04 07:18:19,128 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:18:19,128 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:18:19,129 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:18:19,129 - INFO - joeynmt.training - \tHypothesis: And the disciples , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord .\n",
      "2021-08-04 07:18:19,129 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:18:19,130 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:18:19,130 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:18:19,130 - INFO - joeynmt.training - \tHypothesis: But He said , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-08-04 07:18:19,131 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:18:19,132 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:18:19,132 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:18:19,133 - INFO - joeynmt.training - \tHypothesis: And He said to the Lord , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-08-04 07:18:19,133 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      600: bleu:   0.54, loss: 11534.6328, ppl:  72.9345, duration: 9.9094s\n",
      "2021-08-04 07:18:32,708 - INFO - joeynmt.training - Epoch   2, Step:      700, Batch Loss:     4.320620, Tokens per Sec:     5209, Lr: 0.000300\n",
      "2021-08-04 07:18:33,446 - INFO - joeynmt.training - Epoch   2: total training loss 1609.29\n",
      "2021-08-04 07:18:33,447 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-04 07:18:45,803 - INFO - joeynmt.training - Epoch   3, Step:      800, Batch Loss:     4.499962, Tokens per Sec:     5531, Lr: 0.000300\n",
      "2021-08-04 07:18:54,847 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:18:54,848 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:18:55,751 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:18:55,752 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:18:55,752 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:18:55,753 - INFO - joeynmt.training - \tHypothesis: And they had not not not not not not not not not not not not not not not not not not not not not not not be things .\n",
      "2021-08-04 07:18:55,753 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:18:55,753 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:18:55,754 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:18:55,754 - INFO - joeynmt.training - \tHypothesis: And they had had come to the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord who who who will be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be be things .\n",
      "2021-08-04 07:18:55,754 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:18:55,755 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:18:55,755 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:18:55,755 - INFO - joeynmt.training - \tHypothesis: Then He said to them , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ The disciples .\n",
      "2021-08-04 07:18:55,755 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:18:55,756 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:18:55,756 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:18:55,757 - INFO - joeynmt.training - \tHypothesis: Then He said to them , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-08-04 07:18:55,757 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step      800: bleu:   0.45, loss: 11055.4141, ppl:  61.0289, duration: 9.9530s\n",
      "2021-08-04 07:19:09,205 - INFO - joeynmt.training - Epoch   3, Step:      900, Batch Loss:     4.194473, Tokens per Sec:     5259, Lr: 0.000300\n",
      "2021-08-04 07:19:22,387 - INFO - joeynmt.training - Epoch   3, Step:     1000, Batch Loss:     4.145971, Tokens per Sec:     5388, Lr: 0.000300\n",
      "2021-08-04 07:19:29,523 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:19:29,523 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:19:30,870 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:19:30,872 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:19:30,872 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:19:30,872 - INFO - joeynmt.training - \tHypothesis: For I have not not not not not not not not not not not not not not not not not not not not not in the Lord .\n",
      "2021-08-04 07:19:30,873 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:19:30,873 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:19:30,873 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:19:30,874 - INFO - joeynmt.training - \tHypothesis: For I have not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not things .\n",
      "2021-08-04 07:19:30,874 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:19:30,875 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:19:30,875 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:19:30,875 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ What is the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord of the Lord . ”\n",
      "2021-08-04 07:19:30,875 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:19:30,876 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:19:30,876 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:19:30,876 - INFO - joeynmt.training - \tHypothesis: And when He said to them , “ What is the Lord , I have not not not not not not not not not not not not not not not not not not not not not not not not not not not not a own . ”\n",
      "2021-08-04 07:19:30,877 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     1000: bleu:   1.31, loss: 10761.1436, ppl:  54.7027, duration: 8.4886s\n",
      "2021-08-04 07:19:38,537 - INFO - joeynmt.training - Epoch   3: total training loss 1470.87\n",
      "2021-08-04 07:19:38,537 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-04 07:19:44,607 - INFO - joeynmt.training - Epoch   4, Step:     1100, Batch Loss:     3.696151, Tokens per Sec:     4949, Lr: 0.000300\n",
      "2021-08-04 07:19:57,668 - INFO - joeynmt.training - Epoch   4, Step:     1200, Batch Loss:     4.020280, Tokens per Sec:     5321, Lr: 0.000300\n",
      "2021-08-04 07:20:06,711 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:20:06,712 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:20:07,598 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:20:07,599 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:20:07,600 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:20:07,600 - INFO - joeynmt.training - \tHypothesis: For I have not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not in the Lord .\n",
      "2021-08-04 07:20:07,600 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:20:07,601 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:20:07,602 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:20:07,602 - INFO - joeynmt.training - \tHypothesis: But I have not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not in the law .\n",
      "2021-08-04 07:20:07,603 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:20:07,603 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:20:07,604 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:20:07,604 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ You have come to you , and you have come to you , and you have be done . ”\n",
      "2021-08-04 07:20:07,604 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:20:07,605 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:20:07,605 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:20:07,605 - INFO - joeynmt.training - \tHypothesis: But the Lord is the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord of the Lord , and the Lord , and the Lord is the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord of the Lord of the Lord , and the Lord , and\n",
      "2021-08-04 07:20:07,606 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     1200: bleu:   1.09, loss: 10515.9922, ppl:  49.9361, duration: 9.9374s\n",
      "2021-08-04 07:20:21,123 - INFO - joeynmt.training - Epoch   4, Step:     1300, Batch Loss:     3.628506, Tokens per Sec:     5361, Lr: 0.000300\n",
      "2021-08-04 07:20:34,125 - INFO - joeynmt.training - Epoch   4, Step:     1400, Batch Loss:     3.931695, Tokens per Sec:     5430, Lr: 0.000300\n",
      "2021-08-04 07:20:38,613 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:20:38,614 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:20:39,493 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:20:39,494 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:20:39,495 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:20:39,495 - INFO - joeynmt.training - \tHypothesis: For I have been been been been been been given to you to you , but you will be in the Lord .\n",
      "2021-08-04 07:20:39,495 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:20:39,496 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:20:39,496 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:20:39,496 - INFO - joeynmt.training - \tHypothesis: For I have been been been been been been been been given to you , but you , but you have been been been been been been been been been given to you .\n",
      "2021-08-04 07:20:39,497 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:20:39,497 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:20:39,498 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:20:39,498 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ The Lord , I have been been been been been been been been been been done . ”\n",
      "2021-08-04 07:20:39,499 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:20:39,499 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:20:39,499 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:20:39,500 - INFO - joeynmt.training - \tHypothesis: “ But I have been been been been been been been a man , and the Lord , and the Lord is a man who is a man who is the Lord . ”\n",
      "2021-08-04 07:20:39,500 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     1400: bleu:   1.76, loss: 10267.2793, ppl:  45.5246, duration: 5.3742s\n",
      "2021-08-04 07:20:41,121 - INFO - joeynmt.training - Epoch   4: total training loss 1409.12\n",
      "2021-08-04 07:20:41,122 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-04 07:20:53,080 - INFO - joeynmt.training - Epoch   5, Step:     1500, Batch Loss:     3.969625, Tokens per Sec:     5253, Lr: 0.000300\n",
      "2021-08-04 07:21:06,158 - INFO - joeynmt.training - Epoch   5, Step:     1600, Batch Loss:     4.046546, Tokens per Sec:     5457, Lr: 0.000300\n",
      "2021-08-04 07:21:11,785 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:21:11,785 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:21:12,615 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:21:12,616 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:21:12,617 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:21:12,617 - INFO - joeynmt.training - \tHypothesis: For I have not not not not not not be in the world , but you have not not be in the world .\n",
      "2021-08-04 07:21:12,617 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:21:12,618 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:21:12,618 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:21:12,618 - INFO - joeynmt.training - \tHypothesis: For I have not not not not be in the Lord , but you have not not not be in the world , but the Lord , but you have not be in the Father .\n",
      "2021-08-04 07:21:12,618 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:21:12,619 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:21:12,619 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:21:12,620 - INFO - joeynmt.training - \tHypothesis: Then He said to them , “ The Lord , and the Lord is the Lord , and the Lord , and the Lord . ”\n",
      "2021-08-04 07:21:12,620 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:21:12,621 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:21:12,621 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:21:12,622 - INFO - joeynmt.training - \tHypothesis: And I have come to them , “ I have come to you , and the Lord , and the Lord , and the Lord , and the Son of the Lord , and the Lord , and the Lord , and the Lord . ”\n",
      "2021-08-04 07:21:12,622 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     1600: bleu:   1.83, loss: 10147.8867, ppl:  43.5475, duration: 6.4635s\n",
      "2021-08-04 07:21:26,126 - INFO - joeynmt.training - Epoch   5, Step:     1700, Batch Loss:     3.977114, Tokens per Sec:     5226, Lr: 0.000300\n",
      "2021-08-04 07:21:34,688 - INFO - joeynmt.training - Epoch   5: total training loss 1356.84\n",
      "2021-08-04 07:21:34,688 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-04 07:21:39,342 - INFO - joeynmt.training - Epoch   6, Step:     1800, Batch Loss:     3.579168, Tokens per Sec:     5564, Lr: 0.000300\n",
      "2021-08-04 07:21:47,174 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:21:47,175 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:21:48,026 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:21:48,028 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:21:48,028 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:21:48,028 - INFO - joeynmt.training - \tHypothesis: For I have been been been been been been been to you , but you will be in the world .\n",
      "2021-08-04 07:21:48,028 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:21:48,029 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:21:48,029 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:21:48,030 - INFO - joeynmt.training - \tHypothesis: For I have been been been been been been in the Lord , and the Lord , and the Lord is not in the Lord , and the Lord .\n",
      "2021-08-04 07:21:48,030 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:21:48,031 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:21:48,031 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:21:48,031 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ You have the Lord , and the Lord is the Lord , and the Lord , and the Lord . ”\n",
      "2021-08-04 07:21:48,031 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:21:48,033 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:21:48,033 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:21:48,033 - INFO - joeynmt.training - \tHypothesis: But I am the word of the Lord , and the Lord , and the Lord , and the Lord , “ You have been been been been been been been been given to the Son of the Lord , and the Lord . ”\n",
      "2021-08-04 07:21:48,033 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     1800: bleu:   2.22, loss: 9945.2461, ppl:  40.3864, duration: 8.6906s\n",
      "2021-08-04 07:22:01,648 - INFO - joeynmt.training - Epoch   6, Step:     1900, Batch Loss:     3.985570, Tokens per Sec:     5301, Lr: 0.000300\n",
      "2021-08-04 07:22:14,715 - INFO - joeynmt.training - Epoch   6, Step:     2000, Batch Loss:     3.446528, Tokens per Sec:     5446, Lr: 0.000300\n",
      "2021-08-04 07:22:18,582 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:22:18,583 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:22:19,817 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:22:19,818 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:22:19,819 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:22:19,819 - INFO - joeynmt.training - \tHypothesis: For you have not not not be in the law , but you have not not be in the world .\n",
      "2021-08-04 07:22:19,819 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:22:19,820 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:22:19,820 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:22:19,820 - INFO - joeynmt.training - \tHypothesis: For if you have been been been been known to the Lord , but the Lord is not the Lord , but the Lord , but the Lord is not of God .\n",
      "2021-08-04 07:22:19,821 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:22:19,821 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:22:19,822 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:22:19,822 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Do not be in the world , and the Lord , and the Son of the Lord . ”\n",
      "2021-08-04 07:22:19,822 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:22:19,823 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:22:19,823 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:22:19,824 - INFO - joeynmt.training - \tHypothesis: And if you have been been been been been known to the law , and the Lord , and the Lord , saying , “ The Lord is the Son of the Lord , and the Lord . ”\n",
      "2021-08-04 07:22:19,824 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     2000: bleu:   2.90, loss: 9796.0361, ppl:  38.2064, duration: 5.1079s\n",
      "2021-08-04 07:22:33,159 - INFO - joeynmt.training - Epoch   6, Step:     2100, Batch Loss:     3.396153, Tokens per Sec:     5220, Lr: 0.000300\n",
      "2021-08-04 07:22:35,513 - INFO - joeynmt.training - Epoch   6: total training loss 1316.52\n",
      "2021-08-04 07:22:35,514 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-04 07:22:46,252 - INFO - joeynmt.training - Epoch   7, Step:     2200, Batch Loss:     3.201539, Tokens per Sec:     5492, Lr: 0.000300\n",
      "2021-08-04 07:22:51,280 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:22:51,281 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:22:52,116 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:22:52,119 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:22:52,119 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:22:52,119 - INFO - joeynmt.training - \tHypothesis: For if you have not not not not be in the law , but you have not not not of the law .\n",
      "2021-08-04 07:22:52,119 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:22:52,120 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:22:52,120 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:22:52,120 - INFO - joeynmt.training - \tHypothesis: For the Lord is not not the Lord , but the Lord , but the Lord is not not not not of the Lord , but the Lord , but of the Lord is not not not not not not .\n",
      "2021-08-04 07:22:52,121 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:22:52,121 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:22:52,121 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:22:52,122 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ The kingdom of the law , and the law is the earth . ”\n",
      "2021-08-04 07:22:52,122 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:22:52,123 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:22:52,123 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:22:52,123 - INFO - joeynmt.training - \tHypothesis: And they did not be afraid , saying , “ Do not be afraid , but the word of the world , but the world , and the Son of the Son of the world , and the Son of the Son of the world . ”\n",
      "2021-08-04 07:22:52,124 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     2200: bleu:   3.11, loss: 9678.1533, ppl:  36.5677, duration: 5.8710s\n",
      "2021-08-04 07:23:05,606 - INFO - joeynmt.training - Epoch   7, Step:     2300, Batch Loss:     3.717970, Tokens per Sec:     5254, Lr: 0.000300\n",
      "2021-08-04 07:23:18,689 - INFO - joeynmt.training - Epoch   7, Step:     2400, Batch Loss:     3.736270, Tokens per Sec:     5446, Lr: 0.000300\n",
      "2021-08-04 07:23:25,161 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:23:25,161 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:23:26,000 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:23:26,001 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:23:26,001 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:23:26,002 - INFO - joeynmt.training - \tHypothesis: For if you do not be not not be in the world , but the world is not not not not not of the world .\n",
      "2021-08-04 07:23:26,002 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:23:26,003 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:23:26,003 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:23:26,003 - INFO - joeynmt.training - \tHypothesis: For if you have been been been in the Lord , we have been given to the Lord , that you may be given to the Lord ,\n",
      "2021-08-04 07:23:26,003 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:23:26,004 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:23:26,004 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:23:26,005 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ The kingdom of the temple of the temple of the temple of the temple , and the Jews . ”\n",
      "2021-08-04 07:23:26,005 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:23:26,006 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:23:26,006 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:23:26,007 - INFO - joeynmt.training - \tHypothesis: And when they had been been been a man , he said to him , “ The city of the temple of the temple , and the Jews , and the Jews will be given to the sea . ”\n",
      "2021-08-04 07:23:26,007 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     2400: bleu:   2.35, loss: 9538.6074, ppl:  34.7184, duration: 7.3170s\n",
      "2021-08-04 07:23:35,247 - INFO - joeynmt.training - Epoch   7: total training loss 1274.48\n",
      "2021-08-04 07:23:35,247 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-04 07:23:39,552 - INFO - joeynmt.training - Epoch   8, Step:     2500, Batch Loss:     3.279047, Tokens per Sec:     4963, Lr: 0.000300\n",
      "2021-08-04 07:23:52,669 - INFO - joeynmt.training - Epoch   8, Step:     2600, Batch Loss:     3.584110, Tokens per Sec:     5484, Lr: 0.000300\n",
      "2021-08-04 07:23:57,986 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:23:57,987 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:23:58,809 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:23:58,811 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:23:58,812 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:23:58,812 - INFO - joeynmt.training - \tHypothesis: For if you do not be able to be able to be able to you , but to be able to be able to be able to be saved .\n",
      "2021-08-04 07:23:58,812 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:23:58,813 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:23:58,813 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:23:58,813 - INFO - joeynmt.training - \tHypothesis: For if you have been been been in the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord .\n",
      "2021-08-04 07:23:58,814 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:23:58,814 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:23:58,814 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:23:58,815 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ This is the kingdom of the earth . ” And He said to them , “ The Son of the Jews . ”\n",
      "2021-08-04 07:23:58,815 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:23:58,815 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:23:58,816 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:23:58,816 - INFO - joeynmt.training - \tHypothesis: But they were filled with them , saying , “ The Jews who are the Jews who are the Jews who are the Jews , and the Jews who are the Son of God . ”\n",
      "2021-08-04 07:23:58,816 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     2600: bleu:   3.59, loss: 9480.0615, ppl:  33.9707, duration: 6.1466s\n",
      "2021-08-04 07:24:12,453 - INFO - joeynmt.training - Epoch   8, Step:     2700, Batch Loss:     3.774696, Tokens per Sec:     5286, Lr: 0.000300\n",
      "2021-08-04 07:24:25,437 - INFO - joeynmt.training - Epoch   8, Step:     2800, Batch Loss:     3.267276, Tokens per Sec:     5394, Lr: 0.000300\n",
      "2021-08-04 07:24:30,826 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:24:30,826 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:24:31,665 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:24:31,667 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:24:31,667 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:24:31,667 - INFO - joeynmt.training - \tHypothesis: For if you do not be in your own own , but the law is not in the world .\n",
      "2021-08-04 07:24:31,667 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:24:31,668 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:24:31,669 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:24:31,669 - INFO - joeynmt.training - \tHypothesis: For if you have been in the Lord , I am not in the Lord , that you may be in the Lord , and the Lord , that you may be in you .\n",
      "2021-08-04 07:24:31,669 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:24:31,670 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:24:31,670 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:24:31,671 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ I have a man of the people . ”\n",
      "2021-08-04 07:24:31,671 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:24:31,672 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:24:31,672 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:24:31,672 - INFO - joeynmt.training - \tHypothesis: But if they had come to them , they said to him , “ The kingdom of Man is the sea , and the sea , and the Jews will be afraid . ”\n",
      "2021-08-04 07:24:31,672 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     2800: bleu:   4.18, loss: 9280.2266, ppl:  31.5376, duration: 6.2343s\n",
      "2021-08-04 07:24:34,583 - INFO - joeynmt.training - Epoch   8: total training loss 1239.02\n",
      "2021-08-04 07:24:34,583 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-04 07:24:45,271 - INFO - joeynmt.training - Epoch   9, Step:     2900, Batch Loss:     3.344804, Tokens per Sec:     5238, Lr: 0.000300\n",
      "2021-08-04 07:24:58,423 - INFO - joeynmt.training - Epoch   9, Step:     3000, Batch Loss:     3.539868, Tokens per Sec:     5435, Lr: 0.000300\n",
      "2021-08-04 07:25:03,023 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:25:03,024 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:25:04,309 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:25:04,310 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:25:04,310 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:25:04,311 - INFO - joeynmt.training - \tHypothesis: For if you do not be unish , but the world is not not not in the world .\n",
      "2021-08-04 07:25:04,311 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:25:04,312 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:25:04,312 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:25:04,312 - INFO - joeynmt.training - \tHypothesis: For the Lord is not the Lord , but the Lord , but the Lord is not the Lord , but the Lord is not the Father who is not of the Father .\n",
      "2021-08-04 07:25:04,313 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:25:04,313 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:25:04,313 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:25:04,314 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ I have been done to you , and the Jews who are the Jews . ”\n",
      "2021-08-04 07:25:04,314 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:25:04,315 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:25:04,315 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:25:04,315 - INFO - joeynmt.training - \tHypothesis: For the word of the world will be fulfilled , saying , “ The Jews who are the other days of the sea , and the Jews who are the Jews . ”\n",
      "2021-08-04 07:25:04,316 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     3000: bleu:   3.72, loss: 9160.7695, ppl:  30.1672, duration: 5.8917s\n",
      "2021-08-04 07:25:18,081 - INFO - joeynmt.training - Epoch   9, Step:     3100, Batch Loss:     3.550247, Tokens per Sec:     5297, Lr: 0.000300\n",
      "2021-08-04 07:25:27,659 - INFO - joeynmt.training - Epoch   9: total training loss 1205.87\n",
      "2021-08-04 07:25:27,659 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-04 07:25:31,149 - INFO - joeynmt.training - Epoch  10, Step:     3200, Batch Loss:     3.264526, Tokens per Sec:     5281, Lr: 0.000300\n",
      "2021-08-04 07:25:35,991 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:25:35,991 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:25:36,840 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:25:36,841 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:25:36,841 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:25:36,842 - INFO - joeynmt.training - \tHypothesis: For we do not be able to be able to be able to be able to be able to be able .\n",
      "2021-08-04 07:25:36,842 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:25:36,843 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:25:36,843 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:25:36,843 - INFO - joeynmt.training - \tHypothesis: For you have been a man , and you have not to you , but to you , but to be in the Lord , and to you , and not to be in the Lord .\n",
      "2021-08-04 07:25:36,843 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:25:36,844 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:25:36,844 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:25:36,844 - INFO - joeynmt.training - \tHypothesis: Then He said to them , “ I have been given to you , and the Pharisees , and the Pharisees , and the Pharisees , and the Pharisees , and the Pharisees . ”\n",
      "2021-08-04 07:25:36,845 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:25:36,846 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:25:36,846 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:25:36,846 - INFO - joeynmt.training - \tHypothesis: For the world is not able to the world , and they will be afraid , saying , “ The third day is the third day of the sea . ”\n",
      "2021-08-04 07:25:36,846 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     3200: bleu:   4.14, loss: 9103.0908, ppl:  29.5271, duration: 5.6971s\n",
      "2021-08-04 07:25:50,518 - INFO - joeynmt.training - Epoch  10, Step:     3300, Batch Loss:     3.068048, Tokens per Sec:     5329, Lr: 0.000300\n",
      "2021-08-04 07:26:03,569 - INFO - joeynmt.training - Epoch  10, Step:     3400, Batch Loss:     3.391898, Tokens per Sec:     5369, Lr: 0.000300\n",
      "2021-08-04 07:26:07,231 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:26:07,231 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:26:08,062 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:26:08,064 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:26:08,064 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:26:08,064 - INFO - joeynmt.training - \tHypothesis: For if you do not be unish , but do not be unish .\n",
      "2021-08-04 07:26:08,064 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:26:08,065 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:26:08,065 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:26:08,065 - INFO - joeynmt.training - \tHypothesis: For if you have been a man , you have not seen , but you are not of the Lord , but the Lord is not of the Lord .\n",
      "2021-08-04 07:26:08,066 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:26:08,066 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:26:08,067 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:26:08,067 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let you be afraid , and the people are the people . ”\n",
      "2021-08-04 07:26:08,067 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:26:08,068 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:26:08,068 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:26:08,068 - INFO - joeynmt.training - \tHypothesis: For the world is not a man who is a man who does not be afraid , and the other of the other will be fulfilled . ”\n",
      "2021-08-04 07:26:08,068 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     3400: bleu:   4.43, loss: 8974.2363, ppl:  28.1455, duration: 4.4989s\n",
      "2021-08-04 07:26:21,765 - INFO - joeynmt.training - Epoch  10, Step:     3500, Batch Loss:     3.065090, Tokens per Sec:     5250, Lr: 0.000300\n",
      "2021-08-04 07:26:25,094 - INFO - joeynmt.training - Epoch  10: total training loss 1171.57\n",
      "2021-08-04 07:26:25,095 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-04 07:26:35,005 - INFO - joeynmt.training - Epoch  11, Step:     3600, Batch Loss:     3.488127, Tokens per Sec:     5426, Lr: 0.000300\n",
      "2021-08-04 07:26:38,214 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:26:38,214 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:26:39,065 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:26:39,066 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:26:39,067 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:26:39,067 - INFO - joeynmt.training - \tHypothesis: For if you do not be unish , but the law is not able .\n",
      "2021-08-04 07:26:39,067 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:26:39,069 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:26:39,069 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:26:39,069 - INFO - joeynmt.training - \tHypothesis: For if you have been a apostle , you have not known , but the Lord , but the Lord is not in the Lord .\n",
      "2021-08-04 07:26:39,069 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:26:39,070 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:26:39,070 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:26:39,070 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Let you be afraid , and the Pharisees , and the Pharisees and the Pharisees and the Pharisees and the Pharisees , and the Pharisees and the Pharisees . ”\n",
      "2021-08-04 07:26:39,071 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:26:39,072 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:26:39,072 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:26:39,072 - INFO - joeynmt.training - \tHypothesis: For they will be fulfilled , saying , “ Let us be afraid , and they will be afraid . ”\n",
      "2021-08-04 07:26:39,072 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step     3600: bleu:   4.21, loss: 8884.9707, ppl:  27.2265, duration: 4.0671s\n",
      "2021-08-04 07:26:52,771 - INFO - joeynmt.training - Epoch  11, Step:     3700, Batch Loss:     3.554456, Tokens per Sec:     5228, Lr: 0.000300\n",
      "2021-08-04 07:27:05,922 - INFO - joeynmt.training - Epoch  11, Step:     3800, Batch Loss:     3.630595, Tokens per Sec:     5510, Lr: 0.000300\n",
      "2021-08-04 07:27:09,809 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:27:09,810 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:27:10,658 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:27:10,661 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:27:10,662 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:27:10,662 - INFO - joeynmt.training - \tHypothesis: For if you do not be able to be able to be able .\n",
      "2021-08-04 07:27:10,662 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:27:10,663 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:27:10,663 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:27:10,663 - INFO - joeynmt.training - \tHypothesis: For if you are the Lord , you are not in the Lord , but the Lord is not in the Lord , but the Lord is not in the Lord .\n",
      "2021-08-04 07:27:10,664 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:27:10,664 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:27:10,664 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:27:10,665 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let you be afraid , and the Pharisees , and the Pharisees will be afraid . ”\n",
      "2021-08-04 07:27:10,665 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:27:10,666 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:27:10,666 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:27:10,666 - INFO - joeynmt.training - \tHypothesis: For it is written , “ He who is not a man who is a man who is a man who is a man who is a man . ”\n",
      "2021-08-04 07:27:10,666 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step     3800: bleu:   5.54, loss: 8826.9736, ppl:  26.6456, duration: 4.7439s\n",
      "2021-08-04 07:27:20,565 - INFO - joeynmt.training - Epoch  11: total training loss 1140.13\n",
      "2021-08-04 07:27:20,566 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-04 07:27:24,246 - INFO - joeynmt.training - Epoch  12, Step:     3900, Batch Loss:     2.805736, Tokens per Sec:     4898, Lr: 0.000300\n",
      "2021-08-04 07:27:37,437 - INFO - joeynmt.training - Epoch  12, Step:     4000, Batch Loss:     3.449531, Tokens per Sec:     5397, Lr: 0.000300\n",
      "2021-08-04 07:27:41,181 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:27:41,181 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:27:42,204 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:27:42,205 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:27:42,206 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:27:42,206 - INFO - joeynmt.training - \tHypothesis: For the love of the love of the love of the flesh is not worthy of the world .\n",
      "2021-08-04 07:27:42,206 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:27:42,207 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:27:42,207 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:27:42,207 - INFO - joeynmt.training - \tHypothesis: For you are not of the Lord Jesus Christ , but the Lord is not in the Lord , but the Lord is not in the gospel of the gospel of the gospel .\n",
      "2021-08-04 07:27:42,208 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:27:42,208 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:27:42,208 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:27:42,209 - INFO - joeynmt.training - \tHypothesis: Jesus said to them , “ Let you be afraid , and you have done with the Jews . ”\n",
      "2021-08-04 07:27:42,209 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:27:42,210 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:27:42,210 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:27:42,210 - INFO - joeynmt.training - \tHypothesis: For the law is not lawful for it , but the other hour is not lawful to the man .\n",
      "2021-08-04 07:27:42,210 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     4000: bleu:   4.71, loss: 8744.0283, ppl:  25.8362, duration: 4.7733s\n",
      "2021-08-04 07:27:55,753 - INFO - joeynmt.training - Epoch  12, Step:     4100, Batch Loss:     3.986320, Tokens per Sec:     5183, Lr: 0.000300\n",
      "2021-08-04 07:28:08,945 - INFO - joeynmt.training - Epoch  12, Step:     4200, Batch Loss:     2.778886, Tokens per Sec:     5433, Lr: 0.000300\n",
      "2021-08-04 07:28:14,078 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:28:14,078 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:28:14,930 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:28:14,931 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:28:14,932 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:28:14,932 - INFO - joeynmt.training - \tHypothesis: For if you do not be saved , but not be ungodly .\n",
      "2021-08-04 07:28:14,932 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:28:14,933 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:28:14,933 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:28:14,933 - INFO - joeynmt.training - \tHypothesis: For you are not of the Lord , but the Lord , but the Lord , but the Lord , but the Lord , but the truth , but now I have been given to you .\n",
      "2021-08-04 07:28:14,934 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:28:14,934 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:28:14,935 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:28:14,935 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let us be afraid , and you have done to the Pharisees . ”\n",
      "2021-08-04 07:28:14,935 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:28:14,936 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:28:14,936 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:28:14,936 - INFO - joeynmt.training - \tHypothesis: For it is written , “ If anyone has a man , let him be a man , let him be saved . ”\n",
      "2021-08-04 07:28:14,936 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     4200: bleu:   4.34, loss: 8621.1689, ppl:  24.6823, duration: 5.9908s\n",
      "2021-08-04 07:28:18,729 - INFO - joeynmt.training - Epoch  12: total training loss 1126.30\n",
      "2021-08-04 07:28:18,730 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-04 07:28:28,510 - INFO - joeynmt.training - Epoch  13, Step:     4300, Batch Loss:     3.155663, Tokens per Sec:     5215, Lr: 0.000300\n",
      "2021-08-04 07:28:41,633 - INFO - joeynmt.training - Epoch  13, Step:     4400, Batch Loss:     2.859960, Tokens per Sec:     5461, Lr: 0.000300\n",
      "2021-08-04 07:28:47,189 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:28:47,190 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:28:48,036 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:28:48,037 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:28:48,038 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:28:48,038 - INFO - joeynmt.training - \tHypothesis: For if you do not be unish , do not be unish .\n",
      "2021-08-04 07:28:48,038 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:28:48,039 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:28:48,039 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:28:48,039 - INFO - joeynmt.training - \tHypothesis: For if you have been a man , you have not a man , but in the Lord , and not to be with me , but to the gospel of the Lord ,\n",
      "2021-08-04 07:28:48,039 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:28:48,040 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:28:48,040 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:28:48,040 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let us be afraid , and your eyes be afraid . ”\n",
      "2021-08-04 07:28:48,041 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:28:48,042 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:28:48,042 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:28:48,043 - INFO - joeynmt.training - \tHypothesis: For if anyone does not be given to him , he who does not be a man who does not be a man who sat on his own house .\n",
      "2021-08-04 07:28:48,043 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step     4400: bleu:   4.71, loss: 8571.1328, ppl:  24.2273, duration: 6.4094s\n",
      "2021-08-04 07:29:01,661 - INFO - joeynmt.training - Epoch  13, Step:     4500, Batch Loss:     3.182921, Tokens per Sec:     5345, Lr: 0.000300\n",
      "2021-08-04 07:29:12,013 - INFO - joeynmt.training - Epoch  13: total training loss 1089.77\n",
      "2021-08-04 07:29:12,013 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-04 07:29:14,819 - INFO - joeynmt.training - Epoch  14, Step:     4600, Batch Loss:     3.080486, Tokens per Sec:     5565, Lr: 0.000300\n",
      "2021-08-04 07:29:18,739 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:29:18,740 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:29:19,586 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:29:19,587 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:29:19,587 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:29:19,587 - INFO - joeynmt.training - \tHypothesis: For the love of the love of the flesh is not worthy of the body .\n",
      "2021-08-04 07:29:19,588 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:29:19,589 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:29:19,589 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:29:19,589 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , brethren , brethren , brethren , I am not of the Lord , but I am not of you .\n",
      "2021-08-04 07:29:19,590 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:29:19,590 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:29:19,590 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:29:19,591 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let us be afraid , and you are afraid . ”\n",
      "2021-08-04 07:29:19,591 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:29:19,592 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:29:19,592 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:29:19,593 - INFO - joeynmt.training - \tHypothesis: For it is written , “ If anyone has a man desires to be revealed , let him be afraid , and let him be afraid . ”\n",
      "2021-08-04 07:29:19,593 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step     4600: bleu:   5.18, loss: 8531.2549, ppl:  23.8706, duration: 4.7736s\n",
      "2021-08-04 07:29:33,311 - INFO - joeynmt.training - Epoch  14, Step:     4700, Batch Loss:     3.103908, Tokens per Sec:     5291, Lr: 0.000300\n",
      "2021-08-04 07:29:46,438 - INFO - joeynmt.training - Epoch  14, Step:     4800, Batch Loss:     2.642408, Tokens per Sec:     5404, Lr: 0.000300\n",
      "2021-08-04 07:29:49,976 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:29:49,976 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:29:50,848 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:29:50,849 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:29:50,849 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:29:50,850 - INFO - joeynmt.training - \tHypothesis: For if you do not be saved , but your members is not in the body .\n",
      "2021-08-04 07:29:50,850 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:29:50,850 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:29:50,851 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:29:50,851 - INFO - joeynmt.training - \tHypothesis: For you are not of the Lord , but you are not of the Lord , but you will be with you , but to the Father who sent you .\n",
      "2021-08-04 07:29:50,851 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:29:50,852 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:29:50,852 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:29:50,853 - INFO - joeynmt.training - \tHypothesis: Jesus said to them , “ Let us be afraid , and you will be afraid . ”\n",
      "2021-08-04 07:29:50,853 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:29:50,854 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:29:50,854 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:29:50,854 - INFO - joeynmt.training - \tHypothesis: For if anyone has been given to them , he who has a man who had a man was a man , he who was a great great .\n",
      "2021-08-04 07:29:50,854 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step     4800: bleu:   4.93, loss: 8429.5537, ppl:  22.9847, duration: 4.4156s\n",
      "2021-08-04 07:30:04,449 - INFO - joeynmt.training - Epoch  14, Step:     4900, Batch Loss:     2.969960, Tokens per Sec:     5264, Lr: 0.000300\n",
      "2021-08-04 07:30:08,349 - INFO - joeynmt.training - Epoch  14: total training loss 1067.52\n",
      "2021-08-04 07:30:08,350 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-04 07:30:17,529 - INFO - joeynmt.training - Epoch  15, Step:     5000, Batch Loss:     3.004205, Tokens per Sec:     5519, Lr: 0.000300\n",
      "2021-08-04 07:30:20,594 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:30:20,594 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:30:21,890 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:30:21,892 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:30:21,892 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:30:21,893 - INFO - joeynmt.training - \tHypothesis: For you are not of love , but not of the body of the body is not of the body .\n",
      "2021-08-04 07:30:21,893 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:30:21,894 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:30:21,894 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:30:21,895 - INFO - joeynmt.training - \tHypothesis: For you have not been in the Lord , but you are not of the Lord , but you will be with you .\n",
      "2021-08-04 07:30:21,895 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:30:21,895 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:30:21,896 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:30:21,896 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ Let us be afraid , and you will be afraid . ”\n",
      "2021-08-04 07:30:21,896 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:30:21,897 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:30:21,897 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:30:21,897 - INFO - joeynmt.training - \tHypothesis: For the things which are spoken , He will be fulfilled , and the other will be fulfilled , and the other will be fulfilled . ”\n",
      "2021-08-04 07:30:21,898 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step     5000: bleu:   6.22, loss: 8372.5293, ppl:  22.5024, duration: 4.3685s\n",
      "2021-08-04 07:30:35,478 - INFO - joeynmt.training - Epoch  15, Step:     5100, Batch Loss:     2.906727, Tokens per Sec:     5223, Lr: 0.000300\n",
      "2021-08-04 07:30:48,768 - INFO - joeynmt.training - Epoch  15, Step:     5200, Batch Loss:     2.887135, Tokens per Sec:     5455, Lr: 0.000300\n",
      "2021-08-04 07:30:53,164 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:30:53,165 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:30:53,999 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:30:54,000 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:30:54,000 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:30:54,001 - INFO - joeynmt.training - \tHypothesis: For if the love of the love of the love of the flesh ,\n",
      "2021-08-04 07:30:54,001 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:30:54,002 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:30:54,002 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:30:54,002 - INFO - joeynmt.training - \tHypothesis: For you are not of the Lord , and you are not of the Lord , but you are not of the Lord .\n",
      "2021-08-04 07:30:54,003 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:30:54,003 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:30:54,003 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:30:54,004 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let your hearts be afraid , and your children be saved . ”\n",
      "2021-08-04 07:30:54,004 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:30:54,004 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:30:54,005 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:30:54,005 - INFO - joeynmt.training - \tHypothesis: For if they had come to them , they will be afraid , and the other other other , and the one will be taken away . ”\n",
      "2021-08-04 07:30:54,005 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step     5200: bleu:   5.12, loss: 8353.7129, ppl:  22.3455, duration: 5.2363s\n",
      "2021-08-04 07:31:05,109 - INFO - joeynmt.training - Epoch  15: total training loss 1050.13\n",
      "2021-08-04 07:31:05,109 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-04 07:31:07,495 - INFO - joeynmt.training - Epoch  16, Step:     5300, Batch Loss:     3.008322, Tokens per Sec:     5569, Lr: 0.000300\n",
      "2021-08-04 07:31:20,740 - INFO - joeynmt.training - Epoch  16, Step:     5400, Batch Loss:     3.097382, Tokens per Sec:     5473, Lr: 0.000300\n",
      "2021-08-04 07:31:25,378 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:31:25,378 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:31:26,197 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:31:26,199 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:31:26,199 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:31:26,199 - INFO - joeynmt.training - \tHypothesis: For in love , not in love , but in unmembers , but in unmembers .\n",
      "2021-08-04 07:31:26,199 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:31:26,200 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:31:26,201 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:31:26,201 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , brethren , brethren , not be with you , but you are not of the Lord .\n",
      "2021-08-04 07:31:26,201 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:31:26,202 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:31:26,202 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:31:26,202 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let you be afraid , and you are forgiven you , and you are the Pharisees . ”\n",
      "2021-08-04 07:31:26,203 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:31:26,203 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:31:26,204 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:31:26,204 - INFO - joeynmt.training - \tHypothesis: For they did not know that it is not lawful to be fulfilled , but it is not lawful to be a man .\n",
      "2021-08-04 07:31:26,204 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     5400: bleu:   5.91, loss: 8316.4219, ppl:  22.0377, duration: 5.4631s\n",
      "2021-08-04 07:31:39,799 - INFO - joeynmt.training - Epoch  16, Step:     5500, Batch Loss:     2.991889, Tokens per Sec:     5237, Lr: 0.000300\n",
      "2021-08-04 07:31:52,919 - INFO - joeynmt.training - Epoch  16, Step:     5600, Batch Loss:     2.712386, Tokens per Sec:     5405, Lr: 0.000300\n",
      "2021-08-04 07:31:56,386 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:31:56,387 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:31:57,221 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:31:57,222 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:31:57,222 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:31:57,222 - INFO - joeynmt.training - \tHypothesis: For the love of love of sin is not worthy of the body .\n",
      "2021-08-04 07:31:57,223 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:31:57,223 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:31:57,224 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:31:57,224 - INFO - joeynmt.training - \tHypothesis: For you are not of the Lord , but the Lord , but the Lord , and the Lord , and the things which you have been given to you , but I have no longer of you .\n",
      "2021-08-04 07:31:57,224 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:31:57,225 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:31:57,225 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:31:57,225 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let us be afraid , and your children are forgiven you . ”\n",
      "2021-08-04 07:31:57,226 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:31:57,226 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:31:57,227 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:31:57,227 - INFO - joeynmt.training - \tHypothesis: For they have no one of them , and they have no one of the ground , and the other other other other other other other .\n",
      "2021-08-04 07:31:57,227 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     5600: bleu:   5.81, loss: 8237.6592, ppl:  21.4016, duration: 4.3080s\n",
      "2021-08-04 07:32:01,680 - INFO - joeynmt.training - Epoch  16: total training loss 1029.79\n",
      "2021-08-04 07:32:01,681 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-08-04 07:32:10,827 - INFO - joeynmt.training - Epoch  17, Step:     5700, Batch Loss:     2.969181, Tokens per Sec:     5320, Lr: 0.000300\n",
      "2021-08-04 07:32:23,962 - INFO - joeynmt.training - Epoch  17, Step:     5800, Batch Loss:     2.692656, Tokens per Sec:     5409, Lr: 0.000300\n",
      "2021-08-04 07:32:28,024 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:32:28,024 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:32:28,890 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:32:28,891 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:32:28,891 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:32:28,892 - INFO - joeynmt.training - \tHypothesis: For the love of love of faith is not worthy of the flesh .\n",
      "2021-08-04 07:32:28,892 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:32:28,893 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:32:28,893 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:32:28,893 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , brethren , do not be with you , but the Lord , but you are not of you , but you will not be saved .\n",
      "2021-08-04 07:32:28,894 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:32:28,894 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:32:28,894 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:32:28,895 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to you , and to be afraid . ”\n",
      "2021-08-04 07:32:28,895 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:32:28,896 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:32:28,896 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:32:28,896 - INFO - joeynmt.training - \tHypothesis: For they will not know that it is written , “ The city of the sun is like a man , and the waying on the sea . ”\n",
      "2021-08-04 07:32:28,896 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step     5800: bleu:   6.71, loss: 8212.8662, ppl:  21.2052, duration: 4.9338s\n",
      "2021-08-04 07:32:42,437 - INFO - joeynmt.training - Epoch  17, Step:     5900, Batch Loss:     3.285956, Tokens per Sec:     5278, Lr: 0.000300\n",
      "2021-08-04 07:32:53,535 - INFO - joeynmt.training - Epoch  17: total training loss 1007.15\n",
      "2021-08-04 07:32:53,535 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-08-04 07:32:55,535 - INFO - joeynmt.training - Epoch  18, Step:     6000, Batch Loss:     2.961579, Tokens per Sec:     5583, Lr: 0.000300\n",
      "2021-08-04 07:32:58,745 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:32:58,745 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:33:00,048 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:33:00,049 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:33:00,050 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:33:00,050 - INFO - joeynmt.training - \tHypothesis: For if the love of the love of the love of the flesh , do not be saved .\n",
      "2021-08-04 07:33:00,050 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:33:00,051 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:33:00,051 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:33:00,051 - INFO - joeynmt.training - \tHypothesis: For you are not of the Lord , but you are not of the Lord , but the Lord , but the things which are in you , but I have not seen the gospel of the Lord .\n",
      "2021-08-04 07:33:00,052 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:33:00,052 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:33:00,053 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:33:00,053 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let us be afraid , and your hearts be afraid . ”\n",
      "2021-08-04 07:33:00,053 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:33:00,054 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:33:00,054 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:33:00,054 - INFO - joeynmt.training - \tHypothesis: For if they had come , they will not be found , and the sun was a man , and the other fell down from the ground .\n",
      "2021-08-04 07:33:00,055 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     6000: bleu:   6.11, loss: 8147.4629, ppl:  20.6956, duration: 4.5186s\n",
      "2021-08-04 07:33:13,555 - INFO - joeynmt.training - Epoch  18, Step:     6100, Batch Loss:     2.935606, Tokens per Sec:     5312, Lr: 0.000300\n",
      "2021-08-04 07:33:26,742 - INFO - joeynmt.training - Epoch  18, Step:     6200, Batch Loss:     2.855085, Tokens per Sec:     5437, Lr: 0.000300\n",
      "2021-08-04 07:33:29,963 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:33:29,964 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:33:30,816 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:33:30,818 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:33:30,818 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:33:30,818 - INFO - joeynmt.training - \tHypothesis: For if your love is not worthy of your members , but your members is not worthy .\n",
      "2021-08-04 07:33:30,818 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:33:30,819 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:33:30,819 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:33:30,820 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , brethren , brethren , brethren , not only in the Lord , but in the Lord , but you will be judged .\n",
      "2021-08-04 07:33:30,820 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:33:30,821 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:33:30,821 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:33:30,822 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Let your hearts be afraid , and your children be with the Pharisees . ”\n",
      "2021-08-04 07:33:30,822 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:33:30,823 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:33:30,824 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:33:30,824 - INFO - joeynmt.training - \tHypothesis: For if they had come to them , they will not be afraid , but the rock , and the bridegroom was taken away .\n",
      "2021-08-04 07:33:30,825 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     6200: bleu:   6.13, loss: 8113.3164, ppl:  20.4345, duration: 4.0821s\n",
      "2021-08-04 07:33:44,326 - INFO - joeynmt.training - Epoch  18, Step:     6300, Batch Loss:     2.951393, Tokens per Sec:     5168, Lr: 0.000300\n",
      "2021-08-04 07:33:49,412 - INFO - joeynmt.training - Epoch  18: total training loss 997.63\n",
      "2021-08-04 07:33:49,412 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-08-04 07:33:57,421 - INFO - joeynmt.training - Epoch  19, Step:     6400, Batch Loss:     2.866093, Tokens per Sec:     5416, Lr: 0.000300\n",
      "2021-08-04 07:34:01,751 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:34:01,752 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:34:02,594 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:34:02,595 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:34:02,595 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:34:02,596 - INFO - joeynmt.training - \tHypothesis: For the love of the love of the love of the flesh ,\n",
      "2021-08-04 07:34:02,596 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:34:02,597 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:34:02,597 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:34:02,597 - INFO - joeynmt.training - \tHypothesis: For you are not of the Lord , but you are not of the Lord , but you are not of the Lord .\n",
      "2021-08-04 07:34:02,597 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:34:02,598 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:34:02,598 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:34:02,598 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to the Pharisees , and you are willing to eat the Pharisees . ”\n",
      "2021-08-04 07:34:02,599 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:34:02,599 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:34:02,600 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:34:02,600 - INFO - joeynmt.training - \tHypothesis: For the things which are not lawful for the sun , but the sun was not like a man , but the other day .\n",
      "2021-08-04 07:34:02,600 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     6400: bleu:   6.65, loss: 8086.1309, ppl:  20.2289, duration: 5.1784s\n",
      "2021-08-04 07:34:15,978 - INFO - joeynmt.training - Epoch  19, Step:     6500, Batch Loss:     2.824945, Tokens per Sec:     5290, Lr: 0.000300\n",
      "2021-08-04 07:34:29,026 - INFO - joeynmt.training - Epoch  19, Step:     6600, Batch Loss:     2.808777, Tokens per Sec:     5434, Lr: 0.000300\n",
      "2021-08-04 07:34:32,518 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:34:32,518 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:34:33,351 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:34:33,352 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:34:33,353 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:34:33,353 - INFO - joeynmt.training - \tHypothesis: For not love love , but to be uncleanful .\n",
      "2021-08-04 07:34:33,353 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:34:33,354 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:34:33,354 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:34:33,354 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , do not be with you , but the Lord of the Lord , but you will not be judged by the Lord .\n",
      "2021-08-04 07:34:33,355 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:34:33,355 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:34:33,355 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:34:33,356 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Let us go into your hearts , and be afraid . ”\n",
      "2021-08-04 07:34:33,356 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:34:33,357 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:34:33,357 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:34:33,357 - INFO - joeynmt.training - \tHypothesis: For they did not have been given to them , but they were afraid , and they were afraid , saying , “ Sir , we shall be taken into the city of the city . ”\n",
      "2021-08-04 07:34:33,357 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     6600: bleu:   6.73, loss: 8074.5654, ppl:  20.1421, duration: 4.3307s\n",
      "2021-08-04 07:34:45,828 - INFO - joeynmt.training - Epoch  19: total training loss 978.10\n",
      "2021-08-04 07:34:45,829 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-08-04 07:34:46,906 - INFO - joeynmt.training - Epoch  20, Step:     6700, Batch Loss:     2.666465, Tokens per Sec:     5329, Lr: 0.000300\n",
      "2021-08-04 07:35:00,027 - INFO - joeynmt.training - Epoch  20, Step:     6800, Batch Loss:     3.020275, Tokens per Sec:     5440, Lr: 0.000300\n",
      "2021-08-04 07:35:03,393 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:35:03,394 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:35:04,239 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:35:04,241 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:35:04,241 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:35:04,241 - INFO - joeynmt.training - \tHypothesis: For love love , not in love , but in uncleanness ,\n",
      "2021-08-04 07:35:04,242 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:35:04,242 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:35:04,243 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:35:04,243 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , brethren , do not be with you , but to the Lord , but to you who are in the Lord , but to you who are in the Lord .\n",
      "2021-08-04 07:35:04,243 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:35:04,244 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:35:04,244 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:35:04,245 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to your bed , and be afraid . ”\n",
      "2021-08-04 07:35:04,245 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:35:04,245 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:35:04,246 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:35:04,246 - INFO - joeynmt.training - \tHypothesis: For they did not see that it was no one who had been done , but the other fell on the ground was at the road . And when they had come , they were greatly .\n",
      "2021-08-04 07:35:04,246 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     6800: bleu:   7.32, loss: 8008.5073, ppl:  19.6533, duration: 4.2189s\n",
      "2021-08-04 07:35:17,771 - INFO - joeynmt.training - Epoch  20, Step:     6900, Batch Loss:     2.761844, Tokens per Sec:     5258, Lr: 0.000300\n",
      "2021-08-04 07:35:30,949 - INFO - joeynmt.training - Epoch  20, Step:     7000, Batch Loss:     2.424229, Tokens per Sec:     5425, Lr: 0.000300\n",
      "2021-08-04 07:35:35,877 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:35:35,878 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:35:35,878 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:35:35,879 - INFO - joeynmt.training - \tHypothesis: For the love of faith is not in the love of the flesh .\n",
      "2021-08-04 07:35:35,879 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:35:35,879 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:35:35,880 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:35:35,880 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , brethren , do not be with you , but to the Lord , but to you who are in the day of the Lord , but you will be judged .\n",
      "2021-08-04 07:35:35,880 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:35:35,881 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:35:35,881 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:35:35,883 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , for yourselves yourselves to the Pharisees , for you are of the Pharisees . ”\n",
      "2021-08-04 07:35:35,883 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:35:35,884 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:35:35,884 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:35:35,884 - INFO - joeynmt.training - \tHypothesis: For they did not see it , but it was a man , that it might be done by the lying of the ground , that it might be done by the city .\n",
      "2021-08-04 07:35:35,884 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     7000: bleu:   6.52, loss: 8028.6914, ppl:  19.8014, duration: 4.9344s\n",
      "2021-08-04 07:35:41,826 - INFO - joeynmt.training - Epoch  20: total training loss 963.74\n",
      "2021-08-04 07:35:41,826 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-08-04 07:35:49,491 - INFO - joeynmt.training - Epoch  21, Step:     7100, Batch Loss:     2.567078, Tokens per Sec:     5064, Lr: 0.000300\n",
      "2021-08-04 07:36:02,655 - INFO - joeynmt.training - Epoch  21, Step:     7200, Batch Loss:     2.646929, Tokens per Sec:     5491, Lr: 0.000300\n",
      "2021-08-04 07:36:06,931 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:36:06,932 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:36:07,895 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:36:07,896 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:36:07,896 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:36:07,897 - INFO - joeynmt.training - \tHypothesis: For not love one another , but to be uncircumcised .\n",
      "2021-08-04 07:36:07,897 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:36:07,898 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:36:07,898 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:36:07,898 - INFO - joeynmt.training - \tHypothesis: But you , brethren , brethren , do not be a minister of the Lord , but you will be a minister of the Lord , but you will be a minister of the Lord .\n",
      "2021-08-04 07:36:07,898 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:36:07,899 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:36:07,899 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:36:07,899 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ Take heed to yourselves to yourselves , for they are of the Pharisees . ”\n",
      "2021-08-04 07:36:07,900 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:36:07,900 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:36:07,900 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:36:07,901 - INFO - joeynmt.training - \tHypothesis: For if they do not come to the ground , they shall be found , and the bridegroom was groom . And when they had come to Him .\n",
      "2021-08-04 07:36:07,901 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step     7200: bleu:   6.88, loss: 7997.0898, ppl:  19.5701, duration: 5.2456s\n",
      "2021-08-04 07:36:21,457 - INFO - joeynmt.training - Epoch  21, Step:     7300, Batch Loss:     2.817942, Tokens per Sec:     5208, Lr: 0.000300\n",
      "2021-08-04 07:36:34,201 - INFO - joeynmt.training - Epoch  21: total training loss 943.51\n",
      "2021-08-04 07:36:34,201 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-08-04 07:36:34,612 - INFO - joeynmt.training - Epoch  22, Step:     7400, Batch Loss:     2.558146, Tokens per Sec:     4938, Lr: 0.000300\n",
      "2021-08-04 07:36:39,211 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:36:39,211 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:36:40,049 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:36:40,051 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:36:40,051 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:36:40,051 - INFO - joeynmt.training - \tHypothesis: For in love , not in love , but in uncleanness ,\n",
      "2021-08-04 07:36:40,051 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:36:40,052 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:36:40,052 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:36:40,052 - INFO - joeynmt.training - \tHypothesis: For you are not of the brethren , but the Lord , and the Lord , and the word of the Lord , but the things which you have not seen , but the gospel of God .\n",
      "2021-08-04 07:36:40,053 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:36:40,053 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:36:40,054 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:36:40,054 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to the Pharisees , and to the Pharisees , and to the Pharisees . ”\n",
      "2021-08-04 07:36:40,054 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:36:40,055 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:36:40,055 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:36:40,055 - INFO - joeynmt.training - \tHypothesis: For if they did not see it , it was no longer , but the birth of the wages was not lawful to eat . ”\n",
      "2021-08-04 07:36:40,055 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step     7400: bleu:   6.27, loss: 7889.9375, ppl:  18.8056, duration: 5.4428s\n",
      "2021-08-04 07:36:53,609 - INFO - joeynmt.training - Epoch  22, Step:     7500, Batch Loss:     2.566214, Tokens per Sec:     5220, Lr: 0.000300\n",
      "2021-08-04 07:37:06,644 - INFO - joeynmt.training - Epoch  22, Step:     7600, Batch Loss:     2.747147, Tokens per Sec:     5436, Lr: 0.000300\n",
      "2021-08-04 07:37:12,067 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:37:12,068 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:37:12,068 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:37:12,068 - INFO - joeynmt.training - \tHypothesis: For in love , not in the love of the flesh , but in the flesh .\n",
      "2021-08-04 07:37:12,068 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:37:12,069 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:37:12,069 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:37:12,069 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , brethren , do not be with you , but the Lord of you have been made to me , but I have no longer a minister of the Lord .\n",
      "2021-08-04 07:37:12,070 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:37:12,070 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:37:12,070 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:37:12,071 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ Take heed to yourselves , and be brought to the Pharisees . ”\n",
      "2021-08-04 07:37:12,071 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:37:12,072 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:37:12,072 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:37:12,072 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was a man , but the birth hour was a man , and the birth hour was taken away . ”\n",
      "2021-08-04 07:37:12,072 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step     7600: bleu:   6.61, loss: 7943.0352, ppl:  19.1806, duration: 5.4275s\n",
      "2021-08-04 07:37:25,593 - INFO - joeynmt.training - Epoch  22, Step:     7700, Batch Loss:     2.558851, Tokens per Sec:     5363, Lr: 0.000300\n",
      "2021-08-04 07:37:32,248 - INFO - joeynmt.training - Epoch  22: total training loss 936.14\n",
      "2021-08-04 07:37:32,249 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-08-04 07:37:38,718 - INFO - joeynmt.training - Epoch  23, Step:     7800, Batch Loss:     2.394051, Tokens per Sec:     5536, Lr: 0.000300\n",
      "2021-08-04 07:37:42,466 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:37:42,467 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:37:43,349 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:37:43,350 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:37:43,350 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:37:43,350 - INFO - joeynmt.training - \tHypothesis: For in love , not in love , but in uncircumcision .\n",
      "2021-08-04 07:37:43,351 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:37:43,351 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:37:43,351 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:37:43,352 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , do not be a bondservant of the Lord , but the Lord of you have not seen the things which you have not seen , but now now now you have been given to you .\n",
      "2021-08-04 07:37:43,352 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:37:43,353 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:37:43,353 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:37:43,353 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ Let your eyes be troubled , and your way be brought to the Pharisees . ”\n",
      "2021-08-04 07:37:43,353 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:37:43,354 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:37:43,354 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:37:43,354 - INFO - joeynmt.training - \tHypothesis: For they did not have been given to them , but it was a man to the centurion , that it was no more than that was taken away from the field .\n",
      "2021-08-04 07:37:43,355 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step     7800: bleu:   6.93, loss: 7886.8906, ppl:  18.7843, duration: 4.6356s\n",
      "2021-08-04 07:37:56,881 - INFO - joeynmt.training - Epoch  23, Step:     7900, Batch Loss:     2.674270, Tokens per Sec:     5259, Lr: 0.000300\n",
      "2021-08-04 07:38:10,007 - INFO - joeynmt.training - Epoch  23, Step:     8000, Batch Loss:     2.413260, Tokens per Sec:     5428, Lr: 0.000300\n",
      "2021-08-04 07:38:13,891 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:38:13,892 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:38:13,892 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:38:13,893 - INFO - joeynmt.training - \tHypothesis: For in love , not in love , nor uncircumcised .\n",
      "2021-08-04 07:38:13,893 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:38:13,894 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:38:13,894 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:38:13,894 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , do not be with you , but the Lord , and you have no more than the same .\n",
      "2021-08-04 07:38:13,894 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:38:13,895 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:38:13,895 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:38:13,895 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to your hearts , and be afraid . ”\n",
      "2021-08-04 07:38:13,896 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:38:13,896 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:38:13,897 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:38:13,898 - INFO - joeynmt.training - \tHypothesis: For they will not be made with a man , but the other other other stones who had been taken away from the ground . And when we had come , we would be taken . ”\n",
      "2021-08-04 07:38:13,898 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step     8000: bleu:   7.38, loss: 7887.5625, ppl:  18.7890, duration: 3.8901s\n",
      "2021-08-04 07:38:27,381 - INFO - joeynmt.training - Epoch  23, Step:     8100, Batch Loss:     2.578842, Tokens per Sec:     5340, Lr: 0.000300\n",
      "2021-08-04 07:38:27,646 - INFO - joeynmt.training - Epoch  23: total training loss 914.08\n",
      "2021-08-04 07:38:27,646 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-08-04 07:38:40,552 - INFO - joeynmt.training - Epoch  24, Step:     8200, Batch Loss:     2.387036, Tokens per Sec:     5449, Lr: 0.000300\n",
      "2021-08-04 07:38:44,005 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:38:44,005 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:38:44,876 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:38:44,877 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:38:44,877 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:38:44,877 - INFO - joeynmt.training - \tHypothesis: For in love , not in vain , nor in vain .\n",
      "2021-08-04 07:38:44,878 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:38:44,878 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:38:44,878 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:38:44,879 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , I do not be judged by the Lord , but the things which you have not seen the Lord has not seen , but the end of you .\n",
      "2021-08-04 07:38:44,879 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:38:44,880 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:38:44,880 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:38:44,880 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ Take heed to the Pharisees , and be afraid , and be afraid . ”\n",
      "2021-08-04 07:38:44,881 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:38:44,881 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:38:44,882 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:38:44,882 - INFO - joeynmt.training - \tHypothesis: For they will not be afraid , but the other other , but the wages was taken away from the ground . And when we had come , we will be taken away . ”\n",
      "2021-08-04 07:38:44,882 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step     8200: bleu:   6.68, loss: 7852.5273, ppl:  18.5457, duration: 4.3298s\n",
      "2021-08-04 07:38:58,415 - INFO - joeynmt.training - Epoch  24, Step:     8300, Batch Loss:     2.587231, Tokens per Sec:     5193, Lr: 0.000300\n",
      "2021-08-04 07:39:11,568 - INFO - joeynmt.training - Epoch  24, Step:     8400, Batch Loss:     2.530558, Tokens per Sec:     5549, Lr: 0.000300\n",
      "2021-08-04 07:39:14,845 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:39:14,846 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:39:15,641 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:39:15,642 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:39:15,642 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:39:15,643 - INFO - joeynmt.training - \tHypothesis: For not love one another , but in ununclearness .\n",
      "2021-08-04 07:39:15,645 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:39:15,646 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:39:15,646 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:39:15,646 - INFO - joeynmt.training - \tHypothesis: For you have not a beloved brethren , but the Lord has not seen the things which you have seen , but the things which are in you have been done .\n",
      "2021-08-04 07:39:15,646 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:39:15,647 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:39:15,647 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:39:15,648 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to yourselves , and be brought to the Pharisees . ”\n",
      "2021-08-04 07:39:15,648 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:39:15,648 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:39:15,649 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:39:15,649 - INFO - joeynmt.training - \tHypothesis: For when they had come to them , they were afraid , and they were afraid , and they were afraid . And the third day was like a great day .\n",
      "2021-08-04 07:39:15,649 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step     8400: bleu:   6.70, loss: 7828.6777, ppl:  18.3820, duration: 4.0810s\n",
      "2021-08-04 07:39:22,966 - INFO - joeynmt.training - Epoch  24: total training loss 902.50\n",
      "2021-08-04 07:39:22,967 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-08-04 07:39:29,173 - INFO - joeynmt.training - Epoch  25, Step:     8500, Batch Loss:     2.482826, Tokens per Sec:     5403, Lr: 0.000300\n",
      "2021-08-04 07:39:42,357 - INFO - joeynmt.training - Epoch  25, Step:     8600, Batch Loss:     2.672074, Tokens per Sec:     5473, Lr: 0.000300\n",
      "2021-08-04 07:39:45,926 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:39:45,926 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:39:47,216 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:39:47,218 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:39:47,218 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:39:47,218 - INFO - joeynmt.training - \tHypothesis: For not love one another , or uncleanness ,\n",
      "2021-08-04 07:39:47,219 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:39:47,220 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:39:47,220 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:39:47,220 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , do not be in the Lord , but the Lord , but the things which you have not seen , but now you have not seen .\n",
      "2021-08-04 07:39:47,220 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:39:47,221 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:39:47,222 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:39:47,222 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to yourselves , be brought to the Pharisees and to the Pharisees . ”\n",
      "2021-08-04 07:39:47,222 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:39:47,223 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:39:47,223 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:39:47,224 - INFO - joeynmt.training - \tHypothesis: For they did not see this time , but it was not lawful for a man to enter the ground . And when we had come to the place , we would have come . ”\n",
      "2021-08-04 07:39:47,224 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step     8600: bleu:   7.96, loss: 7822.1094, ppl:  18.3371, duration: 4.8661s\n",
      "2021-08-04 07:40:00,729 - INFO - joeynmt.training - Epoch  25, Step:     8700, Batch Loss:     2.332713, Tokens per Sec:     5188, Lr: 0.000300\n",
      "2021-08-04 07:40:13,890 - INFO - joeynmt.training - Epoch  25, Step:     8800, Batch Loss:     2.708917, Tokens per Sec:     5444, Lr: 0.000300\n",
      "2021-08-04 07:40:18,496 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:40:18,496 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:40:19,331 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:40:19,332 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:40:19,332 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:40:19,333 - INFO - joeynmt.training - \tHypothesis: For not love one another , but in uncleanness ,\n",
      "2021-08-04 07:40:19,333 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:40:19,333 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:40:19,334 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:40:19,334 - INFO - joeynmt.training - \tHypothesis: For you are not worthy of the Lord , but you are not worthy of the Lord . For you are not a man , but the things which you have not seen .\n",
      "2021-08-04 07:40:19,334 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:40:19,335 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:40:19,335 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:40:19,335 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ Take heed to your way , and to the Pharisees , and to the Pharisees . ”\n",
      "2021-08-04 07:40:19,336 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:40:19,336 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:40:19,336 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:40:19,337 - INFO - joeynmt.training - \tHypothesis: For they did not see it , but they did not enter the village , but the wind was at the night .\n",
      "2021-08-04 07:40:19,337 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step     8800: bleu:   8.10, loss: 7753.3613, ppl:  17.8743, duration: 5.4458s\n",
      "2021-08-04 07:40:20,138 - INFO - joeynmt.training - Epoch  25: total training loss 889.36\n",
      "2021-08-04 07:40:20,139 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-08-04 07:40:32,917 - INFO - joeynmt.training - Epoch  26, Step:     8900, Batch Loss:     2.901957, Tokens per Sec:     5222, Lr: 0.000300\n",
      "2021-08-04 07:40:45,952 - INFO - joeynmt.training - Epoch  26, Step:     9000, Batch Loss:     1.907607, Tokens per Sec:     5541, Lr: 0.000300\n",
      "2021-08-04 07:40:52,891 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:40:52,892 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:40:52,892 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:40:52,892 - INFO - joeynmt.training - \tHypothesis: For all love , not to be uncleanness , but to disputes .\n",
      "2021-08-04 07:40:52,892 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:40:52,893 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:40:52,893 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:40:52,894 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , brethren , do not be afraid , but the Lord , but the things which you have not seen the Lord , but the things which you have not seen .\n",
      "2021-08-04 07:40:52,894 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:40:52,895 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:40:52,895 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:40:52,895 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and to be afraid , for you are going to eat the Pharisees . ”\n",
      "2021-08-04 07:40:52,895 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:40:52,896 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:40:52,896 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:40:52,896 - INFO - joeynmt.training - \tHypothesis: For they will not find a great multitude , but when they had come out of the fire , they would not enter the rock of the city . ”\n",
      "2021-08-04 07:40:52,897 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step     9000: bleu:   6.94, loss: 7782.0840, ppl:  18.0662, duration: 6.9441s\n",
      "2021-08-04 07:41:06,471 - INFO - joeynmt.training - Epoch  26, Step:     9100, Batch Loss:     2.939843, Tokens per Sec:     5357, Lr: 0.000300\n",
      "2021-08-04 07:41:13,776 - INFO - joeynmt.training - Epoch  26: total training loss 872.98\n",
      "2021-08-04 07:41:13,777 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-08-04 07:41:19,536 - INFO - joeynmt.training - Epoch  27, Step:     9200, Batch Loss:     2.188328, Tokens per Sec:     5538, Lr: 0.000300\n",
      "2021-08-04 07:41:24,770 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:41:24,771 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:41:24,772 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:41:24,772 - INFO - joeynmt.training - \tHypothesis: For not love one another , but in unbelief .\n",
      "2021-08-04 07:41:24,772 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:41:24,773 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:41:24,773 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:41:24,773 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , do not be strong , but the Lord , but the Lord of you will be judged .\n",
      "2021-08-04 07:41:24,774 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:41:24,774 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:41:24,774 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:41:24,775 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to the Pharisees and to the Pharisees , and to the Pharisees . ”\n",
      "2021-08-04 07:41:24,775 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:41:24,776 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:41:24,776 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:41:24,776 - INFO - joeynmt.training - \tHypothesis: For they did not enter it , but the rain that they should be put into the barrance . And when they had come out of the city , they would be put . ”\n",
      "2021-08-04 07:41:24,776 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step     9200: bleu:   8.37, loss: 7757.1055, ppl:  17.8992, duration: 5.2401s\n",
      "2021-08-04 07:41:38,272 - INFO - joeynmt.training - Epoch  27, Step:     9300, Batch Loss:     2.796053, Tokens per Sec:     5335, Lr: 0.000300\n",
      "2021-08-04 07:41:51,380 - INFO - joeynmt.training - Epoch  27, Step:     9400, Batch Loss:     2.433111, Tokens per Sec:     5501, Lr: 0.000300\n",
      "2021-08-04 07:41:56,144 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:41:56,144 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:41:57,011 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:41:57,012 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:41:57,013 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:41:57,013 - INFO - joeynmt.training - \tHypothesis: For love one another , not to be uncleanness ,\n",
      "2021-08-04 07:41:57,013 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:41:57,014 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:41:57,014 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:41:57,015 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , do not be with you , but to the Lord , but to you who are in the world , but to you who are not able to do good .\n",
      "2021-08-04 07:41:57,015 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:41:57,015 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:41:57,016 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:41:57,016 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to yourselves , and to the Pharisees , and to eat the Pharisees . ”\n",
      "2021-08-04 07:41:57,016 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:41:57,017 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:41:57,017 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:41:57,017 - INFO - joeynmt.training - \tHypothesis: For they did not enter the coath , but the copper came out of the ship . And when they had come out of the field , they would come . ”\n",
      "2021-08-04 07:41:57,018 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step     9400: bleu:   7.51, loss: 7701.1846, ppl:  17.5308, duration: 5.6372s\n",
      "2021-08-04 07:42:10,644 - INFO - joeynmt.training - Epoch  27, Step:     9500, Batch Loss:     2.404163, Tokens per Sec:     5247, Lr: 0.000300\n",
      "2021-08-04 07:42:11,426 - INFO - joeynmt.training - Epoch  27: total training loss 859.27\n",
      "2021-08-04 07:42:11,427 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-08-04 07:42:23,841 - INFO - joeynmt.training - Epoch  28, Step:     9600, Batch Loss:     2.161448, Tokens per Sec:     5404, Lr: 0.000300\n",
      "2021-08-04 07:42:28,266 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:42:28,267 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:42:28,267 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:42:28,267 - INFO - joeynmt.training - \tHypothesis: For love one another , not to be uncleanful .\n",
      "2021-08-04 07:42:28,268 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:42:28,268 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:42:28,269 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:42:28,269 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , do not be known to the Lord ; but if you have been born of the Lord , you will not have a short .\n",
      "2021-08-04 07:42:28,269 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:42:28,270 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:42:28,270 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:42:28,270 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to the Pharisees and to be afraid . ”\n",
      "2021-08-04 07:42:28,270 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:42:28,271 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:42:28,271 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:42:28,272 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was not lawful for a man to sow , but the night was lying .\n",
      "2021-08-04 07:42:28,272 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step     9600: bleu:   7.99, loss: 7711.3701, ppl:  17.5973, duration: 4.4308s\n",
      "2021-08-04 07:42:41,809 - INFO - joeynmt.training - Epoch  28, Step:     9700, Batch Loss:     2.414437, Tokens per Sec:     5136, Lr: 0.000300\n",
      "2021-08-04 07:42:54,937 - INFO - joeynmt.training - Epoch  28, Step:     9800, Batch Loss:     2.691032, Tokens per Sec:     5475, Lr: 0.000300\n",
      "2021-08-04 07:42:57,938 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:42:57,938 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:42:59,272 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:42:59,273 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:42:59,273 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:42:59,274 - INFO - joeynmt.training - \tHypothesis: For love is not in love , not in vain .\n",
      "2021-08-04 07:42:59,274 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:42:59,275 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:42:59,275 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:42:59,275 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , do not be afraid ; but now the Lord is coming , but you have not seen the things which you have been done .\n",
      "2021-08-04 07:42:59,275 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:42:59,276 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:42:59,276 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:42:59,276 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to yourselves , and to the Pharisees ! ”\n",
      "2021-08-04 07:42:59,277 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:42:59,277 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:42:59,278 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:42:59,278 - INFO - joeynmt.training - \tHypothesis: For they did not have been done in a place , but the wind was sitting at the ground . And when they had come out , they were afraid .\n",
      "2021-08-04 07:42:59,278 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step     9800: bleu:   7.50, loss: 7689.9014, ppl:  17.4574, duration: 4.3404s\n",
      "2021-08-04 07:43:07,310 - INFO - joeynmt.training - Epoch  28: total training loss 856.81\n",
      "2021-08-04 07:43:07,310 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-08-04 07:43:12,832 - INFO - joeynmt.training - Epoch  29, Step:     9900, Batch Loss:     2.075917, Tokens per Sec:     5278, Lr: 0.000300\n",
      "2021-08-04 07:43:25,974 - INFO - joeynmt.training - Epoch  29, Step:    10000, Batch Loss:     2.335994, Tokens per Sec:     5567, Lr: 0.000300\n",
      "2021-08-04 07:43:30,101 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:43:30,103 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:43:30,103 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:43:30,104 - INFO - joeynmt.training - \tHypothesis: For not love one another , not with one another .\n",
      "2021-08-04 07:43:30,104 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:43:30,104 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:43:30,105 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:43:30,105 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , my brethren , do not be trouble , but the Lord is coming of you .\n",
      "2021-08-04 07:43:30,105 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:43:30,106 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:43:30,106 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:43:30,106 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to the Pharisees and to the Pharisees . ”\n",
      "2021-08-04 07:43:30,107 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:43:30,107 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:43:30,108 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:43:30,108 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was not lawful for a man to go into the ground . And when we saw it , they would not be put . ”\n",
      "2021-08-04 07:43:30,108 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    10000: bleu:   8.96, loss: 7697.1289, ppl:  17.5044, duration: 4.1334s\n",
      "2021-08-04 07:43:43,652 - INFO - joeynmt.training - Epoch  29, Step:    10100, Batch Loss:     2.272936, Tokens per Sec:     5266, Lr: 0.000300\n",
      "2021-08-04 07:43:56,666 - INFO - joeynmt.training - Epoch  29, Step:    10200, Batch Loss:     2.168725, Tokens per Sec:     5418, Lr: 0.000300\n",
      "2021-08-04 07:44:01,544 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:44:01,544 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:44:02,391 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:44:02,392 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:44:02,392 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:44:02,392 - INFO - joeynmt.training - \tHypothesis: For love , not in love , but in ungodly ,\n",
      "2021-08-04 07:44:02,392 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:44:02,393 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:44:02,393 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:44:02,394 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , do not be afraid of the Lord , but the Lord of you have no need of you .\n",
      "2021-08-04 07:44:02,394 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:44:02,395 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:44:02,395 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:44:02,395 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to yourselves , and be afraid , and be afraid of the Pharisees . ”\n",
      "2021-08-04 07:44:02,395 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:44:02,396 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:44:02,396 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:44:02,397 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was a man in a village , but a long time was opened .\n",
      "2021-08-04 07:44:02,399 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    10200: bleu:   8.12, loss: 7612.8062, ppl:  16.9640, duration: 5.7320s\n",
      "2021-08-04 07:44:03,765 - INFO - joeynmt.training - Epoch  29: total training loss 837.40\n",
      "2021-08-04 07:44:03,765 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-08-04 07:44:15,964 - INFO - joeynmt.training - Epoch  30, Step:    10300, Batch Loss:     2.153863, Tokens per Sec:     5235, Lr: 0.000300\n",
      "2021-08-04 07:44:29,018 - INFO - joeynmt.training - Epoch  30, Step:    10400, Batch Loss:     2.234634, Tokens per Sec:     5541, Lr: 0.000300\n",
      "2021-08-04 07:44:32,979 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:44:32,980 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:44:32,981 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:44:32,981 - INFO - joeynmt.training - \tHypothesis: For in love , not in love ,\n",
      "2021-08-04 07:44:32,981 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:44:32,982 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:44:32,982 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:44:32,982 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , do not be in the Lord , but also in the Lord . But if you do not have a time , you will be found .\n",
      "2021-08-04 07:44:32,983 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:44:32,983 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:44:32,984 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:44:32,984 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be afraid , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:44:32,984 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:44:32,985 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:44:32,985 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:44:32,985 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was not given to them to the place , but the wind was opened .\n",
      "2021-08-04 07:44:32,986 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    10400: bleu:   9.51, loss: 7663.3164, ppl:  17.2856, duration: 3.9666s\n",
      "2021-08-04 07:44:46,437 - INFO - joeynmt.training - Epoch  30, Step:    10500, Batch Loss:     2.573183, Tokens per Sec:     5288, Lr: 0.000300\n",
      "2021-08-04 07:44:54,749 - INFO - joeynmt.training - Epoch  30: total training loss 835.72\n",
      "2021-08-04 07:44:54,750 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-08-04 07:44:54,750 - INFO - joeynmt.training - Best validation result (greedy) at step    10200:  16.96 ppl.\n",
      "2021-08-04 07:44:54,776 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-08-04 07:44:55,246 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 07:44:55,486 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 07:44:55,560 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/dev.bpe.en)...\n",
      "2021-08-04 07:45:00,703 - INFO - joeynmt.prediction -  dev bleu[13a]:   9.01 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-04 07:45:00,708 - INFO - joeynmt.prediction - Translations saved to: models/lhen_reverse_transformer/00010200.hyps.dev\n",
      "2021-08-04 07:45:00,709 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/test.bpe.en)...\n",
      "2021-08-04 07:45:05,471 - INFO - joeynmt.prediction - test bleu[13a]:   9.18 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-04 07:45:05,476 - INFO - joeynmt.prediction - Translations saved to: models/lhen_reverse_transformer/00010200.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt3$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKD3TP1cevE4",
    "outputId": "e8816080-36d4-44d5-a3b6-df6d75212d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 200\tLoss: 13427.67969\tPPL: 147.46030\tbleu: 0.12029\tLR: 0.00030000\t*\n",
      "Steps: 400\tLoss: 12647.71094\tPPL: 110.33264\tbleu: 0.03086\tLR: 0.00030000\t*\n",
      "Steps: 600\tLoss: 11534.63281\tPPL: 72.93453\tbleu: 0.54131\tLR: 0.00030000\t*\n",
      "Steps: 800\tLoss: 11055.41406\tPPL: 61.02891\tbleu: 0.44899\tLR: 0.00030000\t*\n",
      "Steps: 1000\tLoss: 10761.14355\tPPL: 54.70267\tbleu: 1.30696\tLR: 0.00030000\t*\n",
      "Steps: 1200\tLoss: 10515.99219\tPPL: 49.93612\tbleu: 1.08567\tLR: 0.00030000\t*\n",
      "Steps: 1400\tLoss: 10267.27930\tPPL: 45.52456\tbleu: 1.75825\tLR: 0.00030000\t*\n",
      "Steps: 1600\tLoss: 10147.88672\tPPL: 43.54746\tbleu: 1.83333\tLR: 0.00030000\t*\n",
      "Steps: 1800\tLoss: 9945.24609\tPPL: 40.38637\tbleu: 2.22384\tLR: 0.00030000\t*\n",
      "Steps: 2000\tLoss: 9796.03613\tPPL: 38.20642\tbleu: 2.90277\tLR: 0.00030000\t*\n",
      "Steps: 2200\tLoss: 9678.15332\tPPL: 36.56767\tbleu: 3.10603\tLR: 0.00030000\t*\n",
      "Steps: 2400\tLoss: 9538.60742\tPPL: 34.71838\tbleu: 2.35361\tLR: 0.00030000\t*\n",
      "Steps: 2600\tLoss: 9480.06152\tPPL: 33.97065\tbleu: 3.59264\tLR: 0.00030000\t*\n",
      "Steps: 2800\tLoss: 9280.22656\tPPL: 31.53763\tbleu: 4.17571\tLR: 0.00030000\t*\n",
      "Steps: 3000\tLoss: 9160.76953\tPPL: 30.16725\tbleu: 3.72263\tLR: 0.00030000\t*\n",
      "Steps: 3200\tLoss: 9103.09082\tPPL: 29.52706\tbleu: 4.14206\tLR: 0.00030000\t*\n",
      "Steps: 3400\tLoss: 8974.23633\tPPL: 28.14552\tbleu: 4.42792\tLR: 0.00030000\t*\n",
      "Steps: 3600\tLoss: 8884.97070\tPPL: 27.22652\tbleu: 4.21096\tLR: 0.00030000\t*\n",
      "Steps: 3800\tLoss: 8826.97363\tPPL: 26.64557\tbleu: 5.53513\tLR: 0.00030000\t*\n",
      "Steps: 4000\tLoss: 8744.02832\tPPL: 25.83621\tbleu: 4.70829\tLR: 0.00030000\t*\n",
      "Steps: 4200\tLoss: 8621.16895\tPPL: 24.68232\tbleu: 4.33577\tLR: 0.00030000\t*\n",
      "Steps: 4400\tLoss: 8571.13281\tPPL: 24.22729\tbleu: 4.71412\tLR: 0.00030000\t*\n",
      "Steps: 4600\tLoss: 8531.25488\tPPL: 23.87065\tbleu: 5.17668\tLR: 0.00030000\t*\n",
      "Steps: 4800\tLoss: 8429.55371\tPPL: 22.98469\tbleu: 4.93436\tLR: 0.00030000\t*\n",
      "Steps: 5000\tLoss: 8372.52930\tPPL: 22.50240\tbleu: 6.22137\tLR: 0.00030000\t*\n",
      "Steps: 5200\tLoss: 8353.71289\tPPL: 22.34549\tbleu: 5.11513\tLR: 0.00030000\t*\n",
      "Steps: 5400\tLoss: 8316.42188\tPPL: 22.03773\tbleu: 5.91251\tLR: 0.00030000\t*\n",
      "Steps: 5600\tLoss: 8237.65918\tPPL: 21.40160\tbleu: 5.80584\tLR: 0.00030000\t*\n",
      "Steps: 5800\tLoss: 8212.86621\tPPL: 21.20518\tbleu: 6.70708\tLR: 0.00030000\t*\n",
      "Steps: 6000\tLoss: 8147.46289\tPPL: 20.69564\tbleu: 6.11301\tLR: 0.00030000\t*\n",
      "Steps: 6200\tLoss: 8113.31641\tPPL: 20.43449\tbleu: 6.12895\tLR: 0.00030000\t*\n",
      "Steps: 6400\tLoss: 8086.13086\tPPL: 20.22894\tbleu: 6.65424\tLR: 0.00030000\t*\n",
      "Steps: 6600\tLoss: 8074.56543\tPPL: 20.14212\tbleu: 6.72630\tLR: 0.00030000\t*\n",
      "Steps: 6800\tLoss: 8008.50732\tPPL: 19.65334\tbleu: 7.32395\tLR: 0.00030000\t*\n",
      "Steps: 7000\tLoss: 8028.69141\tPPL: 19.80142\tbleu: 6.52310\tLR: 0.00030000\t\n",
      "Steps: 7200\tLoss: 7997.08984\tPPL: 19.57007\tbleu: 6.88230\tLR: 0.00030000\t*\n",
      "Steps: 7400\tLoss: 7889.93750\tPPL: 18.80557\tbleu: 6.26588\tLR: 0.00030000\t*\n",
      "Steps: 7600\tLoss: 7943.03516\tPPL: 19.18060\tbleu: 6.61129\tLR: 0.00030000\t\n",
      "Steps: 7800\tLoss: 7886.89062\tPPL: 18.78427\tbleu: 6.92908\tLR: 0.00030000\t*\n",
      "Steps: 8000\tLoss: 7887.56250\tPPL: 18.78897\tbleu: 7.38015\tLR: 0.00030000\t\n",
      "Steps: 8200\tLoss: 7852.52734\tPPL: 18.54575\tbleu: 6.67700\tLR: 0.00030000\t*\n",
      "Steps: 8400\tLoss: 7828.67773\tPPL: 18.38199\tbleu: 6.69813\tLR: 0.00030000\t*\n",
      "Steps: 8600\tLoss: 7822.10938\tPPL: 18.33714\tbleu: 7.95662\tLR: 0.00030000\t*\n",
      "Steps: 8800\tLoss: 7753.36133\tPPL: 17.87427\tbleu: 8.10024\tLR: 0.00030000\t*\n",
      "Steps: 9000\tLoss: 7782.08398\tPPL: 18.06622\tbleu: 6.94244\tLR: 0.00030000\t\n",
      "Steps: 9200\tLoss: 7757.10547\tPPL: 17.89917\tbleu: 8.36612\tLR: 0.00030000\t\n",
      "Steps: 9400\tLoss: 7701.18457\tPPL: 17.53079\tbleu: 7.50635\tLR: 0.00030000\t*\n",
      "Steps: 9600\tLoss: 7711.37012\tPPL: 17.59732\tbleu: 7.99190\tLR: 0.00030000\t\n",
      "Steps: 9800\tLoss: 7689.90137\tPPL: 17.45738\tbleu: 7.50289\tLR: 0.00030000\t*\n",
      "Steps: 10000\tLoss: 7697.12891\tPPL: 17.50437\tbleu: 8.95858\tLR: 0.00030000\t\n",
      "Steps: 10200\tLoss: 7612.80615\tPPL: 16.96397\tbleu: 8.12258\tLR: 0.00030000\t*\n",
      "Steps: 10400\tLoss: 7663.31641\tPPL: 17.28564\tbleu: 9.50869\tLR: 0.00030000\t\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lhen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "spYBzAmDEDlO"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 10400\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"{path}/models/lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"{path}/joeynmt/models/{name}_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lhen_reverse_transformer\"', f'model_dir: \"models/lhen_reverse_transformer_continued\"')\n",
    "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "W0rEiN4PyzXo",
    "outputId": "49ec738b-bc44-40a4-d5f7-c8c75f299196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/joeynmt/models/lhen_reverse_transformer/10400.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"          \n",
      "    patience: 5                     \n",
      "    learning_rate_factor: 0.5       \n",
      "    learning_rate_warmup: 1000      \n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 1096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  \n",
      "    validation_freq: 200         # Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lhen_reverse_transformer_continued\"\n",
      "    overwrite: True             # Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             \n",
      "        embeddings:\n",
      "            embedding_dim: 256   \n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         \n",
      "        ff_size: 1024            \n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              \n",
      "        embeddings:\n",
      "            embedding_dim: 256    \n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         \n",
      "        ff_size: 1024            \n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_lhen_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tM-9uxZFigO",
    "outputId": "b95afe7d-45a3-4b44-9eb8-b6af9447d425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-04 07:46:24,224 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-04 07:46:24,251 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-04 07:46:24,365 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-04 07:46:24,636 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-04 07:46:24,640 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-04 07:46:24,648 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-04 07:46:24,649 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 07:46:24,903 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 07:46:25.095565: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 07:46:26,812 - INFO - joeynmt.training - Total params: 12099840\n",
      "2021-08-04 07:46:28,927 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/joeynmt/models/lhen_reverse_transformer/10400.ckpt\n",
      "2021-08-04 07:46:29,511 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-08-04 07:46:29,512 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-08-04 07:46:29,512 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-04 07:46:29,512 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/train.bpe\n",
      "2021-08-04 07:46:29,513 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/dev.bpe\n",
      "2021-08-04 07:46:29,513 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/test.bpe\n",
      "2021-08-04 07:46:29,513 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-04 07:46:29,513 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-04 07:46:29,514 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-04 07:46:29,514 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/vocab.txt\n",
      "2021-08-04 07:46:29,514 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/vocab.txt\n",
      "2021-08-04 07:46:29,514 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-04 07:46:29,514 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-04 07:46:29,515 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhya/joeynmt/models/lhen_reverse_transformer/10400.ckpt\n",
      "2021-08-04 07:46:29,515 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-04 07:46:29,515 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-04 07:46:29,515 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-04 07:46:29,516 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-04 07:46:29,516 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-04 07:46:29,516 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-04 07:46:29,516 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-04 07:46:29,517 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-04 07:46:29,517 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-04 07:46:29,517 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-04 07:46:29,517 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-04 07:46:29,518 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-04 07:46:29,518 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-04 07:46:29,518 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-04 07:46:29,518 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1096\n",
      "2021-08-04 07:46:29,519 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-04 07:46:29,519 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-08-04 07:46:29,519 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-04 07:46:29,519 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-04 07:46:29,519 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-04 07:46:29,520 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-04 07:46:29,520 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 200\n",
      "2021-08-04 07:46:29,520 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-08-04 07:46:29,520 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-04 07:46:29,521 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lhen_reverse_transformer_continued\n",
      "2021-08-04 07:46:29,521 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-04 07:46:29,521 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-04 07:46:29,521 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-04 07:46:29,522 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-04 07:46:29,522 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-04 07:46:29,522 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-04 07:46:29,522 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-04 07:46:29,523 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-04 07:46:29,523 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-04 07:46:29,523 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-04 07:46:29,523 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-04 07:46:29,523 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-04 07:46:29,524 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-04 07:46:29,524 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-04 07:46:29,524 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-04 07:46:29,524 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-04 07:46:29,525 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 07:46:29,525 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-04 07:46:29,525 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-04 07:46:29,525 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-04 07:46:29,526 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-04 07:46:29,526 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-04 07:46:29,526 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-04 07:46:29,526 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-04 07:46:29,527 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-04 07:46:29,527 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 07:46:29,527 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-04 07:46:29,527 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-04 07:46:29,528 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-04 07:46:29,528 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-04 07:46:29,528 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-04 07:46:29,528 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 7745,\n",
      "\tvalid 79,\n",
      "\ttest 79\n",
      "2021-08-04 07:46:29,528 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-08-04 07:46:29,529 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-08-04 07:46:29,529 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-08-04 07:46:29,529 - INFO - joeynmt.helpers - Number of Src words (types): 4061\n",
      "2021-08-04 07:46:29,529 - INFO - joeynmt.helpers - Number of Trg words (types): 4061\n",
      "2021-08-04 07:46:29,530 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4061),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4061))\n",
      "2021-08-04 07:46:29,543 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 1096\n",
      "\ttotal batch size (w. parallel & accumulation): 1096\n",
      "2021-08-04 07:46:29,543 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-04 07:46:42,942 - INFO - joeynmt.training - Epoch   1, Step:    10500, Batch Loss:     2.568332, Tokens per Sec:     5309, Lr: 0.000300\n",
      "2021-08-04 07:46:51,373 - INFO - joeynmt.training - Epoch   1: total training loss 390.90\n",
      "2021-08-04 07:46:51,374 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-04 07:46:56,255 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     2.042043, Tokens per Sec:     5527, Lr: 0.000300\n",
      "2021-08-04 07:47:00,508 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:47:00,509 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:47:00,510 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:47:00,510 - INFO - joeynmt.training - \tHypothesis: For love is not of one mind , not of evil .\n",
      "2021-08-04 07:47:00,510 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:47:00,511 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:47:00,511 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:47:00,511 - INFO - joeynmt.training - \tHypothesis: Brethren , my beloved , do not be afraid , but the Lord , that the things which is in you may be in the beginning , but that you may be a short time .\n",
      "2021-08-04 07:47:00,512 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:47:00,515 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:47:00,515 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:47:00,516 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be afraid , and be afraid of the Pharisees . ”\n",
      "2021-08-04 07:47:00,516 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:47:00,516 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:47:00,517 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:47:00,517 - INFO - joeynmt.training - \tHypothesis: For they did not enter it , but the centurion was a large large large large , but the coxen .\n",
      "2021-08-04 07:47:00,517 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    10600: bleu:   8.63, loss: 7617.0596, ppl:  16.9908, duration: 4.2622s\n",
      "2021-08-04 07:47:14,305 - INFO - joeynmt.training - Epoch   2, Step:    10700, Batch Loss:     1.893893, Tokens per Sec:     5053, Lr: 0.000300\n",
      "2021-08-04 07:47:27,610 - INFO - joeynmt.training - Epoch   2, Step:    10800, Batch Loss:     2.424374, Tokens per Sec:     5321, Lr: 0.000300\n",
      "2021-08-04 07:47:31,981 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:47:31,982 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:47:31,982 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:47:31,983 - INFO - joeynmt.training - \tHypothesis: For in love , not in the love of the flesh .\n",
      "2021-08-04 07:47:31,983 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:47:31,984 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:47:31,984 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:47:31,984 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , I do not be an apostle , but the Lord has need of you . But if you do not have a time , you will be a planner .\n",
      "2021-08-04 07:47:31,984 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:47:31,985 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:47:31,985 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:47:31,985 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be afraid , for your children are outside of the Pharisees . ”\n",
      "2021-08-04 07:47:31,986 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:47:31,986 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:47:31,987 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:47:31,987 - INFO - joeynmt.training - \tHypothesis: For they did not have been spread , but the copper came to the place , but the wind was near . ”\n",
      "2021-08-04 07:47:31,987 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    10800: bleu:   9.13, loss: 7638.0029, ppl:  17.1237, duration: 4.3768s\n",
      "2021-08-04 07:47:45,810 - INFO - joeynmt.training - Epoch   2, Step:    10900, Batch Loss:     1.885006, Tokens per Sec:     5264, Lr: 0.000300\n",
      "2021-08-04 07:47:48,023 - INFO - joeynmt.training - Epoch   2: total training loss 821.83\n",
      "2021-08-04 07:47:48,024 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-04 07:47:59,125 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     2.185342, Tokens per Sec:     5390, Lr: 0.000300\n",
      "2021-08-04 07:48:03,233 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:48:03,234 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:48:03,234 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:48:03,235 - INFO - joeynmt.training - \tHypothesis: For love is not of one another , or evil .\n",
      "2021-08-04 07:48:03,235 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:48:03,236 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:48:03,237 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:48:03,237 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I do not receive the Lord , but the Lord is coming to you , and not to the end of you .\n",
      "2021-08-04 07:48:03,237 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:48:03,238 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:48:03,238 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:48:03,238 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:48:03,238 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:48:03,239 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:48:03,239 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:48:03,240 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was a vision to sown place , but the wind was near .\n",
      "2021-08-04 07:48:03,240 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    11000: bleu:   8.35, loss: 7664.1328, ppl:  17.2909, duration: 4.1143s\n",
      "2021-08-04 07:48:16,836 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     2.050849, Tokens per Sec:     5223, Lr: 0.000300\n",
      "2021-08-04 07:48:30,059 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     2.368460, Tokens per Sec:     5284, Lr: 0.000300\n",
      "2021-08-04 07:48:33,615 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:48:33,615 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:48:34,960 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:48:34,961 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:48:34,961 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:48:34,962 - INFO - joeynmt.training - \tHypothesis: For no one is weak , or what is good .\n",
      "2021-08-04 07:48:34,962 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:48:34,963 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:48:34,963 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:48:34,963 - INFO - joeynmt.training - \tHypothesis: Therefore , brethren , be subject to you , not to the Lord ; but if you have been a preacher of the Lord , you would have no need of you .\n",
      "2021-08-04 07:48:34,964 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:48:34,964 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:48:34,965 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:48:34,965 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:48:34,965 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:48:34,966 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:48:34,966 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:48:34,967 - INFO - joeynmt.training - \tHypothesis: For they did not have a place in the place , but the coath which was taken away from the ground .\n",
      "2021-08-04 07:48:34,967 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    11200: bleu:   8.34, loss: 7594.6572, ppl:  16.8499, duration: 4.9076s\n",
      "2021-08-04 07:48:44,914 - INFO - joeynmt.training - Epoch   3: total training loss 814.25\n",
      "2021-08-04 07:48:44,914 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-04 07:48:48,650 - INFO - joeynmt.training - Epoch   4, Step:    11300, Batch Loss:     1.973167, Tokens per Sec:     5335, Lr: 0.000300\n",
      "2021-08-04 07:49:02,003 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     2.122440, Tokens per Sec:     5429, Lr: 0.000300\n",
      "2021-08-04 07:49:06,426 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:49:06,428 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:49:06,428 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:49:06,428 - INFO - joeynmt.training - \tHypothesis: For in love , not in deceitance ,\n",
      "2021-08-04 07:49:06,428 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:49:06,429 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:49:06,429 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:49:06,429 - INFO - joeynmt.training - \tHypothesis: Brethren , my brethren , do not be afraid ; but the Lord is coming , and you are not able to be like a short .\n",
      "2021-08-04 07:49:06,430 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:49:06,430 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:49:06,431 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:49:06,432 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to the Pharisees and to eat of the Pharisees and to eat . ”\n",
      "2021-08-04 07:49:06,432 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:49:06,433 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:49:06,433 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:49:06,433 - INFO - joeynmt.training - \tHypothesis: For they did not understand this , but that it was given to a place for a place , but to the seventh hour .\n",
      "2021-08-04 07:49:06,433 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    11400: bleu:   8.95, loss: 7604.8120, ppl:  16.9136, duration: 4.4293s\n",
      "2021-08-04 07:49:20,078 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     2.415215, Tokens per Sec:     5151, Lr: 0.000300\n",
      "2021-08-04 07:49:33,514 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     2.531660, Tokens per Sec:     5370, Lr: 0.000300\n",
      "2021-08-04 07:49:37,636 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:49:37,637 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:49:37,637 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:49:37,638 - INFO - joeynmt.training - \tHypothesis: For love is not of one another .\n",
      "2021-08-04 07:49:37,638 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:49:37,638 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:49:37,639 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:49:37,639 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , do not be troubled , but the Lord is coming of you .\n",
      "2021-08-04 07:49:37,639 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:49:37,640 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:49:37,640 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:49:37,640 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:49:37,641 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:49:37,641 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:49:37,642 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:49:37,642 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was a village , but the cox x x , but the south came down from the tomb .\n",
      "2021-08-04 07:49:37,642 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    11600: bleu:   9.36, loss: 7596.6074, ppl:  16.8621, duration: 4.1275s\n",
      "2021-08-04 07:49:40,716 - INFO - joeynmt.training - Epoch   4: total training loss 796.44\n",
      "2021-08-04 07:49:40,716 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-04 07:49:51,511 - INFO - joeynmt.training - Epoch   5, Step:    11700, Batch Loss:     2.574303, Tokens per Sec:     5117, Lr: 0.000300\n",
      "2021-08-04 07:50:04,883 - INFO - joeynmt.training - Epoch   5, Step:    11800, Batch Loss:     2.225419, Tokens per Sec:     5363, Lr: 0.000300\n",
      "2021-08-04 07:50:09,927 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:50:09,928 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:50:09,928 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:50:09,928 - INFO - joeynmt.training - \tHypothesis: For in love , not in love ,\n",
      "2021-08-04 07:50:09,928 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:50:09,929 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:50:09,929 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:50:09,930 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I do not be afraid , but the Lord is coming of you . For if you do not have a good work , you will be judged .\n",
      "2021-08-04 07:50:09,930 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:50:09,931 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:50:09,931 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:50:09,931 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to the Pharisees and to eat the Pharisees . ”\n",
      "2021-08-04 07:50:09,931 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:50:09,932 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:50:09,934 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:50:09,934 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was granted to be a place , but to the wind and it was needed .\n",
      "2021-08-04 07:50:09,935 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    11800: bleu:   8.91, loss: 7614.9150, ppl:  16.9773, duration: 5.0512s\n",
      "2021-08-04 07:50:23,592 - INFO - joeynmt.training - Epoch   5, Step:    11900, Batch Loss:     1.870155, Tokens per Sec:     5198, Lr: 0.000300\n",
      "2021-08-04 07:50:33,587 - INFO - joeynmt.training - Epoch   5: total training loss 788.96\n",
      "2021-08-04 07:50:33,588 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-04 07:50:36,995 - INFO - joeynmt.training - Epoch   6, Step:    12000, Batch Loss:     2.304803, Tokens per Sec:     5485, Lr: 0.000300\n",
      "2021-08-04 07:50:41,399 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:50:41,399 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:50:42,302 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:50:42,303 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:50:42,303 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:50:42,303 - INFO - joeynmt.training - \tHypothesis: For love is not of one mind , not of evil .\n",
      "2021-08-04 07:50:42,304 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:50:42,305 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:50:42,305 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:50:42,305 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , do not be afraid ; but if the Lord is coming , you will be saved .\n",
      "2021-08-04 07:50:42,305 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:50:42,306 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:50:42,306 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:50:42,307 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:50:42,307 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:50:42,307 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:50:42,308 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:50:42,308 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was necessary to enter a place , but the wind ceased .\n",
      "2021-08-04 07:50:42,309 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    12000: bleu:   9.33, loss: 7560.3828, ppl:  16.6365, duration: 5.3128s\n",
      "2021-08-04 07:50:56,016 - INFO - joeynmt.training - Epoch   6, Step:    12100, Batch Loss:     1.975496, Tokens per Sec:     5169, Lr: 0.000300\n",
      "2021-08-04 07:51:09,169 - INFO - joeynmt.training - Epoch   6, Step:    12200, Batch Loss:     1.749842, Tokens per Sec:     5293, Lr: 0.000300\n",
      "2021-08-04 07:51:14,993 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:51:14,994 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:51:14,994 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:51:14,994 - INFO - joeynmt.training - \tHypothesis: For not love one another .\n",
      "2021-08-04 07:51:14,995 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:51:14,996 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:51:14,996 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:51:14,996 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I have no need to be trouble , but I have not been taught in you .\n",
      "2021-08-04 07:51:14,996 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:51:14,998 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:51:14,998 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:51:14,998 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:51:14,998 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:51:14,999 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:51:14,999 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:51:15,000 - INFO - joeynmt.training - \tHypothesis: For they did not know what had been done in the place , but the wind was near . And the seventh hour was taken away . ”\n",
      "2021-08-04 07:51:15,000 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    12200: bleu:   8.54, loss: 7593.8359, ppl:  16.8447, duration: 5.8306s\n",
      "2021-08-04 07:51:28,848 - INFO - joeynmt.training - Epoch   6, Step:    12300, Batch Loss:     2.287759, Tokens per Sec:     5192, Lr: 0.000300\n",
      "2021-08-04 07:51:32,648 - INFO - joeynmt.training - Epoch   6: total training loss 781.20\n",
      "2021-08-04 07:51:32,649 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-04 07:51:42,224 - INFO - joeynmt.training - Epoch   7, Step:    12400, Batch Loss:     2.096198, Tokens per Sec:     5321, Lr: 0.000300\n",
      "2021-08-04 07:51:46,260 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:51:46,261 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:51:46,261 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:51:46,261 - INFO - joeynmt.training - \tHypothesis: For not in love , not in various disputes .\n",
      "2021-08-04 07:51:46,261 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:51:46,262 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:51:46,262 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:51:46,262 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I have no need to you , but the Lord is coming to you . But if you do not have a preferent time , you will be required .\n",
      "2021-08-04 07:51:46,263 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:51:46,263 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:51:46,263 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:51:46,264 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:51:46,264 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:51:46,265 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:51:46,265 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:51:46,265 - INFO - joeynmt.training - \tHypothesis: For they did not have been ented to them , but the copens was not afraid , but the sower was near .\n",
      "2021-08-04 07:51:46,265 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    12400: bleu:   9.03, loss: 7621.6694, ppl:  17.0200, duration: 4.0412s\n",
      "2021-08-04 07:52:00,092 - INFO - joeynmt.training - Epoch   7, Step:    12500, Batch Loss:     2.188038, Tokens per Sec:     5258, Lr: 0.000300\n",
      "2021-08-04 07:52:13,465 - INFO - joeynmt.training - Epoch   7, Step:    12600, Batch Loss:     2.029307, Tokens per Sec:     5344, Lr: 0.000300\n",
      "2021-08-04 07:52:16,861 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:52:16,861 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:52:17,800 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:52:17,801 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:52:17,802 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:52:17,802 - INFO - joeynmt.training - \tHypothesis: For in love , not in deceitance ,\n",
      "2021-08-04 07:52:17,802 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:52:17,803 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:52:17,803 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:52:17,803 - INFO - joeynmt.training - \tHypothesis: Beloved , do not be afraid , but I am afraid ; but the Lord is coming , and you will be saved ; but I will be required to you .\n",
      "2021-08-04 07:52:17,804 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:52:17,804 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:52:17,804 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:52:17,805 - INFO - joeynmt.training - \tHypothesis: And when Jesus had said to them , “ Take heed to the Pharisees and to eat . ”\n",
      "2021-08-04 07:52:17,806 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:52:17,806 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:52:17,807 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:52:17,807 - INFO - joeynmt.training - \tHypothesis: For they did not know it , but it was necessary to the wind , but the wind ceased .\n",
      "2021-08-04 07:52:17,807 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    12600: bleu:   9.00, loss: 7559.9736, ppl:  16.6339, duration: 4.3413s\n",
      "2021-08-04 07:52:28,786 - INFO - joeynmt.training - Epoch   7: total training loss 768.57\n",
      "2021-08-04 07:52:28,787 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-04 07:52:31,593 - INFO - joeynmt.training - Epoch   8, Step:    12700, Batch Loss:     1.998050, Tokens per Sec:     4928, Lr: 0.000300\n",
      "2021-08-04 07:52:44,999 - INFO - joeynmt.training - Epoch   8, Step:    12800, Batch Loss:     2.080680, Tokens per Sec:     5321, Lr: 0.000300\n",
      "2021-08-04 07:52:48,882 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:52:48,884 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:52:48,884 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:52:48,884 - INFO - joeynmt.training - \tHypothesis: For in love , not in one mind or in disputing .\n",
      "2021-08-04 07:52:48,885 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:52:48,885 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:52:48,885 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:52:48,886 - INFO - joeynmt.training - \tHypothesis: Beloved , do not be confident , but the Lord is coming , and I do not have the things which you have seen .\n",
      "2021-08-04 07:52:48,886 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:52:48,887 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:52:48,887 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:52:48,887 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and be troubled , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:52:48,887 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:52:48,888 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:52:48,888 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:52:48,888 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was granted to be broken by the place , but the wind ceased .\n",
      "2021-08-04 07:52:48,889 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    12800: bleu:   9.35, loss: 7597.3389, ppl:  16.8667, duration: 3.8887s\n",
      "2021-08-04 07:53:02,679 - INFO - joeynmt.training - Epoch   8, Step:    12900, Batch Loss:     2.025231, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2021-08-04 07:53:16,135 - INFO - joeynmt.training - Epoch   8, Step:    13000, Batch Loss:     2.213307, Tokens per Sec:     5385, Lr: 0.000300\n",
      "2021-08-04 07:53:21,639 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:53:21,640 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:53:21,641 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:53:21,641 - INFO - joeynmt.training - \tHypothesis: For love is not in love , not in vain .\n",
      "2021-08-04 07:53:21,642 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:53:21,643 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:53:21,644 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:53:21,644 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I do not be troubled , but the Lord is coming to you .\n",
      "2021-08-04 07:53:21,644 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:53:21,645 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:53:21,645 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:53:21,645 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:53:21,646 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:53:21,646 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:53:21,647 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:53:21,647 - INFO - joeynmt.training - \tHypothesis: For they did not understand this , but that it was no more than the south , but when we had come out of the city , then was near .\n",
      "2021-08-04 07:53:21,647 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    13000: bleu:   9.22, loss: 7606.2109, ppl:  16.9224, duration: 5.5120s\n",
      "2021-08-04 07:53:25,937 - INFO - joeynmt.training - Epoch   8: total training loss 761.19\n",
      "2021-08-04 07:53:25,937 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-04 07:53:35,467 - INFO - joeynmt.training - Epoch   9, Step:    13100, Batch Loss:     1.970884, Tokens per Sec:     5114, Lr: 0.000300\n",
      "2021-08-04 07:53:48,785 - INFO - joeynmt.training - Epoch   9, Step:    13200, Batch Loss:     2.202613, Tokens per Sec:     5358, Lr: 0.000300\n",
      "2021-08-04 07:53:53,015 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:53:53,016 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:53:53,016 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:53:53,016 - INFO - joeynmt.training - \tHypothesis: For not love one another , or in love ,\n",
      "2021-08-04 07:53:53,017 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:53:53,017 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:53:53,017 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:53:53,018 - INFO - joeynmt.training - \tHypothesis: Beloved , I have no need to you , but I have no need of the Lord ; but if you have been grieved , you have no more more than what you have .\n",
      "2021-08-04 07:53:53,018 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:53:53,019 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:53:53,019 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:53:53,019 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed to the Pharisees and to eat . ”\n",
      "2021-08-04 07:53:53,019 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:53:53,020 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:53:53,020 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:53:53,020 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was not given to me ; but when the wind was near , we would have passed through the hill . ”\n",
      "2021-08-04 07:53:53,021 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    13200: bleu:   9.45, loss: 7605.7720, ppl:  16.9197, duration: 4.2346s\n",
      "2021-08-04 07:54:06,812 - INFO - joeynmt.training - Epoch   9, Step:    13300, Batch Loss:     2.022076, Tokens per Sec:     5130, Lr: 0.000300\n",
      "2021-08-04 07:54:18,300 - INFO - joeynmt.training - Epoch   9: total training loss 755.68\n",
      "2021-08-04 07:54:18,301 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-04 07:54:20,201 - INFO - joeynmt.training - Epoch  10, Step:    13400, Batch Loss:     1.796936, Tokens per Sec:     5284, Lr: 0.000300\n",
      "2021-08-04 07:54:23,804 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:54:23,805 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:54:24,679 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:54:24,680 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:54:24,680 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:54:24,680 - INFO - joeynmt.training - \tHypothesis: For love no one another .\n",
      "2021-08-04 07:54:24,680 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:54:24,681 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:54:24,681 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:54:24,682 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I do not be afraid , but the Lord is in you . But if you do not have a preacher , you will be troubled .\n",
      "2021-08-04 07:54:24,682 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:54:24,682 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:54:24,683 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:54:24,683 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:54:24,683 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:54:24,684 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:54:24,684 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:54:24,684 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was no more than that it was not lawful for us to stay . And when we came out of the hid of the great day . ”\n",
      "2021-08-04 07:54:24,685 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    13400: bleu:  10.17, loss: 7538.6611, ppl:  16.5026, duration: 4.4828s\n",
      "2021-08-04 07:54:38,535 - INFO - joeynmt.training - Epoch  10, Step:    13500, Batch Loss:     1.971727, Tokens per Sec:     5200, Lr: 0.000300\n",
      "2021-08-04 07:54:51,891 - INFO - joeynmt.training - Epoch  10, Step:    13600, Batch Loss:     2.131854, Tokens per Sec:     5333, Lr: 0.000300\n",
      "2021-08-04 07:54:56,389 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:54:56,390 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:54:56,390 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:54:56,390 - INFO - joeynmt.training - \tHypothesis: For love one another , not in love or in all unbelief .\n",
      "2021-08-04 07:54:56,391 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:54:56,391 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:54:56,392 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:54:56,392 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , do not be troubled , but the Lord is coming of you . But if you have been here , you will not be judged .\n",
      "2021-08-04 07:54:56,392 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:54:56,393 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:54:56,393 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:54:56,393 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ Take heed to yourselves , and be afraid . ”\n",
      "2021-08-04 07:54:56,393 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:54:56,394 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:54:56,394 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:54:56,395 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that it might be given to me a place , but to the wind .\n",
      "2021-08-04 07:54:56,395 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    13600: bleu:   9.48, loss: 7547.8833, ppl:  16.5593, duration: 4.5031s\n",
      "2021-08-04 07:55:10,171 - INFO - joeynmt.training - Epoch  10, Step:    13700, Batch Loss:     2.346681, Tokens per Sec:     5146, Lr: 0.000300\n",
      "2021-08-04 07:55:15,380 - INFO - joeynmt.training - Epoch  10: total training loss 744.48\n",
      "2021-08-04 07:55:15,380 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-04 07:55:23,526 - INFO - joeynmt.training - Epoch  11, Step:    13800, Batch Loss:     1.963943, Tokens per Sec:     5311, Lr: 0.000300\n",
      "2021-08-04 07:55:27,711 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:55:27,713 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:55:27,714 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:55:27,714 - INFO - joeynmt.training - \tHypothesis: For love , not in love , not in vain .\n",
      "2021-08-04 07:55:27,714 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:55:27,715 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:55:27,715 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:55:27,715 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I do not be troubled , but I have written to you , but I have been a great time .\n",
      "2021-08-04 07:55:27,715 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:55:27,716 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:55:27,716 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:55:27,717 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to him , “ Take heed to the Pharisees and to eat . ”\n",
      "2021-08-04 07:55:27,717 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:55:27,717 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:55:27,718 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:55:27,718 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but it was given to them to stay ; but when it was a great wind was near . ”\n",
      "2021-08-04 07:55:27,718 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    13800: bleu:   8.83, loss: 7572.7817, ppl:  16.7133, duration: 4.1913s\n",
      "2021-08-04 07:55:41,532 - INFO - joeynmt.training - Epoch  11, Step:    13900, Batch Loss:     2.100414, Tokens per Sec:     5195, Lr: 0.000300\n",
      "2021-08-04 07:55:54,871 - INFO - joeynmt.training - Epoch  11, Step:    14000, Batch Loss:     2.136618, Tokens per Sec:     5355, Lr: 0.000300\n",
      "2021-08-04 07:55:59,412 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:55:59,414 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:55:59,414 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:55:59,414 - INFO - joeynmt.training - \tHypothesis: For in love , not in vain .\n",
      "2021-08-04 07:55:59,414 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:55:59,417 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:55:59,418 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:55:59,418 - INFO - joeynmt.training - \tHypothesis: Beloved , I do not be children , but the Lord is coming , and I will not be like this time .\n",
      "2021-08-04 07:55:59,418 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:55:59,419 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:55:59,419 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:55:59,419 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and to eat of the Pharisees . ”\n",
      "2021-08-04 07:55:59,420 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:55:59,420 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:55:59,421 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:55:59,421 - INFO - joeynmt.training - \tHypothesis: For they did not enter this time , but that it would be given to me a place where it was lost .\n",
      "2021-08-04 07:55:59,421 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    14000: bleu:  10.06, loss: 7580.3628, ppl:  16.7605, duration: 4.5493s\n",
      "2021-08-04 07:56:11,994 - INFO - joeynmt.training - Epoch  11: total training loss 734.90\n",
      "2021-08-04 07:56:11,994 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-04 07:56:13,229 - INFO - joeynmt.training - Epoch  12, Step:    14100, Batch Loss:     1.996669, Tokens per Sec:     5455, Lr: 0.000300\n",
      "2021-08-04 07:56:26,661 - INFO - joeynmt.training - Epoch  12, Step:    14200, Batch Loss:     2.256333, Tokens per Sec:     5344, Lr: 0.000300\n",
      "2021-08-04 07:56:30,688 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:56:30,689 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:56:30,689 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:56:30,690 - INFO - joeynmt.training - \tHypothesis: For love is not in heart .\n",
      "2021-08-04 07:56:30,690 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:56:30,691 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:56:30,691 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:56:30,691 - INFO - joeynmt.training - \tHypothesis: Beloved , my brethren , be troubled , and I do not have the Lord ; but if you have been partakers of you , you will be saved .\n",
      "2021-08-04 07:56:30,691 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:56:30,692 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:56:30,692 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:56:30,693 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:56:30,693 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:56:30,694 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:56:30,694 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:56:30,694 - INFO - joeynmt.training - \tHypothesis: For they did not know what had been done , but the casts was not broken by the wind . And when they had come down from the mountain .\n",
      "2021-08-04 07:56:30,694 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    14200: bleu:   8.28, loss: 7605.5469, ppl:  16.9182, duration: 4.0313s\n",
      "2021-08-04 07:56:44,618 - INFO - joeynmt.training - Epoch  12, Step:    14300, Batch Loss:     2.122842, Tokens per Sec:     5153, Lr: 0.000300\n",
      "2021-08-04 07:56:58,217 - INFO - joeynmt.training - Epoch  12, Step:    14400, Batch Loss:     2.047285, Tokens per Sec:     5291, Lr: 0.000300\n",
      "2021-08-04 07:57:03,572 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:57:03,573 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:57:03,574 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:57:03,574 - INFO - joeynmt.training - \tHypothesis: For in love , not in love , but in knowledge .\n",
      "2021-08-04 07:57:03,574 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:57:03,575 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:57:03,575 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:57:03,575 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I have no need to be in the Lord , but I have no need to you .\n",
      "2021-08-04 07:57:03,576 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:57:03,576 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:57:03,577 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:57:03,577 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed to the Pharisees and to eat of the Pharisees . ”\n",
      "2021-08-04 07:57:03,577 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:57:03,578 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:57:03,578 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:57:03,578 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was given to them to stay , but the wind was near .\n",
      "2021-08-04 07:57:03,579 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    14400: bleu:   9.19, loss: 7581.3491, ppl:  16.7667, duration: 5.3610s\n",
      "2021-08-04 07:57:09,135 - INFO - joeynmt.training - Epoch  12: total training loss 722.82\n",
      "2021-08-04 07:57:09,136 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-04 07:57:17,451 - INFO - joeynmt.training - Epoch  13, Step:    14500, Batch Loss:     1.800265, Tokens per Sec:     5152, Lr: 0.000300\n",
      "2021-08-04 07:57:30,799 - INFO - joeynmt.training - Epoch  13, Step:    14600, Batch Loss:     2.308776, Tokens per Sec:     5342, Lr: 0.000300\n",
      "2021-08-04 07:57:34,958 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:57:34,959 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:57:34,959 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:57:34,960 - INFO - joeynmt.training - \tHypothesis: Let love be without understanding , not with deceitful .\n",
      "2021-08-04 07:57:34,960 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:57:34,961 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:57:34,961 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:57:34,961 - INFO - joeynmt.training - \tHypothesis: Beloved , my brethren , do not be troubled , but the Lord is coming of you ; but I do not want you to do the gospel of you .\n",
      "2021-08-04 07:57:34,961 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:57:34,962 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:57:34,962 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:57:34,963 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to him , “ Take heed to the Pharisees and to eat of the Pharisees . ”\n",
      "2021-08-04 07:57:34,963 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:57:34,963 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:57:34,964 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:57:34,964 - INFO - joeynmt.training - \tHypothesis: For they did not receive it , but that the centurion was given to the sky , “ It is not lawful for us to be a great day . ”\n",
      "2021-08-04 07:57:34,964 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    14600: bleu:   9.35, loss: 7569.2568, ppl:  16.6915, duration: 4.1643s\n",
      "2021-08-04 07:57:48,927 - INFO - joeynmt.training - Epoch  13, Step:    14700, Batch Loss:     2.323140, Tokens per Sec:     5144, Lr: 0.000210\n",
      "2021-08-04 07:58:01,296 - INFO - joeynmt.training - Epoch  13: total training loss 713.40\n",
      "2021-08-04 07:58:01,296 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-04 07:58:02,271 - INFO - joeynmt.training - Epoch  14, Step:    14800, Batch Loss:     1.931769, Tokens per Sec:     5143, Lr: 0.000210\n",
      "2021-08-04 07:58:05,694 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:58:05,694 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:58:06,586 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:58:06,591 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:58:06,591 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:58:06,591 - INFO - joeynmt.training - \tHypothesis: For love is not in love , not in vain .\n",
      "2021-08-04 07:58:06,592 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:58:06,592 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:58:06,593 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:58:06,593 - INFO - joeynmt.training - \tHypothesis: Brethren , brethren , I have no need to the Lord ; but I have no need of you , but I have no more than the Lord .\n",
      "2021-08-04 07:58:06,593 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:58:06,594 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:58:06,594 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:58:06,594 - INFO - joeynmt.training - \tHypothesis: Then Jesus , looking and said to them , “ Take heed to the Pharisees and to the Pharisees . ”\n",
      "2021-08-04 07:58:06,595 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:58:06,595 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:58:06,596 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:58:06,596 - INFO - joeynmt.training - \tHypothesis: For they did not have much more , but rather rather rather rather rather rather rather rather rather rather than the casts of the sea .\n",
      "2021-08-04 07:58:06,596 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    14800: bleu:   9.38, loss: 7534.4346, ppl:  16.4767, duration: 4.3245s\n",
      "2021-08-04 07:58:20,482 - INFO - joeynmt.training - Epoch  14, Step:    14900, Batch Loss:     2.089258, Tokens per Sec:     5152, Lr: 0.000210\n",
      "2021-08-04 07:58:33,912 - INFO - joeynmt.training - Epoch  14, Step:    15000, Batch Loss:     1.893560, Tokens per Sec:     5301, Lr: 0.000210\n",
      "2021-08-04 07:58:38,109 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:58:38,110 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:58:38,111 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:58:38,111 - INFO - joeynmt.training - \tHypothesis: For in love , not in deceit ,\n",
      "2021-08-04 07:58:38,112 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:58:38,113 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:58:38,113 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:58:38,113 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I do not be afraid , but the Lord is in you . But if you do not have a preacher , you will be a help .\n",
      "2021-08-04 07:58:38,113 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:58:38,114 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:58:38,114 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:58:38,114 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and be afraid , and be afraid . ”\n",
      "2021-08-04 07:58:38,115 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:58:38,115 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:58:38,116 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:58:38,116 - INFO - joeynmt.training - \tHypothesis: For they did not receive it , but that it was given to me to be revealed , “ The wind was near near the mountain . ”\n",
      "2021-08-04 07:58:38,116 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    15000: bleu:   9.14, loss: 7541.7700, ppl:  16.5217, duration: 4.2031s\n",
      "2021-08-04 07:58:52,018 - INFO - joeynmt.training - Epoch  14, Step:    15100, Batch Loss:     1.859939, Tokens per Sec:     5196, Lr: 0.000210\n",
      "2021-08-04 07:58:58,010 - INFO - joeynmt.training - Epoch  14: total training loss 694.79\n",
      "2021-08-04 07:58:58,011 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-04 07:59:05,386 - INFO - joeynmt.training - Epoch  15, Step:    15200, Batch Loss:     2.005600, Tokens per Sec:     5326, Lr: 0.000210\n",
      "2021-08-04 07:59:09,106 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:59:09,107 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:59:09,978 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:59:09,979 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:59:09,979 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:59:09,979 - INFO - joeynmt.training - \tHypothesis: For love is not in heart .\n",
      "2021-08-04 07:59:09,979 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:59:09,980 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:59:09,980 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:59:09,980 - INFO - joeynmt.training - \tHypothesis: Beloved , brethren , I have no need to you ; but I have been a preacher of the Lord , and I do not have known to you .\n",
      "2021-08-04 07:59:09,981 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:59:09,981 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:59:09,982 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:59:09,982 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and be afraid , and be filled with the Pharisees . ”\n",
      "2021-08-04 07:59:09,982 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:59:09,983 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:59:09,983 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:59:09,983 - INFO - joeynmt.training - \tHypothesis: For they did not know it , but that it was given to me a vision , “ The wind was near near to us . ”\n",
      "2021-08-04 07:59:09,984 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    15200: bleu:   9.77, loss: 7531.7764, ppl:  16.4604, duration: 4.5973s\n",
      "2021-08-04 07:59:23,815 - INFO - joeynmt.training - Epoch  15, Step:    15300, Batch Loss:     1.994814, Tokens per Sec:     5154, Lr: 0.000210\n",
      "2021-08-04 07:59:37,194 - INFO - joeynmt.training - Epoch  15, Step:    15400, Batch Loss:     1.657881, Tokens per Sec:     5248, Lr: 0.000210\n",
      "2021-08-04 07:59:41,720 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:59:41,721 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 07:59:41,721 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 07:59:41,721 - INFO - joeynmt.training - \tHypothesis: For love is not in love , not in understanding .\n",
      "2021-08-04 07:59:41,721 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:59:41,722 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 07:59:41,722 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 07:59:41,722 - INFO - joeynmt.training - \tHypothesis: Beloved , I am not worthy of the Lord , but I speak to you ; but I have no hope to you .\n",
      "2021-08-04 07:59:41,723 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:59:41,723 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 07:59:41,724 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 07:59:41,724 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and be afraid , and be afraid . ”\n",
      "2021-08-04 07:59:41,724 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:59:41,725 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 07:59:41,725 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 07:59:41,725 - INFO - joeynmt.training - \tHypothesis: For they did not know what it was granted to be broken by the wind , but the wind was near .\n",
      "2021-08-04 07:59:41,726 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    15400: bleu:   9.85, loss: 7546.9727, ppl:  16.5537, duration: 4.5306s\n",
      "2021-08-04 07:59:55,500 - INFO - joeynmt.training - Epoch  15: total training loss 691.68\n",
      "2021-08-04 07:59:55,500 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-04 07:59:55,645 - INFO - joeynmt.training - Epoch  16, Step:    15500, Batch Loss:     1.696025, Tokens per Sec:     4333, Lr: 0.000210\n",
      "2021-08-04 08:00:09,149 - INFO - joeynmt.training - Epoch  16, Step:    15600, Batch Loss:     2.091791, Tokens per Sec:     5416, Lr: 0.000210\n",
      "2021-08-04 08:00:14,367 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:00:14,368 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:00:14,374 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:00:14,374 - INFO - joeynmt.training - \tHypothesis: Let all love be without understanding .\n",
      "2021-08-04 08:00:14,374 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:00:14,375 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:00:14,375 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:00:14,376 - INFO - joeynmt.training - \tHypothesis: Brethren , I do not be children of the Lord , but I have no need of you . But this I do not want you to boast .\n",
      "2021-08-04 08:00:14,376 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:00:14,377 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:00:14,377 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:00:14,377 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and be troubled . ”\n",
      "2021-08-04 08:00:14,377 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:00:14,379 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:00:14,379 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:00:14,380 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was given to me to be given to the place of the wind , but to the mountain which is near . ”\n",
      "2021-08-04 08:00:14,380 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    15600: bleu:  10.84, loss: 7548.7212, ppl:  16.5645, duration: 5.2298s\n",
      "2021-08-04 08:00:28,374 - INFO - joeynmt.training - Epoch  16, Step:    15700, Batch Loss:     2.111770, Tokens per Sec:     5175, Lr: 0.000210\n",
      "2021-08-04 08:00:41,754 - INFO - joeynmt.training - Epoch  16, Step:    15800, Batch Loss:     1.908453, Tokens per Sec:     5277, Lr: 0.000210\n",
      "2021-08-04 08:00:46,347 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:00:46,348 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:00:46,348 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:00:46,349 - INFO - joeynmt.training - \tHypothesis: For love is not in heart , not in vain .\n",
      "2021-08-04 08:00:46,349 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:00:46,350 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:00:46,350 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:00:46,350 - INFO - joeynmt.training - \tHypothesis: Beloved , do not be partakers of the Lord , but now I am not worthy to come . But I do not speak to you .\n",
      "2021-08-04 08:00:46,351 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:00:46,351 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:00:46,352 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:00:46,352 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed and wrote , and do good to the Pharisees . ”\n",
      "2021-08-04 08:00:46,352 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:00:46,353 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:00:46,354 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:00:46,354 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was granted to be broken by the wind , but the wind was near .\n",
      "2021-08-04 08:00:46,354 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    15800: bleu:   9.30, loss: 7535.9668, ppl:  16.4861, duration: 4.5996s\n",
      "2021-08-04 08:00:53,205 - INFO - joeynmt.training - Epoch  16: total training loss 679.76\n",
      "2021-08-04 08:00:53,205 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-08-04 08:01:00,200 - INFO - joeynmt.training - Epoch  17, Step:    15900, Batch Loss:     1.838314, Tokens per Sec:     5123, Lr: 0.000210\n",
      "2021-08-04 08:01:13,519 - INFO - joeynmt.training - Epoch  17, Step:    16000, Batch Loss:     1.946895, Tokens per Sec:     5377, Lr: 0.000210\n",
      "2021-08-04 08:01:17,875 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:01:17,876 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:01:17,876 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:01:17,877 - INFO - joeynmt.training - \tHypothesis: For love does not love one another , not of these things .\n",
      "2021-08-04 08:01:17,877 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:01:17,878 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:01:17,878 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:01:17,878 - INFO - joeynmt.training - \tHypothesis: Beloved , I do not be children of the Lord , but I will speak to you . But if you do not have a preacher of the Lord , you will be greatly afraid .\n",
      "2021-08-04 08:01:17,878 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:01:17,879 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:01:17,879 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:01:17,880 - INFO - joeynmt.training - \tHypothesis: And when Jesus had said to them , “ Take heed to yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 08:01:17,880 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:01:17,881 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:01:17,881 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:01:17,881 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was given to me ; but if it was granted to us , it would have no place . ”\n",
      "2021-08-04 08:01:17,881 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    16000: bleu:   9.72, loss: 7581.5610, ppl:  16.7680, duration: 4.3622s\n",
      "2021-08-04 08:01:31,760 - INFO - joeynmt.training - Epoch  17, Step:    16100, Batch Loss:     1.922616, Tokens per Sec:     5128, Lr: 0.000210\n",
      "2021-08-04 08:01:45,198 - INFO - joeynmt.training - Epoch  17, Step:    16200, Batch Loss:     2.171205, Tokens per Sec:     5275, Lr: 0.000210\n",
      "2021-08-04 08:01:48,613 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 08:01:48,613 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 08:01:49,959 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:01:49,960 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:01:49,960 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:01:49,960 - INFO - joeynmt.training - \tHypothesis: For in love , not in love , in understanding .\n",
      "2021-08-04 08:01:49,960 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:01:49,961 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:01:49,962 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:01:49,962 - INFO - joeynmt.training - \tHypothesis: Beloved , I do not be My brethren , but I will speak to you ; but if you do not have the Lord , you will be troubled .\n",
      "2021-08-04 08:01:49,962 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:01:49,963 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:01:49,963 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:01:49,963 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and be filled with yourselves . ”\n",
      "2021-08-04 08:01:49,964 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:01:49,964 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:01:49,964 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:01:49,965 - INFO - joeynmt.training - \tHypothesis: For they did not receive it , but rather that it was given to me into the place where it was near .\n",
      "2021-08-04 08:01:49,965 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    16200: bleu:  10.03, loss: 7488.9883, ppl:  16.2006, duration: 4.7659s\n",
      "2021-08-04 08:01:50,372 - INFO - joeynmt.training - Epoch  17: total training loss 677.78\n",
      "2021-08-04 08:01:50,372 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-08-04 08:02:03,831 - INFO - joeynmt.training - Epoch  18, Step:    16300, Batch Loss:     1.778570, Tokens per Sec:     5059, Lr: 0.000210\n",
      "2021-08-04 08:02:17,246 - INFO - joeynmt.training - Epoch  18, Step:    16400, Batch Loss:     1.840810, Tokens per Sec:     5398, Lr: 0.000210\n",
      "2021-08-04 08:02:21,477 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:02:21,478 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:02:21,479 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:02:21,479 - INFO - joeynmt.training - \tHypothesis: For love is not in love , not in understanding .\n",
      "2021-08-04 08:02:21,479 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:02:21,480 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:02:21,480 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:02:21,480 - INFO - joeynmt.training - \tHypothesis: Beloved , I have written to you , brethren , not I have the Lord ; but if you have been perfected , you have nothing .\n",
      "2021-08-04 08:02:21,480 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:02:21,481 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:02:21,481 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:02:21,482 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed and beware of the Pharisees , and be filled with the Pharisees . ”\n",
      "2021-08-04 08:02:21,482 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:02:21,483 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:02:21,484 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:02:21,485 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that it was given to me to stay , but to the south wind . ”\n",
      "2021-08-04 08:02:21,485 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    16400: bleu:  10.00, loss: 7521.4854, ppl:  16.3975, duration: 4.2388s\n",
      "2021-08-04 08:02:35,342 - INFO - joeynmt.training - Epoch  18, Step:    16500, Batch Loss:     1.600457, Tokens per Sec:     5154, Lr: 0.000210\n",
      "2021-08-04 08:02:42,843 - INFO - joeynmt.training - Epoch  18: total training loss 670.38\n",
      "2021-08-04 08:02:42,843 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-08-04 08:02:48,739 - INFO - joeynmt.training - Epoch  19, Step:    16600, Batch Loss:     1.957674, Tokens per Sec:     5305, Lr: 0.000210\n",
      "2021-08-04 08:02:52,864 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:02:52,865 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:02:52,865 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:02:52,866 - INFO - joeynmt.training - \tHypothesis: For in love , not in love ,\n",
      "2021-08-04 08:02:52,866 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:02:52,867 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:02:52,867 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:02:52,867 - INFO - joeynmt.training - \tHypothesis: Brethren , my brethren , do not be troubled ; but I am not able to speak the Lord , but that you may be saved .\n",
      "2021-08-04 08:02:52,868 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:02:52,868 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:02:52,868 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:02:52,869 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed and be filled with the Pharisees . ”\n",
      "2021-08-04 08:02:52,869 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:02:52,870 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:02:52,870 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:02:52,870 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was given to me a place for the stay , but the wind was near . ”\n",
      "2021-08-04 08:02:52,870 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    16600: bleu:  10.85, loss: 7532.7939, ppl:  16.4666, duration: 4.1310s\n",
      "2021-08-04 08:03:06,599 - INFO - joeynmt.training - Epoch  19, Step:    16700, Batch Loss:     2.021698, Tokens per Sec:     5090, Lr: 0.000210\n",
      "2021-08-04 08:03:20,052 - INFO - joeynmt.training - Epoch  19, Step:    16800, Batch Loss:     1.905958, Tokens per Sec:     5360, Lr: 0.000210\n",
      "2021-08-04 08:03:24,243 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:03:24,244 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:03:24,244 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:03:24,244 - INFO - joeynmt.training - \tHypothesis: For love does not love one another .\n",
      "2021-08-04 08:03:24,245 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:03:24,246 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:03:24,246 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:03:24,246 - INFO - joeynmt.training - \tHypothesis: Brethren , brethren , do not be troubled , but the Lord is in you . But I do not want you to do the same , but that you may know the Lord .\n",
      "2021-08-04 08:03:24,246 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:03:24,247 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:03:24,247 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:03:24,248 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and be filled with the Pharisees . ”\n",
      "2021-08-04 08:03:24,248 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:03:24,249 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:03:24,249 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:03:24,249 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was given to me a place for the wind was near . But we ate the south wind was near . ”\n",
      "2021-08-04 08:03:24,249 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    16800: bleu:  10.52, loss: 7526.5449, ppl:  16.4284, duration: 4.1967s\n",
      "2021-08-04 08:03:38,393 - INFO - joeynmt.training - Epoch  19, Step:    16900, Batch Loss:     1.933830, Tokens per Sec:     5068, Lr: 0.000210\n",
      "2021-08-04 08:03:39,715 - INFO - joeynmt.training - Epoch  19: total training loss 666.18\n",
      "2021-08-04 08:03:39,716 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-08-04 08:03:51,898 - INFO - joeynmt.training - Epoch  20, Step:    17000, Batch Loss:     2.086034, Tokens per Sec:     5366, Lr: 0.000210\n",
      "2021-08-04 08:03:56,228 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:03:56,229 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:03:56,229 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:03:56,230 - INFO - joeynmt.training - \tHypothesis: For all things are not of one mind , but of one mind .\n",
      "2021-08-04 08:03:56,230 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:03:56,231 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:03:56,231 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:03:56,231 - INFO - joeynmt.training - \tHypothesis: Beloved , do not be children , brethren , but the Lord is coming . But I do not want you to do the things of you .\n",
      "2021-08-04 08:03:56,231 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:03:56,232 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:03:56,232 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:03:56,232 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed and be troubled . ”\n",
      "2021-08-04 08:03:56,233 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:03:56,233 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:03:56,233 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:03:56,234 - INFO - joeynmt.training - \tHypothesis: For they did not know it , but the temptation has town into the lake ; but when it was opened , we shall reign over the sky . ”\n",
      "2021-08-04 08:03:56,234 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    17000: bleu:  10.18, loss: 7519.0405, ppl:  16.3826, duration: 4.3358s\n",
      "2021-08-04 08:04:09,917 - INFO - joeynmt.training - Epoch  20, Step:    17100, Batch Loss:     2.132567, Tokens per Sec:     5159, Lr: 0.000210\n",
      "2021-08-04 08:04:23,286 - INFO - joeynmt.training - Epoch  20, Step:    17200, Batch Loss:     1.945554, Tokens per Sec:     5293, Lr: 0.000210\n",
      "2021-08-04 08:04:27,456 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:04:27,457 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:04:27,458 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:04:27,458 - INFO - joeynmt.training - \tHypothesis: For in love , not in deceit .\n",
      "2021-08-04 08:04:27,458 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:04:27,459 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:04:27,459 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:04:27,459 - INFO - joeynmt.training - \tHypothesis: Beloved , I do not be afraid ; but if the Lord is in you , I will speak to you .\n",
      "2021-08-04 08:04:27,460 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:04:27,460 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:04:27,460 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:04:27,461 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and be filled with yourselves . ”\n",
      "2021-08-04 08:04:27,461 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:04:27,462 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:04:27,462 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:04:27,462 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that it was given to me to stay , the wind was near .\n",
      "2021-08-04 08:04:27,462 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    17200: bleu:   9.52, loss: 7523.0283, ppl:  16.4069, duration: 4.1762s\n",
      "2021-08-04 08:04:35,780 - INFO - joeynmt.training - Epoch  20: total training loss 656.80\n",
      "2021-08-04 08:04:35,780 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-08-04 08:04:41,287 - INFO - joeynmt.training - Epoch  21, Step:    17300, Batch Loss:     1.627351, Tokens per Sec:     4972, Lr: 0.000210\n",
      "2021-08-04 08:04:54,713 - INFO - joeynmt.training - Epoch  21, Step:    17400, Batch Loss:     1.970703, Tokens per Sec:     5286, Lr: 0.000210\n",
      "2021-08-04 08:04:58,996 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:04:58,997 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:04:58,997 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:04:58,998 - INFO - joeynmt.training - \tHypothesis: All things are not of one mind , but of one another .\n",
      "2021-08-04 08:04:58,998 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:04:58,999 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:04:58,999 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:04:58,999 - INFO - joeynmt.training - \tHypothesis: Brethren , brethren , be troubled , not only to speak ; but I am a vision . But if you do not do the things which I am in you .\n",
      "2021-08-04 08:04:58,999 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:04:59,000 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:04:59,000 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:04:59,000 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and beaten yourselves , and be filled with the Pharisees . ”\n",
      "2021-08-04 08:04:59,001 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:04:59,001 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:04:59,001 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:04:59,002 - INFO - joeynmt.training - \tHypothesis: For they did not know it , but the tempt was broken by the wind , but the camp came to us .\n",
      "2021-08-04 08:04:59,002 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    17400: bleu:   9.67, loss: 7530.9229, ppl:  16.4552, duration: 4.2884s\n",
      "2021-08-04 08:05:12,909 - INFO - joeynmt.training - Epoch  21, Step:    17500, Batch Loss:     1.950116, Tokens per Sec:     5198, Lr: 0.000147\n",
      "2021-08-04 08:05:26,323 - INFO - joeynmt.training - Epoch  21, Step:    17600, Batch Loss:     1.670750, Tokens per Sec:     5367, Lr: 0.000147\n",
      "2021-08-04 08:05:30,070 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 08:05:30,070 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 08:05:30,952 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:05:30,953 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:05:30,953 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:05:30,953 - INFO - joeynmt.training - \tHypothesis: For in love , not in deceitful lusts ,\n",
      "2021-08-04 08:05:30,954 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:05:30,954 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:05:30,955 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:05:30,955 - INFO - joeynmt.training - \tHypothesis: Brethren , my brethren , do not be afraid ; but the Lord is coming , yet I will speak to you .\n",
      "2021-08-04 08:05:30,955 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:05:30,956 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:05:30,956 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:05:30,956 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed and beware of the Pharisees and the Pharisees . ”\n",
      "2021-08-04 08:05:30,957 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:05:30,957 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:05:30,957 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:05:30,958 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was given to me to be given to the place for the south wind . But when we had come down from the mountain .\n",
      "2021-08-04 08:05:30,958 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    17600: bleu:  11.44, loss: 7487.3838, ppl:  16.1909, duration: 4.6342s\n",
      "2021-08-04 08:05:32,707 - INFO - joeynmt.training - Epoch  21: total training loss 645.38\n",
      "2021-08-04 08:05:32,707 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-08-04 08:05:44,817 - INFO - joeynmt.training - Epoch  22, Step:    17700, Batch Loss:     1.670561, Tokens per Sec:     5091, Lr: 0.000147\n",
      "2021-08-04 08:05:58,158 - INFO - joeynmt.training - Epoch  22, Step:    17800, Batch Loss:     1.853234, Tokens per Sec:     5315, Lr: 0.000147\n",
      "2021-08-04 08:06:02,438 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:06:02,439 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:06:02,439 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:06:02,440 - INFO - joeynmt.training - \tHypothesis: For in love , not in love ,\n",
      "2021-08-04 08:06:02,440 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:06:02,441 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:06:02,441 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:06:02,441 - INFO - joeynmt.training - \tHypothesis: Brethren , I do not be children , brethren , but the Lord is coming . You will have no more than the work of you .\n",
      "2021-08-04 08:06:02,441 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:06:02,442 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:06:02,442 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:06:02,443 - INFO - joeynmt.training - \tHypothesis: And when Jesus had said to them , “ Take heed and beware of the Pharisees and the Pharisees . ”\n",
      "2021-08-04 08:06:02,444 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:06:02,446 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:06:02,446 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:06:02,446 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was given to me to be given to the scorneres , but to us the scornerved . ”\n",
      "2021-08-04 08:06:02,447 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step    17800: bleu:  10.24, loss: 7539.6611, ppl:  16.5087, duration: 4.2880s\n",
      "2021-08-04 08:06:16,225 - INFO - joeynmt.training - Epoch  22, Step:    17900, Batch Loss:     1.964890, Tokens per Sec:     5183, Lr: 0.000147\n",
      "2021-08-04 08:06:25,230 - INFO - joeynmt.training - Epoch  22: total training loss 639.70\n",
      "2021-08-04 08:06:25,230 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-08-04 08:06:29,655 - INFO - joeynmt.training - Epoch  23, Step:    18000, Batch Loss:     1.709789, Tokens per Sec:     5189, Lr: 0.000147\n",
      "2021-08-04 08:06:33,939 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:06:33,940 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:06:33,940 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:06:33,940 - INFO - joeynmt.training - \tHypothesis: For all love does not have the same mind , not be deceived .\n",
      "2021-08-04 08:06:33,941 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:06:33,941 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:06:33,942 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:06:33,942 - INFO - joeynmt.training - \tHypothesis: Brethren , I do not be children , but the Lord is coming . But I will speak to you a preferent time , but I will do not come to you .\n",
      "2021-08-04 08:06:33,942 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:06:33,943 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:06:33,943 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:06:33,943 - INFO - joeynmt.training - \tHypothesis: And Jesus said to them , “ Take heed and beware of the Pharisees and the Pharisees . ”\n",
      "2021-08-04 08:06:33,944 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:06:33,944 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:06:33,945 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:06:33,945 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that the temptation was blinded by the wind , but the wind was near .\n",
      "2021-08-04 08:06:33,945 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    18000: bleu:  10.11, loss: 7530.1328, ppl:  16.4504, duration: 4.2898s\n",
      "2021-08-04 08:06:47,772 - INFO - joeynmt.training - Epoch  23, Step:    18100, Batch Loss:     1.601363, Tokens per Sec:     5167, Lr: 0.000147\n",
      "2021-08-04 08:07:01,129 - INFO - joeynmt.training - Epoch  23, Step:    18200, Batch Loss:     1.516631, Tokens per Sec:     5296, Lr: 0.000147\n",
      "2021-08-04 08:07:06,215 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:07:06,216 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:07:06,216 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:07:06,217 - INFO - joeynmt.training - \tHypothesis: For in love , not in love or in love or .\n",
      "2021-08-04 08:07:06,217 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:07:06,218 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:07:06,218 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:07:06,218 - INFO - joeynmt.training - \tHypothesis: Brethren , I urge you , brethren , not only the Lord ; but if you have been partakers of this world , you will have no more abundance .\n",
      "2021-08-04 08:07:06,218 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:07:06,219 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:07:06,219 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:07:06,219 - INFO - joeynmt.training - \tHypothesis: And when Jesus had said to them , “ Take heed and beware of the Pharisees and Pharisees . ”\n",
      "2021-08-04 08:07:06,220 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:07:06,220 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:07:06,221 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:07:06,221 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was vago to the place where it was , but the place where it was near .\n",
      "2021-08-04 08:07:06,221 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    18200: bleu:  10.58, loss: 7534.5303, ppl:  16.4773, duration: 5.0921s\n",
      "2021-08-04 08:07:19,927 - INFO - joeynmt.training - Epoch  23, Step:    18300, Batch Loss:     1.890916, Tokens per Sec:     5225, Lr: 0.000147\n",
      "2021-08-04 08:07:22,685 - INFO - joeynmt.training - Epoch  23: total training loss 632.89\n",
      "2021-08-04 08:07:22,686 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-08-04 08:07:33,206 - INFO - joeynmt.training - Epoch  24, Step:    18400, Batch Loss:     1.747549, Tokens per Sec:     5295, Lr: 0.000147\n",
      "2021-08-04 08:07:37,926 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:07:37,927 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:07:37,927 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:07:37,927 - INFO - joeynmt.training - \tHypothesis: For all love is not in deceitful .\n",
      "2021-08-04 08:07:37,928 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:07:37,928 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:07:37,929 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:07:37,929 - INFO - joeynmt.training - \tHypothesis: Brethren , I do not be children , but the Lord is coming . But I do not have the same mind , but the things which I have spoken to you .\n",
      "2021-08-04 08:07:37,929 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:07:37,930 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:07:37,930 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:07:37,930 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and beware of the Pharisees and Pharisees . ”\n",
      "2021-08-04 08:07:37,931 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:07:37,931 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:07:37,931 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:07:37,932 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was hard for the camp , but the camp was near to the mountain .\n",
      "2021-08-04 08:07:37,932 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step    18400: bleu:  10.26, loss: 7562.8779, ppl:  16.6519, duration: 4.7254s\n",
      "2021-08-04 08:07:51,813 - INFO - joeynmt.training - Epoch  24, Step:    18500, Batch Loss:     1.779428, Tokens per Sec:     5145, Lr: 0.000147\n",
      "2021-08-04 08:08:05,132 - INFO - joeynmt.training - Epoch  24, Step:    18600, Batch Loss:     1.548625, Tokens per Sec:     5327, Lr: 0.000147\n",
      "2021-08-04 08:08:09,449 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:08:09,451 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:08:09,451 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:08:09,452 - INFO - joeynmt.training - \tHypothesis: Let all love be without hypocrisy .\n",
      "2021-08-04 08:08:09,452 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:08:09,452 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:08:09,453 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:08:09,453 - INFO - joeynmt.training - \tHypothesis: Brethren , I write to you , brethren , not to the Lord ; but I do not have the same mind .\n",
      "2021-08-04 08:08:09,453 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:08:09,454 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:08:09,454 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:08:09,455 - INFO - joeynmt.training - \tHypothesis: And when Jesus had said to them , “ Take heed and beware of the Pharisees and the Pharisees of the Pharisees . ”\n",
      "2021-08-04 08:08:09,455 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:08:09,456 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:08:09,456 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:08:09,456 - INFO - joeynmt.training - \tHypothesis: For they did not know it , but that it was given to me to the place that the wind should not be broken .\n",
      "2021-08-04 08:08:09,456 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step    18600: bleu:  10.96, loss: 7550.7690, ppl:  16.5771, duration: 4.3235s\n",
      "2021-08-04 08:08:19,546 - INFO - joeynmt.training - Epoch  24: total training loss 625.19\n",
      "2021-08-04 08:08:19,546 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-08-04 08:08:23,314 - INFO - joeynmt.training - Epoch  25, Step:    18700, Batch Loss:     1.620291, Tokens per Sec:     4981, Lr: 0.000147\n",
      "2021-08-04 08:08:36,635 - INFO - joeynmt.training - Epoch  25, Step:    18800, Batch Loss:     1.526772, Tokens per Sec:     5452, Lr: 0.000147\n",
      "2021-08-04 08:08:41,612 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:08:41,613 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:08:41,613 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:08:41,614 - INFO - joeynmt.training - \tHypothesis: For love does not love one another .\n",
      "2021-08-04 08:08:41,614 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:08:41,614 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:08:41,615 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:08:41,615 - INFO - joeynmt.training - \tHypothesis: Beloved , do not be My brethren , but I speak to you ; but I am not a few time .\n",
      "2021-08-04 08:08:41,615 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:08:41,616 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:08:41,616 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:08:41,616 - INFO - joeynmt.training - \tHypothesis: And when Jesus had said to them , “ Take heed and beware of the Pharisees and the Pharisees . ”\n",
      "2021-08-04 08:08:41,617 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:08:41,617 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:08:41,618 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:08:41,618 - INFO - joeynmt.training - \tHypothesis: For they did not know that it was given to me to stay , but the wind was near .\n",
      "2021-08-04 08:08:41,618 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    18800: bleu:  10.53, loss: 7535.1445, ppl:  16.4810, duration: 4.9831s\n",
      "2021-08-04 08:08:55,337 - INFO - joeynmt.training - Epoch  25, Step:    18900, Batch Loss:     1.655277, Tokens per Sec:     5111, Lr: 0.000103\n",
      "2021-08-04 08:09:08,697 - INFO - joeynmt.training - Epoch  25, Step:    19000, Batch Loss:     1.904296, Tokens per Sec:     5312, Lr: 0.000103\n",
      "2021-08-04 08:09:13,148 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:09:13,149 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:09:13,150 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:09:13,150 - INFO - joeynmt.training - \tHypothesis: For all love is not in heart .\n",
      "2021-08-04 08:09:13,150 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:09:13,151 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:09:13,151 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:09:13,151 - INFO - joeynmt.training - \tHypothesis: Beloved , I do not be afraid ; but the Lord is in this age , yet I do not have a few time .\n",
      "2021-08-04 08:09:13,152 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:09:13,152 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:09:13,152 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:09:13,155 - INFO - joeynmt.training - \tHypothesis: And when Jesus had called them , He said to them , “ Take heed to the Pharisees and the Pharisees . ”\n",
      "2021-08-04 08:09:13,155 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:09:13,156 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:09:13,156 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:09:13,156 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that the temptation was given to the ship , but the wind ceased .\n",
      "2021-08-04 08:09:13,157 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    19000: bleu:  10.14, loss: 7518.1011, ppl:  16.3769, duration: 4.4593s\n",
      "2021-08-04 08:09:16,694 - INFO - joeynmt.training - Epoch  25: total training loss 620.56\n",
      "2021-08-04 08:09:16,695 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-08-04 08:09:26,972 - INFO - joeynmt.training - Epoch  26, Step:    19100, Batch Loss:     1.919155, Tokens per Sec:     5216, Lr: 0.000103\n",
      "2021-08-04 08:09:40,343 - INFO - joeynmt.training - Epoch  26, Step:    19200, Batch Loss:     1.814786, Tokens per Sec:     5334, Lr: 0.000103\n",
      "2021-08-04 08:09:44,569 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:09:44,569 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:09:44,570 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:09:44,570 - INFO - joeynmt.training - \tHypothesis: For in love , not in deceitful .\n",
      "2021-08-04 08:09:44,570 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:09:44,572 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:09:44,572 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:09:44,573 - INFO - joeynmt.training - \tHypothesis: Brethren , stand fast in the Lord , and I do not have the same mind ; but I do not have the same time .\n",
      "2021-08-04 08:09:44,573 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:09:44,574 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:09:44,574 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:09:44,574 - INFO - joeynmt.training - \tHypothesis: And when Jesus had called them , He said to them , “ Take heed to the Pharisees and the Pharisees of the Pharisees . ”\n",
      "2021-08-04 08:09:44,574 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:09:44,575 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:09:44,575 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:09:44,576 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that it was given to stay for the south wind , but the camp was near .\n",
      "2021-08-04 08:09:44,576 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step    19200: bleu:  10.02, loss: 7546.3423, ppl:  16.5498, duration: 4.2322s\n",
      "2021-08-04 08:09:58,392 - INFO - joeynmt.training - Epoch  26, Step:    19300, Batch Loss:     2.099090, Tokens per Sec:     5164, Lr: 0.000103\n",
      "2021-08-04 08:10:08,872 - INFO - joeynmt.training - Epoch  26: total training loss 609.91\n",
      "2021-08-04 08:10:08,873 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-08-04 08:10:11,844 - INFO - joeynmt.training - Epoch  27, Step:    19400, Batch Loss:     1.877039, Tokens per Sec:     5376, Lr: 0.000103\n",
      "2021-08-04 08:10:16,183 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:10:16,184 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:10:16,185 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:10:16,185 - INFO - joeynmt.training - \tHypothesis: Let all love be without understanding , not of evil .\n",
      "2021-08-04 08:10:16,185 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:10:16,186 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:10:16,186 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:10:16,186 - INFO - joeynmt.training - \tHypothesis: Brethren , I write to you , brethren , I am not worthy to speak ; but I do not have the Lord , but that you may be a short .\n",
      "2021-08-04 08:10:16,187 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:10:16,187 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:10:16,188 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:10:16,188 - INFO - joeynmt.training - \tHypothesis: And when Jesus had called them , He said to them , “ Take heed to the Pharisees and to the Pharisees . ”\n",
      "2021-08-04 08:10:16,188 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:10:16,189 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:10:16,189 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:10:16,189 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that it was given to the place that the wind should not enter the mountain .\n",
      "2021-08-04 08:10:16,189 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    19400: bleu:  11.54, loss: 7535.3730, ppl:  16.4824, duration: 4.3444s\n",
      "2021-08-04 08:10:30,549 - INFO - joeynmt.training - Epoch  27, Step:    19500, Batch Loss:     1.409875, Tokens per Sec:     4980, Lr: 0.000103\n",
      "2021-08-04 08:10:43,966 - INFO - joeynmt.training - Epoch  27, Step:    19600, Batch Loss:     1.595204, Tokens per Sec:     5400, Lr: 0.000103\n",
      "2021-08-04 08:10:48,385 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:10:48,387 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:10:48,387 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:10:48,387 - INFO - joeynmt.training - \tHypothesis: For in love , not in love , not with deceitful .\n",
      "2021-08-04 08:10:48,387 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:10:48,388 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:10:48,388 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:10:48,389 - INFO - joeynmt.training - \tHypothesis: Brethren , brethren , I do not be afraid ; but the Lord is coming , yet I will speak to you .\n",
      "2021-08-04 08:10:48,389 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:10:48,390 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:10:48,391 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:10:48,391 - INFO - joeynmt.training - \tHypothesis: Then Jesus , answering , said , “ Take heed and be filled with the Pharisees . ”\n",
      "2021-08-04 08:10:48,391 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:10:48,392 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:10:48,392 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:10:48,392 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that it was given to the place that the wind should not enter the mountain .\n",
      "2021-08-04 08:10:48,393 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    19600: bleu:  10.27, loss: 7560.9707, ppl:  16.6401, duration: 4.4267s\n",
      "2021-08-04 08:11:02,229 - INFO - joeynmt.training - Epoch  27, Step:    19700, Batch Loss:     2.209237, Tokens per Sec:     5115, Lr: 0.000103\n",
      "2021-08-04 08:11:06,124 - INFO - joeynmt.training - Epoch  27: total training loss 605.44\n",
      "2021-08-04 08:11:06,125 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-08-04 08:11:15,615 - INFO - joeynmt.training - Epoch  28, Step:    19800, Batch Loss:     1.543088, Tokens per Sec:     5284, Lr: 0.000103\n",
      "2021-08-04 08:11:20,390 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:11:20,391 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:11:20,392 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:11:20,392 - INFO - joeynmt.training - \tHypothesis: All things are not of one mind , not of one mind .\n",
      "2021-08-04 08:11:20,392 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:11:20,393 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:11:20,393 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:11:20,393 - INFO - joeynmt.training - \tHypothesis: Beloved , I do not be children , brethren , but the Lord is in this age ; but I do not want you to boast .\n",
      "2021-08-04 08:11:20,393 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:11:20,394 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:11:20,394 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:11:20,395 - INFO - joeynmt.training - \tHypothesis: And when Jesus had called them , He said , “ Take heed to yourselves . ”\n",
      "2021-08-04 08:11:20,395 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:11:20,396 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:11:20,396 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:11:20,396 - INFO - joeynmt.training - \tHypothesis: For they did not know it , but that it was given to the place that the wind should not enter the mountain . ”\n",
      "2021-08-04 08:11:20,397 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step    19800: bleu:  10.91, loss: 7546.7944, ppl:  16.5526, duration: 4.7804s\n",
      "2021-08-04 08:11:34,232 - INFO - joeynmt.training - Epoch  28, Step:    19900, Batch Loss:     1.554742, Tokens per Sec:     5194, Lr: 0.000103\n",
      "2021-08-04 08:11:47,550 - INFO - joeynmt.training - Epoch  28, Step:    20000, Batch Loss:     1.885536, Tokens per Sec:     5241, Lr: 0.000103\n",
      "2021-08-04 08:11:51,979 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:11:51,980 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:11:51,980 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:11:51,981 - INFO - joeynmt.training - \tHypothesis: All things are not of one mind , not of evil .\n",
      "2021-08-04 08:11:51,981 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:11:51,982 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:11:51,982 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:11:51,982 - INFO - joeynmt.training - \tHypothesis: Brethren , I write to you , brethren , I am not worthy to come . But I will speak the Lord , but you will be troubled .\n",
      "2021-08-04 08:11:51,982 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:11:51,983 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:11:51,983 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:11:51,983 - INFO - joeynmt.training - \tHypothesis: Then Jesus said to them , “ Take heed and beware of the Pharisees . ”\n",
      "2021-08-04 08:11:51,984 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:11:51,984 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:11:51,985 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:11:51,985 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that the temptation was given to the ship . But when we had come near the mountain , we would have a great place . ”\n",
      "2021-08-04 08:11:51,985 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step    20000: bleu:  10.19, loss: 7560.5938, ppl:  16.6378, duration: 4.4343s\n",
      "2021-08-04 08:12:03,726 - INFO - joeynmt.training - Epoch  28: total training loss 607.95\n",
      "2021-08-04 08:12:03,727 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-08-04 08:12:05,736 - INFO - joeynmt.training - Epoch  29, Step:    20100, Batch Loss:     1.839875, Tokens per Sec:     5079, Lr: 0.000072\n",
      "2021-08-04 08:12:19,115 - INFO - joeynmt.training - Epoch  29, Step:    20200, Batch Loss:     1.710794, Tokens per Sec:     5401, Lr: 0.000072\n",
      "2021-08-04 08:12:23,671 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:12:23,671 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:12:23,672 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:12:23,673 - INFO - joeynmt.training - \tHypothesis: Let all love be without understanding .\n",
      "2021-08-04 08:12:23,674 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:12:23,674 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:12:23,675 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:12:23,675 - INFO - joeynmt.training - \tHypothesis: Brethren , I write to you , brethren , I am not worthy to speak to you ; but I do not want you to boast in the Lord .\n",
      "2021-08-04 08:12:23,675 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:12:23,676 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:12:23,676 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:12:23,676 - INFO - joeynmt.training - \tHypothesis: And when Jesus had looked around at the midst of the Pharisees , He said to them , “ Be of the Pharisees . ”\n",
      "2021-08-04 08:12:23,677 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:12:23,677 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:12:23,677 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:12:23,678 - INFO - joeynmt.training - \tHypothesis: For they did not know this , but that it was given to the place that the wind should not enter the mountain .\n",
      "2021-08-04 08:12:23,678 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    20200: bleu:  10.76, loss: 7548.8721, ppl:  16.5654, duration: 4.5626s\n",
      "2021-08-04 08:12:37,490 - INFO - joeynmt.training - Epoch  29, Step:    20300, Batch Loss:     1.618964, Tokens per Sec:     5092, Lr: 0.000072\n",
      "2021-08-04 08:12:50,876 - INFO - joeynmt.training - Epoch  29, Step:    20400, Batch Loss:     1.939601, Tokens per Sec:     5381, Lr: 0.000072\n",
      "2021-08-04 08:12:55,126 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:12:55,127 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:12:55,127 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:12:55,128 - INFO - joeynmt.training - \tHypothesis: Let all love be without understanding , not of one mind .\n",
      "2021-08-04 08:12:55,128 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:12:55,128 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:12:55,129 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:12:55,129 - INFO - joeynmt.training - \tHypothesis: Brethren , brethren , I do not be afraid ; but I will speak to you the Lord , but I will not be troubled .\n",
      "2021-08-04 08:12:55,129 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:12:55,130 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:12:55,130 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:12:55,130 - INFO - joeynmt.training - \tHypothesis: And when Jesus had called them , He said to them , “ Take heed and be filled with the Pharisees . ”\n",
      "2021-08-04 08:12:55,131 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:12:55,131 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:12:55,132 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:12:55,132 - INFO - joeynmt.training - \tHypothesis: For they did not know it , but that it was given to the place that the wind should not enter the mountain .\n",
      "2021-08-04 08:12:55,132 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    20400: bleu:  10.32, loss: 7541.1548, ppl:  16.5179, duration: 4.2560s\n",
      "2021-08-04 08:13:00,190 - INFO - joeynmt.training - Epoch  29: total training loss 595.78\n",
      "2021-08-04 08:13:00,190 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-08-04 08:13:08,886 - INFO - joeynmt.training - Epoch  30, Step:    20500, Batch Loss:     1.950330, Tokens per Sec:     5073, Lr: 0.000072\n",
      "2021-08-04 08:13:22,306 - INFO - joeynmt.training - Epoch  30, Step:    20600, Batch Loss:     1.541532, Tokens per Sec:     5405, Lr: 0.000072\n",
      "2021-08-04 08:13:26,745 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:13:26,746 - INFO - joeynmt.training - \tSource:     obuheeli shibuli nebiima ebibi kata okhwiyenyela noho , okhusinyikha tawe .\n",
      "2021-08-04 08:13:26,747 - INFO - joeynmt.training - \tReference:  does not behave rudely , does not seek its own , is not provoked , thinks no evil ;\n",
      "2021-08-04 08:13:26,747 - INFO - joeynmt.training - \tHypothesis: All love does not love one another .\n",
      "2021-08-04 08:13:26,747 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:13:26,748 - INFO - joeynmt.training - \tSource:     Mukhumalilisia , abaana befwe , mwikhoyenje mu , Omwami . Shiengalushilanga mukandamala okhuhandika , khale ta , nebutswa bwakhaba obulayi khwinywe , nengalushilamwo endio .\n",
      "2021-08-04 08:13:26,748 - INFO - joeynmt.training - \tReference:  Finally , my brethren , rejoice in the Lord . For me to write the same things to you is not tedious , but for you it is safe .\n",
      "2021-08-04 08:13:26,748 - INFO - joeynmt.training - \tHypothesis: Beloved , do not be children , brethren , but I speak the Lord ; but I am not of you . But this confident , that you may be a help .\n",
      "2021-08-04 08:13:26,749 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:13:26,749 - INFO - joeynmt.training - \tSource:     Yesu nabachetsiyia naboola ari “ Mube meeso , nimwiliinda abeene okhurula khwimela elia , Abafarisayo nende elia Herode . ”\n",
      "2021-08-04 08:13:26,750 - INFO - joeynmt.training - \tReference:  Then He charged them , saying , “ Take heed , beware of the leaven of the Pharisees and the leaven of Herod . ”\n",
      "2021-08-04 08:13:26,750 - INFO - joeynmt.training - \tHypothesis: And when Jesus had called them , He said to them , “ Take heed and sees the Pharisees of Israel . ”\n",
      "2021-08-04 08:13:26,750 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:13:26,751 - INFO - joeynmt.training - \tSource:     Shichila bakhaywa okhwisumilisilia , akalachiilisibwa mbu , “ Kata isolo niyiba niyakhasenakhwo , khulukuku olweshikulu eshio okhuula yilaswe namachina , ifwe . ”\n",
      "2021-08-04 08:13:26,751 - INFO - joeynmt.training - \tReference:  ( For they could not endure what was commanded : “ And if so much as a beast touches the mountain , it shall be stoned or shot with an arrow . ”\n",
      "2021-08-04 08:13:26,751 - INFO - joeynmt.training - \tHypothesis: For they did not know it , but that it was given to me to stay , the wind blew near .\n",
      "2021-08-04 08:13:26,751 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    20600: bleu:  10.98, loss: 7549.8081, ppl:  16.5712, duration: 4.4445s\n",
      "2021-08-04 08:13:40,472 - INFO - joeynmt.training - Epoch  30, Step:    20700, Batch Loss:     1.572777, Tokens per Sec:     5163, Lr: 0.000072\n",
      "2021-08-04 08:13:52,408 - INFO - joeynmt.training - Epoch  30: total training loss 589.75\n",
      "2021-08-04 08:13:52,408 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-08-04 08:13:52,409 - INFO - joeynmt.training - Best validation result (greedy) at step    17600:  16.19 ppl.\n",
      "2021-08-04 08:13:52,435 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-08-04 08:13:52,931 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 08:13:53,186 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 08:13:53,263 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/dev.bpe.en)...\n",
      "2021-08-04 08:13:58,834 - INFO - joeynmt.prediction -  dev bleu[13a]:  11.22 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-04 08:13:58,854 - INFO - joeynmt.prediction - Translations saved to: models/lhen_reverse_transformer_continued/00017600.hyps.dev\n",
      "2021-08-04 08:13:58,854 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhya/test.bpe.en)...\n",
      "2021-08-04 08:14:03,823 - INFO - joeynmt.prediction - test bleu[13a]:  12.66 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-04 08:14:03,828 - INFO - joeynmt.prediction - Translations saved to: models/lhen_reverse_transformer_continued/00017600.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_lhen_reload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bgrg-2UJGGZp",
    "outputId": "bd7df222-0f17-4fcc-de1a-68bbdd46c1e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 10600\tLoss: 7617.05957\tPPL: 16.99083\tbleu: 8.62748\tLR: 0.00030000\t\n",
      "Steps: 10800\tLoss: 7638.00293\tPPL: 17.12368\tbleu: 9.12874\tLR: 0.00030000\t\n",
      "Steps: 11000\tLoss: 7664.13281\tPPL: 17.29088\tbleu: 8.35299\tLR: 0.00030000\t\n",
      "Steps: 11200\tLoss: 7594.65723\tPPL: 16.84986\tbleu: 8.34067\tLR: 0.00030000\t*\n",
      "Steps: 11400\tLoss: 7604.81201\tPPL: 16.91362\tbleu: 8.94634\tLR: 0.00030000\t\n",
      "Steps: 11600\tLoss: 7596.60742\tPPL: 16.86209\tbleu: 9.35923\tLR: 0.00030000\t\n",
      "Steps: 11800\tLoss: 7614.91504\tPPL: 16.97728\tbleu: 8.90905\tLR: 0.00030000\t\n",
      "Steps: 12000\tLoss: 7560.38281\tPPL: 16.63646\tbleu: 9.33145\tLR: 0.00030000\t*\n",
      "Steps: 12200\tLoss: 7593.83594\tPPL: 16.84472\tbleu: 8.53919\tLR: 0.00030000\t\n",
      "Steps: 12400\tLoss: 7621.66943\tPPL: 17.01998\tbleu: 9.02539\tLR: 0.00030000\t\n",
      "Steps: 12600\tLoss: 7559.97363\tPPL: 16.63392\tbleu: 9.00430\tLR: 0.00030000\t*\n",
      "Steps: 12800\tLoss: 7597.33887\tPPL: 16.86668\tbleu: 9.35436\tLR: 0.00030000\t\n",
      "Steps: 13000\tLoss: 7606.21094\tPPL: 16.92242\tbleu: 9.21840\tLR: 0.00030000\t\n",
      "Steps: 13200\tLoss: 7605.77197\tPPL: 16.91965\tbleu: 9.44939\tLR: 0.00030000\t\n",
      "Steps: 13400\tLoss: 7538.66113\tPPL: 16.50261\tbleu: 10.16722\tLR: 0.00030000\t*\n",
      "Steps: 13600\tLoss: 7547.88330\tPPL: 16.55930\tbleu: 9.47819\tLR: 0.00030000\t\n",
      "Steps: 13800\tLoss: 7572.78174\tPPL: 16.71334\tbleu: 8.83460\tLR: 0.00030000\t\n",
      "Steps: 14000\tLoss: 7580.36279\tPPL: 16.76053\tbleu: 10.06193\tLR: 0.00030000\t\n",
      "Steps: 14200\tLoss: 7605.54688\tPPL: 16.91824\tbleu: 8.27910\tLR: 0.00030000\t\n",
      "Steps: 14400\tLoss: 7581.34912\tPPL: 16.76668\tbleu: 9.19178\tLR: 0.00030000\t\n",
      "Steps: 14600\tLoss: 7569.25684\tPPL: 16.69145\tbleu: 9.35236\tLR: 0.00021000\t\n",
      "Steps: 14800\tLoss: 7534.43457\tPPL: 16.47669\tbleu: 9.37685\tLR: 0.00021000\t*\n",
      "Steps: 15000\tLoss: 7541.77002\tPPL: 16.52170\tbleu: 9.13984\tLR: 0.00021000\t\n",
      "Steps: 15200\tLoss: 7531.77637\tPPL: 16.46041\tbleu: 9.76803\tLR: 0.00021000\t*\n",
      "Steps: 15400\tLoss: 7546.97266\tPPL: 16.55370\tbleu: 9.85324\tLR: 0.00021000\t\n",
      "Steps: 15600\tLoss: 7548.72119\tPPL: 16.56446\tbleu: 10.84264\tLR: 0.00021000\t\n",
      "Steps: 15800\tLoss: 7535.96680\tPPL: 16.48608\tbleu: 9.29759\tLR: 0.00021000\t\n",
      "Steps: 16000\tLoss: 7581.56104\tPPL: 16.76800\tbleu: 9.72433\tLR: 0.00021000\t\n",
      "Steps: 16200\tLoss: 7488.98828\tPPL: 16.20056\tbleu: 10.02692\tLR: 0.00021000\t*\n",
      "Steps: 16400\tLoss: 7521.48535\tPPL: 16.39754\tbleu: 9.99810\tLR: 0.00021000\t\n",
      "Steps: 16600\tLoss: 7532.79395\tPPL: 16.46664\tbleu: 10.85221\tLR: 0.00021000\t\n",
      "Steps: 16800\tLoss: 7526.54492\tPPL: 16.42842\tbleu: 10.51929\tLR: 0.00021000\t\n",
      "Steps: 17000\tLoss: 7519.04053\tPPL: 16.38263\tbleu: 10.17506\tLR: 0.00021000\t\n",
      "Steps: 17200\tLoss: 7523.02832\tPPL: 16.40695\tbleu: 9.52468\tLR: 0.00021000\t\n",
      "Steps: 17400\tLoss: 7530.92285\tPPL: 16.45518\tbleu: 9.66602\tLR: 0.00014700\t\n",
      "Steps: 17600\tLoss: 7487.38379\tPPL: 16.19090\tbleu: 11.43782\tLR: 0.00014700\t*\n",
      "Steps: 17800\tLoss: 7539.66113\tPPL: 16.50875\tbleu: 10.23890\tLR: 0.00014700\t\n",
      "Steps: 18000\tLoss: 7530.13281\tPPL: 16.45035\tbleu: 10.10997\tLR: 0.00014700\t\n",
      "Steps: 18200\tLoss: 7534.53027\tPPL: 16.47728\tbleu: 10.57937\tLR: 0.00014700\t\n",
      "Steps: 18400\tLoss: 7562.87793\tPPL: 16.65190\tbleu: 10.26246\tLR: 0.00014700\t\n",
      "Steps: 18600\tLoss: 7550.76904\tPPL: 16.57709\tbleu: 10.96049\tLR: 0.00014700\t\n",
      "Steps: 18800\tLoss: 7535.14453\tPPL: 16.48104\tbleu: 10.53214\tLR: 0.00010290\t\n",
      "Steps: 19000\tLoss: 7518.10107\tPPL: 16.37691\tbleu: 10.14240\tLR: 0.00010290\t\n",
      "Steps: 19200\tLoss: 7546.34229\tPPL: 16.54982\tbleu: 10.02495\tLR: 0.00010290\t\n",
      "Steps: 19400\tLoss: 7535.37305\tPPL: 16.48244\tbleu: 11.53757\tLR: 0.00010290\t\n",
      "Steps: 19600\tLoss: 7560.97070\tPPL: 16.64009\tbleu: 10.26555\tLR: 0.00010290\t\n",
      "Steps: 19800\tLoss: 7546.79443\tPPL: 16.55260\tbleu: 10.90876\tLR: 0.00010290\t\n",
      "Steps: 20000\tLoss: 7560.59375\tPPL: 16.63776\tbleu: 10.18880\tLR: 0.00007203\t\n",
      "Steps: 20200\tLoss: 7548.87207\tPPL: 16.56539\tbleu: 10.75988\tLR: 0.00007203\t\n",
      "Steps: 20400\tLoss: 7541.15479\tPPL: 16.51792\tbleu: 10.31929\tLR: 0.00007203\t\n",
      "Steps: 20600\tLoss: 7549.80811\tPPL: 16.57116\tbleu: 10.97998\tLR: 0.00007203\t\n"
     ]
    }
   ],
   "source": [
    "! cat \"joeynmt/models/lhen_reverse_transformer_continued/validations.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NmXKvDRSLMo"
   },
   "source": [
    "#### Sample translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVY0eGK7SP8O",
    "outputId": "05fd2f38-886b-483a-d66e-faff501615ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I know that I am a son of our brother , and sent him away from my brother to death .\n",
      "But we have no need of God for us , that we might be justified by the grace of God through Jesus Christ ,\n",
      "For I remember that the gospel of the gospel which was in me , that I might wish with you in the beginning\n",
      "For He taught them as some of the scribes , but they did not receive authority .\n",
      "Paul , an apostle of Jesus Christ , according to the will of God our Lord Jesus Christ ,\n",
      "Now when the disciples had come into the middle of the boat , they saw the linen cloth lying on the sea , and they came to Him .\n",
      "that you may walk according to the flesh , according to the lust of a good man , according to the resurrection of Christ ,\n",
      "Therefore , “ If anyone says , “ Lord , if we are willing , let us eat such things . ”\n",
      "You are chosen by God , who is sanctified by Him who is sanctified by the flesh ?\n",
      "“ I have many things to you , but you do not seek Me .\n"
     ]
    }
   ],
   "source": [
    "# Candidates\n",
    "! head \"joeynmt/models/lhen_reverse_transformer_continued/00017600.hyps.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xUyfhcTSrZD",
    "outputId": "83f6ddc8-0e67-4471-cc3b-a0a2c1cc3f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know that our brother Timothy has been set free , with whom I shall see you if he comes shortly .\n",
      "And not only that , but we also rejoice in God through our Lord Jesus Christ , through whom we have now received the reconciliation .\n",
      "You know that because of physical infirmity I preached the gospel to you at the first .\n",
      "for He taught them as one having authority , and not as the scribes .\n",
      "Paul , an apostle of Jesus Christ by the will of God , and Timothy our brother ,\n",
      "So when they had rowed about three or four miles , they saw Jesus walking on the sea and drawing near the boat ; and they were afraid .\n",
      "that you may approve the things that are excellent , that you may be sincere and without offense till the day of Christ ,\n",
      "Instead you ought to say , “ If the Lord wills , we shall live and do this or that . ”\n",
      "Foolish ones ! Did not He who made the outside make the inside also ?\n",
      "“ I still have many things to say to you , but you can not bear them now .\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "! head \"test.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1eIiI68SkIU",
    "outputId": "9d76c800-04c4-4c2c-cbea-c94cf71824ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ndenya , mumanye mbu omwana wefwe Timotseyo yaboololwe , okhurula mumbohe . Naba niyakhetsa lwangu nembe , ninaye olwa nanditse okhumulola inywe .\n",
      "Nebutswa shikali ako konyene ta ; khwikhoyanga khulwa okhubela aka Nyasaye yakhukholelaokhubirira mu Yesu , Kristo , owakhukholile bulano okhuba abetsa ba Nyasaye . Adamu nende Kristo ,\n",
      "Mwitsulilanga eshiachila isie , nemuyaalila Injiili olwambeli lwene ; kali shichila mbu , ndali nindwala .\n",
      "okhuba shiyabeechesia shinga abeechesia bandi , bamalako bechesinjia ta , habula yabeechesia nobunyali .\n",
      "Eyirula khwisie Paulo , ouli omurume wa Yesu Kristo , khulwa okhwenya khwa Nyasaye , khandi okhurula khu , Timotseyo omwana wefwe .\n",
      "Ne olwa abeechi bali nibafuchile , eliaro oluchendo oluhela tsimailo tsitaru noho tsine , balola , Yesu nachendanga khumaatsi , niyetsa ahambi nende , eliaro , nibaria muno .\n",
      "kho , munyalilwe okhwahula akali amalayi okhushila . Mana , mulaba abalekhuule okhurula mushifwabi shiosi , khandi , okhubula eshikha khunyanga eya Kristo alikalushilakhwo. ,\n",
      "Akamukhoyeele okhuboola , ngakano : “ Omwami naba niyakhachama , khwakhamenye , khukhole kano noho ako . ”\n",
      "Inywe abayingwa , Nyasaye owalonga , khwikulu khwabio shiali ye khandi owalonga mukari , wabio ?\n",
      "“ Isie endi namakhuwa amanji akokhumuboolela nebutswa bulano shimunyala okhukefwila tawe .\n"
     ]
    }
   ],
   "source": [
    "! head \"test.lh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SmqbLB2Qiw8"
   },
   "source": [
    "### Reverse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "vkwaVphVNc2T"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (source_language, target_language3)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{source_language}{target_language3}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language3}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"joeynmt/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 1096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 200         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\", source_language=source_language, target_language3=target_language3)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXxz_b-21ZY6",
    "outputId": "2674fd41-8f85-4f15-e22a-8921ba249c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-10 10:01:14,809 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-10 10:01:14,859 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-10 10:01:16,638 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-10 10:01:17,391 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-10 10:01:18,749 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-10 10:01:20,134 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-10 10:01:20,134 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 10:01:20,341 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 10:01:20.573978: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-10 10:01:22,413 - INFO - joeynmt.training - Total params: 12097024\n",
      "2021-07-10 10:01:25,778 - INFO - joeynmt.helpers - cfg.name                           : enlh_transformer\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.trg                       : lh\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1096\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 200\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/enlh_transformer\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 5904,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "\t[TRG] Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - Number of Src words (types): 4050\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - Number of Trg words (types): 4050\n",
      "2021-07-10 10:01:25,789 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4050),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4050))\n",
      "2021-07-10 10:01:25,807 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 1096\n",
      "\ttotal batch size (w. parallel & accumulation): 1096\n",
      "2021-07-10 10:01:25,808 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-10 10:01:31,873 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.710139, Tokens per Sec:    11926, Lr: 0.000300\n",
      "2021-07-10 10:01:37,775 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.553401, Tokens per Sec:    12046, Lr: 0.000300\n",
      "2021-07-10 10:02:17,566 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:02:17,567 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:02:18,261 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:02:18,262 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:02:18,262 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:02:18,262 - INFO - joeynmt.training - \tHypothesis: Ne “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ , “ , “ “ “ “ , “ , “ , “ , “ , “ , “ , “ , “ , “ , “ , “ “ , “ , “ , Ne , “ , Ne , , , Ne , “ , “ , “ , “ , “ , “ , , , , , , , , Ne , “ , “ , “ , “ , “ , , , “ “ , “ , “\n",
      "2021-07-10 10:02:18,262 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:02:18,263 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:02:18,264 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:02:18,264 - INFO - joeynmt.training - \tHypothesis: Ne “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ , “ , “ “ “ “ , “ , “ , “ , “ , “ , “ , “ , “ , Ne , “ , “ , “ , “ , Ne , Ne , , , , Ne , Ne , Ne , Ne , Ne , Ne , Ne , , ,\n",
      "2021-07-10 10:02:18,264 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:02:18,265 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:02:18,265 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:02:18,265 - INFO - joeynmt.training - \tHypothesis: Ne “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ , “ “ “ “ “ ,\n",
      "2021-07-10 10:02:18,265 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:02:18,266 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:02:18,266 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:02:18,266 - INFO - joeynmt.training - \tHypothesis: Ne “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ , “ , “ “ “ “ , “ , “ , “ , “ , “ , “ , “ , “ , “ , “ , “ “ , “ , “ , “ , Ne , , , , Ne , “ , “ , “ , “ , “ , “ , , , , , , , , , “ , “ “ , “ , “ , “ , , , “ “ “ , “ , “\n",
      "2021-07-10 10:02:18,266 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      200: bleu:   0.04, loss: 204323.4844, ppl: 250.9473, duration: 40.4913s\n",
      "2021-07-10 10:02:24,502 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.236579, Tokens per Sec:    11311, Lr: 0.000300\n",
      "2021-07-10 10:02:24,729 - INFO - joeynmt.training - Epoch   1: total training loss 1716.58\n",
      "2021-07-10 10:02:24,729 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-10 10:02:30,935 - INFO - joeynmt.training - Epoch   2, Step:      400, Batch Loss:     5.197055, Tokens per Sec:    11092, Lr: 0.000300\n",
      "2021-07-10 10:03:13,946 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:03:13,947 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:03:14,652 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:03:14,654 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:03:14,654 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:03:14,654 - INFO - joeynmt.training - \tHypothesis: Ne , “ “ “ “ “ Ne , “ “ Ne , “ Ne , “ Ne , “ “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , Ne , “ Ne , “ Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , “ Ne\n",
      "2021-07-10 10:03:14,654 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:03:14,655 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:03:14,655 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:03:14,655 - INFO - joeynmt.training - \tHypothesis: Ne , “ “ “ “ “ Ne , “ “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne , Ne , “ Ne , Ne , Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , Ne , Ne , Ne , Ne , “ Ne , Ne , Ne , “ Ne , Ne , Ne , Ne , “ Ne , “ Ne ,\n",
      "2021-07-10 10:03:14,655 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:03:14,656 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:03:14,656 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:03:14,656 - INFO - joeynmt.training - \tHypothesis: Ne , “ “ “ Ne , “ “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne ,\n",
      "2021-07-10 10:03:14,656 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:03:14,657 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:03:14,657 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:03:14,657 - INFO - joeynmt.training - \tHypothesis: Ne , “ “ “ “ “ “ Ne , “ “ “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne , “ Ne , “ Ne , Ne , “ Ne , “ Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , “\n",
      "2021-07-10 10:03:14,657 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      400: bleu:   0.03, loss: 196611.8594, ppl: 203.7122, duration: 43.7214s\n",
      "2021-07-10 10:03:20,614 - INFO - joeynmt.training - Epoch   2, Step:      500, Batch Loss:     5.240234, Tokens per Sec:    11718, Lr: 0.000300\n",
      "2021-07-10 10:03:27,086 - INFO - joeynmt.training - Epoch   2, Step:      600, Batch Loss:     5.081684, Tokens per Sec:    10842, Lr: 0.000300\n",
      "2021-07-10 10:04:10,467 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:04:10,468 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:04:11,158 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:04:11,159 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:04:11,159 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:04:11,160 - INFO - joeynmt.training - \tHypothesis: Kho ari , “ “ “ “ “ “ “ “ “ “ “ Kho ari , “ “ “ “ “ “ Kho ari , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ Kho , “ “ “ Kho , “ Kho , “ “ Kho , “ “ “ “ “ Kho , “ Kho , “ “ Kho , “ Kho , “ Kho , “ “ Kho , “ Kho , “ “ “ Kho , “ Kho ari , “ “ “ “ “ “ Kho ari\n",
      "2021-07-10 10:04:11,160 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:04:11,160 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:04:11,160 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - \tHypothesis: Kho ari , “ “ “ “ “ “ “ “ “ “ “ “ Kho ari , “ “ “ “ “ “ Kho ari , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ Kho , “ “ “ “ Kho , “ “ “ “ “ Kho , “ “ “ “ Kho , “ Kho , “ “ Kho , “ Kho , “ Kho , “ “ “ Kho , “ Kho ari , “ Kho , “ Kho ari , “ “ “ “ “ “ Kho ari\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - \tHypothesis: Ne , , , , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , ne , ne , ne , ne , ne , ne , ne , ne ,\n",
      "2021-07-10 10:04:11,162 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:04:11,162 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:04:11,162 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:04:11,162 - INFO - joeynmt.training - \tHypothesis: Ne , , , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne ,\n",
      "2021-07-10 10:04:11,163 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      600: bleu:   0.20, loss: 185051.2031, ppl: 149.0213, duration: 44.0762s\n",
      "2021-07-10 10:04:11,850 - INFO - joeynmt.training - Epoch   2: total training loss 1609.12\n",
      "2021-07-10 10:04:11,850 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-10 10:04:17,114 - INFO - joeynmt.training - Epoch   3, Step:      700, Batch Loss:     4.843796, Tokens per Sec:    12053, Lr: 0.000300\n",
      "2021-07-10 10:04:23,508 - INFO - joeynmt.training - Epoch   3, Step:      800, Batch Loss:     4.941718, Tokens per Sec:    11057, Lr: 0.000300\n",
      "2021-07-10 10:05:06,621 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:05:06,621 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:05:06,621 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:05:06,870 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:05:06,871 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:05:08,004 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:05:08,005 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:05:08,005 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:05:08,005 - INFO - joeynmt.training - \tHypothesis: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-10 10:05:08,005 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - \tHypothesis: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , Nyasaye ,\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa ,\n",
      "2021-07-10 10:05:08,008 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step      800: bleu:   0.04, loss: 180492.9688, ppl: 131.7395, duration: 44.4991s\n",
      "2021-07-10 10:05:14,033 - INFO - joeynmt.training - Epoch   3, Step:      900, Batch Loss:     4.920640, Tokens per Sec:    12033, Lr: 0.000300\n",
      "2021-07-10 10:05:15,228 - INFO - joeynmt.training - Epoch   3: total training loss 1494.00\n",
      "2021-07-10 10:05:15,228 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-10 10:05:20,459 - INFO - joeynmt.training - Epoch   4, Step:     1000, Batch Loss:     4.775295, Tokens per Sec:    11661, Lr: 0.000300\n",
      "2021-07-10 10:06:03,729 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:06:03,730 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:06:03,730 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:06:03,825 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:06:03,825 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:06:04,504 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:06:04,505 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:06:04,505 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:06:04,505 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu\n",
      "2021-07-10 10:06:04,505 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:06:04,506 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:06:04,506 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:06:04,506 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu , nababoolela ari , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-10 10:06:04,506 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:06:04,507 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:06:04,507 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:06:04,507 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa ,\n",
      "2021-07-10 10:06:04,507 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:06:04,508 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:06:04,508 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:06:04,508 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa ,\n",
      "2021-07-10 10:06:04,508 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     1000: bleu:   0.05, loss: 177280.9688, ppl: 120.7798, duration: 44.0486s\n",
      "2021-07-10 10:06:10,456 - INFO - joeynmt.training - Epoch   4, Step:     1100, Batch Loss:     4.727084, Tokens per Sec:    11941, Lr: 0.000300\n",
      "2021-07-10 10:06:16,953 - INFO - joeynmt.training - Epoch   4, Step:     1200, Batch Loss:     4.852685, Tokens per Sec:    10960, Lr: 0.000300\n",
      "2021-07-10 10:07:00,015 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:07:00,015 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:07:00,016 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:07:00,377 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:07:00,377 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:07:01,056 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:07:01,056 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:07:01,057 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:07:01,057 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu\n",
      "2021-07-10 10:07:01,057 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:07:01,058 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:07:01,058 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:07:01,058 - INFO - joeynmt.training - \tHypothesis: Mana , nababoolela ari , “ Nranga , ne , “ Ntsanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne\n",
      "2021-07-10 10:07:01,058 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , nibayanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne ,\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:07:01,060 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:07:01,060 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , btsanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , btsilililililililililililililililililililililililililililililililililililililililil@@\n",
      "2021-07-10 10:07:01,060 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     1200: bleu:   0.14, loss: 174162.8906, ppl: 111.0134, duration: 44.1068s\n",
      "2021-07-10 10:07:02,301 - INFO - joeynmt.training - Epoch   4: total training loss 1464.13\n",
      "2021-07-10 10:07:02,301 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-10 10:07:07,077 - INFO - joeynmt.training - Epoch   5, Step:     1300, Batch Loss:     4.396784, Tokens per Sec:    11852, Lr: 0.000300\n",
      "2021-07-10 10:07:13,526 - INFO - joeynmt.training - Epoch   5, Step:     1400, Batch Loss:     4.548765, Tokens per Sec:    11202, Lr: 0.000300\n",
      "2021-07-10 10:07:56,671 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:07:56,672 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:07:56,672 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:07:56,994 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:07:56,994 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:07:57,665 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:07:57,666 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:07:57,666 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:07:57,666 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu Yesu Yesu Yesu Yesu Yesu , Yesu , Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu ari , “ Nboolela , “ Omwami , ne , wwalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal@@\n",
      "2021-07-10 10:07:57,666 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:07:57,667 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:07:57,667 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:07:57,667 - INFO - joeynmt.training - \tHypothesis: Mana , nibakalusia ari , “ Nranga , ne , owenya , ne , ne , owranga , ne , ne , owranga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , owranga , ne , owranga , owranga . ”\n",
      "2021-07-10 10:07:57,667 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - \tHypothesis: Mana , nibenya , nibenya , ne , nibenya , ne , ne , nibalalalalalalalalalalalalalalalalalalalalalalalalalala. ,\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:07:57,669 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:07:57,669 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , basanga , ne , nibasanga , ne , ne , nibasanga , ne , ne , nibasanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , basanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , branga ,\n",
      "2021-07-10 10:07:57,669 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     1400: bleu:   0.86, loss: 171274.1875, ppl: 102.6716, duration: 44.1429s\n",
      "2021-07-10 10:08:03,649 - INFO - joeynmt.training - Epoch   5, Step:     1500, Batch Loss:     4.469375, Tokens per Sec:    11930, Lr: 0.000300\n",
      "2021-07-10 10:08:05,020 - INFO - joeynmt.training - Epoch   5: total training loss 1409.95\n",
      "2021-07-10 10:08:05,021 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-10 10:08:10,176 - INFO - joeynmt.training - Epoch   6, Step:     1600, Batch Loss:     4.660630, Tokens per Sec:    10445, Lr: 0.000300\n",
      "2021-07-10 10:08:53,189 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:08:53,189 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:08:53,190 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:08:53,503 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:08:53,504 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:08:54,203 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:08:54,204 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:08:54,204 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:08:54,204 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu , nababoolela ari , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi , “ N. ”\n",
      "2021-07-10 10:08:54,204 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:08:54,205 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:08:54,205 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:08:54,205 - INFO - joeynmt.training - \tHypothesis: Mana Yesu nababoolela ari , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi\n",
      "2021-07-10 10:08:54,205 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:08:54,206 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:08:54,206 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:08:54,206 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibamanya mbu , “ Omundu , baboolile , ne , ne , ne , ne , ne , nibaranga .\n",
      "2021-07-10 10:08:54,206 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:08:54,207 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:08:54,207 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:08:54,207 - INFO - joeynmt.training - \tHypothesis: Ne olwa , olwa , nibaranga , ne , ne , ne , ne , nibaranga , ne , ne , ne , ne , ne , ne , nibaranga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , nibaranga , baranga .\n",
      "2021-07-10 10:08:54,207 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     1600: bleu:   0.78, loss: 168059.3906, ppl:  94.1230, duration: 44.0312s\n",
      "2021-07-10 10:09:00,263 - INFO - joeynmt.training - Epoch   6, Step:     1700, Batch Loss:     4.814085, Tokens per Sec:    11716, Lr: 0.000300\n",
      "2021-07-10 10:09:06,775 - INFO - joeynmt.training - Epoch   6, Step:     1800, Batch Loss:     4.419515, Tokens per Sec:    10809, Lr: 0.000300\n",
      "2021-07-10 10:09:48,977 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:09:48,978 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:09:48,978 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:09:49,257 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:09:49,257 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:09:50,342 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:09:50,343 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:09:50,343 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:09:50,343 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali yali yali yali yali yali yali , nababoolela ari , “ Neeeeeeeeeeeeeeeiiiio. ”\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali yali yali yali yali yali yali yali yali yali yali yali yali yali yali yali yali yali , naboola ari , “ Nalalalalalalalalalalalo. ”\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:09:50,345 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:09:50,345 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:09:50,345 - INFO - joeynmt.training - \tHypothesis: Mana , nibaboolela mbu , “ Neeeeeayo , nibirila .\n",
      "2021-07-10 10:09:50,345 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:09:50,346 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:09:50,346 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:09:50,346 - INFO - joeynmt.training - \tHypothesis: Mana , nibaria , nibiria , ne , nibiria , ne , nibiria , ne , nibiria , ne , nibiria , ne , nibaria .\n",
      "2021-07-10 10:09:50,346 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     1800: bleu:   1.46, loss: 165015.8750, ppl:  86.6867, duration: 43.5705s\n",
      "2021-07-10 10:09:52,123 - INFO - joeynmt.training - Epoch   6: total training loss 1388.87\n",
      "2021-07-10 10:09:52,123 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-10 10:09:56,229 - INFO - joeynmt.training - Epoch   7, Step:     1900, Batch Loss:     4.167235, Tokens per Sec:    12083, Lr: 0.000300\n",
      "2021-07-10 10:10:02,623 - INFO - joeynmt.training - Epoch   7, Step:     2000, Batch Loss:     4.246118, Tokens per Sec:    11227, Lr: 0.000300\n",
      "2021-07-10 10:10:45,607 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:10:45,608 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:10:45,608 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:10:45,911 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:10:45,912 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:10:46,605 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:10:46,606 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:10:46,606 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:10:46,606 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali , nibareeba ari , “ Nalalalaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaawe . ”\n",
      "2021-07-10 10:10:46,606 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:10:46,607 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:10:46,607 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:10:46,607 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali yali yali yali yali , naboolela , naboola ari , “ Nalalaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaawe . ”\n",
      "2021-07-10 10:10:46,608 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:10:46,609 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:10:46,609 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:10:46,609 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibaboolanga , ne , nibaboola , ne , nibahelesia .\n",
      "2021-07-10 10:10:46,609 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:10:46,610 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:10:46,610 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:10:46,610 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibaranga , nibiri , ne , nibiri , ne , nibaria , ne , nibaria , ne , nibaria , ne , nibaria , ne , nibaria , ne , nibaria .\n",
      "2021-07-10 10:10:46,610 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     2000: bleu:   1.23, loss: 162329.7500, ppl:  80.6133, duration: 43.9875s\n",
      "2021-07-10 10:10:52,618 - INFO - joeynmt.training - Epoch   7, Step:     2100, Batch Loss:     4.309657, Tokens per Sec:    12012, Lr: 0.000300\n",
      "2021-07-10 10:10:54,572 - INFO - joeynmt.training - Epoch   7: total training loss 1332.86\n",
      "2021-07-10 10:10:54,572 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-10 10:10:59,134 - INFO - joeynmt.training - Epoch   8, Step:     2200, Batch Loss:     4.481798, Tokens per Sec:    10428, Lr: 0.000300\n",
      "2021-07-10 10:11:42,278 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:11:42,278 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:11:42,279 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:11:42,603 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:11:42,604 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:11:43,285 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:11:43,285 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:11:43,286 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:11:43,286 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali , nibareeba ari , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi\n",
      "2021-07-10 10:11:43,286 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:11:43,286 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - \tHypothesis: Mana , nibareeba ari , “ Nolwa , olwa , bareeba mbu , “ Nalile , ne , ne , ne , olwa , owanyanga , ne , ne , ne , ne olwa olwa , owanyie . ”\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibaana befwe , nibirila , nibirila , nibirila , nibirila .\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - \tHypothesis: Mana , nibaana befwe , nibiri , nibiri , ne , nibiri , nibiru , ne , nibiru , ne , nibiru , ne , nibiru , ne , nibiru .\n",
      "2021-07-10 10:11:43,289 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     2200: bleu:   1.33, loss: 159668.2812, ppl:  75.0154, duration: 44.1539s\n",
      "2021-07-10 10:11:49,208 - INFO - joeynmt.training - Epoch   8, Step:     2300, Batch Loss:     4.213748, Tokens per Sec:    11908, Lr: 0.000300\n",
      "2021-07-10 10:11:55,842 - INFO - joeynmt.training - Epoch   8, Step:     2400, Batch Loss:     4.350402, Tokens per Sec:    10703, Lr: 0.000300\n",
      "2021-07-10 10:12:38,943 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:12:38,943 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:12:38,943 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:12:39,261 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:12:39,262 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:12:39,964 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:12:39,965 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:12:39,965 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:12:39,965 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibareeba bari , “ Nolwa , , , , Omwana wa , womuboolela mbu , “ Nalalalalalolie , womuboolela mbu , ‘ Nalalalalalalalie , womuboolela mbu , ‘ Nalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalo. ”\n",
      "2021-07-10 10:12:39,965 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - \tHypothesis: Ne olwa , yali , yali , namuboolela ari , “ Nalalalile , mbu , ‘ Omwami , womuboolela mbu , ‘ Omwami , womuboolela mbu , ‘ Omwami . ”\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - \tHypothesis: Ne olwa , bali , nibareeba mbu , “ Abalila , abandu bandi , nibatiile .\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:12:39,968 - INFO - joeynmt.training - \tHypothesis: Ne olwa , yali , niyiri , yamini , ne , niyiri , niyiri , ne , niyiria , ne , niyiria , ne , nibarumi , nibaranga , ne , nibaranga .\n",
      "2021-07-10 10:12:39,968 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     2400: bleu:   2.22, loss: 157068.5938, ppl:  69.9229, duration: 44.1252s\n",
      "2021-07-10 10:12:42,291 - INFO - joeynmt.training - Epoch   8: total training loss 1307.90\n",
      "2021-07-10 10:12:42,291 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-10 10:12:46,047 - INFO - joeynmt.training - Epoch   9, Step:     2500, Batch Loss:     4.650863, Tokens per Sec:    11731, Lr: 0.000300\n",
      "2021-07-10 10:12:52,569 - INFO - joeynmt.training - Epoch   9, Step:     2600, Batch Loss:     4.045369, Tokens per Sec:    10815, Lr: 0.000300\n",
      "2021-07-10 10:13:35,497 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:13:35,498 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:13:35,498 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:13:35,806 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:13:35,806 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:13:36,475 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:13:36,476 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:13:36,476 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:13:36,476 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali , niyareeba ari , “ Omundu yesi yesi , ouranga , ne , owenya , ne , ne , ne , niyaboolela ari , “ Shimulenya , okhuranga , okhurula , okhuranga , ne , mulenya , okhuranga , ne , mulenya , ne , okhuranga . ”\n",
      "2021-07-10 10:13:36,476 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:13:36,477 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:13:36,477 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:13:36,477 - INFO - joeynmt.training - \tHypothesis: Mana , Petero , nasinjiile , ne , nababoolela ari , “ Nalie , ” Yesu , namuboolela ari , “ Omwami , ne , shiyamanya mbu , “ Omwami . ”\n",
      "2021-07-10 10:13:36,477 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - \tHypothesis: Ne olwa , bali nibareeba , nibareeba mbu , baliho , nibareeba , nibareeba bari ,\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:13:36,479 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:13:36,479 - INFO - joeynmt.training - \tHypothesis: Mana , yali niyenjile , ne , niyiria , ne , niyiria , ne , nibibili , ne , nibibili , ne , nibibili , ne , nibibili , nibibili , ne , nibibili .\n",
      "2021-07-10 10:13:36,479 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     2600: bleu:   2.16, loss: 154639.6875, ppl:  65.4778, duration: 43.9100s\n",
      "2021-07-10 10:13:42,362 - INFO - joeynmt.training - Epoch   9, Step:     2700, Batch Loss:     4.370677, Tokens per Sec:    12076, Lr: 0.000300\n",
      "2021-07-10 10:13:45,369 - INFO - joeynmt.training - Epoch   9: total training loss 1272.80\n",
      "2021-07-10 10:13:45,369 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-10 10:13:48,777 - INFO - joeynmt.training - Epoch  10, Step:     2800, Batch Loss:     3.925977, Tokens per Sec:    11813, Lr: 0.000300\n",
      "2021-07-10 10:14:31,847 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:14:31,848 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:14:31,848 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:14:32,168 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:14:32,169 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:14:33,271 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:14:33,272 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:14:33,272 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:14:33,272 - INFO - joeynmt.training - \tHypothesis: Mana Yesu , nahulilakhwo , ari , “ Omwami , ne , namuboolela ari , “ Omwami , shiboolela ari , “ Omwami , shiboolanga mbu , ‘ Omwami , ndaboolile , mbu , ‘ Omwami . ”\n",
      "2021-07-10 10:14:33,272 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tHypothesis: Mana Yesu , namuboolela ari , “ Omwami , namuboolela ari , “ Omwami , namuboolela ari , “ Omwami , iwe , iwe , iwe , iwe , iwe , wamwinjisia . ”\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - \tHypothesis: Ne olwa , abandu bahulilakhwo amakhuwa ako , banyoola , ne , nibaboolile .\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - \tHypothesis: Mana Petero , nahulilakhwo amakhuwa ako , ne , yanyoola , ne , nanyoola , ne , nahonile , ne , nibaboolela ari , “ Abarula , ne , banyanga , banyanga , ne , bahonile .\n",
      "2021-07-10 10:14:33,275 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     2800: bleu:   2.34, loss: 152782.4688, ppl:  62.2706, duration: 44.4973s\n",
      "2021-07-10 10:14:39,318 - INFO - joeynmt.training - Epoch  10, Step:     2900, Batch Loss:     3.930479, Tokens per Sec:    11664, Lr: 0.000300\n",
      "2021-07-10 10:14:45,942 - INFO - joeynmt.training - Epoch  10, Step:     3000, Batch Loss:     4.256003, Tokens per Sec:    10974, Lr: 0.000300\n",
      "2021-07-10 10:15:29,079 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:15:29,080 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:15:29,080 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:15:29,393 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:15:29,394 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:15:30,077 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:15:30,078 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:15:30,078 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:15:30,078 - INFO - joeynmt.training - \tHypothesis: Mana Yesu , nabakalusia bari , “ Niwe , iwe , iwe , ne , ndenya , mbu , ‘ Niwe , ’ , ndenya , ne , ndenya , okhuboolela mbu , ‘ Shimulola , ’ , mwenya , okhuboolela mbu , ndenya . ”\n",
      "2021-07-10 10:15:30,079 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:15:30,079 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:15:30,079 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:15:30,080 - INFO - joeynmt.training - \tHypothesis: Mana , Petero , nasinjila , ne , naboolela Petero ari , “ Niwe , Omwana womundu , womundu , witsa , ’ ”\n",
      "2021-07-10 10:15:30,080 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:15:30,080 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:15:30,080 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:15:30,082 - INFO - joeynmt.training - \tHypothesis: Ne olwa , abandu batiila , nibatiila , ne nibanyoola , ne nibanyoola , ne nibanyoola .\n",
      "2021-07-10 10:15:30,082 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:15:30,084 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:15:30,084 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:15:30,085 - INFO - joeynmt.training - \tHypothesis: Mana , Paulo yali niyiria , ne , niyiria , ne , niyiria , ne , nibarula , ne , nibarula , nibarula , ne nibarula , nibarula , ne , nibarula , nibarula , okhusaala .\n",
      "2021-07-10 10:15:30,085 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     3000: bleu:   2.10, loss: 149558.0156, ppl:  57.0709, duration: 44.1420s\n",
      "2021-07-10 10:15:32,977 - INFO - joeynmt.training - Epoch  10: total training loss 1239.03\n",
      "2021-07-10 10:15:32,977 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-10 10:15:36,035 - INFO - joeynmt.training - Epoch  11, Step:     3100, Batch Loss:     3.824946, Tokens per Sec:    11748, Lr: 0.000300\n",
      "2021-07-10 10:15:42,500 - INFO - joeynmt.training - Epoch  11, Step:     3200, Batch Loss:     4.099649, Tokens per Sec:    10921, Lr: 0.000300\n",
      "2021-07-10 10:16:25,556 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:16:25,557 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:16:25,557 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:16:25,874 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:16:25,874 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:16:26,563 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:16:26,564 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:16:26,564 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:16:26,564 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyamanya mbu , Yesu , nababoolela ari , “ Niwe , iwe , wanje , ne , ndaboolile , mbu , ndaboolile , mbu , “ Omwami , ndaboolile . ”\n",
      "2021-07-10 10:16:26,564 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyamanya mbu , Yesu yali , niyaboolile , ne , nabareeba ari , “ Omwami , ”\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - \tHypothesis: Ne olwa bali nibaboolile , abandu bahonibwa , nibahonibwe .\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:16:26,568 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yali niyiranga , ne , nabiranga , ne , nabiranga , nabirukha , ne , nabirukha , nabirukha , nibahelesia , ne , nibahelesia , nibahelesia , okhusaala .\n",
      "2021-07-10 10:16:26,568 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step     3200: bleu:   2.16, loss: 147525.4375, ppl:  54.0187, duration: 44.0680s\n",
      "2021-07-10 10:16:32,454 - INFO - joeynmt.training - Epoch  11, Step:     3300, Batch Loss:     4.153135, Tokens per Sec:    12159, Lr: 0.000300\n",
      "2021-07-10 10:16:35,654 - INFO - joeynmt.training - Epoch  11: total training loss 1216.30\n",
      "2021-07-10 10:16:35,655 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-10 10:16:38,786 - INFO - joeynmt.training - Epoch  12, Step:     3400, Batch Loss:     3.855164, Tokens per Sec:     9793, Lr: 0.000300\n",
      "2021-07-10 10:17:21,771 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:17:21,772 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:17:21,772 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:17:22,051 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:17:22,052 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:17:22,732 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:17:22,733 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:17:22,733 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:17:22,733 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali niyaboolile , ne nababoolela ari , “ Omwami , ndaboolile , mbu , “ Niwe , ndenya , akanje , akanje , ne , ndaboolile . ”\n",
      "2021-07-10 10:17:22,733 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:17:22,734 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:17:22,734 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:17:22,734 - INFO - joeynmt.training - \tHypothesis: Mana Petero , niyaboole mbu , “ Niwe , owitsa , owomulakusi , womulakusi , womulakusi , womubatiisi . ”\n",
      "2021-07-10 10:17:22,734 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:17:22,735 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:17:22,735 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:17:22,735 - INFO - joeynmt.training - \tHypothesis: Mana , nibanyoola , mwitookho lia Yerusalemu , Yerusalemu , ne , nibenjile .\n",
      "2021-07-10 10:17:22,735 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:17:22,736 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:17:22,736 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:17:22,736 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yali niyiria , owali niyiria , niyiria , ne , niyiria , niyiria , nibenjile , ne , nibenjile , nibenjile .\n",
      "2021-07-10 10:17:22,736 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     3400: bleu:   2.61, loss: 146090.9688, ppl:  51.9634, duration: 43.9499s\n",
      "2021-07-10 10:17:28,765 - INFO - joeynmt.training - Epoch  12, Step:     3500, Batch Loss:     3.602729, Tokens per Sec:    11998, Lr: 0.000300\n",
      "2021-07-10 10:17:35,274 - INFO - joeynmt.training - Epoch  12, Step:     3600, Batch Loss:     3.857409, Tokens per Sec:    10963, Lr: 0.000300\n",
      "2021-07-10 10:18:16,089 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:18:16,089 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:18:16,090 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:18:16,396 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:18:16,397 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:18:17,126 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:18:17,127 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:18:17,127 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:18:17,127 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali , nababoolela ari , “ Omwechesia , shiendi , mbu , , ndenya , mbu , ndenya , mbu , ndenya , okhuboolela , mbu , ‘ Omboolile , okhuboolela . ”\n",
      "2021-07-10 10:18:17,128 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:18:17,128 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:18:17,128 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:18:17,128 - INFO - joeynmt.training - \tHypothesis: Mana , Petero yali niyareeba ari , “ Niwe , omwana wa , Yohana , ne , , yamwinjisia , omwana oyo , wamwinjisia . ”\n",
      "2021-07-10 10:18:17,129 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:18:17,129 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:18:17,129 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:18:17,129 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyiranga , abandu , bobushuru , bahona , ne , nibahona .\n",
      "2021-07-10 10:18:17,130 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:18:17,130 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:18:17,131 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:18:17,133 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yali niyirula , ne , nabirula , ne , nabirula , nibibili , nibibili , ne , nibirula , nibibili , nibibili , ne , nibibili .\n",
      "2021-07-10 10:18:17,133 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     3600: bleu:   2.40, loss: 143505.5312, ppl:  48.4545, duration: 41.8580s\n",
      "2021-07-10 10:18:20,727 - INFO - joeynmt.training - Epoch  12: total training loss 1179.26\n",
      "2021-07-10 10:18:20,728 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-10 10:18:23,142 - INFO - joeynmt.training - Epoch  13, Step:     3700, Batch Loss:     3.952638, Tokens per Sec:    11801, Lr: 0.000300\n",
      "2021-07-10 10:18:29,630 - INFO - joeynmt.training - Epoch  13, Step:     3800, Batch Loss:     3.736598, Tokens per Sec:    11070, Lr: 0.000300\n",
      "2021-07-10 10:19:12,795 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:19:12,796 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:19:12,796 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:19:13,115 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:19:13,116 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:19:14,218 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:19:14,219 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:19:14,220 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:19:14,220 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyaboolela ari , “ Omwechesia , mbu , ndenya , mbu , ndenya , mbu , ndenya , okhuboolela mbu , “ Nisie , ndenya , okhureere , okhureere . ”\n",
      "2021-07-10 10:19:14,220 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:19:14,220 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:19:14,221 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:19:14,221 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyenjile , ne , nalekha , ne , nababoolela ari , “ Omwechesia , mbu , niwe , Omwami , Omwami , witsa , wanje , ne , ndenya . ”\n",
      "2021-07-10 10:19:14,221 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:19:14,222 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:19:14,222 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:19:14,222 - INFO - joeynmt.training - \tHypothesis: Ne olwa , abandu baliho , baliho , baliho , Isabato , ne , nibenjile , abandu baliho .\n",
      "2021-07-10 10:19:14,222 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:19:14,223 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:19:14,223 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:19:14,223 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yali niyenjile , ne , nabibili , ne , nabibili , ne nabibili , nibenjela , ne , nibenjela , nibenjela , ne , nibenjela , nibenjela , ne , nibenjela .\n",
      "2021-07-10 10:19:14,223 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step     3800: bleu:   2.55, loss: 142290.7031, ppl:  46.8886, duration: 44.5923s\n",
      "2021-07-10 10:19:20,181 - INFO - joeynmt.training - Epoch  13, Step:     3900, Batch Loss:     3.605079, Tokens per Sec:    11667, Lr: 0.000300\n",
      "2021-07-10 10:19:24,530 - INFO - joeynmt.training - Epoch  13: total training loss 1153.43\n",
      "2021-07-10 10:19:24,531 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-10 10:19:26,616 - INFO - joeynmt.training - Epoch  14, Step:     4000, Batch Loss:     3.736794, Tokens per Sec:    11842, Lr: 0.000300\n",
      "2021-07-10 10:20:09,710 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:20:09,711 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:20:09,711 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:20:10,051 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:20:10,051 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:20:10,731 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:20:10,734 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:20:10,734 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:20:10,734 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyenya , ne , nababoolela ari , “ Omwechesia , ndenya , mbu , ndenya , mbu , ndenya , okhuboolela mbu , “ Enzenya , okhuboolela , mbu , ndenya , ndenya . ”\n",
      "2021-07-10 10:20:10,734 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:20:10,735 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:20:10,735 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:20:10,735 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyenjile , Yesu , yamanya mbu , Yesu , yamanya mbu , “ Omwechesia , wamwinjisia , ”\n",
      "2021-07-10 10:20:10,735 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyenjile , yanyoola , abandu boosi , banyoola , ne , nibanyoola , abandu boosi .\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:20:10,737 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:20:10,737 - INFO - joeynmt.training - \tHypothesis: Mana Paulo , nabaana befwe , bachendela , ne , nabibili , ne nabibili , ne nabibili , ne nabibili . Ne olwa yali niyenjile , niyenjile , ne nabasilia , nabibili .\n",
      "2021-07-10 10:20:10,737 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step     4000: bleu:   2.48, loss: 140957.5625, ppl:  45.2284, duration: 44.1203s\n",
      "2021-07-10 10:20:16,670 - INFO - joeynmt.training - Epoch  14, Step:     4100, Batch Loss:     3.925256, Tokens per Sec:    11941, Lr: 0.000300\n",
      "2021-07-10 10:20:23,090 - INFO - joeynmt.training - Epoch  14, Step:     4200, Batch Loss:     3.685305, Tokens per Sec:    11259, Lr: 0.000300\n",
      "2021-07-10 10:21:03,699 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:21:03,699 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:21:03,699 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:21:04,023 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:21:04,024 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:21:04,706 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:21:04,706 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:21:04,707 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:21:04,707 - INFO - joeynmt.training - \tHypothesis: Ne olwa , Abafarisayo , bahulilakhwo , Yesu , nababoolela ari , “ Omwechesia , ndenya , ne , ndenya , okhuboolela mbu , “ Endenya , okhuboolela mbu , ekhwo , isie , ndenya , okhuboolela . ”\n",
      "2021-07-10 10:21:04,707 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:21:04,707 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyechesinjia , abeechibe , ne , yakalukha munzu , ne , namuboolela ari , “ Omwechesia , obe , iwe , iwe , wanje , ”\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - \tHypothesis: Mana , abandu boosi boosi bali nibaliho , nibaliho , omundu yesi , owomukhonokwe .\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - \tHypothesis: Mana Paulo nende Sila nende Sila , nibabiria , ne nabibili , ne nabibili , ne nabibili , ne , nabibili , ne nabibili . Ne olwa bali nibanywa , nibanywe , nibenjile .\n",
      "2021-07-10 10:21:04,710 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step     4200: bleu:   2.81, loss: 139297.2969, ppl:  43.2427, duration: 41.6188s\n",
      "2021-07-10 10:21:08,868 - INFO - joeynmt.training - Epoch  14: total training loss 1130.41\n",
      "2021-07-10 10:21:08,869 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-10 10:21:10,643 - INFO - joeynmt.training - Epoch  15, Step:     4300, Batch Loss:     3.675772, Tokens per Sec:    11731, Lr: 0.000300\n",
      "2021-07-10 10:21:17,251 - INFO - joeynmt.training - Epoch  15, Step:     4400, Batch Loss:     3.507568, Tokens per Sec:    10898, Lr: 0.000300\n",
      "2021-07-10 10:21:54,961 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:21:54,961 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:21:54,962 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:21:55,286 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:21:55,286 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:21:56,434 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:21:56,435 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:21:56,435 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:21:56,435 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyechesinjia abeechibe , ne , nababoolela ari , “ Omwechesia , nisie , owitsa , wambelesia , okhuboolela , mbu , “ Enzenya , okhuboolela , mbu , ekhwo , nemboolile , okhukholelie . ”\n",
      "2021-07-10 10:21:56,435 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:21:56,436 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:21:56,436 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:21:56,436 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali niyechesinjia , ne , niyechesinjia abeechibe , ne , namuboolela ari , “ Omwechesia , ” , ”\n",
      "2021-07-10 10:21:56,436 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:21:56,437 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:21:56,437 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:21:56,437 - INFO - joeynmt.training - \tHypothesis: Mana , khunyanga eya Isabato , Isabato , yaliho , Isabato , ne , khunyanga eya Isabato .\n",
      "2021-07-10 10:21:56,437 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:21:56,438 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:21:56,438 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:21:56,438 - INFO - joeynmt.training - \tHypothesis: Mana Paulo natsia , ne , natiila , omuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwashio , kwomuyeka kwomuyeka , kwomuyeka kwashio , kwomuyeka , kwashio .\n",
      "2021-07-10 10:21:56,438 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step     4400: bleu:   3.08, loss: 138373.4219, ppl:  42.1757, duration: 39.1864s\n",
      "2021-07-10 10:22:02,347 - INFO - joeynmt.training - Epoch  15, Step:     4500, Batch Loss:     3.543476, Tokens per Sec:    12041, Lr: 0.000300\n",
      "2021-07-10 10:22:07,180 - INFO - joeynmt.training - Epoch  15: total training loss 1102.02\n",
      "2021-07-10 10:22:07,180 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-10 10:22:08,781 - INFO - joeynmt.training - Epoch  16, Step:     4600, Batch Loss:     3.689896, Tokens per Sec:    12181, Lr: 0.000300\n",
      "2021-07-10 10:22:42,416 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:22:42,416 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:22:42,417 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:22:42,731 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:22:42,731 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:22:43,413 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:22:43,414 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:22:43,414 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:22:43,414 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo abo bamureeba , bari , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi , ouboola , mbu , “ Enzenya , okhuboolela , mbu , ‘ Enzenya , nasi , ndenya , okhunzenya . ”\n",
      "2021-07-10 10:22:43,415 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:22:43,415 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:22:43,415 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:22:43,415 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyechesinjia , omundu undi , undi , owali niyechesinjia , ne , naboolela Petero ari , “ Omwechesia , ”\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - \tHypothesis: Ne olwa bali nibanyoola , Yerusalemu , baliho , ahambi , tsinyanga etsio .\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:22:43,417 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:22:43,417 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:22:43,417 - INFO - joeynmt.training - \tHypothesis: Mana Paulo , naboolela Paulo ari , “ Itaru , ne , olwa , ndasasaba , tsinyanga tsiayo , tsiomunyu , tsiomunyu , tsiomunyu , tsiomunyu , tsiomunyu , tsiomunyu .\n",
      "2021-07-10 10:22:43,417 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     4600: bleu:   3.04, loss: 137582.2344, ppl:  41.2830, duration: 34.6364s\n",
      "2021-07-10 10:22:49,440 - INFO - joeynmt.training - Epoch  16, Step:     4700, Batch Loss:     3.641934, Tokens per Sec:    11802, Lr: 0.000300\n",
      "2021-07-10 10:22:55,823 - INFO - joeynmt.training - Epoch  16, Step:     4800, Batch Loss:     3.792781, Tokens per Sec:    11187, Lr: 0.000300\n",
      "2021-07-10 10:23:35,897 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:23:35,898 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:23:35,898 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:23:36,218 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:23:36,219 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:23:36,945 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:23:36,946 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:23:36,946 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:23:36,946 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo , bamusaaya mbu , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi , ouboola , ne , namuboolela ari , “ Ndakhubooleele , , okhunzenya , okhunzenya , okhunzenya . ”\n",
      "2021-07-10 10:23:36,946 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:23:36,947 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:23:36,947 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:23:36,947 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjile , ne , niyenjile , Petero , ne namuboolela ari , “ Omwechesia , obe , ”\n",
      "2021-07-10 10:23:36,947 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:23:36,948 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:23:36,948 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:23:36,948 - INFO - joeynmt.training - \tHypothesis: Ne olwa , khunyanga eya Isabato , yalimwo , yalimwo , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:23:36,948 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:23:36,949 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:23:36,949 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:23:36,949 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali niyenjile , ne , nabula , ne nabibili , ne nabibili , ne , nabibili , ne , nabibili , ne nabibili , ne nabibili .\n",
      "2021-07-10 10:23:36,949 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     4800: bleu:   3.27, loss: 136052.0156, ppl:  39.6096, duration: 41.1264s\n",
      "2021-07-10 10:23:41,834 - INFO - joeynmt.training - Epoch  16: total training loss 1082.23\n",
      "2021-07-10 10:23:41,835 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-10 10:23:43,662 - INFO - joeynmt.training - Epoch  17, Step:     4900, Batch Loss:     3.621493, Tokens per Sec:     9392, Lr: 0.000300\n",
      "2021-07-10 10:23:49,727 - INFO - joeynmt.training - Epoch  17, Step:     5000, Batch Loss:     3.626518, Tokens per Sec:    11719, Lr: 0.000300\n",
      "2021-07-10 10:24:22,867 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:24:22,868 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:24:22,868 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:24:23,171 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:24:23,171 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:24:23,874 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:24:23,875 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:24:23,875 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:24:23,875 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo , nibaboolela Yesu ari , “ Omwechesia , ndenya , okhuboolela mbu , ndekalushe , ne , ndahulila . ”\n",
      "2021-07-10 10:24:23,875 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:24:23,876 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:24:23,876 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:24:23,876 - INFO - joeynmt.training - \tHypothesis: Mana Yesu , namurumile , ne , olwa yali niyenjile , Yesu , namuboolela ari , “ Omwami , ”\n",
      "2021-07-10 10:24:23,876 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:24:23,877 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:24:23,877 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:24:23,877 - INFO - joeynmt.training - \tHypothesis: Ne olwa , khunyanga yaliho , Isabato , yaliho , khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:24:23,877 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:24:23,878 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:24:23,878 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:24:23,878 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yatiila , Paulo , ne nabaranjilila , okhuyila , ne , nabibili , ne , nabaranjilila , okhwisasaba , ne , nabaranjilila , okhwisukunwa , ne nabarwi , nabarwi .\n",
      "2021-07-10 10:24:23,878 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step     5000: bleu:   3.22, loss: 135581.5156, ppl:  39.1088, duration: 34.1510s\n",
      "2021-07-10 10:24:29,744 - INFO - joeynmt.training - Epoch  17, Step:     5100, Batch Loss:     3.146760, Tokens per Sec:    12146, Lr: 0.000300\n",
      "2021-07-10 10:24:34,954 - INFO - joeynmt.training - Epoch  17: total training loss 1065.02\n",
      "2021-07-10 10:24:34,955 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-10 10:24:36,151 - INFO - joeynmt.training - Epoch  18, Step:     5200, Batch Loss:     3.364739, Tokens per Sec:    11958, Lr: 0.000300\n",
      "2021-07-10 10:25:16,690 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:25:16,691 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:25:16,691 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:25:17,000 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:25:17,000 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:25:17,698 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:25:17,699 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:25:17,699 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:25:17,700 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo bandi bandi bandi , bamureeba bari , “ Omwechesia , ndakhuboolela mbu , ndeela , ndeela , ne , ndemulola , ndakhura , ne , ndakhura , ne , ndakhuranjilila okhuboolela . ”\n",
      "2021-07-10 10:25:17,700 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:25:17,700 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjile , yamwitsila , ne , namuboolela ari , “ Omwechesia , omanyile mbu , iwe , iwe , omanyile mbu , iwe , omanyile mbu , omanyile mbu , omanyile mbu , “ Omwechesia . ”\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:25:17,702 - INFO - joeynmt.training - \tHypothesis: Ne olwa , khunyanga eya Isabato , yaliho , khunyanga ya Isabato , Isabato , ne khunyanga ya Isabato , Isabato .\n",
      "2021-07-10 10:25:17,702 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:25:17,702 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:25:17,702 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:25:17,703 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yatiila Paulo , ne , natiila Paulo , ne , natsukuuna , ne , natsukunwa , natsukunwa , ne , natsukunwa , natsukwe , ne , natsukunwa mumulilo .\n",
      "2021-07-10 10:25:17,703 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     5200: bleu:   3.66, loss: 134399.4062, ppl:  37.8784, duration: 41.5509s\n",
      "2021-07-10 10:25:23,605 - INFO - joeynmt.training - Epoch  18, Step:     5300, Batch Loss:     2.991381, Tokens per Sec:    12022, Lr: 0.000300\n",
      "2021-07-10 10:25:29,999 - INFO - joeynmt.training - Epoch  18, Step:     5400, Batch Loss:     3.248828, Tokens per Sec:    11154, Lr: 0.000300\n",
      "2021-07-10 10:26:04,190 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:26:04,191 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:26:04,191 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:26:04,499 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:26:04,500 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:26:05,598 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:26:05,599 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:26:05,599 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:26:05,599 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo bandi bandi bali nibaboolelakhwo Yesu , nababoolela ari , “ Omwechesia , ndenya , okhuboolela mbu , ndamile , okhunzu , ne olwa ndakhuboolela . ”\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyebula , omwana wabwe , owali niyenjile , omwana wabwe , namuboolela ari , “ Omwechesia , ” ,\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:26:05,601 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:26:05,601 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:26:05,601 - INFO - joeynmt.training - \tHypothesis: Ne olwa , Yerusalemu , yaliho , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:26:05,601 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:26:05,602 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:26:05,602 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:26:05,602 - INFO - joeynmt.training - \tHypothesis: Mana Paulo natsia , tsimbilo natsia , tsimbilo , ne , olwa yali niyenjile , niyenjilamwo , ne , niyenjile , ne , niyenjilamwo , ne , niyenjile .\n",
      "2021-07-10 10:26:05,602 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     5400: bleu:   3.71, loss: 133931.3438, ppl:  37.4020, duration: 35.6028s\n",
      "2021-07-10 10:26:10,572 - INFO - joeynmt.training - Epoch  18: total training loss 1049.73\n",
      "2021-07-10 10:26:10,572 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-10 10:26:11,453 - INFO - joeynmt.training - Epoch  19, Step:     5500, Batch Loss:     3.479048, Tokens per Sec:    11280, Lr: 0.000300\n",
      "2021-07-10 10:26:17,856 - INFO - joeynmt.training - Epoch  19, Step:     5600, Batch Loss:     3.286332, Tokens per Sec:    11170, Lr: 0.000300\n",
      "2021-07-10 10:26:56,539 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:26:56,539 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:26:56,539 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:26:56,862 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:26:56,862 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:26:57,557 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:26:57,559 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:26:57,559 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:26:57,560 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo bandi bandi , bamureeba ari , “ Omwechesia , ndeebula , ne , ndaheela , ndakhurumile , ne , ndakhurumile . ”\n",
      "2021-07-10 10:26:57,560 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:26:57,560 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:26:57,560 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:26:57,561 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , omukhasi undi undi , yalimwo , ne , namuboolela ari , “ Omwechesia , ” , ”\n",
      "2021-07-10 10:26:57,561 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:26:57,561 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:26:57,561 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:26:57,562 - INFO - joeynmt.training - \tHypothesis: Ne olwa , Yerusalemu , yaliho abandu abanji , baliho , khunyanga eya Isabato , Isabato , bali , nibanyoola , eshiokhulia .\n",
      "2021-07-10 10:26:57,562 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:26:57,562 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:26:57,562 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:26:57,563 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yalimwo , yakwa , ne , olwa Sila , yakwa , ne , nabarumile , ne , nabarumile , ne , nabarumile , ne , nabarumile .\n",
      "2021-07-10 10:26:57,563 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     5600: bleu:   3.39, loss: 133174.1094, ppl:  36.6439, duration: 39.7067s\n",
      "2021-07-10 10:27:03,468 - INFO - joeynmt.training - Epoch  19, Step:     5700, Batch Loss:     3.517236, Tokens per Sec:    11975, Lr: 0.000300\n",
      "2021-07-10 10:27:09,027 - INFO - joeynmt.training - Epoch  19: total training loss 1035.27\n",
      "2021-07-10 10:27:09,027 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-10 10:27:09,692 - INFO - joeynmt.training - Epoch  20, Step:     5800, Batch Loss:     3.369959, Tokens per Sec:     8731, Lr: 0.000300\n",
      "2021-07-10 10:27:44,432 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:27:44,432 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:27:44,432 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:27:44,751 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:27:44,752 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:27:45,428 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:27:45,429 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:27:45,429 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:27:45,430 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo bandi bandi , bamureeba Yesu ari , “ Isie , ndenya , okhunzinjisia , ne , ndenya , okhuboolela mbu , “ Isie ndenya , okhunzenya , okhunzenya , okhunzenya . ”\n",
      "2021-07-10 10:27:45,430 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:27:45,430 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:27:45,431 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:27:45,431 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , yamanya , Petero , ne , namuboolela ari , “ Omwechesia , omanyile mbu , Yesu , omanyile mbu , Yesu , omanyile mbu , “ Omwechesia , ”\n",
      "2021-07-10 10:27:45,431 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:27:45,431 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:27:45,432 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:27:45,432 - INFO - joeynmt.training - \tHypothesis: Ne olwa tsinyanga etsio , Isabato , yaliho , abandu abanji , baliho , khunyanga eya Isabato .\n",
      "2021-07-10 10:27:45,432 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:27:45,432 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:27:45,433 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:27:45,433 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yatiila , omuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka , kwomuyeka kwomuyeka kwako , ne , nahonokokha .\n",
      "2021-07-10 10:27:45,433 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     5800: bleu:   3.19, loss: 132333.7500, ppl:  35.8206, duration: 35.7408s\n",
      "2021-07-10 10:27:51,302 - INFO - joeynmt.training - Epoch  20, Step:     5900, Batch Loss:     3.457117, Tokens per Sec:    12065, Lr: 0.000300\n",
      "2021-07-10 10:27:57,743 - INFO - joeynmt.training - Epoch  20, Step:     6000, Batch Loss:     3.060367, Tokens per Sec:    10986, Lr: 0.000300\n",
      "2021-07-10 10:28:34,596 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:28:34,597 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:28:34,597 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:28:34,927 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:28:34,930 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:28:35,604 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:28:35,605 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:28:35,605 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:28:35,605 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo bandi bandi , bamureeba bari , “ Omundu yesi yesi yesi oumutiile , ne , namuboolela ari , “ Ekhusaaya , mutiile , ne , olwa ndakhubooleele . ”\n",
      "2021-07-10 10:28:35,606 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:28:35,606 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:28:35,606 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:28:35,606 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , ne , namuboolela ari , “ Omwechesia , ” , Yesu , namuboolela ari , “ Omwechesia , ” ,\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:28:35,608 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:28:35,608 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:28:35,608 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yatiila Paulo , ne , nabuukha , nabirula , ne , nabirula , nibamuyila , mushilibwa , ne , nabuukha , nibamunywekha . Ne olwa yenjela , nibamunyeka , nibamunyanza , mana , nabirulamwo .\n",
      "2021-07-10 10:28:35,608 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     6000: bleu:   3.35, loss: 131670.9531, ppl:  35.1843, duration: 37.8648s\n",
      "2021-07-10 10:28:41,351 - INFO - joeynmt.training - Epoch  20: total training loss 1018.80\n",
      "2021-07-10 10:28:41,352 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-10 10:28:41,538 - INFO - joeynmt.training - Epoch  21, Step:     6100, Batch Loss:     3.430549, Tokens per Sec:    11357, Lr: 0.000300\n",
      "2021-07-10 10:28:47,882 - INFO - joeynmt.training - Epoch  21, Step:     6200, Batch Loss:     3.375226, Tokens per Sec:    11131, Lr: 0.000300\n",
      "2021-07-10 10:29:19,986 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:29:19,986 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:29:19,986 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:29:20,290 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:29:20,291 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:29:20,982 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:29:20,982 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo nende Abafarisayo , naboolela Yesu ari , “ Omundu yesi oumutiile , ne , niyenya , okhuboolela mbu , “ Isie ndemulole , ne , ndenya . ”\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyechesinjia , ne , naboolela Petero ari , “ Omwechesia , ” , namuboolela ari , “ Omwechesia , ” , ”\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:29:20,985 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:29:20,985 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:29:20,985 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yali niyenjisia , ne , nabahenga , ne , nabahenga , ne , nabahenga , ne , nabahenga , ne , nabahenga , ne , nabahenga , nibachinjile .\n",
      "2021-07-10 10:29:20,985 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step     6200: bleu:   3.63, loss: 131429.0312, ppl:  34.9548, duration: 33.1035s\n",
      "2021-07-10 10:29:26,882 - INFO - joeynmt.training - Epoch  21, Step:     6300, Batch Loss:     3.394372, Tokens per Sec:    11793, Lr: 0.000300\n",
      "2021-07-10 10:29:33,293 - INFO - joeynmt.training - Epoch  21, Step:     6400, Batch Loss:     3.191988, Tokens per Sec:    11121, Lr: 0.000300\n",
      "2021-07-10 10:29:58,974 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:29:58,975 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:29:58,975 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:29:59,291 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:29:59,292 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:30:00,400 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:30:00,401 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:30:00,401 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:30:00,401 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo bandi bandi , Abafarisayo nibetsa khu Yesu , ne nababoolela ari , “ Ndenya okhukholeele , ne , nditsulilanga mbu , enzenya . ”\n",
      "2021-07-10 10:30:00,401 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:30:00,402 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:30:00,402 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:30:00,402 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyechesinjia , amakhuwa ako , yamwitsila , namuboolela ari , “ Omwechesia , ” , Yesu namuboolela ari , “ Omwami , ”\n",
      "2021-07-10 10:30:00,402 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:30:00,403 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:30:00,403 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:30:00,403 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato , yaho .\n",
      "2021-07-10 10:30:00,403 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:30:00,404 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:30:00,404 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:30:00,404 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yamanya mbu , batsie , ne nabaroma , nabarunda , ne nabarenga , nabarunda , ne nabarunda , nabarakwe .\n",
      "2021-07-10 10:30:00,404 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step     6400: bleu:   3.73, loss: 130217.8984, ppl:  33.8286, duration: 27.1102s\n",
      "2021-07-10 10:30:00,705 - INFO - joeynmt.training - Epoch  21: total training loss 1009.63\n",
      "2021-07-10 10:30:00,705 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-10 10:30:06,327 - INFO - joeynmt.training - Epoch  22, Step:     6500, Batch Loss:     3.235496, Tokens per Sec:    12043, Lr: 0.000300\n",
      "2021-07-10 10:30:12,727 - INFO - joeynmt.training - Epoch  22, Step:     6600, Batch Loss:     3.029850, Tokens per Sec:    11125, Lr: 0.000300\n",
      "2021-07-10 10:30:46,725 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:30:46,725 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:30:46,725 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:30:47,047 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:30:47,048 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:30:47,739 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:30:47,740 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:30:47,740 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:30:47,740 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo , bamuboolela mbu , “ Omundu yesi yesi yesi yesi yesi , oumuboolela , mbu , “ Isie , enzenya , okhutiilakhwo , ne , ndalola . ”\n",
      "2021-07-10 10:30:47,740 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:30:47,744 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:30:47,744 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:30:47,744 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , yamanya , mbu , Yesu yali niyaheela . Ne namuboolela ari , “ Omwechesia , obe , ninenywe . ”\n",
      "2021-07-10 10:30:47,744 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:30:47,745 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:30:47,745 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:30:47,745 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho omundu undi , weshialo , shialiho , khunyanga eya Isabato .\n",
      "2021-07-10 10:30:47,745 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:30:47,746 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:30:47,746 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:30:47,746 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjelesie , ne , naboha , ne nabiria , mana nabiria , mana nabiria , ne nabiria , nabiria , ne nabiria , nabiria , mana nabiria .\n",
      "2021-07-10 10:30:47,746 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step     6600: bleu:   3.46, loss: 129949.4844, ppl:  33.5839, duration: 35.0188s\n",
      "2021-07-10 10:30:53,609 - INFO - joeynmt.training - Epoch  22, Step:     6700, Batch Loss:     3.186085, Tokens per Sec:    11938, Lr: 0.000300\n",
      "2021-07-10 10:30:54,315 - INFO - joeynmt.training - Epoch  22: total training loss 990.94\n",
      "2021-07-10 10:30:54,316 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-10 10:30:59,999 - INFO - joeynmt.training - Epoch  23, Step:     6800, Batch Loss:     3.140948, Tokens per Sec:    10820, Lr: 0.000300\n",
      "2021-07-10 10:31:29,344 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:31:29,345 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:31:29,345 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:31:30,304 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:31:30,304 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:31:30,304 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:31:30,305 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , Abafarisayo , bamureeba bari , “ Omundu yesi yesi ousuubila , ne , niyenjilamwo , nasi , ndalola , ne , olwa ndola , enzenya . ”\n",
      "2021-07-10 10:31:30,305 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:31:30,305 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:31:30,305 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:31:30,306 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , yasinjila , ne , namuboolela ari , “ Omwechesia , ” , Yesu namuboolela ari , “ Omwami , ”\n",
      "2021-07-10 10:31:30,306 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:31:30,306 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yaliho , khunyanga eya Isabato , ne khunyanga eya Isabato , Isabato ,\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:31:30,308 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yenjilamwo Paulo , ne , naboha , ne nabaroma abo , nibatseshelela , ne , nibatsushilushilamwo , ne nabarakhwo , nibachitiilakhwo , mana nabenjilamwo , nibenjilamwo .\n",
      "2021-07-10 10:31:30,308 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step     6800: bleu:   3.61, loss: 129991.0156, ppl:  33.6217, duration: 30.3080s\n",
      "2021-07-10 10:31:36,212 - INFO - joeynmt.training - Epoch  23, Step:     6900, Batch Loss:     3.529071, Tokens per Sec:    12073, Lr: 0.000300\n",
      "2021-07-10 10:31:42,568 - INFO - joeynmt.training - Epoch  23, Step:     7000, Batch Loss:     3.133763, Tokens per Sec:    11056, Lr: 0.000300\n",
      "2021-07-10 10:32:11,116 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:32:11,117 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:32:11,117 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:32:11,428 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:32:11,429 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:32:12,651 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:32:12,652 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:32:12,652 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:32:12,652 - INFO - joeynmt.training - \tHypothesis: Ne Abafarisayo bandi bandi bandi , bamureeba mbu , “ Omundu yesi yesi oumutiilakhwo , ne , niyenjela , ne , namuboolela ari , “ Isie nditsa , ndachirakhwo , ndachinjile . ”\n",
      "2021-07-10 10:32:12,652 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:32:12,653 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:32:12,653 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:32:12,653 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali niyenjilamwo , yachaaka okhuboola , amakhuwa ako , yamwitsila naboola ari , “ Omwechesia , ”\n",
      "2021-07-10 10:32:12,653 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:32:12,654 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:32:12,654 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:32:12,654 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , ne khunyanga eya Isabato , abandu abanji , baliho .\n",
      "2021-07-10 10:32:12,654 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:32:12,655 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:32:12,655 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:32:12,655 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yatiila Paulo , ne natiila , emiesi chiali , chiali nichitaru , mana , nibenjila muliaro , ne nanyoola , ne nanyoola , eshiselelo , shituutu , ne , nabarulukha , ne , nabaremwa .\n",
      "2021-07-10 10:32:12,655 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step     7000: bleu:   3.86, loss: 128688.9922, ppl:  32.4585, duration: 30.0872s\n",
      "2021-07-10 10:32:13,782 - INFO - joeynmt.training - Epoch  23: total training loss 975.37\n",
      "2021-07-10 10:32:13,783 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-10 10:32:18,895 - INFO - joeynmt.training - Epoch  24, Step:     7100, Batch Loss:     3.327407, Tokens per Sec:    11070, Lr: 0.000300\n",
      "2021-07-10 10:32:25,065 - INFO - joeynmt.training - Epoch  24, Step:     7200, Batch Loss:     3.092860, Tokens per Sec:    11688, Lr: 0.000300\n",
      "2021-07-10 10:32:50,101 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:32:50,101 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:32:50,102 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:32:50,413 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:32:50,414 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:32:51,126 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:32:51,128 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:32:51,128 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:32:51,128 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi yesi oumutiile , ne , niyenjekhwo , ne , ndalola . ”\n",
      "2021-07-10 10:32:51,128 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:32:51,129 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:32:51,129 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:32:51,129 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , omundu undi , yahulila amakhuwa ako , yamwitsila , namuboolela ari , “ Omwechesia , witse , ”\n",
      "2021-07-10 10:32:51,129 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho omundu , womubatiisi , weshialo , shialiho , khunyanga eya Isabato .\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:32:51,131 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:32:51,131 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yakwa , ne , nanyoola oluchendo lwomukhuyu okwo , mana , nabarenga , mana nabarunda , nikukunwa mumulilo okulasilia , eshing'ari , nikasilie , ne , nahonokokha .\n",
      "2021-07-10 10:32:51,131 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step     7200: bleu:   4.14, loss: 128601.6172, ppl:  32.3819, duration: 26.0658s\n",
      "2021-07-10 10:32:57,066 - INFO - joeynmt.training - Epoch  24, Step:     7300, Batch Loss:     3.137117, Tokens per Sec:    11932, Lr: 0.000300\n",
      "2021-07-10 10:32:58,672 - INFO - joeynmt.training - Epoch  24: total training loss 955.35\n",
      "2021-07-10 10:32:58,673 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-07-10 10:33:03,525 - INFO - joeynmt.training - Epoch  25, Step:     7400, Batch Loss:     2.755885, Tokens per Sec:    11257, Lr: 0.000300\n",
      "2021-07-10 10:33:36,066 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:33:36,066 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:33:36,067 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:33:36,387 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:33:36,387 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:33:37,064 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:33:37,065 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:33:37,065 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:33:37,065 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Isie nisie , omutiiti , ne , ndarecheresia , ne , ndaranjilila okhunzira . ”\n",
      "2021-07-10 10:33:37,066 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:33:37,066 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:33:37,066 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:33:37,067 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , omundu oyo , nasinjila , ne , namuboolela ari , “ Omwechesia , ” , yamuboolela ari , “ Omwechesia , oboolile . ”\n",
      "2021-07-10 10:33:37,067 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:33:37,067 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:33:37,067 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:33:37,068 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , yalimwo , khunyanga eya Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:33:37,068 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:33:37,068 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:33:37,068 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:33:37,069 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , mana nabarumbula , tsingubo tsiabwe , mana nabarumbetile , mana , nabarumbete , mana nabaremwa , mana , nabaremwa .\n",
      "2021-07-10 10:33:37,069 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step     7400: bleu:   4.07, loss: 127956.1016, ppl:  31.8215, duration: 33.5428s\n",
      "2021-07-10 10:33:43,000 - INFO - joeynmt.training - Epoch  25, Step:     7500, Batch Loss:     3.027595, Tokens per Sec:    11756, Lr: 0.000300\n",
      "2021-07-10 10:33:49,440 - INFO - joeynmt.training - Epoch  25, Step:     7600, Batch Loss:     3.068221, Tokens per Sec:    11026, Lr: 0.000300\n",
      "2021-07-10 10:34:13,071 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:34:13,071 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:34:13,072 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:34:13,373 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:34:13,373 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:34:14,064 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:34:14,065 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:34:14,065 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:34:14,066 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi yesi yesi , niyamile , ne , ndalola . ”\n",
      "2021-07-10 10:34:14,066 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:34:14,066 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:34:14,067 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:34:14,067 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali nashiboolanga amakhuwa ako , yasinjila , ne , namuboolela ari , “ Omwechesia , oleele ! ”\n",
      "2021-07-10 10:34:14,067 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho omundu , weshialo , shialiho , khunyanga eya Isabato .\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:34:14,069 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:34:14,069 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yakwa , ne , niyenjisia , ne , niyenjilamwo , niyenjilamwo , ne niyenjilamwo , ne , niyenjilamwo , ne niyenjilamwo , ne niyenjilamwo , niyenjilamwo .\n",
      "2021-07-10 10:34:14,069 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step     7600: bleu:   3.88, loss: 127914.7109, ppl:  31.7859, duration: 24.6289s\n",
      "2021-07-10 10:34:15,889 - INFO - joeynmt.training - Epoch  25: total training loss 944.30\n",
      "2021-07-10 10:34:15,889 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-07-10 10:34:19,993 - INFO - joeynmt.training - Epoch  26, Step:     7700, Batch Loss:     2.865616, Tokens per Sec:    11996, Lr: 0.000300\n",
      "2021-07-10 10:34:26,342 - INFO - joeynmt.training - Epoch  26, Step:     7800, Batch Loss:     2.879114, Tokens per Sec:    11132, Lr: 0.000300\n",
      "2021-07-10 10:34:56,011 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:34:56,011 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:34:56,011 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:34:56,333 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:34:56,333 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:34:57,017 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:34:57,018 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:34:57,018 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:34:57,018 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Oyo , nditsa , ne , nditsa , nditsa , ne , olwa nditsa , nditsa , nditsa , ndachinjia . ”\n",
      "2021-07-10 10:34:57,018 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:34:57,019 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:34:57,019 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:34:57,019 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjisia , yalola , Petero , yabukula Petero , Petero , namuboolela ari , “ Omwechesia , obe , ”\n",
      "2021-07-10 10:34:57,019 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yachaaka okhuchinga , lweinyanza , ne khunyanga eya Isabato , Isabato , yalimwo .\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:34:57,021 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yakwa , ne , nabarasi , nabarunda , mana nabarulamwo , mana nabarulamwo , mana nabararulamwo , bwisolo , mana nabarulukha , nibachitiilwa .\n",
      "2021-07-10 10:34:57,021 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step     7800: bleu:   3.76, loss: 127741.4375, ppl:  31.6374, duration: 30.6781s\n",
      "2021-07-10 10:35:02,994 - INFO - joeynmt.training - Epoch  26, Step:     7900, Batch Loss:     3.120385, Tokens per Sec:    11807, Lr: 0.000300\n",
      "2021-07-10 10:35:05,640 - INFO - joeynmt.training - Epoch  26: total training loss 931.06\n",
      "2021-07-10 10:35:05,640 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-07-10 10:35:09,475 - INFO - joeynmt.training - Epoch  27, Step:     8000, Batch Loss:     3.154422, Tokens per Sec:    12028, Lr: 0.000300\n",
      "2021-07-10 10:35:34,526 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:35:34,527 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:35:34,527 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:35:34,843 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:35:34,844 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:35:35,974 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:35:35,975 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:35:35,976 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:35:35,976 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yamanya Yesu mbu , “ Omundu yesi yesi yesi , oumuboolela ari , “ Enzu , nditsulilanga , ne ndalola , ebiamo bianje bianje . ”\n",
      "2021-07-10 10:35:35,976 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:35:35,976 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , yalola , mbu , Yesu , yamwitsila namubooleele ari , “ Omwechesia , ”\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yaliho omundu , wambeli , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjilamwo , Paulo , yenjilamwo , mana , naboha , tsingubo tsiomunyanza , mana , niyitsulamwo olusimbi lwomuyeka , kwomuyeka .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step     8000: bleu:   4.07, loss: 127383.2344, ppl:  31.3324, duration: 26.5029s\n",
      "2021-07-10 10:35:41,816 - INFO - joeynmt.training - Epoch  27, Step:     8100, Batch Loss:     3.087347, Tokens per Sec:    12211, Lr: 0.000300\n",
      "2021-07-10 10:35:48,182 - INFO - joeynmt.training - Epoch  27, Step:     8200, Batch Loss:     3.020096, Tokens per Sec:    11173, Lr: 0.000300\n",
      "2021-07-10 10:36:19,824 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:36:19,824 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:36:19,824 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:36:20,159 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:36:20,159 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:36:20,837 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:36:20,838 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:36:20,839 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:36:20,839 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi oumutiile , ne , niyenjilamwo , ne , niyenjilamwo , ne , siesi endi omukhuyu kwanje . ”\n",
      "2021-07-10 10:36:20,839 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:36:20,839 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:36:20,840 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:36:20,840 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali nashiboolanga amakhuwa ako , yasinjila , ne , namuboolela ari , “ Omwechesia , olole , ” , Yesu namuboolela ari , “ Niwe , omuhonia watoto . ”\n",
      "2021-07-10 10:36:20,840 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:36:20,840 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:36:20,841 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:36:20,841 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yaliho , khunyanga eya Isabato , Isabato , abandu babili babili babili babili babili babili babili , bataru .\n",
      "2021-07-10 10:36:20,841 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:36:20,842 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:36:20,842 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:36:20,842 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yenjilamwo , ne , naboha , tsingubo tsiomunyiri . Ne olwa Paulo yali niyenjilamwo , niyenjilamwo , ne nafwa , ne nafwa .\n",
      "2021-07-10 10:36:20,842 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step     8200: bleu:   4.22, loss: 126984.2031, ppl:  30.9961, duration: 32.6596s\n",
      "2021-07-10 10:36:23,331 - INFO - joeynmt.training - Epoch  27: total training loss 918.25\n",
      "2021-07-10 10:36:23,332 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-07-10 10:36:26,729 - INFO - joeynmt.training - Epoch  28, Step:     8300, Batch Loss:     2.923501, Tokens per Sec:    11894, Lr: 0.000300\n",
      "2021-07-10 10:36:33,119 - INFO - joeynmt.training - Epoch  28, Step:     8400, Batch Loss:     3.279203, Tokens per Sec:    10979, Lr: 0.000300\n",
      "2021-07-10 10:37:10,139 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:37:10,140 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:37:10,140 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:37:10,457 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:37:10,457 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:37:11,149 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:37:11,151 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:37:11,151 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:37:11,151 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yareeba Yesu ari , “ Omundu yesi ouboola , mbu , yambwe , ne , yambambo , ne , ndalola , ne , nemalile okhuchiakhwo . ”\n",
      "2021-07-10 10:37:11,151 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:37:11,153 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:37:11,153 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:37:11,153 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali nashisinjiileho , yalola , Mariamu , yalola mbu , Yesu , yamanya mbu , “ Omwami , ” , namuboolela ari , “ Wenya , okhukholeele . ”\n",
      "2021-07-10 10:37:11,153 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yachaaka okhuchinga , khunyanga eya Isabato , abandu babili bataru , bataru , bataru .\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:37:11,155 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:37:11,155 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yenjilamwo , mana naboha , tsingubo tsiomunyanza , mana nabiria , mana nabirila , nibachibotokhana , ne , nibachinjile , okhwinyoola .\n",
      "2021-07-10 10:37:11,155 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step     8400: bleu:   4.21, loss: 126799.0234, ppl:  30.8413, duration: 38.0352s\n",
      "2021-07-10 10:37:17,108 - INFO - joeynmt.training - Epoch  28, Step:     8500, Batch Loss:     3.149360, Tokens per Sec:    12161, Lr: 0.000300\n",
      "2021-07-10 10:37:20,413 - INFO - joeynmt.training - Epoch  28: total training loss 903.76\n",
      "2021-07-10 10:37:20,413 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-07-10 10:37:23,544 - INFO - joeynmt.training - Epoch  29, Step:     8600, Batch Loss:     2.965389, Tokens per Sec:    11581, Lr: 0.000300\n",
      "2021-07-10 10:37:53,595 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:37:53,595 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:37:53,595 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:37:54,554 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:37:54,555 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:37:54,555 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:37:54,555 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu yesi oufukula , ne , niyamboolela ari , “ Enditsa , ne , ndalola , ne , ndalola , ndalola . ”\n",
      "2021-07-10 10:37:54,556 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:37:54,557 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:37:54,557 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:37:54,557 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , Petero yali nashiboolanga amakhuwa ako , yasinjila , namurusia , namuboolela ari , “ Omwami , olola ! ”\n",
      "2021-07-10 10:37:54,557 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , Isabato , yalimwo lisabo lia Pasaka .\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:37:54,559 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:37:54,559 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjilamwo , ne , niyenjilamwo , ne , niyenjilamwo , ne , niyenjilamwo , ne , niyenjilamwo , ne natsayi , boosi , nibachingwa , ne , nibachitiilakhwo .\n",
      "2021-07-10 10:37:54,559 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step     8600: bleu:   4.17, loss: 126825.8359, ppl:  30.8636, duration: 31.0145s\n",
      "2021-07-10 10:38:00,488 - INFO - joeynmt.training - Epoch  29, Step:     8700, Batch Loss:     2.795248, Tokens per Sec:    12069, Lr: 0.000300\n",
      "2021-07-10 10:38:06,859 - INFO - joeynmt.training - Epoch  29, Step:     8800, Batch Loss:     2.980321, Tokens per Sec:    11426, Lr: 0.000300\n",
      "2021-07-10 10:38:33,356 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:38:33,357 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:38:33,357 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:38:33,661 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:38:33,661 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:38:34,376 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:38:34,377 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:38:34,377 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:38:34,377 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi oumutiile , ne , niyamboolela ari , “ Endi namarwi kanje , nditsule , ne nditsula , ndalachinjia . ”\n",
      "2021-07-10 10:38:34,378 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:38:34,378 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:38:34,378 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:38:34,378 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali niyenjisinjia , yalola , mbu , Mariamu , namurume , ne namuboolela ari , “ Omwami , olole ! ”\n",
      "2021-07-10 10:38:34,379 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:38:34,379 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:38:34,379 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:38:34,379 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yalimwo , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , yaho , khunyanga eya Isabato .\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjilamwo , mana nabarenga , ne , nabarunda , mana nabarumbula , mana , nabarumbete , ne nibachinjile , okhwikanzu , mana yenjilamwo , mana yenjilamwo .\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step     8800: bleu:   4.32, loss: 126406.0469, ppl:  30.5153, duration: 27.5205s\n",
      "2021-07-10 10:38:37,487 - INFO - joeynmt.training - Epoch  29: total training loss 891.35\n",
      "2021-07-10 10:38:37,488 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-07-10 10:38:40,336 - INFO - joeynmt.training - Epoch  30, Step:     8900, Batch Loss:     2.915074, Tokens per Sec:    12269, Lr: 0.000300\n",
      "2021-07-10 10:38:46,668 - INFO - joeynmt.training - Epoch  30, Step:     9000, Batch Loss:     2.881948, Tokens per Sec:    11096, Lr: 0.000300\n",
      "2021-07-10 10:39:14,877 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:39:14,878 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:39:14,878 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:39:15,190 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:39:15,190 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:39:16,317 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:39:16,318 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:39:16,318 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:39:16,319 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo , naboolela Yesu ari , “ Omundu yesi oumutiile , ne , niyamboolela ari , “ Isie nisie , nditsa , ne , nditsa , ndalola , ne ndalola , enzia , ne siesi enzia . ”\n",
      "2021-07-10 10:39:16,319 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:39:16,319 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:39:16,319 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:39:16,320 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , omwitookho elo , yamwitsila , yasinjila , ne namuboolela ari , “ Omwami , olole , olole , olole , olole , olole , olole . ”\n",
      "2021-07-10 10:39:16,320 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:39:16,320 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaho khunyanga eya Isabato , yaho , khunyanga eya Isabato , yaho , khunyanga eya Isabato ,\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:39:16,322 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yenjisia , ne , niyenjisia , mana , naboha , ne naniina , mana , naboha , ne naniala , olusimbi lwomulwomuyeka okwo .\n",
      "2021-07-10 10:39:16,322 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step     9000: bleu:   4.05, loss: 126229.0703, ppl:  30.3696, duration: 29.6530s\n",
      "2021-07-10 10:39:22,186 - INFO - joeynmt.training - Epoch  30, Step:     9100, Batch Loss:     2.785678, Tokens per Sec:    11888, Lr: 0.000300\n",
      "2021-07-10 10:39:26,113 - INFO - joeynmt.training - Epoch  30: total training loss 883.99\n",
      "2021-07-10 10:39:26,113 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-07-10 10:39:26,114 - INFO - joeynmt.training - Best validation result (greedy) at step     9000:  30.37 ppl.\n",
      "2021-07-10 10:39:26,133 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-10 10:39:26,494 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 10:39:26,678 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 10:39:26,751 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.lh)...\n",
      "2021-07-10 10:39:56,542 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:39:56,542 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:39:56,543 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:39:56,819 - INFO - joeynmt.prediction -  dev bleu[13a]:   4.64 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 10:39:56,823 - INFO - joeynmt.prediction - Translations saved to: models/enlh_transformer/00009000.hyps.dev\n",
      "2021-07-10 10:39:56,824 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.lh)...\n",
      "2021-07-10 10:40:27,097 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:40:27,097 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:40:27,097 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:40:27,391 - INFO - joeynmt.prediction - test bleu[13a]:   4.49 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 10:40:27,396 - INFO - joeynmt.prediction - Translations saved to: models/enlh_transformer/00009000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ93HroS3Saq"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 9000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"joeynmt/models/enlh_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/{name}_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/enlh_transformer\"', f'model_dir: \"models/enlh_transformer_continued\"')\n",
    "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-0GBGh0B_vi",
    "outputId": "1209d812-f4f3-4a76-fb12-47a4abd253b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"enlh_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"en\"\n",
      "    trg: \"lh\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/enlh_transformer/9000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 1096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 3600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 200         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/enlh_transformer_continued\"\n",
      "    overwrite: False\n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_enlh_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxHfalSrCHj6",
    "outputId": "fb32f6bb-03f4-45a6-ebf1-38921cdb0ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-10 10:58:03,748 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-10 10:58:03,782 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-10 10:58:03,854 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-10 10:58:04,095 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-10 10:58:04,114 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-10 10:58:04,127 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-10 10:58:04,127 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 10:58:04,331 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 10:58:04.499348: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-10 10:58:05,951 - INFO - joeynmt.training - Total params: 12097024\n",
      "2021-07-10 10:58:09,252 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/enlh_transformer/9000.ckpt\n",
      "2021-07-10 10:58:09,707 - INFO - joeynmt.helpers - cfg.name                           : enlh_transformer\n",
      "2021-07-10 10:58:09,707 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.trg                       : lh\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/enlh_transformer/9000.ckpt\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1096\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 200\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/enlh_transformer_continued\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 5904,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "\t[TRG] Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - Number of Src words (types): 4050\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - Number of Trg words (types): 4050\n",
      "2021-07-10 10:58:09,718 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4050),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4050))\n",
      "2021-07-10 10:58:09,729 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 1096\n",
      "\ttotal batch size (w. parallel & accumulation): 1096\n",
      "2021-07-10 10:58:09,729 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-10 10:58:15,633 - INFO - joeynmt.training - Epoch   1, Step:     9100, Batch Loss:     2.815874, Tokens per Sec:    11812, Lr: 0.000300\n",
      "2021-07-10 10:58:18,982 - INFO - joeynmt.training - Epoch   1: total training loss 461.35\n",
      "2021-07-10 10:58:18,982 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-10 10:58:21,432 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     3.022842, Tokens per Sec:    11915, Lr: 0.000300\n",
      "2021-07-10 10:58:49,659 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:58:49,660 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:58:49,660 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:58:49,975 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:58:49,976 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:58:50,611 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:58:50,612 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:58:50,612 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:58:50,612 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , bamusila , ne , namuboolela ari , “ Omundu yesi oumutiile , okhutiilakhwo , ne , ndalamuhelesia , eshitabu eshikanye shianje . ”\n",
      "2021-07-10 10:58:50,612 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyenjisinjia , amakhuwa ako , yamwitsila namurusia , ne namuboolela ari , “ Omwami , witsulilanga , mbu , olole . ”\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo , khunyanga eya Isabato , yalimwo lisabo liabandu , abandu abanji .\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:58:50,615 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , mana , nabarechelesia , olukhongo lwomuyeka kwomukhuyu kwkwarulukha , mana , nakusilia , omuyeka kwako nikarulukha , mana , nakusilia .\n",
      "2021-07-10 10:58:50,615 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     9200: bleu:   4.09, loss: 126204.0547, ppl:  30.3490, duration: 29.1821s\n",
      "2021-07-10 10:58:56,466 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     2.703839, Tokens per Sec:    12327, Lr: 0.000300\n",
      "2021-07-10 10:59:02,807 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     2.915945, Tokens per Sec:    11221, Lr: 0.000300\n",
      "2021-07-10 10:59:25,785 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:59:25,786 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:59:25,786 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:59:26,101 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:59:26,101 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:59:26,742 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:59:26,744 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:59:26,744 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:59:26,744 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , nababoolela ari , “ Omundu yesi oumutiile , okhutiilakhwo , ne , ndalola , shinga olwa nditsa , ndachinjile . ”\n",
      "2021-07-10 10:59:26,744 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:59:26,745 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:59:26,745 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:59:26,745 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyenjisinjia amakhuwa ako , yasinjila , natsia , tsimbilo , naboola ari , “ Omwami , ”\n",
      "2021-07-10 10:59:26,745 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga yambeli , khunyanga yambeli yaho , khunyanga eya Isabato , abandu bataru bataru .\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:59:26,747 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:59:26,747 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabohwa , tsingubo tsiomunyiri . Mana Paulo , niyenjilamwo , ne natiila mushiina shiayo , ne natsayi eyo , yenyokha .\n",
      "2021-07-10 10:59:26,747 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     9400: bleu:   4.29, loss: 126050.9922, ppl:  30.2237, duration: 23.9392s\n",
      "2021-07-10 10:59:30,572 - INFO - joeynmt.training - Epoch   2: total training loss 874.14\n",
      "2021-07-10 10:59:30,572 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-10 10:59:32,643 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     2.666778, Tokens per Sec:    12110, Lr: 0.000300\n",
      "2021-07-10 10:59:39,012 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     2.913727, Tokens per Sec:    10806, Lr: 0.000300\n",
      "2021-07-10 11:00:10,864 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:00:10,865 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:00:10,865 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:00:11,846 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:00:11,847 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:00:11,847 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:00:11,847 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo nende Abafarisayo , nibaboolanga mbu , “ Omundu yesi yesi oumutiile , ne , niyamboolela ari , “ Isie nditsa , nditsa , ne , nditsa , enzia . ”\n",
      "2021-07-10 11:00:11,847 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjisinjia amakhuwa ako , yasinjila , natsia , tsimbilo , naboolela Yesu ari , “ Omwechesia , olole , olole . ”\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , abandu ba Kaisaria , baliho khunyanga eya Isabato ,\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabohwa , tsingubo tsiomunyiri , mana nibatsuuuna , mana nibachinjia , okhwinwa . Ne olwa yenjilamwo , yenjilamwo yinyoola yinyoola .\n",
      "2021-07-10 11:00:11,850 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     9600: bleu:   4.32, loss: 126058.9141, ppl:  30.2302, duration: 32.8369s\n",
      "2021-07-10 11:00:17,781 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     2.961181, Tokens per Sec:    12174, Lr: 0.000300\n",
      "2021-07-10 11:00:22,445 - INFO - joeynmt.training - Epoch   3: total training loss 860.57\n",
      "2021-07-10 11:00:22,445 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-10 11:00:24,260 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     2.845104, Tokens per Sec:    11519, Lr: 0.000300\n",
      "2021-07-10 11:00:48,993 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:00:48,994 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:00:48,994 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:00:49,312 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:00:49,312 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:00:50,060 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:00:50,061 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:00:50,061 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:00:50,061 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Omundu yesi oumutiile , okhutiilakhwo , ne , ndalola , ebiamo ebinji . ”\n",
      "2021-07-10 11:00:50,061 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:00:50,062 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:00:50,062 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:00:50,062 - INFO - joeynmt.training - \tHypothesis: Ne olwa Herode yali nashiboolanga amakhuwa ako , yamwitsila , yasinjila , ne namuboolela ari , “ Omwechesia , iwe , Omusamaria , ”\n",
      "2021-07-10 11:00:50,062 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:00:50,063 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:00:50,064 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:00:50,064 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo , khunyanga eya Isabato , abandu babili bataru bataru , baliho , khunyanga eya Isabato .\n",
      "2021-07-10 11:00:50,064 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:00:50,065 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:00:50,065 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:00:50,065 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , ne , nabohwa , ne , nabiria , mana , nabiria , mana , nabiria , nibachinjile , okhwibishilo . Ne olwa yenjilamwo , yeniya .\n",
      "2021-07-10 11:00:50,065 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     9800: bleu:   4.41, loss: 125691.2031, ppl:  29.9311, duration: 25.8044s\n",
      "2021-07-10 11:00:55,954 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     2.790478, Tokens per Sec:    12052, Lr: 0.000300\n",
      "2021-07-10 11:01:02,287 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     2.732747, Tokens per Sec:    11368, Lr: 0.000300\n",
      "2021-07-10 11:01:32,220 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:01:32,220 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:01:32,220 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:01:32,528 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:01:32,528 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:01:33,253 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:01:33,255 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:01:33,255 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:01:33,255 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi oumutiile , okhutiilakhwo , ne , ndalola , ne nenzia , ne , nenzia . ”\n",
      "2021-07-10 11:01:33,256 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:01:33,256 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:01:33,256 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:01:33,256 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali nashiboolanga amakhuwa ako , yasinjila , namusinjila namusinjila , namuboolela ari , “ Omwechesia , olole ! ”\n",
      "2021-07-10 11:01:33,257 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:01:33,257 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:01:33,257 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:01:33,257 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo , khunyanga eya Isabato , yalimwo lisabo lia Pasaka .\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabohela , tsingubo tsiabwe , mana , nibaboha , tsingubo tsiomunyiri . Ne , yenyokha okwo , nikwolulimi lwokhwibohwa , mana , nibachitiilwa .\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    10000: bleu:   4.43, loss: 125599.3281, ppl:  29.8568, duration: 30.9707s\n",
      "2021-07-10 11:01:37,759 - INFO - joeynmt.training - Epoch   4: total training loss 848.22\n",
      "2021-07-10 11:01:37,760 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-10 11:01:39,200 - INFO - joeynmt.training - Epoch   5, Step:    10100, Batch Loss:     2.561778, Tokens per Sec:    11834, Lr: 0.000300\n",
      "2021-07-10 11:01:45,626 - INFO - joeynmt.training - Epoch   5, Step:    10200, Batch Loss:     2.706297, Tokens per Sec:    10987, Lr: 0.000300\n",
      "2021-07-10 11:02:14,653 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:02:14,654 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:02:14,654 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:02:14,972 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:02:14,972 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:02:15,683 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:02:15,684 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:02:15,684 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:02:15,684 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi oumutiile , ne , niyamboolela ari , “ Enzenya , enzenya , enzenya , okhunzira . ”\n",
      "2021-07-10 11:02:15,684 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:02:15,685 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:02:15,685 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:02:15,685 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yosefu yalola , amakhuwa ako , yamwitsila namulola , namuboolela ari , “ Mariamu , witse , ”\n",
      "2021-07-10 11:02:15,685 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:02:15,686 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:02:15,686 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:02:15,686 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu abanji bataru , baliho khunyanga eya Isabato ,\n",
      "2021-07-10 11:02:15,686 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:02:15,687 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:02:15,687 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:02:15,687 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yamenya Korinelio , mana , yakwa mumeeli , mana , niyefwalile tsingubo tsiomunyiri , mana , nibakwa mumeeli , mana , niyefwalile tsingubo tsiomunyiri .\n",
      "2021-07-10 11:02:15,687 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    10200: bleu:   4.39, loss: 125502.9375, ppl:  29.7791, duration: 30.0607s\n",
      "2021-07-10 11:02:21,594 - INFO - joeynmt.training - Epoch   5, Step:    10300, Batch Loss:     2.588690, Tokens per Sec:    12132, Lr: 0.000300\n",
      "2021-07-10 11:02:26,701 - INFO - joeynmt.training - Epoch   5: total training loss 834.76\n",
      "2021-07-10 11:02:26,701 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-10 11:02:27,959 - INFO - joeynmt.training - Epoch   6, Step:    10400, Batch Loss:     2.486108, Tokens per Sec:    11262, Lr: 0.000300\n",
      "2021-07-10 11:02:52,623 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:02:52,623 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:02:52,624 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:02:52,937 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:02:52,937 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:02:53,609 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:02:53,610 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:02:53,610 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:02:53,610 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , bamureeba mbu , “ Omundu yesi oumutiile , ne , nenzie , ne , ndalola . ”\n",
      "2021-07-10 11:02:53,610 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:02:53,611 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:02:53,611 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:02:53,611 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , amakhuwa ako , yalimwo Mariamu , yamwitsila namulola , namuboolela ari , “ Omwami , witse , mukholeele ! ”\n",
      "2021-07-10 11:02:53,611 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:02:53,612 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:02:53,612 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:02:53,612 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu abanji , bahonokooshe khunyanga eya Isabato .\n",
      "2021-07-10 11:02:53,612 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:02:53,613 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:02:53,613 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:02:53,613 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabarusia , mana , nabaremwa nikarusibwemwo , mana , nabaremwa nikaremwa nikasukunwa munyanza .\n",
      "2021-07-10 11:02:53,613 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    10400: bleu:   4.71, loss: 125289.6797, ppl:  29.6078, duration: 25.6532s\n",
      "2021-07-10 11:02:59,521 - INFO - joeynmt.training - Epoch   6, Step:    10500, Batch Loss:     2.754767, Tokens per Sec:    12128, Lr: 0.000300\n",
      "2021-07-10 11:03:05,962 - INFO - joeynmt.training - Epoch   6, Step:    10600, Batch Loss:     2.872789, Tokens per Sec:    11186, Lr: 0.000300\n",
      "2021-07-10 11:03:31,535 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:03:31,536 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:03:31,536 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:03:31,842 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:03:31,842 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:03:32,530 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:03:32,530 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:03:32,531 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:03:32,531 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu outiile , okhutiilakhwo , ne , niyamboolela ari , “ Nditsulilanga , shinga olwa nditsa , ndalola , ne ndalola , enzia . ”\n",
      "2021-07-10 11:03:32,531 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali nashiboolanga amakhuwa ako , yalimwo Mariamu , namurumile . Mana Yesu namuboolela ari , “ Omwami , olola , olole , olaba , ninashio . ”\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , yalimwo lisabo liabandu ebikhumila bibili .\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:03:32,535 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , yatsukha , ne naniina , mana nabarusia , mana nabarunda , nibakwa hasi . Mana , nibatsuuushilo eshibia yali niyiboha .\n",
      "2021-07-10 11:03:32,535 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    10600: bleu:   4.58, loss: 125227.2031, ppl:  29.5578, duration: 26.5723s\n",
      "2021-07-10 11:03:37,827 - INFO - joeynmt.training - Epoch   6: total training loss 822.21\n",
      "2021-07-10 11:03:37,827 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-10 11:03:38,839 - INFO - joeynmt.training - Epoch   7, Step:    10700, Batch Loss:     2.621115, Tokens per Sec:    11790, Lr: 0.000300\n",
      "2021-07-10 11:03:45,271 - INFO - joeynmt.training - Epoch   7, Step:    10800, Batch Loss:     2.478525, Tokens per Sec:    11152, Lr: 0.000300\n",
      "2021-07-10 11:04:08,309 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:04:08,310 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:04:08,310 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:04:09,265 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yamanya Yesu mbu , yaboolelakhwo omundu yesi yesi yesi , ne niyamboolela ari , “ Enzenya , okhunzira , ne ndalola , omukhuyu kuno nikulukha . ”\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , amakhuwa ako , yakalukha munzu , nyinamwana , namusaaya ari , “ Omwami , yambile , ”\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:04:09,268 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo lisabo liabandu , abandu bataru , abandu bataru , bali ahambi ahambi okhuula .\n",
      "2021-07-10 11:04:09,268 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:04:09,268 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:04:09,268 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:04:09,269 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , mana nabohwa , ne , nabohwa hasi , mana , nibahelesia eshinjia , mana nibatsuuushilamwo , eshibia , mana yenjilamwo .\n",
      "2021-07-10 11:04:09,269 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    10800: bleu:   4.65, loss: 125648.3828, ppl:  29.8964, duration: 23.9970s\n",
      "2021-07-10 11:04:15,312 - INFO - joeynmt.training - Epoch   7, Step:    10900, Batch Loss:     2.407380, Tokens per Sec:    11592, Lr: 0.000300\n",
      "2021-07-10 11:04:21,069 - INFO - joeynmt.training - Epoch   7: total training loss 818.83\n",
      "2021-07-10 11:04:21,070 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-10 11:04:21,728 - INFO - joeynmt.training - Epoch   8, Step:    11000, Batch Loss:     2.361984, Tokens per Sec:    11776, Lr: 0.000300\n",
      "2021-07-10 11:04:48,674 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:04:48,674 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:04:48,674 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:04:48,987 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:04:48,987 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:04:49,732 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:04:49,733 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:04:49,733 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:04:49,733 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , bamureeba mbu , “ Omundu yesi yesi , niyamboolela mbu , anywe , ne , ndalamutiilakhwo , nasi siesi , ndalola . ”\n",
      "2021-07-10 11:04:49,733 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:04:49,734 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:04:49,734 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:04:49,734 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali nashiboolanga amakhuwa ako , yasinjila , ne , namuboolela ari , “ Omwechesia , wenya , okhukhwitsulila . ”\n",
      "2021-07-10 11:04:49,734 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:04:49,735 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:04:49,735 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:04:49,735 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu abanji bataru , bataru , bataru , bataru .\n",
      "2021-07-10 11:04:49,735 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:04:49,736 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:04:49,736 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:04:49,736 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , yafimbwa , mana , nibaboha , mana nibachirusia , mana nibachirusia , mana nibachinjile , mana yebohwe , mana yebohwe .\n",
      "2021-07-10 11:04:49,736 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    11000: bleu:   4.81, loss: 125224.7500, ppl:  29.5559, duration: 28.0075s\n",
      "2021-07-10 11:04:55,791 - INFO - joeynmt.training - Epoch   8, Step:    11100, Batch Loss:     2.363841, Tokens per Sec:    11848, Lr: 0.000300\n",
      "2021-07-10 11:05:02,054 - INFO - joeynmt.training - Epoch   8, Step:    11200, Batch Loss:     2.571990, Tokens per Sec:    11283, Lr: 0.000300\n",
      "2021-07-10 11:05:30,828 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:05:30,828 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:05:30,828 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:05:31,771 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:05:31,771 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibakalukha khu Yesu , ne , naboolela Yesu ari , “ Endeebula , shinga olwa nditsa , nditsule , ne , ndikomba nindikhulanga . ”\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - \tHypothesis: Olwa Yesu yali nashiboolanga amakhuwa ako , yasinjila elwanyi , ne , namuboolela ari , “ Omwechesia , olole , olole , olole , olole ! ”\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu abanji barechekha , lisabo lia Pasaka .\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:05:31,774 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:05:31,775 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:05:31,775 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , yatsuhanda , ne , nabarakhwo , tsingubo tsindiiti , tsiomunyiri . Ne olwa yakwa , mutsimbeka tsiosi tsiali nitsihwele , mana yinwa .\n",
      "2021-07-10 11:05:31,775 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    11200: bleu:   4.94, loss: 125290.9062, ppl:  29.6088, duration: 29.7213s\n",
      "2021-07-10 11:05:37,742 - INFO - joeynmt.training - Epoch   8: total training loss 809.72\n",
      "2021-07-10 11:05:37,742 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-10 11:05:38,051 - INFO - joeynmt.training - Epoch   9, Step:    11300, Batch Loss:     2.410297, Tokens per Sec:    11464, Lr: 0.000300\n",
      "2021-07-10 11:05:44,422 - INFO - joeynmt.training - Epoch   9, Step:    11400, Batch Loss:     2.827872, Tokens per Sec:    11158, Lr: 0.000300\n",
      "2021-07-10 11:06:11,742 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:06:11,743 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:06:11,743 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:06:12,743 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:06:12,743 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:06:12,744 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:06:12,745 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo bandi nibetsa khu Yesu , ne , nababoolela ari , “ Endeebula , shinga olwa ndebukwe , ne nenzia , nenzia , ne nenzia . ”\n",
      "2021-07-10 11:06:12,746 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:06:12,746 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:06:12,746 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:06:12,746 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali niyenjisinjia amakhuwa ako , yamwitsila namubooleele ari , “ Omwechesia , witse , munzu mwa Yesu , yamwitsila , mana , namubooleele . ”\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu bataru bataru bataru bataru , bahoniibwe .\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:06:12,748 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:06:12,748 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:06:12,748 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yamenya Korinelio , yatarandula tsingubo tsiabwe , mana , niyefwalile tsingubo tsiomunyiri , mana , niyefwalile tsingubo tsiomunyiri . Ne olwa yenjilamwo , yenoosia okhwinoosia shinga lwomuyeka kwomumioyo , kwatsayi .\n",
      "2021-07-10 11:06:12,748 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    11400: bleu:   5.04, loss: 125270.7109, ppl:  29.5927, duration: 28.3260s\n",
      "2021-07-10 11:06:18,616 - INFO - joeynmt.training - Epoch   9, Step:    11500, Batch Loss:     2.530859, Tokens per Sec:    12122, Lr: 0.000300\n",
      "2021-07-10 11:06:25,059 - INFO - joeynmt.training - Epoch   9, Step:    11600, Batch Loss:     2.617125, Tokens per Sec:    10899, Lr: 0.000300\n",
      "2021-07-10 11:06:49,280 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:06:49,280 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:06:49,280 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:06:50,255 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:06:50,256 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:06:50,256 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:06:50,256 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo bandi nibetsa khu Yesu , ne , nababoolela ari , “ Omundu yesi yesi , niyamboolela mbu , enzie , ndalola , ne ndalola , shinga olwa nditsa . ”\n",
      "2021-07-10 11:06:50,256 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:06:50,257 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:06:50,257 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:06:50,257 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , amakhuwa ako , Petero yali niyenjisinjia , omukhaana wabwe , namuboolela ari , “ Omwechesia , omwamwiwibwi , iwe witsa . ”\n",
      "2021-07-10 11:06:50,257 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:06:50,258 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:06:50,258 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:06:50,258 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yalimwo khunyanga eya Isabato , khunyanga eya Isabato , yalimwo khunyanga eya Isabato ,\n",
      "2021-07-10 11:06:50,258 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:06:50,259 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:06:50,259 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:06:50,259 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , nibabohela tsingubo tsiomunyiri , mana nabarulula , mana nabarulula , emiandu chitaru , ne nabarulula , eminyoola .\n",
      "2021-07-10 11:06:50,259 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    11600: bleu:   4.68, loss: 125587.0625, ppl:  29.8469, duration: 25.2000s\n",
      "2021-07-10 11:06:50,335 - INFO - joeynmt.training - Epoch   9: total training loss 799.19\n",
      "2021-07-10 11:06:50,336 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-10 11:06:56,173 - INFO - joeynmt.training - Epoch  10, Step:    11700, Batch Loss:     2.498553, Tokens per Sec:    12146, Lr: 0.000300\n",
      "2021-07-10 11:07:02,487 - INFO - joeynmt.training - Epoch  10, Step:    11800, Batch Loss:     2.479457, Tokens per Sec:    10868, Lr: 0.000300\n",
      "2021-07-10 11:07:37,887 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:07:37,887 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:07:37,888 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:07:38,867 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:07:38,868 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:07:38,868 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:07:38,868 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , Yesu yamanya mbu , Yesu yambilwe , ne nababoolela ari , “ Endeebula , ne , ndalamutiilakhwo , ne nenzie . ”\n",
      "2021-07-10 11:07:38,868 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:07:38,869 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:07:38,869 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:07:38,869 - INFO - joeynmt.training - \tHypothesis: Ne olwa Pilato yali nashiboolanga amakhuwa ako , yasinjila , natsia ewa Mariamu yali namusaaya ari , “ Omwechesia , ”\n",
      "2021-07-10 11:07:38,869 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:07:38,870 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:07:38,870 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:07:38,870 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Isabato , khunyanga eya Isabato , abandu bataru bataru bahoniibwe .\n",
      "2021-07-10 11:07:38,870 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:07:38,871 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:07:38,871 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:07:38,871 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohela , tsingubo tsiabwe mana , nibabukula , tsimbeka tsiosi . Mana Paulo nalaka mbu , “ Ikonika , kwomuyeka kwasimeeli yakwa , mutsimbwa tsiomunyiri . ”\n",
      "2021-07-10 11:07:38,871 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    11800: bleu:   4.82, loss: 125947.5078, ppl:  30.1392, duration: 36.3842s\n",
      "2021-07-10 11:07:44,792 - INFO - joeynmt.training - Epoch  10, Step:    11900, Batch Loss:     2.730109, Tokens per Sec:    12287, Lr: 0.000300\n",
      "2021-07-10 11:07:45,216 - INFO - joeynmt.training - Epoch  10: total training loss 791.60\n",
      "2021-07-10 11:07:45,216 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-10 11:07:51,191 - INFO - joeynmt.training - Epoch  11, Step:    12000, Batch Loss:     2.633260, Tokens per Sec:    11228, Lr: 0.000300\n",
      "2021-07-10 11:08:15,378 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:08:15,378 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:08:15,378 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:08:16,380 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:08:16,381 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:08:16,381 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:08:16,381 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi bandi , bamureeba mbu , “ Omundu yesi yesi outiilakhwo , ne , niyenjiile , ne nemirwe chianje . ”\n",
      "2021-07-10 11:08:16,381 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:08:16,382 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:08:16,382 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:08:16,382 - INFO - joeynmt.training - \tHypothesis: Ne olwa Pilato yali nashiboolanga amakhuwa ako , yasinjila elwanyi , ne , namuboolela ari , “ Omwechesia , wabeele , omwitsawo . ”\n",
      "2021-07-10 11:08:16,382 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu bataru bataru , bataru bataru , bachesi .\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:08:16,384 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:08:16,384 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , mana , nabuukha , nabaremwa nikarulamwo , mana , nibakwa mumeeli . Mana nibakwa mumeeli , mutsimbeka tsiosi tsiali nitsihutsa .\n",
      "2021-07-10 11:08:16,384 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    12000: bleu:   4.88, loss: 125565.1484, ppl:  29.8292, duration: 25.1926s\n",
      "2021-07-10 11:08:22,264 - INFO - joeynmt.training - Epoch  11, Step:    12100, Batch Loss:     2.692736, Tokens per Sec:    12185, Lr: 0.000300\n",
      "2021-07-10 11:08:28,582 - INFO - joeynmt.training - Epoch  11, Step:    12200, Batch Loss:     2.529123, Tokens per Sec:    10971, Lr: 0.000300\n",
      "2021-07-10 11:08:58,858 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:08:58,858 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:08:58,859 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:08:59,854 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:08:59,855 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:08:59,855 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:08:59,855 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , Abafarisayo bamureeba mbu , “ Ounzikaane , ” Yesu nababoolela ari , “ Enditsa , ne , ndalamuhelesia , ne , emirwe chienywe . ”\n",
      "2021-07-10 11:08:59,855 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:08:59,856 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:08:59,856 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:08:59,856 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yamwitsila , yasinjila elwanyi , namuboolela ari , “ Omwami , witse , mwitsile . ”\n",
      "2021-07-10 11:08:59,856 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:08:59,857 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:08:59,857 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:08:59,857 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo Isabato , khunyanga eya Isabato , abandu abanji bahonokokha .\n",
      "2021-07-10 11:08:59,857 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:08:59,858 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:08:59,858 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:08:59,858 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana , nibabohela , tsingubo tsiomunyiri . Mana , nibenjila muliaro , nibachinjile , okhwinia , mana nibachirusia , mana nibachinjile .\n",
      "2021-07-10 11:08:59,858 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    12200: bleu:   4.75, loss: 125384.8281, ppl:  29.6841, duration: 31.2754s\n",
      "2021-07-10 11:09:00,679 - INFO - joeynmt.training - Epoch  11: total training loss 781.74\n",
      "2021-07-10 11:09:00,679 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-10 11:09:05,925 - INFO - joeynmt.training - Epoch  12, Step:    12300, Batch Loss:     2.622324, Tokens per Sec:    11967, Lr: 0.000210\n",
      "2021-07-10 11:09:12,240 - INFO - joeynmt.training - Epoch  12, Step:    12400, Batch Loss:     2.682939, Tokens per Sec:    11245, Lr: 0.000210\n",
      "2021-07-10 11:09:40,498 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:09:40,499 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:09:40,499 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:09:40,815 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:09:40,816 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:09:41,510 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:09:41,511 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:09:41,511 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:09:41,511 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo bamureeba bari , “ Omundu yesi , ouboola mbu , anyalilwe okhulola , ne nemirwe chianje . ”\n",
      "2021-07-10 11:09:41,512 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:09:41,513 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:09:41,513 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:09:41,513 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola amakhuwa ako , yakalukha ingo , yalola Mariamu , namusinjila namusinjila naboola ari , “ Omwechesia , witse , muchele ! ”\n",
      "2021-07-10 11:09:41,514 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:09:41,514 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:09:41,514 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:09:41,514 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu bataru bataru bahoniibwe .\n",
      "2021-07-10 11:09:41,515 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:09:41,515 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:09:41,515 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:09:41,516 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , nibabohwa hasi , mana nibaboholela , tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluyoka lwolufwa lwomutarakwe .\n",
      "2021-07-10 11:09:41,516 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    12400: bleu:   5.24, loss: 125131.1797, ppl:  29.4812, duration: 29.2752s\n",
      "2021-07-10 11:09:47,387 - INFO - joeynmt.training - Epoch  12, Step:    12500, Batch Loss:     2.704675, Tokens per Sec:    12143, Lr: 0.000210\n",
      "2021-07-10 11:09:48,412 - INFO - joeynmt.training - Epoch  12: total training loss 755.69\n",
      "2021-07-10 11:09:48,413 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-10 11:09:53,819 - INFO - joeynmt.training - Epoch  13, Step:    12600, Batch Loss:     2.553833, Tokens per Sec:    10869, Lr: 0.000210\n",
      "2021-07-10 11:10:18,989 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:10:18,989 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:10:18,989 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:10:19,301 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:10:19,301 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:10:19,996 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:10:19,997 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:10:19,997 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:10:19,997 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Enditsa , ne , nemekha , ne nemalile , okhutiilakhwo . ”\n",
      "2021-07-10 11:10:19,997 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:10:19,998 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:10:19,998 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:10:19,998 - INFO - joeynmt.training - \tHypothesis: Ne olwa yoola ako , yalimwo omundu undi yetsa , nashilondakhwo , yaboola ari , “ Omwechesia , witse , injelekha wa Yesu , wabali. ”\n",
      "2021-07-10 11:10:19,998 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yelilukha , khunyanga eya Isabato , ne khunyanga eya Isabato , yanyoolamwo .\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:10:20,000 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yamenya , Paulo nende Sila , mana nibabohwa hasi , mana , nibabohololo , mana nibenjilamwo , shinga lwoluyoka lwelu lwomuyeka . Ne olwa yenjilamwo , yafimbwa omurwe .\n",
      "2021-07-10 11:10:20,000 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12600: bleu:   5.48, loss: 125061.5156, ppl:  29.4257, duration: 26.1804s\n",
      "2021-07-10 11:10:25,945 - INFO - joeynmt.training - Epoch  13, Step:    12700, Batch Loss:     2.373684, Tokens per Sec:    11999, Lr: 0.000210\n",
      "2021-07-10 11:10:32,230 - INFO - joeynmt.training - Epoch  13, Step:    12800, Batch Loss:     2.581828, Tokens per Sec:    11260, Lr: 0.000210\n",
      "2021-07-10 11:10:55,667 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:10:55,667 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:10:55,667 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:10:55,973 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:10:55,973 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:10:56,651 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:10:56,652 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:10:56,652 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:10:56,652 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibakalukha khu Yesu , ne , naboolela Yesu ari , “ Enjamile , amombakho , ne , ndalola amombakhwa , ne ndalola . ”\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila elwanyi , ne , naboolela Mariamu ari , “ Chende khutsie ewanje , ” ,\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:10:56,654 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:10:56,654 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:10:56,654 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , khunyanga eya Isabato , abandu bali , nibarechekha lisabo lia Pasaka .\n",
      "2021-07-10 11:10:56,654 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:10:56,655 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:10:56,655 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:10:56,655 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana , nibabohela olusimbi lwomuyeka kwarula. , Mana nibakwa hasi , mutsimbwa tsiali tsiali nitsimutsimbeka tsiosi .\n",
      "2021-07-10 11:10:56,655 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12800: bleu:   5.17, loss: 124865.2812, ppl:  29.2700, duration: 24.4245s\n",
      "2021-07-10 11:10:58,031 - INFO - joeynmt.training - Epoch  13: total training loss 750.81\n",
      "2021-07-10 11:10:58,032 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-10 11:11:02,558 - INFO - joeynmt.training - Epoch  14, Step:    12900, Batch Loss:     2.254772, Tokens per Sec:    12016, Lr: 0.000210\n",
      "2021-07-10 11:11:08,894 - INFO - joeynmt.training - Epoch  14, Step:    13000, Batch Loss:     2.652177, Tokens per Sec:    11268, Lr: 0.000210\n",
      "2021-07-10 11:11:34,540 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:11:34,540 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:11:34,541 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:11:35,512 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:11:35,513 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:11:35,513 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:11:35,513 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi , nibamuboolela bari , “ Omundu yesi outiile , okhumutiilakhwo , ne , namarwi kura , ne , nenzia . ”\n",
      "2021-07-10 11:11:35,513 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:11:35,514 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:11:35,514 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:11:35,514 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo , natsia , nibaboolela Mariamu ari , “ Omwechesia , wabeele , omwamiwe ! ”\n",
      "2021-07-10 11:11:35,514 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:11:35,515 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:11:35,515 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:11:35,515 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , eliuba nilihwele , khunyanga yambeli , yaho , khunyanga eya Isabato ,\n",
      "2021-07-10 11:11:35,515 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:11:35,516 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:11:35,516 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:11:35,516 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana nibahelesia , olukhongo lwabwe lwomuyeka , kwarula mumunwa kwkwkwasimwa . Ne olwa yali , niyenjilamwo , niyenjilamwo .\n",
      "2021-07-10 11:11:35,516 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    13000: bleu:   5.28, loss: 125309.5391, ppl:  29.6237, duration: 26.6220s\n",
      "2021-07-10 11:11:41,619 - INFO - joeynmt.training - Epoch  14, Step:    13100, Batch Loss:     2.663562, Tokens per Sec:    11532, Lr: 0.000210\n",
      "2021-07-10 11:11:43,585 - INFO - joeynmt.training - Epoch  14: total training loss 744.73\n",
      "2021-07-10 11:11:43,585 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-10 11:11:47,779 - INFO - joeynmt.training - Epoch  15, Step:    13200, Batch Loss:     2.669472, Tokens per Sec:    12064, Lr: 0.000210\n",
      "2021-07-10 11:12:11,568 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:12:11,568 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:12:11,568 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:12:12,537 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:12:12,538 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:12:12,538 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:12:12,538 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , bamureeba bari , “ Omundu uno , yambelesie , ne , namarwi kanje , ndalola , ne , ndalola , shinga olwa ndikomba . ”\n",
      "2021-07-10 11:12:12,539 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:12:12,539 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:12:12,539 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:12:12,539 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yachaaka okhuboola , amakhuwa ako , yasinjila hakari wabwe , namuboolela ari , “ Omwami , witse , muno. ” Yesu ahonia omundu oyo ,\n",
      "2021-07-10 11:12:12,540 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:12:12,540 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:12:12,540 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:12:12,541 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaho , khunyanga eya Isabato , abandu bataru bataru bataru bahoniibwe .\n",
      "2021-07-10 11:12:12,541 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:12:12,542 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:12:12,542 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:12:12,542 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , mana , nabohwa hasi , mana , nibeluuuuuyia . Mana nibafimbwa , ne nibafimbwa omurwe .\n",
      "2021-07-10 11:12:12,542 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    13200: bleu:   5.07, loss: 125345.5469, ppl:  29.6526, duration: 24.7632s\n",
      "2021-07-10 11:12:18,433 - INFO - joeynmt.training - Epoch  15, Step:    13300, Batch Loss:     2.589403, Tokens per Sec:    12181, Lr: 0.000210\n",
      "2021-07-10 11:12:24,746 - INFO - joeynmt.training - Epoch  15, Step:    13400, Batch Loss:     2.785532, Tokens per Sec:    11125, Lr: 0.000210\n",
      "2021-07-10 11:12:52,402 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:12:52,403 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:12:52,403 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:12:53,377 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:12:53,377 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:12:53,378 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:12:53,378 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno , ali namarwi , ne , nanyiekhwo , ne , nenzie , ne olwa nditsulilanga , ebindu biosi . ”\n",
      "2021-07-10 11:12:53,378 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yachaaka okhuboola , akali nikekholeeshe , Yesu , namurumila naboola ari , “ Omwechesia , iwe , wabeele , okhukholeele tsimbabasi ! ”\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:12:53,380 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:12:53,380 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , liliho , khunyanga eya Isabato , ne khunyanga eya Isabato , yirulangamwo abandu babili .\n",
      "2021-07-10 11:12:53,380 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:12:53,380 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:12:53,381 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:12:53,381 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana , nibaboholela shinga oluyoka lwomulilo . Mana , nibatsuuushilamwo , oluyoka lwelu lwomuyeka kwarakwe .\n",
      "2021-07-10 11:12:53,381 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    13400: bleu:   5.34, loss: 125366.8906, ppl:  29.6697, duration: 28.6345s\n",
      "2021-07-10 11:12:55,460 - INFO - joeynmt.training - Epoch  15: total training loss 737.49\n",
      "2021-07-10 11:12:55,461 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-10 11:12:59,294 - INFO - joeynmt.training - Epoch  16, Step:    13500, Batch Loss:     2.462835, Tokens per Sec:    12422, Lr: 0.000210\n",
      "2021-07-10 11:13:05,687 - INFO - joeynmt.training - Epoch  16, Step:    13600, Batch Loss:     2.268238, Tokens per Sec:    10940, Lr: 0.000210\n",
      "2021-07-10 11:13:29,504 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:13:29,504 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:13:29,505 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:13:30,533 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:13:30,534 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:13:30,534 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:13:30,534 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Eshilenje shianje , ne , nesambambakhwo , ne olwa nditsulilanga , ebindu biosi . ”\n",
      "2021-07-10 11:13:30,534 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:13:30,535 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:13:30,535 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:13:30,535 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola amakhuwa ako , yalimwo , yalimwo , Mariamu natsia elwanyi , naboola ari , “ Omwechesia , wabeele , omwamiwe ! ”\n",
      "2021-07-10 11:13:30,535 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:13:30,536 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:13:30,536 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:13:30,536 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Isabato , yiramwo inyanga , eyilangwa mbu , khunyanga eya Isabato .\n",
      "2021-07-10 11:13:30,536 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:13:30,537 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:13:30,537 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:13:30,537 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabohwa hasi , mana , nibafimbula tsingubo tsiomunyiri . Mana , nibakwa hasi khulwa oluyoka lwomutarakwe , shichila yali niyirula. ,\n",
      "2021-07-10 11:13:30,537 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    13600: bleu:   5.23, loss: 125629.2578, ppl:  29.8810, duration: 24.8500s\n",
      "2021-07-10 11:13:36,525 - INFO - joeynmt.training - Epoch  16, Step:    13700, Batch Loss:     2.373380, Tokens per Sec:    11795, Lr: 0.000210\n",
      "2021-07-10 11:13:39,222 - INFO - joeynmt.training - Epoch  16: total training loss 727.65\n",
      "2021-07-10 11:13:39,222 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-10 11:13:42,825 - INFO - joeynmt.training - Epoch  17, Step:    13800, Batch Loss:     2.388986, Tokens per Sec:    12119, Lr: 0.000210\n",
      "2021-07-10 11:14:08,390 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:14:08,390 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:14:08,390 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:14:09,829 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:14:09,829 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nibamuboolela bari , “ Omundu uno , ali namarwi , ne , nanyoola olusimbi lwanje lwene , lwene , lwene , lwene olwo , ndaboolela , mbu , enzie . ”\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:14:09,831 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , amakhuwa ako , yalimwo , yalimwo Mariamu natsia , tsimbilo naboola ari , “ Omwechesia , wabeele , Omwami ! ”\n",
      "2021-07-10 11:14:09,831 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:14:09,831 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , eliuba liebunganga , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yebohwa hasi , mana , nibaboha olusimbi lwomuyeka , kwarulula tsingubo tsiomunyiri . Ne olwa yali niyefwalile tsingubo tsiomunyiri , tsiomunyiri , kwokhwilukha .\n",
      "2021-07-10 11:14:09,833 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    13800: bleu:   5.36, loss: 125561.7891, ppl:  29.8265, duration: 27.0076s\n",
      "2021-07-10 11:14:15,770 - INFO - joeynmt.training - Epoch  17, Step:    13900, Batch Loss:     2.522278, Tokens per Sec:    12061, Lr: 0.000210\n",
      "2021-07-10 11:14:22,092 - INFO - joeynmt.training - Epoch  17, Step:    14000, Batch Loss:     2.430365, Tokens per Sec:    11027, Lr: 0.000210\n",
      "2021-07-10 11:14:47,321 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:14:47,322 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:14:47,322 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:14:48,289 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:14:48,290 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:14:48,290 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:14:48,290 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno ali namarwi , ne , namanyilekhwo mbu , ali shinga olwa nditsa , ne nemisi , ndalola , ne nenzie . ”\n",
      "2021-07-10 11:14:48,290 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:14:48,291 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:14:48,291 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:14:48,291 - INFO - joeynmt.training - \tHypothesis: Ne olwa yoola ako , yalimwo , yalimwo Mariamu , yakalukha munzu , ne namuboolela ari , “ Omwami , witse , mulole . ”\n",
      "2021-07-10 11:14:48,291 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , khunyanga eya Isabato , abandu bataru balilungwa , khunyanga eya Isabato ,\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:14:48,293 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:14:48,293 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yenjisia , mana , nabohwa hasi , mana , nibafimbwa hasi , mana , nibafimbwa hasi khulwa oluyoka lwelelikulu .\n",
      "2021-07-10 11:14:48,293 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    14000: bleu:   5.42, loss: 125649.4688, ppl:  29.8973, duration: 26.2003s\n",
      "2021-07-10 11:14:50,910 - INFO - joeynmt.training - Epoch  17: total training loss 717.15\n",
      "2021-07-10 11:14:50,911 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-10 11:14:54,189 - INFO - joeynmt.training - Epoch  18, Step:    14100, Batch Loss:     2.105112, Tokens per Sec:    12014, Lr: 0.000147\n",
      "2021-07-10 11:15:00,536 - INFO - joeynmt.training - Epoch  18, Step:    14200, Batch Loss:     2.347688, Tokens per Sec:    11238, Lr: 0.000147\n",
      "2021-07-10 11:15:23,985 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:15:23,985 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:15:23,985 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:15:24,963 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:15:24,964 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:15:24,964 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:15:24,964 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi , nibamuboolela bari , “ Omundu uno , naba nakhwemekha , ne , namarwi kobukusi bwarulula , ne , nemalile okhutiila . ”\n",
      "2021-07-10 11:15:24,965 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:15:24,966 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:15:24,966 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:15:24,966 - INFO - joeynmt.training - \tHypothesis: Ne olwa , Yosefu yali niyenjisinjia amakhuwa ako , yasinjila , natsia elwanyi , naboola ari , “ Omwechesia , witsile ! ”\n",
      "2021-07-10 11:15:24,967 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , khunyanga yambeli yambeli , yalimwo khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:15:24,969 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:15:24,969 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yenjisia , mana nabenama , mana nibafukula , tsingubo tsiomunyiri . Mana nibafukula , oluyoka lwoluyoka lwelu eyo , mana , niyefwalile olusimbi lwokhwikulu .\n",
      "2021-07-10 11:15:24,969 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    14200: bleu:   5.54, loss: 125612.6094, ppl:  29.8675, duration: 24.4329s\n",
      "2021-07-10 11:15:30,920 - INFO - joeynmt.training - Epoch  18, Step:    14300, Batch Loss:     2.474216, Tokens per Sec:    12188, Lr: 0.000147\n",
      "2021-07-10 11:15:34,166 - INFO - joeynmt.training - Epoch  18: total training loss 700.84\n",
      "2021-07-10 11:15:34,167 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-10 11:15:37,293 - INFO - joeynmt.training - Epoch  19, Step:    14400, Batch Loss:     2.051602, Tokens per Sec:    12240, Lr: 0.000147\n",
      "2021-07-10 11:15:58,807 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:15:58,808 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:15:58,808 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:15:59,765 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:15:59,766 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:15:59,766 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:15:59,766 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Lekha , eebula , ne , nenzie mwikanzu ndalola . ”\n",
      "2021-07-10 11:15:59,767 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:15:59,767 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:15:59,767 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:15:59,768 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo , natsia ewa Mariamu yali namubooleele ari , “ Omwechesia , witsile ! ”\n",
      "2021-07-10 11:15:59,768 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:15:59,768 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:15:59,768 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:15:59,769 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , lili ahambi okhuula , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:15:59,769 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:15:59,769 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:15:59,769 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:15:59,770 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , mana nibaboha tsingubo tsiabwe , mana nibafukula , tsimbeka tsiosi . Mana nibafukula , oluyoka lwoluyoka lwelu eyo , mana , niyefwalile olusimbi lwomuyeka .\n",
      "2021-07-10 11:15:59,770 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    14400: bleu:   5.18, loss: 125494.4062, ppl:  29.7722, duration: 22.4764s\n",
      "2021-07-10 11:16:06,059 - INFO - joeynmt.training - Epoch  19, Step:    14500, Batch Loss:     2.284168, Tokens per Sec:    11299, Lr: 0.000147\n",
      "2021-07-10 11:16:12,425 - INFO - joeynmt.training - Epoch  19, Step:    14600, Batch Loss:     2.306697, Tokens per Sec:    11055, Lr: 0.000147\n",
      "2021-07-10 11:16:36,678 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:16:36,679 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:16:36,679 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:16:37,642 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:16:37,644 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:16:37,644 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:16:37,644 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yamanya aka Yesu yali , namubooleele ari , “ Lekha , yambelesiekhwo , ne , nenzie . ”\n",
      "2021-07-10 11:16:37,644 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:16:37,645 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:16:37,645 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:16:37,645 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila , natsia ewa Mariamu yali namubooleele ari , “ Omwechesia , omwitsawo , kube ninenywe. ” ,\n",
      "2021-07-10 11:16:37,645 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:16:37,646 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:16:37,646 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:16:37,646 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , ne khunyanga eya Isabato , yiraho .\n",
      "2021-07-10 11:16:37,646 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:16:37,647 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:16:37,647 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:16:37,647 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yafukula induli , mana , nibabohwa hasi , mana , boosi nibafimbwa hasi . Ne olwa yali niyemilila , okhwinia bieyali niyibohwa .\n",
      "2021-07-10 11:16:37,647 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    14600: bleu:   5.40, loss: 125533.8672, ppl:  29.8040, duration: 25.2214s\n",
      "2021-07-10 11:16:40,734 - INFO - joeynmt.training - Epoch  19: total training loss 700.29\n",
      "2021-07-10 11:16:40,735 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-10 11:16:43,574 - INFO - joeynmt.training - Epoch  20, Step:    14700, Batch Loss:     2.422806, Tokens per Sec:    11947, Lr: 0.000147\n",
      "2021-07-10 11:16:49,955 - INFO - joeynmt.training - Epoch  20, Step:    14800, Batch Loss:     2.231076, Tokens per Sec:    11076, Lr: 0.000147\n",
      "2021-07-10 11:17:14,552 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:17:14,553 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:17:14,553 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:17:15,524 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:17:15,524 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:17:15,525 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:17:15,525 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nibamuboolela bari , “ Omundu uno ali , natseshelela , ne , namarwi kabili , ne , nemalile okhutiilakhwo , ne nimboola , mbu , enzie . ”\n",
      "2021-07-10 11:17:15,525 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila , natsia ewa Mariamu yali namubooleele ari , “ Omwechesia , wabeele , omwitsawe . ”\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:17:15,527 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:17:15,527 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , khunyanga eya Isabato , yiranga , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:17:15,527 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:17:15,527 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:17:15,528 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:17:15,528 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabalindi , mana , nibabohela tsingubo tsiomunyanza , mana , nibaboha amatsi nikarulula tsingubo tsiomunyiri . Ne olwa yali niyenjile , okhwinoosia okhwo mumaatsi , mana natsushilile .\n",
      "2021-07-10 11:17:15,528 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    14800: bleu:   5.50, loss: 125812.4688, ppl:  30.0294, duration: 25.5727s\n",
      "2021-07-10 11:17:21,619 - INFO - joeynmt.training - Epoch  20, Step:    14900, Batch Loss:     1.945742, Tokens per Sec:    11801, Lr: 0.000147\n",
      "2021-07-10 11:17:25,392 - INFO - joeynmt.training - Epoch  20: total training loss 695.92\n",
      "2021-07-10 11:17:25,393 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-10 11:17:27,965 - INFO - joeynmt.training - Epoch  21, Step:    15000, Batch Loss:     2.404124, Tokens per Sec:    12060, Lr: 0.000147\n",
      "2021-07-10 11:17:51,107 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:17:51,107 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:17:51,107 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:17:52,077 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:17:52,078 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:17:52,078 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:17:52,078 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nibamuboolela bari , “ Omundu uno , ali namarwi , ne , naboolela Yesu ari , “ Enditsa , ndalamutiilakhwo , ne , ndalola neimba tsinzo , ne ndalola . ”\n",
      "2021-07-10 11:17:52,079 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:17:52,079 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:17:52,079 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:17:52,079 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo natsia , tsimbilo , naboolela Mariamu ari , “ Omwechesia , wabeele , omwamwibwi . ”\n",
      "2021-07-10 11:17:52,080 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:17:52,080 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:17:52,080 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:17:52,080 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yechesinjia , khunyanga eya Isabato , ne khunyanga eya Isabato , yiruungwa .\n",
      "2021-07-10 11:17:52,081 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:17:52,081 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:17:52,081 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:17:52,082 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe natsia , khung'asia halala nende Sila , nibaboha tsingubo tsiabwe , mana nibafukula , tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluchendo lwelibanzu eyo , mana , niyiboha .\n",
      "2021-07-10 11:17:52,082 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    15000: bleu:   5.48, loss: 126012.8984, ppl:  30.1926, duration: 24.1169s\n",
      "2021-07-10 11:17:58,005 - INFO - joeynmt.training - Epoch  21, Step:    15100, Batch Loss:     2.326992, Tokens per Sec:    12099, Lr: 0.000147\n",
      "2021-07-10 11:18:04,348 - INFO - joeynmt.training - Epoch  21, Step:    15200, Batch Loss:     2.484939, Tokens per Sec:    11226, Lr: 0.000147\n",
      "2021-07-10 11:18:29,357 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:18:29,358 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:18:29,358 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:18:30,338 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:18:30,339 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:18:30,339 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:18:30,339 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Omundu uno , yambelesie emusaaya mbu , emulambwe , ne olwa nditsulilanga , ebindu biosi . ”\n",
      "2021-07-10 11:18:30,339 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:18:30,340 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:18:30,340 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:18:30,340 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola amakhuwa ako , yatsukha natsia , tsimbilo , nasaaya Yesu ari , “ Omwechesia , wabeele , omwitsawo . ”\n",
      "2021-07-10 11:18:30,340 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , khunyanga eya Isabato , yirulayo , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:18:30,342 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:18:30,342 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana , nibaboha hasi , mana , nibahelesia eshinyasio eshiraaka eshiyia eshiyia eshiraambi , mana , emeeli yebohwa .\n",
      "2021-07-10 11:18:30,342 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    15200: bleu:   5.48, loss: 125897.8906, ppl:  30.0988, duration: 25.9936s\n",
      "2021-07-10 11:18:33,912 - INFO - joeynmt.training - Epoch  21: total training loss 686.00\n",
      "2021-07-10 11:18:33,913 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-10 11:18:36,221 - INFO - joeynmt.training - Epoch  22, Step:    15300, Batch Loss:     2.342602, Tokens per Sec:    12307, Lr: 0.000103\n",
      "2021-07-10 11:18:42,585 - INFO - joeynmt.training - Epoch  22, Step:    15400, Batch Loss:     2.169968, Tokens per Sec:    11143, Lr: 0.000103\n",
      "2021-07-10 11:19:05,347 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:19:05,347 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:19:05,348 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:19:06,376 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:19:06,377 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:19:06,377 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:19:06,377 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo bamureeba bari , “ Omundu yesi , outiilakhwo , ne , namarwi ketsukhana , ne , namarwi kokhuhulila . ”\n",
      "2021-07-10 11:19:06,377 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:19:06,378 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:19:06,378 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:19:06,378 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , witse , witse ninaye , ”\n",
      "2021-07-10 11:19:06,378 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:19:06,379 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:19:06,379 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:19:06,379 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Isabato , khunyanga eya Isabato , yiraho , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:19:06,379 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:19:06,380 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:19:06,380 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:19:06,380 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi weimeeli eyo , mana , naboha hasi , mushiina eshiraambi , mana nibahelesia , eshinyasio eshikhongo muno .\n",
      "2021-07-10 11:19:06,380 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step    15400: bleu:   5.60, loss: 125892.4453, ppl:  30.0944, duration: 23.7948s\n",
      "2021-07-10 11:19:12,344 - INFO - joeynmt.training - Epoch  22, Step:    15500, Batch Loss:     2.245534, Tokens per Sec:    12066, Lr: 0.000103\n",
      "2021-07-10 11:19:16,561 - INFO - joeynmt.training - Epoch  22: total training loss 673.96\n",
      "2021-07-10 11:19:16,561 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-10 11:19:18,815 - INFO - joeynmt.training - Epoch  23, Step:    15600, Batch Loss:     2.213375, Tokens per Sec:    11326, Lr: 0.000103\n",
      "2021-07-10 11:19:44,927 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:19:44,927 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:19:44,928 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:19:45,888 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:19:45,889 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:19:45,889 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:19:45,889 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Lekha , ingubo ikho mbu , emulambelesie , ne , ndalola , shinga olwa nditsulilanga . ”\n",
      "2021-07-10 11:19:45,890 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:19:45,890 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:19:45,890 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:19:45,890 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiluutsi , Mariamu yarula natsia elwanyi , naboolela omukhaana oyo ari , “ Chende khutsie ewanje , ” ,\n",
      "2021-07-10 11:19:45,891 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:19:45,891 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:19:45,892 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:19:45,892 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , ne khunyanga eya Isabato , yiruungwa , ahambi okhuula , khunyanga eya Isabato .\n",
      "2021-07-10 11:19:45,892 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:19:45,892 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:19:45,893 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:19:45,893 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , nabahelesia tsimbeka tsiosi , mana , nibitsula tsingubo tsiomunyiri . Ne olwa indiri eyo yali niyefwalile tsingubo tsiomunyiri , mana , niyibohwa .\n",
      "2021-07-10 11:19:45,893 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    15600: bleu:   5.54, loss: 125850.9297, ppl:  30.0606, duration: 27.0773s\n",
      "2021-07-10 11:19:51,885 - INFO - joeynmt.training - Epoch  23, Step:    15700, Batch Loss:     1.913919, Tokens per Sec:    11820, Lr: 0.000103\n",
      "2021-07-10 11:19:58,061 - INFO - joeynmt.training - Epoch  23, Step:    15800, Batch Loss:     2.222816, Tokens per Sec:    11514, Lr: 0.000103\n",
      "2021-07-10 11:20:22,398 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:20:22,398 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:20:22,399 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:20:23,829 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:20:23,829 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:20:23,830 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:20:23,830 - INFO - joeynmt.training - \tHypothesis: Ne Abafarisayo bandi bandi , bamureeba mbu , “ Omundu yesi , outiilakhwo , ne namarwi , ndalola , ne nenzia , ne , ndalola . ”\n",
      "2021-07-10 11:20:23,830 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:20:23,830 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , witse , bwangu , chama , enaayile . ”\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yechesinjia abandu ebikhumila biraano , ne khunyanga eya Isabato , yiruungwa .\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabenjisia , Paulo nende Sila , mana , nabohwa hasi , mana , nibaboholela tsingubo tsiomunyanza . Mana nibatsuuyia , oluchendo lwabwe lwelibishilo , mana , niyibohwa .\n",
      "2021-07-10 11:20:23,833 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    15800: bleu:   5.61, loss: 125933.2812, ppl:  30.1276, duration: 25.7710s\n",
      "2021-07-10 11:20:27,855 - INFO - joeynmt.training - Epoch  23: total training loss 674.94\n",
      "2021-07-10 11:20:27,855 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-10 11:20:29,659 - INFO - joeynmt.training - Epoch  24, Step:    15900, Batch Loss:     2.024278, Tokens per Sec:    11852, Lr: 0.000103\n",
      "2021-07-10 11:20:35,956 - INFO - joeynmt.training - Epoch  24, Step:    16000, Batch Loss:     2.007446, Tokens per Sec:    11273, Lr: 0.000103\n",
      "2021-07-10 11:21:00,337 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:21:00,338 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:21:00,338 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:21:01,319 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:21:01,319 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:21:01,320 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:21:01,320 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu yesi outiile , amapaaroke , ne nababoolela ari , “ Enditsa , ndalamutiilakhwo , ne ndalola , enzia , ne ndalola . ”\n",
      "2021-07-10 11:21:01,320 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:21:01,321 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:21:01,321 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:21:01,323 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo natsia , nibaboolela Mariamu ari , “ Omwechesia , witse , bwangu , witse muno. ” Mana namuboolela ari , “ Wenya mbu , onyala okhukhulola . ”\n",
      "2021-07-10 11:21:01,324 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:21:01,324 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:21:01,324 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:21:01,325 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yiraho khunyanga eya Isabato , ne khunyanga eya Isabato , yiraho .\n",
      "2021-07-10 11:21:01,325 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:21:01,325 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:21:01,325 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:21:01,326 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , naruma tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluyoka lwolutwa lwelu lwalukuku lweinyanza , mana yeniya .\n",
      "2021-07-10 11:21:01,326 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step    16000: bleu:   5.38, loss: 126211.1094, ppl:  30.3548, duration: 25.3705s\n",
      "2021-07-10 11:21:07,189 - INFO - joeynmt.training - Epoch  24, Step:    16100, Batch Loss:     2.094017, Tokens per Sec:    12188, Lr: 0.000103\n",
      "2021-07-10 11:21:11,651 - INFO - joeynmt.training - Epoch  24: total training loss 674.06\n",
      "2021-07-10 11:21:11,651 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-07-10 11:21:13,552 - INFO - joeynmt.training - Epoch  25, Step:    16200, Batch Loss:     2.146438, Tokens per Sec:     8981, Lr: 0.000103\n",
      "2021-07-10 11:21:38,634 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:21:38,635 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:21:38,635 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:21:39,609 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:21:39,610 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:21:39,610 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:21:39,610 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nibamuboolela bari , “ Omundu uno ali , nanyiekhwo , ne olwa yambelesiekhwo , ne , nenzia . ”\n",
      "2021-07-10 11:21:39,610 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:21:39,611 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:21:39,611 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:21:39,611 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo . ”\n",
      "2021-07-10 11:21:39,611 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:21:39,612 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:21:39,612 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:21:39,613 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Isabato , yiruka , khunyanga eya Isabato , ne khunyanga eya Isabato , yiraho .\n",
      "2021-07-10 11:21:39,613 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:21:39,614 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:21:39,614 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:21:39,614 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yakaasia , Paulo yabelihe , mana , nabarusia tsingubo tsiabwe , mana , nibatuuuyia lwa. , Yalasandibwa shinga omulilo kwalio , mana yebohwa , Paulo nende Sila .\n",
      "2021-07-10 11:21:39,614 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    16200: bleu:   5.58, loss: 126307.6797, ppl:  30.4342, duration: 26.0612s\n",
      "2021-07-10 11:21:45,487 - INFO - joeynmt.training - Epoch  25, Step:    16300, Batch Loss:     2.445061, Tokens per Sec:    12091, Lr: 0.000103\n",
      "2021-07-10 11:21:51,782 - INFO - joeynmt.training - Epoch  25, Step:    16400, Batch Loss:     2.286090, Tokens per Sec:    11282, Lr: 0.000103\n",
      "2021-07-10 11:22:18,252 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:22:18,252 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:22:18,252 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:22:19,220 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:22:19,221 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:22:19,221 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:22:19,221 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu yesi outiilanga , ne , yenjiile , ne nababoolela ari , “ Enditsa , nasi nditsa , ne ndalola , enzia . ”\n",
      "2021-07-10 11:22:19,222 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:22:19,223 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:22:19,223 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:22:19,223 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwemitsabibu . ”\n",
      "2021-07-10 11:22:19,223 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:22:19,224 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:22:19,224 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:22:19,225 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , ne khunyanga eya Isabato , yanyoola lichina lieshikhumila ,\n",
      "2021-07-10 11:22:19,225 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:22:19,225 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:22:19,225 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:22:19,226 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe , yabalindi , mana , nabohwa hasi , mana nibahelesia , eshinyasio eshikhongo eshikhongo eshikhongo eshikhongo eshikhongo muno . Ne olwa yali niyefwalile tsingubo tsiomunyiri , mana yebindi bieyali niyefwa .\n",
      "2021-07-10 11:22:19,226 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    16400: bleu:   5.44, loss: 126065.6406, ppl:  30.2357, duration: 27.4439s\n",
      "2021-07-10 11:22:24,031 - INFO - joeynmt.training - Epoch  25: total training loss 667.05\n",
      "2021-07-10 11:22:24,032 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-07-10 11:22:25,153 - INFO - joeynmt.training - Epoch  26, Step:    16500, Batch Loss:     2.293441, Tokens per Sec:    12072, Lr: 0.000072\n",
      "2021-07-10 11:22:31,515 - INFO - joeynmt.training - Epoch  26, Step:    16600, Batch Loss:     2.095215, Tokens per Sec:    11272, Lr: 0.000072\n",
      "2021-07-10 11:22:54,053 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:22:54,054 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:22:54,054 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:22:55,029 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:22:55,030 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:22:55,030 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:22:55,030 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno , naba nanyiekhwo , ne , namarwi kobukusi bwokhubuula tsingano . ”\n",
      "2021-07-10 11:22:55,032 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:22:55,032 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:22:55,032 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:22:55,032 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yasinjila bwangu , naboolela omukhaana oyo ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwemitsabibu . ”\n",
      "2021-07-10 11:22:55,033 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:22:55,033 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:22:55,033 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:22:55,033 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Kaperinaumu , khunyanga eya Isabato , yanyoola khunyanga eya Isabato ,\n",
      "2021-07-10 11:22:55,034 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:22:55,034 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:22:55,034 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:22:55,034 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , nibaboha hasi , mana , nibaboha hasi , mana nibahelesia eshitabu eshikanye eshio . Mana , shiokhwikasia ameetwe okhwirwa khwa Nyasaye ,\n",
      "2021-07-10 11:22:55,035 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step    16600: bleu:   5.50, loss: 126235.6094, ppl:  30.3750, duration: 23.5190s\n",
      "2021-07-10 11:23:00,914 - INFO - joeynmt.training - Epoch  26, Step:    16700, Batch Loss:     2.449845, Tokens per Sec:    11896, Lr: 0.000072\n",
      "2021-07-10 11:23:06,464 - INFO - joeynmt.training - Epoch  26: total training loss 660.18\n",
      "2021-07-10 11:23:06,464 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-07-10 11:23:07,242 - INFO - joeynmt.training - Epoch  27, Step:    16800, Batch Loss:     2.141901, Tokens per Sec:    12063, Lr: 0.000072\n",
      "2021-07-10 11:23:31,052 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:23:31,052 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:23:31,052 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:23:32,016 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:23:32,017 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:23:32,017 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:23:32,017 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno , naba namanyile , mbu ali neshishieno eshio , ne , nahenga ikulu . Mana Yesu nenzia , ne nenzia . ”\n",
      "2021-07-10 11:23:32,018 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:23:32,018 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:23:32,018 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:23:32,018 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , witse , witse , bwangu ewa Yesu yali , namubooleele . ”\n",
      "2021-07-10 11:23:32,019 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:23:32,019 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:23:32,019 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:23:32,019 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yechesinjia abandu ebikhumila biraano , ne khunyanga eya Isabato , yalimwo .\n",
      "2021-07-10 11:23:32,020 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:23:32,020 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:23:32,020 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:23:32,020 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe , yabalindi , mana , nabohwa hasi , mana nibahelesia , tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluyoka lwelu eyo , yatsushilamwo , mana niyenyokha .\n",
      "2021-07-10 11:23:32,021 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    16800: bleu:   5.62, loss: 126267.8828, ppl:  30.4015, duration: 24.7778s\n",
      "2021-07-10 11:23:37,914 - INFO - joeynmt.training - Epoch  27, Step:    16900, Batch Loss:     2.223786, Tokens per Sec:    12029, Lr: 0.000072\n",
      "2021-07-10 11:23:44,239 - INFO - joeynmt.training - Epoch  27, Step:    17000, Batch Loss:     2.364861, Tokens per Sec:    11144, Lr: 0.000072\n",
      "2021-07-10 11:24:10,450 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:24:10,450 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:24:10,450 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:24:11,450 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:24:11,451 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:24:11,451 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:24:11,451 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno , naba nanyiekhwo , ne , namarwi kobukusi bweikulu , bweisambaraaka . ”\n",
      "2021-07-10 11:24:11,451 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:24:11,452 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:24:11,452 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:24:11,452 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiluutsi , Mariamu , yasinjila elwanyi , naboola ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwa Yohana . ”\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Kaperinaumu , khunyanga eya Isabato , yanyoola khunyanga eya Isabato .\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:24:11,454 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:24:11,454 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:24:11,454 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , nibaboha tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluchendo lwomutarakwe kwasimeeli yenu eyo , mana yebohwa , okhwinoosia okhwikulu .\n",
      "2021-07-10 11:24:11,454 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    17000: bleu:   5.66, loss: 126297.0000, ppl:  30.4254, duration: 27.2150s\n",
      "2021-07-10 11:24:16,891 - INFO - joeynmt.training - Epoch  27: total training loss 655.61\n",
      "2021-07-10 11:24:16,892 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-07-10 11:24:17,375 - INFO - joeynmt.training - Epoch  28, Step:    17100, Batch Loss:     2.236680, Tokens per Sec:    12010, Lr: 0.000072\n",
      "2021-07-10 11:24:23,749 - INFO - joeynmt.training - Epoch  28, Step:    17200, Batch Loss:     2.159129, Tokens per Sec:    11082, Lr: 0.000072\n",
      "2021-07-10 11:24:46,555 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:24:46,556 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:24:46,556 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:24:47,981 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:24:47,982 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:24:47,982 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:24:47,982 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yamanya aka Yesu yali namubooleele , shichila yali namubooleele ari , “ Eshilenje shianje , ne , nenzia mwanje ndalola . ”\n",
      "2021-07-10 11:24:47,982 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:24:47,983 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:24:47,983 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:24:47,983 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo , natsia naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwefwe . ”\n",
      "2021-07-10 11:24:47,983 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:24:47,984 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:24:47,984 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:24:47,984 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Kaperinaumu , khunyanga eya Isabato , yanyoola khunyanga eya Isabato ,\n",
      "2021-07-10 11:24:47,984 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:24:47,985 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:24:47,985 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:24:47,985 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yakaasia , Paulo yabelihe , mana , nabarakhwo hasi wamakukho mbu , batsie , mutsimbwa tsiomunyiri . Ne olwa indiri eyo yali niyibohwa , neindiri eyo , niyibohwa .\n",
      "2021-07-10 11:24:47,985 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step    17200: bleu:   5.36, loss: 126373.8516, ppl:  30.4887, duration: 24.2352s\n",
      "2021-07-10 11:24:53,865 - INFO - joeynmt.training - Epoch  28, Step:    17300, Batch Loss:     2.321517, Tokens per Sec:    12366, Lr: 0.000072\n",
      "2021-07-10 11:24:59,987 - INFO - joeynmt.training - Epoch  28: total training loss 651.91\n",
      "2021-07-10 11:24:59,988 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-07-10 11:25:00,235 - INFO - joeynmt.training - Epoch  29, Step:    17400, Batch Loss:     2.255209, Tokens per Sec:    11232, Lr: 0.000072\n",
      "2021-07-10 11:25:21,968 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:25:21,968 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:25:21,969 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:25:22,938 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:25:22,938 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno ali , nanyiekhwo , ne namusaaya mbu , amwikhoye , ne , ndalola khandi shinga olwa nditsa . ”\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwa Yesu . ”\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - \tHypothesis: Khunyanga yambeli yambeli yalimwo , khunyanga eya Isabato , ne khunyanga eya Isabato , yalimwo .\n",
      "2021-07-10 11:25:22,941 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:25:22,941 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:25:22,941 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:25:22,942 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , naruma tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluyoka lwelu eyo , yenyokha okwo , mana yafimbwa omurwe .\n",
      "2021-07-10 11:25:22,942 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    17400: bleu:   5.67, loss: 126363.7969, ppl:  30.4804, duration: 22.7064s\n",
      "2021-07-10 11:25:28,806 - INFO - joeynmt.training - Epoch  29, Step:    17500, Batch Loss:     2.194132, Tokens per Sec:    12101, Lr: 0.000072\n",
      "2021-07-10 11:25:35,135 - INFO - joeynmt.training - Epoch  29, Step:    17600, Batch Loss:     1.242991, Tokens per Sec:    11312, Lr: 0.000072\n",
      "2021-07-10 11:25:58,155 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:25:58,156 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:25:58,156 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:25:59,145 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:25:59,146 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:25:59,146 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:25:59,146 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno ali , natseshelela , ne , namarwi kabili mbu , alolanga , ne nenzie . ”\n",
      "2021-07-10 11:25:59,147 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:25:59,147 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:25:59,147 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila elwanyi , ne , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawe . ”\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - \tHypothesis: Khunyanga yambeli eya Isabato , yiruka , khunyanga eya Isabato , ne khunyanga eya Isabato , yalimwo .\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe natsia ingongo yelihe eyo , mana , nabenoosia shinga oluyoka lwomutarakwe , mana , nabenoosia okhwirwa khwa Paulo , mana , nabarakhwo eshindu shiosi shiosi shiosi eshiyia .\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    17600: bleu:   5.69, loss: 126435.2266, ppl:  30.5394, duration: 24.0141s\n",
      "2021-07-10 11:26:05,028 - INFO - joeynmt.training - Epoch  29, Step:    17700, Batch Loss:     2.339534, Tokens per Sec:    11994, Lr: 0.000050\n",
      "2021-07-10 11:26:05,087 - INFO - joeynmt.training - Epoch  29: total training loss 650.28\n",
      "2021-07-10 11:26:05,087 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-07-10 11:26:11,354 - INFO - joeynmt.training - Epoch  30, Step:    17800, Batch Loss:     1.833692, Tokens per Sec:    11038, Lr: 0.000050\n",
      "2021-07-10 11:26:34,098 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:26:34,098 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:26:34,099 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:26:35,051 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:26:35,052 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:26:35,052 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela mbu , Yesu ali nanyiekhwo , ne namunyoola , nababoolela ari , “ Enditsa , ndalamutiilakhwo , ne ndalaba , neimba , ndalola . ”\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , ne namubooleele . ”\n",
      "2021-07-10 11:26:35,054 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:26:35,054 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:26:35,054 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:26:35,055 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Kaperinaumu , khunyanga eya Isabato , yarechekha , lisabo lia Pasaka .\n",
      "2021-07-10 11:26:35,055 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:26:35,056 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:26:35,056 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:26:35,056 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , nibaboha hasi , mana , nibaboha amatsi amanji . Mana , nibatsushilamwo eshitabu eshikanye eshio , nishikalukhasibungwa .\n",
      "2021-07-10 11:26:35,056 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    17800: bleu:   5.86, loss: 126503.1641, ppl:  30.5955, duration: 23.7014s\n",
      "2021-07-10 11:26:40,935 - INFO - joeynmt.training - Epoch  30, Step:    17900, Batch Loss:     2.247837, Tokens per Sec:    11967, Lr: 0.000050\n",
      "2021-07-10 11:26:47,298 - INFO - joeynmt.training - Epoch  30, Step:    18000, Batch Loss:     1.954968, Tokens per Sec:    11228, Lr: 0.000050\n",
      "2021-07-10 11:27:11,190 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:27:11,191 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:27:11,191 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:27:12,162 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:27:12,162 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:27:12,163 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:27:12,163 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno ali , nanyiekhwo , ne namusaaya mbu , amwikhoye , ne , ndalola khandi shinga olwa nditsa , enzia . ”\n",
      "2021-07-10 11:27:12,163 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:27:12,163 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwa Yesu . ”\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:27:12,165 - INFO - joeynmt.training - \tHypothesis: Khunyanga yambeli eya Isabato , yarechekha , khunyanga eya Isabato , ne khunyanga eya Isabato , yirubeka .\n",
      "2021-07-10 11:27:12,165 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:27:12,165 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:27:12,165 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:27:12,166 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe , yenjisia mushiina , shiokhwinikha , mana nabarusia , ebiralie nibakwalwalie , mana , nabohwa hasi . Yalahelesia eshinyasio eshiyia , eshiyia .\n",
      "2021-07-10 11:27:12,166 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    18000: bleu:   5.71, loss: 126447.5312, ppl:  30.5495, duration: 24.8670s\n",
      "2021-07-10 11:27:12,651 - INFO - joeynmt.training - Epoch  30: total training loss 647.99\n",
      "2021-07-10 11:27:12,652 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-07-10 11:27:12,652 - INFO - joeynmt.training - Best validation result (greedy) at step    12800:  29.27 ppl.\n",
      "2021-07-10 11:27:12,672 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-10 11:27:13,041 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 11:27:13,232 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 11:27:13,295 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.lh)...\n",
      "2021-07-10 11:27:43,039 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:27:43,040 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:27:43,040 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:27:43,325 - INFO - joeynmt.prediction -  dev bleu[13a]:   5.69 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 11:27:43,335 - INFO - joeynmt.prediction - Translations saved to: models/enlh_transformer_continued/00012800.hyps.dev\n",
      "2021-07-10 11:27:43,335 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.lh)...\n",
      "2021-07-10 11:28:12,532 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:28:12,532 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:28:12,533 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:28:12,827 - INFO - joeynmt.prediction - test bleu[13a]:   5.05 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 11:28:12,832 - INFO - joeynmt.prediction - Translations saved to: models/enlh_transformer_continued/00012800.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_enlh_reload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSBIzdFg3TLK",
    "outputId": "ed55608d-9d13-4a28-aef6-535fbf11fcd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 09:06:34,004 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-18 09:06:34,802 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-18 09:06:35,535 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-18 09:06:36,854 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-18 09:06:38,258 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-18 09:06:38,317 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-18 09:07:31,413 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 09:07:31,792 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-18 09:07:31,865 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.lh)...\n",
      "2021-07-18 09:08:00,983 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 09:08:00,983 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 09:08:00,983 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 09:08:01,311 - INFO - joeynmt.prediction -  dev bleu[13a]:   6.39 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-18 09:08:01,311 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.lh)...\n",
      "2021-07-18 09:08:30,804 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 09:08:30,804 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 09:08:30,805 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 09:08:31,104 - INFO - joeynmt.prediction - test bleu[13a]:   5.31 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt test 'models/enlh_transformer_continued/config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTdKNjTu-Zni"
   },
   "source": [
    "# Backtranslation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzIHIEP8yKLB"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZwi9exL-xgG"
   },
   "outputs": [],
   "source": [
    "# Changing to Luganda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koRVt8o8-hbQ"
   },
   "outputs": [],
   "source": [
    "lug = pd.read_csv(\"Luganda.csv\")\n",
    "mon_en = pd.DataFrame(lug['source_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lybf6p-RdXvk"
   },
   "outputs": [],
   "source": [
    "mon_en.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Acz2W3Z8kI6p",
    "outputId": "d24e5525-d0f2-40e0-c451-d847cee57d82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249490</th>\n",
       "      <td>Among these publishers today are third - gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249491</th>\n",
       "      <td>We give thanks to Jehovah and to those early f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249492</th>\n",
       "      <td>15 : 15 , 16 . ​ — From our archives in Portug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249493</th>\n",
       "      <td>See “ There Is More Harvest Work to Be Done ” ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249494</th>\n",
       "      <td>31 - 32 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249495 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_sentence\n",
       "0                      This publication is not for sale .\n",
       "1                                           COVER SUBJECT\n",
       "2       The Bible was completed about two thousand yea...\n",
       "3       Since then , countless other books have come a...\n",
       "4                                     But not the Bible .\n",
       "...                                                   ...\n",
       "249490  Among these publishers today are third - gener...\n",
       "249491  We give thanks to Jehovah and to those early f...\n",
       "249492  15 : 15 , 16 . ​ — From our archives in Portug...\n",
       "249493  See “ There Is More Harvest Work to Be Done ” ...\n",
       "249494                                          31 - 32 .\n",
       "\n",
       "[249495 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbvAjfuO-wBi",
    "outputId": "0a977b96-bc39-4faa-8ec5-067892fd0465"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasNum(mon_en['source_sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MDnIMMeFOWt"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def hasNum(inputString):\n",
    "  input = str(inputString)\n",
    "  return not re.findall('\\d+', input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyCGePkJi515"
   },
   "outputs": [],
   "source": [
    "hasNum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6A8vuksEQi_"
   },
   "outputs": [],
   "source": [
    "mon_en['has_num'] = mon_en['source_sentence'].apply(hasNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "uZivWkAfj3fr",
    "outputId": "f851d477-feda-4e40-877d-abe93a362b48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>has_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consider the following .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Bible has survived many vicious attacks by...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>For example , during the Middle Ages in certai...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scholars who translated the Bible into the ver...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Despite its many enemies , the Bible became ​ ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_sentence  has_num\n",
       "0                 This publication is not for sale .     True\n",
       "1                                      COVER SUBJECT     True\n",
       "2  The Bible was completed about two thousand yea...     True\n",
       "3  Since then , countless other books have come a...     True\n",
       "4                                But not the Bible .     True\n",
       "5                           Consider the following .     True\n",
       "6  The Bible has survived many vicious attacks by...     True\n",
       "7  For example , during the Middle Ages in certai...     True\n",
       "8  Scholars who translated the Bible into the ver...     True\n",
       "9  Despite its many enemies , the Bible became ​ ...     True"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_en.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-BayN_sGG4he",
    "outputId": "b1fc2253-b12a-4ee3-eda6-5a8dada28f69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>has_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>247121</td>\n",
       "      <td>249495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>231428</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>*</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>462</td>\n",
       "      <td>203930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source_sentence has_num\n",
       "count           247121  249495\n",
       "unique          231428       2\n",
       "top                  *    True\n",
       "freq               462  203930"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_en.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "QNb6is7XoJKg",
    "outputId": "55e7147b-b81b-4f21-efad-e90e1a6560c8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>has_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249490</th>\n",
       "      <td>Among these publishers today are third - gener...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249491</th>\n",
       "      <td>We give thanks to Jehovah and to those early f...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249492</th>\n",
       "      <td>15 : 15 , 16 . ​ — From our archives in Portug...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249493</th>\n",
       "      <td>See “ There Is More Harvest Work to Be Done ” ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249494</th>\n",
       "      <td>31 - 32 .</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249495 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_sentence  has_num\n",
       "0                      This publication is not for sale .     True\n",
       "1                                           COVER SUBJECT     True\n",
       "2       The Bible was completed about two thousand yea...     True\n",
       "3       Since then , countless other books have come a...     True\n",
       "4                                     But not the Bible .     True\n",
       "...                                                   ...      ...\n",
       "249490  Among these publishers today are third - gener...    False\n",
       "249491  We give thanks to Jehovah and to those early f...     True\n",
       "249492  15 : 15 , 16 . ​ — From our archives in Portug...    False\n",
       "249493  See “ There Is More Harvest Work to Be Done ” ...    False\n",
       "249494                                          31 - 32 .    False\n",
       "\n",
       "[249495 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trlla8D4Jy_3"
   },
   "outputs": [],
   "source": [
    "mon_en = mon_en[mon_en['has_num'] == True] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsxdPilInOPT"
   },
   "outputs": [],
   "source": [
    "mon_en.drop(['has_num'], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "EWXmOVskoYJn",
    "outputId": "adc18304-1b1d-4ee0-9e08-53e23f2e5904"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249481</th>\n",
       "      <td>With the printing and distribution of Bible li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249483</th>\n",
       "      <td>However , the seeds of truth had been sown .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249484</th>\n",
       "      <td>Amid the upheaval in Europe during the Spanish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249486</th>\n",
       "      <td>After that , the growth in the number of Kingd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249491</th>\n",
       "      <td>We give thanks to Jehovah and to those early f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203930 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_sentence\n",
       "0                      This publication is not for sale .\n",
       "1                                           COVER SUBJECT\n",
       "2       The Bible was completed about two thousand yea...\n",
       "3       Since then , countless other books have come a...\n",
       "4                                     But not the Bible .\n",
       "...                                                   ...\n",
       "249481  With the printing and distribution of Bible li...\n",
       "249483       However , the seeds of truth had been sown .\n",
       "249484  Amid the upheaval in Europe during the Spanish...\n",
       "249486  After that , the growth in the number of Kingd...\n",
       "249491  We give thanks to Jehovah and to those early f...\n",
       "\n",
       "[203930 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-kXQWTbowCM"
   },
   "outputs": [],
   "source": [
    "# Monolingual English\n",
    "#mon_en.to_csv('mon_en.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6MYI_S9opIwP",
    "outputId": "de6eb4c2-dac7-4b11-d126-9fe8a0867211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_sentence\n",
       "0                 This publication is not for sale .\n",
       "1                                      COVER SUBJECT\n",
       "2  The Bible was completed about two thousand yea...\n",
       "3  Since then , countless other books have come a...\n",
       "4                                But not the Bible ."
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon = pd.read_csv(\"mon_en.csv\")\n",
    "mon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRhC2JcCxV7t",
    "outputId": "8a030490-9394-46cf-9bf2-6341da8c843e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqyMXv9nxbpD"
   },
   "outputs": [],
   "source": [
    "mon.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z23XFxhOzteI",
    "outputId": "b5b22670-fbe2-45ab-9d03-7ae71e776a17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shareddrives/NMT_for_African_Language/Luganda\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWxgyZ6kz4Fz"
   },
   "outputs": [],
   "source": [
    "# Changing to Luhyia directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2G0Ux8mU1-YK",
    "outputId": "5cffac30-b018-428d-e51b-a57dd7194f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnfcSBKewTHd"
   },
   "outputs": [],
   "source": [
    "# Getting monolingual BPEs\n",
    "with open(\"mon.\"+source_language, \"w\") as src_file:\n",
    "  for index, row in mon.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < mon.$src > mon.bpe.$src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJfALQDj2R8_",
    "outputId": "be6fad43-baa8-4878-ad95-b34ed492b593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> mon.bpe.en <==\n",
      "This p@@ ub@@ li@@ cation is not for s@@ ale .\n",
      "C@@ O@@ V@@ E@@ R S@@ U@@ B@@ J@@ E@@ C@@ T\n",
      "The B@@ ible was comple@@ ted about two thousand years a@@ go .\n",
      "S@@ in@@ ce then , coun@@ t@@ less other bo@@ ok@@ s have come and gone .\n",
      "But not the B@@ ible .\n",
      "C@@ on@@ si@@ der the follow@@ ing .\n",
      "The B@@ ible has sur@@ v@@ ived many vi@@ ci@@ ous at@@ ta@@ c@@ ks by p@@ ow@@ er@@ ful people .\n",
      "For exam@@ ple , d@@ ur@@ ing the M@@ id@@ d@@ le A@@ g@@ es in certain “ Chris@@ ti@@ an ” l@@ ands , “ the poss@@ ess@@ ion and read@@ ing of the B@@ ible in the ver@@ nac@@ ul@@ ar [ the l@@ ang@@ u@@ age of the comm@@ on people ] was in@@ cre@@ as@@ ingly as@@ so@@ ci@@ ated with her@@ es@@ y and dis@@ sent , ” says the book A@@ n I@@ n@@ t@@ ro@@ du@@ ction to the M@@ e@@ di@@ ev@@ al B@@ ible .\n",
      "S@@ ch@@ ol@@ ars who trans@@ l@@ ated the B@@ ible into the ver@@ nac@@ ul@@ ar or who prom@@ o@@ ted B@@ ible st@@ ud@@ y ris@@ ked their lives . S@@ ome were killed .\n",
      "D@@ es@@ p@@ ite its many enemi@@ es , the B@@ ible became ​ — and contin@@ ues to be — ​ the most wi@@ d@@ ely dis@@ tri@@ bu@@ ted book of all time .\n",
      "\n",
      "==> mon.en <==\n",
      "This publication is not for sale .\n",
      "COVER SUBJECT\n",
      "The Bible was completed about two thousand years ago .\n",
      "Since then , countless other books have come and gone .\n",
      "But not the Bible .\n",
      "Consider the following .\n",
      "The Bible has survived many vicious attacks by powerful people .\n",
      "For example , during the Middle Ages in certain “ Christian ” lands , “ the possession and reading of the Bible in the vernacular [ the language of the common people ] was increasingly associated with heresy and dissent , ” says the book An Introduction to the Medieval Bible .\n",
      "Scholars who translated the Bible into the vernacular or who promoted Bible study risked their lives . Some were killed .\n",
      "Despite its many enemies , the Bible became ​ — and continues to be — ​ the most widely distributed book of all time .\n"
     ]
    }
   ],
   "source": [
    "! head mon.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riF2uGlF2ksO",
    "outputId": "f699b307-40ff-4fb9-bd74-481d4688d29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> mon.bpe.en <==\n",
      "He sought per@@ mis@@ sion to use his h@@ ome for reg@@ ul@@ ar me@@ et@@ ings .\n",
      "In ad@@ d@@ ition , through tr@@ ac@@ ts and bo@@ ok@@ le@@ ts , the word of truth sp@@ read to the fa@@ r re@@ ac@@ hes of the P@@ or@@ t@@ u@@ gu@@ es@@ e Em@@ p@@ ire ​ — A@@ ng@@ ol@@ a , the A@@ z@@ or@@ es , C@@ ap@@ e V@@ er@@ de , E@@ ast T@@ im@@ or , G@@ o@@ a , M@@ a@@ de@@ ira , and M@@ o@@ z@@ am@@ bi@@ qu@@ e .\n",
      "W@@ hi@@ le living in B@@ ra@@ z@@ il , he had heard a p@@ ub@@ li@@ c tal@@ k given by B@@ ro@@ ther Y@@ oung .\n",
      "He read@@ ily re@@ c@@ og@@ ni@@ zed the r@@ ing of truth and was e@@ ag@@ er to hel@@ p B@@ ro@@ ther F@@ er@@ g@@ us@@ on to ex@@ p@@ and the pre@@ aching work .\n",
      "To do so , M@@ anu@@ el began to serve as a col@@ p@@ or@@ te@@ u@@ r , as pi@@ on@@ e@@ ers were then called .\n",
      "W@@ ith the pr@@ in@@ ting and dis@@ tri@@ bu@@ t@@ ion of B@@ ible l@@ it@@ er@@ at@@ ure now well - or@@ g@@ ani@@ zed , the f@@ led@@ gl@@ ing con@@ gre@@ g@@ ation in L@@ is@@ b@@ on th@@ ri@@ ved !\n",
      "H@@ owever , the see@@ ds of truth had been s@@ own .\n",
      "Am@@ id the up@@ heav@@ al in E@@ u@@ ro@@ pe d@@ ur@@ ing the Sp@@ an@@ ish C@@ iv@@ il W@@ ar and W@@ or@@ ld W@@ ar I@@ I , the faithful g@@ rou@@ p of brothers in P@@ or@@ t@@ u@@ ga@@ l man@@ ag@@ ed to sur@@ v@@ ive spirit@@ u@@ ally .\n",
      "After that , the gr@@ ow@@ th in the number of K@@ ing@@ dom pro@@ c@@ la@@ im@@ ers was un@@ st@@ op@@ p@@ able .\n",
      "We give thanks to J@@ e@@ ho@@ v@@ ah and to those ear@@ ly faithful brothers and sis@@ ters who cour@@ ag@@ e@@ ously took the le@@ ad in spe@@ ar@@ he@@ ad@@ ing the work as ‘ p@@ ub@@ li@@ c servants of Christ Jesus to the nations . ’ ​ — R@@ om .\n",
      "\n",
      "==> mon.en <==\n",
      "He sought permission to use his home for regular meetings .\n",
      "In addition , through tracts and booklets , the word of truth spread to the far reaches of the Portuguese Empire ​ — Angola , the Azores , Cape Verde , East Timor , Goa , Madeira , and Mozambique .\n",
      "While living in Brazil , he had heard a public talk given by Brother Young .\n",
      "He readily recognized the ring of truth and was eager to help Brother Ferguson to expand the preaching work .\n",
      "To do so , Manuel began to serve as a colporteur , as pioneers were then called .\n",
      "With the printing and distribution of Bible literature now well - organized , the fledgling congregation in Lisbon thrived !\n",
      "However , the seeds of truth had been sown .\n",
      "Amid the upheaval in Europe during the Spanish Civil War and World War II , the faithful group of brothers in Portugal managed to survive spiritually .\n",
      "After that , the growth in the number of Kingdom proclaimers was unstoppable .\n",
      "We give thanks to Jehovah and to those early faithful brothers and sisters who courageously took the lead in spearheading the work as ‘ public servants of Christ Jesus to the nations . ’ ​ — Rom .\n"
     ]
    }
   ],
   "source": [
    "!tail mon.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUTzXZMopQ7Q",
    "outputId": "ecad7ae3-c9b1-44b3-de21-183da046016e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 09:11:56,551 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-18 09:12:00,358 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 09:12:00,564 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/enlh_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/mon.bpe.en\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/mon.lh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56vQc6fbSZxB",
    "outputId": "07cf7f72-9e0d-44f7-9441-24e887c70adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This publication is not for sale .\n",
      "COVER SUBJECT\n",
      "The Bible was completed about two thousand years ago .\n",
      "Since then , countless other books have come and gone .\n",
      "But not the Bible .\n",
      "Consider the following .\n",
      "The Bible has survived many vicious attacks by powerful people .\n",
      "For example , during the Middle Ages in certain “ Christian ” lands , “ the possession and reading of the Bible in the vernacular [ the language of the common people ] was increasingly associated with heresy and dissent , ” says the book An Introduction to the Medieval Bible .\n",
      "Scholars who translated the Bible into the vernacular or who promoted Bible study risked their lives . Some were killed .\n",
      "Despite its many enemies , the Bible became ​ — and continues to be — ​ the most widely distributed book of all time .\n",
      "Oburume obwomundu shibuliho ta , habula nobwatoto\n",
      "Olunyuma lwetsinyanga tsitaru , Yorodani nende Siria .\n",
      "Yali ahambi isaa yashienda yemiyika , chibili .\n",
      "Abakhalabani bobubeeyi bakhetsukhana , nibakhupa ikha .\n",
      "Nebutswa , shimulamukhalachila eshiina tawe .\n",
      "Mutsinyanga etsio .\n",
      "Nyasaye yaranjilila okhulaasia abandu abanji , ne abandu abanji , nabo , nibahonibwe .\n",
      "Baana befwe , khurumanga khumatookho , keshilaamo mbu , “ Lekha ” ” , mbu , “ Oli ” ” , mbu , “ Oli owobuloho , ” , owobuloho , obuloho , khandi mbu , “ Olunyala , ” ta , ” , shichila buli shindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashilaka ,\n",
      "Biliho ebifwanani biomwikulu , bilanyala okhuboola ta , shichila , Nyasaye yalonga likulu nende ebilimwoyo .\n",
      "Abo abahulilikha shinga abahulilikha , abasuku ba Nyasaye , baloba okhukhalachilwa eshiina ta , baloba okhuhulila , akali amalayi .\n"
     ]
    }
   ],
   "source": [
    "!head mon.en\n",
    "!head mon.lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h82xs4DU8t9K"
   },
   "outputs": [],
   "source": [
    "# Dev data source\n",
    "file1 = ['train.en', 'mon.en']\n",
    "\n",
    "# Dev data target\n",
    "file2 = ['train.lh', 'mon.lh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuEPOeF3zuj0"
   },
   "outputs": [],
   "source": [
    "# Procedure to create concatenated files\n",
    "def create_file(x,filename):\n",
    "  # Open filename in write mode\n",
    "  with open(filename, 'w') as outfile:\n",
    "      for names in x:\n",
    "          # Open each file in read mode\n",
    "          with open(names) as infile:\n",
    "              # read the data and write it in file3\n",
    "              outfile.write(infile.read())\n",
    "          outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfSsVReb9dDd"
   },
   "outputs": [],
   "source": [
    "create_file(file1,'back.en')\n",
    "create_file(file2,'back.lh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BG_IWkOE9qGp"
   },
   "outputs": [],
   "source": [
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input back.$src back.$tgt3 -s 4000 -o bpe.codes.4000 --write-vocabulary vocab2.$src vocab2.$tgt3\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$src < back.$src > back.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$tgt3 < back.$tgt3 > back.bpe.$tgt3\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$src < dev.$src > back_dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$tgt3 < dev.$tgt3 > back_dev.bpe.$tgt3\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$src < test.$src > back_test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$tgt3 < test.$tgt3 > back_test.bpe.$tgt3\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py back.bpe.$src back.bpe.$tgt3 --output_path vocab2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVyKdTdF07yF"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "f-7K3051-_i2"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language3, source_language)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language3}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language3}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 1600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/back_{name}_reverse_transformer\"\n",
    "    overwrite: False              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\", source_language=source_language, target_language3=target_language3)\n",
    "with open(\"joeynmt/configs/back_transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9PqOYzAACSx",
    "outputId": "46dc3534-c97f-47c2-85c4-5c4e0c133df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 11:13:02,325 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-18 11:13:02,785 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-18 11:13:05,985 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-18 11:13:06,237 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-18 11:13:06,262 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-18 11:13:06,831 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-18 11:13:06,831 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 11:13:07,034 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-18 11:13:07.277581: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-18 11:13:08,753 - INFO - joeynmt.training - Total params: 12138240\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/back_lhen_reverse_transformer\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 207289,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] P@@ il@@ a@@ to nakal@@ ukha itookho , ne nal@@ anga Yesu , namureeba , ari , “ Iwe ni@@ we omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then P@@ il@@ ate ent@@ ered the P@@ ra@@ et@@ or@@ i@@ u@@ m again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-18 11:13:11,984 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-18 11:13:11,984 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-18 11:13:11,984 - INFO - joeynmt.helpers - Number of Src words (types): 4211\n",
      "2021-07-18 11:13:11,984 - INFO - joeynmt.helpers - Number of Trg words (types): 4211\n",
      "2021-07-18 11:13:11,985 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4211),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4211))\n",
      "2021-07-18 11:13:11,994 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-18 11:13:11,995 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-18 11:13:24,863 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.652103, Tokens per Sec:    17280, Lr: 0.000300\n",
      "2021-07-18 11:13:37,544 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.488219, Tokens per Sec:    17748, Lr: 0.000300\n",
      "2021-07-18 11:13:50,212 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.178207, Tokens per Sec:    17995, Lr: 0.000300\n",
      "2021-07-18 11:14:03,139 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.167909, Tokens per Sec:    17535, Lr: 0.000300\n",
      "2021-07-18 11:14:16,125 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.638240, Tokens per Sec:    17573, Lr: 0.000300\n",
      "2021-07-18 11:14:28,867 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     5.034882, Tokens per Sec:    17596, Lr: 0.000300\n",
      "2021-07-18 11:14:41,804 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.660217, Tokens per Sec:    17435, Lr: 0.000300\n",
      "2021-07-18 11:14:54,731 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.592145, Tokens per Sec:    17411, Lr: 0.000300\n",
      "2021-07-18 11:15:08,014 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.464002, Tokens per Sec:    17149, Lr: 0.000300\n",
      "2021-07-18 11:15:21,299 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.979547, Tokens per Sec:    17241, Lr: 0.000300\n",
      "2021-07-18 11:15:34,475 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     4.514979, Tokens per Sec:    17143, Lr: 0.000300\n",
      "2021-07-18 11:15:47,832 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.464970, Tokens per Sec:    17239, Lr: 0.000300\n",
      "2021-07-18 11:16:01,084 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.146487, Tokens per Sec:    17110, Lr: 0.000300\n",
      "2021-07-18 11:16:14,631 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.352227, Tokens per Sec:    16841, Lr: 0.000300\n",
      "2021-07-18 11:16:28,100 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.125914, Tokens per Sec:    17007, Lr: 0.000300\n",
      "2021-07-18 11:16:41,494 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.183986, Tokens per Sec:    17045, Lr: 0.000300\n",
      "2021-07-18 11:16:54,988 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.885419, Tokens per Sec:    17211, Lr: 0.000300\n",
      "2021-07-18 11:17:08,353 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.942995, Tokens per Sec:    16801, Lr: 0.000300\n",
      "2021-07-18 11:17:22,085 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.685602, Tokens per Sec:    16800, Lr: 0.000300\n",
      "2021-07-18 11:17:35,595 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.807752, Tokens per Sec:    16637, Lr: 0.000300\n",
      "2021-07-18 11:17:49,214 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.794858, Tokens per Sec:    16676, Lr: 0.000300\n",
      "2021-07-18 11:18:02,775 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     4.047292, Tokens per Sec:    16827, Lr: 0.000300\n",
      "2021-07-18 11:18:16,226 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.761487, Tokens per Sec:    16861, Lr: 0.000300\n",
      "2021-07-18 11:18:19,439 - INFO - joeynmt.training - Epoch   1: total training loss 10391.30\n",
      "2021-07-18 11:18:19,439 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-18 11:18:30,331 - INFO - joeynmt.training - Epoch   2, Step:     2400, Batch Loss:     4.054616, Tokens per Sec:    16152, Lr: 0.000300\n",
      "2021-07-18 11:18:43,936 - INFO - joeynmt.training - Epoch   2, Step:     2500, Batch Loss:     4.055769, Tokens per Sec:    16547, Lr: 0.000300\n",
      "2021-07-18 11:18:57,786 - INFO - joeynmt.training - Epoch   2, Step:     2600, Batch Loss:     3.891661, Tokens per Sec:    16669, Lr: 0.000300\n",
      "2021-07-18 11:19:11,431 - INFO - joeynmt.training - Epoch   2, Step:     2700, Batch Loss:     3.873384, Tokens per Sec:    16385, Lr: 0.000300\n",
      "2021-07-18 11:19:24,906 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     3.737725, Tokens per Sec:    16952, Lr: 0.000300\n",
      "2021-07-18 11:19:38,432 - INFO - joeynmt.training - Epoch   2, Step:     2900, Batch Loss:     3.451412, Tokens per Sec:    16745, Lr: 0.000300\n",
      "2021-07-18 11:19:51,972 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     3.619028, Tokens per Sec:    16764, Lr: 0.000300\n",
      "2021-07-18 11:20:05,706 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     3.575587, Tokens per Sec:    16667, Lr: 0.000300\n",
      "2021-07-18 11:20:19,396 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     3.796655, Tokens per Sec:    16534, Lr: 0.000300\n",
      "2021-07-18 11:20:32,923 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     3.196476, Tokens per Sec:    16537, Lr: 0.000300\n",
      "2021-07-18 11:20:46,482 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     3.207240, Tokens per Sec:    16809, Lr: 0.000300\n",
      "2021-07-18 11:20:59,977 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     3.251864, Tokens per Sec:    16736, Lr: 0.000300\n",
      "2021-07-18 11:21:13,655 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     3.656019, Tokens per Sec:    16612, Lr: 0.000300\n",
      "2021-07-18 11:21:27,320 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     3.539258, Tokens per Sec:    16699, Lr: 0.000300\n",
      "2021-07-18 11:21:40,944 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.729759, Tokens per Sec:    16650, Lr: 0.000300\n",
      "2021-07-18 11:21:54,636 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     3.369380, Tokens per Sec:    16817, Lr: 0.000300\n",
      "2021-07-18 11:22:08,261 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     3.270392, Tokens per Sec:    16700, Lr: 0.000300\n",
      "2021-07-18 11:22:22,048 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     3.304070, Tokens per Sec:    16329, Lr: 0.000300\n",
      "2021-07-18 11:22:35,767 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     3.306737, Tokens per Sec:    16626, Lr: 0.000300\n",
      "2021-07-18 11:22:49,338 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     3.490550, Tokens per Sec:    16649, Lr: 0.000300\n",
      "2021-07-18 11:23:02,991 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     3.330337, Tokens per Sec:    16841, Lr: 0.000300\n",
      "2021-07-18 11:23:16,576 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     3.476504, Tokens per Sec:    16906, Lr: 0.000300\n",
      "2021-07-18 11:23:30,309 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     3.262308, Tokens per Sec:    16687, Lr: 0.000300\n",
      "2021-07-18 11:23:36,467 - INFO - joeynmt.training - Epoch   2: total training loss 8195.23\n",
      "2021-07-18 11:23:36,468 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-18 11:23:44,128 - INFO - joeynmt.training - Epoch   3, Step:     4700, Batch Loss:     3.468995, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-07-18 11:23:58,007 - INFO - joeynmt.training - Epoch   3, Step:     4800, Batch Loss:     3.497197, Tokens per Sec:    16601, Lr: 0.000300\n",
      "2021-07-18 11:24:11,707 - INFO - joeynmt.training - Epoch   3, Step:     4900, Batch Loss:     3.512869, Tokens per Sec:    16655, Lr: 0.000300\n",
      "2021-07-18 11:24:25,136 - INFO - joeynmt.training - Epoch   3, Step:     5000, Batch Loss:     3.397258, Tokens per Sec:    16587, Lr: 0.000300\n",
      "2021-07-18 11:24:48,572 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 11:24:48,572 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 11:24:48,572 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 11:24:48,905 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 11:24:48,906 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 11:24:49,590 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - \tHypothesis: The apostles were to be a man who said , “ If I have been a man , I have been a man , ” said , “ The man of the house of the heavens . ”\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - \tHypothesis: When he was a man , he was a man who had been reported to the son of the son of the son of the son of the son of the son of the son of the Mary . ”\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - \tHypothesis: He was a time to be a time when the day was a time .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - \tHypothesis: Paul was a brief , and he was a brief , and he was a brief , and he was a brief , and the brief of the wedding .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     5000: bleu:   2.01, loss: 118329.8281, ppl:  29.3023, duration: 24.4579s\n",
      "2021-07-18 11:25:03,536 - INFO - joeynmt.training - Epoch   3, Step:     5100, Batch Loss:     3.289864, Tokens per Sec:    16383, Lr: 0.000300\n",
      "2021-07-18 11:25:17,254 - INFO - joeynmt.training - Epoch   3, Step:     5200, Batch Loss:     3.417833, Tokens per Sec:    16777, Lr: 0.000300\n",
      "2021-07-18 11:25:30,817 - INFO - joeynmt.training - Epoch   3, Step:     5300, Batch Loss:     3.418568, Tokens per Sec:    16822, Lr: 0.000300\n",
      "2021-07-18 11:25:44,443 - INFO - joeynmt.training - Epoch   3, Step:     5400, Batch Loss:     3.125911, Tokens per Sec:    16820, Lr: 0.000300\n",
      "2021-07-18 11:25:58,094 - INFO - joeynmt.training - Epoch   3, Step:     5500, Batch Loss:     3.278790, Tokens per Sec:    16774, Lr: 0.000300\n",
      "2021-07-18 11:26:11,888 - INFO - joeynmt.training - Epoch   3, Step:     5600, Batch Loss:     3.390987, Tokens per Sec:    16403, Lr: 0.000300\n",
      "2021-07-18 11:26:25,541 - INFO - joeynmt.training - Epoch   3, Step:     5700, Batch Loss:     3.461645, Tokens per Sec:    16501, Lr: 0.000300\n",
      "2021-07-18 11:26:39,252 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     3.386021, Tokens per Sec:    16958, Lr: 0.000300\n",
      "2021-07-18 11:26:52,856 - INFO - joeynmt.training - Epoch   3, Step:     5900, Batch Loss:     3.469775, Tokens per Sec:    16453, Lr: 0.000300\n",
      "2021-07-18 11:27:06,548 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     3.159902, Tokens per Sec:    16941, Lr: 0.000300\n",
      "2021-07-18 11:27:20,239 - INFO - joeynmt.training - Epoch   3, Step:     6100, Batch Loss:     3.132534, Tokens per Sec:    16627, Lr: 0.000300\n",
      "2021-07-18 11:27:33,846 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     3.480273, Tokens per Sec:    16505, Lr: 0.000300\n",
      "2021-07-18 11:27:47,430 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     3.375218, Tokens per Sec:    16729, Lr: 0.000300\n",
      "2021-07-18 11:28:01,073 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     3.493947, Tokens per Sec:    16707, Lr: 0.000300\n",
      "2021-07-18 11:28:14,647 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     3.207147, Tokens per Sec:    16713, Lr: 0.000300\n",
      "2021-07-18 11:28:28,327 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     3.245547, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 11:28:42,043 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     3.206191, Tokens per Sec:    16686, Lr: 0.000300\n",
      "2021-07-18 11:28:55,731 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     3.404485, Tokens per Sec:    16933, Lr: 0.000300\n",
      "2021-07-18 11:29:09,159 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     3.077116, Tokens per Sec:    16640, Lr: 0.000300\n",
      "2021-07-18 11:29:17,795 - INFO - joeynmt.training - Epoch   3: total training loss 7527.42\n",
      "2021-07-18 11:29:17,795 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-18 11:29:22,997 - INFO - joeynmt.training - Epoch   4, Step:     7000, Batch Loss:     3.296239, Tokens per Sec:    15407, Lr: 0.000300\n",
      "2021-07-18 11:29:36,794 - INFO - joeynmt.training - Epoch   4, Step:     7100, Batch Loss:     3.277412, Tokens per Sec:    16872, Lr: 0.000300\n",
      "2021-07-18 11:29:50,287 - INFO - joeynmt.training - Epoch   4, Step:     7200, Batch Loss:     3.302583, Tokens per Sec:    16696, Lr: 0.000300\n",
      "2021-07-18 11:30:03,942 - INFO - joeynmt.training - Epoch   4, Step:     7300, Batch Loss:     2.985823, Tokens per Sec:    16618, Lr: 0.000300\n",
      "2021-07-18 11:30:17,684 - INFO - joeynmt.training - Epoch   4, Step:     7400, Batch Loss:     3.331528, Tokens per Sec:    16919, Lr: 0.000300\n",
      "2021-07-18 11:30:31,392 - INFO - joeynmt.training - Epoch   4, Step:     7500, Batch Loss:     3.230357, Tokens per Sec:    16938, Lr: 0.000300\n",
      "2021-07-18 11:30:44,848 - INFO - joeynmt.training - Epoch   4, Step:     7600, Batch Loss:     2.926011, Tokens per Sec:    16844, Lr: 0.000300\n",
      "2021-07-18 11:30:58,474 - INFO - joeynmt.training - Epoch   4, Step:     7700, Batch Loss:     3.312961, Tokens per Sec:    16735, Lr: 0.000300\n",
      "2021-07-18 11:31:12,249 - INFO - joeynmt.training - Epoch   4, Step:     7800, Batch Loss:     2.835406, Tokens per Sec:    16489, Lr: 0.000300\n",
      "2021-07-18 11:31:25,975 - INFO - joeynmt.training - Epoch   4, Step:     7900, Batch Loss:     2.861612, Tokens per Sec:    16624, Lr: 0.000300\n",
      "2021-07-18 11:31:39,639 - INFO - joeynmt.training - Epoch   4, Step:     8000, Batch Loss:     2.718804, Tokens per Sec:    16788, Lr: 0.000300\n",
      "2021-07-18 11:31:53,182 - INFO - joeynmt.training - Epoch   4, Step:     8100, Batch Loss:     3.229550, Tokens per Sec:    16779, Lr: 0.000300\n",
      "2021-07-18 11:32:06,723 - INFO - joeynmt.training - Epoch   4, Step:     8200, Batch Loss:     3.479104, Tokens per Sec:    16741, Lr: 0.000300\n",
      "2021-07-18 11:32:20,381 - INFO - joeynmt.training - Epoch   4, Step:     8300, Batch Loss:     3.357882, Tokens per Sec:    16582, Lr: 0.000300\n",
      "2021-07-18 11:32:34,007 - INFO - joeynmt.training - Epoch   4, Step:     8400, Batch Loss:     3.202068, Tokens per Sec:    16654, Lr: 0.000300\n",
      "2021-07-18 11:32:47,666 - INFO - joeynmt.training - Epoch   4, Step:     8500, Batch Loss:     3.038518, Tokens per Sec:    16699, Lr: 0.000300\n",
      "2021-07-18 11:33:01,333 - INFO - joeynmt.training - Epoch   4, Step:     8600, Batch Loss:     3.215507, Tokens per Sec:    16733, Lr: 0.000300\n",
      "2021-07-18 11:33:14,897 - INFO - joeynmt.training - Epoch   4, Step:     8700, Batch Loss:     3.070414, Tokens per Sec:    16593, Lr: 0.000300\n",
      "2021-07-18 11:33:28,559 - INFO - joeynmt.training - Epoch   4, Step:     8800, Batch Loss:     3.164593, Tokens per Sec:    16596, Lr: 0.000300\n",
      "2021-07-18 11:33:42,137 - INFO - joeynmt.training - Epoch   4, Step:     8900, Batch Loss:     2.706897, Tokens per Sec:    16557, Lr: 0.000300\n",
      "2021-07-18 11:33:55,573 - INFO - joeynmt.training - Epoch   4, Step:     9000, Batch Loss:     3.207397, Tokens per Sec:    16870, Lr: 0.000300\n",
      "2021-07-18 11:34:09,165 - INFO - joeynmt.training - Epoch   4, Step:     9100, Batch Loss:     2.665085, Tokens per Sec:    16680, Lr: 0.000300\n",
      "2021-07-18 11:34:22,738 - INFO - joeynmt.training - Epoch   4, Step:     9200, Batch Loss:     2.971258, Tokens per Sec:    16464, Lr: 0.000300\n",
      "2021-07-18 11:34:34,416 - INFO - joeynmt.training - Epoch   4: total training loss 7190.07\n",
      "2021-07-18 11:34:34,416 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-18 11:34:36,720 - INFO - joeynmt.training - Epoch   5, Step:     9300, Batch Loss:     3.196618, Tokens per Sec:    14307, Lr: 0.000300\n",
      "2021-07-18 11:34:50,387 - INFO - joeynmt.training - Epoch   5, Step:     9400, Batch Loss:     2.766790, Tokens per Sec:    16800, Lr: 0.000300\n",
      "2021-07-18 11:35:04,086 - INFO - joeynmt.training - Epoch   5, Step:     9500, Batch Loss:     2.530447, Tokens per Sec:    16509, Lr: 0.000300\n",
      "2021-07-18 11:35:17,618 - INFO - joeynmt.training - Epoch   5, Step:     9600, Batch Loss:     3.336390, Tokens per Sec:    16876, Lr: 0.000300\n",
      "2021-07-18 11:35:31,086 - INFO - joeynmt.training - Epoch   5, Step:     9700, Batch Loss:     3.191032, Tokens per Sec:    16705, Lr: 0.000300\n",
      "2021-07-18 11:35:44,775 - INFO - joeynmt.training - Epoch   5, Step:     9800, Batch Loss:     3.259223, Tokens per Sec:    16632, Lr: 0.000300\n",
      "2021-07-18 11:35:58,537 - INFO - joeynmt.training - Epoch   5, Step:     9900, Batch Loss:     3.201852, Tokens per Sec:    16825, Lr: 0.000300\n",
      "2021-07-18 11:36:12,283 - INFO - joeynmt.training - Epoch   5, Step:    10000, Batch Loss:     3.243787, Tokens per Sec:    16407, Lr: 0.000300\n",
      "2021-07-18 11:36:45,633 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 11:36:45,633 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 11:36:45,634 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 11:36:45,981 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 11:36:45,981 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 11:36:46,663 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 11:36:46,664 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 11:36:46,664 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 11:36:46,665 - INFO - joeynmt.training - \tHypothesis: And the Pharisees , who was a man who had been given him , saying , “ He will be raised , and he will be silver , and he will be silver . ”\n",
      "2021-07-18 11:36:46,665 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 11:36:46,665 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 11:36:46,666 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 11:36:46,667 - INFO - joeynmt.training - \tHypothesis: When he was a great crowd , he was a man of the mother , and he said to Him , “ Lord , and he will be raised . ”\n",
      "2021-07-18 11:36:46,667 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the day .\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 11:36:46,669 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 11:36:46,669 - INFO - joeynmt.training - \tHypothesis: And he was raised , and he was raised , and he was raised , and he was a source of the seven seven of the seven seven seven of the wind .\n",
      "2021-07-18 11:36:46,669 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    10000: bleu:   3.14, loss: 107455.4219, ppl:  21.4830, duration: 34.3853s\n",
      "2021-07-18 11:37:00,389 - INFO - joeynmt.training - Epoch   5, Step:    10100, Batch Loss:     3.214835, Tokens per Sec:    16690, Lr: 0.000300\n",
      "2021-07-18 11:37:14,243 - INFO - joeynmt.training - Epoch   5, Step:    10200, Batch Loss:     3.125715, Tokens per Sec:    16703, Lr: 0.000300\n",
      "2021-07-18 11:37:27,808 - INFO - joeynmt.training - Epoch   5, Step:    10300, Batch Loss:     3.316646, Tokens per Sec:    16475, Lr: 0.000300\n",
      "2021-07-18 11:37:41,488 - INFO - joeynmt.training - Epoch   5, Step:    10400, Batch Loss:     3.055973, Tokens per Sec:    16609, Lr: 0.000300\n",
      "2021-07-18 11:37:55,129 - INFO - joeynmt.training - Epoch   5, Step:    10500, Batch Loss:     3.117477, Tokens per Sec:    16717, Lr: 0.000300\n",
      "2021-07-18 11:38:08,728 - INFO - joeynmt.training - Epoch   5, Step:    10600, Batch Loss:     2.911697, Tokens per Sec:    16705, Lr: 0.000300\n",
      "2021-07-18 11:38:22,530 - INFO - joeynmt.training - Epoch   5, Step:    10700, Batch Loss:     2.984868, Tokens per Sec:    16526, Lr: 0.000300\n",
      "2021-07-18 11:38:36,072 - INFO - joeynmt.training - Epoch   5, Step:    10800, Batch Loss:     3.322159, Tokens per Sec:    16617, Lr: 0.000300\n",
      "2021-07-18 11:38:49,660 - INFO - joeynmt.training - Epoch   5, Step:    10900, Batch Loss:     2.906811, Tokens per Sec:    16745, Lr: 0.000300\n",
      "2021-07-18 11:39:03,022 - INFO - joeynmt.training - Epoch   5, Step:    11000, Batch Loss:     2.893350, Tokens per Sec:    16677, Lr: 0.000300\n",
      "2021-07-18 11:39:16,713 - INFO - joeynmt.training - Epoch   5, Step:    11100, Batch Loss:     3.314165, Tokens per Sec:    16674, Lr: 0.000300\n",
      "2021-07-18 11:39:30,404 - INFO - joeynmt.training - Epoch   5, Step:    11200, Batch Loss:     2.942385, Tokens per Sec:    16613, Lr: 0.000300\n",
      "2021-07-18 11:39:44,031 - INFO - joeynmt.training - Epoch   5, Step:    11300, Batch Loss:     3.192435, Tokens per Sec:    16663, Lr: 0.000300\n",
      "2021-07-18 11:39:57,594 - INFO - joeynmt.training - Epoch   5, Step:    11400, Batch Loss:     2.880817, Tokens per Sec:    16633, Lr: 0.000300\n",
      "2021-07-18 11:40:11,154 - INFO - joeynmt.training - Epoch   5, Step:    11500, Batch Loss:     3.142082, Tokens per Sec:    16870, Lr: 0.000300\n",
      "2021-07-18 11:40:24,754 - INFO - joeynmt.training - Epoch   5, Step:    11600, Batch Loss:     3.189060, Tokens per Sec:    16663, Lr: 0.000300\n",
      "2021-07-18 11:40:25,827 - INFO - joeynmt.training - Epoch   5: total training loss 6979.33\n",
      "2021-07-18 11:40:25,827 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-18 11:40:38,792 - INFO - joeynmt.training - Epoch   6, Step:    11700, Batch Loss:     2.928677, Tokens per Sec:    16372, Lr: 0.000300\n",
      "2021-07-18 11:40:52,457 - INFO - joeynmt.training - Epoch   6, Step:    11800, Batch Loss:     2.441700, Tokens per Sec:    16741, Lr: 0.000300\n",
      "2021-07-18 11:41:06,065 - INFO - joeynmt.training - Epoch   6, Step:    11900, Batch Loss:     3.163107, Tokens per Sec:    16511, Lr: 0.000300\n",
      "2021-07-18 11:41:19,742 - INFO - joeynmt.training - Epoch   6, Step:    12000, Batch Loss:     3.225448, Tokens per Sec:    16616, Lr: 0.000300\n",
      "2021-07-18 11:41:33,284 - INFO - joeynmt.training - Epoch   6, Step:    12100, Batch Loss:     3.053762, Tokens per Sec:    16752, Lr: 0.000300\n",
      "2021-07-18 11:41:46,743 - INFO - joeynmt.training - Epoch   6, Step:    12200, Batch Loss:     2.944340, Tokens per Sec:    16686, Lr: 0.000300\n",
      "2021-07-18 11:42:00,274 - INFO - joeynmt.training - Epoch   6, Step:    12300, Batch Loss:     3.046337, Tokens per Sec:    16659, Lr: 0.000300\n",
      "2021-07-18 11:42:13,929 - INFO - joeynmt.training - Epoch   6, Step:    12400, Batch Loss:     2.950295, Tokens per Sec:    16345, Lr: 0.000300\n",
      "2021-07-18 11:42:27,668 - INFO - joeynmt.training - Epoch   6, Step:    12500, Batch Loss:     2.666849, Tokens per Sec:    16637, Lr: 0.000300\n",
      "2021-07-18 11:42:41,256 - INFO - joeynmt.training - Epoch   6, Step:    12600, Batch Loss:     2.830174, Tokens per Sec:    16727, Lr: 0.000300\n",
      "2021-07-18 11:42:54,647 - INFO - joeynmt.training - Epoch   6, Step:    12700, Batch Loss:     2.856973, Tokens per Sec:    16544, Lr: 0.000300\n",
      "2021-07-18 11:43:08,296 - INFO - joeynmt.training - Epoch   6, Step:    12800, Batch Loss:     2.728805, Tokens per Sec:    16683, Lr: 0.000300\n",
      "2021-07-18 11:43:21,903 - INFO - joeynmt.training - Epoch   6, Step:    12900, Batch Loss:     2.924922, Tokens per Sec:    16558, Lr: 0.000300\n",
      "2021-07-18 11:43:35,645 - INFO - joeynmt.training - Epoch   6, Step:    13000, Batch Loss:     2.544010, Tokens per Sec:    16926, Lr: 0.000300\n",
      "2021-07-18 11:43:49,279 - INFO - joeynmt.training - Epoch   6, Step:    13100, Batch Loss:     2.668169, Tokens per Sec:    16792, Lr: 0.000300\n",
      "2021-07-18 11:44:02,842 - INFO - joeynmt.training - Epoch   6, Step:    13200, Batch Loss:     2.896705, Tokens per Sec:    16727, Lr: 0.000300\n",
      "2021-07-18 11:44:16,750 - INFO - joeynmt.training - Epoch   6, Step:    13300, Batch Loss:     3.067722, Tokens per Sec:    16571, Lr: 0.000300\n",
      "2021-07-18 11:44:30,563 - INFO - joeynmt.training - Epoch   6, Step:    13400, Batch Loss:     2.941170, Tokens per Sec:    16565, Lr: 0.000300\n",
      "2021-07-18 11:44:44,196 - INFO - joeynmt.training - Epoch   6, Step:    13500, Batch Loss:     2.299691, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-18 11:44:57,973 - INFO - joeynmt.training - Epoch   6, Step:    13600, Batch Loss:     2.765565, Tokens per Sec:    16716, Lr: 0.000300\n",
      "2021-07-18 11:45:11,711 - INFO - joeynmt.training - Epoch   6, Step:    13700, Batch Loss:     2.785002, Tokens per Sec:    16774, Lr: 0.000300\n",
      "2021-07-18 11:45:25,506 - INFO - joeynmt.training - Epoch   6, Step:    13800, Batch Loss:     3.036847, Tokens per Sec:    16816, Lr: 0.000300\n",
      "2021-07-18 11:45:39,112 - INFO - joeynmt.training - Epoch   6, Step:    13900, Batch Loss:     2.611948, Tokens per Sec:    16601, Lr: 0.000300\n",
      "2021-07-18 11:45:42,666 - INFO - joeynmt.training - Epoch   6: total training loss 6793.23\n",
      "2021-07-18 11:45:42,667 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-18 11:45:52,900 - INFO - joeynmt.training - Epoch   7, Step:    14000, Batch Loss:     2.982269, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-07-18 11:46:06,569 - INFO - joeynmt.training - Epoch   7, Step:    14100, Batch Loss:     2.837081, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-07-18 11:46:20,169 - INFO - joeynmt.training - Epoch   7, Step:    14200, Batch Loss:     2.618802, Tokens per Sec:    16711, Lr: 0.000300\n",
      "2021-07-18 11:46:33,695 - INFO - joeynmt.training - Epoch   7, Step:    14300, Batch Loss:     2.969814, Tokens per Sec:    16489, Lr: 0.000300\n",
      "2021-07-18 11:46:47,402 - INFO - joeynmt.training - Epoch   7, Step:    14400, Batch Loss:     2.815508, Tokens per Sec:    16875, Lr: 0.000300\n",
      "2021-07-18 11:47:01,155 - INFO - joeynmt.training - Epoch   7, Step:    14500, Batch Loss:     2.807916, Tokens per Sec:    16731, Lr: 0.000300\n",
      "2021-07-18 11:47:14,960 - INFO - joeynmt.training - Epoch   7, Step:    14600, Batch Loss:     2.808668, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-07-18 11:47:28,701 - INFO - joeynmt.training - Epoch   7, Step:    14700, Batch Loss:     2.939392, Tokens per Sec:    16813, Lr: 0.000300\n",
      "2021-07-18 11:47:42,304 - INFO - joeynmt.training - Epoch   7, Step:    14800, Batch Loss:     3.015333, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-18 11:47:55,848 - INFO - joeynmt.training - Epoch   7, Step:    14900, Batch Loss:     2.890965, Tokens per Sec:    16840, Lr: 0.000300\n",
      "2021-07-18 11:48:09,657 - INFO - joeynmt.training - Epoch   7, Step:    15000, Batch Loss:     2.783857, Tokens per Sec:    16793, Lr: 0.000300\n",
      "2021-07-18 11:48:39,225 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 11:48:39,225 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 11:48:39,225 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 11:48:39,564 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 11:48:39,564 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 11:48:40,312 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 11:48:40,313 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 11:48:40,313 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - \tHypothesis: And the Pharisees who had been given to Him , saying , “ He who was with Him , and he said to them , “ I shall go to the house of the house of the house . ”\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - \tHypothesis: When she was studying the Bible , she was a mother , and she asked her to her , “ Mary , and she was a little man , and she was baptized . ”\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the day .\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 11:48:40,316 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 11:48:40,317 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 11:48:40,317 - INFO - joeynmt.training - \tHypothesis: And he who was baptized , and he saw Him , and he saw Him , and he was sleeping , and he was sleeping on the road of the road , and he was sleeping .\n",
      "2021-07-18 11:48:40,317 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    15000: bleu:   3.78, loss: 102888.7109, ppl:  18.8575, duration: 30.6592s\n",
      "2021-07-18 11:48:53,961 - INFO - joeynmt.training - Epoch   7, Step:    15100, Batch Loss:     3.057984, Tokens per Sec:    16551, Lr: 0.000300\n",
      "2021-07-18 11:49:07,703 - INFO - joeynmt.training - Epoch   7, Step:    15200, Batch Loss:     2.851904, Tokens per Sec:    16858, Lr: 0.000300\n",
      "2021-07-18 11:49:21,685 - INFO - joeynmt.training - Epoch   7, Step:    15300, Batch Loss:     2.973060, Tokens per Sec:    16619, Lr: 0.000300\n",
      "2021-07-18 11:49:35,447 - INFO - joeynmt.training - Epoch   7, Step:    15400, Batch Loss:     2.940523, Tokens per Sec:    16416, Lr: 0.000300\n",
      "2021-07-18 11:49:49,040 - INFO - joeynmt.training - Epoch   7, Step:    15500, Batch Loss:     2.966185, Tokens per Sec:    16412, Lr: 0.000300\n",
      "2021-07-18 11:50:02,559 - INFO - joeynmt.training - Epoch   7, Step:    15600, Batch Loss:     2.975596, Tokens per Sec:    16546, Lr: 0.000300\n",
      "2021-07-18 11:50:16,285 - INFO - joeynmt.training - Epoch   7, Step:    15700, Batch Loss:     2.970467, Tokens per Sec:    16453, Lr: 0.000300\n",
      "2021-07-18 11:50:30,274 - INFO - joeynmt.training - Epoch   7, Step:    15800, Batch Loss:     2.991073, Tokens per Sec:    16731, Lr: 0.000300\n",
      "2021-07-18 11:50:43,925 - INFO - joeynmt.training - Epoch   7, Step:    15900, Batch Loss:     2.941234, Tokens per Sec:    16460, Lr: 0.000300\n",
      "2021-07-18 11:50:57,633 - INFO - joeynmt.training - Epoch   7, Step:    16000, Batch Loss:     2.829274, Tokens per Sec:    16819, Lr: 0.000300\n",
      "2021-07-18 11:51:11,331 - INFO - joeynmt.training - Epoch   7, Step:    16100, Batch Loss:     2.946280, Tokens per Sec:    16695, Lr: 0.000300\n",
      "2021-07-18 11:51:25,094 - INFO - joeynmt.training - Epoch   7, Step:    16200, Batch Loss:     2.887893, Tokens per Sec:    16627, Lr: 0.000300\n",
      "2021-07-18 11:51:30,953 - INFO - joeynmt.training - Epoch   7: total training loss 6674.53\n",
      "2021-07-18 11:51:30,954 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-18 11:51:39,168 - INFO - joeynmt.training - Epoch   8, Step:    16300, Batch Loss:     2.986401, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-07-18 11:51:52,837 - INFO - joeynmt.training - Epoch   8, Step:    16400, Batch Loss:     2.915144, Tokens per Sec:    16506, Lr: 0.000300\n",
      "2021-07-18 11:52:06,819 - INFO - joeynmt.training - Epoch   8, Step:    16500, Batch Loss:     2.760735, Tokens per Sec:    16760, Lr: 0.000300\n",
      "2021-07-18 11:52:20,516 - INFO - joeynmt.training - Epoch   8, Step:    16600, Batch Loss:     2.621777, Tokens per Sec:    16542, Lr: 0.000300\n",
      "2021-07-18 11:52:34,218 - INFO - joeynmt.training - Epoch   8, Step:    16700, Batch Loss:     3.189390, Tokens per Sec:    16513, Lr: 0.000300\n",
      "2021-07-18 11:52:47,985 - INFO - joeynmt.training - Epoch   8, Step:    16800, Batch Loss:     2.971002, Tokens per Sec:    16780, Lr: 0.000300\n",
      "2021-07-18 11:53:01,733 - INFO - joeynmt.training - Epoch   8, Step:    16900, Batch Loss:     3.136696, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-18 11:53:15,505 - INFO - joeynmt.training - Epoch   8, Step:    17000, Batch Loss:     2.779980, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-18 11:53:29,266 - INFO - joeynmt.training - Epoch   8, Step:    17100, Batch Loss:     2.436192, Tokens per Sec:    16549, Lr: 0.000300\n",
      "2021-07-18 11:53:42,897 - INFO - joeynmt.training - Epoch   8, Step:    17200, Batch Loss:     2.741244, Tokens per Sec:    16827, Lr: 0.000300\n",
      "2021-07-18 11:53:56,500 - INFO - joeynmt.training - Epoch   8, Step:    17300, Batch Loss:     2.854635, Tokens per Sec:    16734, Lr: 0.000300\n",
      "2021-07-18 11:54:10,241 - INFO - joeynmt.training - Epoch   8, Step:    17400, Batch Loss:     2.171211, Tokens per Sec:    16340, Lr: 0.000300\n",
      "2021-07-18 11:54:23,975 - INFO - joeynmt.training - Epoch   8, Step:    17500, Batch Loss:     2.886678, Tokens per Sec:    16306, Lr: 0.000300\n",
      "2021-07-18 11:54:37,740 - INFO - joeynmt.training - Epoch   8, Step:    17600, Batch Loss:     2.700441, Tokens per Sec:    16614, Lr: 0.000300\n",
      "2021-07-18 11:54:51,280 - INFO - joeynmt.training - Epoch   8, Step:    17700, Batch Loss:     2.702967, Tokens per Sec:    16606, Lr: 0.000300\n",
      "2021-07-18 11:55:04,741 - INFO - joeynmt.training - Epoch   8, Step:    17800, Batch Loss:     2.715117, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-07-18 11:55:18,398 - INFO - joeynmt.training - Epoch   8, Step:    17900, Batch Loss:     2.714948, Tokens per Sec:    16653, Lr: 0.000300\n",
      "2021-07-18 11:55:32,122 - INFO - joeynmt.training - Epoch   8, Step:    18000, Batch Loss:     2.979905, Tokens per Sec:    16478, Lr: 0.000300\n",
      "2021-07-18 11:55:45,806 - INFO - joeynmt.training - Epoch   8, Step:    18100, Batch Loss:     2.819937, Tokens per Sec:    16654, Lr: 0.000300\n",
      "2021-07-18 11:55:59,395 - INFO - joeynmt.training - Epoch   8, Step:    18200, Batch Loss:     3.098244, Tokens per Sec:    16586, Lr: 0.000300\n",
      "2021-07-18 11:56:12,996 - INFO - joeynmt.training - Epoch   8, Step:    18300, Batch Loss:     2.164204, Tokens per Sec:    16381, Lr: 0.000300\n",
      "2021-07-18 11:56:26,832 - INFO - joeynmt.training - Epoch   8, Step:    18400, Batch Loss:     3.014172, Tokens per Sec:    16487, Lr: 0.000300\n",
      "2021-07-18 11:56:40,643 - INFO - joeynmt.training - Epoch   8, Step:    18500, Batch Loss:     2.843159, Tokens per Sec:    16404, Lr: 0.000300\n",
      "2021-07-18 11:56:49,913 - INFO - joeynmt.training - Epoch   8: total training loss 6595.56\n",
      "2021-07-18 11:56:49,913 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-18 11:56:54,496 - INFO - joeynmt.training - Epoch   9, Step:    18600, Batch Loss:     2.932591, Tokens per Sec:    14736, Lr: 0.000300\n",
      "2021-07-18 11:57:08,244 - INFO - joeynmt.training - Epoch   9, Step:    18700, Batch Loss:     2.293607, Tokens per Sec:    16425, Lr: 0.000300\n",
      "2021-07-18 11:57:21,886 - INFO - joeynmt.training - Epoch   9, Step:    18800, Batch Loss:     2.666705, Tokens per Sec:    16738, Lr: 0.000300\n",
      "2021-07-18 11:57:35,557 - INFO - joeynmt.training - Epoch   9, Step:    18900, Batch Loss:     3.022337, Tokens per Sec:    16512, Lr: 0.000300\n",
      "2021-07-18 11:57:49,162 - INFO - joeynmt.training - Epoch   9, Step:    19000, Batch Loss:     2.551999, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-07-18 11:58:02,886 - INFO - joeynmt.training - Epoch   9, Step:    19100, Batch Loss:     2.876385, Tokens per Sec:    16750, Lr: 0.000300\n",
      "2021-07-18 11:58:16,695 - INFO - joeynmt.training - Epoch   9, Step:    19200, Batch Loss:     3.052240, Tokens per Sec:    16366, Lr: 0.000300\n",
      "2021-07-18 11:58:30,287 - INFO - joeynmt.training - Epoch   9, Step:    19300, Batch Loss:     2.737508, Tokens per Sec:    16679, Lr: 0.000300\n",
      "2021-07-18 11:58:43,817 - INFO - joeynmt.training - Epoch   9, Step:    19400, Batch Loss:     2.894639, Tokens per Sec:    16405, Lr: 0.000300\n",
      "2021-07-18 11:58:57,587 - INFO - joeynmt.training - Epoch   9, Step:    19500, Batch Loss:     2.522689, Tokens per Sec:    16754, Lr: 0.000300\n",
      "2021-07-18 11:59:11,498 - INFO - joeynmt.training - Epoch   9, Step:    19600, Batch Loss:     2.788762, Tokens per Sec:    16613, Lr: 0.000300\n",
      "2021-07-18 11:59:25,121 - INFO - joeynmt.training - Epoch   9, Step:    19700, Batch Loss:     3.152863, Tokens per Sec:    16645, Lr: 0.000300\n",
      "2021-07-18 11:59:38,804 - INFO - joeynmt.training - Epoch   9, Step:    19800, Batch Loss:     2.967395, Tokens per Sec:    16775, Lr: 0.000300\n",
      "2021-07-18 11:59:52,639 - INFO - joeynmt.training - Epoch   9, Step:    19900, Batch Loss:     2.857632, Tokens per Sec:    16769, Lr: 0.000300\n",
      "2021-07-18 12:00:06,309 - INFO - joeynmt.training - Epoch   9, Step:    20000, Batch Loss:     2.449187, Tokens per Sec:    16634, Lr: 0.000300\n",
      "2021-07-18 12:00:35,921 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 12:00:35,921 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 12:00:35,921 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 12:00:36,263 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 12:00:36,263 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 12:00:37,002 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 12:00:37,003 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 12:00:37,003 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 12:00:37,003 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw Him , and He said to Him , “ He who was with Him , and He said to Him , “ I shall be with you , and I will see you . ”\n",
      "2021-07-18 12:00:37,004 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 12:00:37,004 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 12:00:37,004 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 12:00:37,004 - INFO - joeynmt.training - \tHypothesis: When she was a man , she was able to see her mother , and she was asked to go to Mary , “ Mary , and she was a young man . ”\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the day .\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 12:00:37,006 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 12:00:37,006 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 12:00:37,006 - INFO - joeynmt.training - \tHypothesis: And he was with Him , and when He was in the midst of the sea , and he was raised , and he was sat down to the road of the road .\n",
      "2021-07-18 12:00:37,006 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    20000: bleu:   4.57, loss: 98859.5156, ppl:  16.8087, duration: 30.6970s\n",
      "2021-07-18 12:00:50,674 - INFO - joeynmt.training - Epoch   9, Step:    20100, Batch Loss:     2.747767, Tokens per Sec:    16386, Lr: 0.000300\n",
      "2021-07-18 12:01:04,185 - INFO - joeynmt.training - Epoch   9, Step:    20200, Batch Loss:     3.011094, Tokens per Sec:    16641, Lr: 0.000300\n",
      "2021-07-18 12:01:17,847 - INFO - joeynmt.training - Epoch   9, Step:    20300, Batch Loss:     2.848831, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-18 12:01:31,515 - INFO - joeynmt.training - Epoch   9, Step:    20400, Batch Loss:     2.855074, Tokens per Sec:    16603, Lr: 0.000300\n",
      "2021-07-18 12:01:45,087 - INFO - joeynmt.training - Epoch   9, Step:    20500, Batch Loss:     2.655996, Tokens per Sec:    16409, Lr: 0.000300\n",
      "2021-07-18 12:01:58,639 - INFO - joeynmt.training - Epoch   9, Step:    20600, Batch Loss:     2.717762, Tokens per Sec:    16845, Lr: 0.000300\n",
      "2021-07-18 12:02:12,005 - INFO - joeynmt.training - Epoch   9, Step:    20700, Batch Loss:     2.634065, Tokens per Sec:    16560, Lr: 0.000300\n",
      "2021-07-18 12:02:25,624 - INFO - joeynmt.training - Epoch   9, Step:    20800, Batch Loss:     2.901412, Tokens per Sec:    16841, Lr: 0.000300\n",
      "2021-07-18 12:02:38,804 - INFO - joeynmt.training - Epoch   9: total training loss 6518.89\n",
      "2021-07-18 12:02:38,804 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-18 12:02:39,645 - INFO - joeynmt.training - Epoch  10, Step:    20900, Batch Loss:     2.450933, Tokens per Sec:    10018, Lr: 0.000300\n",
      "2021-07-18 12:02:53,303 - INFO - joeynmt.training - Epoch  10, Step:    21000, Batch Loss:     2.891134, Tokens per Sec:    16886, Lr: 0.000300\n",
      "2021-07-18 12:03:07,044 - INFO - joeynmt.training - Epoch  10, Step:    21100, Batch Loss:     2.874460, Tokens per Sec:    16602, Lr: 0.000300\n",
      "2021-07-18 12:03:20,707 - INFO - joeynmt.training - Epoch  10, Step:    21200, Batch Loss:     2.702536, Tokens per Sec:    16662, Lr: 0.000300\n",
      "2021-07-18 12:03:34,309 - INFO - joeynmt.training - Epoch  10, Step:    21300, Batch Loss:     2.672738, Tokens per Sec:    16915, Lr: 0.000300\n",
      "2021-07-18 12:03:47,833 - INFO - joeynmt.training - Epoch  10, Step:    21400, Batch Loss:     3.028269, Tokens per Sec:    16527, Lr: 0.000300\n",
      "2021-07-18 12:04:01,461 - INFO - joeynmt.training - Epoch  10, Step:    21500, Batch Loss:     2.336853, Tokens per Sec:    16872, Lr: 0.000300\n",
      "2021-07-18 12:04:15,177 - INFO - joeynmt.training - Epoch  10, Step:    21600, Batch Loss:     2.730253, Tokens per Sec:    16311, Lr: 0.000300\n",
      "2021-07-18 12:04:28,789 - INFO - joeynmt.training - Epoch  10, Step:    21700, Batch Loss:     2.800956, Tokens per Sec:    16565, Lr: 0.000300\n",
      "2021-07-18 12:04:42,454 - INFO - joeynmt.training - Epoch  10, Step:    21800, Batch Loss:     2.541595, Tokens per Sec:    16836, Lr: 0.000300\n",
      "2021-07-18 12:04:56,001 - INFO - joeynmt.training - Epoch  10, Step:    21900, Batch Loss:     2.805495, Tokens per Sec:    16584, Lr: 0.000300\n",
      "2021-07-18 12:05:09,643 - INFO - joeynmt.training - Epoch  10, Step:    22000, Batch Loss:     2.706205, Tokens per Sec:    16764, Lr: 0.000300\n",
      "2021-07-18 12:05:23,352 - INFO - joeynmt.training - Epoch  10, Step:    22100, Batch Loss:     2.454398, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-07-18 12:05:36,900 - INFO - joeynmt.training - Epoch  10, Step:    22200, Batch Loss:     2.686183, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-18 12:05:50,471 - INFO - joeynmt.training - Epoch  10, Step:    22300, Batch Loss:     2.545541, Tokens per Sec:    16726, Lr: 0.000300\n",
      "2021-07-18 12:06:03,849 - INFO - joeynmt.training - Epoch  10, Step:    22400, Batch Loss:     2.499347, Tokens per Sec:    16756, Lr: 0.000300\n",
      "2021-07-18 12:06:17,376 - INFO - joeynmt.training - Epoch  10, Step:    22500, Batch Loss:     2.250569, Tokens per Sec:    16674, Lr: 0.000300\n",
      "2021-07-18 12:06:31,056 - INFO - joeynmt.training - Epoch  10, Step:    22600, Batch Loss:     2.729260, Tokens per Sec:    16621, Lr: 0.000300\n",
      "2021-07-18 12:06:44,641 - INFO - joeynmt.training - Epoch  10, Step:    22700, Batch Loss:     2.634743, Tokens per Sec:    16748, Lr: 0.000300\n",
      "2021-07-18 12:06:58,281 - INFO - joeynmt.training - Epoch  10, Step:    22800, Batch Loss:     2.817023, Tokens per Sec:    16751, Lr: 0.000300\n",
      "2021-07-18 12:07:11,776 - INFO - joeynmt.training - Epoch  10, Step:    22900, Batch Loss:     2.212120, Tokens per Sec:    16897, Lr: 0.000300\n",
      "2021-07-18 12:07:25,476 - INFO - joeynmt.training - Epoch  10, Step:    23000, Batch Loss:     2.923583, Tokens per Sec:    16613, Lr: 0.000300\n",
      "2021-07-18 12:07:39,091 - INFO - joeynmt.training - Epoch  10, Step:    23100, Batch Loss:     2.960685, Tokens per Sec:    16610, Lr: 0.000300\n",
      "2021-07-18 12:07:52,797 - INFO - joeynmt.training - Epoch  10, Step:    23200, Batch Loss:     2.976733, Tokens per Sec:    16549, Lr: 0.000300\n",
      "2021-07-18 12:07:55,869 - INFO - joeynmt.training - Epoch  10: total training loss 6443.32\n",
      "2021-07-18 12:07:55,869 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-18 12:08:06,812 - INFO - joeynmt.training - Epoch  11, Step:    23300, Batch Loss:     2.581611, Tokens per Sec:    16136, Lr: 0.000300\n",
      "2021-07-18 12:08:20,398 - INFO - joeynmt.training - Epoch  11, Step:    23400, Batch Loss:     2.932013, Tokens per Sec:    16655, Lr: 0.000300\n",
      "2021-07-18 12:08:33,949 - INFO - joeynmt.training - Epoch  11, Step:    23500, Batch Loss:     2.771335, Tokens per Sec:    16758, Lr: 0.000300\n",
      "2021-07-18 12:08:47,467 - INFO - joeynmt.training - Epoch  11, Step:    23600, Batch Loss:     2.705549, Tokens per Sec:    16873, Lr: 0.000300\n",
      "2021-07-18 12:09:01,039 - INFO - joeynmt.training - Epoch  11, Step:    23700, Batch Loss:     2.906891, Tokens per Sec:    16644, Lr: 0.000300\n",
      "2021-07-18 12:09:14,708 - INFO - joeynmt.training - Epoch  11, Step:    23800, Batch Loss:     2.888039, Tokens per Sec:    16618, Lr: 0.000300\n",
      "2021-07-18 12:09:28,217 - INFO - joeynmt.training - Epoch  11, Step:    23900, Batch Loss:     2.676244, Tokens per Sec:    16911, Lr: 0.000300\n",
      "2021-07-18 12:09:41,757 - INFO - joeynmt.training - Epoch  11, Step:    24000, Batch Loss:     2.711621, Tokens per Sec:    16622, Lr: 0.000300\n",
      "2021-07-18 12:09:55,222 - INFO - joeynmt.training - Epoch  11, Step:    24100, Batch Loss:     2.699487, Tokens per Sec:    16824, Lr: 0.000300\n",
      "2021-07-18 12:10:08,941 - INFO - joeynmt.training - Epoch  11, Step:    24200, Batch Loss:     2.714545, Tokens per Sec:    16553, Lr: 0.000300\n",
      "2021-07-18 12:10:22,454 - INFO - joeynmt.training - Epoch  11, Step:    24300, Batch Loss:     2.898083, Tokens per Sec:    16398, Lr: 0.000300\n",
      "2021-07-18 12:10:35,967 - INFO - joeynmt.training - Epoch  11, Step:    24400, Batch Loss:     2.501831, Tokens per Sec:    16823, Lr: 0.000300\n",
      "2021-07-18 12:10:49,604 - INFO - joeynmt.training - Epoch  11, Step:    24500, Batch Loss:     2.801112, Tokens per Sec:    16670, Lr: 0.000300\n",
      "2021-07-18 12:11:03,172 - INFO - joeynmt.training - Epoch  11, Step:    24600, Batch Loss:     2.740498, Tokens per Sec:    16854, Lr: 0.000300\n",
      "2021-07-18 12:11:16,820 - INFO - joeynmt.training - Epoch  11, Step:    24700, Batch Loss:     2.582118, Tokens per Sec:    16684, Lr: 0.000300\n",
      "2021-07-18 12:11:30,592 - INFO - joeynmt.training - Epoch  11, Step:    24800, Batch Loss:     2.751882, Tokens per Sec:    16412, Lr: 0.000300\n",
      "2021-07-18 12:11:44,037 - INFO - joeynmt.training - Epoch  11, Step:    24900, Batch Loss:     2.641244, Tokens per Sec:    16807, Lr: 0.000300\n",
      "2021-07-18 12:11:57,591 - INFO - joeynmt.training - Epoch  11, Step:    25000, Batch Loss:     2.104525, Tokens per Sec:    16545, Lr: 0.000300\n",
      "2021-07-18 12:12:23,162 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 12:12:23,162 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 12:12:23,162 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 12:12:23,541 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 12:12:23,541 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 12:12:24,256 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 12:12:24,257 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 12:12:24,257 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 12:12:24,257 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to Him , “ He who is in the midst of the mouth , and He said to them , “ I shall come to you . ”\n",
      "2021-07-18 12:12:24,258 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 12:12:24,258 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 12:12:24,258 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 12:12:24,259 - INFO - joeynmt.training - \tHypothesis: When she was baptized , she went to Mary , and said to her , “ Mary , and she was a man , and said to him , “ You have come to me . ”\n",
      "2021-07-18 12:12:24,259 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 12:12:24,259 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 12:12:24,259 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 12:12:24,260 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day .\n",
      "2021-07-18 12:12:24,260 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 12:12:24,260 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 12:12:24,260 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 12:12:24,261 - INFO - joeynmt.training - \tHypothesis: And he had seen Him , and when He was in the midst of the bread , he was raised up , and he was a bread of the bread .\n",
      "2021-07-18 12:12:24,261 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    25000: bleu:   5.49, loss: 96824.3672, ppl:  15.8601, duration: 26.6690s\n",
      "2021-07-18 12:12:38,251 - INFO - joeynmt.training - Epoch  11, Step:    25100, Batch Loss:     2.567255, Tokens per Sec:    16665, Lr: 0.000300\n",
      "2021-07-18 12:12:51,814 - INFO - joeynmt.training - Epoch  11, Step:    25200, Batch Loss:     2.771243, Tokens per Sec:    16682, Lr: 0.000300\n",
      "2021-07-18 12:13:05,223 - INFO - joeynmt.training - Epoch  11, Step:    25300, Batch Loss:     2.407258, Tokens per Sec:    16799, Lr: 0.000300\n",
      "2021-07-18 12:13:18,789 - INFO - joeynmt.training - Epoch  11, Step:    25400, Batch Loss:     2.909930, Tokens per Sec:    16916, Lr: 0.000300\n",
      "2021-07-18 12:13:32,508 - INFO - joeynmt.training - Epoch  11, Step:    25500, Batch Loss:     2.830852, Tokens per Sec:    16390, Lr: 0.000300\n",
      "2021-07-18 12:13:39,371 - INFO - joeynmt.training - Epoch  11: total training loss 6394.96\n",
      "2021-07-18 12:13:39,371 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-18 12:13:46,318 - INFO - joeynmt.training - Epoch  12, Step:    25600, Batch Loss:     2.664860, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-07-18 12:13:59,996 - INFO - joeynmt.training - Epoch  12, Step:    25700, Batch Loss:     2.713388, Tokens per Sec:    16712, Lr: 0.000300\n",
      "2021-07-18 12:14:13,541 - INFO - joeynmt.training - Epoch  12, Step:    25800, Batch Loss:     2.709301, Tokens per Sec:    16616, Lr: 0.000300\n",
      "2021-07-18 12:14:27,165 - INFO - joeynmt.training - Epoch  12, Step:    25900, Batch Loss:     3.020360, Tokens per Sec:    16684, Lr: 0.000300\n",
      "2021-07-18 12:14:40,428 - INFO - joeynmt.training - Epoch  12, Step:    26000, Batch Loss:     2.667823, Tokens per Sec:    16465, Lr: 0.000300\n",
      "2021-07-18 12:14:53,946 - INFO - joeynmt.training - Epoch  12, Step:    26100, Batch Loss:     2.581939, Tokens per Sec:    16562, Lr: 0.000300\n",
      "2021-07-18 12:15:07,646 - INFO - joeynmt.training - Epoch  12, Step:    26200, Batch Loss:     2.716557, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 12:15:21,377 - INFO - joeynmt.training - Epoch  12, Step:    26300, Batch Loss:     2.791682, Tokens per Sec:    16573, Lr: 0.000300\n",
      "2021-07-18 12:15:34,945 - INFO - joeynmt.training - Epoch  12, Step:    26400, Batch Loss:     2.664400, Tokens per Sec:    16870, Lr: 0.000300\n",
      "2021-07-18 12:15:48,467 - INFO - joeynmt.training - Epoch  12, Step:    26500, Batch Loss:     2.959663, Tokens per Sec:    16736, Lr: 0.000300\n",
      "2021-07-18 12:16:02,044 - INFO - joeynmt.training - Epoch  12, Step:    26600, Batch Loss:     2.373386, Tokens per Sec:    16663, Lr: 0.000300\n",
      "2021-07-18 12:16:15,713 - INFO - joeynmt.training - Epoch  12, Step:    26700, Batch Loss:     2.748428, Tokens per Sec:    16637, Lr: 0.000300\n",
      "2021-07-18 12:16:29,303 - INFO - joeynmt.training - Epoch  12, Step:    26800, Batch Loss:     2.450042, Tokens per Sec:    16729, Lr: 0.000300\n",
      "2021-07-18 12:16:42,975 - INFO - joeynmt.training - Epoch  12, Step:    26900, Batch Loss:     2.689230, Tokens per Sec:    16752, Lr: 0.000300\n",
      "2021-07-18 12:16:56,576 - INFO - joeynmt.training - Epoch  12, Step:    27000, Batch Loss:     2.582199, Tokens per Sec:    17001, Lr: 0.000300\n",
      "2021-07-18 12:17:10,130 - INFO - joeynmt.training - Epoch  12, Step:    27100, Batch Loss:     2.849659, Tokens per Sec:    17011, Lr: 0.000300\n",
      "2021-07-18 12:17:23,803 - INFO - joeynmt.training - Epoch  12, Step:    27200, Batch Loss:     2.827743, Tokens per Sec:    16476, Lr: 0.000300\n",
      "2021-07-18 12:17:37,393 - INFO - joeynmt.training - Epoch  12, Step:    27300, Batch Loss:     2.900068, Tokens per Sec:    16734, Lr: 0.000300\n",
      "2021-07-18 12:17:51,035 - INFO - joeynmt.training - Epoch  12, Step:    27400, Batch Loss:     2.972263, Tokens per Sec:    16619, Lr: 0.000300\n",
      "2021-07-18 12:18:04,665 - INFO - joeynmt.training - Epoch  12, Step:    27500, Batch Loss:     2.434264, Tokens per Sec:    16866, Lr: 0.000300\n",
      "2021-07-18 12:18:18,438 - INFO - joeynmt.training - Epoch  12, Step:    27600, Batch Loss:     2.841126, Tokens per Sec:    16669, Lr: 0.000300\n",
      "2021-07-18 12:18:32,118 - INFO - joeynmt.training - Epoch  12, Step:    27700, Batch Loss:     2.622101, Tokens per Sec:    16227, Lr: 0.000300\n",
      "2021-07-18 12:18:45,907 - INFO - joeynmt.training - Epoch  12, Step:    27800, Batch Loss:     2.859444, Tokens per Sec:    16550, Lr: 0.000300\n",
      "2021-07-18 12:18:56,298 - INFO - joeynmt.training - Epoch  12: total training loss 6327.44\n",
      "2021-07-18 12:18:56,298 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-18 12:19:00,099 - INFO - joeynmt.training - Epoch  13, Step:    27900, Batch Loss:     2.331542, Tokens per Sec:    15015, Lr: 0.000300\n",
      "2021-07-18 12:19:13,891 - INFO - joeynmt.training - Epoch  13, Step:    28000, Batch Loss:     2.738707, Tokens per Sec:    16398, Lr: 0.000300\n",
      "2021-07-18 12:19:27,427 - INFO - joeynmt.training - Epoch  13, Step:    28100, Batch Loss:     2.717178, Tokens per Sec:    16704, Lr: 0.000300\n",
      "2021-07-18 12:19:41,107 - INFO - joeynmt.training - Epoch  13, Step:    28200, Batch Loss:     2.659916, Tokens per Sec:    16802, Lr: 0.000300\n",
      "2021-07-18 12:19:54,846 - INFO - joeynmt.training - Epoch  13, Step:    28300, Batch Loss:     2.390097, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-07-18 12:20:08,635 - INFO - joeynmt.training - Epoch  13, Step:    28400, Batch Loss:     2.115096, Tokens per Sec:    16379, Lr: 0.000300\n",
      "2021-07-18 12:20:22,464 - INFO - joeynmt.training - Epoch  13, Step:    28500, Batch Loss:     2.841001, Tokens per Sec:    16488, Lr: 0.000300\n",
      "2021-07-18 12:20:36,183 - INFO - joeynmt.training - Epoch  13, Step:    28600, Batch Loss:     2.818252, Tokens per Sec:    16534, Lr: 0.000300\n",
      "2021-07-18 12:20:49,708 - INFO - joeynmt.training - Epoch  13, Step:    28700, Batch Loss:     2.778150, Tokens per Sec:    16622, Lr: 0.000300\n",
      "2021-07-18 12:21:03,402 - INFO - joeynmt.training - Epoch  13, Step:    28800, Batch Loss:     2.303812, Tokens per Sec:    16696, Lr: 0.000300\n",
      "2021-07-18 12:21:17,057 - INFO - joeynmt.training - Epoch  13, Step:    28900, Batch Loss:     2.753738, Tokens per Sec:    16390, Lr: 0.000300\n",
      "2021-07-18 12:21:30,634 - INFO - joeynmt.training - Epoch  13, Step:    29000, Batch Loss:     2.525260, Tokens per Sec:    16855, Lr: 0.000300\n",
      "2021-07-18 12:21:44,292 - INFO - joeynmt.training - Epoch  13, Step:    29100, Batch Loss:     2.815651, Tokens per Sec:    16906, Lr: 0.000300\n",
      "2021-07-18 12:21:58,002 - INFO - joeynmt.training - Epoch  13, Step:    29200, Batch Loss:     2.526947, Tokens per Sec:    16910, Lr: 0.000300\n",
      "2021-07-18 12:22:11,549 - INFO - joeynmt.training - Epoch  13, Step:    29300, Batch Loss:     2.975597, Tokens per Sec:    16530, Lr: 0.000300\n",
      "2021-07-18 12:22:25,149 - INFO - joeynmt.training - Epoch  13, Step:    29400, Batch Loss:     2.576011, Tokens per Sec:    16518, Lr: 0.000300\n",
      "2021-07-18 12:22:38,584 - INFO - joeynmt.training - Epoch  13, Step:    29500, Batch Loss:     2.927302, Tokens per Sec:    16702, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/back_transformer_reverse_$tgt3$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWzqmGvLUcB_"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 25000\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/models/lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_{name}_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/back_lhen_reverse_transformer\"', f'model_dir: \"models/back_lhen_reverse_transformer_continued\"').replace(\n",
    "        f'epochs: 30', f'epochs: 17').replace(f'validation_freq: 5000', f'validation_freq: 2500')\n",
    "with open(\"joeynmt/configs/back_transformer_reverse_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eHd1hHFX0Cfi",
    "outputId": "0c68804c-ddb9-4a7c-b992-ea2ee9d30dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer/25000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 17                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/back_lhen_reverse_transformer_continued\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/back_transformer_reverse_lhen_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4HPfdIf0Cfs",
    "outputId": "f295c31a-0e7b-45f1-c89b-fc96f12a061c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 17:10:05,612 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-18 17:10:05,687 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-18 17:10:09,772 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-18 17:10:10,273 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-18 17:10:10,939 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-18 17:10:12,029 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-18 17:10:12,029 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 17:10:12,411 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-18 17:10:12.672249: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-18 17:10:14,453 - INFO - joeynmt.training - Total params: 12138240\n",
      "2021-07-18 17:10:25,188 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer/25000.ckpt\n",
      "2021-07-18 17:10:25,657 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-18 17:10:25,657 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer/25000.ckpt\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.epochs                : 17\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/back_lhen_reverse_transformer_continued\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 207289,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] P@@ il@@ a@@ to nakal@@ ukha itookho , ne nal@@ anga Yesu , namureeba , ari , “ Iwe ni@@ we omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then P@@ il@@ ate ent@@ ered the P@@ ra@@ et@@ or@@ i@@ u@@ m again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.helpers - Number of Src words (types): 4211\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.helpers - Number of Trg words (types): 4211\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4211),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4211))\n",
      "2021-07-18 17:10:25,686 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-18 17:10:25,686 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-18 17:10:39,819 - INFO - joeynmt.training - Epoch   1, Step:    25100, Batch Loss:     2.523359, Tokens per Sec:    16499, Lr: 0.000300\n",
      "2021-07-18 17:10:52,782 - INFO - joeynmt.training - Epoch   1, Step:    25200, Batch Loss:     2.780542, Tokens per Sec:    17454, Lr: 0.000300\n",
      "2021-07-18 17:11:05,735 - INFO - joeynmt.training - Epoch   1, Step:    25300, Batch Loss:     2.421601, Tokens per Sec:    17389, Lr: 0.000300\n",
      "2021-07-18 17:11:18,885 - INFO - joeynmt.training - Epoch   1, Step:    25400, Batch Loss:     2.887430, Tokens per Sec:    17452, Lr: 0.000300\n",
      "2021-07-18 17:11:32,117 - INFO - joeynmt.training - Epoch   1, Step:    25500, Batch Loss:     2.857241, Tokens per Sec:    16993, Lr: 0.000300\n",
      "2021-07-18 17:11:38,727 - INFO - joeynmt.training - Epoch   1: total training loss 1514.16\n",
      "2021-07-18 17:11:38,728 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-18 17:11:45,559 - INFO - joeynmt.training - Epoch   2, Step:    25600, Batch Loss:     2.688238, Tokens per Sec:    16250, Lr: 0.000300\n",
      "2021-07-18 17:11:58,896 - INFO - joeynmt.training - Epoch   2, Step:    25700, Batch Loss:     2.683969, Tokens per Sec:    17139, Lr: 0.000300\n",
      "2021-07-18 17:12:12,272 - INFO - joeynmt.training - Epoch   2, Step:    25800, Batch Loss:     2.694610, Tokens per Sec:    16826, Lr: 0.000300\n",
      "2021-07-18 17:12:25,762 - INFO - joeynmt.training - Epoch   2, Step:    25900, Batch Loss:     2.996311, Tokens per Sec:    16849, Lr: 0.000300\n",
      "2021-07-18 17:12:38,984 - INFO - joeynmt.training - Epoch   2, Step:    26000, Batch Loss:     2.681271, Tokens per Sec:    16516, Lr: 0.000300\n",
      "2021-07-18 17:12:52,402 - INFO - joeynmt.training - Epoch   2, Step:    26100, Batch Loss:     2.552606, Tokens per Sec:    16685, Lr: 0.000300\n",
      "2021-07-18 17:13:06,010 - INFO - joeynmt.training - Epoch   2, Step:    26200, Batch Loss:     2.712375, Tokens per Sec:    16685, Lr: 0.000300\n",
      "2021-07-18 17:13:19,760 - INFO - joeynmt.training - Epoch   2, Step:    26300, Batch Loss:     2.797812, Tokens per Sec:    16550, Lr: 0.000300\n",
      "2021-07-18 17:13:33,382 - INFO - joeynmt.training - Epoch   2, Step:    26400, Batch Loss:     2.701579, Tokens per Sec:    16804, Lr: 0.000300\n",
      "2021-07-18 17:13:46,973 - INFO - joeynmt.training - Epoch   2, Step:    26500, Batch Loss:     2.954422, Tokens per Sec:    16650, Lr: 0.000300\n",
      "2021-07-18 17:14:00,656 - INFO - joeynmt.training - Epoch   2, Step:    26600, Batch Loss:     2.358116, Tokens per Sec:    16532, Lr: 0.000300\n",
      "2021-07-18 17:14:14,496 - INFO - joeynmt.training - Epoch   2, Step:    26700, Batch Loss:     2.731507, Tokens per Sec:    16433, Lr: 0.000300\n",
      "2021-07-18 17:14:28,293 - INFO - joeynmt.training - Epoch   2, Step:    26800, Batch Loss:     2.417904, Tokens per Sec:    16477, Lr: 0.000300\n",
      "2021-07-18 17:14:42,200 - INFO - joeynmt.training - Epoch   2, Step:    26900, Batch Loss:     2.665245, Tokens per Sec:    16470, Lr: 0.000300\n",
      "2021-07-18 17:14:55,961 - INFO - joeynmt.training - Epoch   2, Step:    27000, Batch Loss:     2.607755, Tokens per Sec:    16804, Lr: 0.000300\n",
      "2021-07-18 17:15:09,749 - INFO - joeynmt.training - Epoch   2, Step:    27100, Batch Loss:     2.908663, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-18 17:15:23,583 - INFO - joeynmt.training - Epoch   2, Step:    27200, Batch Loss:     2.831758, Tokens per Sec:    16284, Lr: 0.000300\n",
      "2021-07-18 17:15:37,313 - INFO - joeynmt.training - Epoch   2, Step:    27300, Batch Loss:     2.906028, Tokens per Sec:    16564, Lr: 0.000300\n",
      "2021-07-18 17:15:51,106 - INFO - joeynmt.training - Epoch   2, Step:    27400, Batch Loss:     2.977713, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-18 17:16:05,013 - INFO - joeynmt.training - Epoch   2, Step:    27500, Batch Loss:     2.432045, Tokens per Sec:    16530, Lr: 0.000300\n",
      "2021-07-18 17:16:38,607 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:16:38,608 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:16:38,608 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:16:38,997 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:16:38,997 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:16:39,741 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:16:39,743 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:16:39,743 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:16:39,743 - INFO - joeynmt.training - \tHypothesis: And the Pharisees saw Him , and He saw Him , and said to Him , “ He who was in the midst of the throne , and they came to Him . ”\n",
      "2021-07-18 17:16:39,743 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:16:39,744 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:16:39,744 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:16:39,745 - INFO - joeynmt.training - \tHypothesis: When Martha came to Mary , Mary came to Mary , and said to Him , “ Mary , and the disciples came to Him , and the Lord came to Him . ”\n",
      "2021-07-18 17:16:39,745 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:16:39,745 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:16:39,745 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:16:39,746 - INFO - joeynmt.training - \tHypothesis: He was a great day for the day of the day of the day .\n",
      "2021-07-18 17:16:39,746 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:16:39,747 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:16:39,747 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:16:39,747 - INFO - joeynmt.training - \tHypothesis: And when he was raised , he was raised up , and when he was raised up , and the bread of the bread , and the collector was raised up .\n",
      "2021-07-18 17:16:39,747 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    27500: bleu:   4.84, loss: 96084.9531, ppl:  15.5288, duration: 34.7342s\n",
      "2021-07-18 17:16:53,741 - INFO - joeynmt.training - Epoch   2, Step:    27600, Batch Loss:     2.834100, Tokens per Sec:    16406, Lr: 0.000300\n",
      "2021-07-18 17:17:07,333 - INFO - joeynmt.training - Epoch   2, Step:    27700, Batch Loss:     2.633597, Tokens per Sec:    16332, Lr: 0.000300\n",
      "2021-07-18 17:17:21,238 - INFO - joeynmt.training - Epoch   2, Step:    27800, Batch Loss:     2.893713, Tokens per Sec:    16411, Lr: 0.000300\n",
      "2021-07-18 17:17:31,731 - INFO - joeynmt.training - Epoch   2: total training loss 6326.45\n",
      "2021-07-18 17:17:31,732 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-18 17:17:35,531 - INFO - joeynmt.training - Epoch   3, Step:    27900, Batch Loss:     2.346241, Tokens per Sec:    15021, Lr: 0.000300\n",
      "2021-07-18 17:17:49,161 - INFO - joeynmt.training - Epoch   3, Step:    28000, Batch Loss:     2.763128, Tokens per Sec:    16593, Lr: 0.000300\n",
      "2021-07-18 17:18:02,841 - INFO - joeynmt.training - Epoch   3, Step:    28100, Batch Loss:     2.707178, Tokens per Sec:    16529, Lr: 0.000300\n",
      "2021-07-18 17:18:16,638 - INFO - joeynmt.training - Epoch   3, Step:    28200, Batch Loss:     2.645268, Tokens per Sec:    16659, Lr: 0.000300\n",
      "2021-07-18 17:18:30,329 - INFO - joeynmt.training - Epoch   3, Step:    28300, Batch Loss:     2.393321, Tokens per Sec:    16653, Lr: 0.000300\n",
      "2021-07-18 17:18:44,095 - INFO - joeynmt.training - Epoch   3, Step:    28400, Batch Loss:     2.090425, Tokens per Sec:    16406, Lr: 0.000300\n",
      "2021-07-18 17:18:57,790 - INFO - joeynmt.training - Epoch   3, Step:    28500, Batch Loss:     2.847047, Tokens per Sec:    16648, Lr: 0.000300\n",
      "2021-07-18 17:19:11,628 - INFO - joeynmt.training - Epoch   3, Step:    28600, Batch Loss:     2.780911, Tokens per Sec:    16392, Lr: 0.000300\n",
      "2021-07-18 17:19:25,374 - INFO - joeynmt.training - Epoch   3, Step:    28700, Batch Loss:     2.773043, Tokens per Sec:    16354, Lr: 0.000300\n",
      "2021-07-18 17:19:39,076 - INFO - joeynmt.training - Epoch   3, Step:    28800, Batch Loss:     2.307051, Tokens per Sec:    16688, Lr: 0.000300\n",
      "2021-07-18 17:19:52,681 - INFO - joeynmt.training - Epoch   3, Step:    28900, Batch Loss:     2.753651, Tokens per Sec:    16449, Lr: 0.000300\n",
      "2021-07-18 17:20:06,327 - INFO - joeynmt.training - Epoch   3, Step:    29000, Batch Loss:     2.535723, Tokens per Sec:    16770, Lr: 0.000300\n",
      "2021-07-18 17:20:20,146 - INFO - joeynmt.training - Epoch   3, Step:    29100, Batch Loss:     2.793529, Tokens per Sec:    16710, Lr: 0.000300\n",
      "2021-07-18 17:20:34,052 - INFO - joeynmt.training - Epoch   3, Step:    29200, Batch Loss:     2.532309, Tokens per Sec:    16671, Lr: 0.000300\n",
      "2021-07-18 17:20:47,855 - INFO - joeynmt.training - Epoch   3, Step:    29300, Batch Loss:     2.977476, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-07-18 17:21:01,622 - INFO - joeynmt.training - Epoch   3, Step:    29400, Batch Loss:     2.572039, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-07-18 17:21:15,215 - INFO - joeynmt.training - Epoch   3, Step:    29500, Batch Loss:     2.919913, Tokens per Sec:    16508, Lr: 0.000300\n",
      "2021-07-18 17:21:29,034 - INFO - joeynmt.training - Epoch   3, Step:    29600, Batch Loss:     2.828959, Tokens per Sec:    16420, Lr: 0.000300\n",
      "2021-07-18 17:21:42,710 - INFO - joeynmt.training - Epoch   3, Step:    29700, Batch Loss:     2.764737, Tokens per Sec:    16715, Lr: 0.000300\n",
      "2021-07-18 17:21:56,500 - INFO - joeynmt.training - Epoch   3, Step:    29800, Batch Loss:     2.714650, Tokens per Sec:    16406, Lr: 0.000300\n",
      "2021-07-18 17:22:10,348 - INFO - joeynmt.training - Epoch   3, Step:    29900, Batch Loss:     2.988300, Tokens per Sec:    16398, Lr: 0.000300\n",
      "2021-07-18 17:22:24,243 - INFO - joeynmt.training - Epoch   3, Step:    30000, Batch Loss:     2.770137, Tokens per Sec:    16400, Lr: 0.000300\n",
      "2021-07-18 17:22:49,146 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:22:49,147 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:22:49,147 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:22:49,515 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:22:49,515 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:22:50,690 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:22:50,691 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:22:50,691 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:22:50,692 - INFO - joeynmt.training - \tHypothesis: And the Pharisees and the Pharisees were like to be a source of the Pharisees , saying , “ I have come to you , and I have come to know . ”\n",
      "2021-07-18 17:22:50,692 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:22:50,692 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:22:50,692 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - \tHypothesis: When she was a great crowd , she was a mother , and her mother , and her mother , and her mother , and said to her , “ Mary , and she was a friend . ”\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day .\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - \tHypothesis: And when he was given to the body , he was raised up , and he was raised up , and the body of the body of the body , and the body of the body was raised up .\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    30000: bleu:   5.37, loss: 95258.6016, ppl:  15.1668, duration: 26.4505s\n",
      "2021-07-18 17:23:04,890 - INFO - joeynmt.training - Epoch   3, Step:    30100, Batch Loss:     2.856949, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-07-18 17:23:18,639 - INFO - joeynmt.training - Epoch   3, Step:    30200, Batch Loss:     2.913859, Tokens per Sec:    16117, Lr: 0.000300\n",
      "2021-07-18 17:23:18,925 - INFO - joeynmt.training - Epoch   3: total training loss 6286.32\n",
      "2021-07-18 17:23:18,926 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-18 17:23:32,958 - INFO - joeynmt.training - Epoch   4, Step:    30300, Batch Loss:     2.750962, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-07-18 17:23:46,719 - INFO - joeynmt.training - Epoch   4, Step:    30400, Batch Loss:     2.782564, Tokens per Sec:    16413, Lr: 0.000300\n",
      "2021-07-18 17:24:00,387 - INFO - joeynmt.training - Epoch   4, Step:    30500, Batch Loss:     2.692506, Tokens per Sec:    16456, Lr: 0.000300\n",
      "2021-07-18 17:24:14,122 - INFO - joeynmt.training - Epoch   4, Step:    30600, Batch Loss:     2.312006, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-07-18 17:24:27,988 - INFO - joeynmt.training - Epoch   4, Step:    30700, Batch Loss:     2.553147, Tokens per Sec:    16748, Lr: 0.000300\n",
      "2021-07-18 17:24:41,643 - INFO - joeynmt.training - Epoch   4, Step:    30800, Batch Loss:     2.866960, Tokens per Sec:    16350, Lr: 0.000300\n",
      "2021-07-18 17:24:55,263 - INFO - joeynmt.training - Epoch   4, Step:    30900, Batch Loss:     2.499128, Tokens per Sec:    16717, Lr: 0.000300\n",
      "2021-07-18 17:25:08,943 - INFO - joeynmt.training - Epoch   4, Step:    31000, Batch Loss:     2.807083, Tokens per Sec:    16367, Lr: 0.000300\n",
      "2021-07-18 17:25:22,673 - INFO - joeynmt.training - Epoch   4, Step:    31100, Batch Loss:     2.810971, Tokens per Sec:    16284, Lr: 0.000300\n",
      "2021-07-18 17:25:36,513 - INFO - joeynmt.training - Epoch   4, Step:    31200, Batch Loss:     2.973747, Tokens per Sec:    16685, Lr: 0.000300\n",
      "2021-07-18 17:25:50,300 - INFO - joeynmt.training - Epoch   4, Step:    31300, Batch Loss:     2.585427, Tokens per Sec:    16581, Lr: 0.000300\n",
      "2021-07-18 17:26:04,104 - INFO - joeynmt.training - Epoch   4, Step:    31400, Batch Loss:     2.546406, Tokens per Sec:    16567, Lr: 0.000300\n",
      "2021-07-18 17:26:17,899 - INFO - joeynmt.training - Epoch   4, Step:    31500, Batch Loss:     2.591547, Tokens per Sec:    16446, Lr: 0.000300\n",
      "2021-07-18 17:26:31,500 - INFO - joeynmt.training - Epoch   4, Step:    31600, Batch Loss:     2.925022, Tokens per Sec:    16322, Lr: 0.000300\n",
      "2021-07-18 17:26:45,267 - INFO - joeynmt.training - Epoch   4, Step:    31700, Batch Loss:     2.417161, Tokens per Sec:    16556, Lr: 0.000300\n",
      "2021-07-18 17:26:59,312 - INFO - joeynmt.training - Epoch   4, Step:    31800, Batch Loss:     2.577895, Tokens per Sec:    16431, Lr: 0.000300\n",
      "2021-07-18 17:27:13,029 - INFO - joeynmt.training - Epoch   4, Step:    31900, Batch Loss:     2.618228, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-07-18 17:27:26,790 - INFO - joeynmt.training - Epoch   4, Step:    32000, Batch Loss:     2.297007, Tokens per Sec:    16514, Lr: 0.000300\n",
      "2021-07-18 17:27:40,539 - INFO - joeynmt.training - Epoch   4, Step:    32100, Batch Loss:     2.868172, Tokens per Sec:    16502, Lr: 0.000300\n",
      "2021-07-18 17:27:54,263 - INFO - joeynmt.training - Epoch   4, Step:    32200, Batch Loss:     2.727189, Tokens per Sec:    16467, Lr: 0.000300\n",
      "2021-07-18 17:28:08,181 - INFO - joeynmt.training - Epoch   4, Step:    32300, Batch Loss:     2.945050, Tokens per Sec:    16593, Lr: 0.000300\n",
      "2021-07-18 17:28:21,954 - INFO - joeynmt.training - Epoch   4, Step:    32400, Batch Loss:     2.305427, Tokens per Sec:    16415, Lr: 0.000300\n",
      "2021-07-18 17:28:35,649 - INFO - joeynmt.training - Epoch   4, Step:    32500, Batch Loss:     3.012115, Tokens per Sec:    16832, Lr: 0.000300\n",
      "2021-07-18 17:29:02,534 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:29:02,534 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:29:02,534 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:29:02,902 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:29:02,903 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:29:03,672 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:29:03,673 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:29:03,673 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:29:03,673 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to Him , “ He who has come to Him , and He who has come to me , and I will see the things I have seen . ”\n",
      "2021-07-18 17:29:03,674 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:29:03,674 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:29:03,674 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:29:03,674 - INFO - joeynmt.training - \tHypothesis: And when she was in the synagogue , she was a mother , and said to her , “ Mary , and the daughter of Mary , and the daughter of Mary . ”\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day .\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:29:03,676 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:29:03,676 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:29:03,676 - INFO - joeynmt.training - \tHypothesis: And he had been given up with the body , and when He was raised , he was raised up , and the body of the body of the body of the body of the body , and the body of the body of the body .\n",
      "2021-07-18 17:29:03,676 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    32500: bleu:   5.56, loss: 94901.8281, ppl:  15.0132, duration: 28.0275s\n",
      "2021-07-18 17:29:07,052 - INFO - joeynmt.training - Epoch   4: total training loss 6232.61\n",
      "2021-07-18 17:29:07,053 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-18 17:29:18,167 - INFO - joeynmt.training - Epoch   5, Step:    32600, Batch Loss:     2.944046, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-07-18 17:29:31,873 - INFO - joeynmt.training - Epoch   5, Step:    32700, Batch Loss:     2.533552, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-07-18 17:29:45,506 - INFO - joeynmt.training - Epoch   5, Step:    32800, Batch Loss:     2.795729, Tokens per Sec:    16424, Lr: 0.000300\n",
      "2021-07-18 17:29:59,210 - INFO - joeynmt.training - Epoch   5, Step:    32900, Batch Loss:     2.261023, Tokens per Sec:    16636, Lr: 0.000300\n",
      "2021-07-18 17:30:12,899 - INFO - joeynmt.training - Epoch   5, Step:    33000, Batch Loss:     2.596199, Tokens per Sec:    16731, Lr: 0.000300\n",
      "2021-07-18 17:30:26,557 - INFO - joeynmt.training - Epoch   5, Step:    33100, Batch Loss:     2.732780, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-07-18 17:30:40,264 - INFO - joeynmt.training - Epoch   5, Step:    33200, Batch Loss:     2.810383, Tokens per Sec:    16501, Lr: 0.000300\n",
      "2021-07-18 17:30:54,135 - INFO - joeynmt.training - Epoch   5, Step:    33300, Batch Loss:     2.747925, Tokens per Sec:    16686, Lr: 0.000300\n",
      "2021-07-18 17:31:07,801 - INFO - joeynmt.training - Epoch   5, Step:    33400, Batch Loss:     2.662137, Tokens per Sec:    16383, Lr: 0.000300\n",
      "2021-07-18 17:31:21,493 - INFO - joeynmt.training - Epoch   5, Step:    33500, Batch Loss:     2.661890, Tokens per Sec:    16414, Lr: 0.000300\n",
      "2021-07-18 17:31:35,252 - INFO - joeynmt.training - Epoch   5, Step:    33600, Batch Loss:     2.482535, Tokens per Sec:    16451, Lr: 0.000300\n",
      "2021-07-18 17:31:49,066 - INFO - joeynmt.training - Epoch   5, Step:    33700, Batch Loss:     2.712437, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-18 17:32:02,762 - INFO - joeynmt.training - Epoch   5, Step:    33800, Batch Loss:     2.592180, Tokens per Sec:    16493, Lr: 0.000300\n",
      "2021-07-18 17:32:16,612 - INFO - joeynmt.training - Epoch   5, Step:    33900, Batch Loss:     2.779282, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-07-18 17:32:30,241 - INFO - joeynmt.training - Epoch   5, Step:    34000, Batch Loss:     3.029863, Tokens per Sec:    16519, Lr: 0.000300\n",
      "2021-07-18 17:32:44,059 - INFO - joeynmt.training - Epoch   5, Step:    34100, Batch Loss:     2.783033, Tokens per Sec:    16756, Lr: 0.000300\n",
      "2021-07-18 17:32:57,854 - INFO - joeynmt.training - Epoch   5, Step:    34200, Batch Loss:     2.600207, Tokens per Sec:    16466, Lr: 0.000300\n",
      "2021-07-18 17:33:11,714 - INFO - joeynmt.training - Epoch   5, Step:    34300, Batch Loss:     2.610632, Tokens per Sec:    16652, Lr: 0.000300\n",
      "2021-07-18 17:33:25,550 - INFO - joeynmt.training - Epoch   5, Step:    34400, Batch Loss:     2.670703, Tokens per Sec:    16528, Lr: 0.000300\n",
      "2021-07-18 17:33:39,290 - INFO - joeynmt.training - Epoch   5, Step:    34500, Batch Loss:     2.806657, Tokens per Sec:    16419, Lr: 0.000300\n",
      "2021-07-18 17:33:52,974 - INFO - joeynmt.training - Epoch   5, Step:    34600, Batch Loss:     2.954773, Tokens per Sec:    16447, Lr: 0.000300\n",
      "2021-07-18 17:34:06,660 - INFO - joeynmt.training - Epoch   5, Step:    34700, Batch Loss:     2.423722, Tokens per Sec:    16301, Lr: 0.000300\n",
      "2021-07-18 17:34:20,415 - INFO - joeynmt.training - Epoch   5, Step:    34800, Batch Loss:     2.671597, Tokens per Sec:    16521, Lr: 0.000300\n",
      "2021-07-18 17:34:27,375 - INFO - joeynmt.training - Epoch   5: total training loss 6207.99\n",
      "2021-07-18 17:34:27,376 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-18 17:34:34,478 - INFO - joeynmt.training - Epoch   6, Step:    34900, Batch Loss:     2.813895, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-07-18 17:34:48,257 - INFO - joeynmt.training - Epoch   6, Step:    35000, Batch Loss:     2.470519, Tokens per Sec:    16805, Lr: 0.000300\n",
      "2021-07-18 17:35:16,381 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:35:16,382 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:35:16,382 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:35:17,546 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:35:17,547 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:35:17,547 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:35:17,547 - INFO - joeynmt.training - \tHypothesis: And the Pharisees and the Pharisees were like to be a man , saying , “ Look ! ” And he said to them , “ Whoever is the city of the city , and they are going to see . ”\n",
      "2021-07-18 17:35:17,547 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:35:17,548 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:35:17,548 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:35:17,548 - INFO - joeynmt.training - \tHypothesis: And when she was at the same time , Mary and Mary , Mary , and Mary , and Mary , and Mary , and Mary , and Mary , “ Mary , and the one who is the one . ”\n",
      "2021-07-18 17:35:17,548 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:35:17,549 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:35:17,549 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:35:17,549 - INFO - joeynmt.training - \tHypothesis: He was a great day of the day of the day of the day .\n",
      "2021-07-18 17:35:17,549 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:35:17,550 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:35:17,550 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:35:17,550 - INFO - joeynmt.training - \tHypothesis: Paul also gave up the body , and when he was in the body , he was a brief , and the bridegroom , and the bridegroom were buried .\n",
      "2021-07-18 17:35:17,550 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    35000: bleu:   5.45, loss: 94904.0625, ppl:  15.0141, duration: 29.2925s\n",
      "2021-07-18 17:35:31,368 - INFO - joeynmt.training - Epoch   6, Step:    35100, Batch Loss:     2.621793, Tokens per Sec:    16373, Lr: 0.000300\n",
      "2021-07-18 17:35:44,992 - INFO - joeynmt.training - Epoch   6, Step:    35200, Batch Loss:     2.581375, Tokens per Sec:    16561, Lr: 0.000300\n",
      "2021-07-18 17:35:58,599 - INFO - joeynmt.training - Epoch   6, Step:    35300, Batch Loss:     2.176227, Tokens per Sec:    16408, Lr: 0.000300\n",
      "2021-07-18 17:36:12,403 - INFO - joeynmt.training - Epoch   6, Step:    35400, Batch Loss:     2.638392, Tokens per Sec:    16486, Lr: 0.000300\n",
      "2021-07-18 17:36:26,325 - INFO - joeynmt.training - Epoch   6, Step:    35500, Batch Loss:     2.894516, Tokens per Sec:    16301, Lr: 0.000300\n",
      "2021-07-18 17:36:40,146 - INFO - joeynmt.training - Epoch   6, Step:    35600, Batch Loss:     2.677186, Tokens per Sec:    16515, Lr: 0.000300\n",
      "2021-07-18 17:36:53,959 - INFO - joeynmt.training - Epoch   6, Step:    35700, Batch Loss:     2.672566, Tokens per Sec:    16388, Lr: 0.000300\n",
      "2021-07-18 17:37:07,846 - INFO - joeynmt.training - Epoch   6, Step:    35800, Batch Loss:     2.661989, Tokens per Sec:    16320, Lr: 0.000300\n",
      "2021-07-18 17:37:21,569 - INFO - joeynmt.training - Epoch   6, Step:    35900, Batch Loss:     2.292081, Tokens per Sec:    16579, Lr: 0.000300\n",
      "2021-07-18 17:37:35,338 - INFO - joeynmt.training - Epoch   6, Step:    36000, Batch Loss:     2.334862, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-18 17:37:48,893 - INFO - joeynmt.training - Epoch   6, Step:    36100, Batch Loss:     2.832502, Tokens per Sec:    16469, Lr: 0.000300\n",
      "2021-07-18 17:38:02,779 - INFO - joeynmt.training - Epoch   6, Step:    36200, Batch Loss:     2.808983, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-18 17:38:16,619 - INFO - joeynmt.training - Epoch   6, Step:    36300, Batch Loss:     2.505994, Tokens per Sec:    16506, Lr: 0.000300\n",
      "2021-07-18 17:38:30,403 - INFO - joeynmt.training - Epoch   6, Step:    36400, Batch Loss:     2.886223, Tokens per Sec:    16786, Lr: 0.000300\n",
      "2021-07-18 17:38:44,079 - INFO - joeynmt.training - Epoch   6, Step:    36500, Batch Loss:     2.740465, Tokens per Sec:    16588, Lr: 0.000300\n",
      "2021-07-18 17:38:57,632 - INFO - joeynmt.training - Epoch   6, Step:    36600, Batch Loss:     2.771126, Tokens per Sec:    16353, Lr: 0.000300\n",
      "2021-07-18 17:39:11,409 - INFO - joeynmt.training - Epoch   6, Step:    36700, Batch Loss:     2.829376, Tokens per Sec:    16767, Lr: 0.000300\n",
      "2021-07-18 17:39:25,115 - INFO - joeynmt.training - Epoch   6, Step:    36800, Batch Loss:     2.558002, Tokens per Sec:    16453, Lr: 0.000300\n",
      "2021-07-18 17:39:38,732 - INFO - joeynmt.training - Epoch   6, Step:    36900, Batch Loss:     2.136938, Tokens per Sec:    16580, Lr: 0.000300\n",
      "2021-07-18 17:39:52,340 - INFO - joeynmt.training - Epoch   6, Step:    37000, Batch Loss:     2.699796, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-07-18 17:40:06,069 - INFO - joeynmt.training - Epoch   6, Step:    37100, Batch Loss:     2.671640, Tokens per Sec:    16484, Lr: 0.000300\n",
      "2021-07-18 17:40:16,585 - INFO - joeynmt.training - Epoch   6: total training loss 6172.57\n",
      "2021-07-18 17:40:16,585 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-18 17:40:20,093 - INFO - joeynmt.training - Epoch   7, Step:    37200, Batch Loss:     2.673325, Tokens per Sec:    14684, Lr: 0.000300\n",
      "2021-07-18 17:40:33,867 - INFO - joeynmt.training - Epoch   7, Step:    37300, Batch Loss:     2.837071, Tokens per Sec:    16585, Lr: 0.000300\n",
      "2021-07-18 17:40:47,576 - INFO - joeynmt.training - Epoch   7, Step:    37400, Batch Loss:     2.585966, Tokens per Sec:    16756, Lr: 0.000300\n",
      "2021-07-18 17:41:01,324 - INFO - joeynmt.training - Epoch   7, Step:    37500, Batch Loss:     2.237097, Tokens per Sec:    16455, Lr: 0.000300\n",
      "2021-07-18 17:41:32,381 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:41:32,381 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:41:32,381 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:41:32,732 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:41:32,732 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:41:33,909 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:41:33,910 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:41:33,910 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:41:33,910 - INFO - joeynmt.training - \tHypothesis: And the Pharisees and the Pharisees were like a man , saying , “ It is a great crowd , and the seven hour , and the hour of the wine . ”\n",
      "2021-07-18 17:41:33,911 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:41:33,911 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:41:33,912 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:41:33,912 - INFO - joeynmt.training - \tHypothesis: When she was a great crowd , Mary and Mary , Mary , and Mary , and Mary , and Mary , and Mary , and Mary .\n",
      "2021-07-18 17:41:33,912 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:41:33,914 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:41:33,914 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:41:33,914 - INFO - joeynmt.training - \tHypothesis: He was a great day of the day of the day of the day .\n",
      "2021-07-18 17:41:33,914 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:41:33,915 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:41:33,915 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:41:33,915 - INFO - joeynmt.training - \tHypothesis: And when he had been given to the body , he was sleeping , and the body of the body of the body of the body of the body , and the bread of the wheat of the heat .\n",
      "2021-07-18 17:41:33,915 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    37500: bleu:   6.02, loss: 93161.5859, ppl:  14.2856, duration: 32.5910s\n",
      "2021-07-18 17:41:47,920 - INFO - joeynmt.training - Epoch   7, Step:    37600, Batch Loss:     2.595340, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-07-18 17:42:01,704 - INFO - joeynmt.training - Epoch   7, Step:    37700, Batch Loss:     2.582844, Tokens per Sec:    16442, Lr: 0.000300\n",
      "2021-07-18 17:42:15,595 - INFO - joeynmt.training - Epoch   7, Step:    37800, Batch Loss:     2.547504, Tokens per Sec:    16807, Lr: 0.000300\n",
      "2021-07-18 17:42:29,484 - INFO - joeynmt.training - Epoch   7, Step:    37900, Batch Loss:     2.415132, Tokens per Sec:    16546, Lr: 0.000300\n",
      "2021-07-18 17:42:43,080 - INFO - joeynmt.training - Epoch   7, Step:    38000, Batch Loss:     2.514435, Tokens per Sec:    16496, Lr: 0.000300\n",
      "2021-07-18 17:42:56,812 - INFO - joeynmt.training - Epoch   7, Step:    38100, Batch Loss:     2.835392, Tokens per Sec:    16506, Lr: 0.000300\n",
      "2021-07-18 17:43:10,569 - INFO - joeynmt.training - Epoch   7, Step:    38200, Batch Loss:     2.635521, Tokens per Sec:    16780, Lr: 0.000300\n",
      "2021-07-18 17:43:24,183 - INFO - joeynmt.training - Epoch   7, Step:    38300, Batch Loss:     2.520733, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 17:43:37,924 - INFO - joeynmt.training - Epoch   7, Step:    38400, Batch Loss:     2.424816, Tokens per Sec:    16662, Lr: 0.000300\n",
      "2021-07-18 17:43:51,619 - INFO - joeynmt.training - Epoch   7, Step:    38500, Batch Loss:     2.761295, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-18 17:44:05,382 - INFO - joeynmt.training - Epoch   7, Step:    38600, Batch Loss:     2.669761, Tokens per Sec:    16556, Lr: 0.000300\n",
      "2021-07-18 17:44:19,121 - INFO - joeynmt.training - Epoch   7, Step:    38700, Batch Loss:     2.629753, Tokens per Sec:    16503, Lr: 0.000300\n",
      "2021-07-18 17:44:32,801 - INFO - joeynmt.training - Epoch   7, Step:    38800, Batch Loss:     2.787641, Tokens per Sec:    16601, Lr: 0.000300\n",
      "2021-07-18 17:44:46,454 - INFO - joeynmt.training - Epoch   7, Step:    38900, Batch Loss:     2.205695, Tokens per Sec:    16429, Lr: 0.000300\n",
      "2021-07-18 17:45:00,243 - INFO - joeynmt.training - Epoch   7, Step:    39000, Batch Loss:     2.776600, Tokens per Sec:    16721, Lr: 0.000300\n",
      "2021-07-18 17:45:14,049 - INFO - joeynmt.training - Epoch   7, Step:    39100, Batch Loss:     2.980540, Tokens per Sec:    16287, Lr: 0.000300\n",
      "2021-07-18 17:45:27,739 - INFO - joeynmt.training - Epoch   7, Step:    39200, Batch Loss:     2.885010, Tokens per Sec:    16641, Lr: 0.000300\n",
      "2021-07-18 17:45:41,366 - INFO - joeynmt.training - Epoch   7, Step:    39300, Batch Loss:     2.643274, Tokens per Sec:    16707, Lr: 0.000300\n",
      "2021-07-18 17:45:54,941 - INFO - joeynmt.training - Epoch   7, Step:    39400, Batch Loss:     2.682798, Tokens per Sec:    16528, Lr: 0.000300\n",
      "2021-07-18 17:46:08,474 - INFO - joeynmt.training - Epoch   7: total training loss 6128.12\n",
      "2021-07-18 17:46:08,475 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-18 17:46:09,093 - INFO - joeynmt.training - Epoch   8, Step:    39500, Batch Loss:     2.175182, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-18 17:46:22,852 - INFO - joeynmt.training - Epoch   8, Step:    39600, Batch Loss:     2.617034, Tokens per Sec:    16449, Lr: 0.000300\n",
      "2021-07-18 17:46:36,599 - INFO - joeynmt.training - Epoch   8, Step:    39700, Batch Loss:     2.576166, Tokens per Sec:    16555, Lr: 0.000300\n",
      "2021-07-18 17:46:50,261 - INFO - joeynmt.training - Epoch   8, Step:    39800, Batch Loss:     2.362172, Tokens per Sec:    16393, Lr: 0.000300\n",
      "2021-07-18 17:47:04,073 - INFO - joeynmt.training - Epoch   8, Step:    39900, Batch Loss:     2.690714, Tokens per Sec:    16757, Lr: 0.000300\n",
      "2021-07-18 17:47:17,845 - INFO - joeynmt.training - Epoch   8, Step:    40000, Batch Loss:     2.753013, Tokens per Sec:    16434, Lr: 0.000300\n",
      "2021-07-18 17:47:46,008 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:47:46,009 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:47:46,009 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:47:47,126 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:47:47,127 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:47:47,127 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:47:47,127 - INFO - joeynmt.training - \tHypothesis: And the Pharisees and the Pharisees saw Him , saying , “ I have come to the door , and I have come to them , and I have come to you . ”\n",
      "2021-07-18 17:47:47,128 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:47:47,128 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:47:47,128 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - \tHypothesis: And when she was a great crowd , He went to Mary , and said to Him , “ Mary , and He said to Him , “ Go , and see Him , and you are the one who is coming . ”\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the end .\n",
      "2021-07-18 17:47:47,130 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:47:47,130 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:47:47,130 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:47:47,130 - INFO - joeynmt.training - \tHypothesis: And he had come to Him , and He was like a fire , and when He was raised , He was raised up , and the fire of the sea , and the sea of the sea , and the sea of the sea .\n",
      "2021-07-18 17:47:47,131 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    40000: bleu:   5.96, loss: 93181.2969, ppl:  14.2936, duration: 29.2855s\n",
      "2021-07-18 17:48:01,390 - INFO - joeynmt.training - Epoch   8, Step:    40100, Batch Loss:     2.733800, Tokens per Sec:    16400, Lr: 0.000300\n",
      "2021-07-18 17:48:14,948 - INFO - joeynmt.training - Epoch   8, Step:    40200, Batch Loss:     2.678736, Tokens per Sec:    16571, Lr: 0.000300\n",
      "2021-07-18 17:48:28,561 - INFO - joeynmt.training - Epoch   8, Step:    40300, Batch Loss:     2.639434, Tokens per Sec:    16516, Lr: 0.000300\n",
      "2021-07-18 17:48:42,358 - INFO - joeynmt.training - Epoch   8, Step:    40400, Batch Loss:     2.326212, Tokens per Sec:    16424, Lr: 0.000300\n",
      "2021-07-18 17:48:56,120 - INFO - joeynmt.training - Epoch   8, Step:    40500, Batch Loss:     2.771110, Tokens per Sec:    16498, Lr: 0.000300\n",
      "2021-07-18 17:49:09,896 - INFO - joeynmt.training - Epoch   8, Step:    40600, Batch Loss:     2.640183, Tokens per Sec:    16619, Lr: 0.000300\n",
      "2021-07-18 17:49:23,573 - INFO - joeynmt.training - Epoch   8, Step:    40700, Batch Loss:     2.594287, Tokens per Sec:    16418, Lr: 0.000300\n",
      "2021-07-18 17:49:37,203 - INFO - joeynmt.training - Epoch   8, Step:    40800, Batch Loss:     2.432509, Tokens per Sec:    16766, Lr: 0.000300\n",
      "2021-07-18 17:49:50,886 - INFO - joeynmt.training - Epoch   8, Step:    40900, Batch Loss:     2.735499, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-07-18 17:50:04,575 - INFO - joeynmt.training - Epoch   8, Step:    41000, Batch Loss:     2.402596, Tokens per Sec:    16349, Lr: 0.000300\n",
      "2021-07-18 17:50:18,285 - INFO - joeynmt.training - Epoch   8, Step:    41100, Batch Loss:     2.736159, Tokens per Sec:    16527, Lr: 0.000300\n",
      "2021-07-18 17:50:31,925 - INFO - joeynmt.training - Epoch   8, Step:    41200, Batch Loss:     2.771257, Tokens per Sec:    16551, Lr: 0.000300\n",
      "2021-07-18 17:50:45,497 - INFO - joeynmt.training - Epoch   8, Step:    41300, Batch Loss:     2.426330, Tokens per Sec:    16533, Lr: 0.000300\n",
      "2021-07-18 17:50:59,196 - INFO - joeynmt.training - Epoch   8, Step:    41400, Batch Loss:     2.645169, Tokens per Sec:    16618, Lr: 0.000300\n",
      "2021-07-18 17:51:12,897 - INFO - joeynmt.training - Epoch   8, Step:    41500, Batch Loss:     2.804753, Tokens per Sec:    16455, Lr: 0.000300\n",
      "2021-07-18 17:51:26,697 - INFO - joeynmt.training - Epoch   8, Step:    41600, Batch Loss:     2.717630, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-07-18 17:51:40,407 - INFO - joeynmt.training - Epoch   8, Step:    41700, Batch Loss:     2.764007, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-07-18 17:51:53,918 - INFO - joeynmt.training - Epoch   8, Step:    41800, Batch Loss:     2.286472, Tokens per Sec:    16385, Lr: 0.000300\n",
      "2021-07-18 17:51:57,746 - INFO - joeynmt.training - Epoch   8: total training loss 6125.30\n",
      "2021-07-18 17:51:57,746 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-18 17:52:08,039 - INFO - joeynmt.training - Epoch   9, Step:    41900, Batch Loss:     2.705353, Tokens per Sec:    16249, Lr: 0.000300\n",
      "2021-07-18 17:52:21,691 - INFO - joeynmt.training - Epoch   9, Step:    42000, Batch Loss:     2.455576, Tokens per Sec:    16395, Lr: 0.000300\n",
      "2021-07-18 17:52:35,285 - INFO - joeynmt.training - Epoch   9, Step:    42100, Batch Loss:     2.451493, Tokens per Sec:    16526, Lr: 0.000300\n",
      "2021-07-18 17:52:48,819 - INFO - joeynmt.training - Epoch   9, Step:    42200, Batch Loss:     2.263031, Tokens per Sec:    16391, Lr: 0.000300\n",
      "2021-07-18 17:53:02,484 - INFO - joeynmt.training - Epoch   9, Step:    42300, Batch Loss:     2.000736, Tokens per Sec:    16409, Lr: 0.000300\n",
      "2021-07-18 17:53:16,192 - INFO - joeynmt.training - Epoch   9, Step:    42400, Batch Loss:     2.622065, Tokens per Sec:    16529, Lr: 0.000300\n",
      "2021-07-18 17:53:29,787 - INFO - joeynmt.training - Epoch   9, Step:    42500, Batch Loss:     2.774410, Tokens per Sec:    16408, Lr: 0.000300\n",
      "2021-07-18 17:53:57,273 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:53:57,273 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:53:57,273 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:53:57,651 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:53:57,652 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:53:58,480 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:53:58,483 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:53:58,483 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:53:58,483 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to Him , “ He who has been given me , and He who has given me a great multitude , and I have come to you . ”\n",
      "2021-07-18 17:53:58,484 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:53:58,484 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:53:58,484 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:53:58,484 - INFO - joeynmt.training - \tHypothesis: When Martha came to Him , He went to Mary , and said to Him , “ Mary , and He said to Him , ‘ Mary , and you are the Lord . ”\n",
      "2021-07-18 17:53:58,485 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:53:58,485 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:53:58,485 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:53:58,486 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the end .\n",
      "2021-07-18 17:53:58,486 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:53:58,486 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:53:58,486 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:53:58,487 - INFO - joeynmt.training - \tHypothesis: And when he had been given , He was a sound of fire , and when He was raised , He was raised up , and the sea of the sea , and the sea of the sea .\n",
      "2021-07-18 17:53:58,487 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    42500: bleu:   5.93, loss: 92938.8906, ppl:  14.1951, duration: 28.6991s\n",
      "2021-07-18 17:54:12,847 - INFO - joeynmt.training - Epoch   9, Step:    42600, Batch Loss:     2.598190, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-07-18 17:54:26,558 - INFO - joeynmt.training - Epoch   9, Step:    42700, Batch Loss:     2.579519, Tokens per Sec:    16588, Lr: 0.000300\n",
      "2021-07-18 17:54:40,300 - INFO - joeynmt.training - Epoch   9, Step:    42800, Batch Loss:     2.663406, Tokens per Sec:    16647, Lr: 0.000300\n",
      "2021-07-18 17:54:54,218 - INFO - joeynmt.training - Epoch   9, Step:    42900, Batch Loss:     2.470744, Tokens per Sec:    16686, Lr: 0.000300\n",
      "2021-07-18 17:55:08,005 - INFO - joeynmt.training - Epoch   9, Step:    43000, Batch Loss:     2.596208, Tokens per Sec:    16635, Lr: 0.000300\n",
      "2021-07-18 17:55:21,945 - INFO - joeynmt.training - Epoch   9, Step:    43100, Batch Loss:     2.900149, Tokens per Sec:    16692, Lr: 0.000300\n",
      "2021-07-18 17:55:35,736 - INFO - joeynmt.training - Epoch   9, Step:    43200, Batch Loss:     2.752972, Tokens per Sec:    16462, Lr: 0.000300\n",
      "2021-07-18 17:55:49,343 - INFO - joeynmt.training - Epoch   9, Step:    43300, Batch Loss:     2.649529, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-07-18 17:56:03,221 - INFO - joeynmt.training - Epoch   9, Step:    43400, Batch Loss:     2.572042, Tokens per Sec:    16429, Lr: 0.000300\n",
      "2021-07-18 17:56:17,009 - INFO - joeynmt.training - Epoch   9, Step:    43500, Batch Loss:     2.678832, Tokens per Sec:    16286, Lr: 0.000300\n",
      "2021-07-18 17:56:30,726 - INFO - joeynmt.training - Epoch   9, Step:    43600, Batch Loss:     2.894085, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-07-18 17:56:44,608 - INFO - joeynmt.training - Epoch   9, Step:    43700, Batch Loss:     2.661068, Tokens per Sec:    16577, Lr: 0.000300\n",
      "2021-07-18 17:56:58,336 - INFO - joeynmt.training - Epoch   9, Step:    43800, Batch Loss:     2.676608, Tokens per Sec:    16785, Lr: 0.000300\n",
      "2021-07-18 17:57:12,171 - INFO - joeynmt.training - Epoch   9, Step:    43900, Batch Loss:     2.531440, Tokens per Sec:    16454, Lr: 0.000300\n",
      "2021-07-18 17:57:25,862 - INFO - joeynmt.training - Epoch   9, Step:    44000, Batch Loss:     2.678597, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 17:57:39,535 - INFO - joeynmt.training - Epoch   9, Step:    44100, Batch Loss:     2.498877, Tokens per Sec:    16445, Lr: 0.000300\n",
      "2021-07-18 17:57:46,978 - INFO - joeynmt.training - Epoch   9: total training loss 6082.81\n",
      "2021-07-18 17:57:46,979 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-18 17:57:53,632 - INFO - joeynmt.training - Epoch  10, Step:    44200, Batch Loss:     2.778127, Tokens per Sec:    15654, Lr: 0.000300\n",
      "2021-07-18 17:58:07,352 - INFO - joeynmt.training - Epoch  10, Step:    44300, Batch Loss:     2.623645, Tokens per Sec:    16479, Lr: 0.000300\n",
      "2021-07-18 17:58:21,111 - INFO - joeynmt.training - Epoch  10, Step:    44400, Batch Loss:     2.688815, Tokens per Sec:    16555, Lr: 0.000300\n",
      "2021-07-18 17:58:34,749 - INFO - joeynmt.training - Epoch  10, Step:    44500, Batch Loss:     2.629601, Tokens per Sec:    16432, Lr: 0.000300\n",
      "2021-07-18 17:58:48,488 - INFO - joeynmt.training - Epoch  10, Step:    44600, Batch Loss:     2.822423, Tokens per Sec:    16518, Lr: 0.000300\n",
      "2021-07-18 17:59:02,280 - INFO - joeynmt.training - Epoch  10, Step:    44700, Batch Loss:     2.502395, Tokens per Sec:    16694, Lr: 0.000300\n",
      "2021-07-18 17:59:15,966 - INFO - joeynmt.training - Epoch  10, Step:    44800, Batch Loss:     2.589581, Tokens per Sec:    16589, Lr: 0.000300\n",
      "2021-07-18 17:59:29,652 - INFO - joeynmt.training - Epoch  10, Step:    44900, Batch Loss:     2.362010, Tokens per Sec:    16554, Lr: 0.000300\n",
      "2021-07-18 17:59:43,394 - INFO - joeynmt.training - Epoch  10, Step:    45000, Batch Loss:     2.784768, Tokens per Sec:    16428, Lr: 0.000300\n",
      "2021-07-18 18:00:13,164 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:00:13,164 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:00:13,164 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:00:13,537 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 18:00:13,538 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 18:00:14,284 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:00:14,286 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:00:14,287 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:00:14,287 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw Him as a man , and said , “ He who is in the midst , and said to them , “ I am the Sabbath , and I am going to you . ”\n",
      "2021-07-18 18:00:14,287 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:00:14,287 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:00:14,288 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:00:14,288 - INFO - joeynmt.training - \tHypothesis: Now when she was born , Mary came to Mary , and said to Mary , “ Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary . ”\n",
      "2021-07-18 18:00:14,288 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:00:14,289 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:00:14,289 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:00:14,289 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day .\n",
      "2021-07-18 18:00:14,289 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:00:14,290 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:00:14,290 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:00:14,291 - INFO - joeynmt.training - \tHypothesis: And Paul was with them , and when he was raised , and when he was raised up , he was raised up , and the fire of the sea , and the sea of the sea .\n",
      "2021-07-18 18:00:14,291 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    45000: bleu:   6.21, loss: 92130.0391, ppl:  13.8711, duration: 30.8963s\n",
      "2021-07-18 18:00:28,110 - INFO - joeynmt.training - Epoch  10, Step:    45100, Batch Loss:     2.500345, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-07-18 18:00:41,831 - INFO - joeynmt.training - Epoch  10, Step:    45200, Batch Loss:     2.757017, Tokens per Sec:    16488, Lr: 0.000300\n",
      "2021-07-18 18:00:55,469 - INFO - joeynmt.training - Epoch  10, Step:    45300, Batch Loss:     2.624921, Tokens per Sec:    16549, Lr: 0.000300\n",
      "2021-07-18 18:01:09,276 - INFO - joeynmt.training - Epoch  10, Step:    45400, Batch Loss:     2.784568, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-18 18:01:23,209 - INFO - joeynmt.training - Epoch  10, Step:    45500, Batch Loss:     2.923378, Tokens per Sec:    16632, Lr: 0.000300\n",
      "2021-07-18 18:01:36,982 - INFO - joeynmt.training - Epoch  10, Step:    45600, Batch Loss:     2.686592, Tokens per Sec:    16626, Lr: 0.000300\n",
      "2021-07-18 18:01:50,734 - INFO - joeynmt.training - Epoch  10, Step:    45700, Batch Loss:     2.440629, Tokens per Sec:    16647, Lr: 0.000300\n",
      "2021-07-18 18:02:04,524 - INFO - joeynmt.training - Epoch  10, Step:    45800, Batch Loss:     2.938115, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-18 18:02:18,400 - INFO - joeynmt.training - Epoch  10, Step:    45900, Batch Loss:     2.651367, Tokens per Sec:    16471, Lr: 0.000300\n",
      "2021-07-18 18:02:32,028 - INFO - joeynmt.training - Epoch  10, Step:    46000, Batch Loss:     2.271327, Tokens per Sec:    16716, Lr: 0.000300\n",
      "2021-07-18 18:02:45,701 - INFO - joeynmt.training - Epoch  10, Step:    46100, Batch Loss:     2.513967, Tokens per Sec:    16585, Lr: 0.000300\n",
      "2021-07-18 18:02:59,341 - INFO - joeynmt.training - Epoch  10, Step:    46200, Batch Loss:     2.330808, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-07-18 18:03:13,088 - INFO - joeynmt.training - Epoch  10, Step:    46300, Batch Loss:     2.803338, Tokens per Sec:    16492, Lr: 0.000300\n",
      "2021-07-18 18:03:26,794 - INFO - joeynmt.training - Epoch  10, Step:    46400, Batch Loss:     2.645252, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-18 18:03:37,943 - INFO - joeynmt.training - Epoch  10: total training loss 6065.30\n",
      "2021-07-18 18:03:37,944 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-18 18:03:40,908 - INFO - joeynmt.training - Epoch  11, Step:    46500, Batch Loss:     2.795692, Tokens per Sec:    15231, Lr: 0.000300\n",
      "2021-07-18 18:03:54,437 - INFO - joeynmt.training - Epoch  11, Step:    46600, Batch Loss:     2.435587, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-07-18 18:04:08,218 - INFO - joeynmt.training - Epoch  11, Step:    46700, Batch Loss:     2.482305, Tokens per Sec:    16780, Lr: 0.000300\n",
      "2021-07-18 18:04:22,044 - INFO - joeynmt.training - Epoch  11, Step:    46800, Batch Loss:     2.458282, Tokens per Sec:    16574, Lr: 0.000300\n",
      "2021-07-18 18:04:35,754 - INFO - joeynmt.training - Epoch  11, Step:    46900, Batch Loss:     2.575109, Tokens per Sec:    16683, Lr: 0.000300\n",
      "2021-07-18 18:04:49,589 - INFO - joeynmt.training - Epoch  11, Step:    47000, Batch Loss:     2.745617, Tokens per Sec:    16414, Lr: 0.000300\n",
      "2021-07-18 18:05:03,299 - INFO - joeynmt.training - Epoch  11, Step:    47100, Batch Loss:     2.588981, Tokens per Sec:    16642, Lr: 0.000300\n",
      "2021-07-18 18:05:17,004 - INFO - joeynmt.training - Epoch  11, Step:    47200, Batch Loss:     2.781968, Tokens per Sec:    16505, Lr: 0.000300\n",
      "2021-07-18 18:05:30,702 - INFO - joeynmt.training - Epoch  11, Step:    47300, Batch Loss:     2.501107, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-07-18 18:05:44,493 - INFO - joeynmt.training - Epoch  11, Step:    47400, Batch Loss:     2.411270, Tokens per Sec:    16476, Lr: 0.000300\n",
      "2021-07-18 18:05:58,367 - INFO - joeynmt.training - Epoch  11, Step:    47500, Batch Loss:     2.392524, Tokens per Sec:    16636, Lr: 0.000300\n",
      "2021-07-18 18:06:24,982 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:06:24,983 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:06:24,983 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:06:26,463 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:06:26,464 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:06:26,464 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:06:26,464 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and said to them , “ He who is in the midst of the Pharisees , and said to them , “ I have come to you . ”\n",
      "2021-07-18 18:06:26,464 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - \tHypothesis: And when she had come to Him , He went to Mary , and said to Mary , “ Mary , and the mother of Mary , and He said to Him , “ Teacher , and see Him . ”\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - \tHypothesis: He was a long time for the day of the day of the day .\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:06:26,467 - INFO - joeynmt.training - \tHypothesis: And when he had come to Him , He saw the fire , and when He was raised , He was raised up , and the fire of the head , and the head of the head of the head .\n",
      "2021-07-18 18:06:26,467 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    47500: bleu:   6.14, loss: 92418.3906, ppl:  13.9857, duration: 28.0990s\n",
      "2021-07-18 18:06:40,349 - INFO - joeynmt.training - Epoch  11, Step:    47600, Batch Loss:     2.727108, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-18 18:06:54,041 - INFO - joeynmt.training - Epoch  11, Step:    47700, Batch Loss:     2.617947, Tokens per Sec:    16725, Lr: 0.000300\n",
      "2021-07-18 18:07:07,677 - INFO - joeynmt.training - Epoch  11, Step:    47800, Batch Loss:     2.658020, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-07-18 18:07:21,498 - INFO - joeynmt.training - Epoch  11, Step:    47900, Batch Loss:     2.474666, Tokens per Sec:    16607, Lr: 0.000300\n",
      "2021-07-18 18:07:35,086 - INFO - joeynmt.training - Epoch  11, Step:    48000, Batch Loss:     2.112014, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-07-18 18:07:48,764 - INFO - joeynmt.training - Epoch  11, Step:    48100, Batch Loss:     2.537533, Tokens per Sec:    16807, Lr: 0.000300\n",
      "2021-07-18 18:08:02,504 - INFO - joeynmt.training - Epoch  11, Step:    48200, Batch Loss:     2.678911, Tokens per Sec:    16736, Lr: 0.000300\n",
      "2021-07-18 18:08:16,317 - INFO - joeynmt.training - Epoch  11, Step:    48300, Batch Loss:     2.605086, Tokens per Sec:    16691, Lr: 0.000300\n",
      "2021-07-18 18:08:30,041 - INFO - joeynmt.training - Epoch  11, Step:    48400, Batch Loss:     2.792831, Tokens per Sec:    16428, Lr: 0.000300\n",
      "2021-07-18 18:08:43,826 - INFO - joeynmt.training - Epoch  11, Step:    48500, Batch Loss:     2.638405, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-07-18 18:08:57,612 - INFO - joeynmt.training - Epoch  11, Step:    48600, Batch Loss:     2.449753, Tokens per Sec:    16547, Lr: 0.000300\n",
      "2021-07-18 18:09:11,403 - INFO - joeynmt.training - Epoch  11, Step:    48700, Batch Loss:     2.969850, Tokens per Sec:    16638, Lr: 0.000300\n",
      "2021-07-18 18:09:25,296 - INFO - joeynmt.training - Epoch  11, Step:    48800, Batch Loss:     2.622201, Tokens per Sec:    16548, Lr: 0.000300\n",
      "2021-07-18 18:09:25,705 - INFO - joeynmt.training - Epoch  11: total training loss 6028.67\n",
      "2021-07-18 18:09:25,705 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-18 18:09:39,400 - INFO - joeynmt.training - Epoch  12, Step:    48900, Batch Loss:     2.785305, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-07-18 18:09:53,057 - INFO - joeynmt.training - Epoch  12, Step:    49000, Batch Loss:     2.599651, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-07-18 18:10:06,913 - INFO - joeynmt.training - Epoch  12, Step:    49100, Batch Loss:     2.622269, Tokens per Sec:    16371, Lr: 0.000300\n",
      "2021-07-18 18:10:20,582 - INFO - joeynmt.training - Epoch  12, Step:    49200, Batch Loss:     2.717268, Tokens per Sec:    16535, Lr: 0.000300\n",
      "2021-07-18 18:10:34,181 - INFO - joeynmt.training - Epoch  12, Step:    49300, Batch Loss:     2.590398, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-07-18 18:10:47,978 - INFO - joeynmt.training - Epoch  12, Step:    49400, Batch Loss:     2.324870, Tokens per Sec:    16926, Lr: 0.000300\n",
      "2021-07-18 18:11:01,793 - INFO - joeynmt.training - Epoch  12, Step:    49500, Batch Loss:     2.676963, Tokens per Sec:    16470, Lr: 0.000300\n",
      "2021-07-18 18:11:15,634 - INFO - joeynmt.training - Epoch  12, Step:    49600, Batch Loss:     2.752771, Tokens per Sec:    16606, Lr: 0.000300\n",
      "2021-07-18 18:11:29,549 - INFO - joeynmt.training - Epoch  12, Step:    49700, Batch Loss:     2.839061, Tokens per Sec:    16448, Lr: 0.000300\n",
      "2021-07-18 18:11:43,108 - INFO - joeynmt.training - Epoch  12, Step:    49800, Batch Loss:     2.551483, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-07-18 18:11:56,875 - INFO - joeynmt.training - Epoch  12, Step:    49900, Batch Loss:     2.566016, Tokens per Sec:    16658, Lr: 0.000300\n",
      "2021-07-18 18:12:10,665 - INFO - joeynmt.training - Epoch  12, Step:    50000, Batch Loss:     2.676202, Tokens per Sec:    16484, Lr: 0.000300\n",
      "2021-07-18 18:12:37,195 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:12:37,195 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:12:37,196 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:12:37,570 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 18:12:37,570 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 18:12:38,376 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:12:38,376 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:12:38,377 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:12:38,377 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees said to them , “ He saw the seven sheep , and said to them , “ Where I have come , I have come to you , and I have come to you . ”\n",
      "2021-07-18 18:12:38,377 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:12:38,378 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:12:38,378 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:12:38,378 - INFO - joeynmt.training - \tHypothesis: And when Martha was at the same time , Mary and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary .\n",
      "2021-07-18 18:12:38,378 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:12:38,379 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:12:38,379 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:12:38,379 - INFO - joeynmt.training - \tHypothesis: He was a long time for the day of the day of the end .\n",
      "2021-07-18 18:12:38,379 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:12:38,380 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:12:38,380 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:12:38,380 - INFO - joeynmt.training - \tHypothesis: And Paul also spoke to Him , and when He was sold , He saw the body of the body , and the body of the body of the body , and the sound of the sea .\n",
      "2021-07-18 18:12:38,380 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    50000: bleu:   6.30, loss: 91502.1641, ppl:  13.6247, duration: 27.7151s\n",
      "2021-07-18 18:12:52,435 - INFO - joeynmt.training - Epoch  12, Step:    50100, Batch Loss:     2.770433, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-07-18 18:13:06,023 - INFO - joeynmt.training - Epoch  12, Step:    50200, Batch Loss:     2.662262, Tokens per Sec:    16461, Lr: 0.000300\n",
      "2021-07-18 18:13:19,954 - INFO - joeynmt.training - Epoch  12, Step:    50300, Batch Loss:     2.737151, Tokens per Sec:    16670, Lr: 0.000300\n",
      "2021-07-18 18:13:33,714 - INFO - joeynmt.training - Epoch  12, Step:    50400, Batch Loss:     2.800951, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 18:13:47,511 - INFO - joeynmt.training - Epoch  12, Step:    50500, Batch Loss:     2.855441, Tokens per Sec:    16659, Lr: 0.000300\n",
      "2021-07-18 18:14:01,407 - INFO - joeynmt.training - Epoch  12, Step:    50600, Batch Loss:     2.653716, Tokens per Sec:    16698, Lr: 0.000300\n",
      "2021-07-18 18:14:15,175 - INFO - joeynmt.training - Epoch  12, Step:    50700, Batch Loss:     2.648063, Tokens per Sec:    16399, Lr: 0.000300\n",
      "2021-07-18 18:14:29,126 - INFO - joeynmt.training - Epoch  12, Step:    50800, Batch Loss:     2.561216, Tokens per Sec:    16720, Lr: 0.000300\n",
      "2021-07-18 18:14:42,797 - INFO - joeynmt.training - Epoch  12, Step:    50900, Batch Loss:     2.790103, Tokens per Sec:    16533, Lr: 0.000300\n",
      "2021-07-18 18:14:56,551 - INFO - joeynmt.training - Epoch  12, Step:    51000, Batch Loss:     2.840385, Tokens per Sec:    16522, Lr: 0.000300\n",
      "2021-07-18 18:15:10,299 - INFO - joeynmt.training - Epoch  12, Step:    51100, Batch Loss:     2.875169, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-18 18:15:13,079 - INFO - joeynmt.training - Epoch  12: total training loss 5989.42\n",
      "2021-07-18 18:15:13,080 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-18 18:15:24,416 - INFO - joeynmt.training - Epoch  13, Step:    51200, Batch Loss:     2.612722, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-07-18 18:15:38,033 - INFO - joeynmt.training - Epoch  13, Step:    51300, Batch Loss:     2.575217, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-07-18 18:15:51,879 - INFO - joeynmt.training - Epoch  13, Step:    51400, Batch Loss:     2.389009, Tokens per Sec:    16655, Lr: 0.000300\n",
      "2021-07-18 18:16:05,513 - INFO - joeynmt.training - Epoch  13, Step:    51500, Batch Loss:     2.557447, Tokens per Sec:    16423, Lr: 0.000300\n",
      "2021-07-18 18:16:19,218 - INFO - joeynmt.training - Epoch  13, Step:    51600, Batch Loss:     2.729674, Tokens per Sec:    16354, Lr: 0.000300\n",
      "2021-07-18 18:16:32,824 - INFO - joeynmt.training - Epoch  13, Step:    51700, Batch Loss:     2.382482, Tokens per Sec:    16625, Lr: 0.000300\n",
      "2021-07-18 18:16:46,597 - INFO - joeynmt.training - Epoch  13, Step:    51800, Batch Loss:     2.635111, Tokens per Sec:    16377, Lr: 0.000300\n",
      "2021-07-18 18:17:00,346 - INFO - joeynmt.training - Epoch  13, Step:    51900, Batch Loss:     2.623150, Tokens per Sec:    16696, Lr: 0.000300\n",
      "2021-07-18 18:17:14,113 - INFO - joeynmt.training - Epoch  13, Step:    52000, Batch Loss:     2.507889, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-07-18 18:17:27,843 - INFO - joeynmt.training - Epoch  13, Step:    52100, Batch Loss:     2.554207, Tokens per Sec:    16555, Lr: 0.000300\n",
      "2021-07-18 18:17:41,536 - INFO - joeynmt.training - Epoch  13, Step:    52200, Batch Loss:     2.675241, Tokens per Sec:    16485, Lr: 0.000300\n",
      "2021-07-18 18:17:55,314 - INFO - joeynmt.training - Epoch  13, Step:    52300, Batch Loss:     2.504127, Tokens per Sec:    16543, Lr: 0.000300\n",
      "2021-07-18 18:18:09,044 - INFO - joeynmt.training - Epoch  13, Step:    52400, Batch Loss:     2.679950, Tokens per Sec:    16432, Lr: 0.000300\n",
      "2021-07-18 18:18:22,816 - INFO - joeynmt.training - Epoch  13, Step:    52500, Batch Loss:     2.532991, Tokens per Sec:    16343, Lr: 0.000300\n",
      "2021-07-18 18:18:50,500 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:18:50,500 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:18:50,501 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:18:51,986 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:18:51,986 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:18:51,986 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and said to them , “ He saw the sword , and said to them , “ Look ! And I have come to you , and I have come to you . ”\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - \tHypothesis: And when Martha came to Mary , Mary and Mary and Mary , and Mary , and Mary , and Mary , “ came to Him , and the Lord was coming . ”\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - \tHypothesis: He was a man in the day of the day of the day .\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:18:51,989 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:18:51,989 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:18:51,989 - INFO - joeynmt.training - \tHypothesis: Then he took the steps of the sea , and when He was sleeping , He was sleeping , and the sea of the sea , and the sea of the sea , and the sea of the sea .\n",
      "2021-07-18 18:18:51,989 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    52500: bleu:   6.20, loss: 91796.3359, ppl:  13.7396, duration: 29.1727s\n",
      "2021-07-18 18:19:05,971 - INFO - joeynmt.training - Epoch  13, Step:    52600, Batch Loss:     2.784657, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-07-18 18:19:19,778 - INFO - joeynmt.training - Epoch  13, Step:    52700, Batch Loss:     2.486948, Tokens per Sec:    16558, Lr: 0.000300\n",
      "2021-07-18 18:19:33,421 - INFO - joeynmt.training - Epoch  13, Step:    52800, Batch Loss:     2.356462, Tokens per Sec:    16632, Lr: 0.000300\n",
      "2021-07-18 18:19:47,367 - INFO - joeynmt.training - Epoch  13, Step:    52900, Batch Loss:     2.326673, Tokens per Sec:    16729, Lr: 0.000300\n",
      "2021-07-18 18:20:01,005 - INFO - joeynmt.training - Epoch  13, Step:    53000, Batch Loss:     2.559882, Tokens per Sec:    16575, Lr: 0.000300\n",
      "2021-07-18 18:20:14,771 - INFO - joeynmt.training - Epoch  13, Step:    53100, Batch Loss:     2.751396, Tokens per Sec:    16548, Lr: 0.000300\n",
      "2021-07-18 18:20:28,656 - INFO - joeynmt.training - Epoch  13, Step:    53200, Batch Loss:     2.517473, Tokens per Sec:    16715, Lr: 0.000300\n",
      "2021-07-18 18:20:42,265 - INFO - joeynmt.training - Epoch  13, Step:    53300, Batch Loss:     2.563397, Tokens per Sec:    16545, Lr: 0.000300\n",
      "2021-07-18 18:20:56,173 - INFO - joeynmt.training - Epoch  13, Step:    53400, Batch Loss:     2.775694, Tokens per Sec:    16373, Lr: 0.000300\n",
      "2021-07-18 18:21:02,274 - INFO - joeynmt.training - Epoch  13: total training loss 5991.60\n",
      "2021-07-18 18:21:02,275 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-18 18:21:10,237 - INFO - joeynmt.training - Epoch  14, Step:    53500, Batch Loss:     2.422273, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-07-18 18:21:23,854 - INFO - joeynmt.training - Epoch  14, Step:    53600, Batch Loss:     2.488612, Tokens per Sec:    16565, Lr: 0.000300\n",
      "2021-07-18 18:21:37,572 - INFO - joeynmt.training - Epoch  14, Step:    53700, Batch Loss:     2.782307, Tokens per Sec:    16645, Lr: 0.000300\n",
      "2021-07-18 18:21:51,206 - INFO - joeynmt.training - Epoch  14, Step:    53800, Batch Loss:     2.585469, Tokens per Sec:    16400, Lr: 0.000300\n",
      "2021-07-18 18:22:04,915 - INFO - joeynmt.training - Epoch  14, Step:    53900, Batch Loss:     2.577602, Tokens per Sec:    16351, Lr: 0.000300\n",
      "2021-07-18 18:22:18,554 - INFO - joeynmt.training - Epoch  14, Step:    54000, Batch Loss:     2.635331, Tokens per Sec:    16407, Lr: 0.000300\n",
      "2021-07-18 18:22:32,129 - INFO - joeynmt.training - Epoch  14, Step:    54100, Batch Loss:     2.397973, Tokens per Sec:    16513, Lr: 0.000300\n",
      "2021-07-18 18:22:45,809 - INFO - joeynmt.training - Epoch  14, Step:    54200, Batch Loss:     2.540159, Tokens per Sec:    16557, Lr: 0.000300\n",
      "2021-07-18 18:22:59,667 - INFO - joeynmt.training - Epoch  14, Step:    54300, Batch Loss:     2.418574, Tokens per Sec:    16789, Lr: 0.000300\n",
      "2021-07-18 18:23:13,511 - INFO - joeynmt.training - Epoch  14, Step:    54400, Batch Loss:     2.798739, Tokens per Sec:    16430, Lr: 0.000300\n",
      "2021-07-18 18:23:27,284 - INFO - joeynmt.training - Epoch  14, Step:    54500, Batch Loss:     2.575266, Tokens per Sec:    16448, Lr: 0.000300\n",
      "2021-07-18 18:23:40,927 - INFO - joeynmt.training - Epoch  14, Step:    54600, Batch Loss:     2.630668, Tokens per Sec:    16328, Lr: 0.000300\n",
      "2021-07-18 18:23:54,542 - INFO - joeynmt.training - Epoch  14, Step:    54700, Batch Loss:     2.598145, Tokens per Sec:    16710, Lr: 0.000300\n",
      "2021-07-18 18:24:08,255 - INFO - joeynmt.training - Epoch  14, Step:    54800, Batch Loss:     2.642677, Tokens per Sec:    16531, Lr: 0.000300\n",
      "2021-07-18 18:24:22,092 - INFO - joeynmt.training - Epoch  14, Step:    54900, Batch Loss:     2.535192, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-18 18:24:35,685 - INFO - joeynmt.training - Epoch  14, Step:    55000, Batch Loss:     2.277778, Tokens per Sec:    16481, Lr: 0.000300\n",
      "2021-07-18 18:25:06,135 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:25:06,136 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:25:06,136 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:25:06,524 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 18:25:06,524 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 18:25:07,368 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:25:07,371 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:25:07,371 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:25:07,371 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He who is a man , and he who is in the midst of the seven , and he who is in the midst of the seven times . ”\n",
      "2021-07-18 18:25:07,371 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:25:07,372 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:25:07,372 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:25:07,372 - INFO - joeynmt.training - \tHypothesis: And when Martha was a great crowd , Mary and Mary , Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary .\n",
      "2021-07-18 18:25:07,372 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:25:07,373 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:25:07,373 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:25:07,373 - INFO - joeynmt.training - \tHypothesis: He was a long time for the day of the day of the day of the day .\n",
      "2021-07-18 18:25:07,373 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:25:07,374 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:25:07,374 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:25:07,374 - INFO - joeynmt.training - \tHypothesis: And Paul had spoken , and when He was in the midst of the sea , He was sat down and sat down , and the sea of the sea , and the sea of the sea .\n",
      "2021-07-18 18:25:07,374 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    55000: bleu:   6.39, loss: 90852.1328, ppl:  13.3742, duration: 31.6888s\n",
      "2021-07-18 18:25:21,316 - INFO - joeynmt.training - Epoch  14, Step:    55100, Batch Loss:     2.543422, Tokens per Sec:    16469, Lr: 0.000300\n",
      "2021-07-18 18:25:35,009 - INFO - joeynmt.training - Epoch  14, Step:    55200, Batch Loss:     2.774153, Tokens per Sec:    16491, Lr: 0.000300\n",
      "2021-07-18 18:25:48,822 - INFO - joeynmt.training - Epoch  14, Step:    55300, Batch Loss:     2.810834, Tokens per Sec:    16460, Lr: 0.000300\n",
      "2021-07-18 18:26:02,509 - INFO - joeynmt.training - Epoch  14, Step:    55400, Batch Loss:     2.676061, Tokens per Sec:    16415, Lr: 0.000300\n",
      "2021-07-18 18:26:16,348 - INFO - joeynmt.training - Epoch  14, Step:    55500, Batch Loss:     2.611952, Tokens per Sec:    16608, Lr: 0.000300\n",
      "2021-07-18 18:26:30,127 - INFO - joeynmt.training - Epoch  14, Step:    55600, Batch Loss:     2.501096, Tokens per Sec:    16528, Lr: 0.000300\n",
      "2021-07-18 18:26:43,883 - INFO - joeynmt.training - Epoch  14, Step:    55700, Batch Loss:     2.471441, Tokens per Sec:    16522, Lr: 0.000300\n",
      "2021-07-18 18:26:53,778 - INFO - joeynmt.training - Epoch  14: total training loss 5984.50\n",
      "2021-07-18 18:26:53,779 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-18 18:26:57,930 - INFO - joeynmt.training - Epoch  15, Step:    55800, Batch Loss:     2.687990, Tokens per Sec:    14945, Lr: 0.000300\n",
      "2021-07-18 18:27:11,652 - INFO - joeynmt.training - Epoch  15, Step:    55900, Batch Loss:     2.660452, Tokens per Sec:    16895, Lr: 0.000300\n",
      "2021-07-18 18:27:25,211 - INFO - joeynmt.training - Epoch  15, Step:    56000, Batch Loss:     2.649322, Tokens per Sec:    16414, Lr: 0.000300\n",
      "2021-07-18 18:27:38,893 - INFO - joeynmt.training - Epoch  15, Step:    56100, Batch Loss:     2.627172, Tokens per Sec:    16547, Lr: 0.000300\n",
      "2021-07-18 18:27:52,629 - INFO - joeynmt.training - Epoch  15, Step:    56200, Batch Loss:     2.576288, Tokens per Sec:    16640, Lr: 0.000300\n",
      "2021-07-18 18:28:06,464 - INFO - joeynmt.training - Epoch  15, Step:    56300, Batch Loss:     2.822463, Tokens per Sec:    16392, Lr: 0.000300\n",
      "2021-07-18 18:28:20,062 - INFO - joeynmt.training - Epoch  15, Step:    56400, Batch Loss:     2.715123, Tokens per Sec:    16570, Lr: 0.000300\n",
      "2021-07-18 18:28:33,716 - INFO - joeynmt.training - Epoch  15, Step:    56500, Batch Loss:     2.671602, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-18 18:28:47,419 - INFO - joeynmt.training - Epoch  15, Step:    56600, Batch Loss:     2.770893, Tokens per Sec:    16612, Lr: 0.000300\n",
      "2021-07-18 18:29:01,179 - INFO - joeynmt.training - Epoch  15, Step:    56700, Batch Loss:     2.540199, Tokens per Sec:    16418, Lr: 0.000300\n",
      "2021-07-18 18:29:14,918 - INFO - joeynmt.training - Epoch  15, Step:    56800, Batch Loss:     2.643060, Tokens per Sec:    16299, Lr: 0.000300\n",
      "2021-07-18 18:29:28,591 - INFO - joeynmt.training - Epoch  15, Step:    56900, Batch Loss:     2.112634, Tokens per Sec:    16662, Lr: 0.000300\n",
      "2021-07-18 18:29:42,352 - INFO - joeynmt.training - Epoch  15, Step:    57000, Batch Loss:     2.242307, Tokens per Sec:    16528, Lr: 0.000300\n",
      "2021-07-18 18:29:55,916 - INFO - joeynmt.training - Epoch  15, Step:    57100, Batch Loss:     2.600213, Tokens per Sec:    16541, Lr: 0.000300\n",
      "2021-07-18 18:30:09,762 - INFO - joeynmt.training - Epoch  15, Step:    57200, Batch Loss:     2.434771, Tokens per Sec:    16521, Lr: 0.000300\n",
      "2021-07-18 18:30:23,656 - INFO - joeynmt.training - Epoch  15, Step:    57300, Batch Loss:     2.783355, Tokens per Sec:    16791, Lr: 0.000300\n",
      "2021-07-18 18:30:37,079 - INFO - joeynmt.training - Epoch  15, Step:    57400, Batch Loss:     2.754578, Tokens per Sec:    16304, Lr: 0.000300\n",
      "2021-07-18 18:30:50,836 - INFO - joeynmt.training - Epoch  15, Step:    57500, Batch Loss:     2.416583, Tokens per Sec:    16636, Lr: 0.000300\n",
      "2021-07-18 18:31:19,976 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:31:19,977 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:31:19,977 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:31:21,079 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:31:21,080 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:31:21,080 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:31:21,080 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw Him , and said to them , “ I have come to the city , and I have come to you . ”\n",
      "2021-07-18 18:31:21,080 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:31:21,081 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:31:21,081 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:31:21,081 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary .\n",
      "2021-07-18 18:31:21,081 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:31:21,082 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:31:21,082 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:31:21,082 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the Sabbath .\n",
      "2021-07-18 18:31:21,082 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:31:21,083 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:31:21,083 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:31:21,083 - INFO - joeynmt.training - \tHypothesis: And when Paul had been given up , He was in the body , when He was sat down , He was sat down , and the sound of the sea , and the sound of the sea .\n",
      "2021-07-18 18:31:21,083 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    57500: bleu:   6.50, loss: 90869.3828, ppl:  13.3808, duration: 30.2473s\n",
      "2021-07-18 18:31:34,979 - INFO - joeynmt.training - Epoch  15, Step:    57600, Batch Loss:     2.471746, Tokens per Sec:    16455, Lr: 0.000300\n",
      "2021-07-18 18:31:48,676 - INFO - joeynmt.training - Epoch  15, Step:    57700, Batch Loss:     2.502301, Tokens per Sec:    16775, Lr: 0.000300\n",
      "2021-07-18 18:32:02,441 - INFO - joeynmt.training - Epoch  15, Step:    57800, Batch Loss:     2.517187, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-07-18 18:32:16,158 - INFO - joeynmt.training - Epoch  15, Step:    57900, Batch Loss:     2.449076, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-18 18:32:29,791 - INFO - joeynmt.training - Epoch  15, Step:    58000, Batch Loss:     2.590324, Tokens per Sec:    16446, Lr: 0.000300\n",
      "2021-07-18 18:32:43,268 - INFO - joeynmt.training - Epoch  15, Step:    58100, Batch Loss:     2.510573, Tokens per Sec:    16457, Lr: 0.000300\n",
      "2021-07-18 18:32:43,817 - INFO - joeynmt.training - Epoch  15: total training loss 5975.85\n",
      "2021-07-18 18:32:43,818 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-18 18:32:57,418 - INFO - joeynmt.training - Epoch  16, Step:    58200, Batch Loss:     2.657489, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-07-18 18:33:11,255 - INFO - joeynmt.training - Epoch  16, Step:    58300, Batch Loss:     2.643729, Tokens per Sec:    16641, Lr: 0.000300\n",
      "2021-07-18 18:33:25,013 - INFO - joeynmt.training - Epoch  16, Step:    58400, Batch Loss:     2.564490, Tokens per Sec:    16580, Lr: 0.000300\n",
      "2021-07-18 18:33:38,816 - INFO - joeynmt.training - Epoch  16, Step:    58500, Batch Loss:     2.566830, Tokens per Sec:    16800, Lr: 0.000300\n",
      "2021-07-18 18:33:52,399 - INFO - joeynmt.training - Epoch  16, Step:    58600, Batch Loss:     2.509922, Tokens per Sec:    16526, Lr: 0.000300\n",
      "2021-07-18 18:34:06,181 - INFO - joeynmt.training - Epoch  16, Step:    58700, Batch Loss:     2.606077, Tokens per Sec:    16409, Lr: 0.000300\n",
      "2021-07-18 18:34:20,126 - INFO - joeynmt.training - Epoch  16, Step:    58800, Batch Loss:     2.610657, Tokens per Sec:    16546, Lr: 0.000300\n",
      "2021-07-18 18:34:33,641 - INFO - joeynmt.training - Epoch  16, Step:    58900, Batch Loss:     2.751345, Tokens per Sec:    16452, Lr: 0.000300\n",
      "2021-07-18 18:34:47,399 - INFO - joeynmt.training - Epoch  16, Step:    59000, Batch Loss:     2.247504, Tokens per Sec:    16515, Lr: 0.000300\n",
      "2021-07-18 18:35:01,050 - INFO - joeynmt.training - Epoch  16, Step:    59100, Batch Loss:     2.665253, Tokens per Sec:    16477, Lr: 0.000300\n",
      "2021-07-18 18:35:15,066 - INFO - joeynmt.training - Epoch  16, Step:    59200, Batch Loss:     2.707827, Tokens per Sec:    16566, Lr: 0.000300\n",
      "2021-07-18 18:35:28,746 - INFO - joeynmt.training - Epoch  16, Step:    59300, Batch Loss:     2.691685, Tokens per Sec:    16656, Lr: 0.000300\n",
      "2021-07-18 18:35:42,521 - INFO - joeynmt.training - Epoch  16, Step:    59400, Batch Loss:     2.827472, Tokens per Sec:    16360, Lr: 0.000300\n",
      "2021-07-18 18:35:56,303 - INFO - joeynmt.training - Epoch  16, Step:    59500, Batch Loss:     2.412101, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-07-18 18:36:09,945 - INFO - joeynmt.training - Epoch  16, Step:    59600, Batch Loss:     2.265986, Tokens per Sec:    16551, Lr: 0.000300\n",
      "2021-07-18 18:36:23,589 - INFO - joeynmt.training - Epoch  16, Step:    59700, Batch Loss:     2.730962, Tokens per Sec:    16442, Lr: 0.000300\n",
      "2021-07-18 18:36:37,212 - INFO - joeynmt.training - Epoch  16, Step:    59800, Batch Loss:     2.514449, Tokens per Sec:    16490, Lr: 0.000300\n",
      "2021-07-18 18:36:51,107 - INFO - joeynmt.training - Epoch  16, Step:    59900, Batch Loss:     2.542433, Tokens per Sec:    16676, Lr: 0.000300\n",
      "2021-07-18 18:37:04,941 - INFO - joeynmt.training - Epoch  16, Step:    60000, Batch Loss:     2.831256, Tokens per Sec:    16490, Lr: 0.000300\n",
      "2021-07-18 18:37:31,530 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:37:31,531 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:37:31,531 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:37:32,693 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:37:32,694 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:37:32,694 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:37:32,694 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees said to them , “ I saw the seven sheep , and I saw them , and I went to them . ”\n",
      "2021-07-18 18:37:32,695 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:37:32,695 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:37:32,695 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:37:32,695 - INFO - joeynmt.training - \tHypothesis: Now when Martha had heard the voice of Mary , Mary and Mary , and Mary , and Mary said to Him , “ Take the word of Mary . ”\n",
      "2021-07-18 18:37:32,696 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:37:32,696 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:37:32,696 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:37:32,697 - INFO - joeynmt.training - \tHypothesis: He was a day of service in the days of his day .\n",
      "2021-07-18 18:37:32,697 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:37:32,697 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:37:32,697 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:37:32,698 - INFO - joeynmt.training - \tHypothesis: And Paul had seen the sound of the sea , and when he was raised up , he was raised up , and the sea of the sea , and the bread of the sea .\n",
      "2021-07-18 18:37:32,698 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    60000: bleu:   6.43, loss: 90864.1484, ppl:  13.3788, duration: 27.7558s\n",
      "2021-07-18 18:37:46,729 - INFO - joeynmt.training - Epoch  16, Step:    60100, Batch Loss:     2.424499, Tokens per Sec:    16434, Lr: 0.000300\n",
      "2021-07-18 18:38:00,445 - INFO - joeynmt.training - Epoch  16, Step:    60200, Batch Loss:     2.705121, Tokens per Sec:    16465, Lr: 0.000300\n",
      "2021-07-18 18:38:14,251 - INFO - joeynmt.training - Epoch  16, Step:    60300, Batch Loss:     2.648430, Tokens per Sec:    16428, Lr: 0.000300\n",
      "2021-07-18 18:38:27,991 - INFO - joeynmt.training - Epoch  16, Step:    60400, Batch Loss:     2.422471, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-07-18 18:38:31,647 - INFO - joeynmt.training - Epoch  16: total training loss 5935.66\n",
      "2021-07-18 18:38:31,648 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-18 18:38:42,022 - INFO - joeynmt.training - Epoch  17, Step:    60500, Batch Loss:     2.474560, Tokens per Sec:    16019, Lr: 0.000300\n",
      "2021-07-18 18:38:55,843 - INFO - joeynmt.training - Epoch  17, Step:    60600, Batch Loss:     2.507461, Tokens per Sec:    16636, Lr: 0.000300\n",
      "2021-07-18 18:39:09,625 - INFO - joeynmt.training - Epoch  17, Step:    60700, Batch Loss:     2.501543, Tokens per Sec:    16517, Lr: 0.000300\n",
      "2021-07-18 18:39:23,603 - INFO - joeynmt.training - Epoch  17, Step:    60800, Batch Loss:     2.428998, Tokens per Sec:    16541, Lr: 0.000300\n",
      "2021-07-18 18:39:37,393 - INFO - joeynmt.training - Epoch  17, Step:    60900, Batch Loss:     2.668244, Tokens per Sec:    16667, Lr: 0.000300\n",
      "2021-07-18 18:39:50,954 - INFO - joeynmt.training - Epoch  17, Step:    61000, Batch Loss:     2.384507, Tokens per Sec:    16372, Lr: 0.000300\n",
      "2021-07-18 18:40:04,658 - INFO - joeynmt.training - Epoch  17, Step:    61100, Batch Loss:     2.600641, Tokens per Sec:    16370, Lr: 0.000300\n",
      "2021-07-18 18:40:18,539 - INFO - joeynmt.training - Epoch  17, Step:    61200, Batch Loss:     2.326046, Tokens per Sec:    16293, Lr: 0.000300\n",
      "2021-07-18 18:40:32,179 - INFO - joeynmt.training - Epoch  17, Step:    61300, Batch Loss:     2.433665, Tokens per Sec:    16403, Lr: 0.000300\n",
      "2021-07-18 18:40:45,963 - INFO - joeynmt.training - Epoch  17, Step:    61400, Batch Loss:     2.548888, Tokens per Sec:    16539, Lr: 0.000300\n",
      "2021-07-18 18:40:59,687 - INFO - joeynmt.training - Epoch  17, Step:    61500, Batch Loss:     2.552166, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-07-18 18:41:13,407 - INFO - joeynmt.training - Epoch  17, Step:    61600, Batch Loss:     2.625062, Tokens per Sec:    16365, Lr: 0.000300\n",
      "2021-07-18 18:41:27,020 - INFO - joeynmt.training - Epoch  17, Step:    61700, Batch Loss:     2.514465, Tokens per Sec:    16346, Lr: 0.000300\n",
      "2021-07-18 18:41:40,764 - INFO - joeynmt.training - Epoch  17, Step:    61800, Batch Loss:     2.608452, Tokens per Sec:    16353, Lr: 0.000300\n",
      "2021-07-18 18:41:54,422 - INFO - joeynmt.training - Epoch  17, Step:    61900, Batch Loss:     2.543049, Tokens per Sec:    16389, Lr: 0.000300\n",
      "2021-07-18 18:42:08,097 - INFO - joeynmt.training - Epoch  17, Step:    62000, Batch Loss:     2.810324, Tokens per Sec:    16384, Lr: 0.000300\n",
      "2021-07-18 18:42:21,943 - INFO - joeynmt.training - Epoch  17, Step:    62100, Batch Loss:     2.394086, Tokens per Sec:    16713, Lr: 0.000300\n",
      "2021-07-18 18:42:35,687 - INFO - joeynmt.training - Epoch  17, Step:    62200, Batch Loss:     2.783573, Tokens per Sec:    16510, Lr: 0.000300\n",
      "2021-07-18 18:42:49,397 - INFO - joeynmt.training - Epoch  17, Step:    62300, Batch Loss:     2.393838, Tokens per Sec:    16600, Lr: 0.000300\n",
      "2021-07-18 18:43:03,350 - INFO - joeynmt.training - Epoch  17, Step:    62400, Batch Loss:     2.559532, Tokens per Sec:    16768, Lr: 0.000300\n",
      "2021-07-18 18:43:17,007 - INFO - joeynmt.training - Epoch  17, Step:    62500, Batch Loss:     2.522063, Tokens per Sec:    16640, Lr: 0.000300\n",
      "2021-07-18 18:43:43,876 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:43:43,876 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:43:43,877 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:43:45,037 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:43:45,038 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:43:45,038 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:43:45,038 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like them , saying , “ You have come to the door , and I have come to you , and I have come to you . ”\n",
      "2021-07-18 18:43:45,038 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:43:45,039 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:43:45,039 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:43:45,040 - INFO - joeynmt.training - \tHypothesis: And when Martha was going to see Mary , Mary and Mary , and Mary , and Mary said to Him , “ Mary , and you are going to see the little . ”\n",
      "2021-07-18 18:43:45,040 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:43:45,040 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:43:45,041 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:43:45,041 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a day of day .\n",
      "2021-07-18 18:43:45,041 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:43:45,041 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:43:45,042 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:43:45,042 - INFO - joeynmt.training - \tHypothesis: Paul also looked forward to the body , and when he was raised up , he was raised up , and the lamp of the sea , and the lamp of the sea , and the bread of the sea .\n",
      "2021-07-18 18:43:45,042 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    62500: bleu:   6.36, loss: 91230.0547, ppl:  13.5193, duration: 28.0349s\n",
      "2021-07-18 18:43:59,096 - INFO - joeynmt.training - Epoch  17, Step:    62600, Batch Loss:     2.324243, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-07-18 18:44:12,942 - INFO - joeynmt.training - Epoch  17, Step:    62700, Batch Loss:     2.548611, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-07-18 18:44:20,276 - INFO - joeynmt.training - Epoch  17: total training loss 5926.62\n",
      "2021-07-18 18:44:20,276 - INFO - joeynmt.training - Training ended after  17 epochs.\n",
      "2021-07-18 18:44:20,276 - INFO - joeynmt.training - Best validation result (greedy) at step    55000:  13.37 ppl.\n",
      "2021-07-18 18:44:20,298 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-07-18 18:44:20,675 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 18:44:20,880 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-18 18:44:20,949 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe.en)...\n",
      "2021-07-18 18:44:55,658 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:44:55,659 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:44:55,659 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:44:55,996 - INFO - joeynmt.prediction -  dev bleu[13a]:   7.41 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-18 18:44:56,002 - INFO - joeynmt.prediction - Translations saved to: models/back_lhen_reverse_transformer_continued/00055000.hyps.dev\n",
      "2021-07-18 18:44:56,003 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe.en)...\n",
      "2021-07-18 18:45:30,811 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:45:30,812 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:45:30,812 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:45:31,145 - INFO - joeynmt.prediction - test bleu[13a]:   7.09 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-18 18:45:31,151 - INFO - joeynmt.prediction - Translations saved to: models/back_lhen_reverse_transformer_continued/00055000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/back_transformer_reverse_lhen_reload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYXeiMih2BgB"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 62500\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/models/lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_{name}_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/back_lhen_reverse_transformer\"', f'model_dir: \"models/back_lhen_reverse_transformer_continued2\"').replace(\n",
    "            f'validation_freq: 5000', f'validation_freq: 2500')\n",
    "with open(\"joeynmt/configs/back_transformer_reverse_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BIF6sAgSbaEC",
    "outputId": "77ba415d-5c05-4b30-e944-2e389489e229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued/62500.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/back_lhen_reverse_transformer_continued2\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/back_transformer_reverse_lhen_reload2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGpI-9UTbhnS",
    "outputId": "fa5222f7-3cac-4a92-f45c-a33ea4bea1de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-19 06:34:19,360 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-19 06:34:19,477 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-19 06:34:25,415 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-19 06:34:26,608 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-19 06:34:27,539 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-19 06:34:29,217 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-19 06:34:29,218 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-19 06:34:29,748 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-19 06:34:30.003105: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-19 06:34:32,209 - INFO - joeynmt.training - Total params: 12138240\n",
      "2021-07-19 06:34:40,915 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued/62500.ckpt\n",
      "2021-07-19 06:34:41,505 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-19 06:34:41,505 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-19 06:34:41,505 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-19 06:34:41,506 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\n",
      "2021-07-19 06:34:41,506 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\n",
      "2021-07-19 06:34:41,506 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\n",
      "2021-07-19 06:34:41,506 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-19 06:34:41,507 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-19 06:34:41,507 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-19 06:34:41,507 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-19 06:34:41,507 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-19 06:34:41,508 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-19 06:34:41,508 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-19 06:34:41,509 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued/62500.ckpt\n",
      "2021-07-19 06:34:41,509 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-19 06:34:41,509 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-19 06:34:41,509 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-19 06:34:41,510 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-19 06:34:41,510 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-19 06:34:41,510 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-19 06:34:41,510 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-19 06:34:41,511 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-19 06:34:41,511 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-19 06:34:41,511 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-19 06:34:41,511 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-19 06:34:41,512 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-19 06:34:41,512 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-19 06:34:41,512 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-19 06:34:41,512 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-19 06:34:41,514 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-19 06:34:41,514 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
      "2021-07-19 06:34:41,514 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-19 06:34:41,514 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-19 06:34:41,515 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/back_lhen_reverse_transformer_continued2\n",
      "2021-07-19 06:34:41,515 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-19 06:34:41,515 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-19 06:34:41,515 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-19 06:34:41,517 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-19 06:34:41,517 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-19 06:34:41,517 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-19 06:34:41,517 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-19 06:34:41,518 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-19 06:34:41,518 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-19 06:34:41,518 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-19 06:34:41,518 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-19 06:34:41,519 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-19 06:34:41,519 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-19 06:34:41,519 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-19 06:34:41,519 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-19 06:34:41,520 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-19 06:34:41,520 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-19 06:34:41,520 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-19 06:34:41,520 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-19 06:34:41,521 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-19 06:34:41,521 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-19 06:34:41,521 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-19 06:34:41,521 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-19 06:34:41,522 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-19 06:34:41,522 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-19 06:34:41,522 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-19 06:34:41,522 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 207289,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-19 06:34:41,523 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] P@@ il@@ a@@ to nakal@@ ukha itookho , ne nal@@ anga Yesu , namureeba , ari , “ Iwe ni@@ we omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then P@@ il@@ ate ent@@ ered the P@@ ra@@ et@@ or@@ i@@ u@@ m again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-19 06:34:41,523 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-19 06:34:41,523 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-19 06:34:41,523 - INFO - joeynmt.helpers - Number of Src words (types): 4211\n",
      "2021-07-19 06:34:41,524 - INFO - joeynmt.helpers - Number of Trg words (types): 4211\n",
      "2021-07-19 06:34:41,524 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4211),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4211))\n",
      "2021-07-19 06:34:41,542 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-19 06:34:41,542 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-19 06:35:12,220 - INFO - joeynmt.training - Epoch   1, Step:    62600, Batch Loss:     2.346399, Tokens per Sec:     7402, Lr: 0.000300\n",
      "2021-07-19 06:35:41,937 - INFO - joeynmt.training - Epoch   1, Step:    62700, Batch Loss:     2.545072, Tokens per Sec:     7717, Lr: 0.000300\n",
      "2021-07-19 06:35:57,717 - INFO - joeynmt.training - Epoch   1: total training loss 646.74\n",
      "2021-07-19 06:35:57,718 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-19 06:36:12,046 - INFO - joeynmt.training - Epoch   2, Step:    62800, Batch Loss:     2.736987, Tokens per Sec:     7515, Lr: 0.000300\n",
      "2021-07-19 06:36:41,671 - INFO - joeynmt.training - Epoch   2, Step:    62900, Batch Loss:     2.440602, Tokens per Sec:     7804, Lr: 0.000300\n",
      "2021-07-19 06:37:11,165 - INFO - joeynmt.training - Epoch   2, Step:    63000, Batch Loss:     2.554001, Tokens per Sec:     7750, Lr: 0.000300\n",
      "2021-07-19 06:37:40,753 - INFO - joeynmt.training - Epoch   2, Step:    63100, Batch Loss:     2.708537, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 06:38:10,212 - INFO - joeynmt.training - Epoch   2, Step:    63200, Batch Loss:     2.634101, Tokens per Sec:     7593, Lr: 0.000300\n",
      "2021-07-19 06:38:39,464 - INFO - joeynmt.training - Epoch   2, Step:    63300, Batch Loss:     2.772285, Tokens per Sec:     7595, Lr: 0.000300\n",
      "2021-07-19 06:39:08,917 - INFO - joeynmt.training - Epoch   2, Step:    63400, Batch Loss:     2.618553, Tokens per Sec:     7693, Lr: 0.000300\n",
      "2021-07-19 06:39:38,748 - INFO - joeynmt.training - Epoch   2, Step:    63500, Batch Loss:     2.428201, Tokens per Sec:     7764, Lr: 0.000300\n",
      "2021-07-19 06:40:08,238 - INFO - joeynmt.training - Epoch   2, Step:    63600, Batch Loss:     2.755451, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 06:40:37,936 - INFO - joeynmt.training - Epoch   2, Step:    63700, Batch Loss:     2.286687, Tokens per Sec:     7721, Lr: 0.000300\n",
      "2021-07-19 06:41:07,045 - INFO - joeynmt.training - Epoch   2, Step:    63800, Batch Loss:     2.797885, Tokens per Sec:     7587, Lr: 0.000300\n",
      "2021-07-19 06:41:36,709 - INFO - joeynmt.training - Epoch   2, Step:    63900, Batch Loss:     2.845188, Tokens per Sec:     7792, Lr: 0.000300\n",
      "2021-07-19 06:42:06,384 - INFO - joeynmt.training - Epoch   2, Step:    64000, Batch Loss:     2.760698, Tokens per Sec:     7731, Lr: 0.000300\n",
      "2021-07-19 06:42:36,091 - INFO - joeynmt.training - Epoch   2, Step:    64100, Batch Loss:     2.663007, Tokens per Sec:     7784, Lr: 0.000300\n",
      "2021-07-19 06:43:05,565 - INFO - joeynmt.training - Epoch   2, Step:    64200, Batch Loss:     2.447559, Tokens per Sec:     7563, Lr: 0.000300\n",
      "2021-07-19 06:43:35,073 - INFO - joeynmt.training - Epoch   2, Step:    64300, Batch Loss:     1.854980, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 06:44:04,268 - INFO - joeynmt.training - Epoch   2, Step:    64400, Batch Loss:     1.873276, Tokens per Sec:     7631, Lr: 0.000300\n",
      "2021-07-19 06:44:34,021 - INFO - joeynmt.training - Epoch   2, Step:    64500, Batch Loss:     2.559284, Tokens per Sec:     7707, Lr: 0.000300\n",
      "2021-07-19 06:45:03,777 - INFO - joeynmt.training - Epoch   2, Step:    64600, Batch Loss:     2.497647, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 06:45:33,402 - INFO - joeynmt.training - Epoch   2, Step:    64700, Batch Loss:     2.295540, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 06:46:02,968 - INFO - joeynmt.training - Epoch   2, Step:    64800, Batch Loss:     2.707605, Tokens per Sec:     7724, Lr: 0.000300\n",
      "2021-07-19 06:46:32,506 - INFO - joeynmt.training - Epoch   2, Step:    64900, Batch Loss:     2.602335, Tokens per Sec:     7694, Lr: 0.000300\n",
      "2021-07-19 06:47:01,836 - INFO - joeynmt.training - Epoch   2, Step:    65000, Batch Loss:     2.541043, Tokens per Sec:     7598, Lr: 0.000300\n",
      "2021-07-19 06:48:01,459 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 06:48:01,460 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 06:48:01,460 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 06:48:01,889 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 06:48:01,890 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 06:48:02,755 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 06:48:02,757 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 06:48:02,757 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 06:48:02,757 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a man , and said to them , “ You have come to the door , and I have come to you . ”\n",
      "2021-07-19 06:48:02,757 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 06:48:02,758 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 06:48:02,758 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 06:48:02,759 - INFO - joeynmt.training - \tHypothesis: Now Martha and Martha , and the sister came to Mary , and said to Him , “ Teacher , and the Lord , and we are with Him . ”\n",
      "2021-07-19 06:48:02,759 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 06:48:02,760 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 06:48:02,760 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 06:48:02,760 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and the day of the day was near .\n",
      "2021-07-19 06:48:02,760 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 06:48:02,761 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 06:48:02,761 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 06:48:02,761 - INFO - joeynmt.training - \tHypothesis: And when he had come , He saw the body , and saw Him , and the fire of the body , and the fire of the fire , and the fire of the sea , and the right hand of the right hand .\n",
      "2021-07-19 06:48:02,762 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    65000: bleu:   6.62, loss: 90258.0391, ppl:  13.1493, duration: 60.9247s\n",
      "2021-07-19 06:48:25,778 - INFO - joeynmt.training - Epoch   2: total training loss 5908.06\n",
      "2021-07-19 06:48:25,779 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-19 06:48:32,585 - INFO - joeynmt.training - Epoch   3, Step:    65100, Batch Loss:     2.414836, Tokens per Sec:     6814, Lr: 0.000300\n",
      "2021-07-19 06:49:02,227 - INFO - joeynmt.training - Epoch   3, Step:    65200, Batch Loss:     2.787891, Tokens per Sec:     7788, Lr: 0.000300\n",
      "2021-07-19 06:49:31,924 - INFO - joeynmt.training - Epoch   3, Step:    65300, Batch Loss:     2.723508, Tokens per Sec:     7669, Lr: 0.000300\n",
      "2021-07-19 06:50:01,471 - INFO - joeynmt.training - Epoch   3, Step:    65400, Batch Loss:     2.725967, Tokens per Sec:     7685, Lr: 0.000300\n",
      "2021-07-19 06:50:31,090 - INFO - joeynmt.training - Epoch   3, Step:    65500, Batch Loss:     2.502720, Tokens per Sec:     7718, Lr: 0.000300\n",
      "2021-07-19 06:51:00,736 - INFO - joeynmt.training - Epoch   3, Step:    65600, Batch Loss:     2.525380, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 06:51:30,352 - INFO - joeynmt.training - Epoch   3, Step:    65700, Batch Loss:     2.577008, Tokens per Sec:     7679, Lr: 0.000300\n",
      "2021-07-19 06:51:59,963 - INFO - joeynmt.training - Epoch   3, Step:    65800, Batch Loss:     2.718289, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 06:52:29,607 - INFO - joeynmt.training - Epoch   3, Step:    65900, Batch Loss:     2.071284, Tokens per Sec:     7709, Lr: 0.000300\n",
      "2021-07-19 06:52:59,228 - INFO - joeynmt.training - Epoch   3, Step:    66000, Batch Loss:     2.519513, Tokens per Sec:     7697, Lr: 0.000300\n",
      "2021-07-19 06:53:28,862 - INFO - joeynmt.training - Epoch   3, Step:    66100, Batch Loss:     2.458498, Tokens per Sec:     7688, Lr: 0.000300\n",
      "2021-07-19 06:53:58,543 - INFO - joeynmt.training - Epoch   3, Step:    66200, Batch Loss:     2.184022, Tokens per Sec:     7692, Lr: 0.000300\n",
      "2021-07-19 06:54:28,103 - INFO - joeynmt.training - Epoch   3, Step:    66300, Batch Loss:     2.688070, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 06:54:57,480 - INFO - joeynmt.training - Epoch   3, Step:    66400, Batch Loss:     2.234920, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 06:55:27,219 - INFO - joeynmt.training - Epoch   3, Step:    66500, Batch Loss:     2.315522, Tokens per Sec:     7723, Lr: 0.000300\n",
      "2021-07-19 06:55:56,312 - INFO - joeynmt.training - Epoch   3, Step:    66600, Batch Loss:     2.282195, Tokens per Sec:     7579, Lr: 0.000300\n",
      "2021-07-19 06:56:25,984 - INFO - joeynmt.training - Epoch   3, Step:    66700, Batch Loss:     2.451663, Tokens per Sec:     7750, Lr: 0.000300\n",
      "2021-07-19 06:56:55,707 - INFO - joeynmt.training - Epoch   3, Step:    66800, Batch Loss:     2.601985, Tokens per Sec:     7713, Lr: 0.000300\n",
      "2021-07-19 06:57:25,465 - INFO - joeynmt.training - Epoch   3, Step:    66900, Batch Loss:     2.572580, Tokens per Sec:     7715, Lr: 0.000300\n",
      "2021-07-19 06:57:55,123 - INFO - joeynmt.training - Epoch   3, Step:    67000, Batch Loss:     2.611930, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 06:58:24,636 - INFO - joeynmt.training - Epoch   3, Step:    67100, Batch Loss:     2.572382, Tokens per Sec:     7730, Lr: 0.000300\n",
      "2021-07-19 06:58:54,293 - INFO - joeynmt.training - Epoch   3, Step:    67200, Batch Loss:     2.570858, Tokens per Sec:     7649, Lr: 0.000300\n",
      "2021-07-19 06:59:23,959 - INFO - joeynmt.training - Epoch   3, Step:    67300, Batch Loss:     2.707041, Tokens per Sec:     7891, Lr: 0.000300\n",
      "2021-07-19 06:59:51,786 - INFO - joeynmt.training - Epoch   3: total training loss 5872.46\n",
      "2021-07-19 06:59:51,786 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-19 06:59:53,917 - INFO - joeynmt.training - Epoch   4, Step:    67400, Batch Loss:     2.783792, Tokens per Sec:     6737, Lr: 0.000300\n",
      "2021-07-19 07:00:23,359 - INFO - joeynmt.training - Epoch   4, Step:    67500, Batch Loss:     2.699785, Tokens per Sec:     7682, Lr: 0.000300\n",
      "2021-07-19 07:01:14,579 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:01:14,579 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:01:14,579 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:01:15,843 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:01:15,844 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:01:15,844 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:01:15,844 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees , and He said to them , “ Where I am , I am going to see , and I am going to see . ”\n",
      "2021-07-19 07:01:15,844 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:01:15,845 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:01:15,845 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:01:15,846 - INFO - joeynmt.training - \tHypothesis: And Martha , who had heard the voice of Mary , and said to Mary , “ Teacher , and the Lord , and said to Him , “ Take Him , and do not know Him . ”\n",
      "2021-07-19 07:01:15,846 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:01:15,846 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:01:15,847 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:01:15,847 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a great day .\n",
      "2021-07-19 07:01:15,847 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:01:15,848 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:01:15,848 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:01:15,848 - INFO - joeynmt.training - \tHypothesis: And Paul looked for Him , and when He had been raised , and when He was raised up , He was raised up , and the fire , and the fire of the head .\n",
      "2021-07-19 07:01:15,849 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    67500: bleu:   6.42, loss: 90324.8984, ppl:  13.1745, duration: 52.4884s\n",
      "2021-07-19 07:01:45,942 - INFO - joeynmt.training - Epoch   4, Step:    67600, Batch Loss:     2.727720, Tokens per Sec:     7724, Lr: 0.000300\n",
      "2021-07-19 07:02:15,195 - INFO - joeynmt.training - Epoch   4, Step:    67700, Batch Loss:     2.616213, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 07:02:44,654 - INFO - joeynmt.training - Epoch   4, Step:    67800, Batch Loss:     2.429446, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 07:03:14,584 - INFO - joeynmt.training - Epoch   4, Step:    67900, Batch Loss:     2.511340, Tokens per Sec:     7714, Lr: 0.000300\n",
      "2021-07-19 07:03:44,242 - INFO - joeynmt.training - Epoch   4, Step:    68000, Batch Loss:     2.827675, Tokens per Sec:     7798, Lr: 0.000300\n",
      "2021-07-19 07:04:13,603 - INFO - joeynmt.training - Epoch   4, Step:    68100, Batch Loss:     2.527565, Tokens per Sec:     7614, Lr: 0.000300\n",
      "2021-07-19 07:04:42,962 - INFO - joeynmt.training - Epoch   4, Step:    68200, Batch Loss:     2.602662, Tokens per Sec:     7629, Lr: 0.000300\n",
      "2021-07-19 07:05:12,289 - INFO - joeynmt.training - Epoch   4, Step:    68300, Batch Loss:     2.716846, Tokens per Sec:     7640, Lr: 0.000300\n",
      "2021-07-19 07:05:41,962 - INFO - joeynmt.training - Epoch   4, Step:    68400, Batch Loss:     2.713373, Tokens per Sec:     7772, Lr: 0.000300\n",
      "2021-07-19 07:06:11,577 - INFO - joeynmt.training - Epoch   4, Step:    68500, Batch Loss:     2.316921, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 07:06:41,006 - INFO - joeynmt.training - Epoch   4, Step:    68600, Batch Loss:     2.707377, Tokens per Sec:     7554, Lr: 0.000300\n",
      "2021-07-19 07:07:10,835 - INFO - joeynmt.training - Epoch   4, Step:    68700, Batch Loss:     2.214130, Tokens per Sec:     7773, Lr: 0.000300\n",
      "2021-07-19 07:07:40,259 - INFO - joeynmt.training - Epoch   4, Step:    68800, Batch Loss:     2.592070, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-19 07:08:09,971 - INFO - joeynmt.training - Epoch   4, Step:    68900, Batch Loss:     2.511361, Tokens per Sec:     7728, Lr: 0.000300\n",
      "2021-07-19 07:08:39,435 - INFO - joeynmt.training - Epoch   4, Step:    69000, Batch Loss:     2.558335, Tokens per Sec:     7626, Lr: 0.000300\n",
      "2021-07-19 07:09:09,023 - INFO - joeynmt.training - Epoch   4, Step:    69100, Batch Loss:     2.652383, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 07:09:38,627 - INFO - joeynmt.training - Epoch   4, Step:    69200, Batch Loss:     2.504856, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 07:10:08,289 - INFO - joeynmt.training - Epoch   4, Step:    69300, Batch Loss:     2.429661, Tokens per Sec:     7778, Lr: 0.000300\n",
      "2021-07-19 07:10:37,723 - INFO - joeynmt.training - Epoch   4, Step:    69400, Batch Loss:     2.292341, Tokens per Sec:     7695, Lr: 0.000300\n",
      "2021-07-19 07:11:06,970 - INFO - joeynmt.training - Epoch   4, Step:    69500, Batch Loss:     2.741869, Tokens per Sec:     7604, Lr: 0.000300\n",
      "2021-07-19 07:11:36,377 - INFO - joeynmt.training - Epoch   4, Step:    69600, Batch Loss:     2.655686, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 07:12:05,858 - INFO - joeynmt.training - Epoch   4, Step:    69700, Batch Loss:     2.667983, Tokens per Sec:     7740, Lr: 0.000300\n",
      "2021-07-19 07:12:11,212 - INFO - joeynmt.training - Epoch   4: total training loss 5879.79\n",
      "2021-07-19 07:12:11,212 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-19 07:12:35,806 - INFO - joeynmt.training - Epoch   5, Step:    69800, Batch Loss:     2.424563, Tokens per Sec:     7654, Lr: 0.000300\n",
      "2021-07-19 07:13:05,314 - INFO - joeynmt.training - Epoch   5, Step:    69900, Batch Loss:     2.599828, Tokens per Sec:     7712, Lr: 0.000300\n",
      "2021-07-19 07:13:34,943 - INFO - joeynmt.training - Epoch   5, Step:    70000, Batch Loss:     2.418897, Tokens per Sec:     7737, Lr: 0.000300\n",
      "2021-07-19 07:14:33,189 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:14:33,189 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:14:33,190 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:14:33,626 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 07:14:33,627 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 07:14:34,558 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:14:34,560 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:14:34,560 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:14:34,560 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He who is in the midst of the scribes and said to them , “ I have come to you . ”\n",
      "2021-07-19 07:14:34,560 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:14:34,561 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:14:34,561 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:14:34,561 - INFO - joeynmt.training - \tHypothesis: When Martha was a man , she came to Mary , and said to her , “ Mary , and she said to her , “ Teacher , and she was here . ”\n",
      "2021-07-19 07:14:34,562 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:14:34,563 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:14:34,563 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:14:34,563 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a great day .\n",
      "2021-07-19 07:14:34,563 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:14:34,564 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:14:34,564 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:14:34,565 - INFO - joeynmt.training - \tHypothesis: And Paul did not have the body of the body , and when He was raised up , He was sleeping , and the sea of the sea , and the sea was set down .\n",
      "2021-07-19 07:14:34,565 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    70000: bleu:   6.69, loss: 90115.6875, ppl:  13.0960, duration: 59.6215s\n",
      "2021-07-19 07:15:04,460 - INFO - joeynmt.training - Epoch   5, Step:    70100, Batch Loss:     2.299713, Tokens per Sec:     7646, Lr: 0.000300\n",
      "2021-07-19 07:15:33,702 - INFO - joeynmt.training - Epoch   5, Step:    70200, Batch Loss:     2.013454, Tokens per Sec:     7669, Lr: 0.000300\n",
      "2021-07-19 07:16:02,721 - INFO - joeynmt.training - Epoch   5, Step:    70300, Batch Loss:     2.489102, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 07:16:32,326 - INFO - joeynmt.training - Epoch   5, Step:    70400, Batch Loss:     2.679864, Tokens per Sec:     7612, Lr: 0.000300\n",
      "2021-07-19 07:17:01,819 - INFO - joeynmt.training - Epoch   5, Step:    70500, Batch Loss:     2.736821, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 07:17:31,295 - INFO - joeynmt.training - Epoch   5, Step:    70600, Batch Loss:     2.654853, Tokens per Sec:     7663, Lr: 0.000300\n",
      "2021-07-19 07:18:00,914 - INFO - joeynmt.training - Epoch   5, Step:    70700, Batch Loss:     2.487808, Tokens per Sec:     7718, Lr: 0.000300\n",
      "2021-07-19 07:18:30,703 - INFO - joeynmt.training - Epoch   5, Step:    70800, Batch Loss:     2.596038, Tokens per Sec:     7706, Lr: 0.000300\n",
      "2021-07-19 07:19:00,328 - INFO - joeynmt.training - Epoch   5, Step:    70900, Batch Loss:     2.702940, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 07:19:29,767 - INFO - joeynmt.training - Epoch   5, Step:    71000, Batch Loss:     2.543967, Tokens per Sec:     7591, Lr: 0.000300\n",
      "2021-07-19 07:19:59,298 - INFO - joeynmt.training - Epoch   5, Step:    71100, Batch Loss:     2.449262, Tokens per Sec:     7717, Lr: 0.000300\n",
      "2021-07-19 07:20:28,935 - INFO - joeynmt.training - Epoch   5, Step:    71200, Batch Loss:     2.605769, Tokens per Sec:     7726, Lr: 0.000300\n",
      "2021-07-19 07:20:58,534 - INFO - joeynmt.training - Epoch   5, Step:    71300, Batch Loss:     2.602383, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 07:21:28,073 - INFO - joeynmt.training - Epoch   5, Step:    71400, Batch Loss:     2.833060, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-19 07:21:57,777 - INFO - joeynmt.training - Epoch   5, Step:    71500, Batch Loss:     1.846742, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 07:22:27,223 - INFO - joeynmt.training - Epoch   5, Step:    71600, Batch Loss:     2.568074, Tokens per Sec:     7581, Lr: 0.000300\n",
      "2021-07-19 07:22:56,512 - INFO - joeynmt.training - Epoch   5, Step:    71700, Batch Loss:     2.840994, Tokens per Sec:     7624, Lr: 0.000300\n",
      "2021-07-19 07:23:25,914 - INFO - joeynmt.training - Epoch   5, Step:    71800, Batch Loss:     2.323910, Tokens per Sec:     7593, Lr: 0.000300\n",
      "2021-07-19 07:23:55,611 - INFO - joeynmt.training - Epoch   5, Step:    71900, Batch Loss:     2.335999, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 07:24:24,961 - INFO - joeynmt.training - Epoch   5, Step:    72000, Batch Loss:     2.635591, Tokens per Sec:     7717, Lr: 0.000300\n",
      "2021-07-19 07:24:38,829 - INFO - joeynmt.training - Epoch   5: total training loss 5879.53\n",
      "2021-07-19 07:24:38,829 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-19 07:24:54,814 - INFO - joeynmt.training - Epoch   6, Step:    72100, Batch Loss:     2.426062, Tokens per Sec:     7572, Lr: 0.000300\n",
      "2021-07-19 07:25:24,484 - INFO - joeynmt.training - Epoch   6, Step:    72200, Batch Loss:     2.509454, Tokens per Sec:     7733, Lr: 0.000300\n",
      "2021-07-19 07:25:53,714 - INFO - joeynmt.training - Epoch   6, Step:    72300, Batch Loss:     2.302253, Tokens per Sec:     7634, Lr: 0.000300\n",
      "2021-07-19 07:26:23,235 - INFO - joeynmt.training - Epoch   6, Step:    72400, Batch Loss:     2.511296, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 07:26:52,982 - INFO - joeynmt.training - Epoch   6, Step:    72500, Batch Loss:     2.664346, Tokens per Sec:     7786, Lr: 0.000300\n",
      "2021-07-19 07:27:53,310 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:27:53,311 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:27:53,311 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:27:53,726 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 07:27:53,726 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 07:27:54,589 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:27:54,591 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:27:54,591 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:27:54,591 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ As I saw , I saw the scribes and the Pharisees , and said to them , “ I am going to see the seven times of my souls . ”\n",
      "2021-07-19 07:27:54,592 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:27:54,593 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:27:54,593 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:27:54,593 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary and Mary , and Mary , and Mary , and Mary said to Him , “ You are here . ”\n",
      "2021-07-19 07:27:54,593 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:27:54,594 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:27:54,594 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:27:54,594 - INFO - joeynmt.training - \tHypothesis: He was a day of time , and he was a day of judgment .\n",
      "2021-07-19 07:27:54,595 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:27:54,596 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:27:54,596 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:27:54,596 - INFO - joeynmt.training - \tHypothesis: And when he had sat down the sea , he saw the fire , and the fire of the sea , and the sea was cut down and the sea , and the sea was cut down .\n",
      "2021-07-19 07:27:54,596 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    72500: bleu:   6.75, loss: 89952.0469, ppl:  13.0350, duration: 61.6140s\n",
      "2021-07-19 07:28:24,662 - INFO - joeynmt.training - Epoch   6, Step:    72600, Batch Loss:     2.586247, Tokens per Sec:     7687, Lr: 0.000300\n",
      "2021-07-19 07:28:54,253 - INFO - joeynmt.training - Epoch   6, Step:    72700, Batch Loss:     2.817778, Tokens per Sec:     7620, Lr: 0.000300\n",
      "2021-07-19 07:29:23,624 - INFO - joeynmt.training - Epoch   6, Step:    72800, Batch Loss:     2.660525, Tokens per Sec:     7677, Lr: 0.000300\n",
      "2021-07-19 07:29:53,355 - INFO - joeynmt.training - Epoch   6, Step:    72900, Batch Loss:     2.583597, Tokens per Sec:     7792, Lr: 0.000300\n",
      "2021-07-19 07:30:22,997 - INFO - joeynmt.training - Epoch   6, Step:    73000, Batch Loss:     2.547949, Tokens per Sec:     7604, Lr: 0.000300\n",
      "2021-07-19 07:30:52,353 - INFO - joeynmt.training - Epoch   6, Step:    73100, Batch Loss:     2.299666, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 07:31:21,578 - INFO - joeynmt.training - Epoch   6, Step:    73200, Batch Loss:     1.925784, Tokens per Sec:     7729, Lr: 0.000300\n",
      "2021-07-19 07:31:51,104 - INFO - joeynmt.training - Epoch   6, Step:    73300, Batch Loss:     2.509909, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 07:32:20,568 - INFO - joeynmt.training - Epoch   6, Step:    73400, Batch Loss:     2.581945, Tokens per Sec:     7625, Lr: 0.000300\n",
      "2021-07-19 07:32:50,306 - INFO - joeynmt.training - Epoch   6, Step:    73500, Batch Loss:     2.230916, Tokens per Sec:     7769, Lr: 0.000300\n",
      "2021-07-19 07:33:20,101 - INFO - joeynmt.training - Epoch   6, Step:    73600, Batch Loss:     1.950913, Tokens per Sec:     7769, Lr: 0.000300\n",
      "2021-07-19 07:33:49,839 - INFO - joeynmt.training - Epoch   6, Step:    73700, Batch Loss:     2.683997, Tokens per Sec:     7724, Lr: 0.000300\n",
      "2021-07-19 07:34:19,446 - INFO - joeynmt.training - Epoch   6, Step:    73800, Batch Loss:     2.448956, Tokens per Sec:     7712, Lr: 0.000300\n",
      "2021-07-19 07:34:49,004 - INFO - joeynmt.training - Epoch   6, Step:    73900, Batch Loss:     2.616297, Tokens per Sec:     7584, Lr: 0.000300\n",
      "2021-07-19 07:35:18,557 - INFO - joeynmt.training - Epoch   6, Step:    74000, Batch Loss:     2.465466, Tokens per Sec:     7697, Lr: 0.000300\n",
      "2021-07-19 07:35:48,282 - INFO - joeynmt.training - Epoch   6, Step:    74100, Batch Loss:     2.633943, Tokens per Sec:     7747, Lr: 0.000300\n",
      "2021-07-19 07:36:17,472 - INFO - joeynmt.training - Epoch   6, Step:    74200, Batch Loss:     2.679731, Tokens per Sec:     7688, Lr: 0.000300\n",
      "2021-07-19 07:36:46,991 - INFO - joeynmt.training - Epoch   6, Step:    74300, Batch Loss:     2.502028, Tokens per Sec:     7747, Lr: 0.000300\n",
      "2021-07-19 07:37:07,018 - INFO - joeynmt.training - Epoch   6: total training loss 5844.39\n",
      "2021-07-19 07:37:07,019 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-19 07:37:16,873 - INFO - joeynmt.training - Epoch   7, Step:    74400, Batch Loss:     2.620971, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-07-19 07:37:46,414 - INFO - joeynmt.training - Epoch   7, Step:    74500, Batch Loss:     2.674058, Tokens per Sec:     7690, Lr: 0.000300\n",
      "2021-07-19 07:38:15,807 - INFO - joeynmt.training - Epoch   7, Step:    74600, Batch Loss:     2.603203, Tokens per Sec:     7595, Lr: 0.000300\n",
      "2021-07-19 07:38:45,323 - INFO - joeynmt.training - Epoch   7, Step:    74700, Batch Loss:     2.723341, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 07:39:14,853 - INFO - joeynmt.training - Epoch   7, Step:    74800, Batch Loss:     2.397131, Tokens per Sec:     7619, Lr: 0.000300\n",
      "2021-07-19 07:39:44,886 - INFO - joeynmt.training - Epoch   7, Step:    74900, Batch Loss:     2.385890, Tokens per Sec:     7791, Lr: 0.000300\n",
      "2021-07-19 07:40:14,437 - INFO - joeynmt.training - Epoch   7, Step:    75000, Batch Loss:     2.569933, Tokens per Sec:     7596, Lr: 0.000300\n",
      "2021-07-19 07:41:08,935 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:41:08,936 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:41:08,936 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:41:09,347 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 07:41:09,348 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 07:41:10,655 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:41:10,656 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:41:10,656 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:41:10,656 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw Him , and He said to them , “ I have come to the seven times , and I have come to you . ”\n",
      "2021-07-19 07:41:10,657 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:41:10,657 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:41:10,658 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:41:10,658 - INFO - joeynmt.training - \tHypothesis: When Martha had heard the voice , Mary came to Mary , and said to her , “ Mary , and she was born , and she said to Him , “ You are going to see Him . ”\n",
      "2021-07-19 07:41:10,658 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:41:10,659 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:41:10,659 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:41:10,659 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was on the day of his day .\n",
      "2021-07-19 07:41:10,659 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:41:10,660 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:41:10,660 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:41:10,661 - INFO - joeynmt.training - \tHypothesis: Then Paul had sat down the sea , and when he had been sleeping , he was sleeping , and he was sleeping , and he was sleeping .\n",
      "2021-07-19 07:41:10,661 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    75000: bleu:   6.69, loss: 89738.5078, ppl:  12.9558, duration: 56.2233s\n",
      "2021-07-19 07:41:40,397 - INFO - joeynmt.training - Epoch   7, Step:    75100, Batch Loss:     2.510605, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 07:42:10,145 - INFO - joeynmt.training - Epoch   7, Step:    75200, Batch Loss:     2.726663, Tokens per Sec:     7609, Lr: 0.000300\n",
      "2021-07-19 07:42:40,159 - INFO - joeynmt.training - Epoch   7, Step:    75300, Batch Loss:     2.833705, Tokens per Sec:     7701, Lr: 0.000300\n",
      "2021-07-19 07:43:09,749 - INFO - joeynmt.training - Epoch   7, Step:    75400, Batch Loss:     2.228259, Tokens per Sec:     7540, Lr: 0.000300\n",
      "2021-07-19 07:43:39,443 - INFO - joeynmt.training - Epoch   7, Step:    75500, Batch Loss:     2.298226, Tokens per Sec:     7626, Lr: 0.000300\n",
      "2021-07-19 07:44:09,180 - INFO - joeynmt.training - Epoch   7, Step:    75600, Batch Loss:     2.632501, Tokens per Sec:     7617, Lr: 0.000300\n",
      "2021-07-19 07:44:39,057 - INFO - joeynmt.training - Epoch   7, Step:    75700, Batch Loss:     2.519464, Tokens per Sec:     7622, Lr: 0.000300\n",
      "2021-07-19 07:45:08,857 - INFO - joeynmt.training - Epoch   7, Step:    75800, Batch Loss:     2.524806, Tokens per Sec:     7589, Lr: 0.000300\n",
      "2021-07-19 07:45:38,718 - INFO - joeynmt.training - Epoch   7, Step:    75900, Batch Loss:     2.535977, Tokens per Sec:     7655, Lr: 0.000300\n",
      "2021-07-19 07:46:08,509 - INFO - joeynmt.training - Epoch   7, Step:    76000, Batch Loss:     2.686146, Tokens per Sec:     7568, Lr: 0.000300\n",
      "2021-07-19 07:46:38,250 - INFO - joeynmt.training - Epoch   7, Step:    76100, Batch Loss:     2.603976, Tokens per Sec:     7663, Lr: 0.000300\n",
      "2021-07-19 07:47:07,997 - INFO - joeynmt.training - Epoch   7, Step:    76200, Batch Loss:     2.653717, Tokens per Sec:     7699, Lr: 0.000300\n",
      "2021-07-19 07:47:37,818 - INFO - joeynmt.training - Epoch   7, Step:    76300, Batch Loss:     2.335499, Tokens per Sec:     7645, Lr: 0.000300\n",
      "2021-07-19 07:48:07,277 - INFO - joeynmt.training - Epoch   7, Step:    76400, Batch Loss:     2.561674, Tokens per Sec:     7627, Lr: 0.000300\n",
      "2021-07-19 07:48:36,919 - INFO - joeynmt.training - Epoch   7, Step:    76500, Batch Loss:     2.619548, Tokens per Sec:     7542, Lr: 0.000300\n",
      "2021-07-19 07:49:06,695 - INFO - joeynmt.training - Epoch   7, Step:    76600, Batch Loss:     2.364868, Tokens per Sec:     7739, Lr: 0.000300\n",
      "2021-07-19 07:49:34,372 - INFO - joeynmt.training - Epoch   7: total training loss 5847.28\n",
      "2021-07-19 07:49:34,372 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-19 07:49:36,452 - INFO - joeynmt.training - Epoch   8, Step:    76700, Batch Loss:     2.664941, Tokens per Sec:     6704, Lr: 0.000300\n",
      "2021-07-19 07:50:06,154 - INFO - joeynmt.training - Epoch   8, Step:    76800, Batch Loss:     2.294627, Tokens per Sec:     7653, Lr: 0.000300\n",
      "2021-07-19 07:50:35,952 - INFO - joeynmt.training - Epoch   8, Step:    76900, Batch Loss:     2.555516, Tokens per Sec:     7825, Lr: 0.000300\n",
      "2021-07-19 07:51:05,429 - INFO - joeynmt.training - Epoch   8, Step:    77000, Batch Loss:     2.537484, Tokens per Sec:     7719, Lr: 0.000300\n",
      "2021-07-19 07:51:34,920 - INFO - joeynmt.training - Epoch   8, Step:    77100, Batch Loss:     2.482327, Tokens per Sec:     7697, Lr: 0.000300\n",
      "2021-07-19 07:52:04,346 - INFO - joeynmt.training - Epoch   8, Step:    77200, Batch Loss:     2.259924, Tokens per Sec:     7695, Lr: 0.000300\n",
      "2021-07-19 07:52:34,023 - INFO - joeynmt.training - Epoch   8, Step:    77300, Batch Loss:     2.775788, Tokens per Sec:     7655, Lr: 0.000300\n",
      "2021-07-19 07:53:03,543 - INFO - joeynmt.training - Epoch   8, Step:    77400, Batch Loss:     2.601133, Tokens per Sec:     7765, Lr: 0.000300\n",
      "2021-07-19 07:53:32,912 - INFO - joeynmt.training - Epoch   8, Step:    77500, Batch Loss:     2.526324, Tokens per Sec:     7662, Lr: 0.000300\n",
      "2021-07-19 07:54:35,384 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:54:35,384 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:54:35,384 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:54:35,787 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 07:54:35,788 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 07:54:36,640 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:54:36,641 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:54:36,641 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:54:36,642 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ As a man , I saw Him , and said to them , “ I have come and come to the seven hundred and fell into the tomb . ”\n",
      "2021-07-19 07:54:36,642 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:54:36,643 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:54:36,643 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:54:36,644 - INFO - joeynmt.training - \tHypothesis: Now when Martha had heard this , Mary came to Mary and said to her , “ Mary , and the daughter of Mary , and the Lord said to Him , “ You are going to see . ”\n",
      "2021-07-19 07:54:36,644 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:54:36,644 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:54:36,645 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:54:36,645 - INFO - joeynmt.training - \tHypothesis: He was a long time to be a day of judgment on the Sabbath .\n",
      "2021-07-19 07:54:36,645 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:54:36,646 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:54:36,646 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:54:36,647 - INFO - joeynmt.training - \tHypothesis: Then Paul had spoken , and when He had been sat down , when He was sleeping , he was sleeping , and the sea of the sea , and the sea was raised up .\n",
      "2021-07-19 07:54:36,647 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    77500: bleu:   6.57, loss: 89585.5703, ppl:  12.8993, duration: 63.7340s\n",
      "2021-07-19 07:55:06,746 - INFO - joeynmt.training - Epoch   8, Step:    77600, Batch Loss:     2.567941, Tokens per Sec:     7696, Lr: 0.000300\n",
      "2021-07-19 07:55:36,057 - INFO - joeynmt.training - Epoch   8, Step:    77700, Batch Loss:     2.551858, Tokens per Sec:     7754, Lr: 0.000300\n",
      "2021-07-19 07:56:05,505 - INFO - joeynmt.training - Epoch   8, Step:    77800, Batch Loss:     2.744708, Tokens per Sec:     7645, Lr: 0.000300\n",
      "2021-07-19 07:56:34,850 - INFO - joeynmt.training - Epoch   8, Step:    77900, Batch Loss:     2.685618, Tokens per Sec:     7677, Lr: 0.000300\n",
      "2021-07-19 07:57:03,990 - INFO - joeynmt.training - Epoch   8, Step:    78000, Batch Loss:     2.612380, Tokens per Sec:     7661, Lr: 0.000300\n",
      "2021-07-19 07:57:33,158 - INFO - joeynmt.training - Epoch   8, Step:    78100, Batch Loss:     2.659914, Tokens per Sec:     7668, Lr: 0.000300\n",
      "2021-07-19 07:58:02,980 - INFO - joeynmt.training - Epoch   8, Step:    78200, Batch Loss:     2.671862, Tokens per Sec:     7682, Lr: 0.000300\n",
      "2021-07-19 07:58:32,260 - INFO - joeynmt.training - Epoch   8, Step:    78300, Batch Loss:     2.482863, Tokens per Sec:     7841, Lr: 0.000300\n",
      "2021-07-19 07:59:01,734 - INFO - joeynmt.training - Epoch   8, Step:    78400, Batch Loss:     2.651960, Tokens per Sec:     7726, Lr: 0.000300\n",
      "2021-07-19 07:59:31,198 - INFO - joeynmt.training - Epoch   8, Step:    78500, Batch Loss:     2.555839, Tokens per Sec:     7693, Lr: 0.000300\n",
      "2021-07-19 08:00:00,772 - INFO - joeynmt.training - Epoch   8, Step:    78600, Batch Loss:     2.397027, Tokens per Sec:     7664, Lr: 0.000300\n",
      "2021-07-19 08:00:30,231 - INFO - joeynmt.training - Epoch   8, Step:    78700, Batch Loss:     2.604766, Tokens per Sec:     7749, Lr: 0.000300\n",
      "2021-07-19 08:00:59,772 - INFO - joeynmt.training - Epoch   8, Step:    78800, Batch Loss:     2.448844, Tokens per Sec:     7751, Lr: 0.000300\n",
      "2021-07-19 08:01:29,319 - INFO - joeynmt.training - Epoch   8, Step:    78900, Batch Loss:     2.320485, Tokens per Sec:     7741, Lr: 0.000300\n",
      "2021-07-19 08:01:58,927 - INFO - joeynmt.training - Epoch   8, Step:    79000, Batch Loss:     2.636914, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 08:02:02,893 - INFO - joeynmt.training - Epoch   8: total training loss 5819.94\n",
      "2021-07-19 08:02:02,893 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-19 08:02:28,698 - INFO - joeynmt.training - Epoch   9, Step:    79100, Batch Loss:     2.443236, Tokens per Sec:     7665, Lr: 0.000300\n",
      "2021-07-19 08:02:57,898 - INFO - joeynmt.training - Epoch   9, Step:    79200, Batch Loss:     2.300488, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-19 08:03:27,400 - INFO - joeynmt.training - Epoch   9, Step:    79300, Batch Loss:     2.413076, Tokens per Sec:     7690, Lr: 0.000300\n",
      "2021-07-19 08:03:57,094 - INFO - joeynmt.training - Epoch   9, Step:    79400, Batch Loss:     2.494830, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 08:04:26,554 - INFO - joeynmt.training - Epoch   9, Step:    79500, Batch Loss:     2.828069, Tokens per Sec:     7723, Lr: 0.000300\n",
      "2021-07-19 08:04:56,205 - INFO - joeynmt.training - Epoch   9, Step:    79600, Batch Loss:     2.337211, Tokens per Sec:     7778, Lr: 0.000300\n",
      "2021-07-19 08:05:25,505 - INFO - joeynmt.training - Epoch   9, Step:    79700, Batch Loss:     2.472323, Tokens per Sec:     7744, Lr: 0.000300\n",
      "2021-07-19 08:05:54,748 - INFO - joeynmt.training - Epoch   9, Step:    79800, Batch Loss:     2.669052, Tokens per Sec:     7706, Lr: 0.000300\n",
      "2021-07-19 08:06:24,030 - INFO - joeynmt.training - Epoch   9, Step:    79900, Batch Loss:     2.742452, Tokens per Sec:     7699, Lr: 0.000300\n",
      "2021-07-19 08:06:53,701 - INFO - joeynmt.training - Epoch   9, Step:    80000, Batch Loss:     2.602110, Tokens per Sec:     7775, Lr: 0.000300\n",
      "2021-07-19 08:07:55,289 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 08:07:55,290 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 08:07:55,290 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 08:07:56,533 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 08:07:56,534 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 08:07:56,534 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 08:07:56,534 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees had a man , and he saw him , saying , “ I have come to the seven thousand and thousand years , and I have come to you . ”\n",
      "2021-07-19 08:07:56,534 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 08:07:56,535 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 08:07:56,535 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 08:07:56,535 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary and Mary , and Mary , and Mary , and Mary , said to him , “ Teacher , and you are going to know . ”\n",
      "2021-07-19 08:07:56,536 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 08:07:56,536 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 08:07:56,536 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 08:07:56,537 - INFO - joeynmt.training - \tHypothesis: He was a long time to come to the day of the Sabbath .\n",
      "2021-07-19 08:07:56,537 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 08:07:56,538 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 08:07:56,538 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 08:07:56,538 - INFO - joeynmt.training - \tHypothesis: Paul was looking for the sake of the sea , and when he was raised up , he was raised up and sat down with the fire , and the fire of the sea was set out .\n",
      "2021-07-19 08:07:56,539 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    80000: bleu:   6.47, loss: 89616.5391, ppl:  12.9108, duration: 62.8341s\n",
      "2021-07-19 08:08:26,149 - INFO - joeynmt.training - Epoch   9, Step:    80100, Batch Loss:     2.214161, Tokens per Sec:     7635, Lr: 0.000300\n",
      "2021-07-19 08:08:55,645 - INFO - joeynmt.training - Epoch   9, Step:    80200, Batch Loss:     2.246352, Tokens per Sec:     7698, Lr: 0.000300\n",
      "2021-07-19 08:09:25,269 - INFO - joeynmt.training - Epoch   9, Step:    80300, Batch Loss:     2.544671, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 08:09:54,586 - INFO - joeynmt.training - Epoch   9, Step:    80400, Batch Loss:     2.687201, Tokens per Sec:     7807, Lr: 0.000300\n",
      "2021-07-19 08:10:23,992 - INFO - joeynmt.training - Epoch   9, Step:    80500, Batch Loss:     2.606653, Tokens per Sec:     7692, Lr: 0.000300\n",
      "2021-07-19 08:10:53,229 - INFO - joeynmt.training - Epoch   9, Step:    80600, Batch Loss:     2.571099, Tokens per Sec:     7556, Lr: 0.000300\n",
      "2021-07-19 08:11:22,574 - INFO - joeynmt.training - Epoch   9, Step:    80700, Batch Loss:     2.440603, Tokens per Sec:     7662, Lr: 0.000300\n",
      "2021-07-19 08:11:51,866 - INFO - joeynmt.training - Epoch   9, Step:    80800, Batch Loss:     2.414575, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 08:12:21,091 - INFO - joeynmt.training - Epoch   9, Step:    80900, Batch Loss:     2.664021, Tokens per Sec:     7680, Lr: 0.000300\n",
      "2021-07-19 08:12:50,064 - INFO - joeynmt.training - Epoch   9, Step:    81000, Batch Loss:     2.303547, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 08:13:19,571 - INFO - joeynmt.training - Epoch   9, Step:    81100, Batch Loss:     2.481421, Tokens per Sec:     7643, Lr: 0.000300\n",
      "2021-07-19 08:13:48,900 - INFO - joeynmt.training - Epoch   9, Step:    81200, Batch Loss:     2.584865, Tokens per Sec:     7728, Lr: 0.000300\n",
      "2021-07-19 08:14:18,501 - INFO - joeynmt.training - Epoch   9, Step:    81300, Batch Loss:     2.471842, Tokens per Sec:     7671, Lr: 0.000300\n",
      "2021-07-19 08:14:31,988 - INFO - joeynmt.training - Epoch   9: total training loss 5838.05\n",
      "2021-07-19 08:14:31,988 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-19 08:14:48,369 - INFO - joeynmt.training - Epoch  10, Step:    81400, Batch Loss:     2.451086, Tokens per Sec:     7650, Lr: 0.000300\n",
      "2021-07-19 08:15:17,884 - INFO - joeynmt.training - Epoch  10, Step:    81500, Batch Loss:     2.269515, Tokens per Sec:     7683, Lr: 0.000300\n",
      "2021-07-19 08:15:47,623 - INFO - joeynmt.training - Epoch  10, Step:    81600, Batch Loss:     2.584017, Tokens per Sec:     7799, Lr: 0.000300\n",
      "2021-07-19 08:16:17,311 - INFO - joeynmt.training - Epoch  10, Step:    81700, Batch Loss:     2.119755, Tokens per Sec:     7766, Lr: 0.000300\n",
      "2021-07-19 08:16:46,489 - INFO - joeynmt.training - Epoch  10, Step:    81800, Batch Loss:     2.566749, Tokens per Sec:     7638, Lr: 0.000300\n",
      "2021-07-19 08:17:16,063 - INFO - joeynmt.training - Epoch  10, Step:    81900, Batch Loss:     2.566232, Tokens per Sec:     7715, Lr: 0.000300\n",
      "2021-07-19 08:17:45,514 - INFO - joeynmt.training - Epoch  10, Step:    82000, Batch Loss:     2.778504, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 08:18:14,993 - INFO - joeynmt.training - Epoch  10, Step:    82100, Batch Loss:     2.505421, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 08:18:44,471 - INFO - joeynmt.training - Epoch  10, Step:    82200, Batch Loss:     2.293391, Tokens per Sec:     7688, Lr: 0.000300\n",
      "2021-07-19 08:19:14,065 - INFO - joeynmt.training - Epoch  10, Step:    82300, Batch Loss:     2.750319, Tokens per Sec:     7752, Lr: 0.000300\n",
      "2021-07-19 08:19:43,637 - INFO - joeynmt.training - Epoch  10, Step:    82400, Batch Loss:     2.548778, Tokens per Sec:     7792, Lr: 0.000300\n",
      "2021-07-19 08:20:13,419 - INFO - joeynmt.training - Epoch  10, Step:    82500, Batch Loss:     2.631675, Tokens per Sec:     7847, Lr: 0.000300\n",
      "2021-07-19 08:21:07,627 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 08:21:07,627 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 08:21:07,628 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 08:21:08,970 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 08:21:08,971 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 08:21:08,971 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 08:21:08,971 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He saw the scribes and the Pharisees , and said to them , “ I have come and come to you . ”\n",
      "2021-07-19 08:21:08,972 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 08:21:08,973 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 08:21:08,974 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 08:21:08,974 - INFO - joeynmt.training - \tHypothesis: Now when Martha heard that her sister , Mary , and her mother , and her mother , and her mother , said to her , “ See , and you are going to see . ”\n",
      "2021-07-19 08:21:08,974 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 08:21:08,975 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 08:21:08,975 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 08:21:08,976 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and the day was near .\n",
      "2021-07-19 08:21:08,976 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 08:21:08,977 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 08:21:08,977 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 08:21:08,977 - INFO - joeynmt.training - \tHypothesis: Then Paul looked out to Him , and when He was sleeping , He was sleeping , and the sea of the sea , and the sea was sprout of the sea .\n",
      "2021-07-19 08:21:08,977 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    82500: bleu:   6.86, loss: 89637.4922, ppl:  12.9185, duration: 55.5574s\n",
      "2021-07-19 08:21:38,493 - INFO - joeynmt.training - Epoch  10, Step:    82600, Batch Loss:     2.391757, Tokens per Sec:     7687, Lr: 0.000300\n",
      "2021-07-19 08:22:08,055 - INFO - joeynmt.training - Epoch  10, Step:    82700, Batch Loss:     2.695880, Tokens per Sec:     7619, Lr: 0.000300\n",
      "2021-07-19 08:22:37,127 - INFO - joeynmt.training - Epoch  10, Step:    82800, Batch Loss:     2.809226, Tokens per Sec:     7627, Lr: 0.000300\n",
      "2021-07-19 08:23:06,371 - INFO - joeynmt.training - Epoch  10, Step:    82900, Batch Loss:     2.405377, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-19 08:23:35,646 - INFO - joeynmt.training - Epoch  10, Step:    83000, Batch Loss:     2.335890, Tokens per Sec:     7699, Lr: 0.000300\n",
      "2021-07-19 08:24:05,113 - INFO - joeynmt.training - Epoch  10, Step:    83100, Batch Loss:     2.778522, Tokens per Sec:     7758, Lr: 0.000300\n",
      "2021-07-19 08:24:34,565 - INFO - joeynmt.training - Epoch  10, Step:    83200, Batch Loss:     2.503555, Tokens per Sec:     7707, Lr: 0.000300\n",
      "2021-07-19 08:25:03,937 - INFO - joeynmt.training - Epoch  10, Step:    83300, Batch Loss:     2.703060, Tokens per Sec:     7613, Lr: 0.000300\n",
      "2021-07-19 08:25:33,346 - INFO - joeynmt.training - Epoch  10, Step:    83400, Batch Loss:     2.568027, Tokens per Sec:     7739, Lr: 0.000300\n",
      "2021-07-19 08:26:02,539 - INFO - joeynmt.training - Epoch  10, Step:    83500, Batch Loss:     2.432688, Tokens per Sec:     7636, Lr: 0.000300\n",
      "2021-07-19 08:26:32,215 - INFO - joeynmt.training - Epoch  10, Step:    83600, Batch Loss:     2.594275, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 08:26:53,082 - INFO - joeynmt.training - Epoch  10: total training loss 5809.66\n",
      "2021-07-19 08:26:53,083 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-19 08:27:02,065 - INFO - joeynmt.training - Epoch  11, Step:    83700, Batch Loss:     2.643567, Tokens per Sec:     7561, Lr: 0.000300\n",
      "2021-07-19 08:27:31,535 - INFO - joeynmt.training - Epoch  11, Step:    83800, Batch Loss:     2.475717, Tokens per Sec:     7559, Lr: 0.000300\n",
      "2021-07-19 08:28:01,178 - INFO - joeynmt.training - Epoch  11, Step:    83900, Batch Loss:     2.599801, Tokens per Sec:     7790, Lr: 0.000300\n",
      "2021-07-19 08:28:30,563 - INFO - joeynmt.training - Epoch  11, Step:    84000, Batch Loss:     2.459551, Tokens per Sec:     7728, Lr: 0.000300\n",
      "2021-07-19 08:28:59,962 - INFO - joeynmt.training - Epoch  11, Step:    84100, Batch Loss:     1.946769, Tokens per Sec:     7654, Lr: 0.000300\n",
      "2021-07-19 08:29:29,242 - INFO - joeynmt.training - Epoch  11, Step:    84200, Batch Loss:     2.666283, Tokens per Sec:     7709, Lr: 0.000300\n",
      "2021-07-19 08:29:58,833 - INFO - joeynmt.training - Epoch  11, Step:    84300, Batch Loss:     2.390704, Tokens per Sec:     7696, Lr: 0.000300\n",
      "2021-07-19 08:30:28,149 - INFO - joeynmt.training - Epoch  11, Step:    84400, Batch Loss:     2.483641, Tokens per Sec:     7726, Lr: 0.000300\n",
      "2021-07-19 08:30:57,685 - INFO - joeynmt.training - Epoch  11, Step:    84500, Batch Loss:     2.717983, Tokens per Sec:     7835, Lr: 0.000300\n",
      "2021-07-19 08:31:27,308 - INFO - joeynmt.training - Epoch  11, Step:    84600, Batch Loss:     2.771831, Tokens per Sec:     7758, Lr: 0.000300\n",
      "2021-07-19 08:31:56,644 - INFO - joeynmt.training - Epoch  11, Step:    84700, Batch Loss:     2.626935, Tokens per Sec:     7605, Lr: 0.000300\n",
      "2021-07-19 08:32:26,313 - INFO - joeynmt.training - Epoch  11, Step:    84800, Batch Loss:     2.483963, Tokens per Sec:     7680, Lr: 0.000300\n",
      "2021-07-19 08:32:55,622 - INFO - joeynmt.training - Epoch  11, Step:    84900, Batch Loss:     2.557310, Tokens per Sec:     7654, Lr: 0.000300\n",
      "2021-07-19 08:33:25,143 - INFO - joeynmt.training - Epoch  11, Step:    85000, Batch Loss:     2.529170, Tokens per Sec:     7722, Lr: 0.000300\n",
      "2021-07-19 08:34:27,555 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 08:34:27,555 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 08:34:27,555 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 08:34:27,984 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 08:34:27,985 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 08:34:28,861 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 08:34:28,862 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 08:34:28,862 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 08:34:28,862 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees , who had been a man , and said to them , “ I have come to the city , and I have come to the city and come to the twelve . ”\n",
      "2021-07-19 08:34:28,862 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 08:34:28,863 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 08:34:28,863 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 08:34:28,864 - INFO - joeynmt.training - \tHypothesis: When Martha had heard this , Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and said to her , “ Teacher , and we are going to see Him . ”\n",
      "2021-07-19 08:34:28,864 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 08:34:28,864 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 08:34:28,865 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 08:34:28,865 - INFO - joeynmt.training - \tHypothesis: He was a long time for the day of his day , and he was in the Sabbath .\n",
      "2021-07-19 08:34:28,865 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 08:34:28,866 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 08:34:28,866 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 08:34:28,866 - INFO - joeynmt.training - \tHypothesis: Then Paul gave them a cup , and when He had sat down , he was cast out of the sea , and the fire of the sea , and the sea was a of the sea .\n",
      "2021-07-19 08:34:28,866 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    85000: bleu:   6.79, loss: 89185.6641, ppl:  12.7529, duration: 63.7230s\n",
      "2021-07-19 08:34:58,752 - INFO - joeynmt.training - Epoch  11, Step:    85100, Batch Loss:     2.616946, Tokens per Sec:     7722, Lr: 0.000300\n",
      "2021-07-19 08:35:28,489 - INFO - joeynmt.training - Epoch  11, Step:    85200, Batch Loss:     2.215174, Tokens per Sec:     7676, Lr: 0.000300\n",
      "2021-07-19 08:35:58,303 - INFO - joeynmt.training - Epoch  11, Step:    85300, Batch Loss:     2.190792, Tokens per Sec:     7762, Lr: 0.000300\n",
      "2021-07-19 08:36:27,863 - INFO - joeynmt.training - Epoch  11, Step:    85400, Batch Loss:     2.393881, Tokens per Sec:     7673, Lr: 0.000300\n",
      "2021-07-19 08:36:57,279 - INFO - joeynmt.training - Epoch  11, Step:    85500, Batch Loss:     2.299319, Tokens per Sec:     7607, Lr: 0.000300\n",
      "2021-07-19 08:37:26,909 - INFO - joeynmt.training - Epoch  11, Step:    85600, Batch Loss:     2.519327, Tokens per Sec:     7778, Lr: 0.000300\n",
      "2021-07-19 08:37:56,163 - INFO - joeynmt.training - Epoch  11, Step:    85700, Batch Loss:     2.682425, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 08:38:25,480 - INFO - joeynmt.training - Epoch  11, Step:    85800, Batch Loss:     2.603841, Tokens per Sec:     7582, Lr: 0.000300\n",
      "2021-07-19 08:38:54,955 - INFO - joeynmt.training - Epoch  11, Step:    85900, Batch Loss:     2.512598, Tokens per Sec:     7634, Lr: 0.000300\n",
      "2021-07-19 08:39:22,983 - INFO - joeynmt.training - Epoch  11: total training loss 5794.68\n",
      "2021-07-19 08:39:22,984 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-19 08:39:24,730 - INFO - joeynmt.training - Epoch  12, Step:    86000, Batch Loss:     2.406390, Tokens per Sec:     6251, Lr: 0.000300\n",
      "2021-07-19 08:39:53,924 - INFO - joeynmt.training - Epoch  12, Step:    86100, Batch Loss:     2.342146, Tokens per Sec:     7611, Lr: 0.000300\n",
      "2021-07-19 08:40:23,330 - INFO - joeynmt.training - Epoch  12, Step:    86200, Batch Loss:     2.129266, Tokens per Sec:     7673, Lr: 0.000300\n",
      "2021-07-19 08:40:52,524 - INFO - joeynmt.training - Epoch  12, Step:    86300, Batch Loss:     2.348691, Tokens per Sec:     7638, Lr: 0.000300\n",
      "2021-07-19 08:41:22,064 - INFO - joeynmt.training - Epoch  12, Step:    86400, Batch Loss:     2.480468, Tokens per Sec:     7720, Lr: 0.000300\n",
      "2021-07-19 08:41:51,514 - INFO - joeynmt.training - Epoch  12, Step:    86500, Batch Loss:     2.648408, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 08:42:20,855 - INFO - joeynmt.training - Epoch  12, Step:    86600, Batch Loss:     2.695687, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-07-19 08:42:50,424 - INFO - joeynmt.training - Epoch  12, Step:    86700, Batch Loss:     2.329769, Tokens per Sec:     7712, Lr: 0.000300\n",
      "2021-07-19 08:43:19,996 - INFO - joeynmt.training - Epoch  12, Step:    86800, Batch Loss:     2.487303, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 08:43:49,576 - INFO - joeynmt.training - Epoch  12, Step:    86900, Batch Loss:     2.452820, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 08:44:18,965 - INFO - joeynmt.training - Epoch  12, Step:    87000, Batch Loss:     2.402815, Tokens per Sec:     7819, Lr: 0.000300\n",
      "2021-07-19 08:44:48,676 - INFO - joeynmt.training - Epoch  12, Step:    87100, Batch Loss:     2.321086, Tokens per Sec:     7698, Lr: 0.000300\n",
      "2021-07-19 08:45:18,483 - INFO - joeynmt.training - Epoch  12, Step:    87200, Batch Loss:     2.671277, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 08:45:47,968 - INFO - joeynmt.training - Epoch  12, Step:    87300, Batch Loss:     2.220192, Tokens per Sec:     7775, Lr: 0.000300\n",
      "2021-07-19 08:46:17,417 - INFO - joeynmt.training - Epoch  12, Step:    87400, Batch Loss:     2.376643, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 08:46:47,013 - INFO - joeynmt.training - Epoch  12, Step:    87500, Batch Loss:     2.263819, Tokens per Sec:     7863, Lr: 0.000300\n",
      "2021-07-19 08:47:39,847 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 08:47:39,847 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 08:47:39,848 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 08:47:40,272 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 08:47:40,273 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 08:47:41,102 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 08:47:41,104 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 08:47:41,104 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 08:47:41,104 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw him , and He said to them , “ I have come and come to the seven times and come and come to the sword of the sword . ”\n",
      "2021-07-19 08:47:41,104 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 08:47:41,105 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 08:47:41,105 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 08:47:41,106 - INFO - joeynmt.training - \tHypothesis: When Martha had heard the voice of Mary , Mary and Mary , and Mary said to her , “ See , and you are going to see Him . ”\n",
      "2021-07-19 08:47:41,106 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 08:47:41,106 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 08:47:41,107 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 08:47:41,107 - INFO - joeynmt.training - \tHypothesis: He was a day of service , and he was a day of service .\n",
      "2021-07-19 08:47:41,107 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 08:47:41,108 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 08:47:41,108 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 08:47:41,108 - INFO - joeynmt.training - \tHypothesis: Then Paul gave them a car , and when he was on the sea , he was raised up , and he was raised up and sat down and sat down , and the sea of the sea .\n",
      "2021-07-19 08:47:41,109 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    87500: bleu:   6.91, loss: 89144.7344, ppl:  12.7380, duration: 54.0947s\n",
      "2021-07-19 08:48:10,787 - INFO - joeynmt.training - Epoch  12, Step:    87600, Batch Loss:     2.279918, Tokens per Sec:     7579, Lr: 0.000300\n",
      "2021-07-19 08:48:40,251 - INFO - joeynmt.training - Epoch  12, Step:    87700, Batch Loss:     2.612780, Tokens per Sec:     7670, Lr: 0.000300\n",
      "2021-07-19 08:49:09,600 - INFO - joeynmt.training - Epoch  12, Step:    87800, Batch Loss:     2.600908, Tokens per Sec:     7743, Lr: 0.000300\n",
      "2021-07-19 08:49:39,046 - INFO - joeynmt.training - Epoch  12, Step:    87900, Batch Loss:     2.391170, Tokens per Sec:     7692, Lr: 0.000300\n",
      "2021-07-19 08:50:08,553 - INFO - joeynmt.training - Epoch  12, Step:    88000, Batch Loss:     2.545542, Tokens per Sec:     7626, Lr: 0.000300\n",
      "2021-07-19 08:50:38,343 - INFO - joeynmt.training - Epoch  12, Step:    88100, Batch Loss:     2.162135, Tokens per Sec:     7768, Lr: 0.000300\n",
      "2021-07-19 08:51:07,588 - INFO - joeynmt.training - Epoch  12, Step:    88200, Batch Loss:     2.646303, Tokens per Sec:     7618, Lr: 0.000300\n",
      "2021-07-19 08:51:37,056 - INFO - joeynmt.training - Epoch  12, Step:    88300, Batch Loss:     2.588193, Tokens per Sec:     7736, Lr: 0.000300\n",
      "2021-07-19 08:51:43,442 - INFO - joeynmt.training - Epoch  12: total training loss 5790.70\n",
      "2021-07-19 08:51:43,443 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-19 08:52:06,946 - INFO - joeynmt.training - Epoch  13, Step:    88400, Batch Loss:     2.314396, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-07-19 08:52:36,258 - INFO - joeynmt.training - Epoch  13, Step:    88500, Batch Loss:     2.217283, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-19 08:53:05,764 - INFO - joeynmt.training - Epoch  13, Step:    88600, Batch Loss:     2.363145, Tokens per Sec:     7715, Lr: 0.000300\n",
      "2021-07-19 08:53:34,957 - INFO - joeynmt.training - Epoch  13, Step:    88700, Batch Loss:     2.203974, Tokens per Sec:     7706, Lr: 0.000300\n",
      "2021-07-19 08:54:04,701 - INFO - joeynmt.training - Epoch  13, Step:    88800, Batch Loss:     2.476982, Tokens per Sec:     7701, Lr: 0.000300\n",
      "2021-07-19 08:54:34,580 - INFO - joeynmt.training - Epoch  13, Step:    88900, Batch Loss:     2.401724, Tokens per Sec:     7719, Lr: 0.000300\n",
      "2021-07-19 08:55:04,042 - INFO - joeynmt.training - Epoch  13, Step:    89000, Batch Loss:     2.458592, Tokens per Sec:     7754, Lr: 0.000300\n",
      "2021-07-19 08:55:33,624 - INFO - joeynmt.training - Epoch  13, Step:    89100, Batch Loss:     2.728800, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 08:56:03,180 - INFO - joeynmt.training - Epoch  13, Step:    89200, Batch Loss:     2.568518, Tokens per Sec:     7646, Lr: 0.000300\n",
      "2021-07-19 08:56:32,716 - INFO - joeynmt.training - Epoch  13, Step:    89300, Batch Loss:     2.439526, Tokens per Sec:     7760, Lr: 0.000300\n",
      "2021-07-19 08:57:02,526 - INFO - joeynmt.training - Epoch  13, Step:    89400, Batch Loss:     2.143273, Tokens per Sec:     7767, Lr: 0.000300\n",
      "2021-07-19 08:57:32,008 - INFO - joeynmt.training - Epoch  13, Step:    89500, Batch Loss:     2.674571, Tokens per Sec:     7634, Lr: 0.000300\n",
      "2021-07-19 08:58:01,600 - INFO - joeynmt.training - Epoch  13, Step:    89600, Batch Loss:     2.487332, Tokens per Sec:     7735, Lr: 0.000300\n",
      "2021-07-19 08:58:30,921 - INFO - joeynmt.training - Epoch  13, Step:    89700, Batch Loss:     2.485613, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 08:59:00,259 - INFO - joeynmt.training - Epoch  13, Step:    89800, Batch Loss:     2.523307, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 08:59:29,799 - INFO - joeynmt.training - Epoch  13, Step:    89900, Batch Loss:     2.534072, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 08:59:59,466 - INFO - joeynmt.training - Epoch  13, Step:    90000, Batch Loss:     2.421108, Tokens per Sec:     7694, Lr: 0.000300\n",
      "2021-07-19 09:00:54,513 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:00:54,514 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:00:54,514 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:00:54,915 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 09:00:54,915 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 09:00:55,755 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:00:55,757 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:00:55,757 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:00:55,757 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a sword , and said , “ I have come to the seven times , and I have come to you . ”\n",
      "2021-07-19 09:00:55,758 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:00:55,759 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:00:55,759 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:00:55,759 - INFO - joeynmt.training - \tHypothesis: And Martha , who had heard the voice of Mary , and Mary , and said to her , “ Teacher , and we have heard it . ”\n",
      "2021-07-19 09:00:55,759 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:00:55,760 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:00:55,761 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:00:55,761 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the Sabbath , and he was a day .\n",
      "2021-07-19 09:00:55,761 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:00:55,762 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:00:55,762 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:00:55,762 - INFO - joeynmt.training - \tHypothesis: Then Paul gave them the water , and when he was cut off , he was raised up , and was raised up in the head of the sea , and the bread of the sea .\n",
      "2021-07-19 09:00:55,763 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    90000: bleu:   7.14, loss: 88830.9219, ppl:  12.6245, duration: 56.2963s\n",
      "2021-07-19 09:01:25,313 - INFO - joeynmt.training - Epoch  13, Step:    90100, Batch Loss:     2.616355, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-19 09:01:54,947 - INFO - joeynmt.training - Epoch  13, Step:    90200, Batch Loss:     2.246246, Tokens per Sec:     7643, Lr: 0.000300\n",
      "2021-07-19 09:02:24,375 - INFO - joeynmt.training - Epoch  13, Step:    90300, Batch Loss:     2.156032, Tokens per Sec:     7757, Lr: 0.000300\n",
      "2021-07-19 09:02:53,793 - INFO - joeynmt.training - Epoch  13, Step:    90400, Batch Loss:     2.754822, Tokens per Sec:     7727, Lr: 0.000300\n",
      "2021-07-19 09:03:23,562 - INFO - joeynmt.training - Epoch  13, Step:    90500, Batch Loss:     2.005337, Tokens per Sec:     7820, Lr: 0.000300\n",
      "2021-07-19 09:03:52,936 - INFO - joeynmt.training - Epoch  13, Step:    90600, Batch Loss:     2.570472, Tokens per Sec:     7612, Lr: 0.000300\n",
      "2021-07-19 09:04:05,944 - INFO - joeynmt.training - Epoch  13: total training loss 5778.03\n",
      "2021-07-19 09:04:05,944 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-19 09:04:22,426 - INFO - joeynmt.training - Epoch  14, Step:    90700, Batch Loss:     1.827927, Tokens per Sec:     7504, Lr: 0.000300\n",
      "2021-07-19 09:04:52,030 - INFO - joeynmt.training - Epoch  14, Step:    90800, Batch Loss:     2.638566, Tokens per Sec:     7738, Lr: 0.000300\n",
      "2021-07-19 09:05:21,456 - INFO - joeynmt.training - Epoch  14, Step:    90900, Batch Loss:     2.480012, Tokens per Sec:     7670, Lr: 0.000300\n",
      "2021-07-19 09:05:51,174 - INFO - joeynmt.training - Epoch  14, Step:    91000, Batch Loss:     2.593673, Tokens per Sec:     7679, Lr: 0.000300\n",
      "2021-07-19 09:06:20,602 - INFO - joeynmt.training - Epoch  14, Step:    91100, Batch Loss:     2.684629, Tokens per Sec:     7600, Lr: 0.000300\n",
      "2021-07-19 09:06:50,306 - INFO - joeynmt.training - Epoch  14, Step:    91200, Batch Loss:     2.686171, Tokens per Sec:     7665, Lr: 0.000300\n",
      "2021-07-19 09:07:19,737 - INFO - joeynmt.training - Epoch  14, Step:    91300, Batch Loss:     2.501434, Tokens per Sec:     7739, Lr: 0.000300\n",
      "2021-07-19 09:07:49,198 - INFO - joeynmt.training - Epoch  14, Step:    91400, Batch Loss:     2.388778, Tokens per Sec:     7719, Lr: 0.000300\n",
      "2021-07-19 09:08:18,629 - INFO - joeynmt.training - Epoch  14, Step:    91500, Batch Loss:     2.783267, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 09:08:48,472 - INFO - joeynmt.training - Epoch  14, Step:    91600, Batch Loss:     2.309846, Tokens per Sec:     7794, Lr: 0.000300\n",
      "2021-07-19 09:09:18,254 - INFO - joeynmt.training - Epoch  14, Step:    91700, Batch Loss:     2.797705, Tokens per Sec:     7681, Lr: 0.000300\n",
      "2021-07-19 09:09:47,834 - INFO - joeynmt.training - Epoch  14, Step:    91800, Batch Loss:     2.598845, Tokens per Sec:     7723, Lr: 0.000300\n",
      "2021-07-19 09:10:17,203 - INFO - joeynmt.training - Epoch  14, Step:    91900, Batch Loss:     2.432075, Tokens per Sec:     7609, Lr: 0.000300\n",
      "2021-07-19 09:10:46,870 - INFO - joeynmt.training - Epoch  14, Step:    92000, Batch Loss:     2.473405, Tokens per Sec:     7701, Lr: 0.000300\n",
      "2021-07-19 09:11:16,605 - INFO - joeynmt.training - Epoch  14, Step:    92100, Batch Loss:     2.517340, Tokens per Sec:     7730, Lr: 0.000300\n",
      "2021-07-19 09:11:46,100 - INFO - joeynmt.training - Epoch  14, Step:    92200, Batch Loss:     2.201544, Tokens per Sec:     7679, Lr: 0.000300\n",
      "2021-07-19 09:12:15,551 - INFO - joeynmt.training - Epoch  14, Step:    92300, Batch Loss:     2.372685, Tokens per Sec:     7658, Lr: 0.000300\n",
      "2021-07-19 09:12:45,114 - INFO - joeynmt.training - Epoch  14, Step:    92400, Batch Loss:     2.734512, Tokens per Sec:     7671, Lr: 0.000300\n",
      "2021-07-19 09:13:14,666 - INFO - joeynmt.training - Epoch  14, Step:    92500, Batch Loss:     2.392317, Tokens per Sec:     7575, Lr: 0.000300\n",
      "2021-07-19 09:14:12,239 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:14:12,239 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:14:12,239 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:14:13,453 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:14:13,456 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:14:13,456 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:14:13,456 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw the man who had seen him , and said to them , “ I saw the scribes and the Pharisees and the scribes . ”\n",
      "2021-07-19 09:14:13,456 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:14:13,457 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:14:13,457 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:14:13,457 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary came to Mary , and said to her , “ Mary , and her mother , and her mother , and her mother . ”\n",
      "2021-07-19 09:14:13,458 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:14:13,458 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:14:13,459 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:14:13,459 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a day of distress .\n",
      "2021-07-19 09:14:13,459 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:14:13,460 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:14:13,460 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:14:13,460 - INFO - joeynmt.training - \tHypothesis: Paul also looked for the sower , and when he was sleeping , he was raised up , and he was raised up and sleep on the road .\n",
      "2021-07-19 09:14:13,460 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    92500: bleu:   6.74, loss: 89991.4688, ppl:  13.0497, duration: 58.7936s\n",
      "2021-07-19 09:14:43,079 - INFO - joeynmt.training - Epoch  14, Step:    92600, Batch Loss:     2.434109, Tokens per Sec:     7621, Lr: 0.000300\n",
      "2021-07-19 09:15:12,635 - INFO - joeynmt.training - Epoch  14, Step:    92700, Batch Loss:     2.209276, Tokens per Sec:     7537, Lr: 0.000300\n",
      "2021-07-19 09:15:42,209 - INFO - joeynmt.training - Epoch  14, Step:    92800, Batch Loss:     2.002571, Tokens per Sec:     7598, Lr: 0.000300\n",
      "2021-07-19 09:16:11,576 - INFO - joeynmt.training - Epoch  14, Step:    92900, Batch Loss:     2.627333, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 09:16:33,424 - INFO - joeynmt.training - Epoch  14: total training loss 5783.18\n",
      "2021-07-19 09:16:33,425 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-19 09:16:41,515 - INFO - joeynmt.training - Epoch  15, Step:    93000, Batch Loss:     2.239525, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-07-19 09:17:11,090 - INFO - joeynmt.training - Epoch  15, Step:    93100, Batch Loss:     2.610316, Tokens per Sec:     7575, Lr: 0.000300\n",
      "2021-07-19 09:17:40,847 - INFO - joeynmt.training - Epoch  15, Step:    93200, Batch Loss:     2.544096, Tokens per Sec:     7666, Lr: 0.000300\n",
      "2021-07-19 09:18:10,380 - INFO - joeynmt.training - Epoch  15, Step:    93300, Batch Loss:     2.695454, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-19 09:18:40,151 - INFO - joeynmt.training - Epoch  15, Step:    93400, Batch Loss:     2.518601, Tokens per Sec:     7708, Lr: 0.000300\n",
      "2021-07-19 09:19:09,951 - INFO - joeynmt.training - Epoch  15, Step:    93500, Batch Loss:     2.473676, Tokens per Sec:     7742, Lr: 0.000300\n",
      "2021-07-19 09:19:39,749 - INFO - joeynmt.training - Epoch  15, Step:    93600, Batch Loss:     2.721448, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 09:20:09,284 - INFO - joeynmt.training - Epoch  15, Step:    93700, Batch Loss:     2.521717, Tokens per Sec:     7629, Lr: 0.000300\n",
      "2021-07-19 09:20:39,013 - INFO - joeynmt.training - Epoch  15, Step:    93800, Batch Loss:     2.540029, Tokens per Sec:     7588, Lr: 0.000300\n",
      "2021-07-19 09:21:08,631 - INFO - joeynmt.training - Epoch  15, Step:    93900, Batch Loss:     2.674350, Tokens per Sec:     7709, Lr: 0.000300\n",
      "2021-07-19 09:21:38,498 - INFO - joeynmt.training - Epoch  15, Step:    94000, Batch Loss:     2.589412, Tokens per Sec:     7743, Lr: 0.000300\n",
      "2021-07-19 09:22:08,129 - INFO - joeynmt.training - Epoch  15, Step:    94100, Batch Loss:     2.448942, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-07-19 09:22:37,864 - INFO - joeynmt.training - Epoch  15, Step:    94200, Batch Loss:     2.557356, Tokens per Sec:     7671, Lr: 0.000300\n",
      "2021-07-19 09:23:07,457 - INFO - joeynmt.training - Epoch  15, Step:    94300, Batch Loss:     2.242952, Tokens per Sec:     7673, Lr: 0.000300\n",
      "2021-07-19 09:23:36,676 - INFO - joeynmt.training - Epoch  15, Step:    94400, Batch Loss:     2.513350, Tokens per Sec:     7685, Lr: 0.000300\n",
      "2021-07-19 09:24:06,469 - INFO - joeynmt.training - Epoch  15, Step:    94500, Batch Loss:     2.170829, Tokens per Sec:     7706, Lr: 0.000300\n",
      "2021-07-19 09:24:35,951 - INFO - joeynmt.training - Epoch  15, Step:    94600, Batch Loss:     2.481086, Tokens per Sec:     7587, Lr: 0.000300\n",
      "2021-07-19 09:25:05,115 - INFO - joeynmt.training - Epoch  15, Step:    94700, Batch Loss:     2.446076, Tokens per Sec:     7603, Lr: 0.000300\n",
      "2021-07-19 09:25:34,601 - INFO - joeynmt.training - Epoch  15, Step:    94800, Batch Loss:     2.369297, Tokens per Sec:     7625, Lr: 0.000300\n",
      "2021-07-19 09:26:04,223 - INFO - joeynmt.training - Epoch  15, Step:    94900, Batch Loss:     2.519997, Tokens per Sec:     7728, Lr: 0.000300\n",
      "2021-07-19 09:26:34,042 - INFO - joeynmt.training - Epoch  15, Step:    95000, Batch Loss:     2.376582, Tokens per Sec:     7789, Lr: 0.000300\n",
      "2021-07-19 09:27:28,938 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:27:28,939 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:27:28,939 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:27:29,373 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 09:27:29,374 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 09:27:30,690 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:27:30,691 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:27:30,691 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:27:30,692 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a sword , and said to them , “ I have come to the twelve and come to the tomb . ”\n",
      "2021-07-19 09:27:30,692 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:27:30,692 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:27:30,693 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:27:30,693 - INFO - joeynmt.training - \tHypothesis: Now when Martha had heard these things , Mary and Mary said to Mary , “ Mary , and the Lord , and the Lord , and He was coming to Him . ”\n",
      "2021-07-19 09:27:30,693 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:27:30,694 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:27:30,694 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:27:30,695 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the day of his day .\n",
      "2021-07-19 09:27:30,695 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:27:30,695 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:27:30,696 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:27:30,696 - INFO - joeynmt.training - \tHypothesis: Then he had sat down the sea , and when He was cast , he was cut off , and the sea of the sea , and the sea was cut down .\n",
      "2021-07-19 09:27:30,696 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    95000: bleu:   7.33, loss: 88346.5078, ppl:  12.4511, duration: 56.6532s\n",
      "2021-07-19 09:28:00,143 - INFO - joeynmt.training - Epoch  15, Step:    95100, Batch Loss:     2.509693, Tokens per Sec:     7554, Lr: 0.000300\n",
      "2021-07-19 09:28:29,504 - INFO - joeynmt.training - Epoch  15, Step:    95200, Batch Loss:     2.372368, Tokens per Sec:     7639, Lr: 0.000300\n",
      "2021-07-19 09:28:59,153 - INFO - joeynmt.training - Epoch  15, Step:    95300, Batch Loss:     2.414425, Tokens per Sec:     7632, Lr: 0.000300\n",
      "2021-07-19 09:28:59,492 - INFO - joeynmt.training - Epoch  15: total training loss 5767.28\n",
      "2021-07-19 09:28:59,492 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-19 09:29:29,245 - INFO - joeynmt.training - Epoch  16, Step:    95400, Batch Loss:     2.400470, Tokens per Sec:     7576, Lr: 0.000300\n",
      "2021-07-19 09:29:58,834 - INFO - joeynmt.training - Epoch  16, Step:    95500, Batch Loss:     2.258091, Tokens per Sec:     7607, Lr: 0.000300\n",
      "2021-07-19 09:30:28,613 - INFO - joeynmt.training - Epoch  16, Step:    95600, Batch Loss:     2.302876, Tokens per Sec:     7749, Lr: 0.000300\n",
      "2021-07-19 09:30:58,501 - INFO - joeynmt.training - Epoch  16, Step:    95700, Batch Loss:     2.626441, Tokens per Sec:     7757, Lr: 0.000300\n",
      "2021-07-19 09:31:28,073 - INFO - joeynmt.training - Epoch  16, Step:    95800, Batch Loss:     2.365883, Tokens per Sec:     7721, Lr: 0.000300\n",
      "2021-07-19 09:31:57,986 - INFO - joeynmt.training - Epoch  16, Step:    95900, Batch Loss:     2.765775, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 09:32:27,567 - INFO - joeynmt.training - Epoch  16, Step:    96000, Batch Loss:     2.629493, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-19 09:32:57,113 - INFO - joeynmt.training - Epoch  16, Step:    96100, Batch Loss:     2.107132, Tokens per Sec:     7677, Lr: 0.000300\n",
      "2021-07-19 09:33:26,728 - INFO - joeynmt.training - Epoch  16, Step:    96200, Batch Loss:     2.085318, Tokens per Sec:     7668, Lr: 0.000300\n",
      "2021-07-19 09:33:56,124 - INFO - joeynmt.training - Epoch  16, Step:    96300, Batch Loss:     2.599802, Tokens per Sec:     7695, Lr: 0.000300\n",
      "2021-07-19 09:34:25,747 - INFO - joeynmt.training - Epoch  16, Step:    96400, Batch Loss:     2.388417, Tokens per Sec:     7599, Lr: 0.000300\n",
      "2021-07-19 09:34:55,352 - INFO - joeynmt.training - Epoch  16, Step:    96500, Batch Loss:     2.159601, Tokens per Sec:     7628, Lr: 0.000300\n",
      "2021-07-19 09:35:25,124 - INFO - joeynmt.training - Epoch  16, Step:    96600, Batch Loss:     2.593119, Tokens per Sec:     7646, Lr: 0.000300\n",
      "2021-07-19 09:35:54,750 - INFO - joeynmt.training - Epoch  16, Step:    96700, Batch Loss:     2.707765, Tokens per Sec:     7630, Lr: 0.000300\n",
      "2021-07-19 09:36:24,382 - INFO - joeynmt.training - Epoch  16, Step:    96800, Batch Loss:     2.530856, Tokens per Sec:     7617, Lr: 0.000300\n",
      "2021-07-19 09:36:54,142 - INFO - joeynmt.training - Epoch  16, Step:    96900, Batch Loss:     2.704400, Tokens per Sec:     7718, Lr: 0.000300\n",
      "2021-07-19 09:37:24,194 - INFO - joeynmt.training - Epoch  16, Step:    97000, Batch Loss:     2.550211, Tokens per Sec:     7569, Lr: 0.000300\n",
      "2021-07-19 09:37:53,879 - INFO - joeynmt.training - Epoch  16, Step:    97100, Batch Loss:     2.548666, Tokens per Sec:     7637, Lr: 0.000300\n",
      "2021-07-19 09:38:23,521 - INFO - joeynmt.training - Epoch  16, Step:    97200, Batch Loss:     2.396454, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 09:38:53,317 - INFO - joeynmt.training - Epoch  16, Step:    97300, Batch Loss:     2.479393, Tokens per Sec:     7769, Lr: 0.000300\n",
      "2021-07-19 09:39:22,806 - INFO - joeynmt.training - Epoch  16, Step:    97400, Batch Loss:     2.021215, Tokens per Sec:     7690, Lr: 0.000300\n",
      "2021-07-19 09:39:52,524 - INFO - joeynmt.training - Epoch  16, Step:    97500, Batch Loss:     2.605484, Tokens per Sec:     7616, Lr: 0.000300\n",
      "2021-07-19 09:40:49,447 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:40:49,447 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:40:49,447 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:40:50,635 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:40:50,636 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:40:50,636 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:40:50,636 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He who has seen me , and he has seen me , and he has seen me , and I have come to see my house . ”\n",
      "2021-07-19 09:40:50,636 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:40:50,637 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:40:50,637 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:40:50,637 - INFO - joeynmt.training - \tHypothesis: Then Martha had heard that she had heard her , and she said to her , “ See , and the Lord , and the Lord , and the Lord , and the Lord has come . ”\n",
      "2021-07-19 09:40:50,638 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:40:50,638 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:40:50,638 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:40:50,639 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the Sabbath .\n",
      "2021-07-19 09:40:50,639 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:40:50,640 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:40:50,640 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:40:50,640 - INFO - joeynmt.training - \tHypothesis: Paul also had a full full share in the field when he was on the road , and he was a brief car , and he was a brief car .\n",
      "2021-07-19 09:40:50,640 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    97500: bleu:   6.85, loss: 89012.3984, ppl:  12.6900, duration: 58.1158s\n",
      "2021-07-19 09:41:20,327 - INFO - joeynmt.training - Epoch  16, Step:    97600, Batch Loss:     2.474289, Tokens per Sec:     7569, Lr: 0.000300\n",
      "2021-07-19 09:41:26,988 - INFO - joeynmt.training - Epoch  16: total training loss 5745.07\n",
      "2021-07-19 09:41:26,989 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-19 09:41:49,866 - INFO - joeynmt.training - Epoch  17, Step:    97700, Batch Loss:     2.487382, Tokens per Sec:     7561, Lr: 0.000300\n",
      "2021-07-19 09:42:19,359 - INFO - joeynmt.training - Epoch  17, Step:    97800, Batch Loss:     2.204356, Tokens per Sec:     7641, Lr: 0.000300\n",
      "2021-07-19 09:42:49,233 - INFO - joeynmt.training - Epoch  17, Step:    97900, Batch Loss:     2.458670, Tokens per Sec:     7830, Lr: 0.000300\n",
      "2021-07-19 09:43:18,850 - INFO - joeynmt.training - Epoch  17, Step:    98000, Batch Loss:     2.601521, Tokens per Sec:     7586, Lr: 0.000300\n",
      "2021-07-19 09:43:48,638 - INFO - joeynmt.training - Epoch  17, Step:    98100, Batch Loss:     2.450954, Tokens per Sec:     7652, Lr: 0.000300\n",
      "2021-07-19 09:44:18,371 - INFO - joeynmt.training - Epoch  17, Step:    98200, Batch Loss:     2.485296, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-19 09:44:47,738 - INFO - joeynmt.training - Epoch  17, Step:    98300, Batch Loss:     2.495255, Tokens per Sec:     7549, Lr: 0.000300\n",
      "2021-07-19 09:45:17,489 - INFO - joeynmt.training - Epoch  17, Step:    98400, Batch Loss:     2.449237, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 09:45:47,362 - INFO - joeynmt.training - Epoch  17, Step:    98500, Batch Loss:     2.124894, Tokens per Sec:     7759, Lr: 0.000300\n",
      "2021-07-19 09:46:17,085 - INFO - joeynmt.training - Epoch  17, Step:    98600, Batch Loss:     2.476737, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 09:46:46,711 - INFO - joeynmt.training - Epoch  17, Step:    98700, Batch Loss:     1.896112, Tokens per Sec:     7676, Lr: 0.000300\n",
      "2021-07-19 09:47:16,478 - INFO - joeynmt.training - Epoch  17, Step:    98800, Batch Loss:     2.451104, Tokens per Sec:     7641, Lr: 0.000300\n",
      "2021-07-19 09:47:46,206 - INFO - joeynmt.training - Epoch  17, Step:    98900, Batch Loss:     2.654869, Tokens per Sec:     7653, Lr: 0.000300\n",
      "2021-07-19 09:48:15,994 - INFO - joeynmt.training - Epoch  17, Step:    99000, Batch Loss:     2.186166, Tokens per Sec:     7722, Lr: 0.000300\n",
      "2021-07-19 09:48:45,568 - INFO - joeynmt.training - Epoch  17, Step:    99100, Batch Loss:     2.512928, Tokens per Sec:     7688, Lr: 0.000300\n",
      "2021-07-19 09:49:15,211 - INFO - joeynmt.training - Epoch  17, Step:    99200, Batch Loss:     2.553317, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 09:49:44,787 - INFO - joeynmt.training - Epoch  17, Step:    99300, Batch Loss:     2.547517, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 09:50:14,120 - INFO - joeynmt.training - Epoch  17, Step:    99400, Batch Loss:     2.581047, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-07-19 09:50:43,964 - INFO - joeynmt.training - Epoch  17, Step:    99500, Batch Loss:     2.734441, Tokens per Sec:     7638, Lr: 0.000300\n",
      "2021-07-19 09:51:13,372 - INFO - joeynmt.training - Epoch  17, Step:    99600, Batch Loss:     2.587189, Tokens per Sec:     7710, Lr: 0.000300\n",
      "2021-07-19 09:51:43,164 - INFO - joeynmt.training - Epoch  17, Step:    99700, Batch Loss:     2.406415, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-19 09:52:12,840 - INFO - joeynmt.training - Epoch  17, Step:    99800, Batch Loss:     2.209472, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 09:52:42,323 - INFO - joeynmt.training - Epoch  17, Step:    99900, Batch Loss:     2.519387, Tokens per Sec:     7755, Lr: 0.000300\n",
      "2021-07-19 09:52:56,499 - INFO - joeynmt.training - Epoch  17: total training loss 5744.76\n",
      "2021-07-19 09:52:56,499 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-19 09:53:12,262 - INFO - joeynmt.training - Epoch  18, Step:   100000, Batch Loss:     2.393476, Tokens per Sec:     7527, Lr: 0.000300\n",
      "2021-07-19 09:54:10,470 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:54:10,470 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:54:10,470 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:54:11,718 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:54:11,719 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:54:11,720 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:54:11,720 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a man , saying , “ I have come to the scribes and the scribes and the scribes and the scribes and the scribes . ”\n",
      "2021-07-19 09:54:11,720 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:54:11,721 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:54:11,721 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:54:11,721 - INFO - joeynmt.training - \tHypothesis: Now Martha , who had heard the voice of Mary , and Martha , said to Mary , “ See , and the Lord , and you are going to see him . ”\n",
      "2021-07-19 09:54:11,722 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:54:11,722 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:54:11,722 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:54:11,723 - INFO - joeynmt.training - \tHypothesis: He was a day of service in the day of the day of the end .\n",
      "2021-07-19 09:54:11,723 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:54:11,724 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:54:11,724 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:54:11,724 - INFO - joeynmt.training - \tHypothesis: Paul also had sound reasons for the sea , and when he was cast , he was cut off from the sea , and the fire was cut down .\n",
      "2021-07-19 09:54:11,724 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   100000: bleu:   6.88, loss: 88687.5234, ppl:  12.5729, duration: 59.4615s\n",
      "2021-07-19 09:54:41,490 - INFO - joeynmt.training - Epoch  18, Step:   100100, Batch Loss:     2.323984, Tokens per Sec:     7548, Lr: 0.000300\n",
      "2021-07-19 09:55:11,146 - INFO - joeynmt.training - Epoch  18, Step:   100200, Batch Loss:     2.231801, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 09:55:40,915 - INFO - joeynmt.training - Epoch  18, Step:   100300, Batch Loss:     2.398235, Tokens per Sec:     7692, Lr: 0.000300\n",
      "2021-07-19 09:56:10,578 - INFO - joeynmt.training - Epoch  18, Step:   100400, Batch Loss:     2.799313, Tokens per Sec:     7627, Lr: 0.000300\n",
      "2021-07-19 09:56:40,021 - INFO - joeynmt.training - Epoch  18, Step:   100500, Batch Loss:     2.515724, Tokens per Sec:     7617, Lr: 0.000300\n",
      "2021-07-19 09:57:09,510 - INFO - joeynmt.training - Epoch  18, Step:   100600, Batch Loss:     2.649131, Tokens per Sec:     7588, Lr: 0.000300\n",
      "2021-07-19 09:57:38,953 - INFO - joeynmt.training - Epoch  18, Step:   100700, Batch Loss:     2.241764, Tokens per Sec:     7574, Lr: 0.000300\n",
      "2021-07-19 09:58:08,706 - INFO - joeynmt.training - Epoch  18, Step:   100800, Batch Loss:     2.530624, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 09:58:38,928 - INFO - joeynmt.training - Epoch  18, Step:   100900, Batch Loss:     2.456237, Tokens per Sec:     7752, Lr: 0.000300\n",
      "2021-07-19 09:59:08,323 - INFO - joeynmt.training - Epoch  18, Step:   101000, Batch Loss:     2.249847, Tokens per Sec:     7663, Lr: 0.000300\n",
      "2021-07-19 09:59:38,082 - INFO - joeynmt.training - Epoch  18, Step:   101100, Batch Loss:     2.606418, Tokens per Sec:     7699, Lr: 0.000300\n",
      "2021-07-19 10:00:07,760 - INFO - joeynmt.training - Epoch  18, Step:   101200, Batch Loss:     2.375940, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-07-19 10:00:37,124 - INFO - joeynmt.training - Epoch  18, Step:   101300, Batch Loss:     2.274651, Tokens per Sec:     7563, Lr: 0.000300\n",
      "2021-07-19 10:01:06,651 - INFO - joeynmt.training - Epoch  18, Step:   101400, Batch Loss:     2.506844, Tokens per Sec:     7744, Lr: 0.000300\n",
      "2021-07-19 10:01:36,430 - INFO - joeynmt.training - Epoch  18, Step:   101500, Batch Loss:     2.397336, Tokens per Sec:     7660, Lr: 0.000300\n",
      "2021-07-19 10:02:06,013 - INFO - joeynmt.training - Epoch  18, Step:   101600, Batch Loss:     2.388252, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-19 10:02:35,637 - INFO - joeynmt.training - Epoch  18, Step:   101700, Batch Loss:     2.460960, Tokens per Sec:     7721, Lr: 0.000300\n",
      "2021-07-19 10:03:05,420 - INFO - joeynmt.training - Epoch  18, Step:   101800, Batch Loss:     2.351370, Tokens per Sec:     7746, Lr: 0.000300\n",
      "2021-07-19 10:03:34,965 - INFO - joeynmt.training - Epoch  18, Step:   101900, Batch Loss:     2.537027, Tokens per Sec:     7633, Lr: 0.000300\n",
      "2021-07-19 10:04:04,680 - INFO - joeynmt.training - Epoch  18, Step:   102000, Batch Loss:     2.779261, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 10:04:34,406 - INFO - joeynmt.training - Epoch  18, Step:   102100, Batch Loss:     2.597404, Tokens per Sec:     7655, Lr: 0.000300\n",
      "2021-07-19 10:05:04,108 - INFO - joeynmt.training - Epoch  18, Step:   102200, Batch Loss:     2.613183, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 10:05:25,292 - INFO - joeynmt.training - Epoch  18: total training loss 5732.58\n",
      "2021-07-19 10:05:25,293 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-19 10:05:33,895 - INFO - joeynmt.training - Epoch  19, Step:   102300, Batch Loss:     2.619388, Tokens per Sec:     7247, Lr: 0.000300\n",
      "2021-07-19 10:06:03,732 - INFO - joeynmt.training - Epoch  19, Step:   102400, Batch Loss:     2.434654, Tokens per Sec:     7686, Lr: 0.000300\n",
      "2021-07-19 10:06:33,734 - INFO - joeynmt.training - Epoch  19, Step:   102500, Batch Loss:     2.279320, Tokens per Sec:     7720, Lr: 0.000300\n",
      "2021-07-19 10:07:37,379 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 10:07:37,380 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 10:07:37,380 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 10:07:38,631 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 10:07:38,632 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 10:07:38,632 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 10:07:38,632 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw him as He said to them , “ I have come to the seven hundred and fell into the tomb , and I have come to you . ”\n",
      "2021-07-19 10:07:38,633 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 10:07:38,633 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 10:07:38,634 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 10:07:38,634 - INFO - joeynmt.training - \tHypothesis: When Martha was a man , Mary and Mary , and Mary , and Mary , and Mary said to her , “ See , and you are going to know where she was going . ”\n",
      "2021-07-19 10:07:38,634 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 10:07:38,635 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 10:07:38,635 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 10:07:38,635 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the Sabbath , and he was a day .\n",
      "2021-07-19 10:07:38,636 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 10:07:38,636 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 10:07:38,636 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 10:07:38,637 - INFO - joeynmt.training - \tHypothesis: Paul also looked for the sake of the sea , and when he was cut off , he was a fire of the sea , and the sea was cut off the road .\n",
      "2021-07-19 10:07:38,637 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   102500: bleu:   6.56, loss: 89562.3203, ppl:  12.8908, duration: 64.9026s\n",
      "2021-07-19 10:08:08,361 - INFO - joeynmt.training - Epoch  19, Step:   102600, Batch Loss:     2.602778, Tokens per Sec:     7550, Lr: 0.000300\n",
      "2021-07-19 10:08:37,791 - INFO - joeynmt.training - Epoch  19, Step:   102700, Batch Loss:     2.351069, Tokens per Sec:     7582, Lr: 0.000300\n",
      "2021-07-19 10:09:07,441 - INFO - joeynmt.training - Epoch  19, Step:   102800, Batch Loss:     2.479653, Tokens per Sec:     7694, Lr: 0.000300\n",
      "2021-07-19 10:09:37,309 - INFO - joeynmt.training - Epoch  19, Step:   102900, Batch Loss:     2.525683, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 10:10:07,216 - INFO - joeynmt.training - Epoch  19, Step:   103000, Batch Loss:     2.477801, Tokens per Sec:     7783, Lr: 0.000300\n",
      "2021-07-19 10:10:36,563 - INFO - joeynmt.training - Epoch  19, Step:   103100, Batch Loss:     2.384779, Tokens per Sec:     7600, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/back_transformer_reverse_lhen_reload2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFztkp8YeODE"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 102500\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/models/lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_{name}_reverse_transformer_continued2/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/back_lhen_reverse_transformer\"', f'model_dir: \"models/back_lhen_reverse_transformer_continued3\"').replace(\n",
    "            f'validation_freq: 5000', f'validation_freq: 2500').replace(\n",
    "            f'epochs: 30', f'epochs: 11')\n",
    "with open(\"joeynmt/configs/back_transformer_reverse_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "aY_QjWww2lS0",
    "outputId": "8e43604c-b8b3-4a71-c611-9b0776a98ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued2/102500.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 11                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/back_lhen_reverse_transformer_continued3\"\n",
      "    overwrite: False              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/back_transformer_reverse_lhen_reload3.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLFcAIfy2owA",
    "outputId": "18ff262d-5ec3-4f8f-cbe5-1ccace8b2d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-27 07:34:56,135 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-27 07:34:56,204 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-27 07:35:00,825 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-27 07:35:01,381 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-27 07:35:02,417 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-27 07:35:03,772 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-27 07:35:03,772 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-27 07:35:04,161 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-27 07:35:04.414349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-27 07:35:06,069 - INFO - joeynmt.training - Total params: 12138240\n",
      "2021-07-27 07:35:16,618 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued2/102500.ckpt\n",
      "2021-07-27 07:35:17,082 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued2/102500.ckpt\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.epochs                : 11\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/back_lhen_reverse_transformer_continued3\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 207289,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] P@@ il@@ a@@ to nakal@@ ukha itookho , ne nal@@ anga Yesu , namureeba , ari , “ Iwe ni@@ we omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then P@@ il@@ ate ent@@ ered the P@@ ra@@ et@@ or@@ i@@ u@@ m again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-27 07:35:17,093 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-27 07:35:17,093 - INFO - joeynmt.helpers - Number of Src words (types): 4211\n",
      "2021-07-27 07:35:17,093 - INFO - joeynmt.helpers - Number of Trg words (types): 4211\n",
      "2021-07-27 07:35:17,093 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4211),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4211))\n",
      "2021-07-27 07:35:17,120 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-27 07:35:17,120 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-27 07:35:30,485 - INFO - joeynmt.training - Epoch   1, Step:   102600, Batch Loss:     2.609651, Tokens per Sec:    16793, Lr: 0.000300\n",
      "2021-07-27 07:35:43,073 - INFO - joeynmt.training - Epoch   1, Step:   102700, Batch Loss:     2.340782, Tokens per Sec:    17726, Lr: 0.000300\n",
      "2021-07-27 07:35:55,822 - INFO - joeynmt.training - Epoch   1, Step:   102800, Batch Loss:     2.479663, Tokens per Sec:    17893, Lr: 0.000300\n",
      "2021-07-27 07:36:08,760 - INFO - joeynmt.training - Epoch   1, Step:   102900, Batch Loss:     2.559927, Tokens per Sec:    17716, Lr: 0.000300\n",
      "2021-07-27 07:36:21,929 - INFO - joeynmt.training - Epoch   1, Step:   103000, Batch Loss:     2.483108, Tokens per Sec:    17675, Lr: 0.000300\n",
      "2021-07-27 07:36:34,979 - INFO - joeynmt.training - Epoch   1, Step:   103100, Batch Loss:     2.374309, Tokens per Sec:    17093, Lr: 0.000300\n",
      "2021-07-27 07:36:48,146 - INFO - joeynmt.training - Epoch   1, Step:   103200, Batch Loss:     2.598075, Tokens per Sec:    17407, Lr: 0.000300\n",
      "2021-07-27 07:37:01,223 - INFO - joeynmt.training - Epoch   1, Step:   103300, Batch Loss:     2.453875, Tokens per Sec:    17314, Lr: 0.000300\n",
      "2021-07-27 07:37:14,243 - INFO - joeynmt.training - Epoch   1, Step:   103400, Batch Loss:     2.434888, Tokens per Sec:    17157, Lr: 0.000300\n",
      "2021-07-27 07:37:27,473 - INFO - joeynmt.training - Epoch   1, Step:   103500, Batch Loss:     2.692916, Tokens per Sec:    16893, Lr: 0.000300\n",
      "2021-07-27 07:37:40,924 - INFO - joeynmt.training - Epoch   1, Step:   103600, Batch Loss:     2.679719, Tokens per Sec:    17190, Lr: 0.000300\n",
      "2021-07-27 07:37:54,152 - INFO - joeynmt.training - Epoch   1, Step:   103700, Batch Loss:     2.170151, Tokens per Sec:    16982, Lr: 0.000300\n",
      "2021-07-27 07:38:07,514 - INFO - joeynmt.training - Epoch   1, Step:   103800, Batch Loss:     2.432665, Tokens per Sec:    17144, Lr: 0.000300\n",
      "2021-07-27 07:38:20,800 - INFO - joeynmt.training - Epoch   1, Step:   103900, Batch Loss:     2.363423, Tokens per Sec:    17083, Lr: 0.000300\n",
      "2021-07-27 07:38:34,285 - INFO - joeynmt.training - Epoch   1, Step:   104000, Batch Loss:     2.558351, Tokens per Sec:    16915, Lr: 0.000300\n",
      "2021-07-27 07:38:47,765 - INFO - joeynmt.training - Epoch   1, Step:   104100, Batch Loss:     2.478926, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-27 07:39:01,192 - INFO - joeynmt.training - Epoch   1, Step:   104200, Batch Loss:     1.944168, Tokens per Sec:    16905, Lr: 0.000300\n",
      "2021-07-27 07:39:14,641 - INFO - joeynmt.training - Epoch   1, Step:   104300, Batch Loss:     2.703756, Tokens per Sec:    16789, Lr: 0.000300\n",
      "2021-07-27 07:39:28,042 - INFO - joeynmt.training - Epoch   1, Step:   104400, Batch Loss:     2.555315, Tokens per Sec:    16959, Lr: 0.000300\n",
      "2021-07-27 07:39:41,764 - INFO - joeynmt.training - Epoch   1, Step:   104500, Batch Loss:     2.394606, Tokens per Sec:    16850, Lr: 0.000300\n",
      "2021-07-27 07:39:54,797 - INFO - joeynmt.training - Epoch   1: total training loss 5166.13\n",
      "2021-07-27 07:39:54,798 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-27 07:39:55,643 - INFO - joeynmt.training - Epoch   2, Step:   104600, Batch Loss:     2.137786, Tokens per Sec:    10041, Lr: 0.000300\n",
      "2021-07-27 07:40:09,400 - INFO - joeynmt.training - Epoch   2, Step:   104700, Batch Loss:     2.663298, Tokens per Sec:    16846, Lr: 0.000300\n",
      "2021-07-27 07:40:23,151 - INFO - joeynmt.training - Epoch   2, Step:   104800, Batch Loss:     2.565871, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-27 07:40:36,687 - INFO - joeynmt.training - Epoch   2, Step:   104900, Batch Loss:     2.693704, Tokens per Sec:    16769, Lr: 0.000300\n",
      "2021-07-27 07:40:50,194 - INFO - joeynmt.training - Epoch   2, Step:   105000, Batch Loss:     2.621908, Tokens per Sec:    16621, Lr: 0.000300\n",
      "2021-07-27 07:41:18,340 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 07:41:18,340 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 07:41:18,340 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 07:41:19,464 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 07:41:19,465 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 07:41:19,465 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 07:41:19,465 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He saw the scribes and said to them , “ I have come to the tomb and come to the tomb , and I have come to know where I have come . ”\n",
      "2021-07-27 07:41:19,465 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 07:41:19,466 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 07:41:19,466 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 07:41:19,466 - INFO - joeynmt.training - \tHypothesis: Now Martha , who had heard this , and said to Mary , “ Mary , and Mary , and Mary , and He said to Him , “ Teacher , and you are going to know Him . ”\n",
      "2021-07-27 07:41:19,466 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 07:41:19,467 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 07:41:19,467 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 07:41:19,467 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and the day was near .\n",
      "2021-07-27 07:41:19,467 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 07:41:19,468 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 07:41:19,468 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 07:41:19,468 - INFO - joeynmt.training - \tHypothesis: And when Paul had looked at the sea , He was cut off , and was cut off , and the lamp of the sea , and the lamb was cut down .\n",
      "2021-07-27 07:41:19,468 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   105000: bleu:   6.82, loss: 88786.8828, ppl:  12.6086, duration: 29.2742s\n",
      "2021-07-27 07:41:33,367 - INFO - joeynmt.training - Epoch   2, Step:   105100, Batch Loss:     2.434491, Tokens per Sec:    16282, Lr: 0.000300\n",
      "2021-07-27 07:41:46,948 - INFO - joeynmt.training - Epoch   2, Step:   105200, Batch Loss:     2.607318, Tokens per Sec:    16699, Lr: 0.000300\n",
      "2021-07-27 07:42:00,577 - INFO - joeynmt.training - Epoch   2, Step:   105300, Batch Loss:     2.078145, Tokens per Sec:    17058, Lr: 0.000300\n",
      "2021-07-27 07:42:14,159 - INFO - joeynmt.training - Epoch   2, Step:   105400, Batch Loss:     2.669848, Tokens per Sec:    16773, Lr: 0.000300\n",
      "2021-07-27 07:42:27,609 - INFO - joeynmt.training - Epoch   2, Step:   105500, Batch Loss:     2.589326, Tokens per Sec:    16701, Lr: 0.000300\n",
      "2021-07-27 07:42:41,223 - INFO - joeynmt.training - Epoch   2, Step:   105600, Batch Loss:     2.472005, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-27 07:42:54,833 - INFO - joeynmt.training - Epoch   2, Step:   105700, Batch Loss:     2.469747, Tokens per Sec:    17097, Lr: 0.000300\n",
      "2021-07-27 07:43:08,371 - INFO - joeynmt.training - Epoch   2, Step:   105800, Batch Loss:     2.285844, Tokens per Sec:    16956, Lr: 0.000300\n",
      "2021-07-27 07:43:21,750 - INFO - joeynmt.training - Epoch   2, Step:   105900, Batch Loss:     2.289291, Tokens per Sec:    16640, Lr: 0.000300\n",
      "2021-07-27 07:43:35,298 - INFO - joeynmt.training - Epoch   2, Step:   106000, Batch Loss:     2.290530, Tokens per Sec:    16402, Lr: 0.000300\n",
      "2021-07-27 07:43:48,786 - INFO - joeynmt.training - Epoch   2, Step:   106100, Batch Loss:     2.410551, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-07-27 07:44:02,378 - INFO - joeynmt.training - Epoch   2, Step:   106200, Batch Loss:     2.623958, Tokens per Sec:    17034, Lr: 0.000300\n",
      "2021-07-27 07:44:15,896 - INFO - joeynmt.training - Epoch   2, Step:   106300, Batch Loss:     2.604288, Tokens per Sec:    17055, Lr: 0.000300\n",
      "2021-07-27 07:44:29,535 - INFO - joeynmt.training - Epoch   2, Step:   106400, Batch Loss:     2.652615, Tokens per Sec:    16843, Lr: 0.000300\n",
      "2021-07-27 07:44:43,296 - INFO - joeynmt.training - Epoch   2, Step:   106500, Batch Loss:     2.174897, Tokens per Sec:    16651, Lr: 0.000300\n",
      "2021-07-27 07:44:56,776 - INFO - joeynmt.training - Epoch   2, Step:   106600, Batch Loss:     2.476755, Tokens per Sec:    16465, Lr: 0.000300\n",
      "2021-07-27 07:45:10,526 - INFO - joeynmt.training - Epoch   2, Step:   106700, Batch Loss:     2.565251, Tokens per Sec:    16916, Lr: 0.000300\n",
      "2021-07-27 07:45:24,016 - INFO - joeynmt.training - Epoch   2, Step:   106800, Batch Loss:     2.618560, Tokens per Sec:    16957, Lr: 0.000300\n",
      "2021-07-27 07:45:37,675 - INFO - joeynmt.training - Epoch   2, Step:   106900, Batch Loss:     2.347380, Tokens per Sec:    16860, Lr: 0.000300\n",
      "2021-07-27 07:45:39,561 - INFO - joeynmt.training - Epoch   2: total training loss 5700.36\n",
      "2021-07-27 07:45:39,562 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-27 07:45:51,576 - INFO - joeynmt.training - Epoch   3, Step:   107000, Batch Loss:     2.513559, Tokens per Sec:    16251, Lr: 0.000300\n",
      "2021-07-27 07:46:05,296 - INFO - joeynmt.training - Epoch   3, Step:   107100, Batch Loss:     2.559005, Tokens per Sec:    16770, Lr: 0.000300\n",
      "2021-07-27 07:46:18,994 - INFO - joeynmt.training - Epoch   3, Step:   107200, Batch Loss:     2.490681, Tokens per Sec:    16652, Lr: 0.000300\n",
      "2021-07-27 07:46:32,661 - INFO - joeynmt.training - Epoch   3, Step:   107300, Batch Loss:     2.635666, Tokens per Sec:    16760, Lr: 0.000300\n",
      "2021-07-27 07:46:46,004 - INFO - joeynmt.training - Epoch   3, Step:   107400, Batch Loss:     2.700786, Tokens per Sec:    16620, Lr: 0.000300\n",
      "2021-07-27 07:46:59,536 - INFO - joeynmt.training - Epoch   3, Step:   107500, Batch Loss:     2.513614, Tokens per Sec:    16878, Lr: 0.000300\n",
      "2021-07-27 07:47:30,002 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 07:47:30,002 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 07:47:30,003 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 07:47:31,567 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 07:47:31,568 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 07:47:31,568 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 07:47:31,568 - INFO - joeynmt.training - \tHypothesis: The Pharisees , who was like a man , and He saw it , saying , “ I have come to the throne , and I have come to see the treasure of my house . ”\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary came to Mary , and said to her , “ Lord , and we have heard Him , and He is coming . ”\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 07:47:31,570 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 07:47:31,570 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 07:47:31,570 - INFO - joeynmt.training - \tHypothesis: He was a great day of service to the Sabbath , and he was happy .\n",
      "2021-07-27 07:47:31,570 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 07:47:31,571 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 07:47:31,571 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 07:47:31,571 - INFO - joeynmt.training - \tHypothesis: Paul also looked for the sea , and when He was cut off , he was cut off , and was cut off , and the sea was cut down with the road .\n",
      "2021-07-27 07:47:31,571 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   107500: bleu:   6.55, loss: 89133.1953, ppl:  12.7338, duration: 32.0352s\n",
      "2021-07-27 07:47:45,491 - INFO - joeynmt.training - Epoch   3, Step:   107600, Batch Loss:     2.658170, Tokens per Sec:    16492, Lr: 0.000300\n",
      "2021-07-27 07:47:59,096 - INFO - joeynmt.training - Epoch   3, Step:   107700, Batch Loss:     2.448322, Tokens per Sec:    16848, Lr: 0.000300\n",
      "2021-07-27 07:48:12,719 - INFO - joeynmt.training - Epoch   3, Step:   107800, Batch Loss:     2.495867, Tokens per Sec:    16405, Lr: 0.000300\n",
      "2021-07-27 07:48:26,669 - INFO - joeynmt.training - Epoch   3, Step:   107900, Batch Loss:     2.340499, Tokens per Sec:    16275, Lr: 0.000300\n",
      "2021-07-27 07:48:40,284 - INFO - joeynmt.training - Epoch   3, Step:   108000, Batch Loss:     2.409793, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-07-27 07:48:53,851 - INFO - joeynmt.training - Epoch   3, Step:   108100, Batch Loss:     2.406248, Tokens per Sec:    16458, Lr: 0.000300\n",
      "2021-07-27 07:49:07,540 - INFO - joeynmt.training - Epoch   3, Step:   108200, Batch Loss:     2.492194, Tokens per Sec:    16677, Lr: 0.000300\n",
      "2021-07-27 07:49:21,122 - INFO - joeynmt.training - Epoch   3, Step:   108300, Batch Loss:     2.565594, Tokens per Sec:    16606, Lr: 0.000300\n",
      "2021-07-27 07:49:34,890 - INFO - joeynmt.training - Epoch   3, Step:   108400, Batch Loss:     2.356618, Tokens per Sec:    16429, Lr: 0.000300\n",
      "2021-07-27 07:49:48,740 - INFO - joeynmt.training - Epoch   3, Step:   108500, Batch Loss:     2.406695, Tokens per Sec:    16456, Lr: 0.000300\n",
      "2021-07-27 07:50:02,461 - INFO - joeynmt.training - Epoch   3, Step:   108600, Batch Loss:     2.237736, Tokens per Sec:    16645, Lr: 0.000300\n",
      "2021-07-27 07:50:16,053 - INFO - joeynmt.training - Epoch   3, Step:   108700, Batch Loss:     2.425822, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-07-27 07:50:29,848 - INFO - joeynmt.training - Epoch   3, Step:   108800, Batch Loss:     2.439195, Tokens per Sec:    16767, Lr: 0.000300\n",
      "2021-07-27 07:50:43,636 - INFO - joeynmt.training - Epoch   3, Step:   108900, Batch Loss:     2.118551, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-27 07:50:57,351 - INFO - joeynmt.training - Epoch   3, Step:   109000, Batch Loss:     2.746655, Tokens per Sec:    16424, Lr: 0.000300\n",
      "2021-07-27 07:51:10,951 - INFO - joeynmt.training - Epoch   3, Step:   109100, Batch Loss:     2.127923, Tokens per Sec:    16432, Lr: 0.000300\n",
      "2021-07-27 07:51:24,736 - INFO - joeynmt.training - Epoch   3, Step:   109200, Batch Loss:     2.502550, Tokens per Sec:    16355, Lr: 0.000300\n",
      "2021-07-27 07:51:30,680 - INFO - joeynmt.training - Epoch   3: total training loss 5720.83\n",
      "2021-07-27 07:51:30,681 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-27 07:51:38,635 - INFO - joeynmt.training - Epoch   4, Step:   109300, Batch Loss:     2.519888, Tokens per Sec:    16185, Lr: 0.000300\n",
      "2021-07-27 07:51:52,337 - INFO - joeynmt.training - Epoch   4, Step:   109400, Batch Loss:     2.403097, Tokens per Sec:    16786, Lr: 0.000300\n",
      "2021-07-27 07:52:05,762 - INFO - joeynmt.training - Epoch   4, Step:   109500, Batch Loss:     2.587027, Tokens per Sec:    16613, Lr: 0.000300\n",
      "2021-07-27 07:52:19,510 - INFO - joeynmt.training - Epoch   4, Step:   109600, Batch Loss:     2.309095, Tokens per Sec:    16324, Lr: 0.000300\n",
      "2021-07-27 07:52:33,271 - INFO - joeynmt.training - Epoch   4, Step:   109700, Batch Loss:     2.468905, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-27 07:52:46,868 - INFO - joeynmt.training - Epoch   4, Step:   109800, Batch Loss:     2.196414, Tokens per Sec:    16583, Lr: 0.000300\n",
      "2021-07-27 07:53:00,410 - INFO - joeynmt.training - Epoch   4, Step:   109900, Batch Loss:     2.099122, Tokens per Sec:    16584, Lr: 0.000300\n",
      "2021-07-27 07:53:14,055 - INFO - joeynmt.training - Epoch   4, Step:   110000, Batch Loss:     2.599920, Tokens per Sec:    16618, Lr: 0.000300\n",
      "2021-07-27 07:53:41,054 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 07:53:41,054 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 07:53:41,054 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 07:53:42,124 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 07:53:42,125 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 07:53:42,125 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 07:53:42,125 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees , who was like a man , said to them , “ I saw the scribes and the scribes and the Pharisees , and I saw him . ”\n",
      "2021-07-27 07:53:42,125 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 07:53:42,126 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 07:53:42,126 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 07:53:42,126 - INFO - joeynmt.training - \tHypothesis: When Martha had heard this , Mary and Mary came to Mary , and said to her , “ Teacher , and she saw Him , and she was going to see Him . ”\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - \tHypothesis: Jehovah has provided the day for the day of his day .\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 07:53:42,128 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 07:53:42,128 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 07:53:42,128 - INFO - joeynmt.training - \tHypothesis: Then Paul had sat down the tree , and when He had sat down , He was cut down , and the fire was cut down , and the sea was cut down .\n",
      "2021-07-27 07:53:42,128 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   110000: bleu:   7.29, loss: 88425.1719, ppl:  12.4791, duration: 28.0729s\n",
      "2021-07-27 07:53:55,873 - INFO - joeynmt.training - Epoch   4, Step:   110100, Batch Loss:     2.473654, Tokens per Sec:    16326, Lr: 0.000210\n",
      "2021-07-27 07:54:09,407 - INFO - joeynmt.training - Epoch   4, Step:   110200, Batch Loss:     2.525152, Tokens per Sec:    16803, Lr: 0.000210\n",
      "2021-07-27 07:54:23,131 - INFO - joeynmt.training - Epoch   4, Step:   110300, Batch Loss:     2.483147, Tokens per Sec:    16741, Lr: 0.000210\n",
      "2021-07-27 07:54:36,900 - INFO - joeynmt.training - Epoch   4, Step:   110400, Batch Loss:     1.823732, Tokens per Sec:    16851, Lr: 0.000210\n",
      "2021-07-27 07:54:50,514 - INFO - joeynmt.training - Epoch   4, Step:   110500, Batch Loss:     2.579154, Tokens per Sec:    16658, Lr: 0.000210\n",
      "2021-07-27 07:55:04,158 - INFO - joeynmt.training - Epoch   4, Step:   110600, Batch Loss:     2.603118, Tokens per Sec:    16791, Lr: 0.000210\n",
      "2021-07-27 07:55:17,622 - INFO - joeynmt.training - Epoch   4, Step:   110700, Batch Loss:     2.265122, Tokens per Sec:    16810, Lr: 0.000210\n",
      "2021-07-27 07:55:31,280 - INFO - joeynmt.training - Epoch   4, Step:   110800, Batch Loss:     2.386464, Tokens per Sec:    16834, Lr: 0.000210\n",
      "2021-07-27 07:55:44,970 - INFO - joeynmt.training - Epoch   4, Step:   110900, Batch Loss:     2.455701, Tokens per Sec:    16401, Lr: 0.000210\n",
      "2021-07-27 07:55:58,534 - INFO - joeynmt.training - Epoch   4, Step:   111000, Batch Loss:     2.312092, Tokens per Sec:    16585, Lr: 0.000210\n",
      "2021-07-27 07:56:12,055 - INFO - joeynmt.training - Epoch   4, Step:   111100, Batch Loss:     2.401133, Tokens per Sec:    16595, Lr: 0.000210\n",
      "2021-07-27 07:56:25,583 - INFO - joeynmt.training - Epoch   4, Step:   111200, Batch Loss:     2.692233, Tokens per Sec:    16948, Lr: 0.000210\n",
      "2021-07-27 07:56:39,212 - INFO - joeynmt.training - Epoch   4, Step:   111300, Batch Loss:     2.238806, Tokens per Sec:    16576, Lr: 0.000210\n",
      "2021-07-27 07:56:52,864 - INFO - joeynmt.training - Epoch   4, Step:   111400, Batch Loss:     2.447489, Tokens per Sec:    16825, Lr: 0.000210\n",
      "2021-07-27 07:57:06,452 - INFO - joeynmt.training - Epoch   4, Step:   111500, Batch Loss:     2.434512, Tokens per Sec:    16524, Lr: 0.000210\n",
      "2021-07-27 07:57:15,966 - INFO - joeynmt.training - Epoch   4: total training loss 5691.36\n",
      "2021-07-27 07:57:15,966 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-27 07:57:20,234 - INFO - joeynmt.training - Epoch   5, Step:   111600, Batch Loss:     2.483492, Tokens per Sec:    16158, Lr: 0.000210\n",
      "2021-07-27 07:57:33,961 - INFO - joeynmt.training - Epoch   5, Step:   111700, Batch Loss:     2.449212, Tokens per Sec:    16929, Lr: 0.000210\n",
      "2021-07-27 07:57:47,472 - INFO - joeynmt.training - Epoch   5, Step:   111800, Batch Loss:     2.350066, Tokens per Sec:    16872, Lr: 0.000210\n",
      "2021-07-27 07:58:00,988 - INFO - joeynmt.training - Epoch   5, Step:   111900, Batch Loss:     2.243186, Tokens per Sec:    16970, Lr: 0.000210\n",
      "2021-07-27 07:58:14,591 - INFO - joeynmt.training - Epoch   5, Step:   112000, Batch Loss:     2.161210, Tokens per Sec:    16874, Lr: 0.000210\n",
      "2021-07-27 07:58:28,202 - INFO - joeynmt.training - Epoch   5, Step:   112100, Batch Loss:     2.457512, Tokens per Sec:    16486, Lr: 0.000210\n",
      "2021-07-27 07:58:41,873 - INFO - joeynmt.training - Epoch   5, Step:   112200, Batch Loss:     2.348396, Tokens per Sec:    16590, Lr: 0.000210\n",
      "2021-07-27 07:58:55,313 - INFO - joeynmt.training - Epoch   5, Step:   112300, Batch Loss:     2.500723, Tokens per Sec:    16707, Lr: 0.000210\n",
      "2021-07-27 07:59:08,803 - INFO - joeynmt.training - Epoch   5, Step:   112400, Batch Loss:     2.626327, Tokens per Sec:    16823, Lr: 0.000210\n",
      "2021-07-27 07:59:22,413 - INFO - joeynmt.training - Epoch   5, Step:   112500, Batch Loss:     2.437505, Tokens per Sec:    16870, Lr: 0.000210\n",
      "2021-07-27 07:59:48,720 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 07:59:48,720 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 07:59:48,720 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 07:59:49,058 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 07:59:49,059 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 07:59:49,757 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 07:59:49,759 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 07:59:49,759 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 07:59:49,759 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a man , and he said to them , “ I have come to the scribes and see me . ”\n",
      "2021-07-27 07:59:49,759 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 07:59:49,760 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 07:59:49,760 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 07:59:49,760 - INFO - joeynmt.training - \tHypothesis: Now Martha and Martha had found the same way , and Mary said to Mary , “ Teacher , and He was going to see Him . ”\n",
      "2021-07-27 07:59:49,760 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a day of distress .\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 07:59:49,762 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 07:59:49,762 - INFO - joeynmt.training - \tHypothesis: Paul had sat down the tree of the sea , and when he was cut off , he was cut off , and the fire was cut down .\n",
      "2021-07-27 07:59:49,762 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   112500: bleu:   7.13, loss: 88103.1953, ppl:  12.3649, duration: 27.3490s\n",
      "2021-07-27 08:00:03,376 - INFO - joeynmt.training - Epoch   5, Step:   112600, Batch Loss:     2.706132, Tokens per Sec:    16512, Lr: 0.000210\n",
      "2021-07-27 08:00:17,047 - INFO - joeynmt.training - Epoch   5, Step:   112700, Batch Loss:     2.584447, Tokens per Sec:    16981, Lr: 0.000210\n",
      "2021-07-27 08:00:30,765 - INFO - joeynmt.training - Epoch   5, Step:   112800, Batch Loss:     2.264445, Tokens per Sec:    16697, Lr: 0.000210\n",
      "2021-07-27 08:00:44,376 - INFO - joeynmt.training - Epoch   5, Step:   112900, Batch Loss:     2.601777, Tokens per Sec:    16595, Lr: 0.000210\n",
      "2021-07-27 08:00:57,962 - INFO - joeynmt.training - Epoch   5, Step:   113000, Batch Loss:     2.434226, Tokens per Sec:    16813, Lr: 0.000210\n",
      "2021-07-27 08:01:11,398 - INFO - joeynmt.training - Epoch   5, Step:   113100, Batch Loss:     2.222914, Tokens per Sec:    16669, Lr: 0.000210\n",
      "2021-07-27 08:01:24,828 - INFO - joeynmt.training - Epoch   5, Step:   113200, Batch Loss:     2.453332, Tokens per Sec:    16762, Lr: 0.000210\n",
      "2021-07-27 08:01:38,503 - INFO - joeynmt.training - Epoch   5, Step:   113300, Batch Loss:     2.122350, Tokens per Sec:    16731, Lr: 0.000210\n",
      "2021-07-27 08:01:52,225 - INFO - joeynmt.training - Epoch   5, Step:   113400, Batch Loss:     2.462603, Tokens per Sec:    16654, Lr: 0.000210\n",
      "2021-07-27 08:02:05,718 - INFO - joeynmt.training - Epoch   5, Step:   113500, Batch Loss:     2.619079, Tokens per Sec:    16430, Lr: 0.000210\n",
      "2021-07-27 08:02:19,325 - INFO - joeynmt.training - Epoch   5, Step:   113600, Batch Loss:     2.729548, Tokens per Sec:    16594, Lr: 0.000210\n",
      "2021-07-27 08:02:32,867 - INFO - joeynmt.training - Epoch   5, Step:   113700, Batch Loss:     2.547987, Tokens per Sec:    16573, Lr: 0.000210\n",
      "2021-07-27 08:02:46,474 - INFO - joeynmt.training - Epoch   5, Step:   113800, Batch Loss:     2.506671, Tokens per Sec:    17130, Lr: 0.000210\n",
      "2021-07-27 08:02:58,664 - INFO - joeynmt.training - Epoch   5: total training loss 5640.06\n",
      "2021-07-27 08:02:58,664 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-27 08:03:00,157 - INFO - joeynmt.training - Epoch   6, Step:   113900, Batch Loss:     2.651522, Tokens per Sec:    14113, Lr: 0.000210\n",
      "2021-07-27 08:03:13,611 - INFO - joeynmt.training - Epoch   6, Step:   114000, Batch Loss:     2.423502, Tokens per Sec:    16609, Lr: 0.000210\n",
      "2021-07-27 08:03:27,130 - INFO - joeynmt.training - Epoch   6, Step:   114100, Batch Loss:     2.439190, Tokens per Sec:    16577, Lr: 0.000210\n",
      "2021-07-27 08:03:40,773 - INFO - joeynmt.training - Epoch   6, Step:   114200, Batch Loss:     2.556720, Tokens per Sec:    16652, Lr: 0.000210\n",
      "2021-07-27 08:03:54,253 - INFO - joeynmt.training - Epoch   6, Step:   114300, Batch Loss:     1.940488, Tokens per Sec:    16779, Lr: 0.000210\n",
      "2021-07-27 08:04:07,643 - INFO - joeynmt.training - Epoch   6, Step:   114400, Batch Loss:     2.539602, Tokens per Sec:    16727, Lr: 0.000210\n",
      "2021-07-27 08:04:21,309 - INFO - joeynmt.training - Epoch   6, Step:   114500, Batch Loss:     2.601215, Tokens per Sec:    16643, Lr: 0.000210\n",
      "2021-07-27 08:04:34,946 - INFO - joeynmt.training - Epoch   6, Step:   114600, Batch Loss:     2.398460, Tokens per Sec:    16722, Lr: 0.000210\n",
      "2021-07-27 08:04:48,523 - INFO - joeynmt.training - Epoch   6, Step:   114700, Batch Loss:     2.207838, Tokens per Sec:    16505, Lr: 0.000210\n",
      "2021-07-27 08:05:02,316 - INFO - joeynmt.training - Epoch   6, Step:   114800, Batch Loss:     2.243703, Tokens per Sec:    16925, Lr: 0.000210\n",
      "2021-07-27 08:05:15,952 - INFO - joeynmt.training - Epoch   6, Step:   114900, Batch Loss:     2.383183, Tokens per Sec:    16501, Lr: 0.000210\n",
      "2021-07-27 08:05:29,679 - INFO - joeynmt.training - Epoch   6, Step:   115000, Batch Loss:     2.407759, Tokens per Sec:    16727, Lr: 0.000210\n",
      "2021-07-27 08:05:57,321 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:05:57,321 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:05:57,321 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:05:57,654 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 08:05:57,655 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 08:05:58,734 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:05:58,736 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:05:58,736 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:05:58,736 - INFO - joeynmt.training - \tHypothesis: The Pharisees , who was like a man , saw him , and said to them , “ I have come to the seven times , and I have come to know where I have come . ”\n",
      "2021-07-27 08:05:58,736 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:05:58,737 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:05:58,737 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:05:58,737 - INFO - joeynmt.training - \tHypothesis: When Martha and Mary had found the same , Mary and Mary said to her , “ Teacher , and Mary , and He was with Him , and He was with Him . ”\n",
      "2021-07-27 08:05:58,737 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:05:58,738 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:05:58,738 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:05:58,738 - INFO - joeynmt.training - \tHypothesis: He was a day of service today , and he was a day of service .\n",
      "2021-07-27 08:05:58,738 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:05:58,739 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:05:58,739 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:05:58,739 - INFO - joeynmt.training - \tHypothesis: Paul had sat down and looked for the sea , and when He was cut off , He was cut off from the sea , and the fire was cut down .\n",
      "2021-07-27 08:05:58,739 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   115000: bleu:   7.32, loss: 87862.8984, ppl:  12.2804, duration: 29.0597s\n",
      "2021-07-27 08:06:12,570 - INFO - joeynmt.training - Epoch   6, Step:   115100, Batch Loss:     2.355669, Tokens per Sec:    16643, Lr: 0.000210\n",
      "2021-07-27 08:06:26,215 - INFO - joeynmt.training - Epoch   6, Step:   115200, Batch Loss:     2.565229, Tokens per Sec:    17035, Lr: 0.000210\n",
      "2021-07-27 08:06:39,917 - INFO - joeynmt.training - Epoch   6, Step:   115300, Batch Loss:     2.217152, Tokens per Sec:    16383, Lr: 0.000210\n",
      "2021-07-27 08:06:53,423 - INFO - joeynmt.training - Epoch   6, Step:   115400, Batch Loss:     2.571432, Tokens per Sec:    16232, Lr: 0.000210\n",
      "2021-07-27 08:07:07,141 - INFO - joeynmt.training - Epoch   6, Step:   115500, Batch Loss:     2.283047, Tokens per Sec:    17002, Lr: 0.000210\n",
      "2021-07-27 08:07:20,657 - INFO - joeynmt.training - Epoch   6, Step:   115600, Batch Loss:     2.495604, Tokens per Sec:    16737, Lr: 0.000210\n",
      "2021-07-27 08:07:34,254 - INFO - joeynmt.training - Epoch   6, Step:   115700, Batch Loss:     2.626188, Tokens per Sec:    16711, Lr: 0.000210\n",
      "2021-07-27 08:07:47,921 - INFO - joeynmt.training - Epoch   6, Step:   115800, Batch Loss:     2.436184, Tokens per Sec:    16511, Lr: 0.000210\n",
      "2021-07-27 08:08:01,706 - INFO - joeynmt.training - Epoch   6, Step:   115900, Batch Loss:     2.395348, Tokens per Sec:    16773, Lr: 0.000210\n",
      "2021-07-27 08:08:15,433 - INFO - joeynmt.training - Epoch   6, Step:   116000, Batch Loss:     2.482064, Tokens per Sec:    16577, Lr: 0.000210\n",
      "2021-07-27 08:08:29,114 - INFO - joeynmt.training - Epoch   6, Step:   116100, Batch Loss:     2.438924, Tokens per Sec:    16217, Lr: 0.000210\n",
      "2021-07-27 08:08:42,812 - INFO - joeynmt.training - Epoch   6, Step:   116200, Batch Loss:     2.477787, Tokens per Sec:    16635, Lr: 0.000210\n",
      "2021-07-27 08:08:45,131 - INFO - joeynmt.training - Epoch   6: total training loss 5643.48\n",
      "2021-07-27 08:08:45,132 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-27 08:08:56,662 - INFO - joeynmt.training - Epoch   7, Step:   116300, Batch Loss:     2.611559, Tokens per Sec:    16210, Lr: 0.000210\n",
      "2021-07-27 08:09:10,367 - INFO - joeynmt.training - Epoch   7, Step:   116400, Batch Loss:     2.461609, Tokens per Sec:    16986, Lr: 0.000210\n",
      "2021-07-27 08:09:24,106 - INFO - joeynmt.training - Epoch   7, Step:   116500, Batch Loss:     2.614424, Tokens per Sec:    16553, Lr: 0.000210\n",
      "2021-07-27 08:09:37,807 - INFO - joeynmt.training - Epoch   7, Step:   116600, Batch Loss:     2.489360, Tokens per Sec:    16566, Lr: 0.000210\n",
      "2021-07-27 08:09:51,407 - INFO - joeynmt.training - Epoch   7, Step:   116700, Batch Loss:     2.501258, Tokens per Sec:    16745, Lr: 0.000210\n",
      "2021-07-27 08:10:05,053 - INFO - joeynmt.training - Epoch   7, Step:   116800, Batch Loss:     2.428561, Tokens per Sec:    16752, Lr: 0.000210\n",
      "2021-07-27 08:10:18,699 - INFO - joeynmt.training - Epoch   7, Step:   116900, Batch Loss:     2.476547, Tokens per Sec:    16690, Lr: 0.000210\n",
      "2021-07-27 08:10:32,379 - INFO - joeynmt.training - Epoch   7, Step:   117000, Batch Loss:     2.505526, Tokens per Sec:    16575, Lr: 0.000210\n",
      "2021-07-27 08:10:46,105 - INFO - joeynmt.training - Epoch   7, Step:   117100, Batch Loss:     2.418744, Tokens per Sec:    16725, Lr: 0.000210\n",
      "2021-07-27 08:10:59,622 - INFO - joeynmt.training - Epoch   7, Step:   117200, Batch Loss:     2.407301, Tokens per Sec:    16647, Lr: 0.000210\n",
      "2021-07-27 08:11:13,227 - INFO - joeynmt.training - Epoch   7, Step:   117300, Batch Loss:     2.544755, Tokens per Sec:    16986, Lr: 0.000210\n",
      "2021-07-27 08:11:26,794 - INFO - joeynmt.training - Epoch   7, Step:   117400, Batch Loss:     2.222024, Tokens per Sec:    16735, Lr: 0.000210\n",
      "2021-07-27 08:11:40,460 - INFO - joeynmt.training - Epoch   7, Step:   117500, Batch Loss:     2.431499, Tokens per Sec:    16459, Lr: 0.000210\n",
      "2021-07-27 08:12:07,893 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:12:07,894 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:12:07,894 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:12:09,018 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:12:09,018 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:12:09,018 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:12:09,019 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a tree , and he said to them , “ I have come and come down and come down and come down . ”\n",
      "2021-07-27 08:12:09,019 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:12:09,019 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - \tHypothesis: Then Martha and Mary came to her , and her mother came to Mary , and said to her , “ Teacher , and her daughter was coming . ”\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the Passover .\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - \tHypothesis: Then Paul gave them the treasure of the sea , and when He was cast , He was cut off the sea , and the fire was cut down and the sea .\n",
      "2021-07-27 08:12:09,022 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   117500: bleu:   6.91, loss: 88369.0234, ppl:  12.4591, duration: 28.5610s\n",
      "2021-07-27 08:12:22,654 - INFO - joeynmt.training - Epoch   7, Step:   117600, Batch Loss:     2.411600, Tokens per Sec:    16470, Lr: 0.000210\n",
      "2021-07-27 08:12:36,168 - INFO - joeynmt.training - Epoch   7, Step:   117700, Batch Loss:     2.633862, Tokens per Sec:    16355, Lr: 0.000210\n",
      "2021-07-27 08:12:49,887 - INFO - joeynmt.training - Epoch   7, Step:   117800, Batch Loss:     2.490059, Tokens per Sec:    16641, Lr: 0.000210\n",
      "2021-07-27 08:13:03,435 - INFO - joeynmt.training - Epoch   7, Step:   117900, Batch Loss:     2.416721, Tokens per Sec:    16681, Lr: 0.000210\n",
      "2021-07-27 08:13:17,022 - INFO - joeynmt.training - Epoch   7, Step:   118000, Batch Loss:     2.696039, Tokens per Sec:    16514, Lr: 0.000210\n",
      "2021-07-27 08:13:30,683 - INFO - joeynmt.training - Epoch   7, Step:   118100, Batch Loss:     2.574065, Tokens per Sec:    16420, Lr: 0.000210\n",
      "2021-07-27 08:13:44,009 - INFO - joeynmt.training - Epoch   7, Step:   118200, Batch Loss:     2.632088, Tokens per Sec:    16838, Lr: 0.000210\n",
      "2021-07-27 08:13:57,501 - INFO - joeynmt.training - Epoch   7, Step:   118300, Batch Loss:     2.396899, Tokens per Sec:    16663, Lr: 0.000210\n",
      "2021-07-27 08:14:11,004 - INFO - joeynmt.training - Epoch   7, Step:   118400, Batch Loss:     2.500988, Tokens per Sec:    16643, Lr: 0.000210\n",
      "2021-07-27 08:14:24,723 - INFO - joeynmt.training - Epoch   7, Step:   118500, Batch Loss:     2.380573, Tokens per Sec:    16788, Lr: 0.000210\n",
      "2021-07-27 08:14:30,863 - INFO - joeynmt.training - Epoch   7: total training loss 5636.37\n",
      "2021-07-27 08:14:30,864 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-27 08:14:38,611 - INFO - joeynmt.training - Epoch   8, Step:   118600, Batch Loss:     2.548654, Tokens per Sec:    16010, Lr: 0.000210\n",
      "2021-07-27 08:14:52,162 - INFO - joeynmt.training - Epoch   8, Step:   118700, Batch Loss:     2.211355, Tokens per Sec:    16813, Lr: 0.000210\n",
      "2021-07-27 08:15:05,738 - INFO - joeynmt.training - Epoch   8, Step:   118800, Batch Loss:     2.538246, Tokens per Sec:    16971, Lr: 0.000210\n",
      "2021-07-27 08:15:19,273 - INFO - joeynmt.training - Epoch   8, Step:   118900, Batch Loss:     2.161235, Tokens per Sec:    16699, Lr: 0.000210\n",
      "2021-07-27 08:15:32,781 - INFO - joeynmt.training - Epoch   8, Step:   119000, Batch Loss:     2.567760, Tokens per Sec:    16266, Lr: 0.000210\n",
      "2021-07-27 08:15:46,487 - INFO - joeynmt.training - Epoch   8, Step:   119100, Batch Loss:     2.308064, Tokens per Sec:    16984, Lr: 0.000210\n",
      "2021-07-27 08:16:00,001 - INFO - joeynmt.training - Epoch   8, Step:   119200, Batch Loss:     2.367049, Tokens per Sec:    16709, Lr: 0.000210\n",
      "2021-07-27 08:16:13,588 - INFO - joeynmt.training - Epoch   8, Step:   119300, Batch Loss:     2.265653, Tokens per Sec:    16689, Lr: 0.000210\n",
      "2021-07-27 08:16:27,288 - INFO - joeynmt.training - Epoch   8, Step:   119400, Batch Loss:     2.365632, Tokens per Sec:    16780, Lr: 0.000210\n",
      "2021-07-27 08:16:40,888 - INFO - joeynmt.training - Epoch   8, Step:   119500, Batch Loss:     2.591372, Tokens per Sec:    16465, Lr: 0.000210\n",
      "2021-07-27 08:16:54,435 - INFO - joeynmt.training - Epoch   8, Step:   119600, Batch Loss:     2.183113, Tokens per Sec:    16600, Lr: 0.000210\n",
      "2021-07-27 08:17:07,954 - INFO - joeynmt.training - Epoch   8, Step:   119700, Batch Loss:     2.603106, Tokens per Sec:    16782, Lr: 0.000210\n",
      "2021-07-27 08:17:21,415 - INFO - joeynmt.training - Epoch   8, Step:   119800, Batch Loss:     2.385073, Tokens per Sec:    16726, Lr: 0.000210\n",
      "2021-07-27 08:17:35,122 - INFO - joeynmt.training - Epoch   8, Step:   119900, Batch Loss:     2.329810, Tokens per Sec:    16796, Lr: 0.000210\n",
      "2021-07-27 08:17:48,798 - INFO - joeynmt.training - Epoch   8, Step:   120000, Batch Loss:     2.380733, Tokens per Sec:    16619, Lr: 0.000210\n",
      "2021-07-27 08:18:15,915 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:18:15,915 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:18:15,916 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:18:16,241 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 08:18:16,242 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 08:18:17,013 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:18:17,014 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:18:17,014 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:18:17,015 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a tree , and He said to them , “ I have come and come to the tomb and come to me . ”\n",
      "2021-07-27 08:18:17,016 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:18:17,016 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:18:17,016 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:18:17,017 - INFO - joeynmt.training - \tHypothesis: Now Martha and Mary went to the house , and Mary said to Mary , “ Teacher , and Mary , and Mary , and Mary , and Mary . ”\n",
      "2021-07-27 08:18:17,017 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:18:17,017 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:18:17,017 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:18:17,018 - INFO - joeynmt.training - \tHypothesis: He was a day of seventh day , and he was in the days of Noah .\n",
      "2021-07-27 08:18:17,018 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:18:17,018 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:18:17,018 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:18:17,019 - INFO - joeynmt.training - \tHypothesis: And Paul had sound in the sea , and when He had sat on the sea , He was cast , and the sea was cut down and the sea .\n",
      "2021-07-27 08:18:17,019 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   120000: bleu:   7.34, loss: 87600.4766, ppl:  12.1887, duration: 28.2203s\n",
      "2021-07-27 08:18:30,579 - INFO - joeynmt.training - Epoch   8, Step:   120100, Batch Loss:     2.445420, Tokens per Sec:    16505, Lr: 0.000210\n",
      "2021-07-27 08:18:44,255 - INFO - joeynmt.training - Epoch   8, Step:   120200, Batch Loss:     2.420484, Tokens per Sec:    16652, Lr: 0.000210\n",
      "2021-07-27 08:18:57,886 - INFO - joeynmt.training - Epoch   8, Step:   120300, Batch Loss:     2.527857, Tokens per Sec:    16552, Lr: 0.000210\n",
      "2021-07-27 08:19:11,555 - INFO - joeynmt.training - Epoch   8, Step:   120400, Batch Loss:     2.190420, Tokens per Sec:    16978, Lr: 0.000210\n",
      "2021-07-27 08:19:25,252 - INFO - joeynmt.training - Epoch   8, Step:   120500, Batch Loss:     2.366493, Tokens per Sec:    16611, Lr: 0.000210\n",
      "2021-07-27 08:19:38,835 - INFO - joeynmt.training - Epoch   8, Step:   120600, Batch Loss:     2.591762, Tokens per Sec:    16779, Lr: 0.000210\n",
      "2021-07-27 08:19:52,435 - INFO - joeynmt.training - Epoch   8, Step:   120700, Batch Loss:     2.445704, Tokens per Sec:    16870, Lr: 0.000210\n",
      "2021-07-27 08:20:05,998 - INFO - joeynmt.training - Epoch   8, Step:   120800, Batch Loss:     2.305495, Tokens per Sec:    16938, Lr: 0.000210\n",
      "2021-07-27 08:20:15,127 - INFO - joeynmt.training - Epoch   8: total training loss 5620.24\n",
      "2021-07-27 08:20:15,127 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-27 08:20:19,758 - INFO - joeynmt.training - Epoch   9, Step:   120900, Batch Loss:     2.309544, Tokens per Sec:    15494, Lr: 0.000210\n",
      "2021-07-27 08:20:33,489 - INFO - joeynmt.training - Epoch   9, Step:   121000, Batch Loss:     2.126650, Tokens per Sec:    16648, Lr: 0.000210\n",
      "2021-07-27 08:20:47,061 - INFO - joeynmt.training - Epoch   9, Step:   121100, Batch Loss:     2.491347, Tokens per Sec:    16627, Lr: 0.000210\n",
      "2021-07-27 08:21:00,655 - INFO - joeynmt.training - Epoch   9, Step:   121200, Batch Loss:     2.481128, Tokens per Sec:    16904, Lr: 0.000210\n",
      "2021-07-27 08:21:14,088 - INFO - joeynmt.training - Epoch   9, Step:   121300, Batch Loss:     2.425834, Tokens per Sec:    16782, Lr: 0.000210\n",
      "2021-07-27 08:21:27,724 - INFO - joeynmt.training - Epoch   9, Step:   121400, Batch Loss:     2.337860, Tokens per Sec:    16655, Lr: 0.000210\n",
      "2021-07-27 08:21:41,385 - INFO - joeynmt.training - Epoch   9, Step:   121500, Batch Loss:     2.451119, Tokens per Sec:    16768, Lr: 0.000210\n",
      "2021-07-27 08:21:55,012 - INFO - joeynmt.training - Epoch   9, Step:   121600, Batch Loss:     2.487977, Tokens per Sec:    16488, Lr: 0.000210\n",
      "2021-07-27 08:22:08,679 - INFO - joeynmt.training - Epoch   9, Step:   121700, Batch Loss:     2.691375, Tokens per Sec:    16727, Lr: 0.000210\n",
      "2021-07-27 08:22:22,273 - INFO - joeynmt.training - Epoch   9, Step:   121800, Batch Loss:     2.534315, Tokens per Sec:    16699, Lr: 0.000210\n",
      "2021-07-27 08:22:36,020 - INFO - joeynmt.training - Epoch   9, Step:   121900, Batch Loss:     2.209948, Tokens per Sec:    16633, Lr: 0.000210\n",
      "2021-07-27 08:22:49,621 - INFO - joeynmt.training - Epoch   9, Step:   122000, Batch Loss:     2.460074, Tokens per Sec:    16538, Lr: 0.000210\n",
      "2021-07-27 08:23:03,218 - INFO - joeynmt.training - Epoch   9, Step:   122100, Batch Loss:     2.584368, Tokens per Sec:    16928, Lr: 0.000210\n",
      "2021-07-27 08:23:16,803 - INFO - joeynmt.training - Epoch   9, Step:   122200, Batch Loss:     2.641792, Tokens per Sec:    16838, Lr: 0.000210\n",
      "2021-07-27 08:23:30,177 - INFO - joeynmt.training - Epoch   9, Step:   122300, Batch Loss:     2.108025, Tokens per Sec:    16773, Lr: 0.000210\n",
      "2021-07-27 08:23:43,914 - INFO - joeynmt.training - Epoch   9, Step:   122400, Batch Loss:     2.078990, Tokens per Sec:    16466, Lr: 0.000210\n",
      "2021-07-27 08:23:57,493 - INFO - joeynmt.training - Epoch   9, Step:   122500, Batch Loss:     2.023412, Tokens per Sec:    16645, Lr: 0.000210\n",
      "2021-07-27 08:24:23,010 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:24:23,010 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:24:23,011 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:24:24,087 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:24:24,087 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:24:24,087 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:24:24,088 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw him , and said to them , “ I have seen the scribes and the scribes and the scribes and the Pharisees . ”\n",
      "2021-07-27 08:24:24,088 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:24:24,088 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:24:24,088 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:24:24,089 - INFO - joeynmt.training - \tHypothesis: When Martha had heard that Mary was a great crowd , Mary said to him , “ Teacher , and Mary , and Mary , and Mary , and Mary . ”\n",
      "2021-07-27 08:24:24,089 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:24:24,089 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - \tHypothesis: He was a day of service to the Sabbath , and he was very happy .\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:24:24,091 - INFO - joeynmt.training - \tHypothesis: Paul also gave them the treasure of the sea , and when He was cut off , he was cut off and sat down , and the sea was cut down .\n",
      "2021-07-27 08:24:24,091 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   122500: bleu:   7.38, loss: 87918.3750, ppl:  12.2999, duration: 26.5970s\n",
      "2021-07-27 08:24:37,690 - INFO - joeynmt.training - Epoch   9, Step:   122600, Batch Loss:     2.432909, Tokens per Sec:    16495, Lr: 0.000210\n",
      "2021-07-27 08:24:51,171 - INFO - joeynmt.training - Epoch   9, Step:   122700, Batch Loss:     2.292904, Tokens per Sec:    17072, Lr: 0.000210\n",
      "2021-07-27 08:25:04,608 - INFO - joeynmt.training - Epoch   9, Step:   122800, Batch Loss:     2.648040, Tokens per Sec:    16834, Lr: 0.000210\n",
      "2021-07-27 08:25:18,323 - INFO - joeynmt.training - Epoch   9, Step:   122900, Batch Loss:     2.308712, Tokens per Sec:    16956, Lr: 0.000210\n",
      "2021-07-27 08:25:32,011 - INFO - joeynmt.training - Epoch   9, Step:   123000, Batch Loss:     2.517625, Tokens per Sec:    16600, Lr: 0.000210\n",
      "2021-07-27 08:25:45,451 - INFO - joeynmt.training - Epoch   9, Step:   123100, Batch Loss:     2.562794, Tokens per Sec:    16819, Lr: 0.000210\n",
      "2021-07-27 08:25:57,692 - INFO - joeynmt.training - Epoch   9: total training loss 5617.43\n",
      "2021-07-27 08:25:57,692 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-27 08:25:59,019 - INFO - joeynmt.training - Epoch  10, Step:   123200, Batch Loss:     2.192390, Tokens per Sec:    13432, Lr: 0.000210\n",
      "2021-07-27 08:26:12,409 - INFO - joeynmt.training - Epoch  10, Step:   123300, Batch Loss:     2.659591, Tokens per Sec:    16782, Lr: 0.000210\n",
      "2021-07-27 08:26:25,903 - INFO - joeynmt.training - Epoch  10, Step:   123400, Batch Loss:     2.452177, Tokens per Sec:    16595, Lr: 0.000210\n",
      "2021-07-27 08:26:39,526 - INFO - joeynmt.training - Epoch  10, Step:   123500, Batch Loss:     2.554398, Tokens per Sec:    16582, Lr: 0.000210\n",
      "2021-07-27 08:26:53,092 - INFO - joeynmt.training - Epoch  10, Step:   123600, Batch Loss:     2.577075, Tokens per Sec:    16976, Lr: 0.000210\n",
      "2021-07-27 08:27:06,496 - INFO - joeynmt.training - Epoch  10, Step:   123700, Batch Loss:     2.480223, Tokens per Sec:    16791, Lr: 0.000210\n",
      "2021-07-27 08:27:19,844 - INFO - joeynmt.training - Epoch  10, Step:   123800, Batch Loss:     2.359327, Tokens per Sec:    16911, Lr: 0.000210\n",
      "2021-07-27 08:27:33,666 - INFO - joeynmt.training - Epoch  10, Step:   123900, Batch Loss:     2.401583, Tokens per Sec:    16853, Lr: 0.000210\n",
      "2021-07-27 08:27:47,389 - INFO - joeynmt.training - Epoch  10, Step:   124000, Batch Loss:     2.644904, Tokens per Sec:    16507, Lr: 0.000210\n",
      "2021-07-27 08:28:00,987 - INFO - joeynmt.training - Epoch  10, Step:   124100, Batch Loss:     2.537358, Tokens per Sec:    16527, Lr: 0.000210\n",
      "2021-07-27 08:28:14,619 - INFO - joeynmt.training - Epoch  10, Step:   124200, Batch Loss:     2.410045, Tokens per Sec:    16877, Lr: 0.000210\n",
      "2021-07-27 08:28:28,049 - INFO - joeynmt.training - Epoch  10, Step:   124300, Batch Loss:     2.405060, Tokens per Sec:    16891, Lr: 0.000210\n",
      "2021-07-27 08:28:41,624 - INFO - joeynmt.training - Epoch  10, Step:   124400, Batch Loss:     2.538678, Tokens per Sec:    16650, Lr: 0.000210\n",
      "2021-07-27 08:28:55,182 - INFO - joeynmt.training - Epoch  10, Step:   124500, Batch Loss:     1.863895, Tokens per Sec:    16570, Lr: 0.000210\n",
      "2021-07-27 08:29:08,606 - INFO - joeynmt.training - Epoch  10, Step:   124600, Batch Loss:     2.493046, Tokens per Sec:    16761, Lr: 0.000210\n",
      "2021-07-27 08:29:22,199 - INFO - joeynmt.training - Epoch  10, Step:   124700, Batch Loss:     2.522714, Tokens per Sec:    16814, Lr: 0.000210\n",
      "2021-07-27 08:29:35,695 - INFO - joeynmt.training - Epoch  10, Step:   124800, Batch Loss:     2.441285, Tokens per Sec:    16548, Lr: 0.000210\n",
      "2021-07-27 08:29:49,347 - INFO - joeynmt.training - Epoch  10, Step:   124900, Batch Loss:     2.609269, Tokens per Sec:    16375, Lr: 0.000210\n",
      "2021-07-27 08:30:02,988 - INFO - joeynmt.training - Epoch  10, Step:   125000, Batch Loss:     2.325733, Tokens per Sec:    16931, Lr: 0.000210\n",
      "2021-07-27 08:30:29,835 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:30:29,835 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:30:29,835 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:30:30,195 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 08:30:30,196 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 08:30:30,993 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:30:30,994 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:30:30,994 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:30:30,994 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees , as he had seen , He said to them , “ I have come and come to the scribes and said to them , “ I have come and see me . ”\n",
      "2021-07-27 08:30:30,994 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:30:30,995 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:30:30,995 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:30:30,995 - INFO - joeynmt.training - \tHypothesis: When Martha had heard that Mary was a sister , Mary , and Mary , and Mary said to her , “ Teacher , and Mary , and she was born . ”\n",
      "2021-07-27 08:30:30,995 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:30:30,996 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:30:30,996 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:30:30,996 - INFO - joeynmt.training - \tHypothesis: He was a day of service to the Sabbath , and he was very happy .\n",
      "2021-07-27 08:30:30,996 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:30:30,997 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:30:30,997 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:30:30,997 - INFO - joeynmt.training - \tHypothesis: And when Paul had spoken , He had spoken of the sea , and was cut out of the sea , and the fire was cut down .\n",
      "2021-07-27 08:30:30,997 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   125000: bleu:   7.51, loss: 87552.7031, ppl:  12.1721, duration: 28.0084s\n",
      "2021-07-27 08:30:44,710 - INFO - joeynmt.training - Epoch  10, Step:   125100, Batch Loss:     2.710145, Tokens per Sec:    16778, Lr: 0.000210\n",
      "2021-07-27 08:30:58,232 - INFO - joeynmt.training - Epoch  10, Step:   125200, Batch Loss:     2.449889, Tokens per Sec:    16846, Lr: 0.000210\n",
      "2021-07-27 08:31:11,681 - INFO - joeynmt.training - Epoch  10, Step:   125300, Batch Loss:     2.597331, Tokens per Sec:    16847, Lr: 0.000210\n",
      "2021-07-27 08:31:25,227 - INFO - joeynmt.training - Epoch  10, Step:   125400, Batch Loss:     2.706754, Tokens per Sec:    16584, Lr: 0.000210\n",
      "2021-07-27 08:31:38,862 - INFO - joeynmt.training - Epoch  10, Step:   125500, Batch Loss:     2.571770, Tokens per Sec:    16478, Lr: 0.000210\n",
      "2021-07-27 08:31:41,723 - INFO - joeynmt.training - Epoch  10: total training loss 5622.03\n",
      "2021-07-27 08:31:41,723 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-27 08:31:52,470 - INFO - joeynmt.training - Epoch  11, Step:   125600, Batch Loss:     2.555787, Tokens per Sec:    16341, Lr: 0.000210\n",
      "2021-07-27 08:32:06,097 - INFO - joeynmt.training - Epoch  11, Step:   125700, Batch Loss:     2.130382, Tokens per Sec:    16967, Lr: 0.000210\n",
      "2021-07-27 08:32:19,576 - INFO - joeynmt.training - Epoch  11, Step:   125800, Batch Loss:     2.539688, Tokens per Sec:    16935, Lr: 0.000210\n",
      "2021-07-27 08:32:33,135 - INFO - joeynmt.training - Epoch  11, Step:   125900, Batch Loss:     2.427249, Tokens per Sec:    16572, Lr: 0.000210\n",
      "2021-07-27 08:32:46,716 - INFO - joeynmt.training - Epoch  11, Step:   126000, Batch Loss:     2.383571, Tokens per Sec:    16651, Lr: 0.000210\n",
      "2021-07-27 08:33:00,056 - INFO - joeynmt.training - Epoch  11, Step:   126100, Batch Loss:     2.441225, Tokens per Sec:    16599, Lr: 0.000210\n",
      "2021-07-27 08:33:13,359 - INFO - joeynmt.training - Epoch  11, Step:   126200, Batch Loss:     2.131477, Tokens per Sec:    16773, Lr: 0.000210\n",
      "2021-07-27 08:33:26,876 - INFO - joeynmt.training - Epoch  11, Step:   126300, Batch Loss:     2.453427, Tokens per Sec:    16910, Lr: 0.000210\n",
      "2021-07-27 08:33:40,418 - INFO - joeynmt.training - Epoch  11, Step:   126400, Batch Loss:     2.393158, Tokens per Sec:    16814, Lr: 0.000210\n",
      "2021-07-27 08:33:53,900 - INFO - joeynmt.training - Epoch  11, Step:   126500, Batch Loss:     2.584401, Tokens per Sec:    16647, Lr: 0.000210\n",
      "2021-07-27 08:34:07,373 - INFO - joeynmt.training - Epoch  11, Step:   126600, Batch Loss:     2.633683, Tokens per Sec:    16755, Lr: 0.000210\n",
      "2021-07-27 08:34:20,770 - INFO - joeynmt.training - Epoch  11, Step:   126700, Batch Loss:     2.383806, Tokens per Sec:    16784, Lr: 0.000210\n",
      "2021-07-27 08:34:34,352 - INFO - joeynmt.training - Epoch  11, Step:   126800, Batch Loss:     2.627386, Tokens per Sec:    16742, Lr: 0.000210\n",
      "2021-07-27 08:34:48,312 - INFO - joeynmt.training - Epoch  11, Step:   126900, Batch Loss:     2.508087, Tokens per Sec:    17028, Lr: 0.000210\n",
      "2021-07-27 08:35:01,768 - INFO - joeynmt.training - Epoch  11, Step:   127000, Batch Loss:     2.437658, Tokens per Sec:    16597, Lr: 0.000210\n",
      "2021-07-27 08:35:15,354 - INFO - joeynmt.training - Epoch  11, Step:   127100, Batch Loss:     2.445096, Tokens per Sec:    16674, Lr: 0.000210\n",
      "2021-07-27 08:35:28,869 - INFO - joeynmt.training - Epoch  11, Step:   127200, Batch Loss:     2.352874, Tokens per Sec:    16501, Lr: 0.000210\n",
      "2021-07-27 08:35:42,302 - INFO - joeynmt.training - Epoch  11, Step:   127300, Batch Loss:     2.553585, Tokens per Sec:    16950, Lr: 0.000210\n",
      "2021-07-27 08:35:55,767 - INFO - joeynmt.training - Epoch  11, Step:   127400, Batch Loss:     2.436206, Tokens per Sec:    16913, Lr: 0.000210\n",
      "2021-07-27 08:36:09,258 - INFO - joeynmt.training - Epoch  11, Step:   127500, Batch Loss:     2.336831, Tokens per Sec:    16862, Lr: 0.000210\n",
      "2021-07-27 08:36:37,381 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:36:37,381 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:36:37,381 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:36:38,777 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:36:38,779 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:36:38,779 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:36:38,779 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a man , and He said to them , “ I have come and come to the tomb , and I have come to know . ”\n",
      "2021-07-27 08:36:38,779 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:36:38,780 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:36:38,780 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:36:38,780 - INFO - joeynmt.training - \tHypothesis: When Martha had heard that she had been called Mary , Mary , and Mary , and Mary said to Him , “ Teacher , and we have come to Him . ”\n",
      "2021-07-27 08:36:38,780 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - \tHypothesis: He was a day of service , and he was very happy to be happy .\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:36:38,782 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:36:38,782 - INFO - joeynmt.training - \tHypothesis: And Paul had sat out the sea , and when He was cut out of the sea , He was cut off the sea , and the sea was cut down and the sea .\n",
      "2021-07-27 08:36:38,782 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   127500: bleu:   7.36, loss: 87687.9141, ppl:  12.2192, duration: 29.5232s\n",
      "2021-07-27 08:36:52,515 - INFO - joeynmt.training - Epoch  11, Step:   127600, Batch Loss:     2.523544, Tokens per Sec:    16663, Lr: 0.000210\n",
      "2021-07-27 08:37:05,826 - INFO - joeynmt.training - Epoch  11, Step:   127700, Batch Loss:     2.365040, Tokens per Sec:    16722, Lr: 0.000210\n",
      "2021-07-27 08:37:19,352 - INFO - joeynmt.training - Epoch  11, Step:   127800, Batch Loss:     1.654168, Tokens per Sec:    16762, Lr: 0.000210\n",
      "2021-07-27 08:37:26,458 - INFO - joeynmt.training - Epoch  11: total training loss 5617.86\n",
      "2021-07-27 08:37:26,459 - INFO - joeynmt.training - Training ended after  11 epochs.\n",
      "2021-07-27 08:37:26,459 - INFO - joeynmt.training - Best validation result (greedy) at step   125000:  12.17 ppl.\n",
      "2021-07-27 08:37:26,481 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-07-27 08:37:26,843 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-27 08:37:27,043 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-27 08:37:27,115 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe.en)...\n",
      "2021-07-27 08:38:01,728 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:38:01,728 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:38:01,728 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:38:02,031 - INFO - joeynmt.prediction -  dev bleu[13a]:   8.11 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-27 08:38:02,035 - INFO - joeynmt.prediction - Translations saved to: models/back_lhen_reverse_transformer_continued3/00125000.hyps.dev\n",
      "2021-07-27 08:38:02,036 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe.en)...\n",
      "2021-07-27 08:38:35,790 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:38:35,790 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:38:35,790 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:38:36,121 - INFO - joeynmt.prediction - test bleu[13a]:   8.13 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-27 08:38:36,126 - INFO - joeynmt.prediction - Translations saved to: models/back_lhen_reverse_transformer_continued3/00125000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/back_transformer_reverse_lhen_reload3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4lBkqSz2wRS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BGimJSBX4PJ5",
    "EzBBzWMalTpE",
    "U7UpBGEl9T44"
   ],
   "name": "Baseline_models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
