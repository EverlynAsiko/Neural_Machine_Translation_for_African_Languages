{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6cJZzBRP1-N"
   },
   "source": [
    "# Multilingual neural machine translation.\n",
    "\n",
    "For this case, we shall to a many-to-one translation:\n",
    "{Kinyarwanda, Luhya} to English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4V-O3nJPsAA",
    "outputId": "0e815418-450c-4af7-cc6e-357edda12612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Linking to drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcRP_CqbRQzj"
   },
   "outputs": [],
   "source": [
    "# Importing needed libraries for preprocessing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "grB3V9FhReiZ",
    "outputId": "654816af-b0bd-487a-9bbd-115bbbdaf6f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.8.0+cu101\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 763.5 MB 15 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0+cu102\n",
      "    Uninstalling torch-1.9.0+cu102:\n",
      "      Successfully uninstalled torch-1.9.0+cu102\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.8.0+cu101\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "# Install Pytorch with GPU support v1.8.0.\n",
    "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7jAsiRLRlMs"
   },
   "outputs": [],
   "source": [
    "# Filtering warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaH6F-u3RrAb"
   },
   "outputs": [],
   "source": [
    "# Loading the drive\n",
    "import os\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH_IYggrTJKa"
   },
   "outputs": [],
   "source": [
    "# Setting source and target languages\n",
    "source_language = \"en\"\n",
    "target_language = \"rw_lh\"\n",
    "\n",
    "os.environ[\"src\"] = source_language \n",
    "os.environ[\"tgt\"] = target_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G0mZmUETh-A"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uMD6Hic-TSCa",
    "outputId": "4f5c1e8c-83bf-410f-8437-1f7604de82a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Kinyarwanda/train.bpe.en <==\n",
      "R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "You will see the time when God br@@ ings righteous rule to all the earth , und@@ o@@ ing the d@@ am@@ age and inj@@ ust@@ ice brought by human rul@@ er@@ ship .\n",
      "Let us consider f@@ ive reas@@ ons why we should want to follow the Christ .\n",
      "Even in the Bible , the id@@ ea of pers@@ u@@ as@@ ion som@@ et@@ im@@ es has n@@ eg@@ ative con@@ no@@ t@@ ations , den@@ ot@@ ing a cor@@ rup@@ ting or a lead@@ ing as@@ tr@@ ay .\n",
      "For God’s servants to be deliv@@ ered , Satan and his ent@@ ire world@@ wide system of things need to be rem@@ ov@@ ed .\n",
      "I had never heard that name used in my ch@@ urch .\n",
      "S@@ imp@@ ly having authority or a wid@@ er name recogn@@ ition is not the important thing .\n",
      "M@@ ost people do not believe in the spir@@ its .\n",
      "And others are encourag@@ ed to be merc@@ if@@ ul , for merc@@ y beg@@ ets merc@@ y . ​ — Luke 6 : 38 .\n",
      "Like such ro@@ o@@ ts in earth@@ ’s no@@ ur@@ ish@@ ing so@@ il , our m@@ inds and hearts need to del@@ v@@ e exp@@ ans@@ ively into God’s Word and d@@ raw from its life - giving wat@@ ers .\n",
      "\n",
      "==> Kinyarwanda/train.bpe.rw <==\n",
      "A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "U@@ z@@ aba uh@@ ari igihe Imana iz@@ ashy@@ iraho ubutegetsi buk@@ iranuka ku isi hose , ik@@ av@@ an@@ aho ibibi n’@@ akar@@ eng@@ ane byat@@ ewe n’@@ ubutegetsi bw’@@ abantu .\n",
      "Nim@@ ucyo dusuzume impamvu esh@@ anu z@@ agombye gutuma tw@@ ifuza guk@@ urikira Kristo .\n",
      "Nd@@ etse no muri Bibiliya , igit@@ ekerezo cyo kw@@ emeza umuntu ikintu , rimwe na rimwe cy@@ umvikana mu buryo bub@@ i , kig@@ as@@ obanura k@@ osh@@ ya , cyangwa kuy@@ ob@@ ya .\n",
      "Kugira ngo abagaragu b’Imana baz@@ ac@@ ung@@ ur@@ we , Satani na gahunda ye y’@@ ibintu yose yo ku isi hose big@@ omba kuv@@ an@@ waho .\n",
      "Mu idini n@@ abag@@ amo sin@@ ari nar@@ igeze n@@ umva bak@@ oresha iryo z@@ ina .\n",
      "G@@ uh@@ abwa ubut@@ ware gusa cyangwa kugira umw@@ anya ukomeye si cyo kintu cy’@@ ingenzi .\n",
      "Abantu benshi ntib@@ emera imy@@ uka .\n",
      "Iyo tug@@ iriye abantu imbabazi na bo bib@@ at@@ era kugira imbabazi , kuko imbabazi zit@@ urwa izindi . — Luka 6 : 38 .\n",
      "Nk’uko iyo m@@ izi ig@@ ab@@ urira ig@@ iti ib@@ iv@@ uye mu but@@ aka buk@@ ung@@ ah@@ aye , ubwenge n’@@ umutima byacu big@@ omba guc@@ eng@@ era mu Ijambo ry’Imana maze bik@@ av@@ om@@ amo amazi atanga ubuzima .\n",
      "\n",
      "==> Kinyarwanda/train.en <==\n",
      "Right after his baptism , he “ went off into Arabia ” ​ — either the Syrian Desert or possibly some quiet place on the Arabian Peninsula that was conducive to meditation .\n",
      "You will see the time when God brings righteous rule to all the earth , undoing the damage and injustice brought by human rulership .\n",
      "Let us consider five reasons why we should want to follow the Christ .\n",
      "Even in the Bible , the idea of persuasion sometimes has negative connotations , denoting a corrupting or a leading astray .\n",
      "For God’s servants to be delivered , Satan and his entire worldwide system of things need to be removed .\n",
      "I had never heard that name used in my church .\n",
      "Simply having authority or a wider name recognition is not the important thing .\n",
      "Most people do not believe in the spirits .\n",
      "And others are encouraged to be merciful , for mercy begets mercy . ​ — Luke 6 : 38 .\n",
      "Like such roots in earth’s nourishing soil , our minds and hearts need to delve expansively into God’s Word and draw from its life - giving waters .\n",
      "\n",
      "==> Kinyarwanda/train.rw <==\n",
      "Ashobora kuba yaragiye ahantu hatuje mu Butayu bwa Siriya cyangwa se wenda ku Mwigimbakirwa wa Arabiya , uri mu burasirazuba bw’Inyanja Itukura , kugira ngo hamufashe gutekereza .\n",
      "Uzaba uhari igihe Imana izashyiraho ubutegetsi bukiranuka ku isi hose , ikavanaho ibibi n’akarengane byatewe n’ubutegetsi bw’abantu .\n",
      "Nimucyo dusuzume impamvu eshanu zagombye gutuma twifuza gukurikira Kristo .\n",
      "Ndetse no muri Bibiliya , igitekerezo cyo kwemeza umuntu ikintu , rimwe na rimwe cyumvikana mu buryo bubi , kigasobanura koshya , cyangwa kuyobya .\n",
      "Kugira ngo abagaragu b’Imana bazacungurwe , Satani na gahunda ye y’ibintu yose yo ku isi hose bigomba kuvanwaho .\n",
      "Mu idini nabagamo sinari narigeze numva bakoresha iryo zina .\n",
      "Guhabwa ubutware gusa cyangwa kugira umwanya ukomeye si cyo kintu cy’ingenzi .\n",
      "Abantu benshi ntibemera imyuka .\n",
      "Iyo tugiriye abantu imbabazi na bo bibatera kugira imbabazi , kuko imbabazi ziturwa izindi . — Luka 6 : 38 .\n",
      "Nk’uko iyo mizi igaburira igiti ibivuye mu butaka bukungahaye , ubwenge n’umutima byacu bigomba gucengera mu Ijambo ry’Imana maze bikavomamo amazi atanga ubuzima .\n",
      "==> Kinyarwanda/dev.bpe.en <==\n",
      "Rather , he end@@ e@@ av@@ or@@ ed to use P@@ ol@@ ish words that were “ very close to every@@ day spe@@ ech . ”\n",
      "□ What prot@@ ection does divine instruc@@ tion provide for young people ?\n",
      "S@@ tr@@ ing@@ ed in@@ str@@ um@@ ents includ@@ ed l@@ ut@@ es , har@@ p@@ s , and ten - str@@ ing@@ ed in@@ str@@ um@@ ents .\n",
      "As the polit@@ ical situation in the B@@ al@@ tic St@@ ates deter@@ i@@ or@@ ated , ant@@ i - Wit@@ ness s@@ ent@@ im@@ ents gre@@ w and our preaching work was ban@@ ned in L@@ ith@@ u@@ an@@ ia as well .\n",
      "N@@ ever@@ th@@ eless , Jesus ass@@ ured his followers that holy spirit would be with them in car@@ ry@@ ing out the work that he had given to them .\n",
      "A decis@@ ion was made based on Bible principles , and this decis@@ ion was un@@ if@@ or@@ m@@ ly accep@@ ted . ​ — Acts 15 : 1 - 29 .\n",
      "B@@ ut@@ ter@@ f@@ ly\n",
      "● Pe@@ ople with com@@ prom@@ ised imm@@ un@@ e sy@@ st@@ ems\n",
      "In add@@ ition , while many in the comm@@ unity view the nam@@ ing cer@@ em@@ on@@ y as an important r@@ ite of pass@@ age , Christians should be s@@ ens@@ itive to the consci@@ ences of others and consider the im@@ press@@ ion that is given to un@@ believers .\n",
      "Why does Isaiah 30 : 21 speak of Jehovah’s word as coming from “ beh@@ ind you , ” since the pre@@ c@@ ed@@ ing ver@@ se Is@@ a 30 : 20 plac@@ es Jehovah in f@@ ron@@ t by saying , “ Your eyes must become eyes see@@ ing your Gr@@ and In@@ struc@@ t@@ or ” ?\n",
      "\n",
      "==> Kinyarwanda/dev.bpe.rw <==\n",
      "Ahubwo y@@ ih@@ at@@ iye gukoresha amagambo y’@@ I@@ g@@ ip@@ ol@@ onye “ as@@ a cyane n’@@ ay@@ ak@@ oresh@@ waga na rub@@ anda . ”\n",
      "□ Ni ub@@ uhe bur@@ inzi inyigisho z@@ iv@@ a ku Mana zih@@ a abak@@ iri bato ?\n",
      "Mu bik@@ oresho by’@@ umuz@@ ika by@@ ak@@ oresh@@ waga , harimo ne@@ bel@@ u , in@@ anga n’@@ in@@ anga z’@@ imir@@ ya ic@@ um@@ i .\n",
      "Uko ibintu byag@@ endaga biz@@ amba muri L@@ itu@@ wan@@ iya , L@@ at@@ iv@@ iya na Es@@ it@@ on@@ iya , ni na ko abantu bar@@ ush@@ ag@@ aho kw@@ anga Abahamya , kandi no muri L@@ itu@@ wan@@ iya umurimo wacu wo kubwiriza wag@@ eze aho ur@@ ab@@ uz@@ anywa .\n",
      "Icyakora , Yesu y@@ ij@@ eje abigishwa be ko umwuka wera wari kub@@ ashy@@ igikira mu gihe bari kuba bas@@ ohoza umurimo yari yab@@ ash@@ inze .\n",
      "H@@ afash@@ we umwanzuro ush@@ ingiye ku mah@@ ame ya Bibiliya kandi amatorero yose yar@@ aw@@ emeye . — Ibyakozwe 15 : 1 - 29 .\n",
      "Ik@@ iny@@ ug@@ uny@@ ugu\n",
      "● Abantu bafite umubiri ufite ubushobozi buk@@ e bwo kur@@ wanya indwara\n",
      "Byongeye kandi , mu gihe abantu benshi babona ko umuh@@ ango wo kwita umwana izina ari umuh@@ ango w’@@ ingenzi uf@@ itanye isano n’uko umwana ava mu but@@ uro bw’@@ imy@@ uka y’@@ abak@@ ur@@ ambere ak@@ aza mu isi y’abantu , Abakristo bo bag@@ ombye kwit@@ ond@@ a ku bw’@@ imit@@ im@@ anama y’@@ abandi kandi bag@@ at@@ ekereza uko abantu bad@@ ah@@ uje ukwizera bari bub@@ if@@ ate .\n",
      "Kuki muri Yesaya 30 : 21 h@@ avuga ko ijambo rya Yehova rit@@ uruka “ inyuma , ” mu gihe umur@@ ongo ub@@ anz@@ iriza uwo uvuga ko Yehova ari imbere ugira uti ‘ amaso yawe az@@ ajya ar@@ eba ukw@@ igisha ’ ?\n",
      "\n",
      "==> Kinyarwanda/dev.en <==\n",
      "Rather , he endeavored to use Polish words that were “ very close to everyday speech . ”\n",
      "□ What protection does divine instruction provide for young people ?\n",
      "Stringed instruments included lutes , harps , and ten - stringed instruments .\n",
      "As the political situation in the Baltic States deteriorated , anti - Witness sentiments grew and our preaching work was banned in Lithuania as well .\n",
      "Nevertheless , Jesus assured his followers that holy spirit would be with them in carrying out the work that he had given to them .\n",
      "A decision was made based on Bible principles , and this decision was uniformly accepted . ​ — Acts 15 : 1 - 29 .\n",
      "Butterfly\n",
      "● People with compromised immune systems\n",
      "In addition , while many in the community view the naming ceremony as an important rite of passage , Christians should be sensitive to the consciences of others and consider the impression that is given to unbelievers .\n",
      "Why does Isaiah 30 : 21 speak of Jehovah’s word as coming from “ behind you , ” since the preceding verse Isa 30 : 20 places Jehovah in front by saying , “ Your eyes must become eyes seeing your Grand Instructor ” ?\n",
      "\n",
      "==> Kinyarwanda/dev.rw <==\n",
      "Ahubwo yihatiye gukoresha amagambo y’Igipolonye “ asa cyane n’ayakoreshwaga na rubanda . ”\n",
      "□ Ni ubuhe burinzi inyigisho ziva ku Mana ziha abakiri bato ?\n",
      "Mu bikoresho by’umuzika byakoreshwaga , harimo nebelu , inanga n’inanga z’imirya icumi .\n",
      "Uko ibintu byagendaga bizamba muri Lituwaniya , Lativiya na Esitoniya , ni na ko abantu barushagaho kwanga Abahamya , kandi no muri Lituwaniya umurimo wacu wo kubwiriza wageze aho urabuzanywa .\n",
      "Icyakora , Yesu yijeje abigishwa be ko umwuka wera wari kubashyigikira mu gihe bari kuba basohoza umurimo yari yabashinze .\n",
      "Hafashwe umwanzuro ushingiye ku mahame ya Bibiliya kandi amatorero yose yarawemeye . — Ibyakozwe 15 : 1 - 29 .\n",
      "Ikinyugunyugu\n",
      "● Abantu bafite umubiri ufite ubushobozi buke bwo kurwanya indwara\n",
      "Byongeye kandi , mu gihe abantu benshi babona ko umuhango wo kwita umwana izina ari umuhango w’ingenzi ufitanye isano n’uko umwana ava mu buturo bw’imyuka y’abakurambere akaza mu isi y’abantu , Abakristo bo bagombye kwitonda ku bw’imitimanama y’abandi kandi bagatekereza uko abantu badahuje ukwizera bari bubifate .\n",
      "Kuki muri Yesaya 30 : 21 havuga ko ijambo rya Yehova rituruka “ inyuma , ” mu gihe umurongo ubanziriza uwo uvuga ko Yehova ari imbere ugira uti ‘ amaso yawe azajya areba ukwigisha ’ ?\n"
     ]
    }
   ],
   "source": [
    "! head Kinyarwanda/train.*\n",
    "! head Kinyarwanda/dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "IP5nJ822UGJM",
    "outputId": "73cdbd2b-5230-47d1-c5b5-741c04f9b832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Luhyia/train.bpe.en <==\n",
      " T@@ hat day was the P@@ re@@ par@@ ation, and the Sab@@ b@@ ath drew ne@@ ar@@ .\n",
      " Behold, I am coming qui@@ ck@@ l@@ y@@ ! H@@ old fast what you ha@@ ve, that no one may take your crow@@ n.\n",
      " The next day, because he wan@@ ted to know for certain why he was acc@@ us@@ ed by the Jews, he r@@ ele@@ as@@ ed him from his bond@@ s, and commanded the chief priests and all their coun@@ ci@@ l to appear@@ , and brought Paul down and set him before them. \n",
      "\n",
      " This He said, sig@@ ni@@ f@@ ying by what death He would di@@ e.\n",
      " Then they said to the wom@@ an, “@@ Now we believ@@ e, not because of what you said, for we our@@ selves have heard Him and we know that this is indeed the Christ, the Sa@@ vi@@ or of the worl@@ d.”\n",
      " But rej@@ ect prof@@ ane and old wi@@ ves@@ ’ f@@ ab@@ les, and ex@@ er@@ c@@ ise your@@ self toward god@@ lin@@ es@@ s.\n",
      " It is written in the prophe@@ ts, ‘@@ And they shall all be taught by God@@ .’ Therefore everyone who has heard and lear@@ ned from the Father comes to Me.\n",
      " Then out of the sm@@ oke lo@@ c@@ us@@ ts came upon the ear@@ th. And to them was given pow@@ er, as the s@@ cor@@ pi@@ ons of the earth have pow@@ er.\n",
      " Jesus said to him, “R@@ is@@ e, take up your b@@ ed and wal@@ k@@ .”\n",
      "\n",
      "==> Luhyia/train.bpe.lh <==\n",
      " Y@@ ali,@@ inyanga yo@@ khwi@@ re@@ chekha khulwa inyanga eya Is@@ aba@@ to ey@@ ali niy@@ ili ahambi okhu@@ chaak@@ a. \n",
      " N@@ di@@ itsanga bwangu o@@ hand@@ e khu aka oli nin@@ ako, kho mbu, omundu yesi yesi,@@ alab@@ uk@@ ul@@ akhwo olu@@ si@@ mb@@ il@@ wo tawe. \n",
      " Omu@@ s@@ injilili wab@@ elihe oyo y@@ enya okhumanya eshi@@ chila,@@ Abayahudi nib@@ enj@@ ililanga Paulo itookh@@ o. Kho iny@@ anga,@@ yal@@ ondakhwo yab@@ ol@@ ola em@@ iny@@ olol@@ o echia bali nibab@@ oy@@ ile,@@ Paulo mana nal@@ aka abesaaliisi aba@@ khongo nende ab@@ eshiin@@ a,@@ boosi okhw@@ aka@@ ana. Mana nay@@ ila Paulo namu@@ s@@ injisia imbeli,@@ w@@ abwe.\n",
      "\n",
      " Y@@ aboola ako khulw@@ okhumany@@ ia,@@ shinga lw@@ okhuf@@ wakh@@ we khw@@ itsa okhuba@@ . \n",
      " Kho nibaboolela omukhasi oyo bari, “I@@ fwe,@@ shikhu@@ suu@@ bile khulwa okhubela aka iwe okhubool@@ ile ta habula khu@@ suubi@@ ile shichila mbu, abeene khw@@ its@@ ile,@@ nikhu@@ hulila na@@ ya@@ ala, ne bulano khumanyile mbu, niye,@@ omu@@ honia w@@ abandu boosi@@ .” Yesu ahonia omwana w@@ omus@@ esi@@ a,\n",
      " Nebutswa,@@ wi@@ h@@ any@@ e okhurula khu@@ tsing@@ ano tsi@@ ab@@ u@@ ts@@ wa ets@@ il@@ akhoy@@ e@@ ele,@@ ta, w@@ ina@@ sie okhwe@@ ka amakhuwa amalayi k@@ obulamu obwa,@@ eshik@@ rist@@ o. \n",
      " Ab@@ al@@ akusi@@ ,@@ ba@@ hand@@ ika mbu, ‘@@ Buli mundu ali@@ e@@ chesi@@ bwa nende,@@ Nyasaye@@ .’ Kho oyo yesi ou@@ hulilanga aka Papa nende,@@ okhwe@@ ka okhurula khuy@@ e, yetsa khw@@ isie. \n",
      " Ne tsi@@ si@@ che nitsi@@ rula mu@@ mw@@ osi nitsi@@ ba khushialo nitsi@@ helesi@@ bwe obunyali obu@@ fwana shinga obwa amak@@ ati@@ a,@@ k@@ eshial@@ o. \n",
      " Yesu namuboolela ari, “S@@ injila wit@@ u@@ ushe omuk@@ e@@ kw@@ o,@@ mana o@@ chend@@ e.” \n",
      "\n",
      "==> Luhyia/train.en <==\n",
      " That day was the Preparation, and the Sabbath drew near.\n",
      " Behold, I am coming quickly! Hold fast what you have, that no one may take your crown.\n",
      " The next day, because he wanted to know for certain why he was accused by the Jews, he released him from his bonds, and commanded the chief priests and all their council to appear, and brought Paul down and set him before them. \n",
      "\n",
      " This He said, signifying by what death He would die.\n",
      " Then they said to the woman, “Now we believe, not because of what you said, for we ourselves have heard Him and we know that this is indeed the Christ, the Savior of the world.”\n",
      " But reject profane and old wives’ fables, and exercise yourself toward godliness.\n",
      " It is written in the prophets, ‘And they shall all be taught by God.’ Therefore everyone who has heard and learned from the Father comes to Me.\n",
      " Then out of the smoke locusts came upon the earth. And to them was given power, as the scorpions of the earth have power.\n",
      " Jesus said to him, “Rise, take up your bed and walk.”\n",
      "\n",
      "==> Luhyia/train.lh <==\n",
      " Yali,inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka. \n",
      " Ndiitsanga bwangu ohande khu aka oli ninako, kho mbu, omundu yesi yesi,alabukulakhwo olusimbilwo tawe. \n",
      " Omusinjilili wabelihe oyo yenya okhumanya eshichila,Abayahudi nibenjililanga Paulo itookho. Kho inyanga,yalondakhwo yabolola eminyololo echia bali nibaboyile,Paulo mana nalaka abesaaliisi abakhongo nende abeshiina,boosi okhwakaana. Mana nayila Paulo namusinjisia imbeli,wabwe.\n",
      "\n",
      " Yaboola ako khulwokhumanyia,shinga lwokhufwakhwe khwitsa okhuba. \n",
      " Kho nibaboolela omukhasi oyo bari, “Ifwe,shikhusuubile khulwa okhubela aka iwe okhuboolile ta habula khusuubiile shichila mbu, abeene khwitsile,nikhuhulila nayaala, ne bulano khumanyile mbu, niye,omuhonia wabandu boosi.” Yesu ahonia omwana womusesia,\n",
      " Nebutswa,wihanye okhurula khutsingano tsiabutswa etsilakhoyeele,ta, winasie okhweka amakhuwa amalayi kobulamu obwa,eshikristo. \n",
      " Abalakusi,bahandika mbu, ‘Buli mundu aliechesibwa nende,Nyasaye.’ Kho oyo yesi ouhulilanga aka Papa nende,okhweka okhurula khuye, yetsa khwisie. \n",
      " Ne tsisiche nitsirula mumwosi nitsiba khushialo nitsihelesibwe obunyali obufwana shinga obwa amakatia,keshialo. \n",
      " Yesu namuboolela ari, “Sinjila wituushe omukekwo,mana ochende.” \n",
      "==> Luhyia/dev.bpe.en <==\n",
      " They brought him who for@@ mer@@ ly was blind to the Pharise@@ es.\n",
      " And whoever li@@ ves and believ@@ es in Me shall never di@@ e. Do you believe this@@ ?”\n",
      " Then he took it dow@@ n, w@@ ra@@ pp@@ ed it in lin@@ en, and laid it in a tom@@ b that was he@@ w@@ n out of the ro@@ ck@@ , where no one had ever la@@ in be@@ for@@ e.\n",
      " Now when they had es@@ ca@@ pe@@ d, they then found out that the is@@ land was called M@@ al@@ ta@@ .\n",
      " Nevertheles@@ s she will be sa@@ ved in chil@@ d@@ be@@ ar@@ ing if they continu@@ e in faith, love, and hol@@ in@@ es@@ s, with self@@ -@@ con@@ tr@@ ol@@ . \n",
      "\n",
      " and this woman was a wi@@ do@@ w of about ei@@ ght@@ y@@ -@@ four year@@ s, who did not depar@@ t from the temple, but ser@@ ved God with f@@ ast@@ ings and pray@@ ers night and day@@ .\n",
      " (@@ as it is writt@@ en, “I have made you a father of many nati@@ on@@ s@@ ”@@ ) in the presence of Him whom he believ@@ ed@@ —@@ God, who gives life to the dead and call@@ s those things which do not ex@@ is@@ t as though they di@@ d;\n",
      " Now there is in Jerusalem by the S@@ he@@ ep G@@ ate a p@@ ool@@ , which is called in H@@ eb@@ re@@ w@@ , Beth@@ es@@ d@@ a, having five p@@ or@@ ch@@ es.\n",
      " Then he go@@ es and tak@@ es with him seven other spir@@ its more wi@@ cked than himself, and they enter and dwell ther@@ e; and the last st@@ ate of that man is wor@@ se than the first@@ . So shall it also be with this wi@@ cked gener@@ ation@@ .”\n",
      "\n",
      "==> Luhyia/dev.bpe.lh <==\n",
      " Kho niba@@ yila omundu owali omubo@@ fu oyo khu,@@ Abafari@@ sa@@ yo@@ . \n",
      " Ne yesi yesi,@@ ou@@ m@@ enya ne nas@@ uu@@ bila mw@@ isie, shi@@ ali@@ fwa tawe. O@@ suubil@@ a,@@ ako@@ ?” \n",
      " Mana nak@@ u@@ ru@@ sia,@@ khumusal@@ aba, nak@@ u@@ f@@ im@@ ba@@ khwo is@@ anda ye@@ ik@@ it@@ ani, ne,@@ nak@@ u@@ yab@@ ila mu@@ ng'@@ ani, ey@@ ali niya@@ yab@@ wa mul@@ w@@ anda y@@ omundu yesi yali nashili okhu@@ yab@@ il@@ w@@ amwo tawe. \n",
      " Olwa khwali khul@@ uk@@ uku ni@@ khwi@@ hon@@ oko@@ os@@ he khw@@ amala ni@@ khumany@@ a mbu, eshi@@ khala@@ chinga eshi@@ o,@@ shil@@ angwa mbu M@@ al@@ it@@ a. \n",
      " Nebutswa omukhasi al@@ ah@@ oni@@ bwa khulw@@ okhwi@@ bul@@ a,@@ aba@@ ana, naba natsi@@ ililila okhw@@ if@@ w@@ ila mubusuubili mubu@@ heel@@ i, nobu@@ takati@@ fu nende obw@@ it@@ e@@ mb@@ elesi@@ .\n",
      "\n",
      " Khandi yam@@ enya nali omule@@ khwa khulwemi@@ y@@ ik@@ a,@@ amakhumi mu@@ n@@ ane na@@ chin@@ e. Nebutswa emi@@ yika echi@@ o,@@ chi@@ osi, yam@@ enyanga butswa muhekalu@@ . Y@@ enam@@ ilanga,@@ OMWAMI Nyasaye eshilo neshi@@ te@@ er@@ e, nah@@ onga inz@@ ala,@@ nende okhusa@@ aya@@ . \n",
      " shinga olwa Amahandik@@ o,@@ kaboolanga mbu, “E@@ khu@@ kholile iwe okhuba s@@ amw@@ ana,@@ Am@@ ahanga am@@ anj@@ i.” Kho obusuubisie buno nobul@@ ayi imbeli,@@ wa Nyasaye, owa Aburahamu ya@@ su@@ bil@@ amwo oul@@ amu@@ s@@ injia,@@ aba@@ fu, khandi owa li@@ khuw@@ ali@@ e li@@ kholanga ebil@@ aliho t@@ a,okhu@@ ba@@ ho. \n",
      " N@@ ali e@@ bwen@@ eyo, li@@ ali@@ yo l@@ iti@@ ba,@@ el@@ il@@ angwa mulu@@ he@@ bur@@ ania mbu, B@@ etsi@@ z@@ at@@ sa, aham@@ bi,@@ khushi@@ li@@ bwa shil@@ angwa mbu, Eshi@@ amak@@ on@@ di@@ . L@@ iti@@ ba,@@ li@@ amaatsi elo liali nebi@@ ro@@ ok@@ oola bir@@ ano. \n",
      " nishi@@ kalu@@ kh@@ ayo shi@@ tsia okhul@@ anga ebishieno b@@ ind@@ i,@@ musafu ebi@@ bi muno, nibi@@ chel@@ ela okhum@@ eny@@ amw@@ o. Ne,@@ olunyuma lw@@ okhumw@@ injil@@ amw@@ o, omundu tsana aba obu@@ bi,@@ okhushila@@ khwo shinga olwa yali olw@@ amb@@ eli. A@@ ko nik@@ o,@@ ak@@ atsia okhwi@@ kholekha khubandu b@@ olwibulo ol@@ um@@ ayan@@ u,@@ lwa bul@@ ano@@ .” N@@ y@@ ina Yesu nende abaana bab@@ we, \n",
      "\n",
      "==> Luhyia/dev.en <==\n",
      " They brought him who formerly was blind to the Pharisees.\n",
      " And whoever lives and believes in Me shall never die. Do you believe this?”\n",
      " Then he took it down, wrapped it in linen, and laid it in a tomb that was hewn out of the rock, where no one had ever lain before.\n",
      " Now when they had escaped, they then found out that the island was called Malta.\n",
      " Nevertheless she will be saved in childbearing if they continue in faith, love, and holiness, with self-control. \n",
      "\n",
      " and this woman was a widow of about eighty-four years, who did not depart from the temple, but served God with fastings and prayers night and day.\n",
      " (as it is written, “I have made you a father of many nations”) in the presence of Him whom he believed—God, who gives life to the dead and calls those things which do not exist as though they did;\n",
      " Now there is in Jerusalem by the Sheep Gate a pool, which is called in Hebrew, Bethesda, having five porches.\n",
      " Then he goes and takes with him seven other spirits more wicked than himself, and they enter and dwell there; and the last state of that man is worse than the first. So shall it also be with this wicked generation.”\n",
      "\n",
      "==> Luhyia/dev.lh <==\n",
      " Kho nibayila omundu owali omubofu oyo khu,Abafarisayo. \n",
      " Ne yesi yesi,oumenya ne nasuubila mwisie, shialifwa tawe. Osuubila,ako?” \n",
      " Mana nakurusia,khumusalaba, nakufimbakhwo isanda yeikitani, ne,nakuyabila mung'ani, eyali niyayabwa mulwanda yomundu yesi yali nashili okhuyabilwamwo tawe. \n",
      " Olwa khwali khulukuku nikhwihonokooshe khwamala nikhumanya mbu, eshikhalachinga eshio,shilangwa mbu Malita. \n",
      " Nebutswa omukhasi alahonibwa khulwokhwibula,abaana, naba natsiililila okhwifwila mubusuubili mubuheeli, nobutakatifu nende obwitembelesi.\n",
      "\n",
      " Khandi yamenya nali omulekhwa khulwemiyika,amakhumi munane nachine. Nebutswa emiyika echio,chiosi, yamenyanga butswa muhekalu. Yenamilanga,OMWAMI Nyasaye eshilo neshiteere, nahonga inzala,nende okhusaaya. \n",
      " shinga olwa Amahandiko,kaboolanga mbu, “Ekhukholile iwe okhuba samwana,Amahanga amanji.” Kho obusuubisie buno nobulayi imbeli,wa Nyasaye, owa Aburahamu yasubilamwo oulamusinjia,abafu, khandi owa likhuwalie likholanga ebilaliho ta,okhubaho. \n",
      " Nali ebweneyo, lialiyo litiba,elilangwa muluheburania mbu, Betsizatsa, ahambi,khushilibwa shilangwa mbu, Eshiamakondi. Litiba,liamaatsi elo liali nebirookoola birano. \n",
      " nishikalukhayo shitsia okhulanga ebishieno bindi,musafu ebibi muno, nibichelela okhumenyamwo. Ne,olunyuma lwokhumwinjilamwo, omundu tsana aba obubi,okhushilakhwo shinga olwa yali olwambeli. Ako niko,akatsia okhwikholekha khubandu bolwibulo olumayanu,lwa bulano.” Nyina Yesu nende abaana babwe, \n"
     ]
    }
   ],
   "source": [
    "! head Luhyia/train.*\n",
    "! head Luhyia/dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_D0BqDaUOF_"
   },
   "outputs": [],
   "source": [
    "pre = '/content/gdrive/Shared drives/NMT_for_African_Language/'\n",
    "# Train data source\n",
    "filenames = [pre+'Kinyarwanda/train.en',pre+'Luhya/train.en']\n",
    "\n",
    "# Train data target\n",
    "filenames2 = [pre+'Kinyarwanda/train.rw',pre+'Luhya/train.lh']\n",
    "\n",
    "# Dev data source\n",
    "file1 = [pre+'Kinyarwanda/dev.en',pre+'Luhya/dev.en']\n",
    "\n",
    "# Dev data target\n",
    "file2 = [pre+'Kinyarwanda/dev.rw',pre+'Luhya/dev.lh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icPaGn2GlpM-"
   },
   "outputs": [],
   "source": [
    "# Changing to Multilingual3 directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5zhE2IimjPs"
   },
   "outputs": [],
   "source": [
    "# Procedure to create concatenated files\n",
    "def create_file(x,filename):\n",
    "  # Open filename in write mode\n",
    "  with open(filename, 'w') as outfile:\n",
    "      for names in x:\n",
    "          # Open each file in read mode\n",
    "          with open(names) as infile:\n",
    "              # read the data and write it in file3\n",
    "              outfile.write(infile.read())\n",
    "          outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ay8047V3oyeG"
   },
   "outputs": [],
   "source": [
    "create_file(filenames,'train.en')\n",
    "create_file(filenames2,'train.rw_lh')\n",
    "create_file(file1,'dev.en')\n",
    "create_file(file2,'dev.rw_lh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSCp4K92o-9_",
    "outputId": "a44a8e02-e6fe-4bbd-d331-b963b0d718a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[428127, 435877]\n",
      "[428127, 435877]\n",
      "[4368, 4448]\n",
      "[4368, 4448]\n"
     ]
    }
   ],
   "source": [
    "def empty_counter(x):\n",
    "  # Opening a file\n",
    "  infile = open(x,\"r\")\n",
    "  empty = []\n",
    "  \n",
    "  for i,line in enumerate(infile):\n",
    "    if not line.strip(): \n",
    "      empty.append(i)\n",
    "\n",
    "  return empty\n",
    "\n",
    "print(empty_counter('train.en'))\n",
    "print(empty_counter('train.rw_lh'))\n",
    "print(empty_counter('dev.en'))\n",
    "print(empty_counter('dev.rw_lh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "mnEREgvqp2SS"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Reference: https://thispointer.com/python-how-to-delete-specific-lines-in-a-file-in-a-memory-efficient-way/\n",
    "def delete_empty_lines(original_file, line_numbers):\n",
    "    \"\"\"In a file, delete the lines at line number in given list\"\"\"\n",
    "    is_skipped = False\n",
    "    counter = 0\n",
    "    # Create name of dummy / temporary file\n",
    "    dummy_file = original_file + '.bak'\n",
    "    # Open original file in read only mode and dummy file in write mode\n",
    "    with open(original_file, 'r') as read_obj, open(dummy_file, 'w') as write_obj:\n",
    "        # Line by line copy data from original file to dummy file\n",
    "        for line in read_obj:\n",
    "            # If current line number exist in list then skip copying that line\n",
    "            if counter not in line_numbers:\n",
    "                write_obj.write(line)\n",
    "            else:\n",
    "                is_skipped = True\n",
    "            counter += 1\n",
    "    # If any line is skipped then rename dummy file as original file\n",
    "    if is_skipped:\n",
    "        os.remove(original_file)\n",
    "        os.rename(dummy_file, original_file)\n",
    "    else:\n",
    "        os.remove(dummy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdwjw7VvqD4C"
   },
   "outputs": [],
   "source": [
    "# Cleaning created batch\n",
    "delete_empty_lines(\"train.en\",empty_counter('train.en'))\n",
    "delete_empty_lines(\"train.rw_lh\",empty_counter('train.rw_lh'))\n",
    "\n",
    "delete_empty_lines(\"dev.en\",empty_counter('dev.en'))\n",
    "delete_empty_lines(\"dev.rw_lh\",empty_counter('dev.rw_lh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dBtQjkaNlv8",
    "outputId": "a4bdffa9-0388-4de5-f232-3da1405b9b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(empty_counter('train.en'))\n",
    "print(empty_counter('train.rw_lh'))\n",
    "print(empty_counter('dev.en'))\n",
    "print(empty_counter('dev.rw_lh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rl8jhljXhz4"
   },
   "outputs": [],
   "source": [
    "# Code adapted from https://www.geeksforgeeks.org/count-number-of-lines-in-a-text-file-in-python/\n",
    "# Count lines in a file\n",
    "def count_lines(filename):\n",
    "  # Opening a file\n",
    "  file = open(filename,\"r\")\n",
    "  Counter = 0\n",
    "    \n",
    "  # Reading from file\n",
    "  Content = file.read()\n",
    "  CoList = Content.split(\"\\n\")\n",
    "    \n",
    "  for i in CoList:\n",
    "      if i:\n",
    "          Counter += 1\n",
    "            \n",
    "  return Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WD4C9tdzXpLL",
    "outputId": "9a2306c6-09e2-4fae-96d3-15f95e1e3704"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_lines('test.bpe.rw_lh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VouMO0lishSQ"
   },
   "source": [
    "### BPE codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5lK1btbKswMN",
    "outputId": "ff15521c-cfaf-46fe-ccb0-b69c4c6599eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual3/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 96 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 3.8 MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 41.3 MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.9.6-py3-none-any.whl (375 kB)\n",
      "\u001b[K     |████████████████████████████████| 375 kB 58.7 MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Collecting astroid<2.7,>=2.6.5\n",
      "  Downloading astroid-2.6.6-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 61.4 MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.9.3-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 69.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting typed-ast<1.5,>=1.4.0\n",
      "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 42.7 MB/s \n",
      "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 4.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=305b9f5f90437494d53e2d97a51e8a7943aa1c6d4beca4eec492cf5d0118ecc4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ip2z02d9/wheels/2e/a9/b9/84da571c44d33793bbb649c7f11a265dabc39b91f2462a1333\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68435 sha256=21cbbf6e6d000da117cc6d59d57bfe840c39406c5d9af0bade412851c4268941\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
      "Successfully built joeynmt wrapt\n",
      "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
      "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed astroid-2.6.6 isort-5.9.3 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.6 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOYk3OK4rw6d"
   },
   "outputs": [],
   "source": [
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt --output_path vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGHVoKL-3a8x"
   },
   "outputs": [],
   "source": [
    "# Applying BPE to tests\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test1.$src > test.bpe.en1\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test1.lh > test.bpe.lh\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test2.$src > test.bpe.en2\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test2.rw > test.bpe.rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mj0HdPZ6t5i3",
    "outputId": "52573601-3ba4-4f48-8c64-cb4c22e15c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Sentences\n",
      "B@@ ach@@ am@@ anga , okh@@ w@@ ik@@ hal@@ a kh@@ ub@@ if@@ umbi hab@@ undu w@@ ol@@ uy@@ ali m@@ um@@ as@@ abo , nende eb@@ if@@ umbi bi@@ ol@@ uy@@ ali bi@@ e@@ imb@@ eli mut@@ s@@ is@@ in@@ ag@@ og@@ i@@ . ,\n",
      "Mana aband@@ u ab@@ ali nib@@ em@@ i@@ ile imb@@ eli nib@@ am@@ uh@@ al@@ ab@@ ila , nib@@ amub@@ ool@@ ela mbu ah@@ ol@@ e@@ el@@ e t@@ si , ne@@ but@@ s@@ wa yam@@ e@@ et@@ a , but@@ s@@ wa okh@@ ul@@ anj@@ il@@ is@@ ia ob@@ ut@@ iny@@ u ari , “ O@@ mw@@ ana wa D@@ a@@ udi ! , W@@ um@@ be@@ el@@ e t@@ s@@ im@@ bab@@ asi ! ”\n",
      "Mana , nib@@ em@@ ba ol@@ w@@ im@@ bo ol@@ uy@@ ia bari “ N@@ iwe o@@ ukw@@ an@@ ile okh@@ ubuk@@ ula esh@@ it@@ abu esh@@ ik@@ anye , nende okh@@ w@@ ik@@ ula eb@@ ib@@ al@@ ik@@ ho bi@@ ash@@ i@@ o@@ . , O@@ kh@@ uba w@@ erwa , ne khul@@ wa okh@@ uf@@ wak@@ h@@ wo kh@@ w@@ esh@@ it@@ is@@ o , war@@ e@@ era kh@@ u Nyasaye aband@@ u okh@@ ur@@ ula mub@@ ul@@ i ol@@ wib@@ ul@@ o ol@@ ul@@ imi amahanga nende okh@@ ur@@ ula mut@@ s@@ imb@@ ia t@@ si@@ os@@ i@@ . ,\n",
      "hab@@ ula ow@@ enya , okh@@ uba om@@ ukh@@ ongo mw@@ iny@@ we , okh@@ u@@ ula abe om@@ uk@@ hal@@ ab@@ ani , wab@@ oo@@ si .\n",
      "Ab@@ uk@@ ula ob@@ ise bih@@ el@@ ile okh@@ w@@ ih@@ enga lik@@ ond@@ ol@@ ie , ne olwa , ar@@ ul@@ aho , y@@ eb@@ il@@ ila bw@@ ang@@ u shinga l@@ w@@ ob@@ w@@ en@@ ib@@ we b@@ uf@@ wan@@ a@@ . ,\n",
      "Combined BPE Vocab\n",
      "ointed\n",
      "Ā@@\n",
      "Ê@@\n",
      "Ă@@\n",
      ";@@\n",
      "̄@@\n",
      "Ï@@\n",
      "̆\n",
      "ϊ@@\n",
      "!@@\n"
     ]
    }
   ],
   "source": [
    "# Some output\n",
    "! echo \"BPE Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gSynCGijJUlV",
    "outputId": "c318898b-74af-4cb6-b6a4-941a84cb25da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.bpe.en <==\n",
      "But if anyone lov@@ es God , this one is known by Him .\n",
      "And the second is like it : ‘ You shall love your neigh@@ b@@ or as yourself . ’\n",
      "until the day in which He was taken up , after He through the H@@ o@@ ly Sp@@ ir@@ it had given command@@ ments to the apostles whom He had cho@@ s@@ en ,\n",
      "For what if some did not believe ? W@@ ill their un@@ beli@@ ef make the faith@@ ful@@ ness of God without ef@@ fect ?\n",
      "And when you go into a hous@@ eho@@ ld , gre@@ et it .\n",
      "And a very great mul@@ t@@ itude sp@@ read their clo@@ th@@ es on the ro@@ ad ; others c@@ ut down br@@ an@@ ch@@ es from the tre@@ es and sp@@ read them on the ro@@ ad .\n",
      "And we heard this vo@@ ice which came from heaven when we were with Him on the holy m@@ ount@@ ain .\n",
      "O@@ r those e@@ igh@@ te@@ en on whom the tower in S@@ il@@ o@@ am f@@ ell and k@@ ill@@ ed them , do you think that they were wor@@ se sin@@ n@@ ers than all other men who dw@@ el@@ t in Jerusalem ?\n",
      "For I be@@ ar him witness that he has a great z@@ eal for you , and those who are in L@@ a@@ od@@ ic@@ ea , and those in H@@ i@@ er@@ ap@@ ol@@ is .\n",
      "\n",
      "\n",
      "==> train.bpe.rw_lh <==\n",
      "N@@ eb@@ uts@@ wa , om@@ undu y@@ esi ou@@ he@@ ela Nyasaye , om@@ w@@ en@@ o@@ yo Nyasaye , yam@@ um@@ anya .\n",
      "El@@ i@@ ak@@ hab@@ ili el@@ ili shing@@ a el@@ o , nd@@ iel@@ ino mbu , ‘ O@@ he@@ el@@ e ow@@ ash@@ io shing@@ a ol@@ wa w@@ ih@@ e@@ ela , om@@ w@@ ene . ’\n",
      "okh@@ u@@ ula , kh@@ uny@@ anga e@@ ya yab@@ uk@@ ul@@ il@@ wak@@ h@@ wo n@@ ay@@ il@@ wa mw@@ ik@@ ul@@ u . N@@ e , n@@ ash@@ ili okh@@ uy@@ il@@ wa mw@@ ik@@ ul@@ u , y@@ ech@@ es@@ ia aba y@@ ali n@@ iy@@ a@@ ah@@ ula , okh@@ uba abar@@ um@@ ebe mub@@ uny@@ ali bwa R@@ o@@ ho O@@ mut@@ ak@@ at@@ if@@ u@@ . ,\n",
      "N@@ eb@@ uts@@ wa abandi kh@@ ub@@ o sh@@ ib@@ ali , abas@@ u@@ ub@@ il@@ wa ta . K@@ ho k@@ ano k@@ akh@@ am@@ any@@ is@@ ia mbu , Nyasaye , shi@@ ali om@@ us@@ u@@ ub@@ il@@ wa ta no@@ ho ?\n",
      "N@@ e ol@@ wa mw@@ inj@@ ila mun@@ z@@ u mub@@ ash@@ i@@ es@@ ie , om@@ ul@@ em@@ be .\n",
      "Ab@@ and@@ u ab@@ anj@@ i nib@@ a@@ ala , eb@@ if@@ wal@@ o bi@@ ab@@ we k@@ hum@@ uh@@ anda , ne abandi nib@@ ar@@ ema amas@@ aka k@@ em@@ is@@ a@@ ala nib@@ a@@ ala k@@ hum@@ uh@@ anda ok@@ wo .\n",
      "K@@ h@@ w@@ ali n@@ ik@@ h@@ ul@@ i , n@@ in@@ aye kh@@ ush@@ ik@@ ul@@ u esh@@ it@@ ak@@ at@@ if@@ u ol@@ wa kh@@ wah@@ ul@@ ila om@@ w@@ o@@ yo , n@@ ik@@ ur@@ ula mw@@ ik@@ ul@@ u e@@ wa Nyasaye .\n",
      "No@@ ho , m@@ up@@ a@@ ar@@ anga mbu , aband@@ u ek@@ hum@@ i nam@@ un@@ ane b@@ om@@ un@@ a@@ ara , kw@@ akw@@ ila nib@@ af@@ wa boo@@ si mul@@ uk@@ ongo l@@ wa S@@ il@@ o@@ amu , b@@ ali , abon@@ o@@ oni okh@@ ush@@ ila aband@@ u boo@@ si abam@@ eny@@ anga mu , Yerusalemu ?\n",
      "Es@@ ie om@@ w@@ ene end@@ i om@@ ut@@ erer@@ eri kh@@ ubuk@@ hal@@ aban@@ ib@@ we , ob@@ ut@@ iny@@ u kh@@ ul@@ w@@ eny@@ we nende kh@@ ul@@ wa aband@@ u ab@@ ali mu , L@@ a@@ od@@ ik@@ ia nende kh@@ ul@@ wa abo ab@@ ali H@@ i@@ er@@ ap@@ ol@@ i .\n",
      "\n",
      "\n",
      "==> train.en <==\n",
      "But if anyone loves God , this one is known by Him .\n",
      "And the second is like it : ‘ You shall love your neighbor as yourself . ’\n",
      "until the day in which He was taken up , after He through the Holy Spirit had given commandments to the apostles whom He had chosen ,\n",
      "For what if some did not believe ? Will their unbelief make the faithfulness of God without effect ?\n",
      "And when you go into a household , greet it .\n",
      "And a very great multitude spread their clothes on the road ; others cut down branches from the trees and spread them on the road .\n",
      "And we heard this voice which came from heaven when we were with Him on the holy mountain .\n",
      "Or those eighteen on whom the tower in Siloam fell and killed them , do you think that they were worse sinners than all other men who dwelt in Jerusalem ?\n",
      "For I bear him witness that he has a great zeal for you , and those who are in Laodicea , and those in Hierapolis .\n",
      "\n",
      "\n",
      "==> train.rw_lh <==\n",
      "Nebutswa , omundu yesi ouheela Nyasaye , omwenoyo Nyasaye , yamumanya .\n",
      "Eliakhabili elili shinga elo , ndielino mbu , ‘ Oheele owashio shinga olwa wiheela , omwene . ’\n",
      "okhuula , khunyanga eya yabukulilwakhwo nayilwa mwikulu . Ne , nashili okhuyilwa mwikulu , yechesia aba yali niyaahula , okhuba abarumebe mubunyali bwa Roho Omutakatifu. ,\n",
      "Nebutswa abandi khubo shibali , abasuubilwa ta . Kho kano kakhamanyisia mbu , Nyasaye , shiali omusuubilwa ta noho ?\n",
      "Ne olwa mwinjila munzu mubashiesie , omulembe .\n",
      "Abandu abanji nibaala , ebifwalo biabwe khumuhanda , ne abandi nibarema amasaka kemisaala nibaala khumuhanda okwo .\n",
      "Khwali nikhuli , ninaye khushikulu eshitakatifu olwa khwahulila omwoyo , nikurula mwikulu ewa Nyasaye .\n",
      "Noho , mupaaranga mbu , abandu ekhumi namunane bomunaara , kwakwila nibafwa boosi mulukongo lwa Siloamu , bali , abonooni okhushila abandu boosi abamenyanga mu , Yerusalemu ?\n",
      "Esie omwene endi omuterereri khubukhalabanibwe , obutinyu khulwenywe nende khulwa abandu abali mu , Laodikia nende khulwa abo abali Hierapoli .\n",
      "\n",
      "==> dev.bpe.en <==\n",
      "I do not say this to con@@ dem@@ n ; for I have said before that you are in our hearts , to d@@ ie together and to live together .\n",
      "So when they were fill@@ ed , He said to His disciples , “ G@@ ather up the f@@ r@@ ag@@ ments that remain , so that nothing is lost . ”\n",
      "When the D@@ ay of P@@ ent@@ ec@@ ost had fully come , they were all with one acc@@ ord in one place .\n",
      "For it has been declar@@ ed to me concer@@ ning you , my bre@@ th@@ ren , by those of Ch@@ lo@@ e ’ s hous@@ eho@@ ld , that there are cont@@ ent@@ ions among you .\n",
      "For “ who has known the mind of the Lord that he may instruc@@ t Him ? ” But we have the mind of Christ .\n",
      "And do not become id@@ ol@@ at@@ ers as were some of them . As it is written , “ The people s@@ at down to eat and dr@@ ink , and ro@@ se up to pl@@ ay . ”\n",
      "Now f@@ ive of them were wise , and f@@ ive were f@@ ool@@ ish .\n",
      "When He op@@ ened the second se@@ al , I heard the second living cre@@ ature saying , “ C@@ ome and see . ”\n",
      "But let none of you suf@@ fer as a mur@@ d@@ er@@ er , a th@@ ief , an ev@@ il@@ do@@ er , or as a bus@@ y@@ body in other people ’ s matters .\n",
      "\n",
      "\n",
      "==> dev.bpe.rw_lh <==\n",
      "Sh@@ i@@ emb@@ ool@@ anga k@@ ano , kh@@ ul@@ wa ok@@ hum@@ uk@@ hal@@ ach@@ ila esh@@ i@@ ina ta , okh@@ uba , shing@@ a , nd@@ amub@@ ool@@ ela kh@@ al@@ e , mul@@ i abah@@ e@@ el@@ wa mun@@ o kh@@ w@@ if@@ we , ne , kh@@ ub@@ et@@ s@@ anga hal@@ ala b@@ ul@@ i l@@ w@@ o@@ si , k@@ ata n@@ ik@@ h@@ uba ab@@ al@@ amu no@@ ho , n@@ ik@@ h@@ uf@@ wa .\n",
      "N@@ e ol@@ wa boo@@ si b@@ ali nib@@ ek@@ ure yab@@ ool@@ ela ab@@ e@@ ech@@ ib@@ e ari , “ Muk@@ h@@ ung@@ '@@ as@@ ie e@@ bit@@ onye bit@@ ony@@ ile , bi@@ o@@ si , k@@ ho mbu kh@@ ul@@ esh@@ e okh@@ us@@ as@@ i@@ akh@@ wo esh@@ ind@@ u shi@@ o@@ si shi@@ o@@ si t@@ awe . ”\n",
      "N@@ e ol@@ wa iny@@ anga ya P@@ end@@ ek@@ ote yo@@ l@@ a , abas@@ u@@ ub@@ ili boo@@ si , bak@@ h@@ ung@@ '@@ ana hab@@ undu hal@@ ala .\n",
      "O@@ kh@@ uba ab@@ a@@ ana b@@ ef@@ we aband@@ u bandi ab@@ om@@ un@@ z@@ u e@@ ya K@@ u@@ lo@@ e b@@ amb@@ ool@@ el@@ e but@@ s@@ wa , hab@@ ul@@ af@@ u mbu , ob@@ us@@ ool@@ o b@@ ul@@ i hak@@ ari mw@@ iny@@ we@@ . ,\n",
      "Sh@@ inga Amah@@ andiko kab@@ ool@@ anga mbu “ N@@ i@@ w@@ i@@ ina o@@ um@@ any@@ ile am@@ ap@@ a@@ aro aka O@@ mw@@ ami ? , N@@ i@@ w@@ i@@ ina oun@@ y@@ ala ok@@ hum@@ uc@@ hel@@ ela ? ” , N@@ eb@@ uts@@ wa if@@ we kh@@ ul@@ i nam@@ ay@@ il@@ il@@ is@@ io aka Kristo .\n",
      "no@@ ho k@@ ata , okh@@ w@@ in@@ am@@ ila eb@@ if@@ wan@@ ani , shing@@ a bal@@ ala kh@@ ub@@ o bak@@ hol@@ a , t@@ awe . Sh@@ inga ol@@ wa Amah@@ andiko kab@@ ool@@ anga mbu , “ Ab@@ and@@ u , b@@ ek@@ hal@@ a hasi okh@@ ul@@ ia l@@ is@@ abo el@@ i@@ am@@ ala lik@@ al@@ uk@@ h@@ ane okh@@ uba , esh@@ if@@ wab@@ wi esh@@ i@@ ob@@ um@@ e@@ esi nende ob@@ uy@@ il@@ ani . ”\n",
      "Bar@@ ano kh@@ ub@@ o , b@@ ali abay@@ ing@@ wa , ne bar@@ ano bandi b@@ ali ab@@ ach@@ esi .\n",
      "Mana E@@ sh@@ im@@ em@@ e shi@@ el@@ ik@@ ond@@ i n@@ ish@@ i@@ ik@@ ula esh@@ ib@@ al@@ ik@@ ho , shi@@ ak@@ hab@@ ili ne n@@ imb@@ ul@@ ila esh@@ il@@ on@@ je esh@@ i@@ ak@@ hab@@ ili esh@@ il@@ im@@ w@@ o@@ yo n@@ ish@@ ib@@ ool@@ a sh@@ iri , “ Y@@ it@@ sa ! ”\n",
      "N@@ eb@@ uts@@ wa om@@ undu y@@ esi kh@@ w@@ iny@@ we al@@ any@@ as@@ ib@@ wa shing@@ a , om@@ uy@@ iri , no@@ ho om@@ w@@ if@@ i , no@@ ho om@@ uk@@ hol@@ i w@@ amak@@ h@@ uwa amab@@ i , no@@ ho om@@ undu we@@ ind@@ ob@@ o@@ yo t@@ awe .\n",
      "\n",
      "\n",
      "==> dev.en <==\n",
      "I do not say this to condemn ; for I have said before that you are in our hearts , to die together and to live together .\n",
      "So when they were filled , He said to His disciples , “ Gather up the fragments that remain , so that nothing is lost . ”\n",
      "When the Day of Pentecost had fully come , they were all with one accord in one place .\n",
      "For it has been declared to me concerning you , my brethren , by those of Chloe ’ s household , that there are contentions among you .\n",
      "For “ who has known the mind of the Lord that he may instruct Him ? ” But we have the mind of Christ .\n",
      "And do not become idolaters as were some of them . As it is written , “ The people sat down to eat and drink , and rose up to play . ”\n",
      "Now five of them were wise , and five were foolish .\n",
      "When He opened the second seal , I heard the second living creature saying , “ Come and see . ”\n",
      "But let none of you suffer as a murderer , a thief , an evildoer , or as a busybody in other people ’ s matters .\n",
      "\n",
      "\n",
      "==> dev.rw_lh <==\n",
      "Shiemboolanga kano , khulwa okhumukhalachila eshiina ta , okhuba , shinga , ndamuboolela khale , muli abaheelwa muno khwifwe , ne , khubetsanga halala buli lwosi , kata nikhuba abalamu noho , nikhufwa .\n",
      "Ne olwa boosi bali nibekure yaboolela abeechibe ari , “ Mukhung'asie ebitonye bitonyile , biosi , kho mbu khuleshe okhusasiakhwo eshindu shiosi shiosi tawe . ”\n",
      "Ne olwa inyanga ya Pendekote yola , abasuubili boosi , bakhung'ana habundu halala .\n",
      "Okhuba abaana befwe abandu bandi abomunzu eya Kuloe bamboolele butswa , habulafu mbu , obusoolo buli hakari mwinywe. ,\n",
      "Shinga Amahandiko kaboolanga mbu “ Niwiina oumanyile amapaaro aka Omwami ? , Niwiina ounyala okhumuchelela ? ” , Nebutswa ifwe khuli namayililisio aka Kristo .\n",
      "noho kata , okhwinamila ebifwanani , shinga balala khubo bakhola , tawe . Shinga olwa Amahandiko kaboolanga mbu , “ Abandu , bekhala hasi okhulia lisabo eliamala likalukhane okhuba , eshifwabwi eshiobumeesi nende obuyilani . ”\n",
      "Barano khubo , bali abayingwa , ne barano bandi bali abachesi .\n",
      "Mana Eshimeme shielikondi nishiikula eshibalikho , shiakhabili ne nimbulila eshilonje eshiakhabili eshilimwoyo nishiboola shiri , “ Yitsa ! ”\n",
      "Nebutswa omundu yesi khwinywe alanyasibwa shinga , omuyiri , noho omwifi , noho omukholi wamakhuwa amabi , noho omundu weindoboyo tawe .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! tail train.*\n",
    "! tail dev.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9-N3k9-OO9-"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plc04U6hOQ9u"
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "b8Smh87evI9G"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language, source_language)\n",
    "path = \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3\"\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"{path}/train.bpe\"\n",
    "    dev:   \"{path}/dev.bpe\"\n",
    "    test:  \"{path}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"{path}/vocab.txt\"\n",
    "    trg_vocab: \"{path}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 2000\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 200\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer\"\n",
    "    overwrite: True \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, path=path, source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ej5aVkIwwHu7",
    "outputId": "3f74b2ed-cbde-4863-925b-07cfaf7086d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-02 08:34:04,737 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-02 08:34:04,801 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-02 08:34:12,559 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-02 08:34:12,838 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-02 08:34:12,930 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-02 08:34:13,167 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-02 08:34:13,167 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-02 08:34:14,605 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-02 08:34:14.851194: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-02 08:34:16,474 - INFO - joeynmt.training - Total params: 12177920\n",
      "2021-08-02 08:34:20,035 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
      "2021-08-02 08:34:20,035 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
      "2021-08-02 08:34:20,035 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-02 08:34:20,035 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
      "2021-08-02 08:34:20,035 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
      "2021-08-02 08:34:20,036 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
      "2021-08-02 08:34:20,036 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-02 08:34:20,036 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-02 08:34:20,036 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-02 08:34:20,036 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-02 08:34:20,036 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-02 08:34:20,036 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-02 08:34:20,036 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-02 08:34:20,037 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-02 08:34:20,037 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-02 08:34:20,037 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-02 08:34:20,037 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-02 08:34:20,037 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-02 08:34:20,037 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-02 08:34:20,037 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-02 08:34:20,037 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-02 08:34:20,038 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-02 08:34:20,038 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-02 08:34:20,038 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-02 08:34:20,038 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-02 08:34:20,038 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-02 08:34:20,038 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-02 08:34:20,039 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-02 08:34:20,039 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-02 08:34:20,039 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-02 08:34:20,039 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-02 08:34:20,039 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-02 08:34:20,039 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-02 08:34:20,039 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-02 08:34:20,039 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-02 08:34:20,040 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-02 08:34:20,040 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-02 08:34:20,040 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer\n",
      "2021-08-02 08:34:20,040 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-02 08:34:20,040 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-02 08:34:20,040 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-02 08:34:20,040 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-02 08:34:20,040 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-02 08:34:20,041 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-02 08:34:20,041 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-02 08:34:20,041 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-02 08:34:20,041 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-02 08:34:20,041 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-02 08:34:20,041 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-02 08:34:20,042 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-02 08:34:20,042 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-02 08:34:20,042 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-02 08:34:20,042 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-02 08:34:20,042 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-02 08:34:20,042 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-02 08:34:20,042 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-02 08:34:20,043 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-02 08:34:20,043 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-02 08:34:20,043 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-02 08:34:20,044 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-02 08:34:20,044 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-02 08:34:20,044 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-02 08:34:20,044 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-02 08:34:20,044 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-02 08:34:20,045 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-02 08:34:20,045 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-02 08:34:20,045 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-02 08:34:20,045 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-02 08:34:20,045 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-02 08:34:20,045 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434091,\n",
      "\tvalid 4447,\n",
      "\ttest 79\n",
      "2021-08-02 08:34:20,046 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-02 08:34:20,046 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-02 08:34:20,046 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-02 08:34:20,046 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
      "2021-08-02 08:34:20,047 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
      "2021-08-02 08:34:20,047 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
      "2021-08-02 08:34:20,071 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-02 08:34:20,072 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-02 08:34:46,704 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.467759, Tokens per Sec:    16264, Lr: 0.000300\n",
      "2021-08-02 08:35:13,221 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.010992, Tokens per Sec:    16848, Lr: 0.000300\n",
      "2021-08-02 08:35:40,190 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.775280, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-08-02 08:36:07,926 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.614727, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-08-02 08:36:35,259 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.440430, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-02 08:37:02,359 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.539218, Tokens per Sec:    16330, Lr: 0.000300\n",
      "2021-08-02 08:37:29,708 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.506474, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-08-02 08:37:56,784 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.984763, Tokens per Sec:    16058, Lr: 0.000300\n",
      "2021-08-02 08:38:24,079 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.809094, Tokens per Sec:    16166, Lr: 0.000300\n",
      "2021-08-02 08:38:51,298 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.131192, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-08-02 08:39:18,067 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.688938, Tokens per Sec:    16129, Lr: 0.000300\n",
      "2021-08-02 08:39:45,449 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.652559, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-08-02 08:40:12,425 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.620502, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-02 08:40:39,624 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.779252, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-02 08:41:06,912 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.463588, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-02 08:41:34,018 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.800351, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-08-02 08:42:01,237 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.371956, Tokens per Sec:    16330, Lr: 0.000300\n",
      "2021-08-02 08:42:28,391 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.547623, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-02 08:42:55,375 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     3.599836, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-08-02 08:43:22,409 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     3.366376, Tokens per Sec:    16169, Lr: 0.000300\n",
      "2021-08-02 08:43:49,515 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     3.405528, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-02 08:44:16,614 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     3.309952, Tokens per Sec:    16178, Lr: 0.000300\n",
      "2021-08-02 08:44:43,689 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     3.434138, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-02 08:45:10,698 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     3.300473, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-02 08:45:37,957 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     3.019615, Tokens per Sec:    16196, Lr: 0.000300\n",
      "2021-08-02 08:47:10,995 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:47:10,995 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:47:10,996 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:47:12,335 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:47:12,335 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:47:13,081 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:47:13,082 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 08:47:13,082 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 08:47:13,082 - INFO - joeynmt.training - \tHypothesis: He said : “ I have been a God , but I am the God . ”\n",
      "2021-08-02 08:47:13,083 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:47:13,083 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 08:47:13,083 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 08:47:13,083 - INFO - joeynmt.training - \tHypothesis: King Solomon wrote : “ The Israelites are in the day of Jehovah , and we are not to be saved to the sign of the sign of Jehovah . ”\n",
      "2021-08-02 08:47:13,084 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:47:13,084 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 08:47:13,084 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 08:47:13,084 - INFO - joeynmt.training - \tHypothesis: Secember : The prophecy of the prophecy of Isaiah , the Messiah was written in the Kingdom Hall .\n",
      "2021-08-02 08:47:13,084 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:47:13,085 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 08:47:13,085 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 08:47:13,085 - INFO - joeynmt.training - \tHypothesis: How does Christ “ the Lord ” and “ the Devil ” ?\n",
      "2021-08-02 08:47:13,086 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     5000: bleu:   6.49, loss: 380856.1875, ppl:  21.5256, duration: 95.1282s\n",
      "2021-08-02 08:47:40,464 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     3.566763, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-02 08:48:07,581 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     3.135911, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-08-02 08:48:16,358 - INFO - joeynmt.training - Epoch   1: total training loss 21202.24\n",
      "2021-08-02 08:48:16,359 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-02 08:48:35,328 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     3.448527, Tokens per Sec:    15483, Lr: 0.000300\n",
      "2021-08-02 08:49:02,482 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     3.118887, Tokens per Sec:    16543, Lr: 0.000300\n",
      "2021-08-02 08:49:29,725 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     3.218023, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-02 08:49:56,822 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     2.930922, Tokens per Sec:    16352, Lr: 0.000300\n",
      "2021-08-02 08:50:23,941 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     3.264126, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-08-02 08:50:51,007 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     3.064362, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-08-02 08:51:18,010 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     3.444261, Tokens per Sec:    16277, Lr: 0.000300\n",
      "2021-08-02 08:51:45,359 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     3.087311, Tokens per Sec:    16289, Lr: 0.000300\n",
      "2021-08-02 08:52:12,447 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     2.873076, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-02 08:52:39,487 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     3.007529, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-08-02 08:53:06,203 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     3.036778, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-08-02 08:53:33,502 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     2.700362, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-08-02 08:54:00,537 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     3.065008, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-02 08:54:27,836 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     3.094694, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-08-02 08:54:55,145 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     3.108602, Tokens per Sec:    16108, Lr: 0.000300\n",
      "2021-08-02 08:55:22,199 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     2.673821, Tokens per Sec:    16239, Lr: 0.000300\n",
      "2021-08-02 08:55:49,530 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     2.831945, Tokens per Sec:    16283, Lr: 0.000300\n",
      "2021-08-02 08:56:16,376 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     2.904048, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-02 08:56:43,623 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     2.704943, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-08-02 08:57:10,640 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     2.555255, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-08-02 08:57:37,707 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     2.789951, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-08-02 08:58:05,013 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     2.982403, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-08-02 08:58:32,076 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     3.071900, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-08-02 09:00:10,022 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:00:10,022 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:00:10,022 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:00:11,311 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:00:11,312 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:00:12,109 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:00:12,110 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:00:12,111 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:00:12,111 - INFO - joeynmt.training - \tHypothesis: He said : “ I have been given to God , but I have a God - given spirit . ”\n",
      "2021-08-02 09:00:12,111 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:00:12,113 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:00:12,113 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:00:12,113 - INFO - joeynmt.training - \tHypothesis: King Solomon said : “ We are not a sort of Jehovah , and we are many of the most important things that we have been saved . ”\n",
      "2021-08-02 09:00:12,113 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:00:12,114 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:00:12,114 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:00:12,114 - INFO - joeynmt.training - \tHypothesis: Certain : Disodus will be fulfilled in the book of Daniel , which is a Kingdom of God’s Kingdom .\n",
      "2021-08-02 09:00:12,114 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:00:12,115 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:00:12,115 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:00:12,115 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on the watch ” and “ the wicked ” ?\n",
      "2021-08-02 09:00:12,115 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    10000: bleu:  11.24, loss: 324362.8438, ppl:  13.6532, duration: 100.0391s\n",
      "2021-08-02 09:00:39,756 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     2.892188, Tokens per Sec:    16114, Lr: 0.000300\n",
      "2021-08-02 09:01:06,759 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     2.622734, Tokens per Sec:    16283, Lr: 0.000300\n",
      "2021-08-02 09:01:33,997 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     2.940567, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-02 09:02:01,016 - INFO - joeynmt.training - Epoch   2, Step:    10800, Batch Loss:     2.751576, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-02 09:02:17,071 - INFO - joeynmt.training - Epoch   2: total training loss 16013.47\n",
      "2021-08-02 09:02:17,071 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-02 09:02:28,746 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     2.899813, Tokens per Sec:    15555, Lr: 0.000300\n",
      "2021-08-02 09:02:56,067 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     2.721923, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-08-02 09:03:23,289 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     2.839511, Tokens per Sec:    15745, Lr: 0.000300\n",
      "2021-08-02 09:03:50,495 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     2.606316, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-08-02 09:04:17,617 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     2.868169, Tokens per Sec:    15921, Lr: 0.000300\n",
      "2021-08-02 09:04:44,828 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     2.555382, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-02 09:05:11,484 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     2.484423, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-08-02 09:05:38,681 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     2.628749, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-02 09:06:05,797 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     2.739856, Tokens per Sec:    16353, Lr: 0.000300\n",
      "2021-08-02 09:06:33,073 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     2.464047, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-02 09:06:59,995 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     2.632920, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-02 09:07:27,116 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     2.726382, Tokens per Sec:    16392, Lr: 0.000300\n",
      "2021-08-02 09:07:54,410 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     2.421372, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-02 09:08:21,428 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     2.371274, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-08-02 09:08:48,313 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     2.777992, Tokens per Sec:    16238, Lr: 0.000300\n",
      "2021-08-02 09:09:15,452 - INFO - joeynmt.training - Epoch   3, Step:    14000, Batch Loss:     2.854669, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-02 09:09:42,473 - INFO - joeynmt.training - Epoch   3, Step:    14200, Batch Loss:     2.627899, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-02 09:10:09,529 - INFO - joeynmt.training - Epoch   3, Step:    14400, Batch Loss:     2.706284, Tokens per Sec:    16318, Lr: 0.000300\n",
      "2021-08-02 09:10:36,589 - INFO - joeynmt.training - Epoch   3, Step:    14600, Batch Loss:     2.452060, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-02 09:11:03,286 - INFO - joeynmt.training - Epoch   3, Step:    14800, Batch Loss:     2.471935, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-08-02 09:11:30,652 - INFO - joeynmt.training - Epoch   3, Step:    15000, Batch Loss:     2.716199, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-08-02 09:13:08,800 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:13:08,801 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:13:08,801 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:13:10,013 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:13:10,013 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:13:10,730 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:13:10,731 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:13:10,731 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:13:10,731 - INFO - joeynmt.training - \tHypothesis: He said : “ But I have been given to God , that is the right thing to me . ”\n",
      "2021-08-02 09:13:10,731 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:13:10,732 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:13:10,732 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:13:10,732 - INFO - joeynmt.training - \tHypothesis: King Solomon ancient Israel said : “ Let us be saved to Jehovah , we are more important than the things of the sort of the sort of the sort of the silver . ”\n",
      "2021-08-02 09:13:10,732 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:13:10,732 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:13:10,733 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:13:10,733 - INFO - joeynmt.training - \tHypothesis: Cananaani : Don’t have the prophecy of Daniel to the Kingdom of God’s Kingdom .\n",
      "2021-08-02 09:13:10,733 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:13:10,733 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:13:10,733 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:13:10,734 - INFO - joeynmt.training - \tHypothesis: How does Christ “ keep on the sort of the wicked ” and bring him to him ?\n",
      "2021-08-02 09:13:10,734 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    15000: bleu:  15.12, loss: 292323.9375, ppl:  10.5463, duration: 100.0810s\n",
      "2021-08-02 09:13:37,910 - INFO - joeynmt.training - Epoch   3, Step:    15200, Batch Loss:     2.954492, Tokens per Sec:    15933, Lr: 0.000300\n",
      "2021-08-02 09:14:05,094 - INFO - joeynmt.training - Epoch   3, Step:    15400, Batch Loss:     2.655889, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-02 09:14:32,457 - INFO - joeynmt.training - Epoch   3, Step:    15600, Batch Loss:     2.378737, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-02 09:14:59,374 - INFO - joeynmt.training - Epoch   3, Step:    15800, Batch Loss:     3.132912, Tokens per Sec:    16206, Lr: 0.000300\n",
      "2021-08-02 09:15:26,757 - INFO - joeynmt.training - Epoch   3, Step:    16000, Batch Loss:     2.383016, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-08-02 09:15:53,809 - INFO - joeynmt.training - Epoch   3, Step:    16200, Batch Loss:     2.300127, Tokens per Sec:    16300, Lr: 0.000300\n",
      "2021-08-02 09:16:18,904 - INFO - joeynmt.training - Epoch   3: total training loss 14371.21\n",
      "2021-08-02 09:16:18,905 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-02 09:16:21,504 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     2.703365, Tokens per Sec:    12168, Lr: 0.000300\n",
      "2021-08-02 09:16:48,451 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     2.356190, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-02 09:17:15,181 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     2.443160, Tokens per Sec:    16181, Lr: 0.000300\n",
      "2021-08-02 09:17:42,397 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     2.766623, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-08-02 09:18:09,404 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     2.452398, Tokens per Sec:    16418, Lr: 0.000300\n",
      "2021-08-02 09:18:36,458 - INFO - joeynmt.training - Epoch   4, Step:    17400, Batch Loss:     2.577488, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-02 09:19:03,431 - INFO - joeynmt.training - Epoch   4, Step:    17600, Batch Loss:     2.854709, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-08-02 09:19:30,717 - INFO - joeynmt.training - Epoch   4, Step:    17800, Batch Loss:     2.345048, Tokens per Sec:    16396, Lr: 0.000300\n",
      "2021-08-02 09:19:57,774 - INFO - joeynmt.training - Epoch   4, Step:    18000, Batch Loss:     2.763969, Tokens per Sec:    16354, Lr: 0.000300\n",
      "2021-08-02 09:20:24,934 - INFO - joeynmt.training - Epoch   4, Step:    18200, Batch Loss:     2.573262, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-02 09:20:52,004 - INFO - joeynmt.training - Epoch   4, Step:    18400, Batch Loss:     2.436239, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-08-02 09:21:19,002 - INFO - joeynmt.training - Epoch   4, Step:    18600, Batch Loss:     2.983008, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-08-02 09:21:46,104 - INFO - joeynmt.training - Epoch   4, Step:    18800, Batch Loss:     2.381914, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-08-02 09:22:13,033 - INFO - joeynmt.training - Epoch   4, Step:    19000, Batch Loss:     2.530374, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-08-02 09:22:40,418 - INFO - joeynmt.training - Epoch   4, Step:    19200, Batch Loss:     2.653968, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-02 09:23:07,335 - INFO - joeynmt.training - Epoch   4, Step:    19400, Batch Loss:     2.459174, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-08-02 09:23:34,475 - INFO - joeynmt.training - Epoch   4, Step:    19600, Batch Loss:     2.330763, Tokens per Sec:    16108, Lr: 0.000300\n",
      "2021-08-02 09:24:01,539 - INFO - joeynmt.training - Epoch   4, Step:    19800, Batch Loss:     2.328113, Tokens per Sec:    16396, Lr: 0.000300\n",
      "2021-08-02 09:24:28,226 - INFO - joeynmt.training - Epoch   4, Step:    20000, Batch Loss:     2.450736, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-08-02 09:26:05,875 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:26:05,876 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:26:05,876 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:26:07,198 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:26:07,199 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:26:08,021 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:26:08,022 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:26:08,022 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:26:08,022 - INFO - joeynmt.training - \tHypothesis: He said : “ But it is to be a God that is to be a good to me . ”\n",
      "2021-08-02 09:26:08,023 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:26:08,023 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:26:08,023 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:26:08,023 - INFO - joeynmt.training - \tHypothesis: “ Let us keep on the one that is the one who is in the eyes of Jehovah , ” says Solomon ancient Israel , “ we are more than the things that are going to be replaced . ”\n",
      "2021-08-02 09:26:08,023 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:26:08,024 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:26:08,024 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:26:08,024 - INFO - joeynmt.training - \tHypothesis: Canada : Dr another prophecy in the book of Daniel has been represented on God’s Kingdom .\n",
      "2021-08-02 09:26:08,024 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:26:08,025 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:26:08,025 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:26:08,025 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on the watch ” and bring him to him a person ?\n",
      "2021-08-02 09:26:08,025 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    20000: bleu:  16.91, loss: 275248.5625, ppl:   9.1905, duration: 99.7984s\n",
      "2021-08-02 09:26:35,593 - INFO - joeynmt.training - Epoch   4, Step:    20200, Batch Loss:     2.393131, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-02 09:27:02,540 - INFO - joeynmt.training - Epoch   4, Step:    20400, Batch Loss:     2.415057, Tokens per Sec:    16249, Lr: 0.000300\n",
      "2021-08-02 09:27:29,845 - INFO - joeynmt.training - Epoch   4, Step:    20600, Batch Loss:     2.236806, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-02 09:27:56,700 - INFO - joeynmt.training - Epoch   4, Step:    20800, Batch Loss:     2.404798, Tokens per Sec:    16251, Lr: 0.000300\n",
      "2021-08-02 09:28:23,658 - INFO - joeynmt.training - Epoch   4, Step:    21000, Batch Loss:     2.276371, Tokens per Sec:    16056, Lr: 0.000300\n",
      "2021-08-02 09:28:50,822 - INFO - joeynmt.training - Epoch   4, Step:    21200, Batch Loss:     2.462265, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-02 09:29:17,777 - INFO - joeynmt.training - Epoch   4, Step:    21400, Batch Loss:     2.547759, Tokens per Sec:    16320, Lr: 0.000300\n",
      "2021-08-02 09:29:44,727 - INFO - joeynmt.training - Epoch   4, Step:    21600, Batch Loss:     2.446071, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-08-02 09:30:11,667 - INFO - joeynmt.training - Epoch   4, Step:    21800, Batch Loss:     2.408332, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-02 09:30:19,318 - INFO - joeynmt.training - Epoch   4: total training loss 13391.90\n",
      "2021-08-02 09:30:19,318 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-02 09:30:39,410 - INFO - joeynmt.training - Epoch   5, Step:    22000, Batch Loss:     2.165271, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-08-02 09:31:06,444 - INFO - joeynmt.training - Epoch   5, Step:    22200, Batch Loss:     2.498509, Tokens per Sec:    16196, Lr: 0.000300\n",
      "2021-08-02 09:31:33,720 - INFO - joeynmt.training - Epoch   5, Step:    22400, Batch Loss:     2.095824, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-02 09:32:00,503 - INFO - joeynmt.training - Epoch   5, Step:    22600, Batch Loss:     2.048044, Tokens per Sec:    16302, Lr: 0.000300\n",
      "2021-08-02 09:32:27,687 - INFO - joeynmt.training - Epoch   5, Step:    22800, Batch Loss:     2.467491, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-08-02 09:32:54,899 - INFO - joeynmt.training - Epoch   5, Step:    23000, Batch Loss:     2.370018, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-08-02 09:33:21,913 - INFO - joeynmt.training - Epoch   5, Step:    23200, Batch Loss:     2.362868, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-02 09:33:49,004 - INFO - joeynmt.training - Epoch   5, Step:    23400, Batch Loss:     2.598476, Tokens per Sec:    16083, Lr: 0.000300\n",
      "2021-08-02 09:34:15,945 - INFO - joeynmt.training - Epoch   5, Step:    23600, Batch Loss:     2.283217, Tokens per Sec:    16444, Lr: 0.000300\n",
      "2021-08-02 09:34:43,070 - INFO - joeynmt.training - Epoch   5, Step:    23800, Batch Loss:     2.163183, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-08-02 09:35:09,769 - INFO - joeynmt.training - Epoch   5, Step:    24000, Batch Loss:     2.331916, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-02 09:35:36,864 - INFO - joeynmt.training - Epoch   5, Step:    24200, Batch Loss:     2.227870, Tokens per Sec:    16309, Lr: 0.000300\n",
      "2021-08-02 09:36:04,146 - INFO - joeynmt.training - Epoch   5, Step:    24400, Batch Loss:     2.320628, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-08-02 09:36:31,252 - INFO - joeynmt.training - Epoch   5, Step:    24600, Batch Loss:     2.436585, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-02 09:36:58,153 - INFO - joeynmt.training - Epoch   5, Step:    24800, Batch Loss:     2.309869, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-02 09:37:25,326 - INFO - joeynmt.training - Epoch   5, Step:    25000, Batch Loss:     2.177998, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-02 09:39:12,744 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:39:12,744 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:39:12,745 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:39:13,980 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:39:13,981 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:39:14,704 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:39:14,705 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:39:14,705 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:39:14,705 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been prepared to draw close to God , that he is good to me . ”\n",
      "2021-08-02 09:39:14,705 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:39:14,706 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:39:14,707 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:39:14,707 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , the most riches of the riches of the representation . ”\n",
      "2021-08-02 09:39:14,707 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:39:14,707 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:39:14,708 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:39:14,708 - INFO - joeynmt.training - \tHypothesis: Canaesius : Day another prophecy in the book of Daniel revealed what it says about God’s Kingdom .\n",
      "2021-08-02 09:39:14,708 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:39:14,708 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:39:14,708 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:39:14,709 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on the ground ” and bring it to him ?\n",
      "2021-08-02 09:39:14,709 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    25000: bleu:  18.78, loss: 261483.2969, ppl:   8.2255, duration: 109.3823s\n",
      "2021-08-02 09:39:42,299 - INFO - joeynmt.training - Epoch   5, Step:    25200, Batch Loss:     2.145916, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-02 09:40:09,259 - INFO - joeynmt.training - Epoch   5, Step:    25400, Batch Loss:     2.153238, Tokens per Sec:    16216, Lr: 0.000300\n",
      "2021-08-02 09:40:36,441 - INFO - joeynmt.training - Epoch   5, Step:    25600, Batch Loss:     2.196393, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-02 09:41:03,596 - INFO - joeynmt.training - Epoch   5, Step:    25800, Batch Loss:     2.276237, Tokens per Sec:    16053, Lr: 0.000300\n",
      "2021-08-02 09:41:30,645 - INFO - joeynmt.training - Epoch   5, Step:    26000, Batch Loss:     2.307131, Tokens per Sec:    16275, Lr: 0.000300\n",
      "2021-08-02 09:41:57,720 - INFO - joeynmt.training - Epoch   5, Step:    26200, Batch Loss:     2.072987, Tokens per Sec:    16496, Lr: 0.000300\n",
      "2021-08-02 09:42:24,755 - INFO - joeynmt.training - Epoch   5, Step:    26400, Batch Loss:     2.298010, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-08-02 09:42:51,744 - INFO - joeynmt.training - Epoch   5, Step:    26600, Batch Loss:     2.270513, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-02 09:43:18,757 - INFO - joeynmt.training - Epoch   5, Step:    26800, Batch Loss:     2.604651, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-08-02 09:43:45,924 - INFO - joeynmt.training - Epoch   5, Step:    27000, Batch Loss:     2.249605, Tokens per Sec:    16188, Lr: 0.000300\n",
      "2021-08-02 09:44:13,208 - INFO - joeynmt.training - Epoch   5, Step:    27200, Batch Loss:     2.144746, Tokens per Sec:    16453, Lr: 0.000300\n",
      "2021-08-02 09:44:29,046 - INFO - joeynmt.training - Epoch   5: total training loss 12702.43\n",
      "2021-08-02 09:44:29,047 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-02 09:44:41,018 - INFO - joeynmt.training - Epoch   6, Step:    27400, Batch Loss:     2.042599, Tokens per Sec:    15419, Lr: 0.000300\n",
      "2021-08-02 09:45:08,017 - INFO - joeynmt.training - Epoch   6, Step:    27600, Batch Loss:     2.214181, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-02 09:45:35,144 - INFO - joeynmt.training - Epoch   6, Step:    27800, Batch Loss:     2.071435, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-02 09:46:02,329 - INFO - joeynmt.training - Epoch   6, Step:    28000, Batch Loss:     2.015456, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-08-02 09:46:29,239 - INFO - joeynmt.training - Epoch   6, Step:    28200, Batch Loss:     2.241901, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-02 09:46:56,298 - INFO - joeynmt.training - Epoch   6, Step:    28400, Batch Loss:     2.252993, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-02 09:47:23,264 - INFO - joeynmt.training - Epoch   6, Step:    28600, Batch Loss:     2.379774, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-08-02 09:47:50,371 - INFO - joeynmt.training - Epoch   6, Step:    28800, Batch Loss:     2.234816, Tokens per Sec:    16217, Lr: 0.000300\n",
      "2021-08-02 09:48:17,494 - INFO - joeynmt.training - Epoch   6, Step:    29000, Batch Loss:     2.220130, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-02 09:48:44,551 - INFO - joeynmt.training - Epoch   6, Step:    29200, Batch Loss:     2.328846, Tokens per Sec:    16072, Lr: 0.000300\n",
      "2021-08-02 09:49:11,772 - INFO - joeynmt.training - Epoch   6, Step:    29400, Batch Loss:     2.204006, Tokens per Sec:    16437, Lr: 0.000300\n",
      "2021-08-02 09:49:38,999 - INFO - joeynmt.training - Epoch   6, Step:    29600, Batch Loss:     2.625598, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-02 09:50:06,155 - INFO - joeynmt.training - Epoch   6, Step:    29800, Batch Loss:     2.049171, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-08-02 09:50:33,487 - INFO - joeynmt.training - Epoch   6, Step:    30000, Batch Loss:     2.578643, Tokens per Sec:    16421, Lr: 0.000300\n",
      "2021-08-02 09:52:09,897 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:52:09,898 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:52:09,898 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:52:11,236 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:52:11,236 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:52:12,005 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:52:12,006 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 09:52:12,006 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 09:52:12,007 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been dedicated to God is the good of me . ”\n",
      "2021-08-02 09:52:12,007 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:52:12,007 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 09:52:12,007 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 09:52:12,008 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are not in the fear of Jehovah , and we are more than the riches of the representation . ”\n",
      "2021-08-02 09:52:12,008 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:52:12,008 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 09:52:12,008 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 09:52:12,009 - INFO - joeynmt.training - \tHypothesis: Canizon : Dore another prophecy in the book of Daniel reports what it says about God’s Kingdom .\n",
      "2021-08-02 09:52:12,009 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:52:12,009 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 09:52:12,009 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 09:52:12,010 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep on the sake of the one ” and bring it to him ?\n",
      "2021-08-02 09:52:12,010 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    30000: bleu:  19.91, loss: 252243.2656, ppl:   7.6353, duration: 98.5229s\n",
      "2021-08-02 09:52:39,218 - INFO - joeynmt.training - Epoch   6, Step:    30200, Batch Loss:     2.130486, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-02 09:53:06,246 - INFO - joeynmt.training - Epoch   6, Step:    30400, Batch Loss:     1.860519, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-08-02 09:53:33,414 - INFO - joeynmt.training - Epoch   6, Step:    30600, Batch Loss:     2.312668, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-02 09:54:00,442 - INFO - joeynmt.training - Epoch   6, Step:    30800, Batch Loss:     1.978505, Tokens per Sec:    16318, Lr: 0.000300\n",
      "2021-08-02 09:54:27,449 - INFO - joeynmt.training - Epoch   6, Step:    31000, Batch Loss:     2.118667, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-02 09:54:54,372 - INFO - joeynmt.training - Epoch   6, Step:    31200, Batch Loss:     2.046248, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-08-02 09:55:21,320 - INFO - joeynmt.training - Epoch   6, Step:    31400, Batch Loss:     2.413022, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-02 09:55:48,425 - INFO - joeynmt.training - Epoch   6, Step:    31600, Batch Loss:     2.300636, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-02 09:56:15,461 - INFO - joeynmt.training - Epoch   6, Step:    31800, Batch Loss:     2.312579, Tokens per Sec:    16236, Lr: 0.000300\n",
      "2021-08-02 09:56:42,560 - INFO - joeynmt.training - Epoch   6, Step:    32000, Batch Loss:     2.241534, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-08-02 09:57:09,711 - INFO - joeynmt.training - Epoch   6, Step:    32200, Batch Loss:     2.033545, Tokens per Sec:    16327, Lr: 0.000300\n",
      "2021-08-02 09:57:36,683 - INFO - joeynmt.training - Epoch   6, Step:    32400, Batch Loss:     2.255952, Tokens per Sec:    16326, Lr: 0.000300\n",
      "2021-08-02 09:58:03,656 - INFO - joeynmt.training - Epoch   6, Step:    32600, Batch Loss:     2.307910, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-02 09:58:28,208 - INFO - joeynmt.training - Epoch   6: total training loss 12251.88\n",
      "2021-08-02 09:58:28,209 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-02 09:58:31,628 - INFO - joeynmt.training - Epoch   7, Step:    32800, Batch Loss:     2.164999, Tokens per Sec:    13300, Lr: 0.000300\n",
      "2021-08-02 09:58:58,406 - INFO - joeynmt.training - Epoch   7, Step:    33000, Batch Loss:     1.956361, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-02 09:59:25,798 - INFO - joeynmt.training - Epoch   7, Step:    33200, Batch Loss:     2.071223, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-02 09:59:52,898 - INFO - joeynmt.training - Epoch   7, Step:    33400, Batch Loss:     2.387670, Tokens per Sec:    16347, Lr: 0.000300\n",
      "2021-08-02 10:00:19,898 - INFO - joeynmt.training - Epoch   7, Step:    33600, Batch Loss:     2.115567, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-02 10:00:47,138 - INFO - joeynmt.training - Epoch   7, Step:    33800, Batch Loss:     2.166372, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-02 10:01:14,008 - INFO - joeynmt.training - Epoch   7, Step:    34000, Batch Loss:     2.088117, Tokens per Sec:    16220, Lr: 0.000300\n",
      "2021-08-02 10:01:41,182 - INFO - joeynmt.training - Epoch   7, Step:    34200, Batch Loss:     2.034904, Tokens per Sec:    16312, Lr: 0.000300\n",
      "2021-08-02 10:02:08,193 - INFO - joeynmt.training - Epoch   7, Step:    34400, Batch Loss:     2.138068, Tokens per Sec:    16100, Lr: 0.000300\n",
      "2021-08-02 10:02:35,374 - INFO - joeynmt.training - Epoch   7, Step:    34600, Batch Loss:     2.221695, Tokens per Sec:    16327, Lr: 0.000300\n",
      "2021-08-02 10:03:02,649 - INFO - joeynmt.training - Epoch   7, Step:    34800, Batch Loss:     2.194239, Tokens per Sec:    16322, Lr: 0.000300\n",
      "2021-08-02 10:03:29,689 - INFO - joeynmt.training - Epoch   7, Step:    35000, Batch Loss:     1.951402, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-02 10:05:12,251 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:05:12,252 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:05:12,252 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:05:13,568 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:05:13,568 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:05:14,297 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:05:14,298 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:05:14,298 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:05:14,298 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I was approaching God , that is good to me . ”\n",
      "2021-08-02 10:05:14,298 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:05:14,299 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:05:14,299 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:05:14,299 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Let us be a fear of Jehovah , we are more than many wealth . ”\n",
      "2021-08-02 10:05:14,299 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:05:14,300 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:05:14,300 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:05:14,300 - INFO - joeynmt.training - \tHypothesis: Canaius : Day another prophecy in the book of Daniel repeatedly reports about God’s Kingdom .\n",
      "2021-08-02 10:05:14,300 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:05:14,301 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:05:14,301 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:05:14,301 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep on the sorrow ” and bring it to it ?\n",
      "2021-08-02 10:05:14,301 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    35000: bleu:  20.82, loss: 246034.1094, ppl:   7.2626, duration: 104.6120s\n",
      "2021-08-02 10:05:41,771 - INFO - joeynmt.training - Epoch   7, Step:    35200, Batch Loss:     1.961871, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-02 10:06:08,490 - INFO - joeynmt.training - Epoch   7, Step:    35400, Batch Loss:     2.155481, Tokens per Sec:    16122, Lr: 0.000300\n",
      "2021-08-02 10:06:35,534 - INFO - joeynmt.training - Epoch   7, Step:    35600, Batch Loss:     2.049519, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-08-02 10:07:02,802 - INFO - joeynmt.training - Epoch   7, Step:    35800, Batch Loss:     1.794608, Tokens per Sec:    16598, Lr: 0.000300\n",
      "2021-08-02 10:07:29,650 - INFO - joeynmt.training - Epoch   7, Step:    36000, Batch Loss:     2.370502, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-08-02 10:07:56,862 - INFO - joeynmt.training - Epoch   7, Step:    36200, Batch Loss:     2.059207, Tokens per Sec:    16213, Lr: 0.000300\n",
      "2021-08-02 10:08:23,784 - INFO - joeynmt.training - Epoch   7, Step:    36400, Batch Loss:     2.144117, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-08-02 10:08:50,959 - INFO - joeynmt.training - Epoch   7, Step:    36600, Batch Loss:     2.094641, Tokens per Sec:    16312, Lr: 0.000300\n",
      "2021-08-02 10:09:18,308 - INFO - joeynmt.training - Epoch   7, Step:    36800, Batch Loss:     1.982291, Tokens per Sec:    16426, Lr: 0.000300\n",
      "2021-08-02 10:09:45,442 - INFO - joeynmt.training - Epoch   7, Step:    37000, Batch Loss:     2.338083, Tokens per Sec:    16235, Lr: 0.000300\n",
      "2021-08-02 10:10:12,305 - INFO - joeynmt.training - Epoch   7, Step:    37200, Batch Loss:     2.207434, Tokens per Sec:    16185, Lr: 0.000300\n",
      "2021-08-02 10:10:39,281 - INFO - joeynmt.training - Epoch   7, Step:    37400, Batch Loss:     2.273291, Tokens per Sec:    16072, Lr: 0.000300\n",
      "2021-08-02 10:11:06,238 - INFO - joeynmt.training - Epoch   7, Step:    37600, Batch Loss:     2.303406, Tokens per Sec:    16280, Lr: 0.000300\n",
      "2021-08-02 10:11:33,337 - INFO - joeynmt.training - Epoch   7, Step:    37800, Batch Loss:     2.341190, Tokens per Sec:    16196, Lr: 0.000300\n",
      "2021-08-02 10:12:00,159 - INFO - joeynmt.training - Epoch   7, Step:    38000, Batch Loss:     2.000916, Tokens per Sec:    16133, Lr: 0.000300\n",
      "2021-08-02 10:12:27,116 - INFO - joeynmt.training - Epoch   7, Step:    38200, Batch Loss:     2.109028, Tokens per Sec:    16310, Lr: 0.000300\n",
      "2021-08-02 10:12:31,728 - INFO - joeynmt.training - Epoch   7: total training loss 11876.70\n",
      "2021-08-02 10:12:31,729 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-02 10:12:54,940 - INFO - joeynmt.training - Epoch   8, Step:    38400, Batch Loss:     2.197627, Tokens per Sec:    15673, Lr: 0.000300\n",
      "2021-08-02 10:13:21,769 - INFO - joeynmt.training - Epoch   8, Step:    38600, Batch Loss:     2.041948, Tokens per Sec:    16217, Lr: 0.000300\n",
      "2021-08-02 10:13:48,942 - INFO - joeynmt.training - Epoch   8, Step:    38800, Batch Loss:     2.076429, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-02 10:14:15,972 - INFO - joeynmt.training - Epoch   8, Step:    39000, Batch Loss:     2.294882, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-02 10:14:42,886 - INFO - joeynmt.training - Epoch   8, Step:    39200, Batch Loss:     2.161117, Tokens per Sec:    16304, Lr: 0.000300\n",
      "2021-08-02 10:15:09,896 - INFO - joeynmt.training - Epoch   8, Step:    39400, Batch Loss:     2.316792, Tokens per Sec:    16335, Lr: 0.000300\n",
      "2021-08-02 10:15:37,323 - INFO - joeynmt.training - Epoch   8, Step:    39600, Batch Loss:     2.020558, Tokens per Sec:    16242, Lr: 0.000300\n",
      "2021-08-02 10:16:04,135 - INFO - joeynmt.training - Epoch   8, Step:    39800, Batch Loss:     2.231961, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-02 10:16:31,294 - INFO - joeynmt.training - Epoch   8, Step:    40000, Batch Loss:     2.026576, Tokens per Sec:    16310, Lr: 0.000300\n",
      "2021-08-02 10:18:06,974 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:18:06,974 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:18:06,975 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:18:08,183 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:18:08,183 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:18:08,873 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:18:08,874 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:18:08,874 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:18:08,874 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I was approached to God , that is good for me . ”\n",
      "2021-08-02 10:18:08,874 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:18:08,875 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:18:08,875 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:18:08,876 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Let us be in fear of Jehovah , more than many wealth are representing . ”\n",
      "2021-08-02 10:18:08,876 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:18:08,876 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:18:08,876 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:18:08,876 - INFO - joeynmt.training - \tHypothesis: Cameron : Dere another prophecy in the book of Daniel has a report on God’s Kingdom .\n",
      "2021-08-02 10:18:08,877 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:18:08,877 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:18:08,877 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:18:08,877 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to overcome ” and bring it completely ?\n",
      "2021-08-02 10:18:08,877 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    40000: bleu:  21.37, loss: 240938.9375, ppl:   6.9704, duration: 97.5830s\n",
      "2021-08-02 10:18:36,169 - INFO - joeynmt.training - Epoch   8, Step:    40200, Batch Loss:     2.290345, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-02 10:19:03,460 - INFO - joeynmt.training - Epoch   8, Step:    40400, Batch Loss:     2.109724, Tokens per Sec:    16308, Lr: 0.000300\n",
      "2021-08-02 10:19:30,732 - INFO - joeynmt.training - Epoch   8, Step:    40600, Batch Loss:     2.189182, Tokens per Sec:    16385, Lr: 0.000300\n",
      "2021-08-02 10:19:57,553 - INFO - joeynmt.training - Epoch   8, Step:    40800, Batch Loss:     1.976786, Tokens per Sec:    16194, Lr: 0.000300\n",
      "2021-08-02 10:20:24,463 - INFO - joeynmt.training - Epoch   8, Step:    41000, Batch Loss:     2.114452, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-08-02 10:20:51,346 - INFO - joeynmt.training - Epoch   8, Step:    41200, Batch Loss:     1.872610, Tokens per Sec:    16223, Lr: 0.000300\n",
      "2021-08-02 10:21:18,300 - INFO - joeynmt.training - Epoch   8, Step:    41400, Batch Loss:     2.129246, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-02 10:21:45,291 - INFO - joeynmt.training - Epoch   8, Step:    41600, Batch Loss:     1.913637, Tokens per Sec:    16314, Lr: 0.000300\n",
      "2021-08-02 10:22:12,299 - INFO - joeynmt.training - Epoch   8, Step:    41800, Batch Loss:     2.284293, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-08-02 10:22:39,206 - INFO - joeynmt.training - Epoch   8, Step:    42000, Batch Loss:     2.017128, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-02 10:23:06,109 - INFO - joeynmt.training - Epoch   8, Step:    42200, Batch Loss:     2.078944, Tokens per Sec:    16209, Lr: 0.000300\n",
      "2021-08-02 10:23:33,140 - INFO - joeynmt.training - Epoch   8, Step:    42400, Batch Loss:     2.122352, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-08-02 10:24:00,083 - INFO - joeynmt.training - Epoch   8, Step:    42600, Batch Loss:     2.120414, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-08-02 10:24:26,844 - INFO - joeynmt.training - Epoch   8, Step:    42800, Batch Loss:     2.473567, Tokens per Sec:    16130, Lr: 0.000300\n",
      "2021-08-02 10:24:54,145 - INFO - joeynmt.training - Epoch   8, Step:    43000, Batch Loss:     2.393889, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-08-02 10:25:21,367 - INFO - joeynmt.training - Epoch   8, Step:    43200, Batch Loss:     2.139556, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-08-02 10:25:48,241 - INFO - joeynmt.training - Epoch   8, Step:    43400, Batch Loss:     2.142720, Tokens per Sec:    16206, Lr: 0.000300\n",
      "2021-08-02 10:26:15,339 - INFO - joeynmt.training - Epoch   8, Step:    43600, Batch Loss:     2.064404, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-08-02 10:26:28,244 - INFO - joeynmt.training - Epoch   8: total training loss 11616.06\n",
      "2021-08-02 10:26:28,244 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-02 10:26:42,996 - INFO - joeynmt.training - Epoch   9, Step:    43800, Batch Loss:     2.194453, Tokens per Sec:    15623, Lr: 0.000300\n",
      "2021-08-02 10:27:09,638 - INFO - joeynmt.training - Epoch   9, Step:    44000, Batch Loss:     2.177778, Tokens per Sec:    16291, Lr: 0.000300\n",
      "2021-08-02 10:27:36,709 - INFO - joeynmt.training - Epoch   9, Step:    44200, Batch Loss:     2.081509, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-02 10:28:03,695 - INFO - joeynmt.training - Epoch   9, Step:    44400, Batch Loss:     2.252909, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-02 10:28:30,867 - INFO - joeynmt.training - Epoch   9, Step:    44600, Batch Loss:     2.216798, Tokens per Sec:    16216, Lr: 0.000300\n",
      "2021-08-02 10:28:57,696 - INFO - joeynmt.training - Epoch   9, Step:    44800, Batch Loss:     1.801782, Tokens per Sec:    16294, Lr: 0.000300\n",
      "2021-08-02 10:29:24,256 - INFO - joeynmt.training - Epoch   9, Step:    45000, Batch Loss:     2.160858, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-08-02 10:30:57,267 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:30:57,267 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:30:57,267 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:30:58,473 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:30:58,473 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:30:59,176 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:30:59,176 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:30:59,177 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:30:59,177 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been drawn close to God , that is good for me . ”\n",
      "2021-08-02 10:30:59,177 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:30:59,177 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:30:59,177 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:30:59,178 - INFO - joeynmt.training - \tHypothesis: “ Let us be in fear of Jehovah , ” says King Solomon of ancient Israel , “ we are more than many wealth that are standing . ”\n",
      "2021-08-02 10:30:59,178 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:30:59,178 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:30:59,178 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:30:59,178 - INFO - joeynmt.training - \tHypothesis: Cameron : Dere another prophecy in the book of Daniel reveals what it says about God’s Kingdom .\n",
      "2021-08-02 10:30:59,179 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:30:59,179 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:30:59,179 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:30:59,179 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep on bringing up ” and bring it to it ?\n",
      "2021-08-02 10:30:59,180 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    45000: bleu:  21.97, loss: 236354.6094, ppl:   6.7176, duration: 94.9234s\n",
      "2021-08-02 10:31:26,707 - INFO - joeynmt.training - Epoch   9, Step:    45200, Batch Loss:     2.141861, Tokens per Sec:    16008, Lr: 0.000300\n",
      "2021-08-02 10:31:53,731 - INFO - joeynmt.training - Epoch   9, Step:    45400, Batch Loss:     2.204318, Tokens per Sec:    16411, Lr: 0.000300\n",
      "2021-08-02 10:32:20,242 - INFO - joeynmt.training - Epoch   9, Step:    45600, Batch Loss:     1.949734, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-08-02 10:32:47,111 - INFO - joeynmt.training - Epoch   9, Step:    45800, Batch Loss:     2.392537, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-02 10:33:14,021 - INFO - joeynmt.training - Epoch   9, Step:    46000, Batch Loss:     2.732476, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-08-02 10:33:40,988 - INFO - joeynmt.training - Epoch   9, Step:    46200, Batch Loss:     2.024325, Tokens per Sec:    16114, Lr: 0.000300\n",
      "2021-08-02 10:34:08,047 - INFO - joeynmt.training - Epoch   9, Step:    46400, Batch Loss:     1.621876, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-08-02 10:34:35,226 - INFO - joeynmt.training - Epoch   9, Step:    46600, Batch Loss:     2.117541, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-08-02 10:35:02,243 - INFO - joeynmt.training - Epoch   9, Step:    46800, Batch Loss:     2.256955, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-08-02 10:35:28,869 - INFO - joeynmt.training - Epoch   9, Step:    47000, Batch Loss:     2.211118, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-08-02 10:35:55,804 - INFO - joeynmt.training - Epoch   9, Step:    47200, Batch Loss:     2.411288, Tokens per Sec:    16076, Lr: 0.000300\n",
      "2021-08-02 10:36:22,995 - INFO - joeynmt.training - Epoch   9, Step:    47400, Batch Loss:     1.799344, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-08-02 10:36:49,723 - INFO - joeynmt.training - Epoch   9, Step:    47600, Batch Loss:     2.083720, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-02 10:37:16,708 - INFO - joeynmt.training - Epoch   9, Step:    47800, Batch Loss:     2.355285, Tokens per Sec:    16239, Lr: 0.000300\n",
      "2021-08-02 10:37:43,639 - INFO - joeynmt.training - Epoch   9, Step:    48000, Batch Loss:     2.059430, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-08-02 10:38:10,652 - INFO - joeynmt.training - Epoch   9, Step:    48200, Batch Loss:     2.061205, Tokens per Sec:    16367, Lr: 0.000300\n",
      "2021-08-02 10:38:37,935 - INFO - joeynmt.training - Epoch   9, Step:    48400, Batch Loss:     2.100687, Tokens per Sec:    16377, Lr: 0.000300\n",
      "2021-08-02 10:39:04,845 - INFO - joeynmt.training - Epoch   9, Step:    48600, Batch Loss:     2.626220, Tokens per Sec:    16208, Lr: 0.000300\n",
      "2021-08-02 10:39:31,809 - INFO - joeynmt.training - Epoch   9, Step:    48800, Batch Loss:     2.050084, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-02 10:39:58,796 - INFO - joeynmt.training - Epoch   9, Step:    49000, Batch Loss:     2.511717, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-02 10:40:21,997 - INFO - joeynmt.training - Epoch   9: total training loss 11428.66\n",
      "2021-08-02 10:40:21,998 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-02 10:40:26,412 - INFO - joeynmt.training - Epoch  10, Step:    49200, Batch Loss:     1.883676, Tokens per Sec:    14352, Lr: 0.000300\n",
      "2021-08-02 10:40:53,679 - INFO - joeynmt.training - Epoch  10, Step:    49400, Batch Loss:     2.060242, Tokens per Sec:    16169, Lr: 0.000300\n",
      "2021-08-02 10:41:20,706 - INFO - joeynmt.training - Epoch  10, Step:    49600, Batch Loss:     2.113397, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-02 10:41:47,685 - INFO - joeynmt.training - Epoch  10, Step:    49800, Batch Loss:     2.158048, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-08-02 10:42:14,842 - INFO - joeynmt.training - Epoch  10, Step:    50000, Batch Loss:     1.757862, Tokens per Sec:    16474, Lr: 0.000300\n",
      "2021-08-02 10:43:55,343 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:43:55,344 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:43:55,344 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:43:56,569 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:43:56,569 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:43:57,273 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:43:57,274 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:43:57,274 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:43:57,275 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approached to God , that is good for me . ”\n",
      "2021-08-02 10:43:57,275 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:43:57,275 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:43:57,275 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:43:57,275 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are the fear of Jehovah , and we are more than many wealth that are standing . ”\n",
      "2021-08-02 10:43:57,276 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:43:57,276 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:43:57,276 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:43:57,276 - INFO - joeynmt.training - \tHypothesis: Cameron : Come another prophecy of Daniel describes what it says about God’s Kingdom .\n",
      "2021-08-02 10:43:57,276 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:43:57,277 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:43:57,277 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:43:57,277 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to overcome ” and bring it completely ?\n",
      "2021-08-02 10:43:57,277 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    50000: bleu:  22.38, loss: 232201.5156, ppl:   6.4965, duration: 102.4347s\n",
      "2021-08-02 10:44:24,546 - INFO - joeynmt.training - Epoch  10, Step:    50200, Batch Loss:     2.006672, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-02 10:44:51,541 - INFO - joeynmt.training - Epoch  10, Step:    50400, Batch Loss:     1.986238, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-08-02 10:45:18,490 - INFO - joeynmt.training - Epoch  10, Step:    50600, Batch Loss:     1.719371, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-08-02 10:45:45,785 - INFO - joeynmt.training - Epoch  10, Step:    50800, Batch Loss:     1.936596, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-02 10:46:12,786 - INFO - joeynmt.training - Epoch  10, Step:    51000, Batch Loss:     2.411178, Tokens per Sec:    16567, Lr: 0.000300\n",
      "2021-08-02 10:46:39,846 - INFO - joeynmt.training - Epoch  10, Step:    51200, Batch Loss:     1.977372, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-02 10:47:06,807 - INFO - joeynmt.training - Epoch  10, Step:    51400, Batch Loss:     2.239583, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-02 10:47:33,910 - INFO - joeynmt.training - Epoch  10, Step:    51600, Batch Loss:     2.047658, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-02 10:48:00,696 - INFO - joeynmt.training - Epoch  10, Step:    51800, Batch Loss:     2.008191, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-08-02 10:48:27,838 - INFO - joeynmt.training - Epoch  10, Step:    52000, Batch Loss:     1.945633, Tokens per Sec:    16255, Lr: 0.000300\n",
      "2021-08-02 10:48:54,670 - INFO - joeynmt.training - Epoch  10, Step:    52200, Batch Loss:     1.901711, Tokens per Sec:    16310, Lr: 0.000300\n",
      "2021-08-02 10:49:21,780 - INFO - joeynmt.training - Epoch  10, Step:    52400, Batch Loss:     1.911545, Tokens per Sec:    16328, Lr: 0.000300\n",
      "2021-08-02 10:49:48,694 - INFO - joeynmt.training - Epoch  10, Step:    52600, Batch Loss:     2.452286, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-02 10:50:15,426 - INFO - joeynmt.training - Epoch  10, Step:    52800, Batch Loss:     2.024733, Tokens per Sec:    16251, Lr: 0.000300\n",
      "2021-08-02 10:50:42,605 - INFO - joeynmt.training - Epoch  10, Step:    53000, Batch Loss:     1.703495, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-02 10:51:09,646 - INFO - joeynmt.training - Epoch  10, Step:    53200, Batch Loss:     1.859320, Tokens per Sec:    16413, Lr: 0.000300\n",
      "2021-08-02 10:51:36,469 - INFO - joeynmt.training - Epoch  10, Step:    53400, Batch Loss:     1.927490, Tokens per Sec:    16132, Lr: 0.000300\n",
      "2021-08-02 10:52:03,540 - INFO - joeynmt.training - Epoch  10, Step:    53600, Batch Loss:     1.702837, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-02 10:52:30,524 - INFO - joeynmt.training - Epoch  10, Step:    53800, Batch Loss:     2.641515, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-08-02 10:52:57,469 - INFO - joeynmt.training - Epoch  10, Step:    54000, Batch Loss:     1.998892, Tokens per Sec:    16264, Lr: 0.000300\n",
      "2021-08-02 10:53:24,702 - INFO - joeynmt.training - Epoch  10, Step:    54200, Batch Loss:     2.177111, Tokens per Sec:    16276, Lr: 0.000300\n",
      "2021-08-02 10:53:51,641 - INFO - joeynmt.training - Epoch  10, Step:    54400, Batch Loss:     2.050094, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-08-02 10:54:18,567 - INFO - joeynmt.training - Epoch  10, Step:    54600, Batch Loss:     1.841663, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-02 10:54:22,584 - INFO - joeynmt.training - Epoch  10: total training loss 11206.07\n",
      "2021-08-02 10:54:22,584 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-02 10:54:46,218 - INFO - joeynmt.training - Epoch  11, Step:    54800, Batch Loss:     1.919253, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-08-02 10:55:13,263 - INFO - joeynmt.training - Epoch  11, Step:    55000, Batch Loss:     2.117609, Tokens per Sec:    16352, Lr: 0.000300\n",
      "2021-08-02 10:56:44,332 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:56:44,333 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:56:44,333 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:56:45,576 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:56:45,576 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:56:46,316 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:56:46,319 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 10:56:46,319 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 10:56:46,319 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , that is good for me . ”\n",
      "2021-08-02 10:56:46,319 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:56:46,320 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 10:56:46,321 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 10:56:46,321 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are giving Jehovah more than many wealth . ”\n",
      "2021-08-02 10:56:46,321 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:56:46,323 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 10:56:46,323 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 10:56:46,323 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel is a detailed point on God’s Kingdom .\n",
      "2021-08-02 10:56:46,323 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:56:46,324 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 10:56:46,324 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 10:56:46,324 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue to overcome ” and bring it to it completely ?\n",
      "2021-08-02 10:56:46,325 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    55000: bleu:  22.85, loss: 228874.0938, ppl:   6.3246, duration: 93.0612s\n",
      "2021-08-02 10:57:13,556 - INFO - joeynmt.training - Epoch  11, Step:    55200, Batch Loss:     2.166622, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-08-02 10:57:40,728 - INFO - joeynmt.training - Epoch  11, Step:    55400, Batch Loss:     2.298188, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-08-02 10:58:07,961 - INFO - joeynmt.training - Epoch  11, Step:    55600, Batch Loss:     2.167219, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-08-02 10:58:35,152 - INFO - joeynmt.training - Epoch  11, Step:    55800, Batch Loss:     1.965459, Tokens per Sec:    16332, Lr: 0.000300\n",
      "2021-08-02 10:59:01,853 - INFO - joeynmt.training - Epoch  11, Step:    56000, Batch Loss:     1.912748, Tokens per Sec:    16108, Lr: 0.000300\n",
      "2021-08-02 10:59:28,846 - INFO - joeynmt.training - Epoch  11, Step:    56200, Batch Loss:     2.046488, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-02 10:59:55,895 - INFO - joeynmt.training - Epoch  11, Step:    56400, Batch Loss:     1.920786, Tokens per Sec:    16280, Lr: 0.000300\n",
      "2021-08-02 11:00:22,864 - INFO - joeynmt.training - Epoch  11, Step:    56600, Batch Loss:     2.035517, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-08-02 11:00:50,123 - INFO - joeynmt.training - Epoch  11, Step:    56800, Batch Loss:     2.008057, Tokens per Sec:    16301, Lr: 0.000300\n",
      "2021-08-02 11:01:17,010 - INFO - joeynmt.training - Epoch  11, Step:    57000, Batch Loss:     2.012809, Tokens per Sec:    16331, Lr: 0.000300\n",
      "2021-08-02 11:01:44,130 - INFO - joeynmt.training - Epoch  11, Step:    57200, Batch Loss:     2.180554, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-02 11:02:10,912 - INFO - joeynmt.training - Epoch  11, Step:    57400, Batch Loss:     2.074432, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-08-02 11:02:38,216 - INFO - joeynmt.training - Epoch  11, Step:    57600, Batch Loss:     2.015018, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-08-02 11:03:05,579 - INFO - joeynmt.training - Epoch  11, Step:    57800, Batch Loss:     2.050149, Tokens per Sec:    16169, Lr: 0.000300\n",
      "2021-08-02 11:03:32,676 - INFO - joeynmt.training - Epoch  11, Step:    58000, Batch Loss:     2.026305, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-02 11:03:59,480 - INFO - joeynmt.training - Epoch  11, Step:    58200, Batch Loss:     2.014634, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-08-02 11:04:26,982 - INFO - joeynmt.training - Epoch  11, Step:    58400, Batch Loss:     1.782944, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-08-02 11:04:53,661 - INFO - joeynmt.training - Epoch  11, Step:    58600, Batch Loss:     2.192568, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-02 11:05:20,787 - INFO - joeynmt.training - Epoch  11, Step:    58800, Batch Loss:     2.047035, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-02 11:05:48,043 - INFO - joeynmt.training - Epoch  11, Step:    59000, Batch Loss:     2.059106, Tokens per Sec:    16218, Lr: 0.000300\n",
      "2021-08-02 11:06:15,003 - INFO - joeynmt.training - Epoch  11, Step:    59200, Batch Loss:     2.178036, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-02 11:06:42,417 - INFO - joeynmt.training - Epoch  11, Step:    59400, Batch Loss:     1.815235, Tokens per Sec:    16270, Lr: 0.000300\n",
      "2021-08-02 11:07:09,322 - INFO - joeynmt.training - Epoch  11, Step:    59600, Batch Loss:     2.042093, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-02 11:07:36,510 - INFO - joeynmt.training - Epoch  11, Step:    59800, Batch Loss:     1.934521, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-02 11:08:03,722 - INFO - joeynmt.training - Epoch  11, Step:    60000, Batch Loss:     1.923058, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-08-02 11:09:35,967 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:09:35,968 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:09:35,968 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:09:37,200 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:09:37,200 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:09:37,885 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:09:37,885 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:09:37,886 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:09:37,886 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I was approaching God , that is good for me . ”\n",
      "2021-08-02 11:09:37,886 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:09:37,886 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:09:37,887 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:09:37,887 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are giving fear of Jehovah , more than many wealth are standing . ”\n",
      "2021-08-02 11:09:37,887 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:09:37,887 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:09:37,888 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:09:37,888 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel foretells what it says about God’s Kingdom .\n",
      "2021-08-02 11:09:37,888 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:09:37,888 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:09:37,888 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:09:37,889 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on the conquest ” and bring it completely ?\n",
      "2021-08-02 11:09:37,889 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    60000: bleu:  23.05, loss: 226095.0156, ppl:   6.1845, duration: 94.1661s\n",
      "2021-08-02 11:09:50,622 - INFO - joeynmt.training - Epoch  11: total training loss 11048.10\n",
      "2021-08-02 11:09:50,623 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-02 11:10:05,626 - INFO - joeynmt.training - Epoch  12, Step:    60200, Batch Loss:     1.991652, Tokens per Sec:    15422, Lr: 0.000300\n",
      "2021-08-02 11:10:32,981 - INFO - joeynmt.training - Epoch  12, Step:    60400, Batch Loss:     2.066565, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-02 11:10:59,949 - INFO - joeynmt.training - Epoch  12, Step:    60600, Batch Loss:     1.864381, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-02 11:11:27,072 - INFO - joeynmt.training - Epoch  12, Step:    60800, Batch Loss:     1.847407, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-02 11:11:54,002 - INFO - joeynmt.training - Epoch  12, Step:    61000, Batch Loss:     1.944237, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-08-02 11:12:21,088 - INFO - joeynmt.training - Epoch  12, Step:    61200, Batch Loss:     2.026890, Tokens per Sec:    16339, Lr: 0.000300\n",
      "2021-08-02 11:12:48,393 - INFO - joeynmt.training - Epoch  12, Step:    61400, Batch Loss:     1.823745, Tokens per Sec:    16235, Lr: 0.000300\n",
      "2021-08-02 11:13:15,183 - INFO - joeynmt.training - Epoch  12, Step:    61600, Batch Loss:     1.979260, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-08-02 11:13:42,245 - INFO - joeynmt.training - Epoch  12, Step:    61800, Batch Loss:     2.525183, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-08-02 11:14:09,563 - INFO - joeynmt.training - Epoch  12, Step:    62000, Batch Loss:     1.926719, Tokens per Sec:    16248, Lr: 0.000300\n",
      "2021-08-02 11:14:36,512 - INFO - joeynmt.training - Epoch  12, Step:    62200, Batch Loss:     2.011139, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-02 11:15:03,281 - INFO - joeynmt.training - Epoch  12, Step:    62400, Batch Loss:     2.023749, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-08-02 11:15:30,544 - INFO - joeynmt.training - Epoch  12, Step:    62600, Batch Loss:     1.999865, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-08-02 11:15:57,539 - INFO - joeynmt.training - Epoch  12, Step:    62800, Batch Loss:     1.993924, Tokens per Sec:    16423, Lr: 0.000300\n",
      "2021-08-02 11:16:24,590 - INFO - joeynmt.training - Epoch  12, Step:    63000, Batch Loss:     1.855937, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-08-02 11:16:51,254 - INFO - joeynmt.training - Epoch  12, Step:    63200, Batch Loss:     2.012523, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-02 11:17:18,076 - INFO - joeynmt.training - Epoch  12, Step:    63400, Batch Loss:     2.035351, Tokens per Sec:    16264, Lr: 0.000300\n",
      "2021-08-02 11:17:45,056 - INFO - joeynmt.training - Epoch  12, Step:    63600, Batch Loss:     2.059460, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-08-02 11:18:12,106 - INFO - joeynmt.training - Epoch  12, Step:    63800, Batch Loss:     2.011464, Tokens per Sec:    16322, Lr: 0.000300\n",
      "2021-08-02 11:18:39,366 - INFO - joeynmt.training - Epoch  12, Step:    64000, Batch Loss:     2.002874, Tokens per Sec:    16281, Lr: 0.000300\n",
      "2021-08-02 11:19:06,411 - INFO - joeynmt.training - Epoch  12, Step:    64200, Batch Loss:     1.937905, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-08-02 11:19:33,294 - INFO - joeynmt.training - Epoch  12, Step:    64400, Batch Loss:     2.065974, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-02 11:20:00,068 - INFO - joeynmt.training - Epoch  12, Step:    64600, Batch Loss:     2.054499, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-08-02 11:20:27,197 - INFO - joeynmt.training - Epoch  12, Step:    64800, Batch Loss:     2.120717, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-02 11:20:54,037 - INFO - joeynmt.training - Epoch  12, Step:    65000, Batch Loss:     2.173054, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-08-02 11:22:29,927 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:22:29,927 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:22:29,927 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:22:31,232 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:22:31,232 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:22:31,970 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:22:31,970 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:22:31,970 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:22:31,970 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God is good for me . ”\n",
      "2021-08-02 11:22:31,971 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:22:31,971 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:22:31,971 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:22:31,971 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , and we are more than many wealth that are standing . ”\n",
      "2021-08-02 11:22:31,972 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:22:31,972 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:22:31,972 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:22:31,972 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The prophecy of Daniel has a detailment on God’s Kingdom .\n",
      "2021-08-02 11:22:31,973 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:22:31,973 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:22:31,973 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:22:31,974 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ continue to conquer ” and to completely successfully ?\n",
      "2021-08-02 11:22:31,974 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    65000: bleu:  23.40, loss: 223887.2031, ppl:   6.0755, duration: 97.9369s\n",
      "2021-08-02 11:22:59,201 - INFO - joeynmt.training - Epoch  12, Step:    65200, Batch Loss:     2.415000, Tokens per Sec:    16255, Lr: 0.000300\n",
      "2021-08-02 11:23:26,082 - INFO - joeynmt.training - Epoch  12, Step:    65400, Batch Loss:     2.087849, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-02 11:23:48,125 - INFO - joeynmt.training - Epoch  12: total training loss 10934.91\n",
      "2021-08-02 11:23:48,126 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-02 11:23:53,679 - INFO - joeynmt.training - Epoch  13, Step:    65600, Batch Loss:     1.992577, Tokens per Sec:    14833, Lr: 0.000300\n",
      "2021-08-02 11:24:20,855 - INFO - joeynmt.training - Epoch  13, Step:    65800, Batch Loss:     1.893302, Tokens per Sec:    16621, Lr: 0.000300\n",
      "2021-08-02 11:24:47,755 - INFO - joeynmt.training - Epoch  13, Step:    66000, Batch Loss:     1.899480, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-02 11:25:14,988 - INFO - joeynmt.training - Epoch  13, Step:    66200, Batch Loss:     2.037442, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-08-02 11:25:41,773 - INFO - joeynmt.training - Epoch  13, Step:    66400, Batch Loss:     1.861177, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-02 11:26:08,764 - INFO - joeynmt.training - Epoch  13, Step:    66600, Batch Loss:     1.916429, Tokens per Sec:    16189, Lr: 0.000300\n",
      "2021-08-02 11:26:35,909 - INFO - joeynmt.training - Epoch  13, Step:    66800, Batch Loss:     2.126904, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-02 11:27:02,998 - INFO - joeynmt.training - Epoch  13, Step:    67000, Batch Loss:     2.087680, Tokens per Sec:    16395, Lr: 0.000300\n",
      "2021-08-02 11:27:30,006 - INFO - joeynmt.training - Epoch  13, Step:    67200, Batch Loss:     1.934294, Tokens per Sec:    16188, Lr: 0.000300\n",
      "2021-08-02 11:27:56,965 - INFO - joeynmt.training - Epoch  13, Step:    67400, Batch Loss:     2.081162, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-02 11:28:24,015 - INFO - joeynmt.training - Epoch  13, Step:    67600, Batch Loss:     1.978018, Tokens per Sec:    16294, Lr: 0.000300\n",
      "2021-08-02 11:28:51,352 - INFO - joeynmt.training - Epoch  13, Step:    67800, Batch Loss:     1.765488, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-02 11:29:18,366 - INFO - joeynmt.training - Epoch  13, Step:    68000, Batch Loss:     1.929320, Tokens per Sec:    16277, Lr: 0.000300\n",
      "2021-08-02 11:29:45,414 - INFO - joeynmt.training - Epoch  13, Step:    68200, Batch Loss:     1.789310, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-08-02 11:30:12,652 - INFO - joeynmt.training - Epoch  13, Step:    68400, Batch Loss:     2.196811, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-02 11:30:39,593 - INFO - joeynmt.training - Epoch  13, Step:    68600, Batch Loss:     2.228474, Tokens per Sec:    16378, Lr: 0.000300\n",
      "2021-08-02 11:31:06,583 - INFO - joeynmt.training - Epoch  13, Step:    68800, Batch Loss:     2.050131, Tokens per Sec:    16298, Lr: 0.000300\n",
      "2021-08-02 11:31:33,883 - INFO - joeynmt.training - Epoch  13, Step:    69000, Batch Loss:     1.918938, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-02 11:32:00,797 - INFO - joeynmt.training - Epoch  13, Step:    69200, Batch Loss:     2.055512, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-08-02 11:32:27,766 - INFO - joeynmt.training - Epoch  13, Step:    69400, Batch Loss:     1.852760, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-08-02 11:32:54,910 - INFO - joeynmt.training - Epoch  13, Step:    69600, Batch Loss:     1.862453, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-02 11:33:21,885 - INFO - joeynmt.training - Epoch  13, Step:    69800, Batch Loss:     2.077907, Tokens per Sec:    16277, Lr: 0.000300\n",
      "2021-08-02 11:33:48,824 - INFO - joeynmt.training - Epoch  13, Step:    70000, Batch Loss:     1.915372, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-08-02 11:35:20,234 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:35:20,234 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:35:20,235 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:35:21,437 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:35:21,437 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:35:22,132 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:35:22,133 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:35:22,133 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:35:22,133 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been drawing close to God , that is good for me . ”\n",
      "2021-08-02 11:35:22,133 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:35:22,134 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:35:22,134 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:35:22,134 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , more than many wealth are standing . ”\n",
      "2021-08-02 11:35:22,134 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:35:22,135 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:35:22,135 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:35:22,135 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy of Daniel about God’s Kingdom .\n",
      "2021-08-02 11:35:22,135 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:35:22,135 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:35:22,136 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:35:22,136 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue to resist ” and to complete it ?\n",
      "2021-08-02 11:35:22,136 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    70000: bleu:  23.51, loss: 222232.1250, ppl:   5.9950, duration: 93.3112s\n",
      "2021-08-02 11:35:49,678 - INFO - joeynmt.training - Epoch  13, Step:    70200, Batch Loss:     1.880884, Tokens per Sec:    16282, Lr: 0.000300\n",
      "2021-08-02 11:36:16,466 - INFO - joeynmt.training - Epoch  13, Step:    70400, Batch Loss:     1.995248, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-08-02 11:36:43,218 - INFO - joeynmt.training - Epoch  13, Step:    70600, Batch Loss:     2.041076, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-02 11:37:10,149 - INFO - joeynmt.training - Epoch  13, Step:    70800, Batch Loss:     1.866229, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-02 11:37:37,280 - INFO - joeynmt.training - Epoch  13, Step:    71000, Batch Loss:     2.173694, Tokens per Sec:    16242, Lr: 0.000300\n",
      "2021-08-02 11:37:39,938 - INFO - joeynmt.training - Epoch  13: total training loss 10792.66\n",
      "2021-08-02 11:37:39,939 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-02 11:38:04,581 - INFO - joeynmt.training - Epoch  14, Step:    71200, Batch Loss:     2.032307, Tokens per Sec:    15737, Lr: 0.000300\n",
      "2021-08-02 11:38:31,715 - INFO - joeynmt.training - Epoch  14, Step:    71400, Batch Loss:     1.802357, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-02 11:38:58,674 - INFO - joeynmt.training - Epoch  14, Step:    71600, Batch Loss:     1.708993, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-02 11:39:25,726 - INFO - joeynmt.training - Epoch  14, Step:    71800, Batch Loss:     1.954015, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-02 11:39:52,727 - INFO - joeynmt.training - Epoch  14, Step:    72000, Batch Loss:     2.269260, Tokens per Sec:    16124, Lr: 0.000300\n",
      "2021-08-02 11:40:19,602 - INFO - joeynmt.training - Epoch  14, Step:    72200, Batch Loss:     2.064125, Tokens per Sec:    16409, Lr: 0.000300\n",
      "2021-08-02 11:40:46,718 - INFO - joeynmt.training - Epoch  14, Step:    72400, Batch Loss:     1.979593, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-08-02 11:41:13,638 - INFO - joeynmt.training - Epoch  14, Step:    72600, Batch Loss:     1.792432, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-08-02 11:41:40,720 - INFO - joeynmt.training - Epoch  14, Step:    72800, Batch Loss:     2.136008, Tokens per Sec:    16246, Lr: 0.000300\n",
      "2021-08-02 11:42:07,629 - INFO - joeynmt.training - Epoch  14, Step:    73000, Batch Loss:     1.815079, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-08-02 11:42:34,724 - INFO - joeynmt.training - Epoch  14, Step:    73200, Batch Loss:     2.058828, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-08-02 11:43:01,494 - INFO - joeynmt.training - Epoch  14, Step:    73400, Batch Loss:     1.923557, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-02 11:43:28,592 - INFO - joeynmt.training - Epoch  14, Step:    73600, Batch Loss:     2.359243, Tokens per Sec:    16195, Lr: 0.000300\n",
      "2021-08-02 11:43:55,501 - INFO - joeynmt.training - Epoch  14, Step:    73800, Batch Loss:     1.996708, Tokens per Sec:    16318, Lr: 0.000300\n",
      "2021-08-02 11:44:22,409 - INFO - joeynmt.training - Epoch  14, Step:    74000, Batch Loss:     1.823419, Tokens per Sec:    16264, Lr: 0.000300\n",
      "2021-08-02 11:44:49,395 - INFO - joeynmt.training - Epoch  14, Step:    74200, Batch Loss:     2.060693, Tokens per Sec:    16030, Lr: 0.000300\n",
      "2021-08-02 11:45:16,486 - INFO - joeynmt.training - Epoch  14, Step:    74400, Batch Loss:     1.923360, Tokens per Sec:    16343, Lr: 0.000300\n",
      "2021-08-02 11:45:43,630 - INFO - joeynmt.training - Epoch  14, Step:    74600, Batch Loss:     1.882875, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-08-02 11:46:11,043 - INFO - joeynmt.training - Epoch  14, Step:    74800, Batch Loss:     2.192658, Tokens per Sec:    16535, Lr: 0.000300\n",
      "2021-08-02 11:46:38,248 - INFO - joeynmt.training - Epoch  14, Step:    75000, Batch Loss:     1.816118, Tokens per Sec:    16118, Lr: 0.000300\n",
      "2021-08-02 11:48:09,592 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:48:09,593 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:48:09,593 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:48:10,929 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:48:10,930 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:48:11,685 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:48:11,686 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 11:48:11,686 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 11:48:11,686 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , that is good for me . ”\n",
      "2021-08-02 11:48:11,686 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:48:11,687 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 11:48:11,687 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 11:48:11,687 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , more than many wealth are standing . ”\n",
      "2021-08-02 11:48:11,687 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:48:11,688 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 11:48:11,688 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 11:48:11,688 - INFO - joeynmt.training - \tHypothesis: Cameron : Look !\n",
      "2021-08-02 11:48:11,688 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:48:11,688 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 11:48:11,689 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 11:48:11,689 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on conquering ” and conquering it completely ?\n",
      "2021-08-02 11:48:11,689 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    75000: bleu:  23.79, loss: 220343.4219, ppl:   5.9044, duration: 93.4404s\n",
      "2021-08-02 11:48:39,227 - INFO - joeynmt.training - Epoch  14, Step:    75200, Batch Loss:     2.156598, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-02 11:49:06,010 - INFO - joeynmt.training - Epoch  14, Step:    75400, Batch Loss:     1.909956, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-02 11:49:33,253 - INFO - joeynmt.training - Epoch  14, Step:    75600, Batch Loss:     1.733068, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-08-02 11:50:00,262 - INFO - joeynmt.training - Epoch  14, Step:    75800, Batch Loss:     1.852776, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-08-02 11:50:27,339 - INFO - joeynmt.training - Epoch  14, Step:    76000, Batch Loss:     1.942680, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-08-02 11:50:54,459 - INFO - joeynmt.training - Epoch  14, Step:    76200, Batch Loss:     1.875197, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-08-02 11:51:21,510 - INFO - joeynmt.training - Epoch  14, Step:    76400, Batch Loss:     1.818909, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-02 11:51:33,756 - INFO - joeynmt.training - Epoch  14: total training loss 10702.78\n",
      "2021-08-02 11:51:33,756 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-02 11:51:49,615 - INFO - joeynmt.training - Epoch  15, Step:    76600, Batch Loss:     1.754848, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-08-02 11:52:16,389 - INFO - joeynmt.training - Epoch  15, Step:    76800, Batch Loss:     1.895324, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-02 11:52:43,510 - INFO - joeynmt.training - Epoch  15, Step:    77000, Batch Loss:     1.923101, Tokens per Sec:    16269, Lr: 0.000300\n",
      "2021-08-02 11:53:10,684 - INFO - joeynmt.training - Epoch  15, Step:    77200, Batch Loss:     1.974043, Tokens per Sec:    16303, Lr: 0.000300\n",
      "2021-08-02 11:53:38,019 - INFO - joeynmt.training - Epoch  15, Step:    77400, Batch Loss:     1.990081, Tokens per Sec:    16227, Lr: 0.000300\n",
      "2021-08-02 11:54:04,661 - INFO - joeynmt.training - Epoch  15, Step:    77600, Batch Loss:     1.701676, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-02 11:54:32,016 - INFO - joeynmt.training - Epoch  15, Step:    77800, Batch Loss:     1.950743, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-08-02 11:54:59,622 - INFO - joeynmt.training - Epoch  15, Step:    78000, Batch Loss:     1.766551, Tokens per Sec:    16577, Lr: 0.000300\n",
      "2021-08-02 11:55:26,727 - INFO - joeynmt.training - Epoch  15, Step:    78200, Batch Loss:     1.883269, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-02 11:55:54,028 - INFO - joeynmt.training - Epoch  15, Step:    78400, Batch Loss:     2.003330, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-08-02 11:56:20,986 - INFO - joeynmt.training - Epoch  15, Step:    78600, Batch Loss:     1.834958, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-02 11:56:48,447 - INFO - joeynmt.training - Epoch  15, Step:    78800, Batch Loss:     1.895085, Tokens per Sec:    16314, Lr: 0.000300\n",
      "2021-08-02 11:57:15,237 - INFO - joeynmt.training - Epoch  15, Step:    79000, Batch Loss:     1.881117, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-02 11:57:42,388 - INFO - joeynmt.training - Epoch  15, Step:    79200, Batch Loss:     1.785837, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-02 11:58:09,651 - INFO - joeynmt.training - Epoch  15, Step:    79400, Batch Loss:     1.954645, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-08-02 11:58:36,908 - INFO - joeynmt.training - Epoch  15, Step:    79600, Batch Loss:     2.036800, Tokens per Sec:    15929, Lr: 0.000300\n",
      "2021-08-02 11:59:03,991 - INFO - joeynmt.training - Epoch  15, Step:    79800, Batch Loss:     1.919533, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-02 11:59:31,235 - INFO - joeynmt.training - Epoch  15, Step:    80000, Batch Loss:     1.884566, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-02 12:01:04,214 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 12:01:04,215 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 12:01:04,215 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 12:01:05,441 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 12:01:05,441 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 12:01:06,172 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 12:01:06,173 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 12:01:06,173 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 12:01:06,173 - INFO - joeynmt.training - \tHypothesis: He sang : “ I have been approaching God , but I have been approaching me . ”\n",
      "2021-08-02 12:01:06,173 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 12:01:06,174 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 12:01:06,174 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 12:01:06,174 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , more than many wealth are standing . ”\n",
      "2021-08-02 12:01:06,174 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 12:01:06,176 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 12:01:06,176 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 12:01:06,176 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies of the book of Daniel related to God’s Kingdom .\n",
      "2021-08-02 12:01:06,176 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 12:01:06,176 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 12:01:06,176 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 12:01:06,177 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to overcome ” and bring it to complete ?\n",
      "2021-08-02 12:01:06,177 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    80000: bleu:  24.04, loss: 217604.9531, ppl:   5.7756, duration: 94.9414s\n",
      "2021-08-02 12:01:33,558 - INFO - joeynmt.training - Epoch  15, Step:    80200, Batch Loss:     2.031386, Tokens per Sec:    15848, Lr: 0.000300\n",
      "2021-08-02 12:02:00,671 - INFO - joeynmt.training - Epoch  15, Step:    80400, Batch Loss:     1.960810, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-08-02 12:02:27,620 - INFO - joeynmt.training - Epoch  15, Step:    80600, Batch Loss:     1.769964, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-08-02 12:02:54,833 - INFO - joeynmt.training - Epoch  15, Step:    80800, Batch Loss:     1.940977, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-08-02 12:03:21,988 - INFO - joeynmt.training - Epoch  15, Step:    81000, Batch Loss:     2.041446, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-02 12:03:49,264 - INFO - joeynmt.training - Epoch  15, Step:    81200, Batch Loss:     1.749235, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-08-02 12:04:16,304 - INFO - joeynmt.training - Epoch  15, Step:    81400, Batch Loss:     1.750744, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-02 12:04:43,452 - INFO - joeynmt.training - Epoch  15, Step:    81600, Batch Loss:     1.905028, Tokens per Sec:    16298, Lr: 0.000300\n",
      "2021-08-02 12:05:10,435 - INFO - joeynmt.training - Epoch  15, Step:    81800, Batch Loss:     1.947161, Tokens per Sec:    16140, Lr: 0.000300\n",
      "2021-08-02 12:05:30,476 - INFO - joeynmt.training - Epoch  15: total training loss 10579.16\n",
      "2021-08-02 12:05:30,476 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-02 12:05:38,325 - INFO - joeynmt.training - Epoch  16, Step:    82000, Batch Loss:     1.701604, Tokens per Sec:    14638, Lr: 0.000300\n",
      "2021-08-02 12:06:05,330 - INFO - joeynmt.training - Epoch  16, Step:    82200, Batch Loss:     1.925360, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-02 12:06:32,487 - INFO - joeynmt.training - Epoch  16, Step:    82400, Batch Loss:     1.958548, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-02 12:06:59,451 - INFO - joeynmt.training - Epoch  16, Step:    82600, Batch Loss:     1.801995, Tokens per Sec:    16027, Lr: 0.000300\n",
      "2021-08-02 12:07:26,751 - INFO - joeynmt.training - Epoch  16, Step:    82800, Batch Loss:     1.841167, Tokens per Sec:    16364, Lr: 0.000300\n",
      "2021-08-02 12:07:53,931 - INFO - joeynmt.training - Epoch  16, Step:    83000, Batch Loss:     2.051433, Tokens per Sec:    16015, Lr: 0.000300\n",
      "2021-08-02 12:08:21,110 - INFO - joeynmt.training - Epoch  16, Step:    83200, Batch Loss:     2.091063, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-08-02 12:08:48,319 - INFO - joeynmt.training - Epoch  16, Step:    83400, Batch Loss:     1.818880, Tokens per Sec:    16308, Lr: 0.000300\n",
      "2021-08-02 12:09:15,544 - INFO - joeynmt.training - Epoch  16, Step:    83600, Batch Loss:     1.942730, Tokens per Sec:    16372, Lr: 0.000300\n",
      "2021-08-02 12:09:42,661 - INFO - joeynmt.training - Epoch  16, Step:    83800, Batch Loss:     1.835698, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-08-02 12:10:09,458 - INFO - joeynmt.training - Epoch  16, Step:    84000, Batch Loss:     2.035233, Tokens per Sec:    16141, Lr: 0.000300\n",
      "2021-08-02 12:10:36,652 - INFO - joeynmt.training - Epoch  16, Step:    84200, Batch Loss:     2.010391, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-02 12:11:03,715 - INFO - joeynmt.training - Epoch  16, Step:    84400, Batch Loss:     1.827770, Tokens per Sec:    16278, Lr: 0.000300\n",
      "2021-08-02 12:11:30,860 - INFO - joeynmt.training - Epoch  16, Step:    84600, Batch Loss:     1.689755, Tokens per Sec:    16015, Lr: 0.000300\n",
      "2021-08-02 12:11:57,872 - INFO - joeynmt.training - Epoch  16, Step:    84800, Batch Loss:     1.861368, Tokens per Sec:    16247, Lr: 0.000300\n",
      "2021-08-02 12:12:25,005 - INFO - joeynmt.training - Epoch  16, Step:    85000, Batch Loss:     2.023921, Tokens per Sec:    16410, Lr: 0.000300\n",
      "2021-08-02 12:14:00,445 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 12:14:00,445 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 12:14:00,445 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 12:14:01,777 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 12:14:01,778 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 12:14:02,550 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 12:14:02,552 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-02 12:14:02,553 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-02 12:14:02,553 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , that is good for me . ”\n",
      "2021-08-02 12:14:02,553 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 12:14:02,553 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-02 12:14:02,554 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-02 12:14:02,554 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , beyond the many wealth that are standing . ”\n",
      "2021-08-02 12:14:02,554 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 12:14:02,555 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-02 12:14:02,555 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-02 12:14:02,555 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy of Daniel , which is about God’s Kingdom .\n",
      "2021-08-02 12:14:02,555 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 12:14:02,556 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-02 12:14:02,556 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-02 12:14:02,556 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and will he completely succeed ?\n",
      "2021-08-02 12:14:02,556 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    85000: bleu:  24.45, loss: 216847.7656, ppl:   5.7404, duration: 97.5508s\n",
      "2021-08-02 12:14:29,937 - INFO - joeynmt.training - Epoch  16, Step:    85200, Batch Loss:     2.124860, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-08-02 12:14:56,962 - INFO - joeynmt.training - Epoch  16, Step:    85400, Batch Loss:     1.922495, Tokens per Sec:    16397, Lr: 0.000300\n",
      "2021-08-02 12:15:24,301 - INFO - joeynmt.training - Epoch  16, Step:    85600, Batch Loss:     1.807021, Tokens per Sec:    16276, Lr: 0.000300\n",
      "2021-08-02 12:15:51,221 - INFO - joeynmt.training - Epoch  16, Step:    85800, Batch Loss:     1.920975, Tokens per Sec:    16189, Lr: 0.000300\n",
      "2021-08-02 12:16:18,040 - INFO - joeynmt.training - Epoch  16, Step:    86000, Batch Loss:     1.785652, Tokens per Sec:    16250, Lr: 0.000300\n",
      "2021-08-02 12:16:44,962 - INFO - joeynmt.training - Epoch  16, Step:    86200, Batch Loss:     1.902160, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-02 12:17:11,686 - INFO - joeynmt.training - Epoch  16, Step:    86400, Batch Loss:     2.009372, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-02 12:17:38,782 - INFO - joeynmt.training - Epoch  16, Step:    86600, Batch Loss:     2.467263, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-08-02 12:18:05,444 - INFO - joeynmt.training - Epoch  16, Step:    86800, Batch Loss:     1.933168, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-02 12:18:32,271 - INFO - joeynmt.training - Epoch  16, Step:    87000, Batch Loss:     1.953486, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-02 12:18:59,460 - INFO - joeynmt.training - Epoch  16, Step:    87200, Batch Loss:     1.877953, Tokens per Sec:    16208, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt$src.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzRdhzzPgrU9"
   },
   "source": [
    "16 epochs ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlJ8i_1H3afn",
    "outputId": "d77a92f4-3203-4d4b-ce1b-1b362cf1464c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5000\tLoss: 380856.18750\tPPL: 21.52559\tbleu: 6.48574\tLR: 0.00030000\t*\n",
      "Steps: 10000\tLoss: 324362.84375\tPPL: 13.65320\tbleu: 11.24357\tLR: 0.00030000\t*\n",
      "Steps: 15000\tLoss: 292323.93750\tPPL: 10.54634\tbleu: 15.11782\tLR: 0.00030000\t*\n",
      "Steps: 20000\tLoss: 275248.56250\tPPL: 9.19052\tbleu: 16.90517\tLR: 0.00030000\t*\n",
      "Steps: 25000\tLoss: 261483.29688\tPPL: 8.22552\tbleu: 18.77979\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 252243.26562\tPPL: 7.63526\tbleu: 19.91301\tLR: 0.00030000\t*\n",
      "Steps: 35000\tLoss: 246034.10938\tPPL: 7.26261\tbleu: 20.82374\tLR: 0.00030000\t*\n",
      "Steps: 40000\tLoss: 240938.93750\tPPL: 6.97044\tbleu: 21.36843\tLR: 0.00030000\t*\n",
      "Steps: 45000\tLoss: 236354.60938\tPPL: 6.71762\tbleu: 21.96751\tLR: 0.00030000\t*\n",
      "Steps: 50000\tLoss: 232201.51562\tPPL: 6.49651\tbleu: 22.37740\tLR: 0.00030000\t*\n",
      "Steps: 55000\tLoss: 228874.09375\tPPL: 6.32462\tbleu: 22.85484\tLR: 0.00030000\t*\n",
      "Steps: 60000\tLoss: 226095.01562\tPPL: 6.18455\tbleu: 23.04758\tLR: 0.00030000\t*\n",
      "Steps: 65000\tLoss: 223887.20312\tPPL: 6.07548\tbleu: 23.40011\tLR: 0.00030000\t*\n",
      "Steps: 70000\tLoss: 222232.12500\tPPL: 5.99499\tbleu: 23.51185\tLR: 0.00030000\t*\n",
      "Steps: 75000\tLoss: 220343.42188\tPPL: 5.90443\tbleu: 23.79195\tLR: 0.00030000\t*\n",
      "Steps: 80000\tLoss: 217604.95312\tPPL: 5.77555\tbleu: 24.03702\tLR: 0.00030000\t*\n",
      "Steps: 85000\tLoss: 216847.76562\tPPL: 5.74042\tbleu: 24.45111\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/rw_lhen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJWjURXjwSnG"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 85000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued\"').replace(\n",
    "        f'epochs: 30', f'epochs: 15')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CY-9k-G7VMd6",
    "outputId": "235d2270-9a1e-48a8-9405-d8b03db89d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rw_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer/85000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 2000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 15                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rw_lhen_reverse_transformer_continued\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_rw_lhen_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zG7hbW9vuvT2",
    "outputId": "6ef97a21-09d1-4f0b-9ae1-5b47b2e3b927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 07:43:02,687 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 07:43:02,749 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 07:43:12,100 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 07:43:12,684 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 07:43:13,386 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 07:43:13,965 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 07:43:13,966 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 07:43:15,418 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 07:43:15.668851: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 07:43:17,348 - INFO - joeynmt.training - Total params: 12177920\n",
      "2021-08-03 07:43:21,034 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer/85000.ckpt\n",
      "2021-08-03 07:43:21,471 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
      "2021-08-03 07:43:21,471 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
      "2021-08-03 07:43:21,471 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 07:43:21,471 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
      "2021-08-03 07:43:21,471 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
      "2021-08-03 07:43:21,472 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
      "2021-08-03 07:43:21,472 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 07:43:21,472 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 07:43:21,472 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 07:43:21,472 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-03 07:43:21,472 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-03 07:43:21,472 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 07:43:21,472 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 07:43:21,473 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer/85000.ckpt\n",
      "2021-08-03 07:43:21,473 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 07:43:21,473 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 07:43:21,473 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 07:43:21,473 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 07:43:21,473 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 07:43:21,473 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 07:43:21,474 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 07:43:21,474 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 07:43:21,474 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 07:43:21,474 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 07:43:21,474 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 07:43:21,474 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 07:43:21,474 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 07:43:21,474 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 07:43:21,475 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 07:43:21,475 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 07:43:21,475 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-03 07:43:21,475 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 07:43:21,475 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 07:43:21,475 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 07:43:21,475 - INFO - joeynmt.helpers - cfg.training.epochs                : 15\n",
      "2021-08-03 07:43:21,476 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-03 07:43:21,476 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-03 07:43:21,476 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 07:43:21,476 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued\n",
      "2021-08-03 07:43:21,476 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 07:43:21,476 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 07:43:21,476 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 07:43:21,477 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 07:43:21,477 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 07:43:21,477 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 07:43:21,477 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 07:43:21,477 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 07:43:21,477 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 07:43:21,477 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 07:43:21,477 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 07:43:21,478 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 07:43:21,478 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 07:43:21,478 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 07:43:21,478 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 07:43:21,478 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 07:43:21,478 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 07:43:21,479 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 07:43:21,479 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 07:43:21,479 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 07:43:21,479 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 07:43:21,479 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 07:43:21,479 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 07:43:21,479 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 07:43:21,480 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 07:43:21,480 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 07:43:21,480 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 07:43:21,480 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 07:43:21,480 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 07:43:21,480 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 07:43:21,480 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 07:43:21,480 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434091,\n",
      "\tvalid 4447,\n",
      "\ttest 79\n",
      "2021-08-03 07:43:21,481 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-03 07:43:21,481 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 07:43:21,481 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 07:43:21,481 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
      "2021-08-03 07:43:21,481 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
      "2021-08-03 07:43:21,481 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
      "2021-08-03 07:43:21,491 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 07:43:21,491 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 07:43:48,489 - INFO - joeynmt.training - Epoch   1, Step:    85200, Batch Loss:     2.095306, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-08-03 07:44:14,302 - INFO - joeynmt.training - Epoch   1, Step:    85400, Batch Loss:     1.913032, Tokens per Sec:    17166, Lr: 0.000300\n",
      "2021-08-03 07:44:40,328 - INFO - joeynmt.training - Epoch   1, Step:    85600, Batch Loss:     1.817296, Tokens per Sec:    17097, Lr: 0.000300\n",
      "2021-08-03 07:45:06,585 - INFO - joeynmt.training - Epoch   1, Step:    85800, Batch Loss:     1.936488, Tokens per Sec:    16597, Lr: 0.000300\n",
      "2021-08-03 07:45:32,746 - INFO - joeynmt.training - Epoch   1, Step:    86000, Batch Loss:     1.801027, Tokens per Sec:    16660, Lr: 0.000300\n",
      "2021-08-03 07:45:59,145 - INFO - joeynmt.training - Epoch   1, Step:    86200, Batch Loss:     1.908487, Tokens per Sec:    16349, Lr: 0.000300\n",
      "2021-08-03 07:46:25,684 - INFO - joeynmt.training - Epoch   1, Step:    86400, Batch Loss:     2.013402, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-03 07:46:52,337 - INFO - joeynmt.training - Epoch   1, Step:    86600, Batch Loss:     2.474955, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-08-03 07:47:19,152 - INFO - joeynmt.training - Epoch   1, Step:    86800, Batch Loss:     1.923881, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-08-03 07:47:45,878 - INFO - joeynmt.training - Epoch   1, Step:    87000, Batch Loss:     1.917161, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-08-03 07:48:13,120 - INFO - joeynmt.training - Epoch   1, Step:    87200, Batch Loss:     1.851813, Tokens per Sec:    16176, Lr: 0.000300\n",
      "2021-08-03 07:48:40,204 - INFO - joeynmt.training - Epoch   1, Step:    87400, Batch Loss:     1.807924, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-08-03 07:48:42,661 - INFO - joeynmt.training - Epoch   1: total training loss 4652.23\n",
      "2021-08-03 07:48:42,661 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 07:49:08,005 - INFO - joeynmt.training - Epoch   2, Step:    87600, Batch Loss:     1.675423, Tokens per Sec:    15646, Lr: 0.000300\n",
      "2021-08-03 07:49:35,274 - INFO - joeynmt.training - Epoch   2, Step:    87800, Batch Loss:     1.834576, Tokens per Sec:    16243, Lr: 0.000300\n",
      "2021-08-03 07:50:02,581 - INFO - joeynmt.training - Epoch   2, Step:    88000, Batch Loss:     1.901875, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-08-03 07:50:29,776 - INFO - joeynmt.training - Epoch   2, Step:    88200, Batch Loss:     1.985321, Tokens per Sec:    15951, Lr: 0.000300\n",
      "2021-08-03 07:50:56,819 - INFO - joeynmt.training - Epoch   2, Step:    88400, Batch Loss:     2.026715, Tokens per Sec:    16208, Lr: 0.000300\n",
      "2021-08-03 07:51:24,089 - INFO - joeynmt.training - Epoch   2, Step:    88600, Batch Loss:     1.907260, Tokens per Sec:    16223, Lr: 0.000300\n",
      "2021-08-03 07:51:51,464 - INFO - joeynmt.training - Epoch   2, Step:    88800, Batch Loss:     2.008819, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-08-03 07:52:18,916 - INFO - joeynmt.training - Epoch   2, Step:    89000, Batch Loss:     1.908844, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-08-03 07:52:46,159 - INFO - joeynmt.training - Epoch   2, Step:    89200, Batch Loss:     1.957057, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-08-03 07:53:13,343 - INFO - joeynmt.training - Epoch   2, Step:    89400, Batch Loss:     1.806105, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-08-03 07:53:40,669 - INFO - joeynmt.training - Epoch   2, Step:    89600, Batch Loss:     1.769389, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-03 07:54:07,957 - INFO - joeynmt.training - Epoch   2, Step:    89800, Batch Loss:     1.866737, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-08-03 07:54:35,414 - INFO - joeynmt.training - Epoch   2, Step:    90000, Batch Loss:     1.996438, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-03 07:56:13,228 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 07:56:13,228 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 07:56:13,229 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 07:56:14,480 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 07:56:14,480 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 07:56:15,254 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 07:56:15,255 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 07:56:15,255 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 07:56:15,255 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have approached God , that is good for me . ”\n",
      "2021-08-03 07:56:15,256 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 07:56:15,256 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 07:56:15,256 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 07:56:15,256 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , and we are more than many treasures that are standing . ”\n",
      "2021-08-03 07:56:15,257 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 07:56:15,257 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 07:56:15,257 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 07:56:15,257 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! The other prophecy of Daniel reveals what it says about God’s Kingdom .\n",
      "2021-08-03 07:56:15,258 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 07:56:15,258 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 07:56:15,258 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 07:56:15,258 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and to bring an end to it ?\n",
      "2021-08-03 07:56:15,259 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    90000: bleu:  24.52, loss: 215451.1875, ppl:   5.6762, duration: 99.8442s\n",
      "2021-08-03 07:56:42,489 - INFO - joeynmt.training - Epoch   2, Step:    90200, Batch Loss:     1.613999, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-08-03 07:57:09,849 - INFO - joeynmt.training - Epoch   2, Step:    90400, Batch Loss:     1.968582, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-03 07:57:36,845 - INFO - joeynmt.training - Epoch   2, Step:    90600, Batch Loss:     1.746541, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-03 07:58:04,064 - INFO - joeynmt.training - Epoch   2, Step:    90800, Batch Loss:     1.967353, Tokens per Sec:    16381, Lr: 0.000300\n",
      "2021-08-03 07:58:31,380 - INFO - joeynmt.training - Epoch   2, Step:    91000, Batch Loss:     1.847177, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-03 07:58:58,493 - INFO - joeynmt.training - Epoch   2, Step:    91200, Batch Loss:     1.852096, Tokens per Sec:    16140, Lr: 0.000300\n",
      "2021-08-03 07:59:25,715 - INFO - joeynmt.training - Epoch   2, Step:    91400, Batch Loss:     2.238641, Tokens per Sec:    15891, Lr: 0.000300\n",
      "2021-08-03 07:59:53,081 - INFO - joeynmt.training - Epoch   2, Step:    91600, Batch Loss:     1.772417, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-03 08:00:20,116 - INFO - joeynmt.training - Epoch   2, Step:    91800, Batch Loss:     1.954817, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-08-03 08:00:47,225 - INFO - joeynmt.training - Epoch   2, Step:    92000, Batch Loss:     1.730286, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-03 08:01:14,417 - INFO - joeynmt.training - Epoch   2, Step:    92200, Batch Loss:     1.968528, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-08-03 08:01:42,035 - INFO - joeynmt.training - Epoch   2, Step:    92400, Batch Loss:     2.144320, Tokens per Sec:    16448, Lr: 0.000300\n",
      "2021-08-03 08:02:09,301 - INFO - joeynmt.training - Epoch   2, Step:    92600, Batch Loss:     1.670721, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-08-03 08:02:36,645 - INFO - joeynmt.training - Epoch   2, Step:    92800, Batch Loss:     2.115054, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-03 08:02:46,767 - INFO - joeynmt.training - Epoch   2: total training loss 10423.72\n",
      "2021-08-03 08:02:46,768 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 08:03:04,132 - INFO - joeynmt.training - Epoch   3, Step:    93000, Batch Loss:     1.838604, Tokens per Sec:    15081, Lr: 0.000300\n",
      "2021-08-03 08:03:31,357 - INFO - joeynmt.training - Epoch   3, Step:    93200, Batch Loss:     1.960357, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-03 08:03:58,785 - INFO - joeynmt.training - Epoch   3, Step:    93400, Batch Loss:     1.997978, Tokens per Sec:    16419, Lr: 0.000300\n",
      "2021-08-03 08:04:26,064 - INFO - joeynmt.training - Epoch   3, Step:    93600, Batch Loss:     1.870571, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-08-03 08:04:53,314 - INFO - joeynmt.training - Epoch   3, Step:    93800, Batch Loss:     1.830681, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-03 08:05:20,249 - INFO - joeynmt.training - Epoch   3, Step:    94000, Batch Loss:     1.773704, Tokens per Sec:    16143, Lr: 0.000300\n",
      "2021-08-03 08:05:47,420 - INFO - joeynmt.training - Epoch   3, Step:    94200, Batch Loss:     1.597706, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-03 08:06:14,594 - INFO - joeynmt.training - Epoch   3, Step:    94400, Batch Loss:     1.884437, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-03 08:06:41,814 - INFO - joeynmt.training - Epoch   3, Step:    94600, Batch Loss:     1.837655, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-08-03 08:07:08,997 - INFO - joeynmt.training - Epoch   3, Step:    94800, Batch Loss:     1.845823, Tokens per Sec:    15905, Lr: 0.000300\n",
      "2021-08-03 08:07:36,184 - INFO - joeynmt.training - Epoch   3, Step:    95000, Batch Loss:     2.200794, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-03 08:09:09,181 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:09:09,182 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:09:09,182 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:09:10,505 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:09:10,505 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:09:11,311 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:09:11,312 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:09:11,312 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:09:11,313 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have approached God , that is good for me . ”\n",
      "2021-08-03 08:09:11,313 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:09:11,313 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:09:11,313 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:09:11,314 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , and we are more than many treasures that are standing . ”\n",
      "2021-08-03 08:09:11,314 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:09:11,315 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:09:11,316 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:09:11,316 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel describes God’s Kingdom .\n",
      "2021-08-03 08:09:11,316 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:09:11,316 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:09:11,316 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:09:11,316 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and conquer completely ?\n",
      "2021-08-03 08:09:11,317 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    95000: bleu:  24.54, loss: 213882.7812, ppl:   5.6049, duration: 95.1322s\n",
      "2021-08-03 08:09:38,606 - INFO - joeynmt.training - Epoch   3, Step:    95200, Batch Loss:     1.896722, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-08-03 08:10:05,951 - INFO - joeynmt.training - Epoch   3, Step:    95400, Batch Loss:     1.758136, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-03 08:10:33,432 - INFO - joeynmt.training - Epoch   3, Step:    95600, Batch Loss:     1.845664, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-08-03 08:11:00,762 - INFO - joeynmt.training - Epoch   3, Step:    95800, Batch Loss:     1.849197, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-08-03 08:11:28,016 - INFO - joeynmt.training - Epoch   3, Step:    96000, Batch Loss:     1.968216, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-03 08:11:55,103 - INFO - joeynmt.training - Epoch   3, Step:    96200, Batch Loss:     2.102947, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-03 08:12:22,360 - INFO - joeynmt.training - Epoch   3, Step:    96400, Batch Loss:     1.948423, Tokens per Sec:    16027, Lr: 0.000300\n",
      "2021-08-03 08:12:49,197 - INFO - joeynmt.training - Epoch   3, Step:    96600, Batch Loss:     1.716048, Tokens per Sec:    15872, Lr: 0.000300\n",
      "2021-08-03 08:13:16,425 - INFO - joeynmt.training - Epoch   3, Step:    96800, Batch Loss:     2.144299, Tokens per Sec:    15787, Lr: 0.000300\n",
      "2021-08-03 08:13:43,652 - INFO - joeynmt.training - Epoch   3, Step:    97000, Batch Loss:     1.890636, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-08-03 08:14:10,976 - INFO - joeynmt.training - Epoch   3, Step:    97200, Batch Loss:     2.061985, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-08-03 08:14:38,086 - INFO - joeynmt.training - Epoch   3, Step:    97400, Batch Loss:     2.047127, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-08-03 08:15:05,146 - INFO - joeynmt.training - Epoch   3, Step:    97600, Batch Loss:     2.072735, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-08-03 08:15:32,324 - INFO - joeynmt.training - Epoch   3, Step:    97800, Batch Loss:     1.995741, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-08-03 08:15:59,750 - INFO - joeynmt.training - Epoch   3, Step:    98000, Batch Loss:     2.057136, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-03 08:16:26,973 - INFO - joeynmt.training - Epoch   3, Step:    98200, Batch Loss:     2.004405, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-08-03 08:16:46,433 - INFO - joeynmt.training - Epoch   3: total training loss 10372.08\n",
      "2021-08-03 08:16:46,434 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 08:16:54,812 - INFO - joeynmt.training - Epoch   4, Step:    98400, Batch Loss:     1.762626, Tokens per Sec:    14472, Lr: 0.000300\n",
      "2021-08-03 08:17:22,060 - INFO - joeynmt.training - Epoch   4, Step:    98600, Batch Loss:     1.735601, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-03 08:17:49,424 - INFO - joeynmt.training - Epoch   4, Step:    98800, Batch Loss:     1.819060, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-03 08:18:16,962 - INFO - joeynmt.training - Epoch   4, Step:    99000, Batch Loss:     1.841913, Tokens per Sec:    16050, Lr: 0.000300\n",
      "2021-08-03 08:18:44,179 - INFO - joeynmt.training - Epoch   4, Step:    99200, Batch Loss:     1.947350, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-08-03 08:19:11,399 - INFO - joeynmt.training - Epoch   4, Step:    99400, Batch Loss:     1.878302, Tokens per Sec:    16076, Lr: 0.000300\n",
      "2021-08-03 08:19:38,351 - INFO - joeynmt.training - Epoch   4, Step:    99600, Batch Loss:     1.898644, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-08-03 08:20:05,675 - INFO - joeynmt.training - Epoch   4, Step:    99800, Batch Loss:     2.307127, Tokens per Sec:    16290, Lr: 0.000300\n",
      "2021-08-03 08:20:33,010 - INFO - joeynmt.training - Epoch   4, Step:   100000, Batch Loss:     1.816217, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-03 08:22:09,284 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:22:09,285 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:22:09,285 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:22:10,549 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:22:10,549 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:22:11,288 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:22:11,292 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:22:11,292 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:22:11,292 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , that is good for me . ”\n",
      "2021-08-03 08:22:11,292 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:22:11,293 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:22:11,293 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:22:11,293 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , more than many treasures that are standing . ”\n",
      "2021-08-03 08:22:11,294 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:22:11,294 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:22:11,294 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:22:11,294 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy of Daniel in the book of Daniel relates to what it says about God’s Kingdom .\n",
      "2021-08-03 08:22:11,295 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:22:11,295 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:22:11,295 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:22:11,295 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on conquering ” and conquering completely ?\n",
      "2021-08-03 08:22:11,296 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   100000: bleu:  24.80, loss: 211976.7969, ppl:   5.5194, duration: 98.2850s\n",
      "2021-08-03 08:22:38,858 - INFO - joeynmt.training - Epoch   4, Step:   100200, Batch Loss:     1.971259, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-03 08:23:05,935 - INFO - joeynmt.training - Epoch   4, Step:   100400, Batch Loss:     1.864192, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-08-03 08:23:32,974 - INFO - joeynmt.training - Epoch   4, Step:   100600, Batch Loss:     2.003126, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-03 08:24:00,241 - INFO - joeynmt.training - Epoch   4, Step:   100800, Batch Loss:     1.803601, Tokens per Sec:    16333, Lr: 0.000300\n",
      "2021-08-03 08:24:27,396 - INFO - joeynmt.training - Epoch   4, Step:   101000, Batch Loss:     1.945755, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-03 08:24:54,585 - INFO - joeynmt.training - Epoch   4, Step:   101200, Batch Loss:     1.819112, Tokens per Sec:    16337, Lr: 0.000300\n",
      "2021-08-03 08:25:21,771 - INFO - joeynmt.training - Epoch   4, Step:   101400, Batch Loss:     1.903256, Tokens per Sec:    15958, Lr: 0.000300\n",
      "2021-08-03 08:25:48,567 - INFO - joeynmt.training - Epoch   4, Step:   101600, Batch Loss:     1.903823, Tokens per Sec:    15956, Lr: 0.000300\n",
      "2021-08-03 08:26:15,857 - INFO - joeynmt.training - Epoch   4, Step:   101800, Batch Loss:     2.073549, Tokens per Sec:    16280, Lr: 0.000300\n",
      "2021-08-03 08:26:42,982 - INFO - joeynmt.training - Epoch   4, Step:   102000, Batch Loss:     1.706001, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-08-03 08:27:10,149 - INFO - joeynmt.training - Epoch   4, Step:   102200, Batch Loss:     1.731859, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-08-03 08:27:37,139 - INFO - joeynmt.training - Epoch   4, Step:   102400, Batch Loss:     1.983765, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-03 08:28:04,402 - INFO - joeynmt.training - Epoch   4, Step:   102600, Batch Loss:     1.838436, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-08-03 08:28:31,539 - INFO - joeynmt.training - Epoch   4, Step:   102800, Batch Loss:     2.127853, Tokens per Sec:    16386, Lr: 0.000300\n",
      "2021-08-03 08:28:58,403 - INFO - joeynmt.training - Epoch   4, Step:   103000, Batch Loss:     1.876866, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-03 08:29:25,602 - INFO - joeynmt.training - Epoch   4, Step:   103200, Batch Loss:     2.239413, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-08-03 08:29:52,728 - INFO - joeynmt.training - Epoch   4, Step:   103400, Batch Loss:     1.893096, Tokens per Sec:    16101, Lr: 0.000300\n",
      "2021-08-03 08:30:20,113 - INFO - joeynmt.training - Epoch   4, Step:   103600, Batch Loss:     2.029880, Tokens per Sec:    16169, Lr: 0.000300\n",
      "2021-08-03 08:30:47,052 - INFO - joeynmt.training - Epoch   4, Step:   103800, Batch Loss:     1.770517, Tokens per Sec:    16182, Lr: 0.000300\n",
      "2021-08-03 08:30:48,162 - INFO - joeynmt.training - Epoch   4: total training loss 10301.30\n",
      "2021-08-03 08:30:48,162 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-03 08:31:14,957 - INFO - joeynmt.training - Epoch   5, Step:   104000, Batch Loss:     2.014499, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-03 08:31:41,754 - INFO - joeynmt.training - Epoch   5, Step:   104200, Batch Loss:     1.805707, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-03 08:32:09,092 - INFO - joeynmt.training - Epoch   5, Step:   104400, Batch Loss:     2.168491, Tokens per Sec:    16232, Lr: 0.000300\n",
      "2021-08-03 08:32:36,498 - INFO - joeynmt.training - Epoch   5, Step:   104600, Batch Loss:     1.928807, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-03 08:33:03,758 - INFO - joeynmt.training - Epoch   5, Step:   104800, Batch Loss:     1.775838, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-03 08:33:30,413 - INFO - joeynmt.training - Epoch   5, Step:   105000, Batch Loss:     1.972204, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-08-03 08:35:05,133 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:35:05,134 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:35:05,134 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:35:06,463 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:35:06,464 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:35:07,588 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:35:07,589 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:35:07,589 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:35:07,589 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , that is good for me . ”\n",
      "2021-08-03 08:35:07,589 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:35:07,590 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:35:07,590 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:35:07,590 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are living in fear of Jehovah , more than many treasures that are standing . ”\n",
      "2021-08-03 08:35:07,590 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:35:07,591 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:35:07,591 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:35:07,591 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-03 08:35:07,591 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:35:07,592 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:35:07,592 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:35:07,592 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and to complete the victory ?\n",
      "2021-08-03 08:35:07,592 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   105000: bleu:  24.80, loss: 211389.2656, ppl:   5.4934, duration: 97.1786s\n",
      "2021-08-03 08:35:34,956 - INFO - joeynmt.training - Epoch   5, Step:   105200, Batch Loss:     1.567838, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-08-03 08:36:02,049 - INFO - joeynmt.training - Epoch   5, Step:   105400, Batch Loss:     1.908502, Tokens per Sec:    16184, Lr: 0.000300\n",
      "2021-08-03 08:36:29,014 - INFO - joeynmt.training - Epoch   5, Step:   105600, Batch Loss:     1.840341, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-03 08:36:56,016 - INFO - joeynmt.training - Epoch   5, Step:   105800, Batch Loss:     1.878523, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-03 08:37:23,334 - INFO - joeynmt.training - Epoch   5, Step:   106000, Batch Loss:     1.648832, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-03 08:37:50,475 - INFO - joeynmt.training - Epoch   5, Step:   106200, Batch Loss:     1.748944, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-08-03 08:38:17,350 - INFO - joeynmt.training - Epoch   5, Step:   106400, Batch Loss:     1.862193, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-08-03 08:38:44,346 - INFO - joeynmt.training - Epoch   5, Step:   106600, Batch Loss:     1.731854, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-08-03 08:39:11,537 - INFO - joeynmt.training - Epoch   5, Step:   106800, Batch Loss:     1.945504, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-03 08:39:38,738 - INFO - joeynmt.training - Epoch   5, Step:   107000, Batch Loss:     1.875614, Tokens per Sec:    16574, Lr: 0.000300\n",
      "2021-08-03 08:40:06,081 - INFO - joeynmt.training - Epoch   5, Step:   107200, Batch Loss:     2.008445, Tokens per Sec:    16326, Lr: 0.000300\n",
      "2021-08-03 08:40:32,713 - INFO - joeynmt.training - Epoch   5, Step:   107400, Batch Loss:     1.877006, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-08-03 08:40:59,916 - INFO - joeynmt.training - Epoch   5, Step:   107600, Batch Loss:     1.760009, Tokens per Sec:    16210, Lr: 0.000300\n",
      "2021-08-03 08:41:27,051 - INFO - joeynmt.training - Epoch   5, Step:   107800, Batch Loss:     2.145493, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-08-03 08:41:54,003 - INFO - joeynmt.training - Epoch   5, Step:   108000, Batch Loss:     1.708092, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-03 08:42:21,188 - INFO - joeynmt.training - Epoch   5, Step:   108200, Batch Loss:     1.680208, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-03 08:42:48,185 - INFO - joeynmt.training - Epoch   5, Step:   108400, Batch Loss:     1.735778, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-08-03 08:43:15,179 - INFO - joeynmt.training - Epoch   5, Step:   108600, Batch Loss:     2.052338, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-03 08:43:42,249 - INFO - joeynmt.training - Epoch   5, Step:   108800, Batch Loss:     2.002468, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-03 08:44:09,481 - INFO - joeynmt.training - Epoch   5, Step:   109000, Batch Loss:     1.939580, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-08-03 08:44:36,421 - INFO - joeynmt.training - Epoch   5, Step:   109200, Batch Loss:     1.762750, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-03 08:44:46,871 - INFO - joeynmt.training - Epoch   5: total training loss 10248.57\n",
      "2021-08-03 08:44:46,871 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-03 08:45:04,370 - INFO - joeynmt.training - Epoch   6, Step:   109400, Batch Loss:     1.916642, Tokens per Sec:    15720, Lr: 0.000300\n",
      "2021-08-03 08:45:31,478 - INFO - joeynmt.training - Epoch   6, Step:   109600, Batch Loss:     1.791493, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-03 08:45:58,408 - INFO - joeynmt.training - Epoch   6, Step:   109800, Batch Loss:     1.868310, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-08-03 08:46:25,584 - INFO - joeynmt.training - Epoch   6, Step:   110000, Batch Loss:     1.923083, Tokens per Sec:    16143, Lr: 0.000300\n",
      "2021-08-03 08:48:00,661 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:48:00,661 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:48:00,662 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:48:01,951 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:48:01,951 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:48:02,743 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:48:02,744 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 08:48:02,744 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 08:48:02,744 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , that is good for me . ”\n",
      "2021-08-03 08:48:02,744 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:48:02,745 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 08:48:02,745 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 08:48:02,745 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel declared : “ We are in fear of Jehovah , more than many riches that are standing . ”\n",
      "2021-08-03 08:48:02,745 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:48:02,745 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 08:48:02,745 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 08:48:02,746 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel describes God’s Kingdom .\n",
      "2021-08-03 08:48:02,746 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:48:02,746 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 08:48:02,747 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 08:48:02,747 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ continue to conquer ” and conquer completely ?\n",
      "2021-08-03 08:48:02,747 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   110000: bleu:  24.87, loss: 210520.3125, ppl:   5.4550, duration: 97.1629s\n",
      "2021-08-03 08:48:30,242 - INFO - joeynmt.training - Epoch   6, Step:   110200, Batch Loss:     1.828806, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-08-03 08:48:57,616 - INFO - joeynmt.training - Epoch   6, Step:   110400, Batch Loss:     1.955751, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-03 08:49:24,937 - INFO - joeynmt.training - Epoch   6, Step:   110600, Batch Loss:     1.698058, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-03 08:49:52,354 - INFO - joeynmt.training - Epoch   6, Step:   110800, Batch Loss:     1.932679, Tokens per Sec:    16027, Lr: 0.000300\n",
      "2021-08-03 08:50:19,757 - INFO - joeynmt.training - Epoch   6, Step:   111000, Batch Loss:     2.093512, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-03 08:50:46,451 - INFO - joeynmt.training - Epoch   6, Step:   111200, Batch Loss:     1.914616, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-08-03 08:51:13,768 - INFO - joeynmt.training - Epoch   6, Step:   111400, Batch Loss:     1.728699, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-03 08:51:40,968 - INFO - joeynmt.training - Epoch   6, Step:   111600, Batch Loss:     2.392070, Tokens per Sec:    16239, Lr: 0.000300\n",
      "2021-08-03 08:52:08,090 - INFO - joeynmt.training - Epoch   6, Step:   111800, Batch Loss:     1.885403, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-08-03 08:52:35,285 - INFO - joeynmt.training - Epoch   6, Step:   112000, Batch Loss:     1.824875, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-03 08:53:02,676 - INFO - joeynmt.training - Epoch   6, Step:   112200, Batch Loss:     1.928964, Tokens per Sec:    16301, Lr: 0.000300\n",
      "2021-08-03 08:53:30,061 - INFO - joeynmt.training - Epoch   6, Step:   112400, Batch Loss:     1.972930, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-08-03 08:53:57,544 - INFO - joeynmt.training - Epoch   6, Step:   112600, Batch Loss:     2.001315, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-08-03 08:54:24,633 - INFO - joeynmt.training - Epoch   6, Step:   112800, Batch Loss:     1.884157, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-08-03 08:54:51,575 - INFO - joeynmt.training - Epoch   6, Step:   113000, Batch Loss:     1.707280, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-08-03 08:55:18,734 - INFO - joeynmt.training - Epoch   6, Step:   113200, Batch Loss:     1.975220, Tokens per Sec:    16124, Lr: 0.000300\n",
      "2021-08-03 08:55:45,952 - INFO - joeynmt.training - Epoch   6, Step:   113400, Batch Loss:     1.846902, Tokens per Sec:    16212, Lr: 0.000300\n",
      "2021-08-03 08:56:13,323 - INFO - joeynmt.training - Epoch   6, Step:   113600, Batch Loss:     1.887253, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-08-03 08:56:40,351 - INFO - joeynmt.training - Epoch   6, Step:   113800, Batch Loss:     2.047491, Tokens per Sec:    16021, Lr: 0.000300\n",
      "2021-08-03 08:57:07,764 - INFO - joeynmt.training - Epoch   6, Step:   114000, Batch Loss:     2.164705, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-08-03 08:57:35,208 - INFO - joeynmt.training - Epoch   6, Step:   114200, Batch Loss:     1.925095, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-08-03 08:58:02,293 - INFO - joeynmt.training - Epoch   6, Step:   114400, Batch Loss:     1.688432, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-03 08:58:29,824 - INFO - joeynmt.training - Epoch   6, Step:   114600, Batch Loss:     1.650278, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-03 08:58:47,815 - INFO - joeynmt.training - Epoch   6: total training loss 10164.57\n",
      "2021-08-03 08:58:47,815 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-03 08:58:57,528 - INFO - joeynmt.training - Epoch   7, Step:   114800, Batch Loss:     1.650518, Tokens per Sec:    15147, Lr: 0.000300\n",
      "2021-08-03 08:59:24,742 - INFO - joeynmt.training - Epoch   7, Step:   115000, Batch Loss:     1.732960, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-03 09:00:59,854 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:00:59,854 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:00:59,855 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:01:01,182 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:01:01,183 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:01:01,964 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:01:01,965 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:01:01,965 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:01:01,965 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am drawing close to God , that is good for me . ”\n",
      "2021-08-03 09:01:01,965 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:01:01,966 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:01:01,966 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:01:01,966 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , greater than many riches that are standing . ”\n",
      "2021-08-03 09:01:01,966 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:01:01,967 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:01:01,967 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:01:01,967 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies of Daniel about God’s Kingdom .\n",
      "2021-08-03 09:01:01,967 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:01:01,968 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:01:01,968 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:01:01,968 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ continue to conquer ” and to conquer it ?\n",
      "2021-08-03 09:01:01,968 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   115000: bleu:  25.18, loss: 209341.7656, ppl:   5.4035, duration: 97.2258s\n",
      "2021-08-03 09:01:29,309 - INFO - joeynmt.training - Epoch   7, Step:   115200, Batch Loss:     1.762747, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-03 09:01:56,579 - INFO - joeynmt.training - Epoch   7, Step:   115400, Batch Loss:     1.674719, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-03 09:02:23,732 - INFO - joeynmt.training - Epoch   7, Step:   115600, Batch Loss:     1.806294, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-03 09:02:50,943 - INFO - joeynmt.training - Epoch   7, Step:   115800, Batch Loss:     1.895431, Tokens per Sec:    16354, Lr: 0.000300\n",
      "2021-08-03 09:03:18,167 - INFO - joeynmt.training - Epoch   7, Step:   116000, Batch Loss:     1.947572, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-08-03 09:03:45,244 - INFO - joeynmt.training - Epoch   7, Step:   116200, Batch Loss:     1.768200, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-03 09:04:12,611 - INFO - joeynmt.training - Epoch   7, Step:   116400, Batch Loss:     1.662846, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-03 09:04:39,953 - INFO - joeynmt.training - Epoch   7, Step:   116600, Batch Loss:     1.956736, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-03 09:05:07,458 - INFO - joeynmt.training - Epoch   7, Step:   116800, Batch Loss:     1.994476, Tokens per Sec:    16184, Lr: 0.000300\n",
      "2021-08-03 09:05:34,323 - INFO - joeynmt.training - Epoch   7, Step:   117000, Batch Loss:     1.878560, Tokens per Sec:    16053, Lr: 0.000300\n",
      "2021-08-03 09:06:01,278 - INFO - joeynmt.training - Epoch   7, Step:   117200, Batch Loss:     2.162414, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-08-03 09:06:28,398 - INFO - joeynmt.training - Epoch   7, Step:   117400, Batch Loss:     1.970897, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-03 09:06:55,896 - INFO - joeynmt.training - Epoch   7, Step:   117600, Batch Loss:     1.898438, Tokens per Sec:    16290, Lr: 0.000300\n",
      "2021-08-03 09:07:22,831 - INFO - joeynmt.training - Epoch   7, Step:   117800, Batch Loss:     1.973930, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-08-03 09:07:49,757 - INFO - joeynmt.training - Epoch   7, Step:   118000, Batch Loss:     1.812302, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-08-03 09:08:17,193 - INFO - joeynmt.training - Epoch   7, Step:   118200, Batch Loss:     1.866095, Tokens per Sec:    16008, Lr: 0.000300\n",
      "2021-08-03 09:08:44,244 - INFO - joeynmt.training - Epoch   7, Step:   118400, Batch Loss:     1.926187, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-08-03 09:09:11,300 - INFO - joeynmt.training - Epoch   7, Step:   118600, Batch Loss:     2.192606, Tokens per Sec:    16106, Lr: 0.000300\n",
      "2021-08-03 09:09:38,358 - INFO - joeynmt.training - Epoch   7, Step:   118800, Batch Loss:     1.850085, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-08-03 09:10:05,344 - INFO - joeynmt.training - Epoch   7, Step:   119000, Batch Loss:     1.860017, Tokens per Sec:    16298, Lr: 0.000300\n",
      "2021-08-03 09:10:32,628 - INFO - joeynmt.training - Epoch   7, Step:   119200, Batch Loss:     1.710384, Tokens per Sec:    15926, Lr: 0.000300\n",
      "2021-08-03 09:11:00,020 - INFO - joeynmt.training - Epoch   7, Step:   119400, Batch Loss:     1.763110, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-03 09:11:26,959 - INFO - joeynmt.training - Epoch   7, Step:   119600, Batch Loss:     1.976461, Tokens per Sec:    16216, Lr: 0.000300\n",
      "2021-08-03 09:11:54,128 - INFO - joeynmt.training - Epoch   7, Step:   119800, Batch Loss:     2.332960, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-03 09:12:21,262 - INFO - joeynmt.training - Epoch   7, Step:   120000, Batch Loss:     1.729864, Tokens per Sec:    16166, Lr: 0.000300\n",
      "2021-08-03 09:13:56,438 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:13:56,438 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:13:56,439 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:13:57,775 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:13:57,776 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:13:58,530 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:13:58,531 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:13:58,531 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:13:58,531 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , that is good for me . ”\n",
      "2021-08-03 09:13:58,532 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:13:58,532 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:13:58,532 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:13:58,532 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our fear of Jehovah is greater than many riches that are standing . ”\n",
      "2021-08-03 09:13:58,533 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:13:58,533 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:13:58,533 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:13:58,533 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies in the book of Daniel about God’s Kingdom .\n",
      "2021-08-03 09:13:58,534 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:13:58,534 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:13:58,534 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:13:58,534 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ keep on conquering ” and conquering it completely ?\n",
      "2021-08-03 09:13:58,535 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   120000: bleu:  25.24, loss: 207900.5781, ppl:   5.3411, duration: 97.2727s\n",
      "2021-08-03 09:14:25,908 - INFO - joeynmt.training - Epoch   7: total training loss 10131.22\n",
      "2021-08-03 09:14:25,909 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-03 09:14:26,766 - INFO - joeynmt.training - Epoch   8, Step:   120200, Batch Loss:     2.008145, Tokens per Sec:     3657, Lr: 0.000300\n",
      "2021-08-03 09:14:53,883 - INFO - joeynmt.training - Epoch   8, Step:   120400, Batch Loss:     1.748743, Tokens per Sec:    16264, Lr: 0.000300\n",
      "2021-08-03 09:15:21,062 - INFO - joeynmt.training - Epoch   8, Step:   120600, Batch Loss:     1.885723, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-03 09:15:48,137 - INFO - joeynmt.training - Epoch   8, Step:   120800, Batch Loss:     1.738792, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-08-03 09:16:15,185 - INFO - joeynmt.training - Epoch   8, Step:   121000, Batch Loss:     1.758869, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-08-03 09:16:42,244 - INFO - joeynmt.training - Epoch   8, Step:   121200, Batch Loss:     1.753948, Tokens per Sec:    16336, Lr: 0.000300\n",
      "2021-08-03 09:17:09,466 - INFO - joeynmt.training - Epoch   8, Step:   121400, Batch Loss:     2.266600, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-08-03 09:17:36,531 - INFO - joeynmt.training - Epoch   8, Step:   121600, Batch Loss:     1.905370, Tokens per Sec:    16147, Lr: 0.000300\n",
      "2021-08-03 09:18:03,774 - INFO - joeynmt.training - Epoch   8, Step:   121800, Batch Loss:     2.146849, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-08-03 09:18:30,782 - INFO - joeynmt.training - Epoch   8, Step:   122000, Batch Loss:     1.794651, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-03 09:18:57,984 - INFO - joeynmt.training - Epoch   8, Step:   122200, Batch Loss:     1.705983, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-08-03 09:19:25,144 - INFO - joeynmt.training - Epoch   8, Step:   122400, Batch Loss:     1.717149, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-08-03 09:19:52,129 - INFO - joeynmt.training - Epoch   8, Step:   122600, Batch Loss:     1.514950, Tokens per Sec:    16385, Lr: 0.000300\n",
      "2021-08-03 09:20:19,278 - INFO - joeynmt.training - Epoch   8, Step:   122800, Batch Loss:     1.815069, Tokens per Sec:    15916, Lr: 0.000300\n",
      "2021-08-03 09:20:46,138 - INFO - joeynmt.training - Epoch   8, Step:   123000, Batch Loss:     1.749947, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-03 09:21:12,917 - INFO - joeynmt.training - Epoch   8, Step:   123200, Batch Loss:     1.787898, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-08-03 09:21:40,338 - INFO - joeynmt.training - Epoch   8, Step:   123400, Batch Loss:     1.829901, Tokens per Sec:    16223, Lr: 0.000300\n",
      "2021-08-03 09:22:07,389 - INFO - joeynmt.training - Epoch   8, Step:   123600, Batch Loss:     2.202005, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-08-03 09:22:34,496 - INFO - joeynmt.training - Epoch   8, Step:   123800, Batch Loss:     1.843155, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-08-03 09:23:01,519 - INFO - joeynmt.training - Epoch   8, Step:   124000, Batch Loss:     1.716990, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-03 09:23:28,623 - INFO - joeynmt.training - Epoch   8, Step:   124200, Batch Loss:     1.964817, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-03 09:23:55,854 - INFO - joeynmt.training - Epoch   8, Step:   124400, Batch Loss:     1.894476, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-08-03 09:24:22,930 - INFO - joeynmt.training - Epoch   8, Step:   124600, Batch Loss:     1.831569, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-08-03 09:24:49,854 - INFO - joeynmt.training - Epoch   8, Step:   124800, Batch Loss:     1.813914, Tokens per Sec:    16244, Lr: 0.000300\n",
      "2021-08-03 09:25:17,154 - INFO - joeynmt.training - Epoch   8, Step:   125000, Batch Loss:     1.838469, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-03 09:26:51,282 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:26:51,282 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:26:51,283 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:26:53,635 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:26:53,636 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:26:53,636 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:26:53,637 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , that good for me . ”\n",
      "2021-08-03 09:26:53,637 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:26:53,637 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:26:53,638 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:26:53,638 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , more than many wealth are standing . ”\n",
      "2021-08-03 09:26:53,638 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:26:53,638 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:26:53,639 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:26:53,639 - INFO - joeynmt.training - \tHypothesis: Cameron : Here is another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-03 09:26:53,639 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:26:53,640 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:26:53,640 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:26:53,640 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ continue to conquer ” and to conquer it completely ?\n",
      "2021-08-03 09:26:53,641 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   125000: bleu:  25.44, loss: 207952.7656, ppl:   5.3433, duration: 96.4863s\n",
      "2021-08-03 09:27:20,828 - INFO - joeynmt.training - Epoch   8, Step:   125200, Batch Loss:     2.025521, Tokens per Sec:    16124, Lr: 0.000300\n",
      "2021-08-03 09:27:48,060 - INFO - joeynmt.training - Epoch   8, Step:   125400, Batch Loss:     1.674344, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-03 09:28:15,260 - INFO - joeynmt.training - Epoch   8, Step:   125600, Batch Loss:     2.077841, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-08-03 09:28:25,127 - INFO - joeynmt.training - Epoch   8: total training loss 10112.40\n",
      "2021-08-03 09:28:25,127 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-03 09:28:42,897 - INFO - joeynmt.training - Epoch   9, Step:   125800, Batch Loss:     1.822945, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-08-03 09:29:10,072 - INFO - joeynmt.training - Epoch   9, Step:   126000, Batch Loss:     1.933422, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-03 09:29:37,193 - INFO - joeynmt.training - Epoch   9, Step:   126200, Batch Loss:     1.770230, Tokens per Sec:    16314, Lr: 0.000300\n",
      "2021-08-03 09:30:04,386 - INFO - joeynmt.training - Epoch   9, Step:   126400, Batch Loss:     1.822054, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-08-03 09:30:31,789 - INFO - joeynmt.training - Epoch   9, Step:   126600, Batch Loss:     1.977938, Tokens per Sec:    16214, Lr: 0.000300\n",
      "2021-08-03 09:30:58,800 - INFO - joeynmt.training - Epoch   9, Step:   126800, Batch Loss:     1.812289, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-03 09:31:26,004 - INFO - joeynmt.training - Epoch   9, Step:   127000, Batch Loss:     1.795515, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-03 09:31:53,269 - INFO - joeynmt.training - Epoch   9, Step:   127200, Batch Loss:     2.004426, Tokens per Sec:    16481, Lr: 0.000300\n",
      "2021-08-03 09:32:20,554 - INFO - joeynmt.training - Epoch   9, Step:   127400, Batch Loss:     1.919645, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-08-03 09:32:47,757 - INFO - joeynmt.training - Epoch   9, Step:   127600, Batch Loss:     1.711008, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-03 09:33:14,971 - INFO - joeynmt.training - Epoch   9, Step:   127800, Batch Loss:     1.782678, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-08-03 09:33:41,670 - INFO - joeynmt.training - Epoch   9, Step:   128000, Batch Loss:     1.938059, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-08-03 09:34:09,062 - INFO - joeynmt.training - Epoch   9, Step:   128200, Batch Loss:     1.742684, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-08-03 09:34:36,180 - INFO - joeynmt.training - Epoch   9, Step:   128400, Batch Loss:     1.557221, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-08-03 09:35:03,250 - INFO - joeynmt.training - Epoch   9, Step:   128600, Batch Loss:     1.622031, Tokens per Sec:    16182, Lr: 0.000300\n",
      "2021-08-03 09:35:30,504 - INFO - joeynmt.training - Epoch   9, Step:   128800, Batch Loss:     1.877426, Tokens per Sec:    16166, Lr: 0.000300\n",
      "2021-08-03 09:35:57,473 - INFO - joeynmt.training - Epoch   9, Step:   129000, Batch Loss:     1.759971, Tokens per Sec:    16397, Lr: 0.000300\n",
      "2021-08-03 09:36:24,741 - INFO - joeynmt.training - Epoch   9, Step:   129200, Batch Loss:     1.849563, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-08-03 09:36:51,698 - INFO - joeynmt.training - Epoch   9, Step:   129400, Batch Loss:     1.513298, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-08-03 09:37:18,961 - INFO - joeynmt.training - Epoch   9, Step:   129600, Batch Loss:     1.705965, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-08-03 09:37:46,269 - INFO - joeynmt.training - Epoch   9, Step:   129800, Batch Loss:     1.775865, Tokens per Sec:    16101, Lr: 0.000300\n",
      "2021-08-03 09:38:13,075 - INFO - joeynmt.training - Epoch   9, Step:   130000, Batch Loss:     1.908748, Tokens per Sec:    15875, Lr: 0.000300\n",
      "2021-08-03 09:39:48,085 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:39:48,085 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:39:48,085 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:39:49,432 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:39:49,432 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:39:50,601 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:39:50,602 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:39:50,602 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:39:50,602 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , O Jehovah , is good for me . ”\n",
      "2021-08-03 09:39:50,602 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:39:50,602 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:39:50,603 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:39:50,603 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are living in fear of Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-03 09:39:50,603 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:39:50,603 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:39:50,603 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:39:50,604 - INFO - joeynmt.training - \tHypothesis: Cameron : Here is another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-03 09:39:50,604 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:39:50,604 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:39:50,604 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:39:50,604 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ continue to conquer ” and conquer completely ?\n",
      "2021-08-03 09:39:50,605 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   130000: bleu:  25.38, loss: 207022.9531, ppl:   5.3034, duration: 97.5294s\n",
      "2021-08-03 09:40:17,914 - INFO - joeynmt.training - Epoch   9, Step:   130200, Batch Loss:     1.838598, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-08-03 09:40:44,837 - INFO - joeynmt.training - Epoch   9, Step:   130400, Batch Loss:     1.853346, Tokens per Sec:    16130, Lr: 0.000300\n",
      "2021-08-03 09:41:11,953 - INFO - joeynmt.training - Epoch   9, Step:   130600, Batch Loss:     2.018211, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-08-03 09:41:38,957 - INFO - joeynmt.training - Epoch   9, Step:   130800, Batch Loss:     1.729803, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-08-03 09:42:06,097 - INFO - joeynmt.training - Epoch   9, Step:   131000, Batch Loss:     1.509233, Tokens per Sec:    16017, Lr: 0.000300\n",
      "2021-08-03 09:42:24,844 - INFO - joeynmt.training - Epoch   9: total training loss 10046.34\n",
      "2021-08-03 09:42:24,844 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-03 09:42:33,569 - INFO - joeynmt.training - Epoch  10, Step:   131200, Batch Loss:     1.665409, Tokens per Sec:    14911, Lr: 0.000300\n",
      "2021-08-03 09:43:00,610 - INFO - joeynmt.training - Epoch  10, Step:   131400, Batch Loss:     1.708449, Tokens per Sec:    16246, Lr: 0.000300\n",
      "2021-08-03 09:43:27,757 - INFO - joeynmt.training - Epoch  10, Step:   131600, Batch Loss:     1.731376, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-03 09:43:54,798 - INFO - joeynmt.training - Epoch  10, Step:   131800, Batch Loss:     1.790368, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-03 09:44:22,194 - INFO - joeynmt.training - Epoch  10, Step:   132000, Batch Loss:     2.040494, Tokens per Sec:    16288, Lr: 0.000300\n",
      "2021-08-03 09:44:49,551 - INFO - joeynmt.training - Epoch  10, Step:   132200, Batch Loss:     1.621341, Tokens per Sec:    16264, Lr: 0.000300\n",
      "2021-08-03 09:45:16,420 - INFO - joeynmt.training - Epoch  10, Step:   132400, Batch Loss:     2.157907, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-03 09:45:43,549 - INFO - joeynmt.training - Epoch  10, Step:   132600, Batch Loss:     1.964978, Tokens per Sec:    16185, Lr: 0.000300\n",
      "2021-08-03 09:46:10,889 - INFO - joeynmt.training - Epoch  10, Step:   132800, Batch Loss:     1.786315, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-08-03 09:46:37,599 - INFO - joeynmt.training - Epoch  10, Step:   133000, Batch Loss:     1.739661, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-03 09:47:04,909 - INFO - joeynmt.training - Epoch  10, Step:   133200, Batch Loss:     1.828629, Tokens per Sec:    16241, Lr: 0.000300\n",
      "2021-08-03 09:47:32,221 - INFO - joeynmt.training - Epoch  10, Step:   133400, Batch Loss:     1.825889, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-03 09:47:59,146 - INFO - joeynmt.training - Epoch  10, Step:   133600, Batch Loss:     2.001918, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-08-03 09:48:26,464 - INFO - joeynmt.training - Epoch  10, Step:   133800, Batch Loss:     1.810667, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-03 09:48:53,668 - INFO - joeynmt.training - Epoch  10, Step:   134000, Batch Loss:     1.830015, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-03 09:49:20,665 - INFO - joeynmt.training - Epoch  10, Step:   134200, Batch Loss:     1.822235, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-03 09:49:47,753 - INFO - joeynmt.training - Epoch  10, Step:   134400, Batch Loss:     1.661782, Tokens per Sec:    16246, Lr: 0.000300\n",
      "2021-08-03 09:50:15,122 - INFO - joeynmt.training - Epoch  10, Step:   134600, Batch Loss:     1.800456, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-03 09:50:42,201 - INFO - joeynmt.training - Epoch  10, Step:   134800, Batch Loss:     2.143481, Tokens per Sec:    16401, Lr: 0.000300\n",
      "2021-08-03 09:51:09,629 - INFO - joeynmt.training - Epoch  10, Step:   135000, Batch Loss:     1.780307, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-03 09:52:44,788 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:52:44,788 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:52:44,788 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:52:46,017 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:52:46,017 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:52:46,746 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:52:46,746 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 09:52:46,747 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 09:52:46,747 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , in turn , draw close to God , that is good for me . ”\n",
      "2021-08-03 09:52:46,747 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:52:46,747 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 09:52:46,747 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 09:52:46,748 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , greater riches are being stumbling . ”\n",
      "2021-08-03 09:52:46,748 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:52:46,748 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 09:52:46,748 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 09:52:46,748 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel describes God’s Kingdom .\n",
      "2021-08-03 09:52:46,749 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:52:46,749 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 09:52:46,749 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 09:52:46,749 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ continue to conquer ” and to complete the victory ?\n",
      "2021-08-03 09:52:46,750 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   135000: bleu:  25.54, loss: 205954.9688, ppl:   5.2580, duration: 97.1202s\n",
      "2021-08-03 09:53:14,081 - INFO - joeynmt.training - Epoch  10, Step:   135200, Batch Loss:     1.872452, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-03 09:53:41,030 - INFO - joeynmt.training - Epoch  10, Step:   135400, Batch Loss:     1.773821, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-08-03 09:54:07,840 - INFO - joeynmt.training - Epoch  10, Step:   135600, Batch Loss:     1.776190, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-08-03 09:54:35,199 - INFO - joeynmt.training - Epoch  10, Step:   135800, Batch Loss:     1.762086, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-08-03 09:55:02,689 - INFO - joeynmt.training - Epoch  10, Step:   136000, Batch Loss:     1.618806, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-03 09:55:29,758 - INFO - joeynmt.training - Epoch  10, Step:   136200, Batch Loss:     1.777033, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-03 09:55:57,034 - INFO - joeynmt.training - Epoch  10, Step:   136400, Batch Loss:     1.990048, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-03 09:56:24,302 - INFO - joeynmt.training - Epoch  10, Step:   136600, Batch Loss:     1.765913, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-03 09:56:25,259 - INFO - joeynmt.training - Epoch  10: total training loss 10009.13\n",
      "2021-08-03 09:56:25,259 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-03 09:56:52,034 - INFO - joeynmt.training - Epoch  11, Step:   136800, Batch Loss:     1.767554, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-08-03 09:57:19,293 - INFO - joeynmt.training - Epoch  11, Step:   137000, Batch Loss:     1.841208, Tokens per Sec:    15799, Lr: 0.000300\n",
      "2021-08-03 09:57:46,543 - INFO - joeynmt.training - Epoch  11, Step:   137200, Batch Loss:     1.622354, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-08-03 09:58:13,393 - INFO - joeynmt.training - Epoch  11, Step:   137400, Batch Loss:     1.835596, Tokens per Sec:    15859, Lr: 0.000300\n",
      "2021-08-03 09:58:40,753 - INFO - joeynmt.training - Epoch  11, Step:   137600, Batch Loss:     1.987104, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-03 09:59:07,997 - INFO - joeynmt.training - Epoch  11, Step:   137800, Batch Loss:     1.832650, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-03 09:59:35,257 - INFO - joeynmt.training - Epoch  11, Step:   138000, Batch Loss:     1.885746, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-03 10:00:02,768 - INFO - joeynmt.training - Epoch  11, Step:   138200, Batch Loss:     1.872010, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-03 10:00:30,129 - INFO - joeynmt.training - Epoch  11, Step:   138400, Batch Loss:     1.898549, Tokens per Sec:    16247, Lr: 0.000300\n",
      "2021-08-03 10:00:57,485 - INFO - joeynmt.training - Epoch  11, Step:   138600, Batch Loss:     1.881794, Tokens per Sec:    15928, Lr: 0.000300\n",
      "2021-08-03 10:01:24,673 - INFO - joeynmt.training - Epoch  11, Step:   138800, Batch Loss:     1.919907, Tokens per Sec:    16117, Lr: 0.000300\n",
      "2021-08-03 10:01:51,885 - INFO - joeynmt.training - Epoch  11, Step:   139000, Batch Loss:     1.833275, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-08-03 10:02:19,271 - INFO - joeynmt.training - Epoch  11, Step:   139200, Batch Loss:     2.035768, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-03 10:02:46,428 - INFO - joeynmt.training - Epoch  11, Step:   139400, Batch Loss:     1.534263, Tokens per Sec:    16214, Lr: 0.000300\n",
      "2021-08-03 10:03:13,968 - INFO - joeynmt.training - Epoch  11, Step:   139600, Batch Loss:     1.792947, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-08-03 10:03:41,260 - INFO - joeynmt.training - Epoch  11, Step:   139800, Batch Loss:     1.938433, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-03 10:04:08,571 - INFO - joeynmt.training - Epoch  11, Step:   140000, Batch Loss:     1.775000, Tokens per Sec:    15925, Lr: 0.000300\n",
      "2021-08-03 10:05:43,856 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:05:43,857 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:05:43,857 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:05:45,181 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:05:45,181 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:05:45,947 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:05:45,948 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:05:45,948 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:05:45,948 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , that is good for me . ”\n",
      "2021-08-03 10:05:45,949 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:05:45,949 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:05:45,949 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:05:45,949 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , more than many riches that are standing . ”\n",
      "2021-08-03 10:05:45,950 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:05:45,950 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:05:45,950 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:05:45,950 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-03 10:05:45,950 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:05:45,951 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:05:45,951 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:05:45,951 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep on conquering ” and completely victory ?\n",
      "2021-08-03 10:05:45,951 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   140000: bleu:  25.72, loss: 204856.8125, ppl:   5.2117, duration: 97.3798s\n",
      "2021-08-03 10:06:13,692 - INFO - joeynmt.training - Epoch  11, Step:   140200, Batch Loss:     1.902933, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-08-03 10:06:40,910 - INFO - joeynmt.training - Epoch  11, Step:   140400, Batch Loss:     1.646560, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-03 10:07:08,051 - INFO - joeynmt.training - Epoch  11, Step:   140600, Batch Loss:     1.769550, Tokens per Sec:    15673, Lr: 0.000300\n",
      "2021-08-03 10:07:35,385 - INFO - joeynmt.training - Epoch  11, Step:   140800, Batch Loss:     1.752882, Tokens per Sec:    16332, Lr: 0.000300\n",
      "2021-08-03 10:08:02,570 - INFO - joeynmt.training - Epoch  11, Step:   141000, Batch Loss:     2.010590, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-08-03 10:08:30,099 - INFO - joeynmt.training - Epoch  11, Step:   141200, Batch Loss:     1.736801, Tokens per Sec:    16246, Lr: 0.000300\n",
      "2021-08-03 10:08:57,399 - INFO - joeynmt.training - Epoch  11, Step:   141400, Batch Loss:     1.888010, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-03 10:09:24,887 - INFO - joeynmt.training - Epoch  11, Step:   141600, Batch Loss:     1.697773, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-03 10:09:51,947 - INFO - joeynmt.training - Epoch  11, Step:   141800, Batch Loss:     2.103484, Tokens per Sec:    16084, Lr: 0.000300\n",
      "2021-08-03 10:10:19,409 - INFO - joeynmt.training - Epoch  11, Step:   142000, Batch Loss:     2.146895, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-03 10:10:29,301 - INFO - joeynmt.training - Epoch  11: total training loss 9965.43\n",
      "2021-08-03 10:10:29,302 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-03 10:10:47,644 - INFO - joeynmt.training - Epoch  12, Step:   142200, Batch Loss:     1.804490, Tokens per Sec:    15368, Lr: 0.000300\n",
      "2021-08-03 10:11:15,048 - INFO - joeynmt.training - Epoch  12, Step:   142400, Batch Loss:     1.870249, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-03 10:11:42,316 - INFO - joeynmt.training - Epoch  12, Step:   142600, Batch Loss:     1.820640, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-03 10:12:09,604 - INFO - joeynmt.training - Epoch  12, Step:   142800, Batch Loss:     1.863109, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-03 10:12:36,669 - INFO - joeynmt.training - Epoch  12, Step:   143000, Batch Loss:     1.900550, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-03 10:13:03,872 - INFO - joeynmt.training - Epoch  12, Step:   143200, Batch Loss:     1.783995, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-08-03 10:13:31,057 - INFO - joeynmt.training - Epoch  12, Step:   143400, Batch Loss:     1.838094, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-03 10:13:58,404 - INFO - joeynmt.training - Epoch  12, Step:   143600, Batch Loss:     1.744577, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-08-03 10:14:25,974 - INFO - joeynmt.training - Epoch  12, Step:   143800, Batch Loss:     1.783506, Tokens per Sec:    16042, Lr: 0.000300\n",
      "2021-08-03 10:14:53,086 - INFO - joeynmt.training - Epoch  12, Step:   144000, Batch Loss:     1.766784, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-08-03 10:15:20,590 - INFO - joeynmt.training - Epoch  12, Step:   144200, Batch Loss:     1.936658, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-08-03 10:15:47,830 - INFO - joeynmt.training - Epoch  12, Step:   144400, Batch Loss:     1.717825, Tokens per Sec:    16008, Lr: 0.000300\n",
      "2021-08-03 10:16:14,867 - INFO - joeynmt.training - Epoch  12, Step:   144600, Batch Loss:     1.657758, Tokens per Sec:    16204, Lr: 0.000300\n",
      "2021-08-03 10:16:42,406 - INFO - joeynmt.training - Epoch  12, Step:   144800, Batch Loss:     1.845854, Tokens per Sec:    15951, Lr: 0.000300\n",
      "2021-08-03 10:17:09,915 - INFO - joeynmt.training - Epoch  12, Step:   145000, Batch Loss:     1.708745, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-03 10:18:43,197 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:18:43,198 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:18:43,198 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:18:45,144 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:18:45,145 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:18:45,145 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:18:45,145 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-03 10:18:45,145 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:18:45,146 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:18:45,146 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:18:45,146 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , far more than many treasures that are standing . ”\n",
      "2021-08-03 10:18:45,146 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:18:45,147 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:18:45,147 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:18:45,147 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel .\n",
      "2021-08-03 10:18:45,147 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:18:45,147 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:18:45,148 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:18:45,148 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and conquering it completely ?\n",
      "2021-08-03 10:18:45,148 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   145000: bleu:  25.82, loss: 204917.9531, ppl:   5.2142, duration: 95.2325s\n",
      "2021-08-03 10:19:12,740 - INFO - joeynmt.training - Epoch  12, Step:   145200, Batch Loss:     1.771253, Tokens per Sec:    15850, Lr: 0.000300\n",
      "2021-08-03 10:19:40,198 - INFO - joeynmt.training - Epoch  12, Step:   145400, Batch Loss:     1.818770, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-08-03 10:20:07,450 - INFO - joeynmt.training - Epoch  12, Step:   145600, Batch Loss:     1.868148, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-08-03 10:20:34,551 - INFO - joeynmt.training - Epoch  12, Step:   145800, Batch Loss:     1.753157, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-08-03 10:21:01,755 - INFO - joeynmt.training - Epoch  12, Step:   146000, Batch Loss:     1.854684, Tokens per Sec:    16019, Lr: 0.000300\n",
      "2021-08-03 10:21:29,313 - INFO - joeynmt.training - Epoch  12, Step:   146200, Batch Loss:     1.971209, Tokens per Sec:    16178, Lr: 0.000300\n",
      "2021-08-03 10:21:56,936 - INFO - joeynmt.training - Epoch  12, Step:   146400, Batch Loss:     1.839636, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-03 10:22:24,142 - INFO - joeynmt.training - Epoch  12, Step:   146600, Batch Loss:     1.771736, Tokens per Sec:    16260, Lr: 0.000300\n",
      "2021-08-03 10:22:51,236 - INFO - joeynmt.training - Epoch  12, Step:   146800, Batch Loss:     1.727346, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-03 10:23:18,479 - INFO - joeynmt.training - Epoch  12, Step:   147000, Batch Loss:     1.757141, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-03 10:23:45,548 - INFO - joeynmt.training - Epoch  12, Step:   147200, Batch Loss:     1.756768, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-03 10:24:12,998 - INFO - joeynmt.training - Epoch  12, Step:   147400, Batch Loss:     1.929665, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-03 10:24:31,355 - INFO - joeynmt.training - Epoch  12: total training loss 9927.34\n",
      "2021-08-03 10:24:31,355 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-03 10:24:40,561 - INFO - joeynmt.training - Epoch  13, Step:   147600, Batch Loss:     1.836594, Tokens per Sec:    15055, Lr: 0.000300\n",
      "2021-08-03 10:25:08,035 - INFO - joeynmt.training - Epoch  13, Step:   147800, Batch Loss:     1.856692, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-03 10:25:35,403 - INFO - joeynmt.training - Epoch  13, Step:   148000, Batch Loss:     1.852569, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-08-03 10:26:02,539 - INFO - joeynmt.training - Epoch  13, Step:   148200, Batch Loss:     2.020514, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-08-03 10:26:29,877 - INFO - joeynmt.training - Epoch  13, Step:   148400, Batch Loss:     1.541536, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-08-03 10:26:57,200 - INFO - joeynmt.training - Epoch  13, Step:   148600, Batch Loss:     2.307868, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-08-03 10:27:24,171 - INFO - joeynmt.training - Epoch  13, Step:   148800, Batch Loss:     1.881043, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-03 10:27:51,649 - INFO - joeynmt.training - Epoch  13, Step:   149000, Batch Loss:     1.848233, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-03 10:28:19,102 - INFO - joeynmt.training - Epoch  13, Step:   149200, Batch Loss:     1.843022, Tokens per Sec:    16149, Lr: 0.000300\n",
      "2021-08-03 10:28:46,238 - INFO - joeynmt.training - Epoch  13, Step:   149400, Batch Loss:     1.926059, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-08-03 10:29:13,622 - INFO - joeynmt.training - Epoch  13, Step:   149600, Batch Loss:     1.748766, Tokens per Sec:    16035, Lr: 0.000300\n",
      "2021-08-03 10:29:40,721 - INFO - joeynmt.training - Epoch  13, Step:   149800, Batch Loss:     1.993992, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-08-03 10:30:08,136 - INFO - joeynmt.training - Epoch  13, Step:   150000, Batch Loss:     1.776221, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-03 10:31:41,450 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:31:41,450 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:31:41,451 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:31:42,678 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:31:42,679 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:31:43,433 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:31:43,433 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:31:43,433 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:31:43,434 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-03 10:31:43,434 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:31:43,434 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:31:43,434 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:31:43,435 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , far more than many treasures that are standing . ”\n",
      "2021-08-03 10:31:43,435 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:31:43,435 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:31:43,435 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:31:43,435 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies in the book of Daniel describing God’s Kingdom .\n",
      "2021-08-03 10:31:43,435 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:31:43,436 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:31:43,436 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:31:43,436 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and conquering it completely ?\n",
      "2021-08-03 10:31:43,436 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   150000: bleu:  25.82, loss: 203927.3125, ppl:   5.1728, duration: 95.3000s\n",
      "2021-08-03 10:32:11,001 - INFO - joeynmt.training - Epoch  13, Step:   150200, Batch Loss:     2.062879, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-03 10:32:38,404 - INFO - joeynmt.training - Epoch  13, Step:   150400, Batch Loss:     1.703443, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-03 10:33:05,923 - INFO - joeynmt.training - Epoch  13, Step:   150600, Batch Loss:     1.804976, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-03 10:33:33,071 - INFO - joeynmt.training - Epoch  13, Step:   150800, Batch Loss:     1.434571, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-03 10:34:00,626 - INFO - joeynmt.training - Epoch  13, Step:   151000, Batch Loss:     1.902733, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-03 10:34:27,802 - INFO - joeynmt.training - Epoch  13, Step:   151200, Batch Loss:     1.913775, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-08-03 10:34:55,235 - INFO - joeynmt.training - Epoch  13, Step:   151400, Batch Loss:     1.943869, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-08-03 10:35:22,631 - INFO - joeynmt.training - Epoch  13, Step:   151600, Batch Loss:     1.732983, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-03 10:35:49,885 - INFO - joeynmt.training - Epoch  13, Step:   151800, Batch Loss:     1.591339, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-03 10:36:16,941 - INFO - joeynmt.training - Epoch  13, Step:   152000, Batch Loss:     1.876739, Tokens per Sec:    15619, Lr: 0.000300\n",
      "2021-08-03 10:36:44,010 - INFO - joeynmt.training - Epoch  13, Step:   152200, Batch Loss:     1.691288, Tokens per Sec:    16076, Lr: 0.000300\n",
      "2021-08-03 10:37:11,376 - INFO - joeynmt.training - Epoch  13, Step:   152400, Batch Loss:     1.896666, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-03 10:37:38,600 - INFO - joeynmt.training - Epoch  13, Step:   152600, Batch Loss:     2.006028, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-03 10:38:06,283 - INFO - joeynmt.training - Epoch  13, Step:   152800, Batch Loss:     1.682820, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-08-03 10:38:33,057 - INFO - joeynmt.training - Epoch  13: total training loss 9880.93\n",
      "2021-08-03 10:38:33,057 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-03 10:38:34,262 - INFO - joeynmt.training - Epoch  14, Step:   153000, Batch Loss:     1.916268, Tokens per Sec:     7795, Lr: 0.000300\n",
      "2021-08-03 10:39:01,763 - INFO - joeynmt.training - Epoch  14, Step:   153200, Batch Loss:     1.615590, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-03 10:39:29,029 - INFO - joeynmt.training - Epoch  14, Step:   153400, Batch Loss:     1.795594, Tokens per Sec:    16282, Lr: 0.000300\n",
      "2021-08-03 10:39:56,392 - INFO - joeynmt.training - Epoch  14, Step:   153600, Batch Loss:     1.723419, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-03 10:40:23,884 - INFO - joeynmt.training - Epoch  14, Step:   153800, Batch Loss:     1.714917, Tokens per Sec:    16130, Lr: 0.000300\n",
      "2021-08-03 10:40:50,871 - INFO - joeynmt.training - Epoch  14, Step:   154000, Batch Loss:     1.658436, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-03 10:41:18,331 - INFO - joeynmt.training - Epoch  14, Step:   154200, Batch Loss:     1.745230, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-08-03 10:41:45,467 - INFO - joeynmt.training - Epoch  14, Step:   154400, Batch Loss:     1.756226, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-08-03 10:42:12,886 - INFO - joeynmt.training - Epoch  14, Step:   154600, Batch Loss:     2.036666, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-03 10:42:40,083 - INFO - joeynmt.training - Epoch  14, Step:   154800, Batch Loss:     1.825491, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-08-03 10:43:07,087 - INFO - joeynmt.training - Epoch  14, Step:   155000, Batch Loss:     1.848567, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-03 10:44:40,046 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:44:40,046 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:44:40,047 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:44:41,268 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:44:41,268 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:44:42,015 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:44:42,016 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:44:42,016 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:44:42,016 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who is approaching God , is good for me . ”\n",
      "2021-08-03 10:44:42,016 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:44:42,017 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:44:42,017 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:44:42,017 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , greater riches are standing . ”\n",
      "2021-08-03 10:44:42,017 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:44:42,017 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:44:42,018 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:44:42,018 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel describes God’s Kingdom .\n",
      "2021-08-03 10:44:42,018 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:44:42,019 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:44:42,019 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:44:42,019 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and conquering it ?\n",
      "2021-08-03 10:44:42,019 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   155000: bleu:  25.83, loss: 203173.8594, ppl:   5.1415, duration: 94.9324s\n",
      "2021-08-03 10:45:09,867 - INFO - joeynmt.training - Epoch  14, Step:   155200, Batch Loss:     1.874277, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-03 10:45:36,987 - INFO - joeynmt.training - Epoch  14, Step:   155400, Batch Loss:     2.307511, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-08-03 10:46:04,449 - INFO - joeynmt.training - Epoch  14, Step:   155600, Batch Loss:     1.729067, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-08-03 10:46:31,698 - INFO - joeynmt.training - Epoch  14, Step:   155800, Batch Loss:     1.883139, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-08-03 10:46:58,894 - INFO - joeynmt.training - Epoch  14, Step:   156000, Batch Loss:     1.850942, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-08-03 10:47:26,139 - INFO - joeynmt.training - Epoch  14, Step:   156200, Batch Loss:     1.848297, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-08-03 10:47:53,372 - INFO - joeynmt.training - Epoch  14, Step:   156400, Batch Loss:     1.807541, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-08-03 10:48:20,907 - INFO - joeynmt.training - Epoch  14, Step:   156600, Batch Loss:     1.983645, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-03 10:48:48,199 - INFO - joeynmt.training - Epoch  14, Step:   156800, Batch Loss:     1.684984, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-08-03 10:49:15,612 - INFO - joeynmt.training - Epoch  14, Step:   157000, Batch Loss:     1.914860, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-03 10:49:42,945 - INFO - joeynmt.training - Epoch  14, Step:   157200, Batch Loss:     1.834748, Tokens per Sec:    16218, Lr: 0.000300\n",
      "2021-08-03 10:50:10,595 - INFO - joeynmt.training - Epoch  14, Step:   157400, Batch Loss:     1.744948, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-03 10:50:37,724 - INFO - joeynmt.training - Epoch  14, Step:   157600, Batch Loss:     1.681764, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-03 10:51:04,930 - INFO - joeynmt.training - Epoch  14, Step:   157800, Batch Loss:     1.618047, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-08-03 10:51:32,214 - INFO - joeynmt.training - Epoch  14, Step:   158000, Batch Loss:     1.906761, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-08-03 10:51:59,645 - INFO - joeynmt.training - Epoch  14, Step:   158200, Batch Loss:     1.865352, Tokens per Sec:    16281, Lr: 0.000300\n",
      "2021-08-03 10:52:27,016 - INFO - joeynmt.training - Epoch  14, Step:   158400, Batch Loss:     1.952718, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-08-03 10:52:34,494 - INFO - joeynmt.training - Epoch  14: total training loss 9848.91\n",
      "2021-08-03 10:52:34,495 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-03 10:52:54,617 - INFO - joeynmt.training - Epoch  15, Step:   158600, Batch Loss:     1.905520, Tokens per Sec:    15590, Lr: 0.000300\n",
      "2021-08-03 10:53:21,764 - INFO - joeynmt.training - Epoch  15, Step:   158800, Batch Loss:     1.786114, Tokens per Sec:    15774, Lr: 0.000300\n",
      "2021-08-03 10:53:48,699 - INFO - joeynmt.training - Epoch  15, Step:   159000, Batch Loss:     1.905228, Tokens per Sec:    15937, Lr: 0.000300\n",
      "2021-08-03 10:54:15,777 - INFO - joeynmt.training - Epoch  15, Step:   159200, Batch Loss:     2.532675, Tokens per Sec:    15861, Lr: 0.000300\n",
      "2021-08-03 10:54:43,222 - INFO - joeynmt.training - Epoch  15, Step:   159400, Batch Loss:     1.978456, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-08-03 10:55:10,492 - INFO - joeynmt.training - Epoch  15, Step:   159600, Batch Loss:     1.793761, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-08-03 10:55:37,624 - INFO - joeynmt.training - Epoch  15, Step:   159800, Batch Loss:     1.672985, Tokens per Sec:    16118, Lr: 0.000300\n",
      "2021-08-03 10:56:05,419 - INFO - joeynmt.training - Epoch  15, Step:   160000, Batch Loss:     1.909202, Tokens per Sec:    16270, Lr: 0.000300\n",
      "2021-08-03 10:57:39,351 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:57:39,351 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:57:39,351 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:57:40,582 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:57:40,582 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:57:41,326 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:57:41,327 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 10:57:41,327 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 10:57:41,328 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , so good to me . ”\n",
      "2021-08-03 10:57:41,328 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:57:41,333 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 10:57:41,333 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 10:57:41,333 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-03 10:57:41,333 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:57:41,334 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 10:57:41,334 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 10:57:41,334 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-03 10:57:41,334 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:57:41,335 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 10:57:41,335 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 10:57:41,335 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 10:57:41,335 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   160000: bleu:  26.05, loss: 203095.7188, ppl:   5.1382, duration: 95.9155s\n",
      "2021-08-03 10:58:08,854 - INFO - joeynmt.training - Epoch  15, Step:   160200, Batch Loss:     1.620870, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-08-03 10:58:36,252 - INFO - joeynmt.training - Epoch  15, Step:   160400, Batch Loss:     1.679896, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-08-03 10:59:03,198 - INFO - joeynmt.training - Epoch  15, Step:   160600, Batch Loss:     1.667943, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-03 10:59:30,620 - INFO - joeynmt.training - Epoch  15, Step:   160800, Batch Loss:     1.823159, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-03 10:59:57,985 - INFO - joeynmt.training - Epoch  15, Step:   161000, Batch Loss:     1.927827, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-03 11:00:25,254 - INFO - joeynmt.training - Epoch  15, Step:   161200, Batch Loss:     1.773961, Tokens per Sec:    16303, Lr: 0.000300\n",
      "2021-08-03 11:00:52,833 - INFO - joeynmt.training - Epoch  15, Step:   161400, Batch Loss:     1.675797, Tokens per Sec:    16231, Lr: 0.000300\n",
      "2021-08-03 11:01:20,224 - INFO - joeynmt.training - Epoch  15, Step:   161600, Batch Loss:     1.770880, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-03 11:01:47,388 - INFO - joeynmt.training - Epoch  15, Step:   161800, Batch Loss:     1.668918, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-03 11:02:14,785 - INFO - joeynmt.training - Epoch  15, Step:   162000, Batch Loss:     1.974447, Tokens per Sec:    15942, Lr: 0.000300\n",
      "2021-08-03 11:02:42,060 - INFO - joeynmt.training - Epoch  15, Step:   162200, Batch Loss:     1.958949, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-03 11:03:09,470 - INFO - joeynmt.training - Epoch  15, Step:   162400, Batch Loss:     1.891040, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-03 11:03:36,919 - INFO - joeynmt.training - Epoch  15, Step:   162600, Batch Loss:     1.485674, Tokens per Sec:    16159, Lr: 0.000300\n",
      "2021-08-03 11:04:04,227 - INFO - joeynmt.training - Epoch  15, Step:   162800, Batch Loss:     1.760770, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-03 11:04:31,456 - INFO - joeynmt.training - Epoch  15, Step:   163000, Batch Loss:     1.752956, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-03 11:04:58,921 - INFO - joeynmt.training - Epoch  15, Step:   163200, Batch Loss:     2.165150, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-08-03 11:05:26,020 - INFO - joeynmt.training - Epoch  15, Step:   163400, Batch Loss:     1.771674, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-03 11:05:53,607 - INFO - joeynmt.training - Epoch  15, Step:   163600, Batch Loss:     1.615549, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-03 11:06:20,712 - INFO - joeynmt.training - Epoch  15, Step:   163800, Batch Loss:     1.739958, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-03 11:06:36,252 - INFO - joeynmt.training - Epoch  15: total training loss 9810.97\n",
      "2021-08-03 11:06:36,252 - INFO - joeynmt.training - Training ended after  15 epochs.\n",
      "2021-08-03 11:06:36,252 - INFO - joeynmt.training - Best validation result (greedy) at step   160000:   5.14 ppl.\n",
      "2021-08-03 11:06:36,274 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 10000 (with beam_size)\n",
      "2021-08-03 11:06:36,626 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 11:06:36,818 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 11:06:36,879 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe.en)...\n",
      "2021-08-03 11:08:32,431 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:08:32,431 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:08:32,431 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:08:33,632 - INFO - joeynmt.prediction -  dev bleu[13a]:  26.41 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-03 11:08:33,639 - INFO - joeynmt.prediction - Translations saved to: models/rw_lhen_reverse_transformer_continued/00160000.hyps.dev\n",
      "2021-08-03 11:08:33,640 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.en)...\n",
      "2021-08-03 11:08:36,527 - INFO - joeynmt.prediction - test bleu[13a]:   9.14 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-03 11:08:36,530 - INFO - joeynmt.prediction - Translations saved to: models/rw_lhen_reverse_transformer_continued/00160000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H61HNxAlg_7y"
   },
   "source": [
    "15 epochs complete\n",
    "\n",
    "16.5+14.5 = 30 epochs complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWOIl5BILmt6",
    "outputId": "dbfcc722-7cea-4379-e14e-dc482c1274a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 90000\tLoss: 215451.18750\tPPL: 5.67617\tbleu: 24.52493\tLR: 0.00030000\t*\n",
      "Steps: 95000\tLoss: 213882.78125\tPPL: 5.60488\tbleu: 24.53768\tLR: 0.00030000\t*\n",
      "Steps: 100000\tLoss: 211976.79688\tPPL: 5.51945\tbleu: 24.80353\tLR: 0.00030000\t*\n",
      "Steps: 105000\tLoss: 211389.26562\tPPL: 5.49338\tbleu: 24.80485\tLR: 0.00030000\t*\n",
      "Steps: 110000\tLoss: 210520.31250\tPPL: 5.45504\tbleu: 24.86645\tLR: 0.00030000\t*\n",
      "Steps: 115000\tLoss: 209341.76562\tPPL: 5.40348\tbleu: 25.18014\tLR: 0.00030000\t*\n",
      "Steps: 120000\tLoss: 207900.57812\tPPL: 5.34108\tbleu: 25.23656\tLR: 0.00030000\t*\n",
      "Steps: 125000\tLoss: 207952.76562\tPPL: 5.34333\tbleu: 25.43960\tLR: 0.00030000\t\n",
      "Steps: 130000\tLoss: 207022.95312\tPPL: 5.30344\tbleu: 25.38143\tLR: 0.00030000\t*\n",
      "Steps: 135000\tLoss: 205954.96875\tPPL: 5.25799\tbleu: 25.53875\tLR: 0.00030000\t*\n",
      "Steps: 140000\tLoss: 204856.81250\tPPL: 5.21167\tbleu: 25.71922\tLR: 0.00030000\t*\n",
      "Steps: 145000\tLoss: 204917.95312\tPPL: 5.21423\tbleu: 25.81782\tLR: 0.00030000\t\n",
      "Steps: 150000\tLoss: 203927.31250\tPPL: 5.17277\tbleu: 25.81764\tLR: 0.00030000\t*\n",
      "Steps: 155000\tLoss: 203173.85938\tPPL: 5.14146\tbleu: 25.82641\tLR: 0.00030000\t*\n",
      "Steps: 160000\tLoss: 203095.71875\tPPL: 5.13822\tbleu: 26.04675\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "! cat \"joeynmt/models/rw_lhen_reverse_transformer_continued/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhC8exsX0JDl",
    "outputId": "9a043381-ccdd-4139-a892-7896daaffc2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 19:34:57,393 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 19:35:01,042 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 19:35:01,304 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/rw_lhen_reverse_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/translation.bpe.lh_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMPvti0N1T85",
    "outputId": "8232d5db-3ed0-404c-edc8-60f9bc5f5774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 9.1 40.2/13.7/5.5/2.3 (BP = 1.000 ratio = 1.005 hyp_len = 2023 ref_len = 2013)\n"
     ]
    }
   ],
   "source": [
    "!cat \"translation.bpe.lh_en\" | sacrebleu \"test1.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeJt3YhH2MJL",
    "outputId": "2c79c2a8-0ac3-4ff4-821f-1529fcd52482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 19:35:32,764 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 19:35:35,593 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 19:35:35,849 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/rw_lhen_reverse_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.rw\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/translation.bpe.rw_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4O429tX3FCA",
    "outputId": "b1763ec5-cbe1-435e-b6d9-5e141e895976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
      "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 26.0 59.4/34.6/23.2/16.6 (BP = 0.871 ratio = 0.879 hyp_len = 74877 ref_len = 85182)\n"
     ]
    }
   ],
   "source": [
    "!cat \"translation.bpe.rw_en\" | sacrebleu \"test2.en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tVen4gHg1du"
   },
   "source": [
    "Getting to 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR423oOkggaV"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 160000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued2\"')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7k0I8B06z1zP",
    "outputId": "e8febd22-8c2a-4cf3-8d53-81abc0054979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rw_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued/160000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 2000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rw_lhen_reverse_transformer_continued2\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_rw_lhen_reload2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nn9m-mn3hvyT",
    "outputId": "e248efa3-071e-47f5-c62c-acae45586920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 19:42:24,606 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 19:42:24,633 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 19:42:36,016 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 19:42:36,347 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 19:42:38,343 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 19:42:39,587 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 19:42:39,588 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 19:42:41,188 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 19:42:41.440306: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 19:42:43,583 - INFO - joeynmt.training - Total params: 12177920\n",
      "2021-08-03 19:42:45,817 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued/160000.ckpt\n",
      "2021-08-03 19:42:46,378 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
      "2021-08-03 19:42:46,378 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
      "2021-08-03 19:42:46,379 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 19:42:46,379 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
      "2021-08-03 19:42:46,379 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
      "2021-08-03 19:42:46,379 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
      "2021-08-03 19:42:46,380 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 19:42:46,380 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 19:42:46,380 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 19:42:46,380 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-03 19:42:46,381 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-03 19:42:46,381 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 19:42:46,381 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 19:42:46,381 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued/160000.ckpt\n",
      "2021-08-03 19:42:46,382 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 19:42:46,382 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 19:42:46,382 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 19:42:46,382 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 19:42:46,383 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 19:42:46,383 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 19:42:46,383 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 19:42:46,383 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 19:42:46,384 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 19:42:46,384 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 19:42:46,384 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 19:42:46,384 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 19:42:46,385 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 19:42:46,385 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 19:42:46,385 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 19:42:46,385 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 19:42:46,386 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-03 19:42:46,386 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 19:42:46,386 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 19:42:46,386 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 19:42:46,386 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-03 19:42:46,387 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-03 19:42:46,387 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-03 19:42:46,387 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 19:42:46,388 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued2\n",
      "2021-08-03 19:42:46,388 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 19:42:46,388 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 19:42:46,388 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 19:42:46,389 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 19:42:46,389 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 19:42:46,389 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 19:42:46,389 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 19:42:46,389 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 19:42:46,390 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 19:42:46,390 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 19:42:46,390 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 19:42:46,390 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 19:42:46,391 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 19:42:46,391 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 19:42:46,391 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 19:42:46,391 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 19:42:46,392 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 19:42:46,392 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 19:42:46,392 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 19:42:46,392 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 19:42:46,393 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 19:42:46,393 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 19:42:46,393 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 19:42:46,394 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 19:42:46,394 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 19:42:46,394 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 19:42:46,394 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 19:42:46,395 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 19:42:46,395 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 19:42:46,395 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 19:42:46,395 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 19:42:46,395 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434091,\n",
      "\tvalid 4447,\n",
      "\ttest 79\n",
      "2021-08-03 19:42:46,396 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-03 19:42:46,396 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 19:42:46,396 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 19:42:46,397 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
      "2021-08-03 19:42:46,397 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
      "2021-08-03 19:42:46,397 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
      "2021-08-03 19:42:46,411 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 19:42:46,412 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 19:43:46,766 - INFO - joeynmt.training - Epoch   1, Step:   160200, Batch Loss:     1.629306, Tokens per Sec:     7241, Lr: 0.000300\n",
      "2021-08-03 19:44:46,138 - INFO - joeynmt.training - Epoch   1, Step:   160400, Batch Loss:     1.656518, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-08-03 19:45:44,908 - INFO - joeynmt.training - Epoch   1, Step:   160600, Batch Loss:     1.653802, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-08-03 19:46:44,250 - INFO - joeynmt.training - Epoch   1, Step:   160800, Batch Loss:     1.786873, Tokens per Sec:     7430, Lr: 0.000300\n",
      "2021-08-03 19:47:43,473 - INFO - joeynmt.training - Epoch   1, Step:   161000, Batch Loss:     1.904705, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-03 19:48:42,960 - INFO - joeynmt.training - Epoch   1, Step:   161200, Batch Loss:     1.760989, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-08-03 19:49:42,622 - INFO - joeynmt.training - Epoch   1, Step:   161400, Batch Loss:     1.667018, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-08-03 19:50:42,029 - INFO - joeynmt.training - Epoch   1, Step:   161600, Batch Loss:     1.764372, Tokens per Sec:     7359, Lr: 0.000300\n",
      "2021-08-03 19:51:41,204 - INFO - joeynmt.training - Epoch   1, Step:   161800, Batch Loss:     1.678826, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-03 19:52:40,315 - INFO - joeynmt.training - Epoch   1, Step:   162000, Batch Loss:     1.960056, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-08-03 19:53:39,884 - INFO - joeynmt.training - Epoch   1, Step:   162200, Batch Loss:     1.933304, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-08-03 19:54:39,349 - INFO - joeynmt.training - Epoch   1, Step:   162400, Batch Loss:     1.878590, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-08-03 19:55:38,856 - INFO - joeynmt.training - Epoch   1, Step:   162600, Batch Loss:     1.484774, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-08-03 19:56:38,173 - INFO - joeynmt.training - Epoch   1, Step:   162800, Batch Loss:     1.730665, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 19:57:37,113 - INFO - joeynmt.training - Epoch   1, Step:   163000, Batch Loss:     1.752391, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-08-03 19:58:36,384 - INFO - joeynmt.training - Epoch   1, Step:   163200, Batch Loss:     2.114668, Tokens per Sec:     7339, Lr: 0.000300\n",
      "2021-08-03 19:59:35,275 - INFO - joeynmt.training - Epoch   1, Step:   163400, Batch Loss:     1.764524, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-08-03 20:00:35,177 - INFO - joeynmt.training - Epoch   1, Step:   163600, Batch Loss:     1.615350, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-08-03 20:01:34,065 - INFO - joeynmt.training - Epoch   1, Step:   163800, Batch Loss:     1.747847, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-08-03 20:02:08,042 - INFO - joeynmt.training - Epoch   1: total training loss 7040.90\n",
      "2021-08-03 20:02:08,043 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 20:02:33,595 - INFO - joeynmt.training - Epoch   2, Step:   164000, Batch Loss:     1.491735, Tokens per Sec:     7166, Lr: 0.000300\n",
      "2021-08-03 20:03:32,741 - INFO - joeynmt.training - Epoch   2, Step:   164200, Batch Loss:     1.705850, Tokens per Sec:     7431, Lr: 0.000300\n",
      "2021-08-03 20:04:31,870 - INFO - joeynmt.training - Epoch   2, Step:   164400, Batch Loss:     1.988401, Tokens per Sec:     7355, Lr: 0.000300\n",
      "2021-08-03 20:05:30,646 - INFO - joeynmt.training - Epoch   2, Step:   164600, Batch Loss:     1.802820, Tokens per Sec:     7368, Lr: 0.000300\n",
      "2021-08-03 20:06:30,396 - INFO - joeynmt.training - Epoch   2, Step:   164800, Batch Loss:     1.732754, Tokens per Sec:     7481, Lr: 0.000300\n",
      "2021-08-03 20:07:29,570 - INFO - joeynmt.training - Epoch   2, Step:   165000, Batch Loss:     1.594671, Tokens per Sec:     7335, Lr: 0.000300\n",
      "2021-08-03 20:11:03,506 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 20:11:03,507 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 20:11:03,507 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 20:11:05,034 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 20:11:05,035 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 20:11:06,261 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 20:11:06,263 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 20:11:06,263 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 20:11:06,264 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been drawing close to God , so good for me . ”\n",
      "2021-08-03 20:11:06,264 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 20:11:06,265 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 20:11:06,265 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 20:11:06,265 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , we are more than many treasures that are standing . ”\n",
      "2021-08-03 20:11:06,265 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 20:11:06,266 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 20:11:06,266 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 20:11:06,267 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy of the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-03 20:11:06,267 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 20:11:06,268 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 20:11:06,268 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 20:11:06,268 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 20:11:06,268 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   165000: bleu:  26.00, loss: 202532.9844, ppl:   5.1150, duration: 216.6981s\n",
      "2021-08-03 20:12:06,209 - INFO - joeynmt.training - Epoch   2, Step:   165200, Batch Loss:     1.779334, Tokens per Sec:     7448, Lr: 0.000300\n",
      "2021-08-03 20:13:05,420 - INFO - joeynmt.training - Epoch   2, Step:   165400, Batch Loss:     1.805294, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-03 20:14:04,723 - INFO - joeynmt.training - Epoch   2, Step:   165600, Batch Loss:     1.491545, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-08-03 20:15:04,233 - INFO - joeynmt.training - Epoch   2, Step:   165800, Batch Loss:     1.765547, Tokens per Sec:     7350, Lr: 0.000300\n",
      "2021-08-03 20:16:03,254 - INFO - joeynmt.training - Epoch   2, Step:   166000, Batch Loss:     1.832606, Tokens per Sec:     7363, Lr: 0.000300\n",
      "2021-08-03 20:17:02,422 - INFO - joeynmt.training - Epoch   2, Step:   166200, Batch Loss:     1.865506, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-08-03 20:18:01,242 - INFO - joeynmt.training - Epoch   2, Step:   166400, Batch Loss:     1.692061, Tokens per Sec:     7338, Lr: 0.000300\n",
      "2021-08-03 20:19:00,561 - INFO - joeynmt.training - Epoch   2, Step:   166600, Batch Loss:     1.828997, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-08-03 20:19:59,820 - INFO - joeynmt.training - Epoch   2, Step:   166800, Batch Loss:     1.810856, Tokens per Sec:     7462, Lr: 0.000300\n",
      "2021-08-03 20:20:58,741 - INFO - joeynmt.training - Epoch   2, Step:   167000, Batch Loss:     1.591621, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-08-03 20:21:57,875 - INFO - joeynmt.training - Epoch   2, Step:   167200, Batch Loss:     1.715144, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-08-03 20:22:56,859 - INFO - joeynmt.training - Epoch   2, Step:   167400, Batch Loss:     1.796167, Tokens per Sec:     7356, Lr: 0.000300\n",
      "2021-08-03 20:23:55,876 - INFO - joeynmt.training - Epoch   2, Step:   167600, Batch Loss:     1.704712, Tokens per Sec:     7426, Lr: 0.000300\n",
      "2021-08-03 20:24:55,124 - INFO - joeynmt.training - Epoch   2, Step:   167800, Batch Loss:     1.568672, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-08-03 20:25:54,559 - INFO - joeynmt.training - Epoch   2, Step:   168000, Batch Loss:     1.707942, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-08-03 20:26:54,324 - INFO - joeynmt.training - Epoch   2, Step:   168200, Batch Loss:     1.866550, Tokens per Sec:     7462, Lr: 0.000300\n",
      "2021-08-03 20:27:53,872 - INFO - joeynmt.training - Epoch   2, Step:   168400, Batch Loss:     1.729749, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-08-03 20:28:52,412 - INFO - joeynmt.training - Epoch   2, Step:   168600, Batch Loss:     1.969427, Tokens per Sec:     7319, Lr: 0.000300\n",
      "2021-08-03 20:29:52,047 - INFO - joeynmt.training - Epoch   2, Step:   168800, Batch Loss:     1.914453, Tokens per Sec:     7450, Lr: 0.000300\n",
      "2021-08-03 20:30:51,585 - INFO - joeynmt.training - Epoch   2, Step:   169000, Batch Loss:     1.704405, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-08-03 20:31:50,710 - INFO - joeynmt.training - Epoch   2, Step:   169200, Batch Loss:     2.262300, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-08-03 20:32:41,199 - INFO - joeynmt.training - Epoch   2: total training loss 9774.54\n",
      "2021-08-03 20:32:41,199 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 20:32:50,479 - INFO - joeynmt.training - Epoch   3, Step:   169400, Batch Loss:     1.918482, Tokens per Sec:     6988, Lr: 0.000300\n",
      "2021-08-03 20:33:49,987 - INFO - joeynmt.training - Epoch   3, Step:   169600, Batch Loss:     1.650613, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-08-03 20:34:49,126 - INFO - joeynmt.training - Epoch   3, Step:   169800, Batch Loss:     1.803759, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-08-03 20:35:48,762 - INFO - joeynmt.training - Epoch   3, Step:   170000, Batch Loss:     1.786351, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-08-03 20:39:24,333 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 20:39:24,333 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 20:39:24,333 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 20:39:25,830 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 20:39:25,830 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 20:39:27,017 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 20:39:27,019 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 20:39:27,019 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 20:39:27,020 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , so good for me . ”\n",
      "2021-08-03 20:39:27,020 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 20:39:27,021 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 20:39:27,022 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 20:39:27,022 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , more than many treasures that are standing . ”\n",
      "2021-08-03 20:39:27,022 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 20:39:27,023 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 20:39:27,023 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 20:39:27,023 - INFO - joeynmt.training - \tHypothesis: Cameron : Look ! Another prophecy in the book of Daniel has a proof of of God’s Kingdom .\n",
      "2021-08-03 20:39:27,023 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 20:39:27,024 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 20:39:27,024 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 20:39:27,024 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely resist ?\n",
      "2021-08-03 20:39:27,025 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   170000: bleu:  26.17, loss: 202143.4688, ppl:   5.0989, duration: 218.2620s\n",
      "2021-08-03 20:40:26,608 - INFO - joeynmt.training - Epoch   3, Step:   170200, Batch Loss:     1.792848, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-03 20:41:25,881 - INFO - joeynmt.training - Epoch   3, Step:   170400, Batch Loss:     1.675259, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-08-03 20:42:25,131 - INFO - joeynmt.training - Epoch   3, Step:   170600, Batch Loss:     1.530089, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-08-03 20:43:24,246 - INFO - joeynmt.training - Epoch   3, Step:   170800, Batch Loss:     1.694454, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-08-03 20:44:23,377 - INFO - joeynmt.training - Epoch   3, Step:   171000, Batch Loss:     1.644048, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-08-03 20:45:22,643 - INFO - joeynmt.training - Epoch   3, Step:   171200, Batch Loss:     1.805143, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-08-03 20:46:21,627 - INFO - joeynmt.training - Epoch   3, Step:   171400, Batch Loss:     1.646782, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-08-03 20:47:19,964 - INFO - joeynmt.training - Epoch   3, Step:   171600, Batch Loss:     1.700470, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-08-03 20:48:19,032 - INFO - joeynmt.training - Epoch   3, Step:   171800, Batch Loss:     1.901034, Tokens per Sec:     7543, Lr: 0.000300\n",
      "2021-08-03 20:49:18,153 - INFO - joeynmt.training - Epoch   3, Step:   172000, Batch Loss:     1.631397, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-08-03 20:50:16,779 - INFO - joeynmt.training - Epoch   3, Step:   172200, Batch Loss:     1.896210, Tokens per Sec:     7298, Lr: 0.000300\n",
      "2021-08-03 20:51:15,632 - INFO - joeynmt.training - Epoch   3, Step:   172400, Batch Loss:     1.864177, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-08-03 20:52:14,798 - INFO - joeynmt.training - Epoch   3, Step:   172600, Batch Loss:     1.837373, Tokens per Sec:     7399, Lr: 0.000300\n",
      "2021-08-03 20:53:13,736 - INFO - joeynmt.training - Epoch   3, Step:   172800, Batch Loss:     1.597695, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-03 20:54:12,568 - INFO - joeynmt.training - Epoch   3, Step:   173000, Batch Loss:     1.762127, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-08-03 20:55:11,538 - INFO - joeynmt.training - Epoch   3, Step:   173200, Batch Loss:     1.576370, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-08-03 20:56:10,546 - INFO - joeynmt.training - Epoch   3, Step:   173400, Batch Loss:     1.792175, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 20:57:09,401 - INFO - joeynmt.training - Epoch   3, Step:   173600, Batch Loss:     1.919655, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-08-03 20:58:08,463 - INFO - joeynmt.training - Epoch   3, Step:   173800, Batch Loss:     1.857118, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-08-03 20:59:07,452 - INFO - joeynmt.training - Epoch   3, Step:   174000, Batch Loss:     1.849895, Tokens per Sec:     7431, Lr: 0.000300\n",
      "2021-08-03 21:00:06,899 - INFO - joeynmt.training - Epoch   3, Step:   174200, Batch Loss:     1.746317, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-08-03 21:01:06,407 - INFO - joeynmt.training - Epoch   3, Step:   174400, Batch Loss:     1.825857, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-03 21:02:05,561 - INFO - joeynmt.training - Epoch   3, Step:   174600, Batch Loss:     1.640549, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-08-03 21:03:04,658 - INFO - joeynmt.training - Epoch   3, Step:   174800, Batch Loss:     1.733140, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-08-03 21:03:13,219 - INFO - joeynmt.training - Epoch   3: total training loss 9757.02\n",
      "2021-08-03 21:03:13,220 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 21:04:04,546 - INFO - joeynmt.training - Epoch   4, Step:   175000, Batch Loss:     1.641282, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-08-03 21:07:47,168 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:07:47,168 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:07:47,168 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:07:48,614 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:07:48,615 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:07:49,484 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:07:49,485 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 21:07:49,488 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 21:07:49,488 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , so good for me . ”\n",
      "2021-08-03 21:07:49,489 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:07:49,489 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 21:07:49,490 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 21:07:49,491 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-03 21:07:49,491 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:07:49,492 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 21:07:49,492 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 21:07:49,492 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies in the book of Daniel that describe God’s Kingdom .\n",
      "2021-08-03 21:07:49,493 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:07:49,493 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 21:07:49,493 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 21:07:49,494 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-03 21:07:49,494 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   175000: bleu:  26.29, loss: 200694.0938, ppl:   5.0397, duration: 224.9476s\n",
      "2021-08-03 21:08:48,403 - INFO - joeynmt.training - Epoch   4, Step:   175200, Batch Loss:     1.808549, Tokens per Sec:     7283, Lr: 0.000300\n",
      "2021-08-03 21:09:47,946 - INFO - joeynmt.training - Epoch   4, Step:   175400, Batch Loss:     1.702976, Tokens per Sec:     7570, Lr: 0.000300\n",
      "2021-08-03 21:10:47,044 - INFO - joeynmt.training - Epoch   4, Step:   175600, Batch Loss:     2.273512, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-08-03 21:11:46,509 - INFO - joeynmt.training - Epoch   4, Step:   175800, Batch Loss:     1.899466, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-03 21:12:45,766 - INFO - joeynmt.training - Epoch   4, Step:   176000, Batch Loss:     1.840900, Tokens per Sec:     7356, Lr: 0.000300\n",
      "2021-08-03 21:13:44,745 - INFO - joeynmt.training - Epoch   4, Step:   176200, Batch Loss:     1.798396, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-08-03 21:14:44,443 - INFO - joeynmt.training - Epoch   4, Step:   176400, Batch Loss:     1.746714, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-08-03 21:15:43,887 - INFO - joeynmt.training - Epoch   4, Step:   176600, Batch Loss:     1.768006, Tokens per Sec:     7392, Lr: 0.000300\n",
      "2021-08-03 21:16:43,038 - INFO - joeynmt.training - Epoch   4, Step:   176800, Batch Loss:     2.225897, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-08-03 21:17:41,379 - INFO - joeynmt.training - Epoch   4, Step:   177000, Batch Loss:     1.733261, Tokens per Sec:     7288, Lr: 0.000300\n",
      "2021-08-03 21:18:40,645 - INFO - joeynmt.training - Epoch   4, Step:   177200, Batch Loss:     1.713997, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-08-03 21:19:39,795 - INFO - joeynmt.training - Epoch   4, Step:   177400, Batch Loss:     1.817725, Tokens per Sec:     7488, Lr: 0.000300\n",
      "2021-08-03 21:20:38,607 - INFO - joeynmt.training - Epoch   4, Step:   177600, Batch Loss:     1.964893, Tokens per Sec:     7318, Lr: 0.000300\n",
      "2021-08-03 21:21:37,737 - INFO - joeynmt.training - Epoch   4, Step:   177800, Batch Loss:     1.676536, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-03 21:22:36,792 - INFO - joeynmt.training - Epoch   4, Step:   178000, Batch Loss:     1.652616, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-08-03 21:23:36,398 - INFO - joeynmt.training - Epoch   4, Step:   178200, Batch Loss:     1.692479, Tokens per Sec:     7484, Lr: 0.000300\n",
      "2021-08-03 21:24:35,017 - INFO - joeynmt.training - Epoch   4, Step:   178400, Batch Loss:     2.904007, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-03 21:25:34,852 - INFO - joeynmt.training - Epoch   4, Step:   178600, Batch Loss:     1.850027, Tokens per Sec:     7603, Lr: 0.000300\n",
      "2021-08-03 21:26:33,649 - INFO - joeynmt.training - Epoch   4, Step:   178800, Batch Loss:     1.630099, Tokens per Sec:     7275, Lr: 0.000300\n",
      "2021-08-03 21:27:32,961 - INFO - joeynmt.training - Epoch   4, Step:   179000, Batch Loss:     1.831275, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-08-03 21:28:32,215 - INFO - joeynmt.training - Epoch   4, Step:   179200, Batch Loss:     1.497391, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-08-03 21:29:31,992 - INFO - joeynmt.training - Epoch   4, Step:   179400, Batch Loss:     1.861131, Tokens per Sec:     7462, Lr: 0.000300\n",
      "2021-08-03 21:30:31,082 - INFO - joeynmt.training - Epoch   4, Step:   179600, Batch Loss:     1.594069, Tokens per Sec:     7311, Lr: 0.000300\n",
      "2021-08-03 21:31:30,342 - INFO - joeynmt.training - Epoch   4, Step:   179800, Batch Loss:     1.938872, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-08-03 21:32:29,983 - INFO - joeynmt.training - Epoch   4, Step:   180000, Batch Loss:     1.752781, Tokens per Sec:     7600, Lr: 0.000300\n",
      "2021-08-03 21:36:11,053 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:36:11,053 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:36:11,053 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:36:13,439 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:36:13,440 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 21:36:13,440 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 21:36:13,441 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approaches God , is good for me . ”\n",
      "2021-08-03 21:36:13,441 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:36:13,441 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 21:36:13,442 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 21:36:13,442 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , more than many treasures that are standing . ”\n",
      "2021-08-03 21:36:13,442 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:36:13,443 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 21:36:13,443 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 21:36:13,443 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies in the book of Daniel that describe God’s Kingdom .\n",
      "2021-08-03 21:36:13,444 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:36:13,444 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 21:36:13,445 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 21:36:13,445 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and conquering completely ?\n",
      "2021-08-03 21:36:13,445 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   180000: bleu:  26.13, loss: 201159.8906, ppl:   5.0587, duration: 223.4615s\n",
      "2021-08-03 21:37:12,113 - INFO - joeynmt.training - Epoch   4, Step:   180200, Batch Loss:     1.692120, Tokens per Sec:     7352, Lr: 0.000300\n",
      "2021-08-03 21:37:35,749 - INFO - joeynmt.training - Epoch   4: total training loss 9699.59\n",
      "2021-08-03 21:37:35,750 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-03 21:38:12,205 - INFO - joeynmt.training - Epoch   5, Step:   180400, Batch Loss:     1.919240, Tokens per Sec:     7275, Lr: 0.000300\n",
      "2021-08-03 21:39:11,027 - INFO - joeynmt.training - Epoch   5, Step:   180600, Batch Loss:     1.633139, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-03 21:40:10,188 - INFO - joeynmt.training - Epoch   5, Step:   180800, Batch Loss:     1.735166, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-08-03 21:41:09,369 - INFO - joeynmt.training - Epoch   5, Step:   181000, Batch Loss:     1.722695, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-08-03 21:42:09,109 - INFO - joeynmt.training - Epoch   5, Step:   181200, Batch Loss:     1.700855, Tokens per Sec:     7528, Lr: 0.000300\n",
      "2021-08-03 21:43:08,319 - INFO - joeynmt.training - Epoch   5, Step:   181400, Batch Loss:     1.610959, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-08-03 21:44:07,388 - INFO - joeynmt.training - Epoch   5, Step:   181600, Batch Loss:     1.808749, Tokens per Sec:     7387, Lr: 0.000300\n",
      "2021-08-03 21:45:06,569 - INFO - joeynmt.training - Epoch   5, Step:   181800, Batch Loss:     1.833387, Tokens per Sec:     7516, Lr: 0.000300\n",
      "2021-08-03 21:46:05,788 - INFO - joeynmt.training - Epoch   5, Step:   182000, Batch Loss:     2.234019, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-08-03 21:47:04,819 - INFO - joeynmt.training - Epoch   5, Step:   182200, Batch Loss:     1.760293, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-08-03 21:48:04,016 - INFO - joeynmt.training - Epoch   5, Step:   182400, Batch Loss:     1.756474, Tokens per Sec:     7443, Lr: 0.000300\n",
      "2021-08-03 21:49:03,060 - INFO - joeynmt.training - Epoch   5, Step:   182600, Batch Loss:     1.896972, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-08-03 21:50:02,249 - INFO - joeynmt.training - Epoch   5, Step:   182800, Batch Loss:     1.666064, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-03 21:51:01,500 - INFO - joeynmt.training - Epoch   5, Step:   183000, Batch Loss:     1.801070, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-08-03 21:52:00,343 - INFO - joeynmt.training - Epoch   5, Step:   183200, Batch Loss:     1.577204, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-08-03 21:53:00,008 - INFO - joeynmt.training - Epoch   5, Step:   183400, Batch Loss:     1.728938, Tokens per Sec:     7489, Lr: 0.000300\n",
      "2021-08-03 21:53:59,424 - INFO - joeynmt.training - Epoch   5, Step:   183600, Batch Loss:     1.755129, Tokens per Sec:     7369, Lr: 0.000300\n",
      "2021-08-03 21:54:58,185 - INFO - joeynmt.training - Epoch   5, Step:   183800, Batch Loss:     1.775694, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-08-03 21:55:57,259 - INFO - joeynmt.training - Epoch   5, Step:   184000, Batch Loss:     1.653348, Tokens per Sec:     7470, Lr: 0.000300\n",
      "2021-08-03 21:56:56,162 - INFO - joeynmt.training - Epoch   5, Step:   184200, Batch Loss:     1.829100, Tokens per Sec:     7343, Lr: 0.000300\n",
      "2021-08-03 21:57:55,340 - INFO - joeynmt.training - Epoch   5, Step:   184400, Batch Loss:     1.890805, Tokens per Sec:     7356, Lr: 0.000300\n",
      "2021-08-03 21:58:54,475 - INFO - joeynmt.training - Epoch   5, Step:   184600, Batch Loss:     1.737382, Tokens per Sec:     7296, Lr: 0.000300\n",
      "2021-08-03 21:59:53,250 - INFO - joeynmt.training - Epoch   5, Step:   184800, Batch Loss:     1.655722, Tokens per Sec:     7261, Lr: 0.000300\n",
      "2021-08-03 22:00:52,921 - INFO - joeynmt.training - Epoch   5, Step:   185000, Batch Loss:     1.799832, Tokens per Sec:     7403, Lr: 0.000300\n",
      "2021-08-03 22:04:35,121 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:04:35,122 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:04:35,122 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:04:36,703 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 22:04:36,703 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 22:04:37,711 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:04:37,712 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 22:04:37,713 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 22:04:37,713 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I am approaching God , so good for me . ”\n",
      "2021-08-03 22:04:37,713 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:04:37,714 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 22:04:37,714 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 22:04:37,714 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , more than many riches that are standing . ”\n",
      "2021-08-03 22:04:37,715 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:04:37,715 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 22:04:37,716 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 22:04:37,716 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-03 22:04:37,716 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:04:37,717 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 22:04:37,717 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 22:04:37,717 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 22:04:37,718 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   185000: bleu:  26.54, loss: 199862.5000, ppl:   5.0061, duration: 224.7966s\n",
      "2021-08-03 22:05:37,452 - INFO - joeynmt.training - Epoch   5, Step:   185200, Batch Loss:     1.751849, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-08-03 22:06:36,761 - INFO - joeynmt.training - Epoch   5, Step:   185400, Batch Loss:     1.971016, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-08-03 22:07:35,538 - INFO - joeynmt.training - Epoch   5, Step:   185600, Batch Loss:     1.591082, Tokens per Sec:     7351, Lr: 0.000300\n",
      "2021-08-03 22:08:16,880 - INFO - joeynmt.training - Epoch   5: total training loss 9700.65\n",
      "2021-08-03 22:08:16,880 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-03 22:08:35,699 - INFO - joeynmt.training - Epoch   6, Step:   185800, Batch Loss:     1.740731, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-08-03 22:09:35,283 - INFO - joeynmt.training - Epoch   6, Step:   186000, Batch Loss:     1.821316, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-08-03 22:10:34,630 - INFO - joeynmt.training - Epoch   6, Step:   186200, Batch Loss:     1.848718, Tokens per Sec:     7258, Lr: 0.000300\n",
      "2021-08-03 22:11:34,301 - INFO - joeynmt.training - Epoch   6, Step:   186400, Batch Loss:     1.657418, Tokens per Sec:     7458, Lr: 0.000300\n",
      "2021-08-03 22:12:33,533 - INFO - joeynmt.training - Epoch   6, Step:   186600, Batch Loss:     1.596058, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-03 22:13:32,418 - INFO - joeynmt.training - Epoch   6, Step:   186800, Batch Loss:     1.741949, Tokens per Sec:     7329, Lr: 0.000300\n",
      "2021-08-03 22:14:31,411 - INFO - joeynmt.training - Epoch   6, Step:   187000, Batch Loss:     1.594926, Tokens per Sec:     7337, Lr: 0.000300\n",
      "2021-08-03 22:15:30,654 - INFO - joeynmt.training - Epoch   6, Step:   187200, Batch Loss:     1.723787, Tokens per Sec:     7447, Lr: 0.000300\n",
      "2021-08-03 22:16:30,424 - INFO - joeynmt.training - Epoch   6, Step:   187400, Batch Loss:     1.916931, Tokens per Sec:     7538, Lr: 0.000300\n",
      "2021-08-03 22:17:29,126 - INFO - joeynmt.training - Epoch   6, Step:   187600, Batch Loss:     1.847746, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-08-03 22:18:28,440 - INFO - joeynmt.training - Epoch   6, Step:   187800, Batch Loss:     1.784114, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-08-03 22:19:27,595 - INFO - joeynmt.training - Epoch   6, Step:   188000, Batch Loss:     1.674976, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-08-03 22:20:26,699 - INFO - joeynmt.training - Epoch   6, Step:   188200, Batch Loss:     1.623895, Tokens per Sec:     7387, Lr: 0.000300\n",
      "2021-08-03 22:21:26,407 - INFO - joeynmt.training - Epoch   6, Step:   188400, Batch Loss:     1.936019, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-08-03 22:22:25,199 - INFO - joeynmt.training - Epoch   6, Step:   188600, Batch Loss:     2.028196, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-08-03 22:23:24,200 - INFO - joeynmt.training - Epoch   6, Step:   188800, Batch Loss:     1.776428, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-08-03 22:24:23,096 - INFO - joeynmt.training - Epoch   6, Step:   189000, Batch Loss:     1.679299, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-08-03 22:25:22,011 - INFO - joeynmt.training - Epoch   6, Step:   189200, Batch Loss:     1.576689, Tokens per Sec:     7403, Lr: 0.000300\n",
      "2021-08-03 22:26:21,305 - INFO - joeynmt.training - Epoch   6, Step:   189400, Batch Loss:     1.947896, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-08-03 22:27:20,710 - INFO - joeynmt.training - Epoch   6, Step:   189600, Batch Loss:     1.927977, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-08-03 22:28:19,825 - INFO - joeynmt.training - Epoch   6, Step:   189800, Batch Loss:     1.667136, Tokens per Sec:     7412, Lr: 0.000300\n",
      "2021-08-03 22:29:19,170 - INFO - joeynmt.training - Epoch   6, Step:   190000, Batch Loss:     1.593843, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-08-03 22:33:00,704 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:33:00,704 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:33:00,705 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:33:02,220 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 22:33:02,221 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 22:33:03,115 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:33:03,116 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 22:33:03,116 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 22:33:03,117 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so good for me . ”\n",
      "2021-08-03 22:33:03,117 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:33:03,118 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 22:33:03,119 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 22:33:03,119 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-03 22:33:03,120 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:33:03,120 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 22:33:03,121 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 22:33:03,121 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-03 22:33:03,121 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:33:03,122 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 22:33:03,122 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 22:33:03,122 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-03 22:33:03,122 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   190000: bleu:  26.63, loss: 199795.1250, ppl:   5.0034, duration: 223.9518s\n",
      "2021-08-03 22:34:02,517 - INFO - joeynmt.training - Epoch   6, Step:   190200, Batch Loss:     1.709311, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-08-03 22:35:01,699 - INFO - joeynmt.training - Epoch   6, Step:   190400, Batch Loss:     1.896854, Tokens per Sec:     7394, Lr: 0.000300\n",
      "2021-08-03 22:36:00,817 - INFO - joeynmt.training - Epoch   6, Step:   190600, Batch Loss:     1.735380, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-08-03 22:37:00,046 - INFO - joeynmt.training - Epoch   6, Step:   190800, Batch Loss:     1.759637, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-08-03 22:37:59,072 - INFO - joeynmt.training - Epoch   6, Step:   191000, Batch Loss:     1.844337, Tokens per Sec:     7392, Lr: 0.000300\n",
      "2021-08-03 22:38:57,298 - INFO - joeynmt.training - Epoch   6: total training loss 9680.45\n",
      "2021-08-03 22:38:57,298 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-03 22:38:58,626 - INFO - joeynmt.training - Epoch   7, Step:   191200, Batch Loss:     1.766503, Tokens per Sec:     4090, Lr: 0.000300\n",
      "2021-08-03 22:39:57,723 - INFO - joeynmt.training - Epoch   7, Step:   191400, Batch Loss:     1.488093, Tokens per Sec:     7352, Lr: 0.000300\n",
      "2021-08-03 22:40:57,193 - INFO - joeynmt.training - Epoch   7, Step:   191600, Batch Loss:     1.638128, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 22:41:56,577 - INFO - joeynmt.training - Epoch   7, Step:   191800, Batch Loss:     1.635004, Tokens per Sec:     7369, Lr: 0.000300\n",
      "2021-08-03 22:42:55,422 - INFO - joeynmt.training - Epoch   7, Step:   192000, Batch Loss:     1.753710, Tokens per Sec:     7278, Lr: 0.000300\n",
      "2021-08-03 22:43:55,045 - INFO - joeynmt.training - Epoch   7, Step:   192200, Batch Loss:     1.965909, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-08-03 22:44:54,498 - INFO - joeynmt.training - Epoch   7, Step:   192400, Batch Loss:     1.581017, Tokens per Sec:     7323, Lr: 0.000300\n",
      "2021-08-03 22:45:53,828 - INFO - joeynmt.training - Epoch   7, Step:   192600, Batch Loss:     1.874354, Tokens per Sec:     7418, Lr: 0.000300\n",
      "2021-08-03 22:46:52,581 - INFO - joeynmt.training - Epoch   7, Step:   192800, Batch Loss:     1.565784, Tokens per Sec:     7349, Lr: 0.000300\n",
      "2021-08-03 22:47:51,436 - INFO - joeynmt.training - Epoch   7, Step:   193000, Batch Loss:     2.170204, Tokens per Sec:     7342, Lr: 0.000300\n",
      "2021-08-03 22:48:50,578 - INFO - joeynmt.training - Epoch   7, Step:   193200, Batch Loss:     1.594133, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-03 22:49:50,375 - INFO - joeynmt.training - Epoch   7, Step:   193400, Batch Loss:     1.772122, Tokens per Sec:     7541, Lr: 0.000300\n",
      "2021-08-03 22:50:49,427 - INFO - joeynmt.training - Epoch   7, Step:   193600, Batch Loss:     1.644634, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-08-03 22:51:48,863 - INFO - joeynmt.training - Epoch   7, Step:   193800, Batch Loss:     1.762790, Tokens per Sec:     7449, Lr: 0.000300\n",
      "2021-08-03 22:52:48,076 - INFO - joeynmt.training - Epoch   7, Step:   194000, Batch Loss:     1.782062, Tokens per Sec:     7313, Lr: 0.000300\n",
      "2021-08-03 22:53:47,568 - INFO - joeynmt.training - Epoch   7, Step:   194200, Batch Loss:     1.733304, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-08-03 22:54:47,629 - INFO - joeynmt.training - Epoch   7, Step:   194400, Batch Loss:     2.189469, Tokens per Sec:     7363, Lr: 0.000300\n",
      "2021-08-03 22:55:46,653 - INFO - joeynmt.training - Epoch   7, Step:   194600, Batch Loss:     1.640013, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-08-03 22:56:46,483 - INFO - joeynmt.training - Epoch   7, Step:   194800, Batch Loss:     1.955368, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-08-03 22:57:46,110 - INFO - joeynmt.training - Epoch   7, Step:   195000, Batch Loss:     1.860956, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-08-03 23:01:24,192 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 23:01:24,193 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 23:01:24,193 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 23:01:25,724 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 23:01:25,725 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 23:01:26,638 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 23:01:26,639 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-03 23:01:26,639 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-03 23:01:26,639 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so good for me . ”\n",
      "2021-08-03 23:01:26,639 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 23:01:26,640 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-03 23:01:26,640 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-03 23:01:26,641 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-03 23:01:26,641 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 23:01:26,642 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-03 23:01:26,642 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-03 23:01:26,642 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel .\n",
      "2021-08-03 23:01:26,642 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 23:01:26,643 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-03 23:01:26,643 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-03 23:01:26,643 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep on conquering ” and completely resist ?\n",
      "2021-08-03 23:01:26,644 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   195000: bleu:  26.38, loss: 199478.5625, ppl:   4.9906, duration: 220.5329s\n",
      "2021-08-03 23:02:26,005 - INFO - joeynmt.training - Epoch   7, Step:   195200, Batch Loss:     1.761645, Tokens per Sec:     7403, Lr: 0.000300\n",
      "2021-08-03 23:03:25,518 - INFO - joeynmt.training - Epoch   7, Step:   195400, Batch Loss:     1.800754, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-08-03 23:04:24,662 - INFO - joeynmt.training - Epoch   7, Step:   195600, Batch Loss:     1.828593, Tokens per Sec:     7394, Lr: 0.000300\n",
      "2021-08-03 23:05:23,678 - INFO - joeynmt.training - Epoch   7, Step:   195800, Batch Loss:     1.671997, Tokens per Sec:     7380, Lr: 0.000300\n",
      "2021-08-03 23:06:23,032 - INFO - joeynmt.training - Epoch   7, Step:   196000, Batch Loss:     1.832520, Tokens per Sec:     7276, Lr: 0.000300\n",
      "2021-08-03 23:07:22,116 - INFO - joeynmt.training - Epoch   7, Step:   196200, Batch Loss:     1.284702, Tokens per Sec:     7346, Lr: 0.000300\n",
      "2021-08-03 23:08:21,147 - INFO - joeynmt.training - Epoch   7, Step:   196400, Batch Loss:     1.726214, Tokens per Sec:     7339, Lr: 0.000300\n",
      "2021-08-03 23:09:20,638 - INFO - joeynmt.training - Epoch   7, Step:   196600, Batch Loss:     1.827381, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-08-03 23:09:38,544 - INFO - joeynmt.training - Epoch   7: total training loss 9662.22\n",
      "2021-08-03 23:09:38,545 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-03 23:10:20,616 - INFO - joeynmt.training - Epoch   8, Step:   196800, Batch Loss:     1.588948, Tokens per Sec:     7213, Lr: 0.000300\n",
      "2021-08-03 23:11:19,872 - INFO - joeynmt.training - Epoch   8, Step:   197000, Batch Loss:     1.477106, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-08-03 23:12:19,151 - INFO - joeynmt.training - Epoch   8, Step:   197200, Batch Loss:     1.869105, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-08-03 23:13:18,994 - INFO - joeynmt.training - Epoch   8, Step:   197400, Batch Loss:     1.492328, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 23:14:18,560 - INFO - joeynmt.training - Epoch   8, Step:   197600, Batch Loss:     1.886586, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-08-03 23:15:18,040 - INFO - joeynmt.training - Epoch   8, Step:   197800, Batch Loss:     1.718697, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-03 23:16:16,508 - INFO - joeynmt.training - Epoch   8, Step:   198000, Batch Loss:     1.730808, Tokens per Sec:     7301, Lr: 0.000300\n",
      "2021-08-03 23:17:15,922 - INFO - joeynmt.training - Epoch   8, Step:   198200, Batch Loss:     2.552471, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-08-03 23:18:15,347 - INFO - joeynmt.training - Epoch   8, Step:   198400, Batch Loss:     1.745652, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-08-03 23:19:14,457 - INFO - joeynmt.training - Epoch   8, Step:   198600, Batch Loss:     1.833338, Tokens per Sec:     7326, Lr: 0.000300\n",
      "2021-08-03 23:20:14,045 - INFO - joeynmt.training - Epoch   8, Step:   198800, Batch Loss:     1.735539, Tokens per Sec:     7421, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1dXyGT2aHV0",
    "outputId": "28815af3-4390-47d6-e150-5b5c3538c989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 165000\tLoss: 202532.98438\tPPL: 5.11497\tbleu: 26.00079\tLR: 0.00030000\t*\n",
      "Steps: 170000\tLoss: 202143.46875\tPPL: 5.09894\tbleu: 26.17390\tLR: 0.00030000\t*\n",
      "Steps: 175000\tLoss: 200694.09375\tPPL: 5.03973\tbleu: 26.29414\tLR: 0.00030000\t*\n",
      "Steps: 180000\tLoss: 201159.89062\tPPL: 5.05869\tbleu: 26.12599\tLR: 0.00030000\t\n",
      "Steps: 185000\tLoss: 199862.50000\tPPL: 5.00607\tbleu: 26.53683\tLR: 0.00030000\t*\n",
      "Steps: 190000\tLoss: 199795.12500\tPPL: 5.00335\tbleu: 26.62820\tLR: 0.00030000\t*\n",
      "Steps: 195000\tLoss: 199478.56250\tPPL: 4.99061\tbleu: 26.38397\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "! cat \"joeynmt/models/rw_lhen_reverse_transformer_continued2/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gESx7_Whvk7"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 195000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued2/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued3\"').replace(\n",
    "            f'epochs: 30', f'epochs: 23')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sR3yGxSsznZu",
    "outputId": "0365af06-95a5-4ccf-e341-37df9fd6bc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rw_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued2/195000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 2000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 23                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rw_lhen_reverse_transformer_continued3\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_{name}_reload3.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdlsoez5znSy",
    "outputId": "2994de16-f3d7-4872-f7c8-f2dc2f49d754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 23:49:04,547 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 23:49:04,609 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 23:49:14,169 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 23:49:14,838 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 23:49:16,142 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 23:49:17,282 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 23:49:17,282 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 23:49:18,889 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 23:49:19.144075: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 23:49:20,599 - INFO - joeynmt.training - Total params: 12177920\n",
      "2021-08-03 23:49:30,894 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued2/195000.ckpt\n",
      "2021-08-03 23:49:31,362 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
      "2021-08-03 23:49:31,367 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
      "2021-08-03 23:49:31,367 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 23:49:31,367 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
      "2021-08-03 23:49:31,367 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
      "2021-08-03 23:49:31,367 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
      "2021-08-03 23:49:31,367 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 23:49:31,367 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 23:49:31,368 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 23:49:31,368 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-03 23:49:31,368 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-03 23:49:31,368 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 23:49:31,368 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 23:49:31,368 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued2/195000.ckpt\n",
      "2021-08-03 23:49:31,368 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 23:49:31,369 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 23:49:31,369 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 23:49:31,369 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 23:49:31,369 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 23:49:31,369 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 23:49:31,369 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 23:49:31,369 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 23:49:31,369 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 23:49:31,370 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 23:49:31,370 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 23:49:31,370 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 23:49:31,370 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 23:49:31,370 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 23:49:31,370 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 23:49:31,370 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 23:49:31,370 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-03 23:49:31,371 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 23:49:31,371 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 23:49:31,371 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 23:49:31,371 - INFO - joeynmt.helpers - cfg.training.epochs                : 23\n",
      "2021-08-03 23:49:31,372 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-03 23:49:31,372 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-03 23:49:31,372 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 23:49:31,373 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued3\n",
      "2021-08-03 23:49:31,373 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 23:49:31,373 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 23:49:31,373 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 23:49:31,373 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 23:49:31,373 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 23:49:31,373 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 23:49:31,373 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 23:49:31,374 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 23:49:31,374 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 23:49:31,374 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 23:49:31,374 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 23:49:31,374 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 23:49:31,374 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 23:49:31,374 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 23:49:31,375 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 23:49:31,375 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 23:49:31,375 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 23:49:31,375 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 23:49:31,375 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 23:49:31,375 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 23:49:31,375 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 23:49:31,375 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 23:49:31,376 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 23:49:31,376 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 23:49:31,376 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 23:49:31,376 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 23:49:31,376 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 23:49:31,376 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 23:49:31,376 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 23:49:31,376 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 23:49:31,377 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 23:49:31,377 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434091,\n",
      "\tvalid 4447,\n",
      "\ttest 79\n",
      "2021-08-03 23:49:31,377 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-03 23:49:31,377 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 23:49:31,377 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 23:49:31,377 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
      "2021-08-03 23:49:31,378 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
      "2021-08-03 23:49:31,378 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
      "2021-08-03 23:49:31,388 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 23:49:31,388 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 23:49:59,287 - INFO - joeynmt.training - Epoch   1, Step:   195200, Batch Loss:     1.763539, Tokens per Sec:    15752, Lr: 0.000300\n",
      "2021-08-03 23:50:25,597 - INFO - joeynmt.training - Epoch   1, Step:   195400, Batch Loss:     1.811502, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-08-03 23:50:51,948 - INFO - joeynmt.training - Epoch   1, Step:   195600, Batch Loss:     1.830405, Tokens per Sec:    16596, Lr: 0.000300\n",
      "2021-08-03 23:51:18,760 - INFO - joeynmt.training - Epoch   1, Step:   195800, Batch Loss:     1.682908, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-03 23:51:45,772 - INFO - joeynmt.training - Epoch   1, Step:   196000, Batch Loss:     1.850635, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-08-03 23:52:12,774 - INFO - joeynmt.training - Epoch   1, Step:   196200, Batch Loss:     1.305338, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-03 23:52:39,897 - INFO - joeynmt.training - Epoch   1, Step:   196400, Batch Loss:     1.711918, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-08-03 23:53:07,587 - INFO - joeynmt.training - Epoch   1, Step:   196600, Batch Loss:     1.855348, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-03 23:53:15,870 - INFO - joeynmt.training - Epoch   1: total training loss 2942.06\n",
      "2021-08-03 23:53:15,871 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 23:53:35,665 - INFO - joeynmt.training - Epoch   2, Step:   196800, Batch Loss:     1.583383, Tokens per Sec:    15331, Lr: 0.000300\n",
      "2021-08-03 23:54:03,325 - INFO - joeynmt.training - Epoch   2, Step:   197000, Batch Loss:     1.524417, Tokens per Sec:    15726, Lr: 0.000300\n",
      "2021-08-03 23:54:30,859 - INFO - joeynmt.training - Epoch   2, Step:   197200, Batch Loss:     1.889367, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-03 23:54:58,747 - INFO - joeynmt.training - Epoch   2, Step:   197400, Batch Loss:     1.503594, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-03 23:55:26,561 - INFO - joeynmt.training - Epoch   2, Step:   197600, Batch Loss:     1.892799, Tokens per Sec:    15859, Lr: 0.000300\n",
      "2021-08-03 23:55:54,313 - INFO - joeynmt.training - Epoch   2, Step:   197800, Batch Loss:     1.695908, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-03 23:56:21,851 - INFO - joeynmt.training - Epoch   2, Step:   198000, Batch Loss:     1.741473, Tokens per Sec:    15501, Lr: 0.000300\n",
      "2021-08-03 23:56:49,615 - INFO - joeynmt.training - Epoch   2, Step:   198200, Batch Loss:     2.507536, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-08-03 23:57:17,519 - INFO - joeynmt.training - Epoch   2, Step:   198400, Batch Loss:     1.743150, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-08-03 23:57:45,321 - INFO - joeynmt.training - Epoch   2, Step:   198600, Batch Loss:     1.818597, Tokens per Sec:    15575, Lr: 0.000300\n",
      "2021-08-03 23:58:13,375 - INFO - joeynmt.training - Epoch   2, Step:   198800, Batch Loss:     1.737945, Tokens per Sec:    15762, Lr: 0.000300\n",
      "2021-08-03 23:58:41,085 - INFO - joeynmt.training - Epoch   2, Step:   199000, Batch Loss:     1.727636, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-03 23:59:09,391 - INFO - joeynmt.training - Epoch   2, Step:   199200, Batch Loss:     1.764153, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-08-03 23:59:37,253 - INFO - joeynmt.training - Epoch   2, Step:   199400, Batch Loss:     1.750316, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-08-04 00:00:05,097 - INFO - joeynmt.training - Epoch   2, Step:   199600, Batch Loss:     1.974204, Tokens per Sec:    15658, Lr: 0.000300\n",
      "2021-08-04 00:00:32,916 - INFO - joeynmt.training - Epoch   2, Step:   199800, Batch Loss:     1.673595, Tokens per Sec:    15604, Lr: 0.000300\n",
      "2021-08-04 00:01:00,830 - INFO - joeynmt.training - Epoch   2, Step:   200000, Batch Loss:     1.661489, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-08-04 00:02:36,792 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:02:36,792 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:02:36,793 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:02:38,107 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:02:38,107 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:02:39,249 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:02:39,250 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 00:02:39,250 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 00:02:39,250 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been drawing close to God , so good for me . ”\n",
      "2021-08-04 00:02:39,250 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:02:39,251 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 00:02:39,251 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 00:02:39,251 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-04 00:02:39,251 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:02:39,252 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 00:02:39,252 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 00:02:39,252 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 00:02:39,252 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:02:39,252 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 00:02:39,253 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 00:02:39,253 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 00:02:39,253 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   200000: bleu:  26.66, loss: 198443.6719, ppl:   4.9492, duration: 98.4223s\n",
      "2021-08-04 00:03:07,546 - INFO - joeynmt.training - Epoch   2, Step:   200200, Batch Loss:     1.682582, Tokens per Sec:    15677, Lr: 0.000300\n",
      "2021-08-04 00:03:34,969 - INFO - joeynmt.training - Epoch   2, Step:   200400, Batch Loss:     1.612549, Tokens per Sec:    15496, Lr: 0.000300\n",
      "2021-08-04 00:04:03,125 - INFO - joeynmt.training - Epoch   2, Step:   200600, Batch Loss:     1.612907, Tokens per Sec:    15780, Lr: 0.000300\n",
      "2021-08-04 00:04:30,817 - INFO - joeynmt.training - Epoch   2, Step:   200800, Batch Loss:     1.611770, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-08-04 00:04:58,657 - INFO - joeynmt.training - Epoch   2, Step:   201000, Batch Loss:     1.719788, Tokens per Sec:    15476, Lr: 0.000300\n",
      "2021-08-04 00:05:26,543 - INFO - joeynmt.training - Epoch   2, Step:   201200, Batch Loss:     1.663927, Tokens per Sec:    15596, Lr: 0.000300\n",
      "2021-08-04 00:05:54,019 - INFO - joeynmt.training - Epoch   2, Step:   201400, Batch Loss:     1.811843, Tokens per Sec:    15391, Lr: 0.000300\n",
      "2021-08-04 00:06:22,246 - INFO - joeynmt.training - Epoch   2, Step:   201600, Batch Loss:     1.632713, Tokens per Sec:    15562, Lr: 0.000300\n",
      "2021-08-04 00:06:49,927 - INFO - joeynmt.training - Epoch   2, Step:   201800, Batch Loss:     1.416817, Tokens per Sec:    15626, Lr: 0.000300\n",
      "2021-08-04 00:07:18,298 - INFO - joeynmt.training - Epoch   2, Step:   202000, Batch Loss:     1.893009, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-04 00:07:36,060 - INFO - joeynmt.training - Epoch   2: total training loss 9641.04\n",
      "2021-08-04 00:07:36,061 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-04 00:07:46,849 - INFO - joeynmt.training - Epoch   3, Step:   202200, Batch Loss:     2.268680, Tokens per Sec:    15012, Lr: 0.000300\n",
      "2021-08-04 00:08:14,803 - INFO - joeynmt.training - Epoch   3, Step:   202400, Batch Loss:     1.613668, Tokens per Sec:    15478, Lr: 0.000300\n",
      "2021-08-04 00:08:43,010 - INFO - joeynmt.training - Epoch   3, Step:   202600, Batch Loss:     1.728441, Tokens per Sec:    15585, Lr: 0.000300\n",
      "2021-08-04 00:09:11,087 - INFO - joeynmt.training - Epoch   3, Step:   202800, Batch Loss:     1.812151, Tokens per Sec:    15742, Lr: 0.000300\n",
      "2021-08-04 00:09:39,002 - INFO - joeynmt.training - Epoch   3, Step:   203000, Batch Loss:     1.792930, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-08-04 00:10:07,107 - INFO - joeynmt.training - Epoch   3, Step:   203200, Batch Loss:     1.747487, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-08-04 00:10:34,703 - INFO - joeynmt.training - Epoch   3, Step:   203400, Batch Loss:     1.646034, Tokens per Sec:    15650, Lr: 0.000300\n",
      "2021-08-04 00:11:02,799 - INFO - joeynmt.training - Epoch   3, Step:   203600, Batch Loss:     1.917113, Tokens per Sec:    15648, Lr: 0.000300\n",
      "2021-08-04 00:11:30,783 - INFO - joeynmt.training - Epoch   3, Step:   203800, Batch Loss:     1.756774, Tokens per Sec:    15754, Lr: 0.000300\n",
      "2021-08-04 00:11:58,461 - INFO - joeynmt.training - Epoch   3, Step:   204000, Batch Loss:     1.753822, Tokens per Sec:    15540, Lr: 0.000300\n",
      "2021-08-04 00:12:26,164 - INFO - joeynmt.training - Epoch   3, Step:   204200, Batch Loss:     1.630893, Tokens per Sec:    15722, Lr: 0.000300\n",
      "2021-08-04 00:12:53,918 - INFO - joeynmt.training - Epoch   3, Step:   204400, Batch Loss:     1.636591, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-08-04 00:13:22,150 - INFO - joeynmt.training - Epoch   3, Step:   204600, Batch Loss:     1.560084, Tokens per Sec:    15753, Lr: 0.000300\n",
      "2021-08-04 00:13:50,414 - INFO - joeynmt.training - Epoch   3, Step:   204800, Batch Loss:     2.018333, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-08-04 00:14:18,228 - INFO - joeynmt.training - Epoch   3, Step:   205000, Batch Loss:     1.804556, Tokens per Sec:    15613, Lr: 0.000300\n",
      "2021-08-04 00:15:53,768 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:15:53,768 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:15:53,768 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:15:55,062 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:15:55,063 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:15:56,190 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:15:56,192 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 00:15:56,192 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 00:15:56,192 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so good for me . ”\n",
      "2021-08-04 00:15:56,192 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:15:56,193 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 00:15:56,193 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 00:15:56,193 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , more than many treasures that are standing . ”\n",
      "2021-08-04 00:15:56,193 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:15:56,194 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 00:15:56,194 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 00:15:56,194 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 00:15:56,194 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:15:56,195 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 00:15:56,195 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 00:15:56,195 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-04 00:15:56,195 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   205000: bleu:  26.49, loss: 198349.5469, ppl:   4.9454, duration: 97.9672s\n",
      "2021-08-04 00:16:24,235 - INFO - joeynmt.training - Epoch   3, Step:   205200, Batch Loss:     1.796966, Tokens per Sec:    15393, Lr: 0.000300\n",
      "2021-08-04 00:16:52,137 - INFO - joeynmt.training - Epoch   3, Step:   205400, Batch Loss:     1.795640, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-08-04 00:17:20,066 - INFO - joeynmt.training - Epoch   3, Step:   205600, Batch Loss:     1.868433, Tokens per Sec:    15611, Lr: 0.000300\n",
      "2021-08-04 00:17:47,772 - INFO - joeynmt.training - Epoch   3, Step:   205800, Batch Loss:     1.950048, Tokens per Sec:    15531, Lr: 0.000300\n",
      "2021-08-04 00:18:15,620 - INFO - joeynmt.training - Epoch   3, Step:   206000, Batch Loss:     1.942963, Tokens per Sec:    15728, Lr: 0.000300\n",
      "2021-08-04 00:18:43,323 - INFO - joeynmt.training - Epoch   3, Step:   206200, Batch Loss:     1.883233, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-08-04 00:19:11,394 - INFO - joeynmt.training - Epoch   3, Step:   206400, Batch Loss:     1.818451, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-08-04 00:19:39,188 - INFO - joeynmt.training - Epoch   3, Step:   206600, Batch Loss:     1.741842, Tokens per Sec:    15552, Lr: 0.000300\n",
      "2021-08-04 00:20:07,165 - INFO - joeynmt.training - Epoch   3, Step:   206800, Batch Loss:     1.758410, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-08-04 00:20:35,114 - INFO - joeynmt.training - Epoch   3, Step:   207000, Batch Loss:     1.677276, Tokens per Sec:    15805, Lr: 0.000300\n",
      "2021-08-04 00:21:03,159 - INFO - joeynmt.training - Epoch   3, Step:   207200, Batch Loss:     1.703198, Tokens per Sec:    15597, Lr: 0.000300\n",
      "2021-08-04 00:21:30,664 - INFO - joeynmt.training - Epoch   3, Step:   207400, Batch Loss:     1.750078, Tokens per Sec:    15658, Lr: 0.000300\n",
      "2021-08-04 00:21:57,343 - INFO - joeynmt.training - Epoch   3: total training loss 9617.01\n",
      "2021-08-04 00:21:57,344 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-04 00:21:59,273 - INFO - joeynmt.training - Epoch   4, Step:   207600, Batch Loss:     1.600255, Tokens per Sec:    11291, Lr: 0.000300\n",
      "2021-08-04 00:22:27,336 - INFO - joeynmt.training - Epoch   4, Step:   207800, Batch Loss:     1.784789, Tokens per Sec:    15679, Lr: 0.000300\n",
      "2021-08-04 00:22:55,077 - INFO - joeynmt.training - Epoch   4, Step:   208000, Batch Loss:     1.749542, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-08-04 00:23:23,150 - INFO - joeynmt.training - Epoch   4, Step:   208200, Batch Loss:     1.690742, Tokens per Sec:    15651, Lr: 0.000300\n",
      "2021-08-04 00:23:51,013 - INFO - joeynmt.training - Epoch   4, Step:   208400, Batch Loss:     1.705320, Tokens per Sec:    15725, Lr: 0.000300\n",
      "2021-08-04 00:24:18,858 - INFO - joeynmt.training - Epoch   4, Step:   208600, Batch Loss:     1.786225, Tokens per Sec:    15692, Lr: 0.000300\n",
      "2021-08-04 00:24:46,721 - INFO - joeynmt.training - Epoch   4, Step:   208800, Batch Loss:     1.938519, Tokens per Sec:    15749, Lr: 0.000300\n",
      "2021-08-04 00:25:14,689 - INFO - joeynmt.training - Epoch   4, Step:   209000, Batch Loss:     1.697937, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-08-04 00:25:42,572 - INFO - joeynmt.training - Epoch   4, Step:   209200, Batch Loss:     1.642509, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-08-04 00:26:10,585 - INFO - joeynmt.training - Epoch   4, Step:   209400, Batch Loss:     1.866904, Tokens per Sec:    15514, Lr: 0.000300\n",
      "2021-08-04 00:26:38,379 - INFO - joeynmt.training - Epoch   4, Step:   209600, Batch Loss:     1.801070, Tokens per Sec:    15673, Lr: 0.000300\n",
      "2021-08-04 00:27:06,322 - INFO - joeynmt.training - Epoch   4, Step:   209800, Batch Loss:     1.648717, Tokens per Sec:    15520, Lr: 0.000300\n",
      "2021-08-04 00:27:34,250 - INFO - joeynmt.training - Epoch   4, Step:   210000, Batch Loss:     1.782511, Tokens per Sec:    15937, Lr: 0.000300\n",
      "2021-08-04 00:29:10,073 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:29:10,073 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:29:10,073 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:29:11,396 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:29:11,396 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:29:12,220 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:29:12,221 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 00:29:12,221 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 00:29:12,222 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been drawing close to God , so good for me . ”\n",
      "2021-08-04 00:29:12,222 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:29:12,222 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 00:29:12,222 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 00:29:12,222 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our reverence is in fear of Jehovah , greater than many riches that are standing . ”\n",
      "2021-08-04 00:29:12,223 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:29:12,223 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 00:29:12,223 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 00:29:12,223 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel describes God’s Kingdom .\n",
      "2021-08-04 00:29:12,223 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:29:12,224 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 00:29:12,224 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 00:29:12,224 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and conquering it ?\n",
      "2021-08-04 00:29:12,224 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   210000: bleu:  26.72, loss: 198074.8750, ppl:   4.9345, duration: 97.9738s\n",
      "2021-08-04 00:29:40,447 - INFO - joeynmt.training - Epoch   4, Step:   210200, Batch Loss:     1.567971, Tokens per Sec:    15597, Lr: 0.000300\n",
      "2021-08-04 00:30:08,052 - INFO - joeynmt.training - Epoch   4, Step:   210400, Batch Loss:     1.763416, Tokens per Sec:    15561, Lr: 0.000300\n",
      "2021-08-04 00:30:35,893 - INFO - joeynmt.training - Epoch   4, Step:   210600, Batch Loss:     1.614781, Tokens per Sec:    15523, Lr: 0.000300\n",
      "2021-08-04 00:31:04,096 - INFO - joeynmt.training - Epoch   4, Step:   210800, Batch Loss:     1.826301, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-04 00:31:31,928 - INFO - joeynmt.training - Epoch   4, Step:   211000, Batch Loss:     1.892835, Tokens per Sec:    15678, Lr: 0.000300\n",
      "2021-08-04 00:31:59,752 - INFO - joeynmt.training - Epoch   4, Step:   211200, Batch Loss:     1.790983, Tokens per Sec:    15548, Lr: 0.000300\n",
      "2021-08-04 00:32:27,978 - INFO - joeynmt.training - Epoch   4, Step:   211400, Batch Loss:     1.539910, Tokens per Sec:    15834, Lr: 0.000300\n",
      "2021-08-04 00:32:55,989 - INFO - joeynmt.training - Epoch   4, Step:   211600, Batch Loss:     1.902303, Tokens per Sec:    15712, Lr: 0.000300\n",
      "2021-08-04 00:33:23,988 - INFO - joeynmt.training - Epoch   4, Step:   211800, Batch Loss:     1.929758, Tokens per Sec:    15574, Lr: 0.000300\n",
      "2021-08-04 00:33:51,728 - INFO - joeynmt.training - Epoch   4, Step:   212000, Batch Loss:     1.617100, Tokens per Sec:    15723, Lr: 0.000300\n",
      "2021-08-04 00:34:19,904 - INFO - joeynmt.training - Epoch   4, Step:   212200, Batch Loss:     1.783136, Tokens per Sec:    15619, Lr: 0.000300\n",
      "2021-08-04 00:34:47,456 - INFO - joeynmt.training - Epoch   4, Step:   212400, Batch Loss:     1.585467, Tokens per Sec:    15632, Lr: 0.000300\n",
      "2021-08-04 00:35:15,259 - INFO - joeynmt.training - Epoch   4, Step:   212600, Batch Loss:     1.729290, Tokens per Sec:    15637, Lr: 0.000300\n",
      "2021-08-04 00:35:43,349 - INFO - joeynmt.training - Epoch   4, Step:   212800, Batch Loss:     1.640098, Tokens per Sec:    15681, Lr: 0.000300\n",
      "2021-08-04 00:36:11,253 - INFO - joeynmt.training - Epoch   4, Step:   213000, Batch Loss:     1.744610, Tokens per Sec:    15623, Lr: 0.000300\n",
      "2021-08-04 00:36:18,770 - INFO - joeynmt.training - Epoch   4: total training loss 9588.54\n",
      "2021-08-04 00:36:18,770 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-04 00:36:39,842 - INFO - joeynmt.training - Epoch   5, Step:   213200, Batch Loss:     1.838361, Tokens per Sec:    15412, Lr: 0.000300\n",
      "2021-08-04 00:37:07,813 - INFO - joeynmt.training - Epoch   5, Step:   213400, Batch Loss:     1.724892, Tokens per Sec:    15570, Lr: 0.000300\n",
      "2021-08-04 00:37:35,606 - INFO - joeynmt.training - Epoch   5, Step:   213600, Batch Loss:     1.611257, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-08-04 00:38:03,645 - INFO - joeynmt.training - Epoch   5, Step:   213800, Batch Loss:     1.731055, Tokens per Sec:    15574, Lr: 0.000300\n",
      "2021-08-04 00:38:31,554 - INFO - joeynmt.training - Epoch   5, Step:   214000, Batch Loss:     1.849372, Tokens per Sec:    15796, Lr: 0.000300\n",
      "2021-08-04 00:38:59,508 - INFO - joeynmt.training - Epoch   5, Step:   214200, Batch Loss:     1.682693, Tokens per Sec:    15728, Lr: 0.000300\n",
      "2021-08-04 00:39:27,438 - INFO - joeynmt.training - Epoch   5, Step:   214400, Batch Loss:     1.539669, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-08-04 00:39:55,260 - INFO - joeynmt.training - Epoch   5, Step:   214600, Batch Loss:     1.923044, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-08-04 00:40:23,193 - INFO - joeynmt.training - Epoch   5, Step:   214800, Batch Loss:     1.664264, Tokens per Sec:    15526, Lr: 0.000300\n",
      "2021-08-04 00:40:51,201 - INFO - joeynmt.training - Epoch   5, Step:   215000, Batch Loss:     1.793303, Tokens per Sec:    15794, Lr: 0.000300\n",
      "2021-08-04 00:42:25,463 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 00:42:25,464 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 00:42:25,464 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 00:42:26,680 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 00:42:26,681 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 00:42:27,437 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 00:42:27,438 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 00:42:27,438 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 00:42:27,438 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-04 00:42:27,438 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 00:42:27,439 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 00:42:27,439 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 00:42:27,439 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , greater than many riches that are standing . ”\n",
      "2021-08-04 00:42:27,439 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 00:42:27,439 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 00:42:27,440 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 00:42:27,440 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel .\n",
      "2021-08-04 00:42:27,440 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 00:42:27,440 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 00:42:27,440 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 00:42:27,441 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 00:42:27,441 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   215000: bleu:  26.66, loss: 197479.1250, ppl:   4.9108, duration: 96.2391s\n",
      "2021-08-04 00:42:55,767 - INFO - joeynmt.training - Epoch   5, Step:   215200, Batch Loss:     1.551702, Tokens per Sec:    15483, Lr: 0.000300\n",
      "2021-08-04 00:43:23,749 - INFO - joeynmt.training - Epoch   5, Step:   215400, Batch Loss:     1.857193, Tokens per Sec:    15623, Lr: 0.000300\n",
      "2021-08-04 00:43:51,585 - INFO - joeynmt.training - Epoch   5, Step:   215600, Batch Loss:     1.902052, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-08-04 00:44:19,639 - INFO - joeynmt.training - Epoch   5, Step:   215800, Batch Loss:     1.757724, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-08-04 00:44:47,446 - INFO - joeynmt.training - Epoch   5, Step:   216000, Batch Loss:     1.812622, Tokens per Sec:    15644, Lr: 0.000300\n",
      "2021-08-04 00:45:15,404 - INFO - joeynmt.training - Epoch   5, Step:   216200, Batch Loss:     1.529654, Tokens per Sec:    15614, Lr: 0.000300\n",
      "2021-08-04 00:45:42,918 - INFO - joeynmt.training - Epoch   5, Step:   216400, Batch Loss:     1.764512, Tokens per Sec:    15769, Lr: 0.000300\n",
      "2021-08-04 00:46:10,862 - INFO - joeynmt.training - Epoch   5, Step:   216600, Batch Loss:     1.768980, Tokens per Sec:    15651, Lr: 0.000300\n",
      "2021-08-04 00:46:38,784 - INFO - joeynmt.training - Epoch   5, Step:   216800, Batch Loss:     1.847277, Tokens per Sec:    15592, Lr: 0.000300\n",
      "2021-08-04 00:47:06,820 - INFO - joeynmt.training - Epoch   5, Step:   217000, Batch Loss:     1.386384, Tokens per Sec:    15684, Lr: 0.000300\n",
      "2021-08-04 00:47:34,610 - INFO - joeynmt.training - Epoch   5, Step:   217200, Batch Loss:     2.179805, Tokens per Sec:    15710, Lr: 0.000300\n",
      "2021-08-04 00:48:02,519 - INFO - joeynmt.training - Epoch   5, Step:   217400, Batch Loss:     1.668064, Tokens per Sec:    15537, Lr: 0.000300\n",
      "2021-08-04 00:48:30,417 - INFO - joeynmt.training - Epoch   5, Step:   217600, Batch Loss:     1.704190, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-04 00:48:58,307 - INFO - joeynmt.training - Epoch   5, Step:   217800, Batch Loss:     1.494014, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-08-04 00:49:26,417 - INFO - joeynmt.training - Epoch   5, Step:   218000, Batch Loss:     1.577266, Tokens per Sec:    15679, Lr: 0.000300\n",
      "2021-08-04 00:49:54,204 - INFO - joeynmt.training - Epoch   5, Step:   218200, Batch Loss:     1.832410, Tokens per Sec:    15753, Lr: 0.000300\n",
      "2021-08-04 00:50:22,091 - INFO - joeynmt.training - Epoch   5, Step:   218400, Batch Loss:     1.582107, Tokens per Sec:    15512, Lr: 0.000300\n",
      "2021-08-04 00:50:38,280 - INFO - joeynmt.training - Epoch   5: total training loss 9569.03\n",
      "2021-08-04 00:50:38,281 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-04 00:50:50,714 - INFO - joeynmt.training - Epoch   6, Step:   218600, Batch Loss:     1.799666, Tokens per Sec:    15115, Lr: 0.000300\n",
      "2021-08-04 00:51:18,773 - INFO - joeynmt.training - Epoch   6, Step:   218800, Batch Loss:     1.804024, Tokens per Sec:    15534, Lr: 0.000300\n",
      "2021-08-04 00:51:46,801 - INFO - joeynmt.training - Epoch   6, Step:   219000, Batch Loss:     1.815452, Tokens per Sec:    15643, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJRTs_K73SVd",
    "outputId": "2c3c1bbe-2670-4c0f-d7ce-a594e86ff74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 200000\tLoss: 198443.67188\tPPL: 4.94916\tbleu: 26.66173\tLR: 0.00030000\t*\n",
      "Steps: 205000\tLoss: 198349.54688\tPPL: 4.94540\tbleu: 26.48877\tLR: 0.00030000\t*\n",
      "Steps: 210000\tLoss: 198074.87500\tPPL: 4.93447\tbleu: 26.72091\tLR: 0.00030000\t*\n",
      "Steps: 215000\tLoss: 197479.12500\tPPL: 4.91084\tbleu: 26.66221\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "! cat \"joeynmt/models/rw_lhen_reverse_transformer_continued3/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mdOZ1V8z8-u"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 215000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued3/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued4\"').replace(\n",
    "            f'epochs: 30', f'epochs: 18')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload4.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4NNGj8TM4N0A",
    "outputId": "5645ecd8-f5da-49e8-d537-59f7d379906e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rw_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued3/215000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 2000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 18                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rw_lhen_reverse_transformer_continued4\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_{name}_reload4.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Icw7DaXznLP",
    "outputId": "6350b3dc-57a1-4cb2-ba98-3a350afb8666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-04 06:35:34,513 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-04 06:35:34,583 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-04 06:35:47,967 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-04 06:35:49,330 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-04 06:35:50,920 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-04 06:35:51,937 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-04 06:35:51,937 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 06:35:53,808 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 06:35:54.063949: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 06:35:56,182 - INFO - joeynmt.training - Total params: 12177920\n",
      "2021-08-04 06:36:04,850 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued3/215000.ckpt\n",
      "2021-08-04 06:36:05,427 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
      "2021-08-04 06:36:05,427 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
      "2021-08-04 06:36:05,427 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-04 06:36:05,427 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
      "2021-08-04 06:36:05,428 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
      "2021-08-04 06:36:05,428 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
      "2021-08-04 06:36:05,428 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-04 06:36:05,428 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-04 06:36:05,429 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-04 06:36:05,429 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-04 06:36:05,429 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-04 06:36:05,429 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-04 06:36:05,430 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-04 06:36:05,430 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued3/215000.ckpt\n",
      "2021-08-04 06:36:05,430 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-04 06:36:05,430 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-04 06:36:05,430 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-04 06:36:05,431 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-04 06:36:05,431 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-04 06:36:05,431 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-04 06:36:05,431 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-04 06:36:05,432 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-04 06:36:05,432 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-04 06:36:05,432 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-04 06:36:05,432 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-04 06:36:05,433 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-04 06:36:05,433 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-04 06:36:05,433 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-04 06:36:05,433 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-04 06:36:05,434 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-04 06:36:05,434 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-04 06:36:05,434 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-04 06:36:05,434 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-04 06:36:05,435 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-04 06:36:05,435 - INFO - joeynmt.helpers - cfg.training.epochs                : 18\n",
      "2021-08-04 06:36:05,435 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-04 06:36:05,435 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-04 06:36:05,436 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-04 06:36:05,436 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued4\n",
      "2021-08-04 06:36:05,436 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-04 06:36:05,436 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-04 06:36:05,437 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-04 06:36:05,437 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-04 06:36:05,437 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-04 06:36:05,437 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-04 06:36:05,438 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-04 06:36:05,438 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-04 06:36:05,438 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-04 06:36:05,438 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-04 06:36:05,439 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-04 06:36:05,439 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-04 06:36:05,439 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-04 06:36:05,439 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-04 06:36:05,440 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-04 06:36:05,440 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-04 06:36:05,440 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 06:36:05,440 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-04 06:36:05,441 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-04 06:36:05,441 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-04 06:36:05,441 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-04 06:36:05,441 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-04 06:36:05,442 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-04 06:36:05,442 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-04 06:36:05,442 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-04 06:36:05,442 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 06:36:05,443 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-04 06:36:05,443 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-04 06:36:05,443 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-04 06:36:05,443 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-04 06:36:05,444 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-04 06:36:05,444 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434091,\n",
      "\tvalid 4447,\n",
      "\ttest 79\n",
      "2021-08-04 06:36:05,444 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-04 06:36:05,444 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-04 06:36:05,445 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-04 06:36:05,445 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
      "2021-08-04 06:36:05,445 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
      "2021-08-04 06:36:05,446 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
      "2021-08-04 06:36:05,490 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-04 06:36:05,491 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-04 06:37:07,086 - INFO - joeynmt.training - Epoch   1, Step:   215200, Batch Loss:     1.563624, Tokens per Sec:     7120, Lr: 0.000300\n",
      "2021-08-04 06:38:07,588 - INFO - joeynmt.training - Epoch   1, Step:   215400, Batch Loss:     1.822023, Tokens per Sec:     7225, Lr: 0.000300\n",
      "2021-08-04 06:39:07,798 - INFO - joeynmt.training - Epoch   1, Step:   215600, Batch Loss:     1.906330, Tokens per Sec:     7285, Lr: 0.000300\n",
      "2021-08-04 06:40:08,054 - INFO - joeynmt.training - Epoch   1, Step:   215800, Batch Loss:     1.747340, Tokens per Sec:     7270, Lr: 0.000300\n",
      "2021-08-04 06:41:08,512 - INFO - joeynmt.training - Epoch   1, Step:   216000, Batch Loss:     1.791690, Tokens per Sec:     7195, Lr: 0.000300\n",
      "2021-08-04 06:42:08,620 - INFO - joeynmt.training - Epoch   1, Step:   216200, Batch Loss:     1.542822, Tokens per Sec:     7262, Lr: 0.000300\n",
      "2021-08-04 06:43:08,259 - INFO - joeynmt.training - Epoch   1, Step:   216400, Batch Loss:     1.748802, Tokens per Sec:     7275, Lr: 0.000300\n",
      "2021-08-04 06:44:08,305 - INFO - joeynmt.training - Epoch   1, Step:   216600, Batch Loss:     1.777359, Tokens per Sec:     7283, Lr: 0.000300\n",
      "2021-08-04 06:45:08,294 - INFO - joeynmt.training - Epoch   1, Step:   216800, Batch Loss:     1.856241, Tokens per Sec:     7258, Lr: 0.000300\n",
      "2021-08-04 06:46:08,320 - INFO - joeynmt.training - Epoch   1, Step:   217000, Batch Loss:     1.356557, Tokens per Sec:     7325, Lr: 0.000300\n",
      "2021-08-04 06:47:08,547 - INFO - joeynmt.training - Epoch   1, Step:   217200, Batch Loss:     2.196735, Tokens per Sec:     7249, Lr: 0.000300\n",
      "2021-08-04 06:48:08,465 - INFO - joeynmt.training - Epoch   1, Step:   217400, Batch Loss:     1.682971, Tokens per Sec:     7237, Lr: 0.000300\n",
      "2021-08-04 06:49:08,839 - INFO - joeynmt.training - Epoch   1, Step:   217600, Batch Loss:     1.725586, Tokens per Sec:     7397, Lr: 0.000300\n",
      "2021-08-04 06:50:08,941 - INFO - joeynmt.training - Epoch   1, Step:   217800, Batch Loss:     1.512823, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-08-04 06:51:09,495 - INFO - joeynmt.training - Epoch   1, Step:   218000, Batch Loss:     1.587359, Tokens per Sec:     7279, Lr: 0.000300\n",
      "2021-08-04 06:52:09,653 - INFO - joeynmt.training - Epoch   1, Step:   218200, Batch Loss:     1.799190, Tokens per Sec:     7276, Lr: 0.000300\n",
      "2021-08-04 06:53:09,602 - INFO - joeynmt.training - Epoch   1, Step:   218400, Batch Loss:     1.581032, Tokens per Sec:     7216, Lr: 0.000300\n",
      "2021-08-04 06:53:44,629 - INFO - joeynmt.training - Epoch   1: total training loss 6162.91\n",
      "2021-08-04 06:53:44,629 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-04 06:54:10,923 - INFO - joeynmt.training - Epoch   2, Step:   218600, Batch Loss:     1.787421, Tokens per Sec:     7147, Lr: 0.000300\n",
      "2021-08-04 06:55:11,192 - INFO - joeynmt.training - Epoch   2, Step:   218800, Batch Loss:     1.804783, Tokens per Sec:     7232, Lr: 0.000300\n",
      "2021-08-04 06:56:11,230 - INFO - joeynmt.training - Epoch   2, Step:   219000, Batch Loss:     1.795931, Tokens per Sec:     7302, Lr: 0.000300\n",
      "2021-08-04 06:57:11,493 - INFO - joeynmt.training - Epoch   2, Step:   219200, Batch Loss:     1.901342, Tokens per Sec:     7338, Lr: 0.000300\n",
      "2021-08-04 06:58:11,298 - INFO - joeynmt.training - Epoch   2, Step:   219400, Batch Loss:     1.785569, Tokens per Sec:     7212, Lr: 0.000300\n",
      "2021-08-04 06:59:11,672 - INFO - joeynmt.training - Epoch   2, Step:   219600, Batch Loss:     1.513301, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-08-04 07:00:11,278 - INFO - joeynmt.training - Epoch   2, Step:   219800, Batch Loss:     1.470121, Tokens per Sec:     7277, Lr: 0.000300\n",
      "2021-08-04 07:01:11,407 - INFO - joeynmt.training - Epoch   2, Step:   220000, Batch Loss:     1.650256, Tokens per Sec:     7274, Lr: 0.000300\n",
      "2021-08-04 07:04:51,489 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 07:04:51,490 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 07:04:51,490 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 07:04:53,923 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:04:53,924 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 07:04:53,924 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 07:04:53,925 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , so good for me . ”\n",
      "2021-08-04 07:04:53,925 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:04:53,925 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 07:04:53,926 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 07:04:53,926 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our residents are fearing Jehovah , more than many treasures that are standing . ”\n",
      "2021-08-04 07:04:53,926 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:04:53,927 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 07:04:53,927 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 07:04:53,927 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 07:04:53,928 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:04:53,928 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 07:04:53,928 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 07:04:53,929 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-04 07:04:53,929 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   220000: bleu:  26.84, loss: 197999.5625, ppl:   4.9315, duration: 222.5216s\n",
      "2021-08-04 07:05:54,168 - INFO - joeynmt.training - Epoch   2, Step:   220200, Batch Loss:     1.914217, Tokens per Sec:     7312, Lr: 0.000300\n",
      "2021-08-04 07:06:53,990 - INFO - joeynmt.training - Epoch   2, Step:   220400, Batch Loss:     1.600970, Tokens per Sec:     7242, Lr: 0.000300\n",
      "2021-08-04 07:07:54,036 - INFO - joeynmt.training - Epoch   2, Step:   220600, Batch Loss:     1.957000, Tokens per Sec:     7305, Lr: 0.000300\n",
      "2021-08-04 07:08:53,677 - INFO - joeynmt.training - Epoch   2, Step:   220800, Batch Loss:     1.745640, Tokens per Sec:     7245, Lr: 0.000300\n",
      "2021-08-04 07:09:53,589 - INFO - joeynmt.training - Epoch   2, Step:   221000, Batch Loss:     2.022558, Tokens per Sec:     7196, Lr: 0.000300\n",
      "2021-08-04 07:10:53,055 - INFO - joeynmt.training - Epoch   2, Step:   221200, Batch Loss:     1.985612, Tokens per Sec:     7185, Lr: 0.000300\n",
      "2021-08-04 07:11:53,448 - INFO - joeynmt.training - Epoch   2, Step:   221400, Batch Loss:     1.500654, Tokens per Sec:     7366, Lr: 0.000300\n",
      "2021-08-04 07:12:53,696 - INFO - joeynmt.training - Epoch   2, Step:   221600, Batch Loss:     1.701418, Tokens per Sec:     7360, Lr: 0.000300\n",
      "2021-08-04 07:13:54,176 - INFO - joeynmt.training - Epoch   2, Step:   221800, Batch Loss:     1.822901, Tokens per Sec:     7274, Lr: 0.000300\n",
      "2021-08-04 07:14:54,409 - INFO - joeynmt.training - Epoch   2, Step:   222000, Batch Loss:     1.607135, Tokens per Sec:     7288, Lr: 0.000300\n",
      "2021-08-04 07:15:54,651 - INFO - joeynmt.training - Epoch   2, Step:   222200, Batch Loss:     1.621859, Tokens per Sec:     7312, Lr: 0.000300\n",
      "2021-08-04 07:16:54,676 - INFO - joeynmt.training - Epoch   2, Step:   222400, Batch Loss:     1.761106, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-08-04 07:17:54,398 - INFO - joeynmt.training - Epoch   2, Step:   222600, Batch Loss:     1.865314, Tokens per Sec:     7254, Lr: 0.000300\n",
      "2021-08-04 07:18:54,448 - INFO - joeynmt.training - Epoch   2, Step:   222800, Batch Loss:     1.705667, Tokens per Sec:     7281, Lr: 0.000300\n",
      "2021-08-04 07:19:54,084 - INFO - joeynmt.training - Epoch   2, Step:   223000, Batch Loss:     1.587298, Tokens per Sec:     7289, Lr: 0.000300\n",
      "2021-08-04 07:20:54,229 - INFO - joeynmt.training - Epoch   2, Step:   223200, Batch Loss:     1.444918, Tokens per Sec:     7350, Lr: 0.000300\n",
      "2021-08-04 07:21:53,920 - INFO - joeynmt.training - Epoch   2, Step:   223400, Batch Loss:     1.559500, Tokens per Sec:     7254, Lr: 0.000300\n",
      "2021-08-04 07:22:54,049 - INFO - joeynmt.training - Epoch   2, Step:   223600, Batch Loss:     1.785162, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-08-04 07:23:53,839 - INFO - joeynmt.training - Epoch   2, Step:   223800, Batch Loss:     1.763664, Tokens per Sec:     7258, Lr: 0.000300\n",
      "2021-08-04 07:24:49,523 - INFO - joeynmt.training - Epoch   2: total training loss 9568.26\n",
      "2021-08-04 07:24:49,523 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-04 07:24:54,751 - INFO - joeynmt.training - Epoch   3, Step:   224000, Batch Loss:     1.699509, Tokens per Sec:     6636, Lr: 0.000300\n",
      "2021-08-04 07:25:54,422 - INFO - joeynmt.training - Epoch   3, Step:   224200, Batch Loss:     1.648081, Tokens per Sec:     7247, Lr: 0.000300\n",
      "2021-08-04 07:26:54,653 - INFO - joeynmt.training - Epoch   3, Step:   224400, Batch Loss:     1.774523, Tokens per Sec:     7235, Lr: 0.000300\n",
      "2021-08-04 07:27:54,870 - INFO - joeynmt.training - Epoch   3, Step:   224600, Batch Loss:     1.688199, Tokens per Sec:     7325, Lr: 0.000300\n",
      "2021-08-04 07:28:55,033 - INFO - joeynmt.training - Epoch   3, Step:   224800, Batch Loss:     1.803162, Tokens per Sec:     7385, Lr: 0.000300\n",
      "2021-08-04 07:29:55,146 - INFO - joeynmt.training - Epoch   3, Step:   225000, Batch Loss:     2.264225, Tokens per Sec:     7289, Lr: 0.000300\n",
      "2021-08-04 07:33:37,162 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 07:33:37,163 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 07:33:37,163 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 07:33:38,740 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 07:33:38,741 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 07:33:39,690 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 07:33:39,691 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 07:33:39,691 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 07:33:39,691 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have drawn close to God , so good for me . ”\n",
      "2021-08-04 07:33:39,692 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 07:33:39,692 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 07:33:39,693 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 07:33:39,693 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , more than many treasures are standing . ”\n",
      "2021-08-04 07:33:39,693 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 07:33:39,694 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 07:33:39,694 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 07:33:39,695 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies in the book of Daniel that describe God’s Kingdom .\n",
      "2021-08-04 07:33:39,695 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 07:33:39,695 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 07:33:39,696 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 07:33:39,696 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 07:33:39,696 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   225000: bleu:  26.77, loss: 196650.2812, ppl:   4.8781, duration: 224.5503s\n",
      "2021-08-04 07:34:40,185 - INFO - joeynmt.training - Epoch   3, Step:   225200, Batch Loss:     1.726617, Tokens per Sec:     7189, Lr: 0.000300\n",
      "2021-08-04 07:35:40,443 - INFO - joeynmt.training - Epoch   3, Step:   225400, Batch Loss:     1.723285, Tokens per Sec:     7283, Lr: 0.000300\n",
      "2021-08-04 07:36:40,062 - INFO - joeynmt.training - Epoch   3, Step:   225600, Batch Loss:     1.732832, Tokens per Sec:     7242, Lr: 0.000300\n",
      "2021-08-04 07:37:39,934 - INFO - joeynmt.training - Epoch   3, Step:   225800, Batch Loss:     1.780073, Tokens per Sec:     7186, Lr: 0.000300\n",
      "2021-08-04 07:38:40,445 - INFO - joeynmt.training - Epoch   3, Step:   226000, Batch Loss:     1.665194, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-08-04 07:39:40,711 - INFO - joeynmt.training - Epoch   3, Step:   226200, Batch Loss:     1.725741, Tokens per Sec:     7316, Lr: 0.000300\n",
      "2021-08-04 07:40:41,470 - INFO - joeynmt.training - Epoch   3, Step:   226400, Batch Loss:     1.760925, Tokens per Sec:     7366, Lr: 0.000300\n",
      "2021-08-04 07:41:41,281 - INFO - joeynmt.training - Epoch   3, Step:   226600, Batch Loss:     1.649230, Tokens per Sec:     7234, Lr: 0.000300\n",
      "2021-08-04 07:42:41,589 - INFO - joeynmt.training - Epoch   3, Step:   226800, Batch Loss:     1.775394, Tokens per Sec:     7252, Lr: 0.000300\n",
      "2021-08-04 07:43:42,354 - INFO - joeynmt.training - Epoch   3, Step:   227000, Batch Loss:     1.758456, Tokens per Sec:     7285, Lr: 0.000300\n",
      "2021-08-04 07:44:42,435 - INFO - joeynmt.training - Epoch   3, Step:   227200, Batch Loss:     1.454353, Tokens per Sec:     7241, Lr: 0.000300\n",
      "2021-08-04 07:45:42,779 - INFO - joeynmt.training - Epoch   3, Step:   227400, Batch Loss:     1.637457, Tokens per Sec:     7324, Lr: 0.000300\n",
      "2021-08-04 07:46:43,041 - INFO - joeynmt.training - Epoch   3, Step:   227600, Batch Loss:     1.770238, Tokens per Sec:     7250, Lr: 0.000300\n",
      "2021-08-04 07:47:43,191 - INFO - joeynmt.training - Epoch   3, Step:   227800, Batch Loss:     1.860296, Tokens per Sec:     7202, Lr: 0.000300\n",
      "2021-08-04 07:48:43,233 - INFO - joeynmt.training - Epoch   3, Step:   228000, Batch Loss:     1.907867, Tokens per Sec:     7225, Lr: 0.000300\n",
      "2021-08-04 07:49:43,461 - INFO - joeynmt.training - Epoch   3, Step:   228200, Batch Loss:     1.609384, Tokens per Sec:     7291, Lr: 0.000300\n",
      "2021-08-04 07:50:43,854 - INFO - joeynmt.training - Epoch   3, Step:   228400, Batch Loss:     2.034324, Tokens per Sec:     7235, Lr: 0.000300\n",
      "2021-08-04 07:51:44,153 - INFO - joeynmt.training - Epoch   3, Step:   228600, Batch Loss:     1.517951, Tokens per Sec:     7272, Lr: 0.000300\n",
      "2021-08-04 07:52:44,552 - INFO - joeynmt.training - Epoch   3, Step:   228800, Batch Loss:     1.629333, Tokens per Sec:     7247, Lr: 0.000300\n",
      "2021-08-04 07:53:44,375 - INFO - joeynmt.training - Epoch   3, Step:   229000, Batch Loss:     1.570660, Tokens per Sec:     7181, Lr: 0.000300\n",
      "2021-08-04 07:54:44,543 - INFO - joeynmt.training - Epoch   3, Step:   229200, Batch Loss:     1.768567, Tokens per Sec:     7197, Lr: 0.000300\n",
      "2021-08-04 07:55:45,193 - INFO - joeynmt.training - Epoch   3, Step:   229400, Batch Loss:     2.189609, Tokens per Sec:     7255, Lr: 0.000300\n",
      "2021-08-04 07:56:01,614 - INFO - joeynmt.training - Epoch   3: total training loss 9548.99\n",
      "2021-08-04 07:56:01,614 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-04 07:56:46,355 - INFO - joeynmt.training - Epoch   4, Step:   229600, Batch Loss:     1.622906, Tokens per Sec:     7116, Lr: 0.000300\n",
      "2021-08-04 07:57:46,292 - INFO - joeynmt.training - Epoch   4, Step:   229800, Batch Loss:     1.569001, Tokens per Sec:     7130, Lr: 0.000300\n",
      "2021-08-04 07:58:46,966 - INFO - joeynmt.training - Epoch   4, Step:   230000, Batch Loss:     1.589108, Tokens per Sec:     7214, Lr: 0.000300\n",
      "2021-08-04 08:02:32,513 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 08:02:32,513 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 08:02:32,514 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 08:02:35,289 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:02:35,290 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 08:02:35,291 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 08:02:35,291 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , though , are approaching God , so good for me . ”\n",
      "2021-08-04 08:02:35,291 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:02:35,292 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 08:02:35,292 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 08:02:35,292 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are fearing Jehovah , greater than many riches that are standing . ”\n",
      "2021-08-04 08:02:35,293 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:02:35,293 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 08:02:35,294 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 08:02:35,294 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies in the book of Daniel that describe God’s Kingdom .\n",
      "2021-08-04 08:02:35,294 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:02:35,295 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 08:02:35,295 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 08:02:35,295 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 08:02:35,296 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   230000: bleu:  26.93, loss: 196659.3281, ppl:   4.8785, duration: 228.3292s\n",
      "2021-08-04 08:03:36,347 - INFO - joeynmt.training - Epoch   4, Step:   230200, Batch Loss:     1.633347, Tokens per Sec:     7227, Lr: 0.000300\n",
      "2021-08-04 08:04:36,702 - INFO - joeynmt.training - Epoch   4, Step:   230400, Batch Loss:     1.833704, Tokens per Sec:     7262, Lr: 0.000300\n",
      "2021-08-04 08:05:37,049 - INFO - joeynmt.training - Epoch   4, Step:   230600, Batch Loss:     1.818466, Tokens per Sec:     7239, Lr: 0.000300\n",
      "2021-08-04 08:06:37,919 - INFO - joeynmt.training - Epoch   4, Step:   230800, Batch Loss:     1.665759, Tokens per Sec:     7275, Lr: 0.000300\n",
      "2021-08-04 08:07:38,497 - INFO - joeynmt.training - Epoch   4, Step:   231000, Batch Loss:     1.623866, Tokens per Sec:     7301, Lr: 0.000300\n",
      "2021-08-04 08:08:38,892 - INFO - joeynmt.training - Epoch   4, Step:   231200, Batch Loss:     1.661795, Tokens per Sec:     7150, Lr: 0.000300\n",
      "2021-08-04 08:09:39,399 - INFO - joeynmt.training - Epoch   4, Step:   231400, Batch Loss:     1.777660, Tokens per Sec:     7271, Lr: 0.000300\n",
      "2021-08-04 08:10:39,900 - INFO - joeynmt.training - Epoch   4, Step:   231600, Batch Loss:     1.860525, Tokens per Sec:     7225, Lr: 0.000300\n",
      "2021-08-04 08:11:40,244 - INFO - joeynmt.training - Epoch   4, Step:   231800, Batch Loss:     1.671592, Tokens per Sec:     7236, Lr: 0.000300\n",
      "2021-08-04 08:12:39,964 - INFO - joeynmt.training - Epoch   4, Step:   232000, Batch Loss:     1.766373, Tokens per Sec:     7121, Lr: 0.000300\n",
      "2021-08-04 08:13:40,530 - INFO - joeynmt.training - Epoch   4, Step:   232200, Batch Loss:     1.792046, Tokens per Sec:     7347, Lr: 0.000300\n",
      "2021-08-04 08:14:40,726 - INFO - joeynmt.training - Epoch   4, Step:   232400, Batch Loss:     1.783346, Tokens per Sec:     7195, Lr: 0.000300\n",
      "2021-08-04 08:15:40,941 - INFO - joeynmt.training - Epoch   4, Step:   232600, Batch Loss:     1.833176, Tokens per Sec:     7234, Lr: 0.000300\n",
      "2021-08-04 08:16:41,220 - INFO - joeynmt.training - Epoch   4, Step:   232800, Batch Loss:     1.763194, Tokens per Sec:     7185, Lr: 0.000300\n",
      "2021-08-04 08:17:42,383 - INFO - joeynmt.training - Epoch   4, Step:   233000, Batch Loss:     1.969859, Tokens per Sec:     7349, Lr: 0.000300\n",
      "2021-08-04 08:18:42,943 - INFO - joeynmt.training - Epoch   4, Step:   233200, Batch Loss:     1.702612, Tokens per Sec:     7260, Lr: 0.000300\n",
      "2021-08-04 08:19:43,159 - INFO - joeynmt.training - Epoch   4, Step:   233400, Batch Loss:     1.557687, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-08-04 08:20:43,458 - INFO - joeynmt.training - Epoch   4, Step:   233600, Batch Loss:     1.752262, Tokens per Sec:     7250, Lr: 0.000300\n",
      "2021-08-04 08:21:43,982 - INFO - joeynmt.training - Epoch   4, Step:   233800, Batch Loss:     1.592404, Tokens per Sec:     7183, Lr: 0.000300\n",
      "2021-08-04 08:22:43,944 - INFO - joeynmt.training - Epoch   4, Step:   234000, Batch Loss:     1.697941, Tokens per Sec:     7229, Lr: 0.000300\n",
      "2021-08-04 08:23:43,917 - INFO - joeynmt.training - Epoch   4, Step:   234200, Batch Loss:     2.045872, Tokens per Sec:     7194, Lr: 0.000300\n",
      "2021-08-04 08:24:44,163 - INFO - joeynmt.training - Epoch   4, Step:   234400, Batch Loss:     1.671378, Tokens per Sec:     7306, Lr: 0.000300\n",
      "2021-08-04 08:25:44,998 - INFO - joeynmt.training - Epoch   4, Step:   234600, Batch Loss:     1.710425, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-08-04 08:26:45,250 - INFO - joeynmt.training - Epoch   4, Step:   234800, Batch Loss:     1.680841, Tokens per Sec:     7219, Lr: 0.000300\n",
      "2021-08-04 08:27:22,611 - INFO - joeynmt.training - Epoch   4: total training loss 9530.72\n",
      "2021-08-04 08:27:22,612 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-04 08:27:46,060 - INFO - joeynmt.training - Epoch   5, Step:   235000, Batch Loss:     1.707595, Tokens per Sec:     7059, Lr: 0.000300\n",
      "2021-08-04 08:31:25,979 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 08:31:25,979 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 08:31:25,980 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 08:31:27,546 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 08:31:27,546 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 08:31:28,443 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 08:31:28,444 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 08:31:28,444 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 08:31:28,445 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , so good for me . ”\n",
      "2021-08-04 08:31:28,445 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 08:31:28,446 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 08:31:28,446 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 08:31:28,446 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our areas are fearing Jehovah , greater than many riches that are standing . ”\n",
      "2021-08-04 08:31:28,446 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 08:31:28,447 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 08:31:28,447 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 08:31:28,448 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 08:31:28,448 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 08:31:28,449 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 08:31:28,449 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 08:31:28,449 - INFO - joeynmt.training - \tHypothesis: How will Christ “ continue to conquer ” and completely conquer ?\n",
      "2021-08-04 08:31:28,449 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   235000: bleu:  27.03, loss: 195714.1875, ppl:   4.8415, duration: 222.3892s\n",
      "2021-08-04 08:32:29,064 - INFO - joeynmt.training - Epoch   5, Step:   235200, Batch Loss:     1.684333, Tokens per Sec:     7248, Lr: 0.000300\n",
      "2021-08-04 08:33:29,062 - INFO - joeynmt.training - Epoch   5, Step:   235400, Batch Loss:     1.907856, Tokens per Sec:     7288, Lr: 0.000300\n",
      "2021-08-04 08:34:29,745 - INFO - joeynmt.training - Epoch   5, Step:   235600, Batch Loss:     1.739876, Tokens per Sec:     7318, Lr: 0.000300\n",
      "2021-08-04 08:35:29,651 - INFO - joeynmt.training - Epoch   5, Step:   235800, Batch Loss:     1.747760, Tokens per Sec:     7173, Lr: 0.000300\n",
      "2021-08-04 08:36:29,891 - INFO - joeynmt.training - Epoch   5, Step:   236000, Batch Loss:     1.375229, Tokens per Sec:     7228, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload4.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqysVJf5kSJT",
    "outputId": "a48d0b45-a820-4cbf-aafe-f7baf7533d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual3\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1aLpsXnYuvG",
    "outputId": "8fda6b1c-985a-4a46-d15c-27e7e80de074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 220000\tLoss: 197999.56250\tPPL: 4.93148\tbleu: 26.83609\tLR: 0.00030000\t\n",
      "Steps: 225000\tLoss: 196650.28125\tPPL: 4.87814\tbleu: 26.77074\tLR: 0.00030000\t*\n",
      "Steps: 230000\tLoss: 196659.32812\tPPL: 4.87850\tbleu: 26.93449\tLR: 0.00030000\t\n",
      "Steps: 235000\tLoss: 195714.18750\tPPL: 4.84148\tbleu: 27.02745\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "! cat \"joeynmt/models/rw_lhen_reverse_transformer_continued4/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNzPMaYZYunV"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 235000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued4/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued5\"').replace(\n",
    "            f'epochs: 30', f'epochs: 14')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload5.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "15SFkOffZBwN",
    "outputId": "e7860930-f82c-4fd8-fdf3-ddfed0ad62d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rw_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued4/235000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 2000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 14                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rw_lhen_reverse_transformer_continued5\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_{name}_reload5.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvOb4ms4ZSyT",
    "outputId": "c8351dd5-0abd-400f-ad82-21af90391226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-04 09:03:02,238 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-04 09:03:02,309 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-04 09:03:13,049 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-04 09:03:13,739 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-04 09:03:15,066 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-04 09:03:15,761 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-04 09:03:15,761 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 09:03:16,146 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 09:03:16.407646: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-04 09:03:17,979 - INFO - joeynmt.training - Total params: 12177920\n",
      "2021-08-04 09:03:28,456 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued4/235000.ckpt\n",
      "2021-08-04 09:03:28,880 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
      "2021-08-04 09:03:28,881 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
      "2021-08-04 09:03:28,881 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-04 09:03:28,881 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
      "2021-08-04 09:03:28,881 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
      "2021-08-04 09:03:28,881 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
      "2021-08-04 09:03:28,881 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-04 09:03:28,881 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued4/235000.ckpt\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-04 09:03:28,882 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-04 09:03:28,883 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-04 09:03:28,883 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-04 09:03:28,883 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-04 09:03:28,883 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-04 09:03:28,883 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-04 09:03:28,883 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-04 09:03:28,883 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-04 09:03:28,883 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 2000\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-04 09:03:28,884 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-04 09:03:28,885 - INFO - joeynmt.helpers - cfg.training.epochs                : 14\n",
      "2021-08-04 09:03:28,885 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-08-04 09:03:28,885 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-04 09:03:28,885 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-04 09:03:28,885 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued5\n",
      "2021-08-04 09:03:28,885 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-04 09:03:28,885 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-04 09:03:28,885 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-04 09:03:28,886 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-04 09:03:28,886 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-04 09:03:28,886 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-04 09:03:28,886 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-04 09:03:28,886 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-04 09:03:28,886 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-04 09:03:28,886 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-04 09:03:28,886 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-04 09:03:28,887 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-04 09:03:28,887 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-04 09:03:28,887 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-04 09:03:28,887 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-04 09:03:28,887 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-04 09:03:28,887 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 09:03:28,887 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-04 09:03:28,887 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-04 09:03:28,888 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-04 09:03:28,889 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-04 09:03:28,890 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-04 09:03:28,890 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-04 09:03:28,890 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434091,\n",
      "\tvalid 4447,\n",
      "\ttest 79\n",
      "2021-08-04 09:03:28,891 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-08-04 09:03:28,891 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-04 09:03:28,891 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-08-04 09:03:28,891 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
      "2021-08-04 09:03:28,891 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
      "2021-08-04 09:03:28,891 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
      "2021-08-04 09:03:28,904 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-04 09:03:28,904 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-04 09:03:55,683 - INFO - joeynmt.training - Epoch   1, Step:   235200, Batch Loss:     1.732206, Tokens per Sec:    16408, Lr: 0.000300\n",
      "2021-08-04 09:04:21,740 - INFO - joeynmt.training - Epoch   1, Step:   235400, Batch Loss:     1.878868, Tokens per Sec:    16781, Lr: 0.000300\n",
      "2021-08-04 09:04:48,319 - INFO - joeynmt.training - Epoch   1, Step:   235600, Batch Loss:     1.762213, Tokens per Sec:    16707, Lr: 0.000300\n",
      "2021-08-04 09:05:15,380 - INFO - joeynmt.training - Epoch   1, Step:   235800, Batch Loss:     1.763964, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-08-04 09:05:42,912 - INFO - joeynmt.training - Epoch   1, Step:   236000, Batch Loss:     1.386383, Tokens per Sec:    15815, Lr: 0.000300\n",
      "2021-08-04 09:06:10,390 - INFO - joeynmt.training - Epoch   1, Step:   236200, Batch Loss:     1.666212, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-08-04 09:06:37,804 - INFO - joeynmt.training - Epoch   1, Step:   236400, Batch Loss:     1.720986, Tokens per Sec:    16106, Lr: 0.000300\n",
      "2021-08-04 09:07:05,227 - INFO - joeynmt.training - Epoch   1, Step:   236600, Batch Loss:     1.739414, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-04 09:07:32,633 - INFO - joeynmt.training - Epoch   1, Step:   236800, Batch Loss:     1.716203, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-08-04 09:08:00,092 - INFO - joeynmt.training - Epoch   1, Step:   237000, Batch Loss:     1.676170, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-08-04 09:08:27,275 - INFO - joeynmt.training - Epoch   1, Step:   237200, Batch Loss:     1.916201, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-04 09:08:54,824 - INFO - joeynmt.training - Epoch   1, Step:   237400, Batch Loss:     1.694543, Tokens per Sec:    16133, Lr: 0.000300\n",
      "2021-08-04 09:09:22,311 - INFO - joeynmt.training - Epoch   1, Step:   237600, Batch Loss:     1.517673, Tokens per Sec:    16017, Lr: 0.000300\n",
      "2021-08-04 09:09:49,730 - INFO - joeynmt.training - Epoch   1, Step:   237800, Batch Loss:     1.630842, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-04 09:10:17,083 - INFO - joeynmt.training - Epoch   1, Step:   238000, Batch Loss:     1.727330, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-08-04 09:10:43,825 - INFO - joeynmt.training - Epoch   1, Step:   238200, Batch Loss:     1.835716, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-08-04 09:11:11,071 - INFO - joeynmt.training - Epoch   1, Step:   238400, Batch Loss:     1.706004, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-04 09:11:38,505 - INFO - joeynmt.training - Epoch   1, Step:   238600, Batch Loss:     1.617087, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-08-04 09:12:05,623 - INFO - joeynmt.training - Epoch   1, Step:   238800, Batch Loss:     1.815271, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-08-04 09:12:33,210 - INFO - joeynmt.training - Epoch   1, Step:   239000, Batch Loss:     1.738479, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-08-04 09:13:00,587 - INFO - joeynmt.training - Epoch   1, Step:   239200, Batch Loss:     1.558562, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-04 09:13:27,680 - INFO - joeynmt.training - Epoch   1, Step:   239400, Batch Loss:     1.621845, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-08-04 09:13:55,111 - INFO - joeynmt.training - Epoch   1, Step:   239600, Batch Loss:     2.089395, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-04 09:14:22,682 - INFO - joeynmt.training - Epoch   1, Step:   239800, Batch Loss:     1.596985, Tokens per Sec:    16179, Lr: 0.000300\n",
      "2021-08-04 09:14:49,756 - INFO - joeynmt.training - Epoch   1, Step:   240000, Batch Loss:     1.802187, Tokens per Sec:    15937, Lr: 0.000300\n",
      "2021-08-04 09:16:24,260 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 09:16:24,261 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 09:16:24,261 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 09:16:26,269 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 09:16:26,270 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 09:16:26,270 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 09:16:26,271 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-04 09:16:26,271 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 09:16:26,271 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 09:16:26,271 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 09:16:26,272 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our own areas are fearing Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-04 09:16:26,272 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 09:16:26,272 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 09:16:26,272 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 09:16:26,273 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 09:16:26,273 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 09:16:26,273 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 09:16:26,273 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 09:16:26,274 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 09:16:26,274 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step   240000: bleu:  26.84, loss: 195913.1719, ppl:   4.8493, duration: 96.5171s\n",
      "2021-08-04 09:16:53,595 - INFO - joeynmt.training - Epoch   1, Step:   240200, Batch Loss:     1.748701, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-08-04 09:17:19,416 - INFO - joeynmt.training - Epoch   1: total training loss 9375.26\n",
      "2021-08-04 09:17:19,416 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-04 09:17:21,676 - INFO - joeynmt.training - Epoch   2, Step:   240400, Batch Loss:     1.374552, Tokens per Sec:    11212, Lr: 0.000300\n",
      "2021-08-04 09:17:49,142 - INFO - joeynmt.training - Epoch   2, Step:   240600, Batch Loss:     1.640124, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-08-04 09:18:16,449 - INFO - joeynmt.training - Epoch   2, Step:   240800, Batch Loss:     1.877210, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-08-04 09:18:43,450 - INFO - joeynmt.training - Epoch   2, Step:   241000, Batch Loss:     1.853218, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-08-04 09:19:10,805 - INFO - joeynmt.training - Epoch   2, Step:   241200, Batch Loss:     1.674590, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-04 09:19:38,229 - INFO - joeynmt.training - Epoch   2, Step:   241400, Batch Loss:     1.911912, Tokens per Sec:    16391, Lr: 0.000300\n",
      "2021-08-04 09:20:05,654 - INFO - joeynmt.training - Epoch   2, Step:   241600, Batch Loss:     1.773058, Tokens per Sec:    15814, Lr: 0.000300\n",
      "2021-08-04 09:20:32,847 - INFO - joeynmt.training - Epoch   2, Step:   241800, Batch Loss:     1.670965, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-08-04 09:21:00,322 - INFO - joeynmt.training - Epoch   2, Step:   242000, Batch Loss:     1.535402, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-04 09:21:27,941 - INFO - joeynmt.training - Epoch   2, Step:   242200, Batch Loss:     1.774998, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-08-04 09:21:55,117 - INFO - joeynmt.training - Epoch   2, Step:   242400, Batch Loss:     1.889397, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-04 09:22:22,576 - INFO - joeynmt.training - Epoch   2, Step:   242600, Batch Loss:     1.822090, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-08-04 09:22:49,843 - INFO - joeynmt.training - Epoch   2, Step:   242800, Batch Loss:     1.563530, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-04 09:23:17,309 - INFO - joeynmt.training - Epoch   2, Step:   243000, Batch Loss:     1.894542, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-04 09:23:44,710 - INFO - joeynmt.training - Epoch   2, Step:   243200, Batch Loss:     1.648703, Tokens per Sec:    15861, Lr: 0.000300\n",
      "2021-08-04 09:24:12,005 - INFO - joeynmt.training - Epoch   2, Step:   243400, Batch Loss:     1.773107, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-04 09:24:38,958 - INFO - joeynmt.training - Epoch   2, Step:   243600, Batch Loss:     1.662710, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-04 09:25:06,486 - INFO - joeynmt.training - Epoch   2, Step:   243800, Batch Loss:     1.858237, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-08-04 09:25:33,399 - INFO - joeynmt.training - Epoch   2, Step:   244000, Batch Loss:     1.499816, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-04 09:26:00,927 - INFO - joeynmt.training - Epoch   2, Step:   244200, Batch Loss:     1.681626, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-04 09:26:28,218 - INFO - joeynmt.training - Epoch   2, Step:   244400, Batch Loss:     1.728056, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-08-04 09:26:55,379 - INFO - joeynmt.training - Epoch   2, Step:   244600, Batch Loss:     1.563278, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-08-04 09:27:23,033 - INFO - joeynmt.training - Epoch   2, Step:   244800, Batch Loss:     1.637828, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-08-04 09:27:50,498 - INFO - joeynmt.training - Epoch   2, Step:   245000, Batch Loss:     1.686280, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-04 09:29:25,908 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 09:29:25,908 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 09:29:25,908 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 09:29:27,119 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 09:29:27,119 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 09:29:27,897 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 09:29:27,898 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 09:29:27,898 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 09:29:27,898 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-04 09:29:27,898 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 09:29:27,899 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 09:29:27,899 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 09:29:27,899 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our own residents are fearing Jehovah , more than many treasures that are standing . ”\n",
      "2021-08-04 09:29:27,899 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 09:29:27,900 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 09:29:27,900 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 09:29:27,900 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 09:29:27,901 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 09:29:27,901 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 09:29:27,901 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 09:29:27,901 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 09:29:27,902 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   245000: bleu:  27.01, loss: 195532.4062, ppl:   4.8344, duration: 97.4037s\n",
      "2021-08-04 09:29:55,734 - INFO - joeynmt.training - Epoch   2, Step:   245200, Batch Loss:     1.561763, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-04 09:30:22,823 - INFO - joeynmt.training - Epoch   2, Step:   245400, Batch Loss:     1.751663, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-08-04 09:30:50,030 - INFO - joeynmt.training - Epoch   2, Step:   245600, Batch Loss:     1.706565, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-08-04 09:31:17,187 - INFO - joeynmt.training - Epoch   2, Step:   245800, Batch Loss:     1.549981, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-08-04 09:31:23,751 - INFO - joeynmt.training - Epoch   2: total training loss 9475.00\n",
      "2021-08-04 09:31:23,752 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-04 09:31:44,854 - INFO - joeynmt.training - Epoch   3, Step:   246000, Batch Loss:     1.798835, Tokens per Sec:    15636, Lr: 0.000300\n",
      "2021-08-04 09:32:12,497 - INFO - joeynmt.training - Epoch   3, Step:   246200, Batch Loss:     1.817968, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-08-04 09:32:39,797 - INFO - joeynmt.training - Epoch   3, Step:   246400, Batch Loss:     1.512328, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-08-04 09:33:06,966 - INFO - joeynmt.training - Epoch   3, Step:   246600, Batch Loss:     1.786511, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-08-04 09:33:34,493 - INFO - joeynmt.training - Epoch   3, Step:   246800, Batch Loss:     1.719586, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-08-04 09:34:01,588 - INFO - joeynmt.training - Epoch   3, Step:   247000, Batch Loss:     1.673150, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-04 09:34:29,209 - INFO - joeynmt.training - Epoch   3, Step:   247200, Batch Loss:     1.761536, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-08-04 09:34:56,463 - INFO - joeynmt.training - Epoch   3, Step:   247400, Batch Loss:     1.683203, Tokens per Sec:    15864, Lr: 0.000300\n",
      "2021-08-04 09:35:23,827 - INFO - joeynmt.training - Epoch   3, Step:   247600, Batch Loss:     1.781648, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-04 09:35:51,345 - INFO - joeynmt.training - Epoch   3, Step:   247800, Batch Loss:     1.811983, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-08-04 09:36:18,950 - INFO - joeynmt.training - Epoch   3, Step:   248000, Batch Loss:     1.380980, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-04 09:36:46,246 - INFO - joeynmt.training - Epoch   3, Step:   248200, Batch Loss:     1.773241, Tokens per Sec:    16200, Lr: 0.000300\n",
      "2021-08-04 09:37:13,864 - INFO - joeynmt.training - Epoch   3, Step:   248400, Batch Loss:     1.848060, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-04 09:37:41,178 - INFO - joeynmt.training - Epoch   3, Step:   248600, Batch Loss:     2.056085, Tokens per Sec:    15995, Lr: 0.000300\n",
      "2021-08-04 09:38:08,759 - INFO - joeynmt.training - Epoch   3, Step:   248800, Batch Loss:     1.479029, Tokens per Sec:    16295, Lr: 0.000300\n",
      "2021-08-04 09:38:36,077 - INFO - joeynmt.training - Epoch   3, Step:   249000, Batch Loss:     1.627835, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-04 09:39:03,499 - INFO - joeynmt.training - Epoch   3, Step:   249200, Batch Loss:     1.798166, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-08-04 09:39:31,112 - INFO - joeynmt.training - Epoch   3, Step:   249400, Batch Loss:     1.535171, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-08-04 09:39:58,522 - INFO - joeynmt.training - Epoch   3, Step:   249600, Batch Loss:     1.715879, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-04 09:40:25,644 - INFO - joeynmt.training - Epoch   3, Step:   249800, Batch Loss:     1.896307, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-04 09:40:53,119 - INFO - joeynmt.training - Epoch   3, Step:   250000, Batch Loss:     1.610813, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-08-04 09:42:28,073 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 09:42:28,074 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 09:42:28,074 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 09:42:29,333 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 09:42:29,333 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 09:42:30,011 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 09:42:30,012 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 09:42:30,012 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 09:42:30,012 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-04 09:42:30,013 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 09:42:30,013 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 09:42:30,013 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 09:42:30,013 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in fear of Jehovah , more than many treasures are standing . ”\n",
      "2021-08-04 09:42:30,014 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 09:42:30,014 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 09:42:30,014 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 09:42:30,014 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 09:42:30,015 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 09:42:30,015 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 09:42:30,015 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 09:42:30,015 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 09:42:30,016 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   250000: bleu:  27.08, loss: 195113.1250, ppl:   4.8181, duration: 96.8960s\n",
      "2021-08-04 09:42:57,482 - INFO - joeynmt.training - Epoch   3, Step:   250200, Batch Loss:     1.644916, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-08-04 09:43:24,911 - INFO - joeynmt.training - Epoch   3, Step:   250400, Batch Loss:     1.722630, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-08-04 09:43:52,046 - INFO - joeynmt.training - Epoch   3, Step:   250600, Batch Loss:     1.724204, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-04 09:44:19,448 - INFO - joeynmt.training - Epoch   3, Step:   250800, Batch Loss:     1.853290, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-08-04 09:44:46,549 - INFO - joeynmt.training - Epoch   3, Step:   251000, Batch Loss:     1.575989, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-08-04 09:45:14,161 - INFO - joeynmt.training - Epoch   3, Step:   251200, Batch Loss:     1.609037, Tokens per Sec:    16312, Lr: 0.000300\n",
      "2021-08-04 09:45:28,628 - INFO - joeynmt.training - Epoch   3: total training loss 9447.17\n",
      "2021-08-04 09:45:28,628 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-04 09:45:42,345 - INFO - joeynmt.training - Epoch   4, Step:   251400, Batch Loss:     1.456449, Tokens per Sec:    15168, Lr: 0.000300\n",
      "2021-08-04 09:46:09,734 - INFO - joeynmt.training - Epoch   4, Step:   251600, Batch Loss:     1.751694, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-08-04 09:46:36,978 - INFO - joeynmt.training - Epoch   4, Step:   251800, Batch Loss:     1.774333, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-08-04 09:47:04,300 - INFO - joeynmt.training - Epoch   4, Step:   252000, Batch Loss:     1.652866, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-08-04 09:47:31,666 - INFO - joeynmt.training - Epoch   4, Step:   252200, Batch Loss:     1.747710, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-04 09:47:58,913 - INFO - joeynmt.training - Epoch   4, Step:   252400, Batch Loss:     2.029348, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-08-04 09:48:26,471 - INFO - joeynmt.training - Epoch   4, Step:   252600, Batch Loss:     1.955791, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-04 09:48:53,855 - INFO - joeynmt.training - Epoch   4, Step:   252800, Batch Loss:     1.451562, Tokens per Sec:    16223, Lr: 0.000300\n",
      "2021-08-04 09:49:21,308 - INFO - joeynmt.training - Epoch   4, Step:   253000, Batch Loss:     1.768646, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-08-04 09:49:48,453 - INFO - joeynmt.training - Epoch   4, Step:   253200, Batch Loss:     1.765414, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-04 09:50:15,926 - INFO - joeynmt.training - Epoch   4, Step:   253400, Batch Loss:     1.605902, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-08-04 09:50:43,520 - INFO - joeynmt.training - Epoch   4, Step:   253600, Batch Loss:     1.927827, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-08-04 09:51:10,679 - INFO - joeynmt.training - Epoch   4, Step:   253800, Batch Loss:     1.727794, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-08-04 09:51:38,039 - INFO - joeynmt.training - Epoch   4, Step:   254000, Batch Loss:     1.533148, Tokens per Sec:    16117, Lr: 0.000300\n",
      "2021-08-04 09:52:05,527 - INFO - joeynmt.training - Epoch   4, Step:   254200, Batch Loss:     1.858001, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-08-04 09:52:32,755 - INFO - joeynmt.training - Epoch   4, Step:   254400, Batch Loss:     1.934476, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-08-04 09:52:59,959 - INFO - joeynmt.training - Epoch   4, Step:   254600, Batch Loss:     1.608960, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-08-04 09:53:27,214 - INFO - joeynmt.training - Epoch   4, Step:   254800, Batch Loss:     1.728270, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-08-04 09:53:54,340 - INFO - joeynmt.training - Epoch   4, Step:   255000, Batch Loss:     1.782823, Tokens per Sec:    16159, Lr: 0.000300\n",
      "2021-08-04 09:55:30,230 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 09:55:30,231 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 09:55:30,231 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 09:55:31,472 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 09:55:31,472 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 09:55:32,259 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 09:55:32,260 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 09:55:32,260 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 09:55:32,260 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-04 09:55:32,261 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 09:55:32,261 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 09:55:32,261 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 09:55:32,262 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our residents are fearing Jehovah , more than many treasures that are standing . ”\n",
      "2021-08-04 09:55:32,262 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 09:55:32,262 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 09:55:32,262 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 09:55:32,263 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 09:55:32,263 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 09:55:32,263 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 09:55:32,263 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 09:55:32,263 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 09:55:32,264 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   255000: bleu:  26.99, loss: 194757.9062, ppl:   4.8043, duration: 97.9235s\n",
      "2021-08-04 09:55:59,742 - INFO - joeynmt.training - Epoch   4, Step:   255200, Batch Loss:     1.726206, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-08-04 09:56:26,993 - INFO - joeynmt.training - Epoch   4, Step:   255400, Batch Loss:     1.573778, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-08-04 09:56:54,678 - INFO - joeynmt.training - Epoch   4, Step:   255600, Batch Loss:     1.755633, Tokens per Sec:    16159, Lr: 0.000300\n",
      "2021-08-04 09:57:22,033 - INFO - joeynmt.training - Epoch   4, Step:   255800, Batch Loss:     1.698015, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-04 09:57:49,535 - INFO - joeynmt.training - Epoch   4, Step:   256000, Batch Loss:     1.684422, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-08-04 09:58:16,755 - INFO - joeynmt.training - Epoch   4, Step:   256200, Batch Loss:     1.735427, Tokens per Sec:    15585, Lr: 0.000300\n",
      "2021-08-04 09:58:43,990 - INFO - joeynmt.training - Epoch   4, Step:   256400, Batch Loss:     1.628721, Tokens per Sec:    16267, Lr: 0.000300\n",
      "2021-08-04 09:59:11,456 - INFO - joeynmt.training - Epoch   4, Step:   256600, Batch Loss:     1.666507, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-08-04 09:59:34,704 - INFO - joeynmt.training - Epoch   4: total training loss 9446.94\n",
      "2021-08-04 09:59:34,705 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-04 09:59:39,317 - INFO - joeynmt.training - Epoch   5, Step:   256800, Batch Loss:     1.940256, Tokens per Sec:    13061, Lr: 0.000300\n",
      "2021-08-04 10:00:06,354 - INFO - joeynmt.training - Epoch   5, Step:   257000, Batch Loss:     1.741866, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-08-04 10:00:33,774 - INFO - joeynmt.training - Epoch   5, Step:   257200, Batch Loss:     1.699896, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-08-04 10:01:00,862 - INFO - joeynmt.training - Epoch   5, Step:   257400, Batch Loss:     1.884159, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-04 10:01:28,426 - INFO - joeynmt.training - Epoch   5, Step:   257600, Batch Loss:     1.825446, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-08-04 10:01:55,706 - INFO - joeynmt.training - Epoch   5, Step:   257800, Batch Loss:     1.834143, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-04 10:02:22,870 - INFO - joeynmt.training - Epoch   5, Step:   258000, Batch Loss:     1.597840, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-08-04 10:02:50,174 - INFO - joeynmt.training - Epoch   5, Step:   258200, Batch Loss:     1.608587, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-08-04 10:03:17,787 - INFO - joeynmt.training - Epoch   5, Step:   258400, Batch Loss:     2.032926, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-08-04 10:03:45,025 - INFO - joeynmt.training - Epoch   5, Step:   258600, Batch Loss:     1.528666, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-08-04 10:04:12,717 - INFO - joeynmt.training - Epoch   5, Step:   258800, Batch Loss:     1.633134, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-04 10:04:40,107 - INFO - joeynmt.training - Epoch   5, Step:   259000, Batch Loss:     1.649410, Tokens per Sec:    16309, Lr: 0.000300\n",
      "2021-08-04 10:05:07,303 - INFO - joeynmt.training - Epoch   5, Step:   259200, Batch Loss:     1.561167, Tokens per Sec:    15763, Lr: 0.000300\n",
      "2021-08-04 10:05:34,560 - INFO - joeynmt.training - Epoch   5, Step:   259400, Batch Loss:     1.682255, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-08-04 10:06:01,510 - INFO - joeynmt.training - Epoch   5, Step:   259600, Batch Loss:     1.872832, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-08-04 10:06:28,836 - INFO - joeynmt.training - Epoch   5, Step:   259800, Batch Loss:     1.858012, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-08-04 10:06:56,181 - INFO - joeynmt.training - Epoch   5, Step:   260000, Batch Loss:     1.742858, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-08-04 10:08:33,287 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 10:08:33,288 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 10:08:33,288 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 10:08:35,544 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 10:08:35,544 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 10:08:35,545 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 10:08:35,545 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-04 10:08:35,545 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 10:08:35,545 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 10:08:35,545 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 10:08:35,546 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our residents are in fear of Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-04 10:08:35,546 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 10:08:35,546 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 10:08:35,546 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 10:08:35,546 - INFO - joeynmt.training - \tHypothesis: Cameron : Here is another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 10:08:35,547 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 10:08:35,547 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 10:08:35,547 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 10:08:35,547 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 10:08:35,547 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   260000: bleu:  27.20, loss: 194817.6719, ppl:   4.8066, duration: 99.3660s\n",
      "2021-08-04 10:09:03,282 - INFO - joeynmt.training - Epoch   5, Step:   260200, Batch Loss:     1.736315, Tokens per Sec:    15763, Lr: 0.000300\n",
      "2021-08-04 10:09:30,696 - INFO - joeynmt.training - Epoch   5, Step:   260400, Batch Loss:     1.584340, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-04 10:09:57,785 - INFO - joeynmt.training - Epoch   5, Step:   260600, Batch Loss:     1.594988, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-08-04 10:10:25,150 - INFO - joeynmt.training - Epoch   5, Step:   260800, Batch Loss:     1.910222, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-04 10:10:52,342 - INFO - joeynmt.training - Epoch   5, Step:   261000, Batch Loss:     1.646894, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-08-04 10:11:20,098 - INFO - joeynmt.training - Epoch   5, Step:   261200, Batch Loss:     1.811478, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-08-04 10:11:47,275 - INFO - joeynmt.training - Epoch   5, Step:   261400, Batch Loss:     1.707817, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-04 10:12:14,672 - INFO - joeynmt.training - Epoch   5, Step:   261600, Batch Loss:     1.607817, Tokens per Sec:    16058, Lr: 0.000300\n",
      "2021-08-04 10:12:42,188 - INFO - joeynmt.training - Epoch   5, Step:   261800, Batch Loss:     1.850003, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-04 10:13:09,751 - INFO - joeynmt.training - Epoch   5, Step:   262000, Batch Loss:     1.445229, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-04 10:13:36,958 - INFO - joeynmt.training - Epoch   5, Step:   262200, Batch Loss:     1.633124, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-08-04 10:13:43,448 - INFO - joeynmt.training - Epoch   5: total training loss 9458.07\n",
      "2021-08-04 10:13:43,448 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-04 10:14:05,320 - INFO - joeynmt.training - Epoch   6, Step:   262400, Batch Loss:     1.582054, Tokens per Sec:    15293, Lr: 0.000300\n",
      "2021-08-04 10:14:32,274 - INFO - joeynmt.training - Epoch   6, Step:   262600, Batch Loss:     1.736695, Tokens per Sec:    15728, Lr: 0.000300\n",
      "2021-08-04 10:14:59,498 - INFO - joeynmt.training - Epoch   6, Step:   262800, Batch Loss:     1.626175, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-08-04 10:15:26,808 - INFO - joeynmt.training - Epoch   6, Step:   263000, Batch Loss:     1.646992, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-08-04 10:15:54,016 - INFO - joeynmt.training - Epoch   6, Step:   263200, Batch Loss:     1.640571, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-08-04 10:16:21,510 - INFO - joeynmt.training - Epoch   6, Step:   263400, Batch Loss:     1.815703, Tokens per Sec:    15809, Lr: 0.000300\n",
      "2021-08-04 10:16:48,604 - INFO - joeynmt.training - Epoch   6, Step:   263600, Batch Loss:     1.410841, Tokens per Sec:    16056, Lr: 0.000300\n",
      "2021-08-04 10:17:15,902 - INFO - joeynmt.training - Epoch   6, Step:   263800, Batch Loss:     1.833614, Tokens per Sec:    15939, Lr: 0.000300\n",
      "2021-08-04 10:17:43,449 - INFO - joeynmt.training - Epoch   6, Step:   264000, Batch Loss:     1.591823, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-08-04 10:18:10,964 - INFO - joeynmt.training - Epoch   6, Step:   264200, Batch Loss:     1.802120, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-08-04 10:18:38,275 - INFO - joeynmt.training - Epoch   6, Step:   264400, Batch Loss:     1.585442, Tokens per Sec:    16247, Lr: 0.000300\n",
      "2021-08-04 10:19:05,784 - INFO - joeynmt.training - Epoch   6, Step:   264600, Batch Loss:     1.516093, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-08-04 10:19:32,984 - INFO - joeynmt.training - Epoch   6, Step:   264800, Batch Loss:     1.641557, Tokens per Sec:    16130, Lr: 0.000300\n",
      "2021-08-04 10:20:00,562 - INFO - joeynmt.training - Epoch   6, Step:   265000, Batch Loss:     1.988922, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-04 10:21:34,832 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 10:21:34,832 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 10:21:34,833 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 10:21:36,059 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 10:21:36,059 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 10:21:36,929 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 10:21:36,930 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 10:21:36,930 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 10:21:36,930 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who are approaching God , is good for me . ”\n",
      "2021-08-04 10:21:36,931 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 10:21:36,932 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 10:21:36,932 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 10:21:36,933 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our residents are the fear of Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-04 10:21:36,933 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 10:21:36,933 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 10:21:36,933 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 10:21:36,933 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel .\n",
      "2021-08-04 10:21:36,934 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 10:21:36,934 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 10:21:36,934 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 10:21:36,934 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and conquering it completely ?\n",
      "2021-08-04 10:21:36,935 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   265000: bleu:  27.10, loss: 194091.0312, ppl:   4.7786, duration: 96.3724s\n",
      "2021-08-04 10:22:04,537 - INFO - joeynmt.training - Epoch   6, Step:   265200, Batch Loss:     1.622773, Tokens per Sec:    15576, Lr: 0.000300\n",
      "2021-08-04 10:22:31,980 - INFO - joeynmt.training - Epoch   6, Step:   265400, Batch Loss:     1.640816, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-08-04 10:22:59,466 - INFO - joeynmt.training - Epoch   6, Step:   265600, Batch Loss:     1.641254, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-08-04 10:23:27,034 - INFO - joeynmt.training - Epoch   6, Step:   265800, Batch Loss:     2.100131, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-08-04 10:23:54,485 - INFO - joeynmt.training - Epoch   6, Step:   266000, Batch Loss:     1.674661, Tokens per Sec:    15891, Lr: 0.000300\n",
      "2021-08-04 10:24:21,830 - INFO - joeynmt.training - Epoch   6, Step:   266200, Batch Loss:     1.768899, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-08-04 10:24:49,311 - INFO - joeynmt.training - Epoch   6, Step:   266400, Batch Loss:     1.674420, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-04 10:25:16,833 - INFO - joeynmt.training - Epoch   6, Step:   266600, Batch Loss:     1.686432, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-04 10:25:44,251 - INFO - joeynmt.training - Epoch   6, Step:   266800, Batch Loss:     1.740834, Tokens per Sec:    16149, Lr: 0.000300\n",
      "2021-08-04 10:26:11,695 - INFO - joeynmt.training - Epoch   6, Step:   267000, Batch Loss:     1.612667, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-08-04 10:26:39,069 - INFO - joeynmt.training - Epoch   6, Step:   267200, Batch Loss:     1.719542, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-08-04 10:27:06,425 - INFO - joeynmt.training - Epoch   6, Step:   267400, Batch Loss:     1.752024, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-08-04 10:27:33,643 - INFO - joeynmt.training - Epoch   6, Step:   267600, Batch Loss:     1.731522, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-08-04 10:27:49,238 - INFO - joeynmt.training - Epoch   6: total training loss 9424.92\n",
      "2021-08-04 10:27:49,238 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-04 10:28:01,745 - INFO - joeynmt.training - Epoch   7, Step:   267800, Batch Loss:     1.690711, Tokens per Sec:    15591, Lr: 0.000300\n",
      "2021-08-04 10:28:28,968 - INFO - joeynmt.training - Epoch   7, Step:   268000, Batch Loss:     1.775167, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-08-04 10:28:56,658 - INFO - joeynmt.training - Epoch   7, Step:   268200, Batch Loss:     1.696840, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-08-04 10:29:24,012 - INFO - joeynmt.training - Epoch   7, Step:   268400, Batch Loss:     1.570152, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-08-04 10:29:51,485 - INFO - joeynmt.training - Epoch   7, Step:   268600, Batch Loss:     1.584963, Tokens per Sec:    16053, Lr: 0.000300\n",
      "2021-08-04 10:30:18,747 - INFO - joeynmt.training - Epoch   7, Step:   268800, Batch Loss:     1.633204, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-08-04 10:30:46,088 - INFO - joeynmt.training - Epoch   7, Step:   269000, Batch Loss:     1.903668, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-08-04 10:31:13,817 - INFO - joeynmt.training - Epoch   7, Step:   269200, Batch Loss:     1.540762, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-08-04 10:31:41,288 - INFO - joeynmt.training - Epoch   7, Step:   269400, Batch Loss:     1.887735, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-04 10:32:08,744 - INFO - joeynmt.training - Epoch   7, Step:   269600, Batch Loss:     1.689803, Tokens per Sec:    16156, Lr: 0.000300\n",
      "2021-08-04 10:32:36,164 - INFO - joeynmt.training - Epoch   7, Step:   269800, Batch Loss:     1.750745, Tokens per Sec:    15731, Lr: 0.000300\n",
      "2021-08-04 10:33:03,764 - INFO - joeynmt.training - Epoch   7, Step:   270000, Batch Loss:     1.535090, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-08-04 10:34:38,431 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 10:34:38,431 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 10:34:38,432 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 10:34:40,511 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 10:34:40,511 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 10:34:40,512 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 10:34:40,512 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , so good for me . ”\n",
      "2021-08-04 10:34:40,512 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 10:34:40,512 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 10:34:40,513 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 10:34:40,513 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our rest is in fear of Jehovah , greater than many treasures there is standing . ”\n",
      "2021-08-04 10:34:40,513 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 10:34:40,514 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 10:34:40,514 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 10:34:40,514 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel .\n",
      "2021-08-04 10:34:40,514 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 10:34:40,515 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 10:34:40,515 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 10:34:40,515 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 10:34:40,516 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   270000: bleu:  26.99, loss: 194974.9375, ppl:   4.8127, duration: 96.7511s\n",
      "2021-08-04 10:35:08,265 - INFO - joeynmt.training - Epoch   7, Step:   270200, Batch Loss:     1.747134, Tokens per Sec:    15750, Lr: 0.000300\n",
      "2021-08-04 10:35:35,475 - INFO - joeynmt.training - Epoch   7, Step:   270400, Batch Loss:     1.773721, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-08-04 10:36:02,797 - INFO - joeynmt.training - Epoch   7, Step:   270600, Batch Loss:     1.582670, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-08-04 10:36:29,977 - INFO - joeynmt.training - Epoch   7, Step:   270800, Batch Loss:     1.662891, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-08-04 10:36:57,333 - INFO - joeynmt.training - Epoch   7, Step:   271000, Batch Loss:     1.706055, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-08-04 10:37:24,914 - INFO - joeynmt.training - Epoch   7, Step:   271200, Batch Loss:     1.665763, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-08-04 10:37:52,150 - INFO - joeynmt.training - Epoch   7, Step:   271400, Batch Loss:     1.652331, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-04 10:38:19,708 - INFO - joeynmt.training - Epoch   7, Step:   271600, Batch Loss:     1.748269, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-08-04 10:38:47,050 - INFO - joeynmt.training - Epoch   7, Step:   271800, Batch Loss:     1.667192, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-04 10:39:14,277 - INFO - joeynmt.training - Epoch   7, Step:   272000, Batch Loss:     1.485372, Tokens per Sec:    15713, Lr: 0.000300\n",
      "2021-08-04 10:39:41,781 - INFO - joeynmt.training - Epoch   7, Step:   272200, Batch Loss:     1.769732, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-08-04 10:40:09,078 - INFO - joeynmt.training - Epoch   7, Step:   272400, Batch Loss:     1.699364, Tokens per Sec:    15912, Lr: 0.000300\n",
      "2021-08-04 10:40:36,232 - INFO - joeynmt.training - Epoch   7, Step:   272600, Batch Loss:     1.613425, Tokens per Sec:    15956, Lr: 0.000300\n",
      "2021-08-04 10:41:03,646 - INFO - joeynmt.training - Epoch   7, Step:   272800, Batch Loss:     3.069981, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-08-04 10:41:31,048 - INFO - joeynmt.training - Epoch   7, Step:   273000, Batch Loss:     1.692603, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-08-04 10:41:55,902 - INFO - joeynmt.training - Epoch   7: total training loss 9410.05\n",
      "2021-08-04 10:41:55,902 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-04 10:41:59,178 - INFO - joeynmt.training - Epoch   8, Step:   273200, Batch Loss:     1.658125, Tokens per Sec:    12683, Lr: 0.000300\n",
      "2021-08-04 10:42:26,255 - INFO - joeynmt.training - Epoch   8, Step:   273400, Batch Loss:     1.571440, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-08-04 10:42:53,430 - INFO - joeynmt.training - Epoch   8, Step:   273600, Batch Loss:     1.827506, Tokens per Sec:    16156, Lr: 0.000300\n",
      "2021-08-04 10:43:21,113 - INFO - joeynmt.training - Epoch   8, Step:   273800, Batch Loss:     1.625484, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-04 10:43:48,484 - INFO - joeynmt.training - Epoch   8, Step:   274000, Batch Loss:     1.818484, Tokens per Sec:    16122, Lr: 0.000300\n",
      "2021-08-04 10:44:16,157 - INFO - joeynmt.training - Epoch   8, Step:   274200, Batch Loss:     1.383365, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-08-04 10:44:43,595 - INFO - joeynmt.training - Epoch   8, Step:   274400, Batch Loss:     1.533904, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-08-04 10:45:11,024 - INFO - joeynmt.training - Epoch   8, Step:   274600, Batch Loss:     1.671340, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-04 10:45:38,639 - INFO - joeynmt.training - Epoch   8, Step:   274800, Batch Loss:     1.560737, Tokens per Sec:    15766, Lr: 0.000300\n",
      "2021-08-04 10:46:06,276 - INFO - joeynmt.training - Epoch   8, Step:   275000, Batch Loss:     1.776994, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-08-04 10:47:41,008 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 10:47:41,009 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 10:47:41,009 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 10:47:43,006 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 10:47:43,006 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 10:47:43,007 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 10:47:43,007 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , I have drawn close to God , so good is good for me . ”\n",
      "2021-08-04 10:47:43,007 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 10:47:43,007 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 10:47:43,008 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 10:47:43,008 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our residents are in fear of Jehovah , greater than many treasures are standing . ”\n",
      "2021-08-04 10:47:43,008 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 10:47:43,008 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 10:47:43,008 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 10:47:43,009 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 10:47:43,009 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 10:47:43,009 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 10:47:43,009 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 10:47:43,010 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-04 10:47:43,010 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   275000: bleu:  27.24, loss: 194520.6719, ppl:   4.7951, duration: 96.7336s\n",
      "2021-08-04 10:48:10,739 - INFO - joeynmt.training - Epoch   8, Step:   275200, Batch Loss:     1.782519, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-08-04 10:48:38,141 - INFO - joeynmt.training - Epoch   8, Step:   275400, Batch Loss:     1.693034, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-08-04 10:49:05,581 - INFO - joeynmt.training - Epoch   8, Step:   275600, Batch Loss:     1.599990, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-08-04 10:49:32,991 - INFO - joeynmt.training - Epoch   8, Step:   275800, Batch Loss:     1.501444, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-08-04 10:50:00,303 - INFO - joeynmt.training - Epoch   8, Step:   276000, Batch Loss:     1.455826, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-08-04 10:50:27,873 - INFO - joeynmt.training - Epoch   8, Step:   276200, Batch Loss:     1.603191, Tokens per Sec:    15769, Lr: 0.000300\n",
      "2021-08-04 10:50:55,287 - INFO - joeynmt.training - Epoch   8, Step:   276400, Batch Loss:     1.713074, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-08-04 10:51:22,650 - INFO - joeynmt.training - Epoch   8, Step:   276600, Batch Loss:     1.624578, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-08-04 10:51:49,766 - INFO - joeynmt.training - Epoch   8, Step:   276800, Batch Loss:     1.797412, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-08-04 10:52:17,390 - INFO - joeynmt.training - Epoch   8, Step:   277000, Batch Loss:     1.733129, Tokens per Sec:    15731, Lr: 0.000300\n",
      "2021-08-04 10:52:44,679 - INFO - joeynmt.training - Epoch   8, Step:   277200, Batch Loss:     1.534353, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-08-04 10:53:12,461 - INFO - joeynmt.training - Epoch   8, Step:   277400, Batch Loss:     1.723317, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-08-04 10:53:39,921 - INFO - joeynmt.training - Epoch   8, Step:   277600, Batch Loss:     1.417341, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-08-04 10:54:07,388 - INFO - joeynmt.training - Epoch   8, Step:   277800, Batch Loss:     1.607198, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-08-04 10:54:34,495 - INFO - joeynmt.training - Epoch   8, Step:   278000, Batch Loss:     1.889847, Tokens per Sec:    15739, Lr: 0.000300\n",
      "2021-08-04 10:55:01,857 - INFO - joeynmt.training - Epoch   8, Step:   278200, Batch Loss:     2.142779, Tokens per Sec:    16114, Lr: 0.000300\n",
      "2021-08-04 10:55:29,423 - INFO - joeynmt.training - Epoch   8, Step:   278400, Batch Loss:     1.761190, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-08-04 10:55:57,283 - INFO - joeynmt.training - Epoch   8, Step:   278600, Batch Loss:     1.673734, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-08-04 10:56:03,912 - INFO - joeynmt.training - Epoch   8: total training loss 9408.45\n",
      "2021-08-04 10:56:03,913 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-04 10:56:25,220 - INFO - joeynmt.training - Epoch   9, Step:   278800, Batch Loss:     1.772465, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-08-04 10:56:52,809 - INFO - joeynmt.training - Epoch   9, Step:   279000, Batch Loss:     2.073395, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-08-04 10:57:20,400 - INFO - joeynmt.training - Epoch   9, Step:   279200, Batch Loss:     1.813789, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-08-04 10:57:48,061 - INFO - joeynmt.training - Epoch   9, Step:   279400, Batch Loss:     1.920538, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-08-04 10:58:15,411 - INFO - joeynmt.training - Epoch   9, Step:   279600, Batch Loss:     1.529250, Tokens per Sec:    15608, Lr: 0.000300\n",
      "2021-08-04 10:58:42,753 - INFO - joeynmt.training - Epoch   9, Step:   279800, Batch Loss:     1.619415, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-04 10:59:10,246 - INFO - joeynmt.training - Epoch   9, Step:   280000, Batch Loss:     1.751105, Tokens per Sec:    15817, Lr: 0.000300\n",
      "2021-08-04 11:00:45,261 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 11:00:45,261 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 11:00:45,261 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 11:00:46,483 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 11:00:46,483 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 11:00:47,207 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 11:00:47,209 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 11:00:47,209 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 11:00:47,209 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , I , approach God is good for me . ”\n",
      "2021-08-04 11:00:47,210 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 11:00:47,210 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 11:00:47,211 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 11:00:47,211 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our residents are in fear of Jehovah , greater than many riches that are standing . ”\n",
      "2021-08-04 11:00:47,211 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 11:00:47,212 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 11:00:47,212 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 11:00:47,215 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 11:00:47,215 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 11:00:47,216 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 11:00:47,216 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 11:00:47,216 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 11:00:47,216 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   280000: bleu:  27.21, loss: 193428.9219, ppl:   4.7531, duration: 96.9699s\n",
      "2021-08-04 11:01:14,861 - INFO - joeynmt.training - Epoch   9, Step:   280200, Batch Loss:     1.738524, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-08-04 11:01:42,488 - INFO - joeynmt.training - Epoch   9, Step:   280400, Batch Loss:     1.870537, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-08-04 11:02:09,892 - INFO - joeynmt.training - Epoch   9, Step:   280600, Batch Loss:     1.567332, Tokens per Sec:    15921, Lr: 0.000300\n",
      "2021-08-04 11:02:37,302 - INFO - joeynmt.training - Epoch   9, Step:   280800, Batch Loss:     1.797917, Tokens per Sec:    16219, Lr: 0.000300\n",
      "2021-08-04 11:03:04,747 - INFO - joeynmt.training - Epoch   9, Step:   281000, Batch Loss:     1.527281, Tokens per Sec:    15768, Lr: 0.000300\n",
      "2021-08-04 11:03:31,814 - INFO - joeynmt.training - Epoch   9, Step:   281200, Batch Loss:     1.743268, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-08-04 11:03:58,888 - INFO - joeynmt.training - Epoch   9, Step:   281400, Batch Loss:     1.677851, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-08-04 11:04:26,466 - INFO - joeynmt.training - Epoch   9, Step:   281600, Batch Loss:     1.630870, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-08-04 11:04:53,453 - INFO - joeynmt.training - Epoch   9, Step:   281800, Batch Loss:     1.958192, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-04 11:05:20,885 - INFO - joeynmt.training - Epoch   9, Step:   282000, Batch Loss:     1.810012, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-08-04 11:05:47,872 - INFO - joeynmt.training - Epoch   9, Step:   282200, Batch Loss:     1.633185, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-04 11:06:15,088 - INFO - joeynmt.training - Epoch   9, Step:   282400, Batch Loss:     1.571386, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-08-04 11:06:42,249 - INFO - joeynmt.training - Epoch   9, Step:   282600, Batch Loss:     1.723180, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-08-04 11:07:09,342 - INFO - joeynmt.training - Epoch   9, Step:   282800, Batch Loss:     1.519781, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-08-04 11:07:36,336 - INFO - joeynmt.training - Epoch   9, Step:   283000, Batch Loss:     1.757351, Tokens per Sec:    16132, Lr: 0.000300\n",
      "2021-08-04 11:08:03,542 - INFO - joeynmt.training - Epoch   9, Step:   283200, Batch Loss:     1.961151, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-08-04 11:08:30,501 - INFO - joeynmt.training - Epoch   9, Step:   283400, Batch Loss:     2.192583, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-08-04 11:08:57,559 - INFO - joeynmt.training - Epoch   9, Step:   283600, Batch Loss:     1.811316, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-08-04 11:09:24,841 - INFO - joeynmt.training - Epoch   9, Step:   283800, Batch Loss:     1.679731, Tokens per Sec:    16293, Lr: 0.000300\n",
      "2021-08-04 11:09:51,510 - INFO - joeynmt.training - Epoch   9, Step:   284000, Batch Loss:     1.680737, Tokens per Sec:    16143, Lr: 0.000300\n",
      "2021-08-04 11:10:07,963 - INFO - joeynmt.training - Epoch   9: total training loss 9384.31\n",
      "2021-08-04 11:10:07,963 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-04 11:10:19,698 - INFO - joeynmt.training - Epoch  10, Step:   284200, Batch Loss:     1.688579, Tokens per Sec:    15346, Lr: 0.000300\n",
      "2021-08-04 11:10:46,544 - INFO - joeynmt.training - Epoch  10, Step:   284400, Batch Loss:     1.686143, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-08-04 11:11:13,940 - INFO - joeynmt.training - Epoch  10, Step:   284600, Batch Loss:     1.590900, Tokens per Sec:    16136, Lr: 0.000300\n",
      "2021-08-04 11:11:41,004 - INFO - joeynmt.training - Epoch  10, Step:   284800, Batch Loss:     1.616517, Tokens per Sec:    16279, Lr: 0.000300\n",
      "2021-08-04 11:12:08,186 - INFO - joeynmt.training - Epoch  10, Step:   285000, Batch Loss:     1.566398, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-08-04 11:13:45,185 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 11:13:45,185 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 11:13:45,185 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 11:13:47,238 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 11:13:47,239 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 11:13:47,239 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 11:13:47,240 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who approach God , is good for me . ”\n",
      "2021-08-04 11:13:47,240 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 11:13:47,240 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 11:13:47,240 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 11:13:47,241 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our residents are fearing Jehovah , more than many treasures are standing . ”\n",
      "2021-08-04 11:13:47,241 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 11:13:47,241 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 11:13:47,241 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 11:13:47,242 - INFO - joeynmt.training - \tHypothesis: Cameron : Here are other prophecies in the book of Daniel that describe God’s Kingdom .\n",
      "2021-08-04 11:13:47,242 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 11:13:47,242 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 11:13:47,242 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 11:13:47,243 - INFO - joeynmt.training - \tHypothesis: In what way will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-04 11:13:47,243 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   285000: bleu:  27.29, loss: 193735.4375, ppl:   4.7649, duration: 99.0564s\n",
      "2021-08-04 11:14:14,805 - INFO - joeynmt.training - Epoch  10, Step:   285200, Batch Loss:     1.764283, Tokens per Sec:    15962, Lr: 0.000300\n",
      "2021-08-04 11:14:42,152 - INFO - joeynmt.training - Epoch  10, Step:   285400, Batch Loss:     1.500973, Tokens per Sec:    16334, Lr: 0.000300\n",
      "2021-08-04 11:15:09,343 - INFO - joeynmt.training - Epoch  10, Step:   285600, Batch Loss:     1.832925, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-08-04 11:15:36,354 - INFO - joeynmt.training - Epoch  10, Step:   285800, Batch Loss:     1.803707, Tokens per Sec:    16326, Lr: 0.000300\n",
      "2021-08-04 11:16:03,490 - INFO - joeynmt.training - Epoch  10, Step:   286000, Batch Loss:     1.620653, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-04 11:16:30,533 - INFO - joeynmt.training - Epoch  10, Step:   286200, Batch Loss:     1.807382, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-08-04 11:16:57,566 - INFO - joeynmt.training - Epoch  10, Step:   286400, Batch Loss:     1.797788, Tokens per Sec:    16421, Lr: 0.000300\n",
      "2021-08-04 11:17:25,023 - INFO - joeynmt.training - Epoch  10, Step:   286600, Batch Loss:     1.731239, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-08-04 11:17:52,078 - INFO - joeynmt.training - Epoch  10, Step:   286800, Batch Loss:     1.932317, Tokens per Sec:    15921, Lr: 0.000300\n",
      "2021-08-04 11:18:19,262 - INFO - joeynmt.training - Epoch  10, Step:   287000, Batch Loss:     1.901785, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-08-04 11:18:46,344 - INFO - joeynmt.training - Epoch  10, Step:   287200, Batch Loss:     1.628365, Tokens per Sec:    16321, Lr: 0.000300\n",
      "2021-08-04 11:19:13,567 - INFO - joeynmt.training - Epoch  10, Step:   287400, Batch Loss:     1.631028, Tokens per Sec:    16367, Lr: 0.000300\n",
      "2021-08-04 11:19:40,747 - INFO - joeynmt.training - Epoch  10, Step:   287600, Batch Loss:     1.883666, Tokens per Sec:    16295, Lr: 0.000300\n",
      "2021-08-04 11:20:08,134 - INFO - joeynmt.training - Epoch  10, Step:   287800, Batch Loss:     1.677609, Tokens per Sec:    15627, Lr: 0.000300\n",
      "2021-08-04 11:20:35,143 - INFO - joeynmt.training - Epoch  10, Step:   288000, Batch Loss:     1.732669, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-08-04 11:21:02,232 - INFO - joeynmt.training - Epoch  10, Step:   288200, Batch Loss:     1.881452, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-08-04 11:21:29,434 - INFO - joeynmt.training - Epoch  10, Step:   288400, Batch Loss:     1.767074, Tokens per Sec:    16141, Lr: 0.000300\n",
      "2021-08-04 11:21:56,328 - INFO - joeynmt.training - Epoch  10, Step:   288600, Batch Loss:     1.520225, Tokens per Sec:    16224, Lr: 0.000300\n",
      "2021-08-04 11:22:23,761 - INFO - joeynmt.training - Epoch  10, Step:   288800, Batch Loss:     1.680462, Tokens per Sec:    16141, Lr: 0.000300\n",
      "2021-08-04 11:22:50,748 - INFO - joeynmt.training - Epoch  10, Step:   289000, Batch Loss:     1.683336, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-08-04 11:23:17,699 - INFO - joeynmt.training - Epoch  10, Step:   289200, Batch Loss:     1.583039, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-08-04 11:23:44,923 - INFO - joeynmt.training - Epoch  10, Step:   289400, Batch Loss:     1.806771, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-08-04 11:24:08,619 - INFO - joeynmt.training - Epoch  10: total training loss 9346.85\n",
      "2021-08-04 11:24:08,620 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-04 11:24:12,602 - INFO - joeynmt.training - Epoch  11, Step:   289600, Batch Loss:     1.726925, Tokens per Sec:    13929, Lr: 0.000300\n",
      "2021-08-04 11:24:39,547 - INFO - joeynmt.training - Epoch  11, Step:   289800, Batch Loss:     1.633688, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-08-04 11:25:06,839 - INFO - joeynmt.training - Epoch  11, Step:   290000, Batch Loss:     1.540466, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-08-04 11:26:40,181 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 11:26:40,182 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 11:26:40,182 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 11:26:42,096 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 11:26:42,096 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 11:26:42,097 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 11:26:42,097 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I , who are approaching God , is good for me . ”\n",
      "2021-08-04 11:26:42,097 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 11:26:42,097 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 11:26:42,097 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 11:26:42,098 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our own areas are fearing Jehovah , more than many treasures are standing . ”\n",
      "2021-08-04 11:26:42,098 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 11:26:42,098 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 11:26:42,098 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 11:26:42,098 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel .\n",
      "2021-08-04 11:26:42,099 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 11:26:42,099 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 11:26:42,099 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 11:26:42,099 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely overcome ?\n",
      "2021-08-04 11:26:42,100 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   290000: bleu:  27.18, loss: 193765.5625, ppl:   4.7660, duration: 95.2607s\n",
      "2021-08-04 11:27:09,400 - INFO - joeynmt.training - Epoch  11, Step:   290200, Batch Loss:     1.749595, Tokens per Sec:    15599, Lr: 0.000300\n",
      "2021-08-04 11:27:36,554 - INFO - joeynmt.training - Epoch  11, Step:   290400, Batch Loss:     1.694374, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-08-04 11:28:03,635 - INFO - joeynmt.training - Epoch  11, Step:   290600, Batch Loss:     1.634513, Tokens per Sec:    16522, Lr: 0.000300\n",
      "2021-08-04 11:28:31,106 - INFO - joeynmt.training - Epoch  11, Step:   290800, Batch Loss:     1.802407, Tokens per Sec:    16276, Lr: 0.000300\n",
      "2021-08-04 11:28:58,326 - INFO - joeynmt.training - Epoch  11, Step:   291000, Batch Loss:     1.621256, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-08-04 11:29:25,388 - INFO - joeynmt.training - Epoch  11, Step:   291200, Batch Loss:     1.723121, Tokens per Sec:    16382, Lr: 0.000300\n",
      "2021-08-04 11:29:52,484 - INFO - joeynmt.training - Epoch  11, Step:   291400, Batch Loss:     1.797758, Tokens per Sec:    16361, Lr: 0.000300\n",
      "2021-08-04 11:30:19,459 - INFO - joeynmt.training - Epoch  11, Step:   291600, Batch Loss:     1.596888, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-08-04 11:30:46,502 - INFO - joeynmt.training - Epoch  11, Step:   291800, Batch Loss:     1.694430, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-08-04 11:31:13,789 - INFO - joeynmt.training - Epoch  11, Step:   292000, Batch Loss:     2.095829, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-08-04 11:31:40,695 - INFO - joeynmt.training - Epoch  11, Step:   292200, Batch Loss:     1.863625, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-08-04 11:32:07,601 - INFO - joeynmt.training - Epoch  11, Step:   292400, Batch Loss:     1.665204, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-08-04 11:32:34,622 - INFO - joeynmt.training - Epoch  11, Step:   292600, Batch Loss:     1.591592, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-08-04 11:33:01,659 - INFO - joeynmt.training - Epoch  11, Step:   292800, Batch Loss:     1.851297, Tokens per Sec:    16279, Lr: 0.000300\n",
      "2021-08-04 11:33:29,002 - INFO - joeynmt.training - Epoch  11, Step:   293000, Batch Loss:     1.617050, Tokens per Sec:    16233, Lr: 0.000300\n",
      "2021-08-04 11:33:55,871 - INFO - joeynmt.training - Epoch  11, Step:   293200, Batch Loss:     1.512126, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-08-04 11:34:23,302 - INFO - joeynmt.training - Epoch  11, Step:   293400, Batch Loss:     1.946655, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-08-04 11:34:50,413 - INFO - joeynmt.training - Epoch  11, Step:   293600, Batch Loss:     1.727027, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-08-04 11:35:17,391 - INFO - joeynmt.training - Epoch  11, Step:   293800, Batch Loss:     1.629219, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-08-04 11:35:44,556 - INFO - joeynmt.training - Epoch  11, Step:   294000, Batch Loss:     1.645886, Tokens per Sec:    16366, Lr: 0.000300\n",
      "2021-08-04 11:36:11,876 - INFO - joeynmt.training - Epoch  11, Step:   294200, Batch Loss:     1.578610, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-08-04 11:36:39,022 - INFO - joeynmt.training - Epoch  11, Step:   294400, Batch Loss:     1.670508, Tokens per Sec:    16431, Lr: 0.000300\n",
      "2021-08-04 11:37:06,049 - INFO - joeynmt.training - Epoch  11, Step:   294600, Batch Loss:     1.664172, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-08-04 11:37:33,050 - INFO - joeynmt.training - Epoch  11, Step:   294800, Batch Loss:     1.702853, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-08-04 11:38:00,039 - INFO - joeynmt.training - Epoch  11, Step:   295000, Batch Loss:     1.756130, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-08-04 11:39:34,770 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 11:39:34,771 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 11:39:34,771 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 11:39:36,093 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 11:39:36,093 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 11:39:36,874 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 11:39:36,874 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 11:39:36,875 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 11:39:36,875 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , so good for me . ”\n",
      "2021-08-04 11:39:36,875 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 11:39:36,876 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 11:39:36,876 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 11:39:36,876 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in the fear of Jehovah , greater than many treasures that are standing . ”\n",
      "2021-08-04 11:39:36,876 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 11:39:36,877 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 11:39:36,877 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 11:39:36,877 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that says about God’s Kingdom .\n",
      "2021-08-04 11:39:36,877 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 11:39:36,878 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 11:39:36,878 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 11:39:36,878 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 11:39:36,878 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   295000: bleu:  27.36, loss: 192783.3438, ppl:   4.7285, duration: 96.8390s\n",
      "2021-08-04 11:39:41,930 - INFO - joeynmt.training - Epoch  11: total training loss 9349.94\n",
      "2021-08-04 11:39:41,930 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-04 11:40:05,266 - INFO - joeynmt.training - Epoch  12, Step:   295200, Batch Loss:     1.613324, Tokens per Sec:    15733, Lr: 0.000300\n",
      "2021-08-04 11:40:32,530 - INFO - joeynmt.training - Epoch  12, Step:   295400, Batch Loss:     1.657132, Tokens per Sec:    16435, Lr: 0.000300\n",
      "2021-08-04 11:40:59,631 - INFO - joeynmt.training - Epoch  12, Step:   295600, Batch Loss:     1.853883, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-04 11:41:26,439 - INFO - joeynmt.training - Epoch  12, Step:   295800, Batch Loss:     1.713125, Tokens per Sec:    16169, Lr: 0.000300\n",
      "2021-08-04 11:41:53,705 - INFO - joeynmt.training - Epoch  12, Step:   296000, Batch Loss:     1.579350, Tokens per Sec:    16294, Lr: 0.000300\n",
      "2021-08-04 11:42:20,829 - INFO - joeynmt.training - Epoch  12, Step:   296200, Batch Loss:     1.666101, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-08-04 11:42:47,526 - INFO - joeynmt.training - Epoch  12, Step:   296400, Batch Loss:     1.745329, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-08-04 11:43:14,923 - INFO - joeynmt.training - Epoch  12, Step:   296600, Batch Loss:     1.724646, Tokens per Sec:    16371, Lr: 0.000300\n",
      "2021-08-04 11:43:41,904 - INFO - joeynmt.training - Epoch  12, Step:   296800, Batch Loss:     1.586007, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-04 11:44:08,789 - INFO - joeynmt.training - Epoch  12, Step:   297000, Batch Loss:     1.563423, Tokens per Sec:    16157, Lr: 0.000300\n",
      "2021-08-04 11:44:35,743 - INFO - joeynmt.training - Epoch  12, Step:   297200, Batch Loss:     2.241032, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-08-04 11:45:02,756 - INFO - joeynmt.training - Epoch  12, Step:   297400, Batch Loss:     1.688769, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-08-04 11:45:29,874 - INFO - joeynmt.training - Epoch  12, Step:   297600, Batch Loss:     1.674011, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-08-04 11:45:57,059 - INFO - joeynmt.training - Epoch  12, Step:   297800, Batch Loss:     1.677330, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-08-04 11:46:24,116 - INFO - joeynmt.training - Epoch  12, Step:   298000, Batch Loss:     1.650718, Tokens per Sec:    16195, Lr: 0.000300\n",
      "2021-08-04 11:46:51,164 - INFO - joeynmt.training - Epoch  12, Step:   298200, Batch Loss:     1.930586, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-08-04 11:47:18,376 - INFO - joeynmt.training - Epoch  12, Step:   298400, Batch Loss:     1.541914, Tokens per Sec:    16124, Lr: 0.000300\n",
      "2021-08-04 11:47:45,326 - INFO - joeynmt.training - Epoch  12, Step:   298600, Batch Loss:     1.742262, Tokens per Sec:    16291, Lr: 0.000300\n",
      "2021-08-04 11:48:12,524 - INFO - joeynmt.training - Epoch  12, Step:   298800, Batch Loss:     1.726274, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-08-04 11:48:39,667 - INFO - joeynmt.training - Epoch  12, Step:   299000, Batch Loss:     1.750714, Tokens per Sec:    16252, Lr: 0.000300\n",
      "2021-08-04 11:49:06,885 - INFO - joeynmt.training - Epoch  12, Step:   299200, Batch Loss:     1.812175, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-08-04 11:49:33,955 - INFO - joeynmt.training - Epoch  12, Step:   299400, Batch Loss:     1.762850, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-04 11:50:00,749 - INFO - joeynmt.training - Epoch  12, Step:   299600, Batch Loss:     1.745321, Tokens per Sec:    16252, Lr: 0.000300\n",
      "2021-08-04 11:50:27,918 - INFO - joeynmt.training - Epoch  12, Step:   299800, Batch Loss:     1.563587, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-08-04 11:50:55,075 - INFO - joeynmt.training - Epoch  12, Step:   300000, Batch Loss:     1.653860, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-08-04 11:52:28,811 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 11:52:28,811 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 11:52:28,811 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 11:52:30,020 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 11:52:30,020 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 11:52:30,739 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 11:52:30,740 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 11:52:30,740 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 11:52:30,740 - INFO - joeynmt.training - \tHypothesis: He sang : “ As for me , I have drawn close to God , so good for me . ”\n",
      "2021-08-04 11:52:30,740 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 11:52:30,742 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 11:52:30,742 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 11:52:30,742 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ Our own residents are in fear of Jehovah , greater than many treasures are standing . ”\n",
      "2021-08-04 11:52:30,742 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 11:52:30,743 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 11:52:30,743 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 11:52:30,743 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 11:52:30,743 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 11:52:30,744 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 11:52:30,744 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 11:52:30,744 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-04 11:52:30,745 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   300000: bleu:  27.43, loss: 192571.0000, ppl:   4.7204, duration: 95.6690s\n",
      "2021-08-04 11:52:57,799 - INFO - joeynmt.training - Epoch  12, Step:   300200, Batch Loss:     1.696726, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-08-04 11:53:25,161 - INFO - joeynmt.training - Epoch  12, Step:   300400, Batch Loss:     1.814703, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-08-04 11:53:38,381 - INFO - joeynmt.training - Epoch  12: total training loss 9341.78\n",
      "2021-08-04 11:53:38,382 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-04 11:53:52,475 - INFO - joeynmt.training - Epoch  13, Step:   300600, Batch Loss:     1.668117, Tokens per Sec:    15505, Lr: 0.000300\n",
      "2021-08-04 11:54:19,778 - INFO - joeynmt.training - Epoch  13, Step:   300800, Batch Loss:     1.803906, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-08-04 11:54:46,533 - INFO - joeynmt.training - Epoch  13, Step:   301000, Batch Loss:     1.721218, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-08-04 11:55:13,701 - INFO - joeynmt.training - Epoch  13, Step:   301200, Batch Loss:     1.850128, Tokens per Sec:    16140, Lr: 0.000300\n",
      "2021-08-04 11:55:40,835 - INFO - joeynmt.training - Epoch  13, Step:   301400, Batch Loss:     1.694286, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-04 11:56:07,869 - INFO - joeynmt.training - Epoch  13, Step:   301600, Batch Loss:     1.731686, Tokens per Sec:    16213, Lr: 0.000300\n",
      "2021-08-04 11:56:35,166 - INFO - joeynmt.training - Epoch  13, Step:   301800, Batch Loss:     1.739055, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-08-04 11:57:02,188 - INFO - joeynmt.training - Epoch  13, Step:   302000, Batch Loss:     1.596310, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-08-04 11:57:29,338 - INFO - joeynmt.training - Epoch  13, Step:   302200, Batch Loss:     1.641549, Tokens per Sec:    16455, Lr: 0.000300\n",
      "2021-08-04 11:57:56,780 - INFO - joeynmt.training - Epoch  13, Step:   302400, Batch Loss:     1.702640, Tokens per Sec:    16213, Lr: 0.000300\n",
      "2021-08-04 11:58:23,808 - INFO - joeynmt.training - Epoch  13, Step:   302600, Batch Loss:     1.816969, Tokens per Sec:    15865, Lr: 0.000300\n",
      "2021-08-04 11:58:50,920 - INFO - joeynmt.training - Epoch  13, Step:   302800, Batch Loss:     1.871227, Tokens per Sec:    16130, Lr: 0.000300\n",
      "2021-08-04 11:59:18,342 - INFO - joeynmt.training - Epoch  13, Step:   303000, Batch Loss:     1.699295, Tokens per Sec:    15926, Lr: 0.000300\n",
      "2021-08-04 11:59:45,324 - INFO - joeynmt.training - Epoch  13, Step:   303200, Batch Loss:     1.520940, Tokens per Sec:    16329, Lr: 0.000300\n",
      "2021-08-04 12:00:12,747 - INFO - joeynmt.training - Epoch  13, Step:   303400, Batch Loss:     1.649107, Tokens per Sec:    16314, Lr: 0.000300\n",
      "2021-08-04 12:00:39,900 - INFO - joeynmt.training - Epoch  13, Step:   303600, Batch Loss:     1.828832, Tokens per Sec:    16332, Lr: 0.000300\n",
      "2021-08-04 12:01:06,971 - INFO - joeynmt.training - Epoch  13, Step:   303800, Batch Loss:     1.656150, Tokens per Sec:    16244, Lr: 0.000300\n",
      "2021-08-04 12:01:34,113 - INFO - joeynmt.training - Epoch  13, Step:   304000, Batch Loss:     1.624102, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-08-04 12:02:01,338 - INFO - joeynmt.training - Epoch  13, Step:   304200, Batch Loss:     1.570417, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-08-04 12:02:28,087 - INFO - joeynmt.training - Epoch  13, Step:   304400, Batch Loss:     1.910505, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-08-04 12:02:55,228 - INFO - joeynmt.training - Epoch  13, Step:   304600, Batch Loss:     1.717172, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-08-04 12:03:22,283 - INFO - joeynmt.training - Epoch  13, Step:   304800, Batch Loss:     1.862116, Tokens per Sec:    16136, Lr: 0.000300\n",
      "2021-08-04 12:03:49,380 - INFO - joeynmt.training - Epoch  13, Step:   305000, Batch Loss:     1.811473, Tokens per Sec:    16233, Lr: 0.000300\n",
      "2021-08-04 12:05:24,333 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 12:05:24,334 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 12:05:24,334 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 12:05:26,557 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 12:05:26,558 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 12:05:26,558 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 12:05:26,558 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , so good for me . ”\n",
      "2021-08-04 12:05:26,558 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 12:05:26,559 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 12:05:26,559 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 12:05:26,559 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are in the fear of Jehovah , greater than many riches that are standing . ”\n",
      "2021-08-04 12:05:26,559 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 12:05:26,559 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 12:05:26,560 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 12:05:26,560 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 12:05:26,560 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 12:05:26,560 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 12:05:26,560 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 12:05:26,561 - INFO - joeynmt.training - \tHypothesis: How will Christ “ keep conquering ” and completely conquer ?\n",
      "2021-08-04 12:05:26,561 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   305000: bleu:  27.40, loss: 192581.7188, ppl:   4.7208, duration: 97.1803s\n",
      "2021-08-04 12:05:53,833 - INFO - joeynmt.training - Epoch  13, Step:   305200, Batch Loss:     1.710843, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-08-04 12:06:20,930 - INFO - joeynmt.training - Epoch  13, Step:   305400, Batch Loss:     1.805010, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-08-04 12:06:48,015 - INFO - joeynmt.training - Epoch  13, Step:   305600, Batch Loss:     1.984119, Tokens per Sec:    16231, Lr: 0.000300\n",
      "2021-08-04 12:07:14,786 - INFO - joeynmt.training - Epoch  13, Step:   305800, Batch Loss:     1.637361, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-04 12:07:37,364 - INFO - joeynmt.training - Epoch  13: total training loss 9327.02\n",
      "2021-08-04 12:07:37,364 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-04 12:07:42,401 - INFO - joeynmt.training - Epoch  14, Step:   306000, Batch Loss:     1.648643, Tokens per Sec:    14227, Lr: 0.000300\n",
      "2021-08-04 12:08:09,700 - INFO - joeynmt.training - Epoch  14, Step:   306200, Batch Loss:     1.764572, Tokens per Sec:    16249, Lr: 0.000300\n",
      "2021-08-04 12:08:36,544 - INFO - joeynmt.training - Epoch  14, Step:   306400, Batch Loss:     1.763412, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-08-04 12:09:03,676 - INFO - joeynmt.training - Epoch  14, Step:   306600, Batch Loss:     1.760327, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-04 12:09:30,756 - INFO - joeynmt.training - Epoch  14, Step:   306800, Batch Loss:     1.644353, Tokens per Sec:    16267, Lr: 0.000300\n",
      "2021-08-04 12:09:57,566 - INFO - joeynmt.training - Epoch  14, Step:   307000, Batch Loss:     1.885919, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-08-04 12:10:24,810 - INFO - joeynmt.training - Epoch  14, Step:   307200, Batch Loss:     1.816492, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-08-04 12:10:51,859 - INFO - joeynmt.training - Epoch  14, Step:   307400, Batch Loss:     1.752278, Tokens per Sec:    16252, Lr: 0.000300\n",
      "2021-08-04 12:11:19,029 - INFO - joeynmt.training - Epoch  14, Step:   307600, Batch Loss:     1.590357, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-08-04 12:11:46,137 - INFO - joeynmt.training - Epoch  14, Step:   307800, Batch Loss:     1.744537, Tokens per Sec:    16390, Lr: 0.000300\n",
      "2021-08-04 12:12:13,473 - INFO - joeynmt.training - Epoch  14, Step:   308000, Batch Loss:     1.617694, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-04 12:12:40,568 - INFO - joeynmt.training - Epoch  14, Step:   308200, Batch Loss:     1.723538, Tokens per Sec:    16056, Lr: 0.000300\n",
      "2021-08-04 12:13:07,724 - INFO - joeynmt.training - Epoch  14, Step:   308400, Batch Loss:     1.829230, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-08-04 12:13:34,892 - INFO - joeynmt.training - Epoch  14, Step:   308600, Batch Loss:     1.839699, Tokens per Sec:    16363, Lr: 0.000300\n",
      "2021-08-04 12:14:01,955 - INFO - joeynmt.training - Epoch  14, Step:   308800, Batch Loss:     1.581690, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-08-04 12:14:28,968 - INFO - joeynmt.training - Epoch  14, Step:   309000, Batch Loss:     1.851424, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-08-04 12:14:56,519 - INFO - joeynmt.training - Epoch  14, Step:   309200, Batch Loss:     2.037929, Tokens per Sec:    16552, Lr: 0.000300\n",
      "2021-08-04 12:15:23,740 - INFO - joeynmt.training - Epoch  14, Step:   309400, Batch Loss:     1.871779, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-08-04 12:15:50,617 - INFO - joeynmt.training - Epoch  14, Step:   309600, Batch Loss:     1.696143, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-08-04 12:16:17,923 - INFO - joeynmt.training - Epoch  14, Step:   309800, Batch Loss:     1.683472, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-08-04 12:16:44,718 - INFO - joeynmt.training - Epoch  14, Step:   310000, Batch Loss:     1.711965, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-08-04 12:18:19,062 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 12:18:19,062 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 12:18:19,062 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 12:18:20,282 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-04 12:18:20,282 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-04 12:18:21,276 - INFO - joeynmt.training - Example #0\n",
      "2021-08-04 12:18:21,276 - INFO - joeynmt.training - \tSource:     Yararirimbye ati “ ariko jyeweho kwegera Imana ni ko kwiza kuri jye ” .\n",
      "2021-08-04 12:18:21,276 - INFO - joeynmt.training - \tReference:  “ As for me , ” he sings , “ the drawing near to God is good for me . ”\n",
      "2021-08-04 12:18:21,277 - INFO - joeynmt.training - \tHypothesis: He sang : “ But I have been approaching God , so good for me . ”\n",
      "2021-08-04 12:18:21,277 - INFO - joeynmt.training - Example #1\n",
      "2021-08-04 12:18:21,277 - INFO - joeynmt.training - \tSource:     Umwami Salomo wa Isirayeli ya kera yaravuze ati “ uduke turimo kūbaha Uwiteka , turuta ubutunzi bwinshi burimo impagarara ” .\n",
      "2021-08-04 12:18:21,277 - INFO - joeynmt.training - \tReference:  “ Better is a little in the fear of Jehovah than an abundant supply and confusion along with it , ” says King Solomon of ancient Israel .\n",
      "2021-08-04 12:18:21,277 - INFO - joeynmt.training - \tHypothesis: King Solomon of ancient Israel said : “ We are living in fear of Jehovah , more than many riches that are standing . ”\n",
      "2021-08-04 12:18:21,278 - INFO - joeynmt.training - Example #2\n",
      "2021-08-04 12:18:21,278 - INFO - joeynmt.training - \tSource:     Kaniziyo : Dore ubundi buhanuzi bwo mu gitabo cya Daniyeli bugira icyo buvuga ku Bwami bw’Imana .\n",
      "2021-08-04 12:18:21,278 - INFO - joeynmt.training - \tReference:  Cameron : Here’s another prophecy in the book of Daniel that points to God’s Kingdom .\n",
      "2021-08-04 12:18:21,278 - INFO - joeynmt.training - \tHypothesis: Cameron : Consider another prophecy in the book of Daniel that describes God’s Kingdom .\n",
      "2021-08-04 12:18:21,278 - INFO - joeynmt.training - Example #3\n",
      "2021-08-04 12:18:21,279 - INFO - joeynmt.training - \tSource:     Ni mu buhe buryo Kristo ‘ azakomeza kunesha ’ kandi akanesha burundu ?\n",
      "2021-08-04 12:18:21,279 - INFO - joeynmt.training - \tReference:  How will Christ “ go on to victory ” and complete his conquest ?\n",
      "2021-08-04 12:18:21,279 - INFO - joeynmt.training - \tHypothesis: In what ways will Christ “ keep conquering ” and completely conquering ?\n",
      "2021-08-04 12:18:21,279 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   310000: bleu:  27.34, loss: 192079.4062, ppl:   4.7017, duration: 96.5610s\n",
      "2021-08-04 12:18:48,585 - INFO - joeynmt.training - Epoch  14, Step:   310200, Batch Loss:     1.704923, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-08-04 12:19:15,656 - INFO - joeynmt.training - Epoch  14, Step:   310400, Batch Loss:     1.712522, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-08-04 12:19:42,680 - INFO - joeynmt.training - Epoch  14, Step:   310600, Batch Loss:     1.785828, Tokens per Sec:    16189, Lr: 0.000300\n",
      "2021-08-04 12:20:09,767 - INFO - joeynmt.training - Epoch  14, Step:   310800, Batch Loss:     1.803667, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-08-04 12:20:36,631 - INFO - joeynmt.training - Epoch  14, Step:   311000, Batch Loss:     1.643514, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-08-04 12:21:03,607 - INFO - joeynmt.training - Epoch  14, Step:   311200, Batch Loss:     1.761132, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-04 12:21:30,705 - INFO - joeynmt.training - Epoch  14, Step:   311400, Batch Loss:     1.742626, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-04 12:21:34,483 - INFO - joeynmt.training - Epoch  14: total training loss 9310.78\n",
      "2021-08-04 12:21:34,483 - INFO - joeynmt.training - Training ended after  14 epochs.\n",
      "2021-08-04 12:21:34,483 - INFO - joeynmt.training - Best validation result (greedy) at step   310000:   4.70 ppl.\n",
      "2021-08-04 12:21:34,504 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 10000 (with beam_size)\n",
      "2021-08-04 12:21:34,850 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 12:21:35,032 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-04 12:21:35,103 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe.en)...\n",
      "2021-08-04 12:23:31,816 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-04 12:23:31,816 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-04 12:23:31,816 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-04 12:23:32,999 - INFO - joeynmt.prediction -  dev bleu[13a]:  27.88 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-04 12:23:33,007 - INFO - joeynmt.prediction - Translations saved to: models/rw_lhen_reverse_transformer_continued5/00310000.hyps.dev\n",
      "2021-08-04 12:23:33,007 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.en)...\n",
      "2021-08-04 12:23:35,933 - INFO - joeynmt.prediction - test bleu[13a]:  10.21 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-04 12:23:35,937 - INFO - joeynmt.prediction - Translations saved to: models/rw_lhen_reverse_transformer_continued5/00310000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload5.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3YRce6GCl4ru",
    "outputId": "22ce3450-635b-4232-beb5-e1c444c7d57d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-04 12:37:32,968 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-04 12:37:44,774 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 12:37:45,085 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/rw_lhen_reverse_transformer_continued5/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/translation1.bpe.lh_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hJFDFQ8l4rv",
    "outputId": "7576af55-8806-48d9-c9d4-fcca9f14c683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 10.2 42.2/15.7/6.2/3.0 (BP = 0.971 ratio = 0.972 hyp_len = 1956 ref_len = 2013)\n"
     ]
    }
   ],
   "source": [
    "!cat \"translation1.bpe.lh_en\" | sacrebleu \"test1.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nI9xeW4l4rw",
    "outputId": "b4e59cb1-2932-470f-db1a-fd33f1b6caba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-04 12:38:06,863 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-04 12:38:10,827 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-04 12:38:11,026 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/rw_lhen_reverse_transformer_continued5/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.rw\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/translation1.bpe.rw_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5gpf4Vrpl4ry",
    "outputId": "e81dd8a1-a478-49c1-8cd8-1d05c7b51496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
      "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 27.0 60.3/35.8/24.3/17.5 (BP = 0.872 ratio = 0.879 hyp_len = 74885 ref_len = 85182)\n"
     ]
    }
   ],
   "source": [
    "!cat \"translation1.bpe.rw_en\" | sacrebleu \"test2.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUX2lW8cLZBc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KinyarwandaLuhya_Multilingual_NMT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
