{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6cJZzBRP1-N"
   },
   "source": [
    "# Multilingual neural machine translation.\n",
    "\n",
    "For this case, we shall to a many-to-one translation:\n",
    "{Luganda, Luhya} to English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4V-O3nJPsAA",
    "outputId": "1006d3da-2ee2-420d-f743-6a6a1378bcfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Linking to drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcRP_CqbRQzj"
   },
   "outputs": [],
   "source": [
    "# Importing needed libraries for preprocessing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "grB3V9FhReiZ",
    "outputId": "b19289d7-ad48-41fc-f958-ec25f278d0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.8.0+cu101\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 763.5 MB 14 kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0+cu102\n",
      "    Uninstalling torch-1.9.0+cu102:\n",
      "      Successfully uninstalled torch-1.9.0+cu102\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.8.0+cu101\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "# Install Pytorch with GPU support v1.8.0.\n",
    "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7jAsiRLRlMs"
   },
   "outputs": [],
   "source": [
    "# Filtering warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaH6F-u3RrAb"
   },
   "outputs": [],
   "source": [
    "# Loading the drive\n",
    "import os\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH_IYggrTJKa"
   },
   "outputs": [],
   "source": [
    "# Setting source and target languages\n",
    "source_language = \"en\"\n",
    "target_language = \"lg_lh\"\n",
    "\n",
    "os.environ[\"src\"] = source_language \n",
    "os.environ[\"tgt\"] = target_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G0mZmUETh-A"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "VI9NWxchThje",
    "outputId": "a6f5ca6c-6e54-41cb-842c-0af1cd46fae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Luganda/train.bpe.en <==\n",
      "Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "At last , I have the st@@ able family life that I always cr@@ av@@ ed , and I have the loving Father that I always wanted .\n",
      "I was a new husband , only 25 years old and very in@@ experienced , but off we went with confidence in Jehovah .\n",
      "What can you do to show these de@@ a@@ f brothers personal attention ?\n",
      "R@@ ef@@ er@@ r@@ ing to what the rul@@ er@@ ship of God’s Son will accompl@@ ish , Isaiah 9 : 7 says : “ The very z@@ eal of Jehovah of arm@@ ies will do this . ”\n",
      "Jesus is the m@@ igh@@ ti@@ est of all of Jehovah’s spirit sons .\n",
      "The ste@@ ad@@ f@@ ast example set by J@@ ac@@ o@@ b and R@@ ac@@ he@@ l no doubt had a powerful effect on their son Joseph , influ@@ enc@@ ing how he would hand@@ le t@@ ests of his own faith .\n",
      "When s@@ ent@@ enc@@ ing “ the orig@@ in@@ al ser@@ p@@ ent , ” Satan the Devil , God said : “ I shall put en@@ m@@ ity between you and the woman and between your se@@ ed and her se@@ ed . He will br@@ u@@ ise you in the head and you will br@@ u@@ ise him in the h@@ ee@@ l . ”\n",
      "W@@ ill this or@@ de@@ al bring David down to S@@ he@@ ol in g@@ ri@@ ef and dis@@ gr@@ ace ?\n",
      "How can Christian love help to streng@@ then the marriage b@@ ond ?\n",
      "\n",
      "==> Luganda/train.bpe.lg <==\n",
      "Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "N@@ ze ne mukyala wange tuli basanyufu , era nk@@ imanyi nti Katonda anj@@ agala nnyo .\n",
      "Mu kiseera ekyo , nnali nn@@ aak@@ aw@@ asa , nga ndi wa myaka 25 gy@@ okka , era nga s@@ ir@@ ina b@@ um@@ any@@ irivu . Naye nnali muk@@ akafu nti Yakuwa yali ajja ku@@ nn@@ yamba .\n",
      "Mu kibiina k@@ yo bwe mu@@ ba nga mulimu bak@@ igg@@ ala , oyinza kukola ki okulaga nti o@@ faayo ku b’@@ oluganda abo ?\n",
      "Isaaya 9 : 7 wal@@ aga nti Omwana wa Katonda y@@ andibadde Kabaka era nti yand@@ ikol@@ edde abantu ebintu ebirungi bingi .\n",
      "Yesu y’@@ as@@ inga obuyinza mu ba@@ ana ba Yakuwa bonna ab’@@ omwoyo .\n",
      "Eky@@ okulabirako ekirungi Yakobo ne L@@ aak@@ e@@ eri kye baat@@ ek@@ awo mu kw@@ oleka obug@@ umiikiriza ky@@ akwata nnyo ku mutabani waabwe Yusufu , era ekyo ky@@ amu@@ yamba nnyo bwe yay@@ olekagana n’@@ embeera ez@@ aag@@ ez@@ esa okukkiriza kwe .\n",
      "Bwe yali as@@ alira omusango “ omus@@ ot@@ a ogw’@@ edda , ” Setaani Omulyolyomi , Katonda yagamba : “ O@@ bul@@ abe n’@@ abu@@ teek@@ anga wakati wo n’@@ omukazi , era ne wakati w’@@ ez@@ zadde l@@ yo n’@@ ez@@ zadde ly’@@ omukazi : ( ez@@ zadde ly’@@ omukazi ) l@@ iri@@ ku@@ be@@ t@@ ent@@ a omutwe , naawe ol@@ ir@@ ib@@ et@@ ent@@ a ekis@@ inzi@@ iro . ”\n",
      "Em@@ beera eno en@@ zibu en@@ e@@ er@@ eetera Dawudi okuk@@ ka em@@ ag@@ om@@ be nga mun@@ aku@@ w@@ avu ?\n",
      "Okwagala kw’@@ Ekikristaayo ku@@ yinza kutya okuny@@ weza obufumbo ?\n",
      "\n",
      "==> Luganda/train.en <==\n",
      "Eventually , however , the truths I learned from the Bible began to sink deeper into my heart . I realized that if I wanted to serve Jehovah , I had to change my political viewpoints and associations .\n",
      "At last , I have the stable family life that I always craved , and I have the loving Father that I always wanted .\n",
      "I was a new husband , only 25 years old and very inexperienced , but off we went with confidence in Jehovah .\n",
      "What can you do to show these deaf brothers personal attention ?\n",
      "Referring to what the rulership of God’s Son will accomplish , Isaiah 9 : 7 says : “ The very zeal of Jehovah of armies will do this . ”\n",
      "Jesus is the mightiest of all of Jehovah’s spirit sons .\n",
      "The steadfast example set by Jacob and Rachel no doubt had a powerful effect on their son Joseph , influencing how he would handle tests of his own faith .\n",
      "When sentencing “ the original serpent , ” Satan the Devil , God said : “ I shall put enmity between you and the woman and between your seed and her seed . He will bruise you in the head and you will bruise him in the heel . ”\n",
      "Will this ordeal bring David down to Sheol in grief and disgrace ?\n",
      "How can Christian love help to strengthen the marriage bond ?\n",
      "\n",
      "==> Luganda/train.lg <==\n",
      "Naye oluvannyuma lw’ekiseera , nnatandika okukolera ku mazima ge nnali njiga , era nnakiraba nti okusobola okuweereza Yakuwa nnalina okuva mu by’obufuzi n’okuleka emikwano emibi gye nnalina .\n",
      "Nze ne mukyala wange tuli basanyufu , era nkimanyi nti Katonda anjagala nnyo .\n",
      "Mu kiseera ekyo , nnali nnaakawasa , nga ndi wa myaka 25 gyokka , era nga sirina bumanyirivu . Naye nnali mukakafu nti Yakuwa yali ajja kunnyamba .\n",
      "Mu kibiina kyo bwe muba nga mulimu bakiggala , oyinza kukola ki okulaga nti ofaayo ku b’oluganda abo ?\n",
      "Isaaya 9 : 7 walaga nti Omwana wa Katonda yandibadde Kabaka era nti yandikoledde abantu ebintu ebirungi bingi .\n",
      "Yesu y’asinga obuyinza mu baana ba Yakuwa bonna ab’omwoyo .\n",
      "Ekyokulabirako ekirungi Yakobo ne Laakeeri kye baatekawo mu kwoleka obugumiikiriza kyakwata nnyo ku mutabani waabwe Yusufu , era ekyo kyamuyamba nnyo bwe yayolekagana n’embeera ezaagezesa okukkiriza kwe .\n",
      "Bwe yali asalira omusango “ omusota ogw’edda , ” Setaani Omulyolyomi , Katonda yagamba : “ Obulabe n’abuteekanga wakati wo n’omukazi , era ne wakati w’ezzadde lyo n’ezzadde ly’omukazi : ( ezzadde ly’omukazi ) lirikubetenta omutwe , naawe oliribetenta ekisinziiro . ”\n",
      "Embeera eno enzibu eneereetera Dawudi okukka emagombe nga munakuwavu ?\n",
      "Okwagala kw’Ekikristaayo kuyinza kutya okunyweza obufumbo ?\n",
      "==> Luganda/dev.bpe.en <==\n",
      "How important is the Kingdom message ?\n",
      "▪ How Sh@@ ould We “ An@@ s@@ w@@ er E@@ ach Per@@ son ” ?\n",
      "* Jesus made another covenant with them to rule together with him in his Kingdom .\n",
      "How , then , can we keep clean in a world that is mor@@ ally un@@ clean ?\n",
      "It was how they were tre@@ ated .\n",
      "Mat@@ eri@@ al@@ ism may not see@@ m to be an is@@ sue of lo@@ yal@@ ty , but it is .\n",
      "In any case , Psalm 9@@ 0 shows that the life of im@@ perfect humans is sh@@ ort .\n",
      "How can Jesus ’ example help us to de@@ al with the fl@@ aws of others ?\n",
      "E@@ li@@ j@@ ah did not as@@ c@@ end to the heavens that are the spiritual dw@@ ell@@ ing place of Jehovah and his ang@@ el@@ ic sons .\n",
      "That we sim@@ ply cannot s@@ oci@@ al@@ ize with un@@ believers and hope to suff@@ er no ill con@@ se@@ qu@@ ences .\n",
      "\n",
      "==> Luganda/dev.bpe.lg <==\n",
      "Obu@@ baka bw’@@ Obwakabaka bu@@ kulu kw@@ enk@@ ana wa ?\n",
      "▪ Tuyinza T@@ ut@@ ya “ Oku@@ ddamu Buli M@@ untu ” ?\n",
      "* Yesu yakola end@@ agaano endala nabo basobole okufu@@ g@@ ira awamu naye mu Bwakabaka bwe .\n",
      "Kati olwo tuyinza tutya okusigala nga tuli bay@@ on@@ jo mu nsi et@@ ali nn@@ yon@@ jo mu bya mp@@ isa ?\n",
      "Okus@@ ingira ddala engeri gye baali bay@@ is@@ ib@@ wamu ye y@@ anny@@ amba okuk@@ iraba nti be bantu ba Katonda .\n",
      "Okuba n’@@ omwoyo gw’@@ okwagala ebintu kir@@ emesa omuntu okuba omwesigwa , wadde nga kino si kyangu kulaba .\n",
      "Mu buli ngeri , Zabbuli 9@@ 0 eraga nti obulamu bw’@@ abantu abat@@ atuukiridde b@@ um@@ pi .\n",
      "Tuyinza tutya okukoppa Yesu mu ngeri gye tuy@@ is@@ aamu abalala nga bak@@ oze ensobi ?\n",
      "Eri@@ ya te@@ yag@@ enda mu ggulu Yakuwa awamu ne bamalayika gye bab@@ eera .\n",
      "B@@ itu@@ yigiriza nti te@@ tusobola kwe@@ wala ku@@ gwa mu mit@@ aw@@ aana singa tukola omuk@@ wano ku bantu abat@@ ali bakkiriza .\n",
      "\n",
      "==> Luganda/dev.en <==\n",
      "How important is the Kingdom message ?\n",
      "▪ How Should We “ Answer Each Person ” ?\n",
      "* Jesus made another covenant with them to rule together with him in his Kingdom .\n",
      "How , then , can we keep clean in a world that is morally unclean ?\n",
      "It was how they were treated .\n",
      "Materialism may not seem to be an issue of loyalty , but it is .\n",
      "In any case , Psalm 90 shows that the life of imperfect humans is short .\n",
      "How can Jesus ’ example help us to deal with the flaws of others ?\n",
      "Elijah did not ascend to the heavens that are the spiritual dwelling place of Jehovah and his angelic sons .\n",
      "That we simply cannot socialize with unbelievers and hope to suffer no ill consequences .\n",
      "\n",
      "==> Luganda/dev.lg <==\n",
      "Obubaka bw’Obwakabaka bukulu kwenkana wa ?\n",
      "▪ Tuyinza Tutya “ Okuddamu Buli Muntu ” ?\n",
      "* Yesu yakola endagaano endala nabo basobole okufugira awamu naye mu Bwakabaka bwe .\n",
      "Kati olwo tuyinza tutya okusigala nga tuli bayonjo mu nsi etali nnyonjo mu bya mpisa ?\n",
      "Okusingira ddala engeri gye baali bayisibwamu ye yannyamba okukiraba nti be bantu ba Katonda .\n",
      "Okuba n’omwoyo gw’okwagala ebintu kiremesa omuntu okuba omwesigwa , wadde nga kino si kyangu kulaba .\n",
      "Mu buli ngeri , Zabbuli 90 eraga nti obulamu bw’abantu abatatuukiridde bumpi .\n",
      "Tuyinza tutya okukoppa Yesu mu ngeri gye tuyisaamu abalala nga bakoze ensobi ?\n",
      "Eriya teyagenda mu ggulu Yakuwa awamu ne bamalayika gye babeera .\n",
      "Bituyigiriza nti tetusobola kwewala kugwa mu mitawaana singa tukola omukwano ku bantu abatali bakkiriza .\n"
     ]
    }
   ],
   "source": [
    "! head Luganda/train.*\n",
    "! head Luganda/dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "IP5nJ822UGJM",
    "outputId": "70dc6028-37fa-473f-e6be-00d9e306f2d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Luhya/train.bpe.en <==\n",
      "Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "If anyone th@@ in@@ ks himself to be a prophet or spiritual , let him ac@@ knowledge that the things which I write to you are the commandments of the Lord .\n",
      "E@@ very br@@ an@@ ch in Me that does not bear fruit He tak@@ es away ; and every br@@ an@@ ch that be@@ ars fruit He pr@@ un@@ es , that it may bear more fruit .\n",
      "D@@ em@@ et@@ ri@@ us has a good testimony from all , and from the truth its@@ el@@ f . And we also bear witness , and you know that our testimony is true .\n",
      "And supp@@ er being ended , the devil having already put it into the heart of Judas Is@@ c@@ ari@@ ot , Simon ’ s son , to betr@@ ay Him ,\n",
      "im@@ pl@@ or@@ ing us with much ur@@ gen@@ cy that we would receive the gift and the fel@@ low@@ ship of the minis@@ ter@@ ing to the saints .\n",
      "It is written in the prophets , ‘ And they shall all be taught by G@@ od@@ . ’ Therefore everyone who has heard and lear@@ ned from the Father comes to Me .\n",
      "For those who are such do not serve our Lord Jesus Christ , but their own bel@@ ly , and by sm@@ oo@@ th words and fl@@ at@@ ter@@ ing spe@@ e@@ ch dece@@ ive the hearts of the s@@ im@@ ple .\n",
      "So when he had received food , he was streng@@ th@@ ened . Then Saul sp@@ ent some days with the disciples at Damas@@ c@@ us .\n",
      "Therefore if you have not been faithful in the un@@ righteous m@@ am@@ m@@ on , who will comm@@ it to your tr@@ ust the true riches ?\n",
      "\n",
      "==> Luhya/train.bpe.lh <==\n",
      "Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "Omundu yesi ow@@ il@@ olanga mbu , no@@ mur@@ umwa , wa Nyasaye noho mbu ali neshi@@ haanwa eshia Roho okhuula amany@@ e khandi afuchil@@ ile mbu , aka , emu@@ handi@@ chilanga kano nel@@ ilako elia Omwami .\n",
      "A@@ rem@@ anga buli lis@@ aka , mwisie el@@ il@@ am@@ anga ebiamo ta , ne akh@@ alilanga buli lis@@ aka , eli@@ am@@ anga ebiamo kho mbu , li@@ be lil@@ ayi nil@@ im@@ e@@ eta okhw@@ ama , ebiamo ebinji .\n",
      "Buli mundu amw@@ itsoom@@ injia D@@ em@@ eter@@ io ; ne obw@@ atieli , bwene bu@@ mw@@ itsoom@@ injia . Ne nasi em@@ e@@ et@@ akhwo obuloli , bwanje , ne mumanyile mbu ak@@ emb@@ oola n@@ akatoto . Am@@ ash@@ esio K@@ okhumalil@@ isia ,\n",
      "Yesu nende abeechibe bali nib@@ ali@@ itsanga eshiokhulia , eshia ha@@ muk@@ ol@@ oba . Setani yali namalile okhur@@ a mu Yuda omwana wa Simoni Is@@ ik@@ ari@@ o@@ ti amapaaro k@@ okhukh@@ oba Yes@@ u. ,\n",
      "b@@ akhu@@ s@@ aba nib@@ akhu@@ saaya okhu@@ fuchil@@ ilwa okhus@@ anga , mukhu@@ khon@@ ya abakristo bashi@@ abwe aba Yudea .\n",
      "Ab@@ al@@ akusi , b@@ ahandika mbu , ‘ Buli mundu ali@@ eches@@ ibwa nende , Nyasaye. ’ Kho oyo yesi ou@@ hulilanga aka Papa nende , okhw@@ eka okhurula khuye , yetsa khwisie .\n",
      "Okhuba , abakholanga amakhuwa kario shib@@ akh@@ alaban@@ ilanga Kristo , Omwami wefwe ta , habula bakh@@ alaban@@ ilanga ts@@ inda , tsiabwe abeene . B@@ ekh@@ oonyelanga amakhuwa kabwe , k@@ okhu@@ ka@@ at@@ ilisia nende ak@@ okhul@@ aha khulwa okhu@@ ka@@ atia , amapaaro k@@ abat@@ eshele .\n",
      "Ne olwa yamala , okhulia eshiokhulia , omubilikwe kw@@ anyoola amaani . Saulo ayaala Injiili Damas@@ iko Saulo y@@ amenya Damas@@ iko halala nab@@ asuubili khulwa , tsinyanga tsind@@ i@@ iti .\n",
      "Kho , nimul@@ aba ab@@ esi@@ ikwa mubu@@ yinda , bw@@ omushialo shino ta , mwakh@@ aba mur@@ ie ab@@ esi@@ ikwa , mubu@@ yinda bwatoto ?\n",
      "\n",
      "==> Luhya/train.en <==\n",
      "Then Pilate entered the Praetorium again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "If anyone thinks himself to be a prophet or spiritual , let him acknowledge that the things which I write to you are the commandments of the Lord .\n",
      "Every branch in Me that does not bear fruit He takes away ; and every branch that bears fruit He prunes , that it may bear more fruit .\n",
      "Demetrius has a good testimony from all , and from the truth itself . And we also bear witness , and you know that our testimony is true .\n",
      "And supper being ended , the devil having already put it into the heart of Judas Iscariot , Simon ’ s son , to betray Him ,\n",
      "imploring us with much urgency that we would receive the gift and the fellowship of the ministering to the saints .\n",
      "It is written in the prophets , ‘ And they shall all be taught by God. ’ Therefore everyone who has heard and learned from the Father comes to Me .\n",
      "For those who are such do not serve our Lord Jesus Christ , but their own belly , and by smooth words and flattering speech deceive the hearts of the simple .\n",
      "So when he had received food , he was strengthened . Then Saul spent some days with the disciples at Damascus .\n",
      "Therefore if you have not been faithful in the unrighteous mammon , who will commit to your trust the true riches ?\n",
      "\n",
      "==> Luhya/train.lh <==\n",
      "Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "Omundu yesi owilolanga mbu , nomurumwa , wa Nyasaye noho mbu ali neshihaanwa eshia Roho okhuula amanye khandi afuchilile mbu , aka , emuhandichilanga kano nelilako elia Omwami .\n",
      "Aremanga buli lisaka , mwisie elilamanga ebiamo ta , ne akhalilanga buli lisaka , eliamanga ebiamo kho mbu , libe lilayi nilimeeta okhwama , ebiamo ebinji .\n",
      "Buli mundu amwitsoominjia Demeterio ; ne obwatieli , bwene bumwitsoominjia . Ne nasi emeetakhwo obuloli , bwanje , ne mumanyile mbu akemboola nakatoto . Amashesio Kokhumalilisia ,\n",
      "Yesu nende abeechibe bali nibaliitsanga eshiokhulia , eshia hamukoloba . Setani yali namalile okhura mu Yuda omwana wa Simoni Isikarioti amapaaro kokhukhoba Yesu. ,\n",
      "bakhusaba nibakhusaaya okhufuchililwa okhusanga , mukhukhonya abakristo bashiabwe aba Yudea .\n",
      "Abalakusi , bahandika mbu , ‘ Buli mundu aliechesibwa nende , Nyasaye. ’ Kho oyo yesi ouhulilanga aka Papa nende , okhweka okhurula khuye , yetsa khwisie .\n",
      "Okhuba , abakholanga amakhuwa kario shibakhalabanilanga Kristo , Omwami wefwe ta , habula bakhalabanilanga tsinda , tsiabwe abeene . Bekhoonyelanga amakhuwa kabwe , kokhukaatilisia nende akokhulaha khulwa okhukaatia , amapaaro kabateshele .\n",
      "Ne olwa yamala , okhulia eshiokhulia , omubilikwe kwanyoola amaani . Saulo ayaala Injiili Damasiko Saulo yamenya Damasiko halala nabasuubili khulwa , tsinyanga tsindiiti .\n",
      "Kho , nimulaba abesiikwa mubuyinda , bwomushialo shino ta , mwakhaba murie abesiikwa , mubuyinda bwatoto ?\n",
      "==> Luhya/dev.bpe.en <==\n",
      "and the car@@ es of this world , the dece@@ it@@ ful@@ ness of riches , and the des@@ ires for other things ent@@ ering in ch@@ oke the word , and it be@@ comes un@@ fru@@ it@@ ful .\n",
      "Now concerning the minis@@ ter@@ ing to the saints , it is su@@ per@@ fl@@ u@@ ous for me to write to you ;\n",
      "nor thi@@ ev@@ es , nor cov@@ et@@ ous , nor dr@@ un@@ kar@@ ds , nor rev@@ il@@ ers , nor ex@@ tor@@ ti@@ on@@ ers will inher@@ it the kingdom of God .\n",
      "Therefore , in the resurrection , when they rise , whose wife will she be ? For all seven had her as wife . ”\n",
      "Then he said , “ Lord , I believe ! ” And he worshiped Him .\n",
      "And many of them said , “ He has a dem@@ on and is m@@ ad . Why do you lis@@ ten to Him ? ”\n",
      "So we may bol@@ dly say : “ The Lord is my hel@@ per ; I will not fear@@ .@@ What can man do to me ? ”\n",
      "So when Jesus heard these things , He said to him , “ You still l@@ ack one thing . S@@ ell all that you have and dis@@ tri@@ bu@@ te to the poor , and you will have treas@@ ure in heaven ; and come , follow Me . ”\n",
      "Immediately he went up to Jesus and said , “ Gre@@ et@@ ings , R@@ ab@@ bi ! ” and k@@ is@@ sed Him .\n",
      "look@@ ing for the blessed hope and glor@@ i@@ ous appe@@ ar@@ ing of our great God and Savior Jesus Christ ,\n",
      "\n",
      "==> Luhya/dev.bpe.lh <==\n",
      "habula okhuh@@ end@@ ela amakhuwa k@@ omushialo okhu@@ chama obu@@ yinda nende obwikombi bw@@ ebindu b@@ indi b@@ inj@@ ilanga mub@@ o mana b@@ iti@@ iya likhuwa elo nib@@ ili@@ khola , okhu@@ lekha okhw@@ ama ebiamo .\n",
      "Shi@@ bulaho eshichila enz@@ ililile okhumu@@ handichila , khwi@@ khuwa li@@ obukhoonyi khub@@ ak@@ risto ba Yudea ,\n",
      "abe@@ ef@@ i abal@@ ang'@@ u , abam@@ e@@ esi , aba@@ chi@@ khanga abashi@@ abwe nende , ab@@ an@@ uuli , shibal@@ in@@ yoola obwami bwa Nyasaye tawe. ,\n",
      "Kho , khunyanga yo@@ bul@@ amu@@ sh@@ i , omukhasi oyo yakh@@ abe owa , wina khubandu musafu abo , shichila yali omukhasi owa , boosi musafu ? ”\n",
      "Naye naboola ari , “ Omwami , es@@ uub@@ i@@ il@@ e. ” Mana , nas@@ ik@@ ama hasi niy@@ enam@@ ila Yesu .\n",
      "Ab@@ anji khubo baboola bari , “ A@@ li neshi@@ shieno khandi nomul@@ al@@ u ! , E@@ shichila nimu@@ rech@@ er@@ esia ak@@ aboolanga nishi@@ na ? ”\n",
      "Kho ch@@ ende khum@@ e i@@ khol@@ o nikhu@@ boola , khur@@ i “ Omwami niye omu@@ hab@@ ini wanje shi@@ ndal@@ ar@@ itsanga tawe. , Omundu namu@@ ndu anyala okhukhola shiina ? ”\n",
      "Ne olwa Yesu yahulila ario , yamu@@ boolela ari “ O@@ shi@@ le@@ emil@@ ekhwo eshindu shilala eshi@@ okhukhola . T@@ sia , ok@@ usie ebindu biosi ebia oli ninabio , mana ok@@ ab@@ ile , abam@@ anani amapesa k@@ on@@ yool@@ amwo . Ne ol@@ aba nel@@ ib@@ i@@ ishil@@ o , mwikulu ; mana w@@ itse un@@ on@@ de@@ khwo . ”\n",
      "Yuda natsia shil@@ unj@@ i khu Yesu namuboolela ari “ Omul@@ embe ku@@ be khwiwe Omwechesia , ” ne , namu@@ f@@ umb@@ e@@ elela .\n",
      "Khw@@ its@@ u@@ ulemwo obunaayi nikhul@@ indililanga Inyanga y@@ okhukal@@ ukha khw@@ oluyali olwa , Nyasaye wefwe omukhongo nende Omu@@ honia wefwe Yesu , Kristo .\n",
      "\n",
      "==> Luhya/dev.en <==\n",
      "and the cares of this world , the deceitfulness of riches , and the desires for other things entering in choke the word , and it becomes unfruitful .\n",
      "Now concerning the ministering to the saints , it is superfluous for me to write to you ;\n",
      "nor thieves , nor covetous , nor drunkards , nor revilers , nor extortioners will inherit the kingdom of God .\n",
      "Therefore , in the resurrection , when they rise , whose wife will she be ? For all seven had her as wife . ”\n",
      "Then he said , “ Lord , I believe ! ” And he worshiped Him .\n",
      "And many of them said , “ He has a demon and is mad . Why do you listen to Him ? ”\n",
      "So we may boldly say : “ The Lord is my helper ; I will not fear.What can man do to me ? ”\n",
      "So when Jesus heard these things , He said to him , “ You still lack one thing . Sell all that you have and distribute to the poor , and you will have treasure in heaven ; and come , follow Me . ”\n",
      "Immediately he went up to Jesus and said , “ Greetings , Rabbi ! ” and kissed Him .\n",
      "looking for the blessed hope and glorious appearing of our great God and Savior Jesus Christ ,\n",
      "\n",
      "==> Luhya/dev.lh <==\n",
      "habula okhuhendela amakhuwa komushialo okhuchama obuyinda nende obwikombi bwebindu bindi binjilanga mubo mana bitiiya likhuwa elo nibilikhola , okhulekha okhwama ebiamo .\n",
      "Shibulaho eshichila enzililile okhumuhandichila , khwikhuwa liobukhoonyi khubakristo ba Yudea ,\n",
      "abeefi abalang'u , abameesi , abachikhanga abashiabwe nende , abanuuli , shibalinyoola obwami bwa Nyasaye tawe. ,\n",
      "Kho , khunyanga yobulamushi , omukhasi oyo yakhabe owa , wina khubandu musafu abo , shichila yali omukhasi owa , boosi musafu ? ”\n",
      "Naye naboola ari , “ Omwami , esuubiile. ” Mana , nasikama hasi niyenamila Yesu .\n",
      "Abanji khubo baboola bari , “ Ali neshishieno khandi nomulalu ! , Eshichila nimurecheresia akaboolanga nishina ? ”\n",
      "Kho chende khume ikholo nikhuboola , khuri “ Omwami niye omuhabini wanje shindalaritsanga tawe. , Omundu namundu anyala okhukhola shiina ? ”\n",
      "Ne olwa Yesu yahulila ario , yamuboolela ari “ Oshileemilekhwo eshindu shilala eshiokhukhola . Tsia , okusie ebindu biosi ebia oli ninabio , mana okabile , abamanani amapesa konyoolamwo . Ne olaba nelibiishilo , mwikulu ; mana witse unondekhwo . ”\n",
      "Yuda natsia shilunji khu Yesu namuboolela ari “ Omulembe kube khwiwe Omwechesia , ” ne , namufumbeelela .\n",
      "Khwitsuulemwo obunaayi nikhulindililanga Inyanga yokhukalukha khwoluyali olwa , Nyasaye wefwe omukhongo nende Omuhonia wefwe Yesu , Kristo .\n"
     ]
    }
   ],
   "source": [
    "! head Luhya/train.*\n",
    "! head Luhya/dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_D0BqDaUOF_"
   },
   "outputs": [],
   "source": [
    "pre = '/content/gdrive/Shared drives/NMT_for_African_Language/'\n",
    "# Train data source\n",
    "filenames = [pre+'Luganda/train.en',pre+'Luhya/train.en']\n",
    "\n",
    "# Train data target\n",
    "filenames2 = [pre+'Luganda/train.lg',pre+'Luhya/train.lh']\n",
    "\n",
    "# Dev data source\n",
    "file1 = [pre+'Luganda/dev.en',pre+'Luhya/dev.en']\n",
    "\n",
    "# Dev data target\n",
    "file2 = [pre+'Luganda/dev.lg',pre+'Luhya/dev.lh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icPaGn2GlpM-"
   },
   "outputs": [],
   "source": [
    "# Changing to Multilingual2 directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5zhE2IimjPs"
   },
   "outputs": [],
   "source": [
    "# Procedure to create concatenated files\n",
    "def create_file(x,filename):\n",
    "  # Open filename in write mode\n",
    "  with open(filename, 'w') as outfile:\n",
    "      for names in x:\n",
    "          # Open each file in read mode\n",
    "          with open(names) as infile:\n",
    "              # read the data and write it in file3\n",
    "              outfile.write(infile.read())\n",
    "          outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ay8047V3oyeG"
   },
   "outputs": [],
   "source": [
    "create_file(filenames,'train.en')\n",
    "create_file(filenames2,'train.lg_lh')\n",
    "create_file(file1,'dev.en')\n",
    "create_file(file2,'dev.lg_lh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSCp4K92o-9_"
   },
   "outputs": [],
   "source": [
    "# Code adapted from https://www.geeksforgeeks.org/count-number-of-lines-in-a-text-file-in-python/\n",
    "# Count lines in a file\n",
    "def count_lines(filename):\n",
    "  # Opening a file\n",
    "  file = open(filename,\"r\")\n",
    "  Counter = 0\n",
    "    \n",
    "  # Reading from file\n",
    "  Content = file.read()\n",
    "  CoList = Content.split(\"\\n\")\n",
    "    \n",
    "  for i in CoList:\n",
    "      if i:\n",
    "          Counter += 1\n",
    "            \n",
    "  return Counter\n",
    "\n",
    "def empty_counter(x):\n",
    "  # Opening a file\n",
    "  infile = open(x,\"r\")\n",
    "  empty = []\n",
    "  \n",
    "  for i,line in enumerate(infile):\n",
    "    if not line.strip(): \n",
    "      empty.append(i)\n",
    "\n",
    "  return empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnEREgvqp2SS",
    "outputId": "6687de91-d9e0-4b35-8b5a-12687072cf91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[222465, 230215]\n",
      "[222465, 230215]\n",
      "[2270, 2350]\n",
      "[2270, 2350]\n"
     ]
    }
   ],
   "source": [
    "print(empty_counter('train.en'))\n",
    "print(empty_counter('train.lg_lh'))\n",
    "\n",
    "print(empty_counter('dev.en'))\n",
    "print(empty_counter('dev.lg_lh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "fdwjw7VvqD4C"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Reference: https://thispointer.com/python-how-to-delete-specific-lines-in-a-file-in-a-memory-efficient-way/\n",
    "def delete_empty_lines(original_file, line_numbers):\n",
    "    \"\"\"In a file, delete the lines at line number in given list\"\"\"\n",
    "    is_skipped = False\n",
    "    counter = 0\n",
    "    # Create name of dummy / temporary file\n",
    "    dummy_file = original_file + '.bak'\n",
    "    # Open original file in read only mode and dummy file in write mode\n",
    "    with open(original_file, 'r') as read_obj, open(dummy_file, 'w') as write_obj:\n",
    "        # Line by line copy data from original file to dummy file\n",
    "        for line in read_obj:\n",
    "            # If current line number exist in list then skip copying that line\n",
    "            if counter not in line_numbers:\n",
    "                write_obj.write(line)\n",
    "            else:\n",
    "                is_skipped = True\n",
    "            counter += 1\n",
    "    # If any line is skipped then rename dummy file as original file\n",
    "    if is_skipped:\n",
    "        os.remove(original_file)\n",
    "        os.rename(dummy_file, original_file)\n",
    "    else:\n",
    "        os.remove(dummy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xV-NbsN56bD"
   },
   "outputs": [],
   "source": [
    "# Cleaning created batch\n",
    "delete_empty_lines(\"train.en\",empty_counter('train.en'))\n",
    "delete_empty_lines(\"train.lg_lh\",empty_counter('train.lg_lh'))\n",
    "\n",
    "delete_empty_lines(\"dev.en\",empty_counter('dev.en'))\n",
    "delete_empty_lines(\"dev.lg_lh\",empty_counter('dev.lg_lh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHwkU9ydUHAI",
    "outputId": "650a6c52-2059-4bbf-dd92-6d967540ccf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "Number of sentences in train files: 230214 230214\n",
      "Number of sentences in valid files: 2349 2349\n"
     ]
    }
   ],
   "source": [
    "print(empty_counter('train.en'))\n",
    "print(empty_counter('train.lg_lh'))\n",
    "\n",
    "print(empty_counter('dev.en'))\n",
    "print(empty_counter('dev.lg_lh'))\n",
    "\n",
    "# Muiltilingual files\n",
    "m_train = count_lines('train.lg_lh')\n",
    "m_dev = count_lines('dev.lg_lh')\n",
    "\n",
    "print(\"Number of sentences in train files:\", m_train, count_lines('train.en'))\n",
    "print(\"Number of sentences in valid files:\", m_dev, count_lines('dev.en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VouMO0lishSQ"
   },
   "source": [
    "### BPE codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "5lK1btbKswMN",
    "outputId": "28bb190a-bd2a-41cd-8fc1-e09ea84ce5d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 73 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 11.2 MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.6 MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 68.8 MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.9.6-py3-none-any.whl (375 kB)\n",
      "\u001b[K     |████████████████████████████████| 375 kB 73.5 MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Collecting astroid<2.7,>=2.6.5\n",
      "  Downloading astroid-2.6.6-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 69.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.9.3-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 66.9 MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting typed-ast<1.5,>=1.4.0\n",
      "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 64.0 MB/s \n",
      "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 4.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=d142c917d08178f4c369d076c0a58c3f0f76a5503b985d4c47c64417ef44da41\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-f6_r8ns3/wheels/b2/63/79/00b1ca041c00d851cc67df0e726b6636bbb52e38c09a484bbe\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68452 sha256=6e0ca2028bec78e2e07a7a830d70cc4181a83a2355125314b5459f5be972be3e\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
      "Successfully built joeynmt wrapt\n",
      "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
      "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed astroid-2.6.6 isort-5.9.3 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.6 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOYk3OK4rw6d"
   },
   "outputs": [],
   "source": [
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt --output_path vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJ-Xn-Y1x50D"
   },
   "outputs": [],
   "source": [
    "# Applying BPE to tests\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test1.$src > test.bpe.en1\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test1.lh > test.bpe.lh\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test2.$src > test.bpe.en2\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test2.lg > test.bpe.lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mj0HdPZ6t5i3",
    "outputId": "30614611-5f11-492d-e92e-e5e64ad43a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Sentences\n",
      "B@@ ach@@ am@@ anga , okh@@ w@@ ikh@@ ala khub@@ if@@ umb@@ i hab@@ undu w@@ olu@@ yali mum@@ as@@ abo , nende eb@@ if@@ umb@@ i bi@@ olu@@ yali b@@ ie@@ im@@ bel@@ i mut@@ sis@@ in@@ ag@@ og@@ i@@ . ,\n",
      "Mana abandu abali nib@@ em@@ i@@ ile im@@ bel@@ i nib@@ amu@@ hal@@ ab@@ ila , nib@@ am@@ ub@@ oolela mbu a@@ hol@@ e@@ el@@ e t@@ si , n@@ ebutswa yam@@ eeta , but@@ swa okhul@@ anj@@ il@@ is@@ ia obut@@ iny@@ u ari , “ Omwana wa D@@ a@@ udi ! , W@@ um@@ be@@ el@@ e t@@ sim@@ b@@ abas@@ i ! ”\n",
      "Mana , nib@@ em@@ ba olw@@ im@@ bo olu@@ yi@@ a b@@ ari “ N@@ i@@ we ou@@ k@@ wan@@ ile okhu@@ bu@@ kula esh@@ it@@ abu esh@@ ik@@ an@@ ye , nende okh@@ wi@@ kula ebib@@ al@@ ik@@ ho bi@@ as@@ hi@@ o@@ . , O@@ kh@@ uba w@@ erwa , ne khulwa okhu@@ f@@ wak@@ h@@ wo kh@@ w@@ esh@@ it@@ is@@ o , war@@ eera khu Nyasaye abandu okhurula mub@@ uli olw@@ ib@@ ulo olulimi am@@ ah@@ anga nende okhurula mut@@ sim@@ b@@ ia t@@ si@@ os@@ i@@ . ,\n",
      "h@@ abula ow@@ enya , okhu@@ ba omuk@@ h@@ ongo mw@@ inywe , okhu@@ ula abe omuk@@ hal@@ abani , w@@ ab@@ o@@ osi .\n",
      "A@@ bu@@ kula ob@@ ise bi@@ hel@@ ile okh@@ wi@@ h@@ enga lik@@ ond@@ ol@@ ie , ne olwa , ar@@ ul@@ a@@ ho , y@@ eb@@ il@@ ila bw@@ angu shinga lw@@ obw@@ en@@ ibwe bu@@ f@@ wan@@ a@@ . ,\n",
      "Combined BPE Vocab\n",
      "erefore\n",
      "oseewo\n",
      "(@@\n",
      "\\\n",
      "ŋ\n",
      "Ó@@\n",
      "”@@\n",
      "ē@@\n",
      "£\n",
      "taayo\n"
     ]
    }
   ],
   "source": [
    "# Some output\n",
    "! echo \"BPE Sentences\"\n",
    "! tail -n 5 test.bpe.lh\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "gSynCGijJUlV",
    "outputId": "7d03a07a-36be-4fb7-aac2-7c4a7bd0ef81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.bpe.en <==\n",
      "And you shall know the truth , and the truth shall make you free . ”\n",
      "But that you also may know my af@@ fa@@ ir@@ s and how I am doing , T@@ y@@ ch@@ ic@@ us , a bel@@ ov@@ ed brother and faithful minis@@ ter in the Lord , will make all things known to you ;\n",
      "And the second took her , and he died ; nor did he le@@ ave any off@@ s@@ pr@@ ing . And the th@@ ir@@ d lik@@ e@@ wise .\n",
      "because he had often been bo@@ und with sh@@ ac@@ k@@ les and cha@@ in@@ s . And the cha@@ in@@ s had been p@@ ul@@ led a@@ part by him , and the sh@@ ac@@ k@@ les br@@ oken in pi@@ ec@@ es ; ne@@ ither could anyone t@@ ame him .\n",
      "Then He spoke to them a par@@ able : “ L@@ ook at the fi@@ g t@@ ree , and all the tre@@ es .\n",
      "Then I saw another be@@ ast coming up out of the earth , and he had two h@@ or@@ ns like a l@@ am@@ b and spoke like a dr@@ ag@@ on .\n",
      "Jesus answ@@ ered and said to her , “ W@@ ho@@ ever dr@@ in@@ ks of this water will th@@ ir@@ st again ,\n",
      "I th@@ an@@ k my God I speak with t@@ ong@@ u@@ es more than you all ;\n",
      "ro@@ o@@ ted and buil@@ t up in Him and est@@ abl@@ ished in the faith , as you have been taught , ab@@ o@@ und@@ ing in it with th@@ ank@@ s@@ giving .\n",
      "\n",
      "\n",
      "==> train.bpe.lg_lh <==\n",
      "mul@@ am@@ anya ob@@ wat@@ o@@ to , ne ob@@ wat@@ o@@ to obwo , bul@@ am@@ ukh@@ ola okhu@@ ba abal@@ ekh@@ u@@ ule . ”\n",
      "T@@ ik@@ iko omwana we@@ f@@ we omu@@ he@@ el@@ wa khandi , omuk@@ hal@@ abani omus@@ uub@@ il@@ wa w@@ em@@ il@@ im@@ o ec@@ hi@@ a Omwami , al@@ am@@ ub@@ oolela am@@ akhuwa k@@ anj@@ e k@@ o@@ osi , kh@@ o mbu mum@@ an@@ ye , shinga olwa en@@ z@@ il@@ il@@ il@@ anga .\n",
      "Omus@@ i@@ ani w@@ abwe wak@@ hab@@ ili nak@@ al@@ us@@ ia , omuk@@ l@@ h@@ as@@ i@@ we naye y@@ esi n@@ af@@ wa , n@@ al@@ al@@ ekh@@ wo omwana t@@ awe@@ . , K@@ h@@ andi omus@@ i@@ ani w@@ abwe wak@@ h@@ at@@ ar@@ u y@@ esi ar@@ io .\n",
      "Ne akh@@ andi akh@@ anj@@ i yab@@ o@@ h@@ ung@@ wa n@@ eb@@ iti@@ ili bi@@ eb@@ il@@ enj@@ e , nende em@@ iny@@ ol@@ olo . Nebutswa y@@ ach@@ it@@ s@@ an@@ u@@ ul@@ anga , n@@ ich@@ ik@@ ab@@ ukh@@ ana nab@@ io eb@@ iti@@ ili bi@@ eb@@ il@@ enj@@ e , yab@@ ik@@ hal@@ ak@@ ang@@ am@@ wo ebit@@ on@@ ye . K@@ ata shi@@ y@@ ali@@ ho ow@@ ali , nam@@ aani k@@ okhum@@ u@@ hand@@ akhwo tawe .\n",
      "Mana Yesu nab@@ ab@@ oolela olu@@ ch@@ el@@ o l@@ uno ari , “ L@@ ol@@ el@@ e , kh@@ um@@ ukh@@ u@@ y@@ u nende emis@@ aala ch@@ ind@@ i chi@@ osi ;\n",
      "Mana nd@@ al@@ ola okus@@ olo ku@@ ndi okw@@ ar@@ ula mw@@ il@@ oba . K@@ wali , nende t@@ sin@@ z@@ ika t@@ sib@@ ili shinga et@@ si@@ es@@ him@@ e@@ eme shi@@ el@@ ik@@ ond@@ i , ne , kw@@ ab@@ oola shinga okuy@@ okh@@ a .\n",
      "Yesu nak@@ al@@ us@@ ia nab@@ oola ari , “ Om@@ undu y@@ esi y@@ esi , o@@ uny@@ wak@@ h@@ wo am@@ aat@@ si k@@ ano , obul@@ o@@ ho bul@@ am@@ ul@@ ia kh@@ andi@@ . ,\n",
      "Ek@@ h@@ up@@ ila Nyasaye or@@ io , sh@@ ichila emb@@ ool@@ el@@ anga , mut@@ sin@@ imi t@@ sin@@ j@@ en@@ i mun@@ o okh@@ ush@@ ila inywe mw@@ es@@ i@@ . ,\n",
      "Mu@@ y@@ ile em@@ isi chi@@ eny@@ we mu@@ ye mum@@ bas@@ he obulamu bw@@ eny@@ we kh@@ u@@ ye m@@ ana mu@@ be , ab@@ ar@@ al@@ il@@ u mub@@ us@@ uub@@ ili bw@@ eny@@ we , shinga olwa , mw@@ ech@@ es@@ ibwa . Mana muk@@ h@@ up@@ il@@ enj@@ e Nyasaye or@@ io buli , lw@@ osi .\n",
      "\n",
      "\n",
      "==> train.en <==\n",
      "And you shall know the truth , and the truth shall make you free . ”\n",
      "But that you also may know my affairs and how I am doing , Tychicus , a beloved brother and faithful minister in the Lord , will make all things known to you ;\n",
      "And the second took her , and he died ; nor did he leave any offspring . And the third likewise .\n",
      "because he had often been bound with shackles and chains . And the chains had been pulled apart by him , and the shackles broken in pieces ; neither could anyone tame him .\n",
      "Then He spoke to them a parable : “ Look at the fig tree , and all the trees .\n",
      "Then I saw another beast coming up out of the earth , and he had two horns like a lamb and spoke like a dragon .\n",
      "Jesus answered and said to her , “ Whoever drinks of this water will thirst again ,\n",
      "I thank my God I speak with tongues more than you all ;\n",
      "rooted and built up in Him and established in the faith , as you have been taught , abounding in it with thanksgiving .\n",
      "\n",
      "\n",
      "==> train.lg_lh <==\n",
      "mulamanya obwatoto , ne obwatoto obwo , bulamukhola okhuba abalekhuule . ”\n",
      "Tikiko omwana wefwe omuheelwa khandi , omukhalabani omusuubilwa wemilimo echia Omwami , alamuboolela amakhuwa kanje koosi , kho mbu mumanye , shinga olwa enzilililanga .\n",
      "Omusiani wabwe wakhabili nakalusia , omuklhasiwe naye yesi nafwa , nalalekhwo omwana tawe. , Khandi omusiani wabwe wakhataru yesi ario .\n",
      "Ne akhandi akhanji yabohungwa nebitiili biebilenje , nende eminyololo . Nebutswa yachitsanuulanga , nichikabukhana nabio ebitiili biebilenje , yabikhalakangamwo ebitonye . Kata shiyaliho owali , namaani kokhumuhandakhwo tawe .\n",
      "Mana Yesu nababoolela oluchelo luno ari , “ Lolele , khumukhuyu nende emisaala chindi chiosi ;\n",
      "Mana ndalola okusolo kundi okwarula mwiloba . Kwali , nende tsinzika tsibili shinga etsieshimeeme shielikondi , ne , kwaboola shinga okuyokha .\n",
      "Yesu nakalusia naboola ari , “ Omundu yesi yesi , ounywakhwo amaatsi kano , obuloho bulamulia khandi. ,\n",
      "Ekhupila Nyasaye orio , shichila emboolelanga , mutsinimi tsinjeni muno okhushila inywe mwesi. ,\n",
      "Muyile emisi chienywe muye mumbashe obulamu bwenywe khuye mana mube , abaralilu mubusuubili bwenywe , shinga olwa , mwechesibwa . Mana mukhupilenje Nyasaye orio buli , lwosi .\n",
      "\n",
      "==> dev.bpe.en <==\n",
      "I do not say this to con@@ dem@@ n ; for I have said before that you are in our hearts , to di@@ e together and to live together .\n",
      "So when they were fill@@ ed , He said to His disciples , “ G@@ ather up the fr@@ ag@@ ments that remain , so that nothing is lost . ”\n",
      "When the D@@ ay of P@@ ent@@ ec@@ ost had fully come , they were all with one acc@@ ord in one place .\n",
      "For it has been declar@@ ed to me concer@@ ning you , my bre@@ th@@ re@@ n , by those of Ch@@ lo@@ e ’ s hou@@ se@@ hold , that there are cont@@ en@@ tions among you .\n",
      "For “ who has known the mind of the Lord that he may instruc@@ t Him ? ” But we have the mind of Christ .\n",
      "And do not become id@@ ol@@ at@@ ers as were some of them . As it is written , “ The people s@@ at down to eat and dr@@ ink , and ro@@ se up to pl@@ ay . ”\n",
      "Now f@@ ive of them were wise , and f@@ ive were fo@@ ol@@ ish .\n",
      "When He op@@ ened the second se@@ al , I heard the second living cre@@ at@@ ure saying , “ C@@ ome and see . ”\n",
      "But let n@@ one of you suff@@ er as a m@@ ur@@ der@@ er , a th@@ ie@@ f , an ev@@ il@@ do@@ er , or as a bus@@ y@@ body in other people ’ s matters .\n",
      "\n",
      "\n",
      "==> dev.bpe.lg_lh <==\n",
      "S@@ hi@@ emb@@ ool@@ anga k@@ ano , khulwa okhum@@ uk@@ hal@@ ach@@ ila eshi@@ ina ta , okhu@@ ba , shinga , nd@@ am@@ ub@@ oolela kh@@ ale , muli aba@@ he@@ el@@ wa mun@@ o kh@@ w@@ if@@ we , ne , kh@@ u@@ be@@ t@@ s@@ anga hal@@ ala buli lw@@ osi , kata n@@ ikh@@ uba abal@@ amu no@@ ho , n@@ ikh@@ u@@ f@@ wa .\n",
      "Ne olwa bo@@ osi bali nib@@ e@@ ku@@ r@@ e yab@@ oolela abee@@ ch@@ ib@@ e ari , “ Muk@@ h@@ ung@@ '@@ as@@ ie ebit@@ on@@ ye bit@@ ony@@ ile , bi@@ osi , kh@@ o mbu kh@@ ul@@ es@@ he okh@@ us@@ as@@ i@@ akhwo esh@@ ind@@ u shi@@ osi shi@@ osi tawe . ”\n",
      "Ne olwa iny@@ anga ya P@@ end@@ ek@@ o@@ te y@@ ola , abas@@ uub@@ ili bo@@ osi , bak@@ h@@ ung@@ '@@ ana hab@@ undu hal@@ ala .\n",
      "O@@ kh@@ uba abaana be@@ f@@ we abandu b@@ andi ab@@ omun@@ z@@ u eya K@@ ul@@ o@@ e b@@ amb@@ ool@@ el@@ e but@@ swa , hab@@ ul@@ afu mbu , obus@@ ool@@ o buli h@@ ak@@ ari mw@@ iny@@ we@@ . ,\n",
      "Sh@@ inga A@@ ma@@ h@@ andik@@ o k@@ ab@@ ool@@ anga mbu “ N@@ i@@ w@@ iina ou@@ m@@ any@@ ile am@@ ap@@ a@@ ar@@ o aka Omwami ? , N@@ i@@ w@@ iina o@@ uny@@ ala okhum@@ u@@ ch@@ el@@ ela ? ” , Nebutswa if@@ we kh@@ uli nam@@ ay@@ il@@ il@@ is@@ io aka Kristo .\n",
      "no@@ ho kata , okh@@ w@@ in@@ am@@ ila eb@@ if@@ wan@@ ani , shinga balala khub@@ o bak@@ h@@ ola , tawe . Sh@@ inga olwa A@@ ma@@ h@@ andik@@ o k@@ ab@@ ool@@ anga mbu , “ Ab@@ andu , b@@ ekh@@ ala h@@ asi okhul@@ ia lis@@ abo el@@ i@@ amala lik@@ al@@ ukh@@ an@@ e okhu@@ ba , eshi@@ f@@ w@@ ab@@ w@@ i eshi@@ obum@@ e@@ esi nende obu@@ y@@ il@@ ani . ”\n",
      "B@@ ar@@ ano khub@@ o , bali abay@@ ing@@ wa , ne b@@ ar@@ ano b@@ andi bali ab@@ ach@@ esi .\n",
      "Mana E@@ sh@@ im@@ eme shi@@ el@@ ik@@ ond@@ i n@@ is@@ hi@@ i@@ kula esh@@ ib@@ al@@ ik@@ ho , shi@@ akh@@ ab@@ ili ne n@@ im@@ bul@@ ila es@@ hil@@ on@@ j@@ e eshi@@ akh@@ ab@@ ili es@@ hil@@ im@@ woyo n@@ ish@@ ib@@ oola sh@@ iri , “ Y@@ it@@ sa ! ”\n",
      "Nebutswa omundu y@@ esi kh@@ w@@ inywe al@@ any@@ as@@ ibwa shinga , omu@@ y@@ iri , no@@ ho omw@@ if@@ i , no@@ ho omuk@@ hol@@ i w@@ am@@ akhuwa am@@ abi , no@@ ho omundu we@@ ind@@ ob@@ oyo tawe .\n",
      "\n",
      "\n",
      "==> dev.en <==\n",
      "I do not say this to condemn ; for I have said before that you are in our hearts , to die together and to live together .\n",
      "So when they were filled , He said to His disciples , “ Gather up the fragments that remain , so that nothing is lost . ”\n",
      "When the Day of Pentecost had fully come , they were all with one accord in one place .\n",
      "For it has been declared to me concerning you , my brethren , by those of Chloe ’ s household , that there are contentions among you .\n",
      "For “ who has known the mind of the Lord that he may instruct Him ? ” But we have the mind of Christ .\n",
      "And do not become idolaters as were some of them . As it is written , “ The people sat down to eat and drink , and rose up to play . ”\n",
      "Now five of them were wise , and five were foolish .\n",
      "When He opened the second seal , I heard the second living creature saying , “ Come and see . ”\n",
      "But let none of you suffer as a murderer , a thief , an evildoer , or as a busybody in other people ’ s matters .\n",
      "\n",
      "\n",
      "==> dev.lg_lh <==\n",
      "Shiemboolanga kano , khulwa okhumukhalachila eshiina ta , okhuba , shinga , ndamuboolela khale , muli abaheelwa muno khwifwe , ne , khubetsanga halala buli lwosi , kata nikhuba abalamu noho , nikhufwa .\n",
      "Ne olwa boosi bali nibekure yaboolela abeechibe ari , “ Mukhung'asie ebitonye bitonyile , biosi , kho mbu khuleshe okhusasiakhwo eshindu shiosi shiosi tawe . ”\n",
      "Ne olwa inyanga ya Pendekote yola , abasuubili boosi , bakhung'ana habundu halala .\n",
      "Okhuba abaana befwe abandu bandi abomunzu eya Kuloe bamboolele butswa , habulafu mbu , obusoolo buli hakari mwinywe. ,\n",
      "Shinga Amahandiko kaboolanga mbu “ Niwiina oumanyile amapaaro aka Omwami ? , Niwiina ounyala okhumuchelela ? ” , Nebutswa ifwe khuli namayililisio aka Kristo .\n",
      "noho kata , okhwinamila ebifwanani , shinga balala khubo bakhola , tawe . Shinga olwa Amahandiko kaboolanga mbu , “ Abandu , bekhala hasi okhulia lisabo eliamala likalukhane okhuba , eshifwabwi eshiobumeesi nende obuyilani . ”\n",
      "Barano khubo , bali abayingwa , ne barano bandi bali abachesi .\n",
      "Mana Eshimeme shielikondi nishiikula eshibalikho , shiakhabili ne nimbulila eshilonje eshiakhabili eshilimwoyo nishiboola shiri , “ Yitsa ! ”\n",
      "Nebutswa omundu yesi khwinywe alanyasibwa shinga , omuyiri , noho omwifi , noho omukholi wamakhuwa amabi , noho omundu weindoboyo tawe .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! tail train.*\n",
    "! tail dev.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abtMrZzIK3QL"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBArdOdXK6Bd"
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "b8Smh87evI9G"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language, source_language)\n",
    "path = \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2\"\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"{path}/train.bpe\"\n",
    "    dev:   \"{path}/dev.bpe\"\n",
    "    test:  \"{path}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"{path}/vocab.txt\"\n",
    "    trg_vocab: \"{path}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 1000\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 2000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 200\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer\"\n",
    "    overwrite: True \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, path=path, source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ej5aVkIwwHu7",
    "outputId": "c3e49483-1f78-442d-bec7-52be5f5a8b3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-02 07:22:03,851 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-02 07:22:03,931 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-02 07:22:08,687 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-02 07:22:08,987 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-02 07:22:09,037 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-02 07:22:09,041 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-02 07:22:09,042 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-02 07:22:09,430 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-02 07:22:09.686028: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-02 07:22:11,871 - INFO - joeynmt.training - Total params: 12151808\n",
      "2021-08-02 07:22:14,025 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
      "2021-08-02 07:22:14,025 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
      "2021-08-02 07:22:14,025 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-02 07:22:14,025 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
      "2021-08-02 07:22:14,026 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
      "2021-08-02 07:22:14,026 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
      "2021-08-02 07:22:14,026 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-02 07:22:14,026 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-02 07:22:14,027 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-02 07:22:14,027 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-02 07:22:14,027 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-02 07:22:14,027 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-02 07:22:14,027 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-02 07:22:14,028 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-02 07:22:14,028 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-02 07:22:14,028 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-02 07:22:14,028 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-02 07:22:14,029 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-02 07:22:14,029 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-02 07:22:14,029 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-02 07:22:14,029 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-02 07:22:14,029 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-02 07:22:14,030 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-02 07:22:14,030 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-02 07:22:14,030 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-02 07:22:14,030 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-02 07:22:14,031 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-02 07:22:14,031 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-02 07:22:14,031 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-02 07:22:14,031 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-02 07:22:14,031 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-02 07:22:14,032 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-02 07:22:14,032 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-02 07:22:14,032 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-02 07:22:14,032 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-02 07:22:14,033 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-02 07:22:14,033 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-02 07:22:14,033 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer\n",
      "2021-08-02 07:22:14,035 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-02 07:22:14,035 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-02 07:22:14,035 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-02 07:22:14,036 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-02 07:22:14,036 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-02 07:22:14,036 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-02 07:22:14,036 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-02 07:22:14,037 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-02 07:22:14,037 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-02 07:22:14,037 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-02 07:22:14,037 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-02 07:22:14,038 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-02 07:22:14,038 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-02 07:22:14,038 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-02 07:22:14,038 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-02 07:22:14,039 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-02 07:22:14,039 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-02 07:22:14,039 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-02 07:22:14,039 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-02 07:22:14,040 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-02 07:22:14,040 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-02 07:22:14,040 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-02 07:22:14,040 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-02 07:22:14,041 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-02 07:22:14,041 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-02 07:22:14,041 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-02 07:22:14,041 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-02 07:22:14,041 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-02 07:22:14,042 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-02 07:22:14,042 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-02 07:22:14,042 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-02 07:22:14,042 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 229609,\n",
      "\tvalid 2349,\n",
      "\ttest 79\n",
      "2021-08-02 07:22:14,043 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-02 07:22:14,044 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-02 07:22:14,044 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-02 07:22:14,044 - INFO - joeynmt.helpers - Number of Src words (types): 4264\n",
      "2021-08-02 07:22:14,045 - INFO - joeynmt.helpers - Number of Trg words (types): 4264\n",
      "2021-08-02 07:22:14,046 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4264),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4264))\n",
      "2021-08-02 07:22:14,059 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-02 07:22:14,060 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-02 07:23:12,301 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.390028, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-08-02 07:24:09,942 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.109470, Tokens per Sec:     7420, Lr: 0.000300\n",
      "2021-08-02 07:25:07,831 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.641111, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-08-02 07:26:05,774 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.930674, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-02 07:27:03,731 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.406367, Tokens per Sec:     7515, Lr: 0.000300\n",
      "2021-08-02 07:28:01,782 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.128606, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-08-02 07:28:59,425 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.347605, Tokens per Sec:     7485, Lr: 0.000300\n",
      "2021-08-02 07:29:56,881 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.986283, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-08-02 07:30:54,177 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.382978, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-08-02 07:31:51,881 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.895691, Tokens per Sec:     7420, Lr: 0.000300\n",
      "2021-08-02 07:33:55,921 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 07:33:55,922 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 07:33:55,922 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 07:33:56,697 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 07:33:56,698 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 07:33:57,628 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 07:33:57,629 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 07:33:57,630 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 07:33:57,630 - INFO - joeynmt.training - \tHypothesis: If we have been been a person to be a person , we can be a person to be a person .\n",
      "2021-08-02 07:33:57,630 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 07:33:57,631 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 07:33:57,631 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 07:33:57,631 - INFO - joeynmt.training - \tHypothesis: For example , we can be a Bible - century - century - century - century - century - century - century - century .\n",
      "2021-08-02 07:33:57,632 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 07:33:57,632 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 07:33:57,632 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 07:33:57,633 - INFO - joeynmt.training - \tHypothesis: Why can we have been been a person to be a person ?\n",
      "2021-08-02 07:33:57,633 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 07:33:57,633 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 07:33:57,634 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 07:33:57,634 - INFO - joeynmt.training - \tHypothesis: We can be a person to be a person to be a person .\n",
      "2021-08-02 07:33:57,634 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2000: bleu:   2.59, loss: 239046.3281, ppl:  43.9309, duration: 125.7531s\n",
      "2021-08-02 07:34:55,316 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.295618, Tokens per Sec:     7272, Lr: 0.000300\n",
      "2021-08-02 07:35:53,133 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.787450, Tokens per Sec:     7464, Lr: 0.000300\n",
      "2021-08-02 07:36:50,797 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.443413, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-08-02 07:37:48,748 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.741724, Tokens per Sec:     7521, Lr: 0.000300\n",
      "2021-08-02 07:38:08,849 - INFO - joeynmt.training - Epoch   1: total training loss 12329.40\n",
      "2021-08-02 07:38:08,850 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-02 07:38:46,860 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     3.866331, Tokens per Sec:     7352, Lr: 0.000300\n",
      "2021-08-02 07:39:44,380 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     3.711369, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-08-02 07:40:41,684 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     3.347633, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-08-02 07:41:39,163 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     3.200854, Tokens per Sec:     7459, Lr: 0.000300\n",
      "2021-08-02 07:42:36,631 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.645105, Tokens per Sec:     7464, Lr: 0.000300\n",
      "2021-08-02 07:43:34,153 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     3.444898, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-02 07:45:19,070 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 07:45:19,071 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 07:45:19,071 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 07:45:19,872 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 07:45:19,872 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 07:45:20,779 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 07:45:20,780 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 07:45:20,781 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 07:45:20,781 - INFO - joeynmt.training - \tHypothesis: When we have a fine example , we can imitate Jesus and his followers .\n",
      "2021-08-02 07:45:20,781 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 07:45:20,782 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 07:45:20,782 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 07:45:20,782 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we can have a Bible study and the Bible study of the Bible and the Bible study .\n",
      "2021-08-02 07:45:20,782 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 07:45:20,783 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 07:45:20,783 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 07:45:20,784 - INFO - joeynmt.training - \tHypothesis: Why is the people of the people ?\n",
      "2021-08-02 07:45:20,784 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 07:45:20,784 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 07:45:20,785 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 07:45:20,785 - INFO - joeynmt.training - \tHypothesis: It is that Jehovah has given us to be a loving relationship with love .\n",
      "2021-08-02 07:45:20,785 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     4000: bleu:   5.89, loss: 200051.8750, ppl:  23.7024, duration: 106.6314s\n",
      "2021-08-02 07:46:18,461 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     3.102289, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-08-02 07:47:15,376 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     3.231786, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-08-02 07:48:12,327 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     3.504081, Tokens per Sec:     7354, Lr: 0.000300\n",
      "2021-08-02 07:49:10,084 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     2.816771, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-08-02 07:50:07,459 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     3.218680, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-02 07:51:05,380 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     3.316384, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-08-02 07:52:02,854 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     3.034798, Tokens per Sec:     7414, Lr: 0.000300\n",
      "2021-08-02 07:53:00,049 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     3.184350, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-08-02 07:53:42,438 - INFO - joeynmt.training - Epoch   2: total training loss 9436.22\n",
      "2021-08-02 07:53:42,439 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-02 07:53:58,117 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     1.935477, Tokens per Sec:     7201, Lr: 0.000300\n",
      "2021-08-02 07:54:55,878 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     3.194447, Tokens per Sec:     7469, Lr: 0.000300\n",
      "2021-08-02 07:56:54,953 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 07:56:54,954 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 07:56:54,954 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 07:56:55,753 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 07:56:55,753 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 07:56:56,674 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 07:56:56,676 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 07:56:56,676 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 07:56:56,676 - INFO - joeynmt.training - \tHypothesis: When we learn how we learn from faith and imitate Jesus , we can imitate him .\n",
      "2021-08-02 07:56:56,676 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 07:56:56,677 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 07:56:56,677 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 07:56:56,678 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we can be able to help the Bible and to make a good news to do so that we can be able to do so .\n",
      "2021-08-02 07:56:56,678 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 07:56:56,678 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 07:56:56,679 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 07:56:56,679 - INFO - joeynmt.training - \tHypothesis: Why is the Israelites not a safeguard ?\n",
      "2021-08-02 07:56:56,679 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 07:56:56,680 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 07:56:56,680 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 07:56:56,680 - INFO - joeynmt.training - \tHypothesis: Of course , Jehovah is a loving relationship with those who are love for love .\n",
      "2021-08-02 07:56:56,681 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     6000: bleu:   8.95, loss: 181477.9219, ppl:  17.6665, duration: 120.8018s\n",
      "2021-08-02 07:57:54,177 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     3.418353, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-02 07:58:51,600 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     3.233471, Tokens per Sec:     7509, Lr: 0.000300\n",
      "2021-08-02 07:59:49,283 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     3.081888, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-08-02 08:00:46,892 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     2.859481, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-02 08:01:43,997 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     2.994502, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-08-02 08:02:41,819 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     3.131268, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-08-02 08:03:39,496 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     2.965012, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-08-02 08:04:37,193 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     3.395658, Tokens per Sec:     7476, Lr: 0.000300\n",
      "2021-08-02 08:05:34,632 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     3.095825, Tokens per Sec:     7342, Lr: 0.000300\n",
      "2021-08-02 08:06:32,259 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     2.880830, Tokens per Sec:     7502, Lr: 0.000300\n",
      "2021-08-02 08:08:28,180 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:08:28,181 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:08:28,181 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:08:28,988 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:08:28,988 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:08:30,260 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:08:30,262 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 08:08:30,262 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 08:08:30,263 - INFO - joeynmt.training - \tHypothesis: When we discuss how we imitate faith and imitate him , we should imitate him .\n",
      "2021-08-02 08:08:30,263 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:08:30,264 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 08:08:30,264 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 08:08:30,264 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we are able to use the Bible and to make a Scriptural study of people .\n",
      "2021-08-02 08:08:30,264 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:08:30,266 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 08:08:30,267 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 08:08:30,267 - INFO - joeynmt.training - \tHypothesis: Why is the creation of false creatures ?\n",
      "2021-08-02 08:08:30,267 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:08:30,268 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 08:08:30,268 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 08:08:30,268 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is the relationship with his people .\n",
      "2021-08-02 08:08:30,269 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     8000: bleu:  10.73, loss: 170118.2344, ppl:  14.7599, duration: 118.0088s\n",
      "2021-08-02 08:09:28,323 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     2.779451, Tokens per Sec:     7482, Lr: 0.000300\n",
      "2021-08-02 08:10:25,858 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     2.678229, Tokens per Sec:     7464, Lr: 0.000300\n",
      "2021-08-02 08:11:23,233 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     2.925591, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-02 08:11:28,120 - INFO - joeynmt.training - Epoch   3: total training loss 8456.34\n",
      "2021-08-02 08:11:28,121 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-02 08:12:21,188 - INFO - joeynmt.training - Epoch   4, Step:     8800, Batch Loss:     2.611633, Tokens per Sec:     7433, Lr: 0.000300\n",
      "2021-08-02 08:13:18,568 - INFO - joeynmt.training - Epoch   4, Step:     9000, Batch Loss:     2.774101, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-08-02 08:14:15,906 - INFO - joeynmt.training - Epoch   4, Step:     9200, Batch Loss:     2.826567, Tokens per Sec:     7471, Lr: 0.000300\n",
      "2021-08-02 08:15:13,510 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     2.629866, Tokens per Sec:     7497, Lr: 0.000300\n",
      "2021-08-02 08:16:11,270 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     2.640833, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-08-02 08:17:08,520 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     2.521965, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-08-02 08:18:06,196 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     2.801980, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-08-02 08:20:11,654 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:20:11,654 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:20:11,654 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:20:12,430 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:20:12,431 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:20:13,635 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:20:13,636 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 08:20:13,636 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 08:20:13,637 - INFO - joeynmt.training - \tHypothesis: When we consider how he was faith and imitate him , we can imitate him .\n",
      "2021-08-02 08:20:13,637 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:20:13,638 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 08:20:13,638 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 08:20:13,638 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the Scriptures and to make a Bible study of the Scriptures .\n",
      "2021-08-02 08:20:13,639 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:20:13,639 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 08:20:13,640 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 08:20:13,640 - INFO - joeynmt.training - \tHypothesis: Why is the creation of false goals ?\n",
      "2021-08-02 08:20:13,640 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:20:13,641 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 08:20:13,641 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 08:20:13,641 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is the way to his people in his love .\n",
      "2021-08-02 08:20:13,641 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    10000: bleu:  12.37, loss: 161002.0781, ppl:  12.7772, duration: 127.4445s\n",
      "2021-08-02 08:21:11,178 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     2.617307, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-08-02 08:22:08,816 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     2.801827, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-08-02 08:23:06,618 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     2.942050, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-08-02 08:24:03,651 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     2.371354, Tokens per Sec:     7412, Lr: 0.000300\n",
      "2021-08-02 08:25:01,445 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     2.686317, Tokens per Sec:     7512, Lr: 0.000300\n",
      "2021-08-02 08:25:58,881 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     2.751909, Tokens per Sec:     7464, Lr: 0.000300\n",
      "2021-08-02 08:26:56,981 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     2.906781, Tokens per Sec:     7563, Lr: 0.000300\n",
      "2021-08-02 08:27:21,553 - INFO - joeynmt.training - Epoch   4: total training loss 7873.45\n",
      "2021-08-02 08:27:21,554 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-02 08:27:55,205 - INFO - joeynmt.training - Epoch   5, Step:    11600, Batch Loss:     2.770242, Tokens per Sec:     7465, Lr: 0.000300\n",
      "2021-08-02 08:28:52,180 - INFO - joeynmt.training - Epoch   5, Step:    11800, Batch Loss:     2.536622, Tokens per Sec:     7310, Lr: 0.000300\n",
      "2021-08-02 08:29:49,842 - INFO - joeynmt.training - Epoch   5, Step:    12000, Batch Loss:     2.800174, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-08-02 08:31:52,957 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:31:52,957 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:31:52,957 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:31:53,752 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:31:53,752 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:31:54,614 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:31:54,615 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 08:31:54,615 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 08:31:54,615 - INFO - joeynmt.training - \tHypothesis: When we read how we imitate faith and imitate him , we are speaking .\n",
      "2021-08-02 08:31:54,616 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:31:54,616 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 08:31:54,617 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 08:31:54,617 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with preaching and study the Scriptures , we need to be able to study the Scriptures .\n",
      "2021-08-02 08:31:54,617 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:31:54,618 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 08:31:54,618 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 08:31:54,618 - INFO - joeynmt.training - \tHypothesis: Why is the goal of false gods ?\n",
      "2021-08-02 08:31:54,619 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:31:54,619 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 08:31:54,619 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 08:31:54,620 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with his people .\n",
      "2021-08-02 08:31:54,620 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    12000: bleu:  14.21, loss: 154223.2812, ppl:  11.4776, duration: 124.7777s\n",
      "2021-08-02 08:32:52,003 - INFO - joeynmt.training - Epoch   5, Step:    12200, Batch Loss:     2.706333, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-02 08:33:49,788 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     2.738249, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-08-02 08:34:47,745 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     2.712185, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-08-02 08:35:45,492 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     2.674065, Tokens per Sec:     7506, Lr: 0.000300\n",
      "2021-08-02 08:36:43,847 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     2.622463, Tokens per Sec:     7521, Lr: 0.000300\n",
      "2021-08-02 08:37:41,613 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     2.753157, Tokens per Sec:     7488, Lr: 0.000300\n",
      "2021-08-02 08:38:39,126 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     2.575663, Tokens per Sec:     7404, Lr: 0.000300\n",
      "2021-08-02 08:39:36,381 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     2.531531, Tokens per Sec:     7329, Lr: 0.000300\n",
      "2021-08-02 08:40:33,878 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     2.261553, Tokens per Sec:     7546, Lr: 0.000300\n",
      "2021-08-02 08:41:31,244 - INFO - joeynmt.training - Epoch   5, Step:    14000, Batch Loss:     2.771230, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-08-02 08:43:28,603 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:43:28,604 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:43:28,604 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:43:29,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:43:29,428 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:43:30,282 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:43:30,283 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 08:43:30,283 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 08:43:30,283 - INFO - joeynmt.training - \tHypothesis: When we consider how he was in harmony with faith and imitate him , we are speaking .\n",
      "2021-08-02 08:43:30,283 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:43:30,284 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 08:43:30,284 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 08:43:30,285 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the Bible and to study the Scriptures as well as we need to read the Scriptures .\n",
      "2021-08-02 08:43:30,285 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:43:30,285 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 08:43:30,286 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 08:43:30,286 - INFO - joeynmt.training - \tHypothesis: Why is the creation of false gods ?\n",
      "2021-08-02 08:43:30,286 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:43:30,287 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 08:43:30,287 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 08:43:30,287 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is the way of his people and his love .\n",
      "2021-08-02 08:43:30,288 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    14000: bleu:  15.68, loss: 148909.8906, ppl:  10.5520, duration: 119.0429s\n",
      "2021-08-02 08:44:27,592 - INFO - joeynmt.training - Epoch   5, Step:    14200, Batch Loss:     2.767907, Tokens per Sec:     7385, Lr: 0.000300\n",
      "2021-08-02 08:45:14,045 - INFO - joeynmt.training - Epoch   5: total training loss 7507.17\n",
      "2021-08-02 08:45:14,045 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-02 08:45:25,677 - INFO - joeynmt.training - Epoch   6, Step:    14400, Batch Loss:     2.595669, Tokens per Sec:     7097, Lr: 0.000300\n",
      "2021-08-02 08:46:23,312 - INFO - joeynmt.training - Epoch   6, Step:    14600, Batch Loss:     2.516499, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-02 08:47:21,255 - INFO - joeynmt.training - Epoch   6, Step:    14800, Batch Loss:     2.345456, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-08-02 08:48:18,999 - INFO - joeynmt.training - Epoch   6, Step:    15000, Batch Loss:     1.696658, Tokens per Sec:     7430, Lr: 0.000300\n",
      "2021-08-02 08:49:16,642 - INFO - joeynmt.training - Epoch   6, Step:    15200, Batch Loss:     3.069285, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-08-02 08:50:14,426 - INFO - joeynmt.training - Epoch   6, Step:    15400, Batch Loss:     2.678406, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-08-02 08:51:12,180 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     2.472472, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-02 08:52:09,700 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     2.736869, Tokens per Sec:     7440, Lr: 0.000300\n",
      "2021-08-02 08:53:07,364 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     2.606452, Tokens per Sec:     7397, Lr: 0.000300\n",
      "2021-08-02 08:55:09,553 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 08:55:09,554 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 08:55:09,554 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 08:55:10,363 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 08:55:10,363 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 08:55:11,198 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 08:55:11,200 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 08:55:11,200 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 08:55:11,200 - INFO - joeynmt.training - \tHypothesis: When we consider how we imitate faith and imitate him , we speak to us .\n",
      "2021-08-02 08:55:11,201 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 08:55:11,201 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 08:55:11,201 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 08:55:11,202 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with preaching and to study the Scriptures as well as we are able to read the Scriptures .\n",
      "2021-08-02 08:55:11,202 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 08:55:11,203 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 08:55:11,203 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 08:55:11,203 - INFO - joeynmt.training - \tHypothesis: Why are false gods ?\n",
      "2021-08-02 08:55:11,203 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 08:55:11,204 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 08:55:11,204 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 08:55:11,204 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is associated with his people in a way .\n",
      "2021-08-02 08:55:11,205 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    16000: bleu:  16.66, loss: 145315.0469, ppl:   9.9685, duration: 123.8397s\n",
      "2021-08-02 08:56:08,488 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     2.156399, Tokens per Sec:     7329, Lr: 0.000300\n",
      "2021-08-02 08:57:06,425 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     2.611605, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-08-02 08:58:04,555 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     2.323911, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-08-02 08:59:02,787 - INFO - joeynmt.training - Epoch   6, Step:    16800, Batch Loss:     2.560384, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-08-02 09:00:00,679 - INFO - joeynmt.training - Epoch   6, Step:    17000, Batch Loss:     2.335904, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-08-02 09:00:58,523 - INFO - joeynmt.training - Epoch   6, Step:    17200, Batch Loss:     2.605040, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-08-02 09:01:06,685 - INFO - joeynmt.training - Epoch   6: total training loss 7187.14\n",
      "2021-08-02 09:01:06,686 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-02 09:01:56,308 - INFO - joeynmt.training - Epoch   7, Step:    17400, Batch Loss:     2.533160, Tokens per Sec:     7248, Lr: 0.000300\n",
      "2021-08-02 09:02:54,478 - INFO - joeynmt.training - Epoch   7, Step:    17600, Batch Loss:     2.708035, Tokens per Sec:     7449, Lr: 0.000300\n",
      "2021-08-02 09:03:52,211 - INFO - joeynmt.training - Epoch   7, Step:    17800, Batch Loss:     2.210431, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-08-02 09:04:49,789 - INFO - joeynmt.training - Epoch   7, Step:    18000, Batch Loss:     2.473560, Tokens per Sec:     7366, Lr: 0.000300\n",
      "2021-08-02 09:06:30,996 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:06:30,997 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:06:30,997 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:06:31,814 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:06:31,815 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:06:32,668 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:06:32,669 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 09:06:32,669 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 09:06:32,670 - INFO - joeynmt.training - \tHypothesis: When we discuss how faith and imitate him , we are speaking to us .\n",
      "2021-08-02 09:06:32,670 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:06:32,670 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 09:06:32,671 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 09:06:32,671 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our efforts to study the Scriptures if we are able to read the Scriptures .\n",
      "2021-08-02 09:06:32,672 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:06:32,673 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 09:06:32,673 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 09:06:32,674 - INFO - joeynmt.training - \tHypothesis: Why is the gods of false gods ?\n",
      "2021-08-02 09:06:32,674 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:06:32,675 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 09:06:32,675 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 09:06:32,675 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is with his people in a loving way .\n",
      "2021-08-02 09:06:32,676 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    18000: bleu:  17.46, loss: 141519.0781, ppl:   9.3874, duration: 102.8862s\n",
      "2021-08-02 09:07:29,858 - INFO - joeynmt.training - Epoch   7, Step:    18200, Batch Loss:     2.300627, Tokens per Sec:     7315, Lr: 0.000300\n",
      "2021-08-02 09:08:27,588 - INFO - joeynmt.training - Epoch   7, Step:    18400, Batch Loss:     2.644674, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-08-02 09:09:25,819 - INFO - joeynmt.training - Epoch   7, Step:    18600, Batch Loss:     2.437593, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-08-02 09:10:23,168 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     2.604556, Tokens per Sec:     7414, Lr: 0.000300\n",
      "2021-08-02 09:11:22,864 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     2.561827, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-08-02 09:12:21,939 - INFO - joeynmt.training - Epoch   7, Step:    19200, Batch Loss:     2.415758, Tokens per Sec:     7296, Lr: 0.000300\n",
      "2021-08-02 09:13:20,326 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     2.347649, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-08-02 09:14:18,468 - INFO - joeynmt.training - Epoch   7, Step:    19600, Batch Loss:     3.008256, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-08-02 09:15:16,878 - INFO - joeynmt.training - Epoch   7, Step:    19800, Batch Loss:     2.798825, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-08-02 09:16:14,445 - INFO - joeynmt.training - Epoch   7, Step:    20000, Batch Loss:     2.314545, Tokens per Sec:     7502, Lr: 0.000300\n",
      "2021-08-02 09:17:57,485 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:17:57,485 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:17:57,485 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:17:58,273 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:17:58,274 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:17:59,147 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:17:59,150 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 09:17:59,150 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 09:17:59,150 - INFO - joeynmt.training - \tHypothesis: When we examine how he felt faith and imitate him , he speaks to us .\n",
      "2021-08-02 09:17:59,150 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:17:59,151 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 09:17:59,151 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 09:17:59,152 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the teaching and to study the Scriptures as possible .\n",
      "2021-08-02 09:17:59,152 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:17:59,152 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 09:17:59,153 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 09:17:59,153 - INFO - joeynmt.training - \tHypothesis: Why are false gods ?\n",
      "2021-08-02 09:17:59,153 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:17:59,154 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 09:17:59,154 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 09:17:59,154 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is with his people in a loving way .\n",
      "2021-08-02 09:17:59,155 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    20000: bleu:  18.39, loss: 138279.9062, ppl:   8.9184, duration: 104.7086s\n",
      "2021-08-02 09:18:26,384 - INFO - joeynmt.training - Epoch   7: total training loss 6934.45\n",
      "2021-08-02 09:18:26,384 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-02 09:18:58,957 - INFO - joeynmt.training - Epoch   8, Step:    20200, Batch Loss:     2.592527, Tokens per Sec:     7095, Lr: 0.000300\n",
      "2021-08-02 09:19:56,439 - INFO - joeynmt.training - Epoch   8, Step:    20400, Batch Loss:     2.634312, Tokens per Sec:     7530, Lr: 0.000300\n",
      "2021-08-02 09:20:54,995 - INFO - joeynmt.training - Epoch   8, Step:    20600, Batch Loss:     2.067562, Tokens per Sec:     7272, Lr: 0.000300\n",
      "2021-08-02 09:21:52,667 - INFO - joeynmt.training - Epoch   8, Step:    20800, Batch Loss:     2.393276, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-08-02 09:22:50,490 - INFO - joeynmt.training - Epoch   8, Step:    21000, Batch Loss:     2.179784, Tokens per Sec:     7261, Lr: 0.000300\n",
      "2021-08-02 09:23:49,330 - INFO - joeynmt.training - Epoch   8, Step:    21200, Batch Loss:     2.236378, Tokens per Sec:     7311, Lr: 0.000300\n",
      "2021-08-02 09:24:46,343 - INFO - joeynmt.training - Epoch   8, Step:    21400, Batch Loss:     2.372444, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-08-02 09:25:44,613 - INFO - joeynmt.training - Epoch   8, Step:    21600, Batch Loss:     2.323848, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-08-02 09:26:43,441 - INFO - joeynmt.training - Epoch   8, Step:    21800, Batch Loss:     2.264974, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-08-02 09:27:41,009 - INFO - joeynmt.training - Epoch   8, Step:    22000, Batch Loss:     2.434174, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-08-02 09:29:29,515 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:29:29,516 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:29:29,516 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:29:30,309 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:29:30,310 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:29:31,188 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:29:31,190 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 09:29:31,191 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 09:29:31,191 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 09:29:31,191 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:29:31,192 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 09:29:31,192 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 09:29:31,193 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the teaching and to read the Scriptures .\n",
      "2021-08-02 09:29:31,193 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:29:31,194 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 09:29:31,194 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 09:29:31,194 - INFO - joeynmt.training - \tHypothesis: Why are false gods of false gods ?\n",
      "2021-08-02 09:29:31,194 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:29:31,195 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 09:29:31,195 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 09:29:31,195 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is with his people in a way of love .\n",
      "2021-08-02 09:29:31,196 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    22000: bleu:  18.89, loss: 136026.4688, ppl:   8.6060, duration: 110.1865s\n",
      "2021-08-02 09:30:28,824 - INFO - joeynmt.training - Epoch   8, Step:    22200, Batch Loss:     2.149014, Tokens per Sec:     7321, Lr: 0.000300\n",
      "2021-08-02 09:31:26,759 - INFO - joeynmt.training - Epoch   8, Step:    22400, Batch Loss:     2.431760, Tokens per Sec:     7361, Lr: 0.000300\n",
      "2021-08-02 09:32:25,121 - INFO - joeynmt.training - Epoch   8, Step:    22600, Batch Loss:     2.392626, Tokens per Sec:     7343, Lr: 0.000300\n",
      "2021-08-02 09:33:22,706 - INFO - joeynmt.training - Epoch   8, Step:    22800, Batch Loss:     2.434542, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-08-02 09:34:13,066 - INFO - joeynmt.training - Epoch   8: total training loss 6793.60\n",
      "2021-08-02 09:34:13,066 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-02 09:34:20,895 - INFO - joeynmt.training - Epoch   9, Step:    23000, Batch Loss:     2.483285, Tokens per Sec:     6790, Lr: 0.000300\n",
      "2021-08-02 09:35:18,961 - INFO - joeynmt.training - Epoch   9, Step:    23200, Batch Loss:     2.439054, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-08-02 09:36:16,496 - INFO - joeynmt.training - Epoch   9, Step:    23400, Batch Loss:     2.336017, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-02 09:37:13,947 - INFO - joeynmt.training - Epoch   9, Step:    23600, Batch Loss:     2.158777, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-08-02 09:38:11,406 - INFO - joeynmt.training - Epoch   9, Step:    23800, Batch Loss:     2.278486, Tokens per Sec:     7348, Lr: 0.000300\n",
      "2021-08-02 09:39:09,057 - INFO - joeynmt.training - Epoch   9, Step:    24000, Batch Loss:     2.271743, Tokens per Sec:     7542, Lr: 0.000300\n",
      "2021-08-02 09:41:04,278 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:41:04,278 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:41:04,278 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:41:05,080 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:41:05,080 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:41:05,931 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:41:05,931 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 09:41:05,932 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 09:41:05,932 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 09:41:05,932 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:41:05,933 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 09:41:05,933 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 09:41:05,933 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible as we preach and to read the Scriptures if possible .\n",
      "2021-08-02 09:41:05,934 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:41:05,934 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 09:41:05,934 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 09:41:05,935 - INFO - joeynmt.training - \tHypothesis: Why are false gods of false gods ?\n",
      "2021-08-02 09:41:05,935 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:41:05,936 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 09:41:05,936 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 09:41:05,936 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is a close relationship with his people in a way .\n",
      "2021-08-02 09:41:05,936 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    24000: bleu:  19.37, loss: 133939.8125, ppl:   8.3264, duration: 116.8783s\n",
      "2021-08-02 09:42:03,825 - INFO - joeynmt.training - Epoch   9, Step:    24200, Batch Loss:     2.513901, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-08-02 09:43:01,647 - INFO - joeynmt.training - Epoch   9, Step:    24400, Batch Loss:     3.090561, Tokens per Sec:     7414, Lr: 0.000300\n",
      "2021-08-02 09:43:59,324 - INFO - joeynmt.training - Epoch   9, Step:    24600, Batch Loss:     2.180153, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-08-02 09:44:57,122 - INFO - joeynmt.training - Epoch   9, Step:    24800, Batch Loss:     2.272831, Tokens per Sec:     7447, Lr: 0.000300\n",
      "2021-08-02 09:45:54,735 - INFO - joeynmt.training - Epoch   9, Step:    25000, Batch Loss:     2.290934, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-08-02 09:46:52,390 - INFO - joeynmt.training - Epoch   9, Step:    25200, Batch Loss:     1.885486, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-08-02 09:47:51,168 - INFO - joeynmt.training - Epoch   9, Step:    25400, Batch Loss:     2.385671, Tokens per Sec:     7342, Lr: 0.000300\n",
      "2021-08-02 09:48:48,844 - INFO - joeynmt.training - Epoch   9, Step:    25600, Batch Loss:     2.407952, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-02 09:49:46,552 - INFO - joeynmt.training - Epoch   9, Step:    25800, Batch Loss:     2.083293, Tokens per Sec:     7502, Lr: 0.000300\n",
      "2021-08-02 09:49:57,911 - INFO - joeynmt.training - Epoch   9: total training loss 6599.01\n",
      "2021-08-02 09:49:57,912 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-02 09:50:44,401 - INFO - joeynmt.training - Epoch  10, Step:    26000, Batch Loss:     1.833073, Tokens per Sec:     7322, Lr: 0.000300\n",
      "2021-08-02 09:52:29,838 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 09:52:29,839 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 09:52:29,839 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 09:52:30,639 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 09:52:30,639 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 09:52:31,522 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 09:52:31,523 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 09:52:31,524 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 09:52:31,524 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 09:52:31,524 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 09:52:31,525 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 09:52:31,525 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 09:52:31,525 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible as we preach and to read the Scriptures as possible .\n",
      "2021-08-02 09:52:31,526 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 09:52:31,526 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 09:52:31,526 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 09:52:31,527 - INFO - joeynmt.training - \tHypothesis: Why is the gods of false gods ?\n",
      "2021-08-02 09:52:31,527 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 09:52:31,528 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 09:52:31,528 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 09:52:31,528 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is with his people in a loving way .\n",
      "2021-08-02 09:52:31,529 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    26000: bleu:  19.53, loss: 131800.1406, ppl:   8.0492, duration: 107.1272s\n",
      "2021-08-02 09:53:28,979 - INFO - joeynmt.training - Epoch  10, Step:    26200, Batch Loss:     2.358318, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-08-02 09:54:26,636 - INFO - joeynmt.training - Epoch  10, Step:    26400, Batch Loss:     2.294915, Tokens per Sec:     7506, Lr: 0.000300\n",
      "2021-08-02 09:55:24,381 - INFO - joeynmt.training - Epoch  10, Step:    26600, Batch Loss:     2.294810, Tokens per Sec:     7500, Lr: 0.000300\n",
      "2021-08-02 09:56:21,447 - INFO - joeynmt.training - Epoch  10, Step:    26800, Batch Loss:     2.237443, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-08-02 09:57:18,426 - INFO - joeynmt.training - Epoch  10, Step:    27000, Batch Loss:     2.939754, Tokens per Sec:     7345, Lr: 0.000300\n",
      "2021-08-02 09:58:15,693 - INFO - joeynmt.training - Epoch  10, Step:    27200, Batch Loss:     2.315185, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-08-02 09:59:13,649 - INFO - joeynmt.training - Epoch  10, Step:    27400, Batch Loss:     2.272722, Tokens per Sec:     7450, Lr: 0.000300\n",
      "2021-08-02 10:00:11,406 - INFO - joeynmt.training - Epoch  10, Step:    27600, Batch Loss:     2.146944, Tokens per Sec:     7464, Lr: 0.000300\n",
      "2021-08-02 10:01:08,937 - INFO - joeynmt.training - Epoch  10, Step:    27800, Batch Loss:     2.633613, Tokens per Sec:     7485, Lr: 0.000300\n",
      "2021-08-02 10:02:06,784 - INFO - joeynmt.training - Epoch  10, Step:    28000, Batch Loss:     2.144765, Tokens per Sec:     7310, Lr: 0.000300\n",
      "2021-08-02 10:03:48,156 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:03:48,157 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:03:48,157 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:03:48,935 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:03:48,936 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:03:49,793 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:03:49,794 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 10:03:49,794 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 10:03:49,794 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 10:03:49,794 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:03:49,795 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 10:03:49,795 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 10:03:49,796 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the preaching and to study the Scriptures if possible .\n",
      "2021-08-02 10:03:49,796 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:03:49,797 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 10:03:49,797 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 10:03:49,797 - INFO - joeynmt.training - \tHypothesis: Why are false gods not undeceived ?\n",
      "2021-08-02 10:03:49,797 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:03:49,798 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 10:03:49,798 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 10:03:49,798 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is associated with his people in a way of love .\n",
      "2021-08-02 10:03:49,799 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    28000: bleu:  20.10, loss: 129772.4766, ppl:   7.7951, duration: 103.0143s\n",
      "2021-08-02 10:04:47,070 - INFO - joeynmt.training - Epoch  10, Step:    28200, Batch Loss:     2.161996, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-08-02 10:05:43,995 - INFO - joeynmt.training - Epoch  10, Step:    28400, Batch Loss:     2.287195, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-08-02 10:06:41,783 - INFO - joeynmt.training - Epoch  10, Step:    28600, Batch Loss:     2.524720, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-08-02 10:07:16,716 - INFO - joeynmt.training - Epoch  10: total training loss 6517.33\n",
      "2021-08-02 10:07:16,717 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-02 10:07:39,410 - INFO - joeynmt.training - Epoch  11, Step:    28800, Batch Loss:     2.439456, Tokens per Sec:     7195, Lr: 0.000300\n",
      "2021-08-02 10:08:36,727 - INFO - joeynmt.training - Epoch  11, Step:    29000, Batch Loss:     2.222740, Tokens per Sec:     7529, Lr: 0.000300\n",
      "2021-08-02 10:09:34,321 - INFO - joeynmt.training - Epoch  11, Step:    29200, Batch Loss:     2.447434, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-08-02 10:10:32,223 - INFO - joeynmt.training - Epoch  11, Step:    29400, Batch Loss:     2.202763, Tokens per Sec:     7535, Lr: 0.000300\n",
      "2021-08-02 10:11:29,702 - INFO - joeynmt.training - Epoch  11, Step:    29600, Batch Loss:     2.583203, Tokens per Sec:     7326, Lr: 0.000300\n",
      "2021-08-02 10:12:27,776 - INFO - joeynmt.training - Epoch  11, Step:    29800, Batch Loss:     2.040406, Tokens per Sec:     7575, Lr: 0.000300\n",
      "2021-08-02 10:13:25,377 - INFO - joeynmt.training - Epoch  11, Step:    30000, Batch Loss:     2.468858, Tokens per Sec:     7512, Lr: 0.000300\n",
      "2021-08-02 10:15:08,033 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:15:08,033 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:15:08,033 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:15:08,830 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:15:08,831 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:15:09,700 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:15:09,701 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 10:15:09,701 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 10:15:09,701 - INFO - joeynmt.training - \tHypothesis: When we examine how he felt faith and imitate him , he is speaking to us .\n",
      "2021-08-02 10:15:09,701 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:15:09,702 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 10:15:09,702 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 10:15:09,702 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and to read the Scriptures if possible .\n",
      "2021-08-02 10:15:09,703 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:15:09,703 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 10:15:09,704 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 10:15:09,704 - INFO - joeynmt.training - \tHypothesis: Why are false gods of false gods ?\n",
      "2021-08-02 10:15:09,704 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:15:09,705 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 10:15:09,705 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 10:15:09,705 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is close to his people in a loving way .\n",
      "2021-08-02 10:15:09,705 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    30000: bleu:  20.57, loss: 128315.1484, ppl:   7.6174, duration: 104.3279s\n",
      "2021-08-02 10:16:06,901 - INFO - joeynmt.training - Epoch  11, Step:    30200, Batch Loss:     2.293643, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-08-02 10:17:04,308 - INFO - joeynmt.training - Epoch  11, Step:    30400, Batch Loss:     2.581449, Tokens per Sec:     7498, Lr: 0.000300\n",
      "2021-08-02 10:18:01,487 - INFO - joeynmt.training - Epoch  11, Step:    30600, Batch Loss:     1.988236, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-08-02 10:18:58,718 - INFO - joeynmt.training - Epoch  11, Step:    30800, Batch Loss:     2.343927, Tokens per Sec:     7443, Lr: 0.000300\n",
      "2021-08-02 10:19:55,897 - INFO - joeynmt.training - Epoch  11, Step:    31000, Batch Loss:     2.290537, Tokens per Sec:     7392, Lr: 0.000300\n",
      "2021-08-02 10:20:53,874 - INFO - joeynmt.training - Epoch  11, Step:    31200, Batch Loss:     2.221087, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-08-02 10:21:51,023 - INFO - joeynmt.training - Epoch  11, Step:    31400, Batch Loss:     2.048947, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-08-02 10:22:44,335 - INFO - joeynmt.training - Epoch  11: total training loss 6353.92\n",
      "2021-08-02 10:22:44,336 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-02 10:22:49,094 - INFO - joeynmt.training - Epoch  12, Step:    31600, Batch Loss:     2.259889, Tokens per Sec:     6766, Lr: 0.000300\n",
      "2021-08-02 10:23:45,443 - INFO - joeynmt.training - Epoch  12, Step:    31800, Batch Loss:     2.504953, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-08-02 10:24:43,408 - INFO - joeynmt.training - Epoch  12, Step:    32000, Batch Loss:     1.928705, Tokens per Sec:     7426, Lr: 0.000300\n",
      "2021-08-02 10:26:36,140 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:26:36,141 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:26:36,141 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:26:36,941 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:26:36,942 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:26:37,797 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:26:37,799 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 10:26:37,799 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 10:26:37,799 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 10:26:37,800 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:26:37,800 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 10:26:37,801 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 10:26:37,801 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the ministry and to read the Scriptures if possible .\n",
      "2021-08-02 10:26:37,801 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:26:37,802 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 10:26:37,802 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 10:26:37,802 - INFO - joeynmt.training - \tHypothesis: Why are false gods not uncertain ?\n",
      "2021-08-02 10:26:37,802 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:26:37,803 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 10:26:37,803 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 10:26:37,804 - INFO - joeynmt.training - \tHypothesis: True , Jehovah has dealt with his people in a loving way .\n",
      "2021-08-02 10:26:37,804 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    32000: bleu:  20.97, loss: 127159.1094, ppl:   7.4793, duration: 114.3954s\n",
      "2021-08-02 10:27:35,434 - INFO - joeynmt.training - Epoch  12, Step:    32200, Batch Loss:     2.380137, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-08-02 10:28:33,380 - INFO - joeynmt.training - Epoch  12, Step:    32400, Batch Loss:     1.938960, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-08-02 10:29:31,010 - INFO - joeynmt.training - Epoch  12, Step:    32600, Batch Loss:     2.154664, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-08-02 10:30:28,058 - INFO - joeynmt.training - Epoch  12, Step:    32800, Batch Loss:     1.899712, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-08-02 10:31:25,546 - INFO - joeynmt.training - Epoch  12, Step:    33000, Batch Loss:     2.172168, Tokens per Sec:     7377, Lr: 0.000300\n",
      "2021-08-02 10:32:22,814 - INFO - joeynmt.training - Epoch  12, Step:    33200, Batch Loss:     2.225878, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-08-02 10:33:20,415 - INFO - joeynmt.training - Epoch  12, Step:    33400, Batch Loss:     1.967627, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-08-02 10:34:18,269 - INFO - joeynmt.training - Epoch  12, Step:    33600, Batch Loss:     2.403616, Tokens per Sec:     7497, Lr: 0.000300\n",
      "2021-08-02 10:35:16,424 - INFO - joeynmt.training - Epoch  12, Step:    33800, Batch Loss:     2.337998, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-08-02 10:36:13,399 - INFO - joeynmt.training - Epoch  12, Step:    34000, Batch Loss:     2.450284, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-08-02 10:38:01,175 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:38:01,176 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:38:01,176 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:38:01,968 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:38:01,969 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:38:02,883 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:38:02,884 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 10:38:02,885 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 10:38:02,885 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 10:38:02,888 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:38:02,888 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 10:38:02,888 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 10:38:02,889 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and seek to read the Scriptures if possible .\n",
      "2021-08-02 10:38:02,889 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:38:02,890 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 10:38:02,890 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 10:38:02,890 - INFO - joeynmt.training - \tHypothesis: Why are false gods of false gods ?\n",
      "2021-08-02 10:38:02,891 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:38:02,891 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 10:38:02,891 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 10:38:02,892 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is close to his people in a loving way .\n",
      "2021-08-02 10:38:02,892 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    34000: bleu:  21.17, loss: 126031.0625, ppl:   7.3470, duration: 109.4927s\n",
      "2021-08-02 10:39:00,201 - INFO - joeynmt.training - Epoch  12, Step:    34200, Batch Loss:     2.082255, Tokens per Sec:     7322, Lr: 0.000300\n",
      "2021-08-02 10:39:57,893 - INFO - joeynmt.training - Epoch  12, Step:    34400, Batch Loss:     2.174654, Tokens per Sec:     7485, Lr: 0.000300\n",
      "2021-08-02 10:40:16,448 - INFO - joeynmt.training - Epoch  12: total training loss 6294.63\n",
      "2021-08-02 10:40:16,449 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-02 10:40:56,032 - INFO - joeynmt.training - Epoch  13, Step:    34600, Batch Loss:     2.151961, Tokens per Sec:     7316, Lr: 0.000300\n",
      "2021-08-02 10:41:53,605 - INFO - joeynmt.training - Epoch  13, Step:    34800, Batch Loss:     2.157104, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-08-02 10:42:51,266 - INFO - joeynmt.training - Epoch  13, Step:    35000, Batch Loss:     2.135412, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-02 10:43:48,653 - INFO - joeynmt.training - Epoch  13, Step:    35200, Batch Loss:     2.044566, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-08-02 10:44:46,762 - INFO - joeynmt.training - Epoch  13, Step:    35400, Batch Loss:     2.333121, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-08-02 10:45:44,349 - INFO - joeynmt.training - Epoch  13, Step:    35600, Batch Loss:     2.151274, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-08-02 10:46:42,042 - INFO - joeynmt.training - Epoch  13, Step:    35800, Batch Loss:     2.058628, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-02 10:47:39,801 - INFO - joeynmt.training - Epoch  13, Step:    36000, Batch Loss:     2.431476, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-08-02 10:49:34,081 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 10:49:34,081 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 10:49:34,082 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 10:49:34,869 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 10:49:34,870 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 10:49:35,713 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 10:49:35,714 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 10:49:35,715 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 10:49:35,715 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 10:49:35,715 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 10:49:35,716 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 10:49:35,716 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 10:49:35,716 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and to read the Scriptures when possible .\n",
      "2021-08-02 10:49:35,716 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 10:49:35,717 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 10:49:35,718 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 10:49:35,718 - INFO - joeynmt.training - \tHypothesis: Why are false gods of false gods ?\n",
      "2021-08-02 10:49:35,719 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 10:49:35,720 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 10:49:35,720 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 10:49:35,721 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-02 10:49:35,721 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    36000: bleu:  21.66, loss: 125395.4219, ppl:   7.2734, duration: 115.9198s\n",
      "2021-08-02 10:50:33,742 - INFO - joeynmt.training - Epoch  13, Step:    36200, Batch Loss:     2.237367, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-08-02 10:51:31,150 - INFO - joeynmt.training - Epoch  13, Step:    36400, Batch Loss:     2.317577, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-08-02 10:52:29,343 - INFO - joeynmt.training - Epoch  13, Step:    36600, Batch Loss:     1.995202, Tokens per Sec:     7533, Lr: 0.000300\n",
      "2021-08-02 10:53:26,646 - INFO - joeynmt.training - Epoch  13, Step:    36800, Batch Loss:     2.220994, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-08-02 10:54:24,209 - INFO - joeynmt.training - Epoch  13, Step:    37000, Batch Loss:     2.211096, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-08-02 10:55:21,830 - INFO - joeynmt.training - Epoch  13, Step:    37200, Batch Loss:     2.230115, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-08-02 10:56:01,452 - INFO - joeynmt.training - Epoch  13: total training loss 6197.37\n",
      "2021-08-02 10:56:01,453 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-02 10:56:19,616 - INFO - joeynmt.training - Epoch  14, Step:    37400, Batch Loss:     2.431941, Tokens per Sec:     7186, Lr: 0.000300\n",
      "2021-08-02 10:57:17,653 - INFO - joeynmt.training - Epoch  14, Step:    37600, Batch Loss:     2.131309, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-08-02 10:58:15,257 - INFO - joeynmt.training - Epoch  14, Step:    37800, Batch Loss:     2.202340, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-08-02 10:59:12,799 - INFO - joeynmt.training - Epoch  14, Step:    38000, Batch Loss:     2.070035, Tokens per Sec:     7315, Lr: 0.000300\n",
      "2021-08-02 11:00:59,664 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:00:59,665 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:00:59,665 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:01:00,466 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:01:00,466 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:01:01,357 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:01:01,359 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 11:01:01,360 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 11:01:01,360 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking with us .\n",
      "2021-08-02 11:01:01,360 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:01:01,361 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 11:01:01,361 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 11:01:01,361 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and to read the Scriptures if possible .\n",
      "2021-08-02 11:01:01,362 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:01:01,362 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 11:01:01,363 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 11:01:01,363 - INFO - joeynmt.training - \tHypothesis: Why are false gods ?\n",
      "2021-08-02 11:01:01,363 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:01:01,364 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 11:01:01,364 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 11:01:01,364 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is associated with his people in a loving way .\n",
      "2021-08-02 11:01:01,365 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    38000: bleu:  21.54, loss: 123841.5078, ppl:   7.0968, duration: 108.5654s\n",
      "2021-08-02 11:01:59,083 - INFO - joeynmt.training - Epoch  14, Step:    38200, Batch Loss:     2.513208, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-02 11:02:56,920 - INFO - joeynmt.training - Epoch  14, Step:    38400, Batch Loss:     2.155748, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-08-02 11:03:54,549 - INFO - joeynmt.training - Epoch  14, Step:    38600, Batch Loss:     1.893545, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-08-02 11:04:53,223 - INFO - joeynmt.training - Epoch  14, Step:    38800, Batch Loss:     1.990238, Tokens per Sec:     7443, Lr: 0.000300\n",
      "2021-08-02 11:05:51,059 - INFO - joeynmt.training - Epoch  14, Step:    39000, Batch Loss:     2.196692, Tokens per Sec:     7469, Lr: 0.000300\n",
      "2021-08-02 11:06:49,658 - INFO - joeynmt.training - Epoch  14, Step:    39200, Batch Loss:     1.859174, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-08-02 11:07:47,065 - INFO - joeynmt.training - Epoch  14, Step:    39400, Batch Loss:     2.416821, Tokens per Sec:     7366, Lr: 0.000300\n",
      "2021-08-02 11:08:44,829 - INFO - joeynmt.training - Epoch  14, Step:    39600, Batch Loss:     2.275428, Tokens per Sec:     7448, Lr: 0.000300\n",
      "2021-08-02 11:09:42,432 - INFO - joeynmt.training - Epoch  14, Step:    39800, Batch Loss:     2.128048, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-08-02 11:10:40,002 - INFO - joeynmt.training - Epoch  14, Step:    40000, Batch Loss:     2.091991, Tokens per Sec:     7440, Lr: 0.000300\n",
      "2021-08-02 11:12:26,060 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:12:26,061 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:12:26,061 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:12:26,859 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:12:26,860 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:12:27,747 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:12:27,747 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 11:12:27,748 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 11:12:27,748 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 11:12:27,750 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:12:27,751 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 11:12:27,751 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 11:12:27,751 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the ministry and to read the Scriptures if possible .\n",
      "2021-08-02 11:12:27,751 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:12:27,752 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 11:12:27,752 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 11:12:27,752 - INFO - joeynmt.training - \tHypothesis: Why are false gods not uncertain ?\n",
      "2021-08-02 11:12:27,753 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:12:27,754 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 11:12:27,754 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 11:12:27,754 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-02 11:12:27,755 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    40000: bleu:  21.84, loss: 122755.8047, ppl:   6.9759, duration: 107.7522s\n",
      "2021-08-02 11:13:25,911 - INFO - joeynmt.training - Epoch  14, Step:    40200, Batch Loss:     1.820081, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-08-02 11:13:28,045 - INFO - joeynmt.training - Epoch  14: total training loss 6116.56\n",
      "2021-08-02 11:13:28,045 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-02 11:14:23,920 - INFO - joeynmt.training - Epoch  15, Step:    40400, Batch Loss:     2.052425, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-08-02 11:15:21,797 - INFO - joeynmt.training - Epoch  15, Step:    40600, Batch Loss:     2.341323, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-08-02 11:16:19,185 - INFO - joeynmt.training - Epoch  15, Step:    40800, Batch Loss:     2.127653, Tokens per Sec:     7328, Lr: 0.000300\n",
      "2021-08-02 11:17:16,851 - INFO - joeynmt.training - Epoch  15, Step:    41000, Batch Loss:     2.130843, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-02 11:18:14,884 - INFO - joeynmt.training - Epoch  15, Step:    41200, Batch Loss:     1.821338, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-02 11:19:13,165 - INFO - joeynmt.training - Epoch  15, Step:    41400, Batch Loss:     1.895721, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-08-02 11:20:11,360 - INFO - joeynmt.training - Epoch  15, Step:    41600, Batch Loss:     2.089326, Tokens per Sec:     7408, Lr: 0.000300\n",
      "2021-08-02 11:21:09,128 - INFO - joeynmt.training - Epoch  15, Step:    41800, Batch Loss:     2.102872, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-08-02 11:22:06,844 - INFO - joeynmt.training - Epoch  15, Step:    42000, Batch Loss:     1.669991, Tokens per Sec:     7361, Lr: 0.000300\n",
      "2021-08-02 11:23:54,660 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:23:54,661 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:23:54,661 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:23:55,443 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:23:55,444 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:23:56,301 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:23:56,302 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 11:23:56,303 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 11:23:56,303 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 11:23:56,303 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:23:56,304 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 11:23:56,304 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 11:23:56,304 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and to study the Scriptures if possible .\n",
      "2021-08-02 11:23:56,305 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:23:56,305 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 11:23:56,305 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 11:23:56,306 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a various influence ?\n",
      "2021-08-02 11:23:56,306 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:23:56,307 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 11:23:56,307 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 11:23:56,307 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is associated with his people in a loving way .\n",
      "2021-08-02 11:23:56,307 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    42000: bleu:  22.21, loss: 122551.1328, ppl:   6.9533, duration: 109.4630s\n",
      "2021-08-02 11:24:54,439 - INFO - joeynmt.training - Epoch  15, Step:    42200, Batch Loss:     2.512645, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-08-02 11:25:51,762 - INFO - joeynmt.training - Epoch  15, Step:    42400, Batch Loss:     1.982685, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-08-02 11:26:49,819 - INFO - joeynmt.training - Epoch  15, Step:    42600, Batch Loss:     1.834650, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-08-02 11:27:47,675 - INFO - joeynmt.training - Epoch  15, Step:    42800, Batch Loss:     2.039510, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-08-02 11:28:45,621 - INFO - joeynmt.training - Epoch  15, Step:    43000, Batch Loss:     2.214075, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-08-02 11:29:08,184 - INFO - joeynmt.training - Epoch  15: total training loss 6052.70\n",
      "2021-08-02 11:29:08,185 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-02 11:29:43,865 - INFO - joeynmt.training - Epoch  16, Step:    43200, Batch Loss:     2.052231, Tokens per Sec:     7340, Lr: 0.000300\n",
      "2021-08-02 11:30:41,546 - INFO - joeynmt.training - Epoch  16, Step:    43400, Batch Loss:     2.065609, Tokens per Sec:     7403, Lr: 0.000300\n",
      "2021-08-02 11:31:39,549 - INFO - joeynmt.training - Epoch  16, Step:    43600, Batch Loss:     1.897599, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-08-02 11:32:37,242 - INFO - joeynmt.training - Epoch  16, Step:    43800, Batch Loss:     2.158360, Tokens per Sec:     7458, Lr: 0.000300\n",
      "2021-08-02 11:33:35,082 - INFO - joeynmt.training - Epoch  16, Step:    44000, Batch Loss:     1.962069, Tokens per Sec:     7343, Lr: 0.000300\n",
      "2021-08-02 11:35:13,732 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:35:13,732 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:35:13,733 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:35:14,529 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:35:14,529 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:35:15,417 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:35:15,418 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 11:35:15,418 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 11:35:15,419 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 11:35:15,419 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:35:15,420 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 11:35:15,420 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 11:35:15,420 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures as possible .\n",
      "2021-08-02 11:35:15,421 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:35:15,421 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 11:35:15,422 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 11:35:15,422 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unusual ?\n",
      "2021-08-02 11:35:15,422 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:35:15,423 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 11:35:15,423 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 11:35:15,423 - INFO - joeynmt.training - \tHypothesis: True , Jehovah has dealt with his people in a loving way .\n",
      "2021-08-02 11:35:15,424 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    44000: bleu:  22.31, loss: 121417.7891, ppl:   6.8298, duration: 100.3406s\n",
      "2021-08-02 11:36:13,446 - INFO - joeynmt.training - Epoch  16, Step:    44200, Batch Loss:     2.055677, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-08-02 11:37:10,861 - INFO - joeynmt.training - Epoch  16, Step:    44400, Batch Loss:     1.972190, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-08-02 11:38:08,135 - INFO - joeynmt.training - Epoch  16, Step:    44600, Batch Loss:     2.053871, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-02 11:39:05,303 - INFO - joeynmt.training - Epoch  16, Step:    44800, Batch Loss:     2.178530, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-08-02 11:40:03,105 - INFO - joeynmt.training - Epoch  16, Step:    45000, Batch Loss:     2.188596, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-08-02 11:41:00,577 - INFO - joeynmt.training - Epoch  16, Step:    45200, Batch Loss:     2.257505, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-08-02 11:41:58,060 - INFO - joeynmt.training - Epoch  16, Step:    45400, Batch Loss:     2.411813, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-08-02 11:42:55,902 - INFO - joeynmt.training - Epoch  16, Step:    45600, Batch Loss:     2.084411, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-08-02 11:43:53,977 - INFO - joeynmt.training - Epoch  16, Step:    45800, Batch Loss:     1.943560, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-08-02 11:44:38,036 - INFO - joeynmt.training - Epoch  16: total training loss 5996.41\n",
      "2021-08-02 11:44:38,037 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-08-02 11:44:51,803 - INFO - joeynmt.training - Epoch  17, Step:    46000, Batch Loss:     2.075195, Tokens per Sec:     7153, Lr: 0.000300\n",
      "2021-08-02 11:46:38,755 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:46:38,756 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:46:38,756 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:46:39,557 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:46:39,557 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:46:40,413 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:46:40,414 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 11:46:40,415 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 11:46:40,415 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 11:46:40,415 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:46:40,416 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 11:46:40,416 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 11:46:40,416 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and to study the Scriptures if possible .\n",
      "2021-08-02 11:46:40,417 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:46:40,417 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 11:46:40,418 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 11:46:40,418 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a lie ?\n",
      "2021-08-02 11:46:40,418 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:46:40,419 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 11:46:40,419 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 11:46:40,419 - INFO - joeynmt.training - \tHypothesis: True , Jehovah has dealt with his people in a loving way .\n",
      "2021-08-02 11:46:40,420 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    46000: bleu:  22.67, loss: 120600.6797, ppl:   6.7420, duration: 108.6157s\n",
      "2021-08-02 11:47:38,855 - INFO - joeynmt.training - Epoch  17, Step:    46200, Batch Loss:     2.453937, Tokens per Sec:     7433, Lr: 0.000300\n",
      "2021-08-02 11:48:36,074 - INFO - joeynmt.training - Epoch  17, Step:    46400, Batch Loss:     2.170785, Tokens per Sec:     7316, Lr: 0.000300\n",
      "2021-08-02 11:49:34,068 - INFO - joeynmt.training - Epoch  17, Step:    46600, Batch Loss:     2.199180, Tokens per Sec:     7360, Lr: 0.000300\n",
      "2021-08-02 11:50:31,776 - INFO - joeynmt.training - Epoch  17, Step:    46800, Batch Loss:     1.912841, Tokens per Sec:     7474, Lr: 0.000300\n",
      "2021-08-02 11:51:29,473 - INFO - joeynmt.training - Epoch  17, Step:    47000, Batch Loss:     2.070250, Tokens per Sec:     7327, Lr: 0.000300\n",
      "2021-08-02 11:52:27,671 - INFO - joeynmt.training - Epoch  17, Step:    47200, Batch Loss:     2.101421, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-08-02 11:53:25,246 - INFO - joeynmt.training - Epoch  17, Step:    47400, Batch Loss:     1.946202, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-08-02 11:54:23,478 - INFO - joeynmt.training - Epoch  17, Step:    47600, Batch Loss:     1.774555, Tokens per Sec:     7431, Lr: 0.000300\n",
      "2021-08-02 11:55:20,931 - INFO - joeynmt.training - Epoch  17, Step:    47800, Batch Loss:     2.015362, Tokens per Sec:     7548, Lr: 0.000300\n",
      "2021-08-02 11:56:18,617 - INFO - joeynmt.training - Epoch  17, Step:    48000, Batch Loss:     2.035338, Tokens per Sec:     7449, Lr: 0.000300\n",
      "2021-08-02 11:58:05,348 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 11:58:05,348 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 11:58:05,348 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 11:58:06,151 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 11:58:06,152 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 11:58:07,038 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 11:58:07,039 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 11:58:07,039 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 11:58:07,040 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking with us .\n",
      "2021-08-02 11:58:07,040 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 11:58:07,041 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 11:58:07,041 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 11:58:07,041 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-02 11:58:07,041 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 11:58:07,042 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 11:58:07,043 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 11:58:07,043 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-02 11:58:07,043 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 11:58:07,044 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 11:58:07,044 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 11:58:07,045 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-02 11:58:07,045 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    48000: bleu:  22.81, loss: 119717.8281, ppl:   6.6485, duration: 108.4274s\n",
      "2021-08-02 11:59:04,853 - INFO - joeynmt.training - Epoch  17, Step:    48200, Batch Loss:     1.980034, Tokens per Sec:     7356, Lr: 0.000300\n",
      "2021-08-02 12:00:02,482 - INFO - joeynmt.training - Epoch  17, Step:    48400, Batch Loss:     2.134677, Tokens per Sec:     7470, Lr: 0.000300\n",
      "2021-08-02 12:01:00,500 - INFO - joeynmt.training - Epoch  17, Step:    48600, Batch Loss:     2.128634, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-08-02 12:01:58,385 - INFO - joeynmt.training - Epoch  17, Step:    48800, Batch Loss:     2.192289, Tokens per Sec:     7488, Lr: 0.000300\n",
      "2021-08-02 12:02:04,532 - INFO - joeynmt.training - Epoch  17: total training loss 5933.57\n",
      "2021-08-02 12:02:04,532 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-08-02 12:02:56,195 - INFO - joeynmt.training - Epoch  18, Step:    49000, Batch Loss:     1.826431, Tokens per Sec:     7238, Lr: 0.000300\n",
      "2021-08-02 12:03:54,102 - INFO - joeynmt.training - Epoch  18, Step:    49200, Batch Loss:     1.995817, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-08-02 12:04:51,490 - INFO - joeynmt.training - Epoch  18, Step:    49400, Batch Loss:     1.918187, Tokens per Sec:     7431, Lr: 0.000300\n",
      "2021-08-02 12:05:48,993 - INFO - joeynmt.training - Epoch  18, Step:    49600, Batch Loss:     1.831600, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-08-02 12:06:46,656 - INFO - joeynmt.training - Epoch  18, Step:    49800, Batch Loss:     1.968241, Tokens per Sec:     7403, Lr: 0.000300\n",
      "2021-08-02 12:07:44,508 - INFO - joeynmt.training - Epoch  18, Step:    50000, Batch Loss:     2.032471, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-08-02 12:09:27,842 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 12:09:27,842 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 12:09:27,843 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 12:09:28,637 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 12:09:28,638 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 12:09:29,488 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 12:09:29,492 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 12:09:29,492 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 12:09:29,492 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 12:09:29,492 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 12:09:29,493 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 12:09:29,493 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 12:09:29,494 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and to read the Scriptures if possible .\n",
      "2021-08-02 12:09:29,494 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 12:09:29,494 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 12:09:29,495 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 12:09:29,496 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a lifestyle ?\n",
      "2021-08-02 12:09:29,496 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 12:09:29,496 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 12:09:29,497 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 12:09:29,497 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-02 12:09:29,497 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    50000: bleu:  22.80, loss: 119261.8828, ppl:   6.6007, duration: 104.9884s\n",
      "2021-08-02 12:10:27,523 - INFO - joeynmt.training - Epoch  18, Step:    50200, Batch Loss:     2.092854, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-08-02 12:11:24,837 - INFO - joeynmt.training - Epoch  18, Step:    50400, Batch Loss:     2.204249, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-08-02 12:12:21,921 - INFO - joeynmt.training - Epoch  18, Step:    50600, Batch Loss:     2.089711, Tokens per Sec:     7494, Lr: 0.000300\n",
      "2021-08-02 12:13:19,175 - INFO - joeynmt.training - Epoch  18, Step:    50800, Batch Loss:     2.160221, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-08-02 12:14:16,802 - INFO - joeynmt.training - Epoch  18, Step:    51000, Batch Loss:     1.863305, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-08-02 12:15:14,351 - INFO - joeynmt.training - Epoch  18, Step:    51200, Batch Loss:     1.823678, Tokens per Sec:     7524, Lr: 0.000300\n",
      "2021-08-02 12:16:12,608 - INFO - joeynmt.training - Epoch  18, Step:    51400, Batch Loss:     2.391623, Tokens per Sec:     7420, Lr: 0.000300\n",
      "2021-08-02 12:17:09,811 - INFO - joeynmt.training - Epoch  18, Step:    51600, Batch Loss:     2.151103, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-08-02 12:17:36,491 - INFO - joeynmt.training - Epoch  18: total training loss 5889.97\n",
      "2021-08-02 12:17:36,491 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-08-02 12:18:07,388 - INFO - joeynmt.training - Epoch  19, Step:    51800, Batch Loss:     2.108560, Tokens per Sec:     7352, Lr: 0.000300\n",
      "2021-08-02 12:19:04,447 - INFO - joeynmt.training - Epoch  19, Step:    52000, Batch Loss:     1.950553, Tokens per Sec:     7504, Lr: 0.000300\n",
      "2021-08-02 12:20:47,341 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 12:20:47,342 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 12:20:47,342 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 12:20:48,121 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 12:20:48,122 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 12:20:48,937 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 12:20:48,938 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 12:20:48,938 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 12:20:48,939 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking with us .\n",
      "2021-08-02 12:20:48,939 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 12:20:48,939 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 12:20:48,940 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 12:20:48,940 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-02 12:20:48,940 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 12:20:48,941 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 12:20:48,941 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 12:20:48,941 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a lie ?\n",
      "2021-08-02 12:20:48,942 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 12:20:48,943 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 12:20:48,943 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 12:20:48,943 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-02 12:20:48,943 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    52000: bleu:  23.14, loss: 118485.7031, ppl:   6.5201, duration: 104.4959s\n",
      "2021-08-02 12:21:46,617 - INFO - joeynmt.training - Epoch  19, Step:    52200, Batch Loss:     2.110132, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-08-02 12:22:43,492 - INFO - joeynmt.training - Epoch  19, Step:    52400, Batch Loss:     1.924193, Tokens per Sec:     7479, Lr: 0.000300\n",
      "2021-08-02 12:23:40,494 - INFO - joeynmt.training - Epoch  19, Step:    52600, Batch Loss:     2.141419, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-08-02 12:24:37,706 - INFO - joeynmt.training - Epoch  19, Step:    52800, Batch Loss:     1.915366, Tokens per Sec:     7538, Lr: 0.000300\n",
      "2021-08-02 12:25:35,879 - INFO - joeynmt.training - Epoch  19, Step:    53000, Batch Loss:     1.730257, Tokens per Sec:     7561, Lr: 0.000300\n",
      "2021-08-02 12:26:33,443 - INFO - joeynmt.training - Epoch  19, Step:    53200, Batch Loss:     2.155442, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-08-02 12:27:30,597 - INFO - joeynmt.training - Epoch  19, Step:    53400, Batch Loss:     1.734011, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-08-02 12:28:28,395 - INFO - joeynmt.training - Epoch  19, Step:    53600, Batch Loss:     1.884359, Tokens per Sec:     7530, Lr: 0.000300\n",
      "2021-08-02 12:29:25,166 - INFO - joeynmt.training - Epoch  19, Step:    53800, Batch Loss:     2.174797, Tokens per Sec:     7313, Lr: 0.000300\n",
      "2021-08-02 12:30:22,164 - INFO - joeynmt.training - Epoch  19, Step:    54000, Batch Loss:     2.153689, Tokens per Sec:     7474, Lr: 0.000300\n",
      "2021-08-02 12:32:03,296 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-02 12:32:03,296 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-02 12:32:03,296 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-02 12:32:04,063 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-02 12:32:04,064 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-02 12:32:04,905 - INFO - joeynmt.training - Example #0\n",
      "2021-08-02 12:32:04,906 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-02 12:32:04,906 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-02 12:32:04,907 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-02 12:32:04,907 - INFO - joeynmt.training - Example #1\n",
      "2021-08-02 12:32:04,908 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-02 12:32:04,908 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-02 12:32:04,908 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures when possible .\n",
      "2021-08-02 12:32:04,908 - INFO - joeynmt.training - Example #2\n",
      "2021-08-02 12:32:04,910 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-02 12:32:04,911 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-02 12:32:04,911 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unbelievable ?\n",
      "2021-08-02 12:32:04,911 - INFO - joeynmt.training - Example #3\n",
      "2021-08-02 12:32:04,912 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-02 12:32:04,912 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-02 12:32:04,913 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-02 12:32:04,913 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    54000: bleu:  23.37, loss: 117965.1797, ppl:   6.4666, duration: 102.7486s\n",
      "2021-08-02 12:33:02,591 - INFO - joeynmt.training - Epoch  19, Step:    54200, Batch Loss:     2.151645, Tokens per Sec:     7375, Lr: 0.000300\n",
      "2021-08-02 12:34:00,050 - INFO - joeynmt.training - Epoch  19, Step:    54400, Batch Loss:     1.925038, Tokens per Sec:     7531, Lr: 0.000300\n",
      "2021-08-02 12:34:48,494 - INFO - joeynmt.training - Epoch  19: total training loss 5849.27\n",
      "2021-08-02 12:34:48,495 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-08-02 12:34:57,640 - INFO - joeynmt.training - Epoch  20, Step:    54600, Batch Loss:     1.886330, Tokens per Sec:     7048, Lr: 0.000300\n",
      "2021-08-02 12:35:55,353 - INFO - joeynmt.training - Epoch  20, Step:    54800, Batch Loss:     2.007174, Tokens per Sec:     7450, Lr: 0.000300\n",
      "2021-08-02 12:36:52,794 - INFO - joeynmt.training - Epoch  20, Step:    55000, Batch Loss:     2.196921, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-08-02 12:37:50,001 - INFO - joeynmt.training - Epoch  20, Step:    55200, Batch Loss:     1.414576, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-08-02 12:38:47,876 - INFO - joeynmt.training - Epoch  20, Step:    55400, Batch Loss:     1.974208, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-08-02 12:39:45,535 - INFO - joeynmt.training - Epoch  20, Step:    55600, Batch Loss:     2.252867, Tokens per Sec:     7538, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4jHwlYp4lMu",
    "outputId": "4808935d-e513-4c5e-f148-a4145f9216d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2000\tLoss: 239046.32812\tPPL: 43.93091\tbleu: 2.59077\tLR: 0.00030000\t*\n",
      "Steps: 4000\tLoss: 200051.87500\tPPL: 23.70245\tbleu: 5.88961\tLR: 0.00030000\t*\n",
      "Steps: 6000\tLoss: 181477.92188\tPPL: 17.66646\tbleu: 8.94599\tLR: 0.00030000\t*\n",
      "Steps: 8000\tLoss: 170118.23438\tPPL: 14.75991\tbleu: 10.73275\tLR: 0.00030000\t*\n",
      "Steps: 10000\tLoss: 161002.07812\tPPL: 12.77721\tbleu: 12.36705\tLR: 0.00030000\t*\n",
      "Steps: 12000\tLoss: 154223.28125\tPPL: 11.47759\tbleu: 14.21227\tLR: 0.00030000\t*\n",
      "Steps: 14000\tLoss: 148909.89062\tPPL: 10.55203\tbleu: 15.67760\tLR: 0.00030000\t*\n",
      "Steps: 16000\tLoss: 145315.04688\tPPL: 9.96854\tbleu: 16.65696\tLR: 0.00030000\t*\n",
      "Steps: 18000\tLoss: 141519.07812\tPPL: 9.38739\tbleu: 17.46200\tLR: 0.00030000\t*\n",
      "Steps: 20000\tLoss: 138279.90625\tPPL: 8.91836\tbleu: 18.38668\tLR: 0.00030000\t*\n",
      "Steps: 22000\tLoss: 136026.46875\tPPL: 8.60595\tbleu: 18.89399\tLR: 0.00030000\t*\n",
      "Steps: 24000\tLoss: 133939.81250\tPPL: 8.32643\tbleu: 19.37147\tLR: 0.00030000\t*\n",
      "Steps: 26000\tLoss: 131800.14062\tPPL: 8.04924\tbleu: 19.53451\tLR: 0.00030000\t*\n",
      "Steps: 28000\tLoss: 129772.47656\tPPL: 7.79507\tbleu: 20.09657\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 128315.14844\tPPL: 7.61737\tbleu: 20.56627\tLR: 0.00030000\t*\n",
      "Steps: 32000\tLoss: 127159.10938\tPPL: 7.47930\tbleu: 20.97242\tLR: 0.00030000\t*\n",
      "Steps: 34000\tLoss: 126031.06250\tPPL: 7.34697\tbleu: 21.16603\tLR: 0.00030000\t*\n",
      "Steps: 36000\tLoss: 125395.42188\tPPL: 7.27345\tbleu: 21.65910\tLR: 0.00030000\t*\n",
      "Steps: 38000\tLoss: 123841.50781\tPPL: 7.09678\tbleu: 21.54042\tLR: 0.00030000\t*\n",
      "Steps: 40000\tLoss: 122755.80469\tPPL: 6.97590\tbleu: 21.84254\tLR: 0.00030000\t*\n",
      "Steps: 42000\tLoss: 122551.13281\tPPL: 6.95335\tbleu: 22.20969\tLR: 0.00030000\t*\n",
      "Steps: 44000\tLoss: 121417.78906\tPPL: 6.82976\tbleu: 22.31341\tLR: 0.00030000\t*\n",
      "Steps: 46000\tLoss: 120600.67969\tPPL: 6.74202\tbleu: 22.67372\tLR: 0.00030000\t*\n",
      "Steps: 48000\tLoss: 119717.82812\tPPL: 6.64849\tbleu: 22.81077\tLR: 0.00030000\t*\n",
      "Steps: 50000\tLoss: 119261.88281\tPPL: 6.60069\tbleu: 22.79885\tLR: 0.00030000\t*\n",
      "Steps: 52000\tLoss: 118485.70312\tPPL: 6.52012\tbleu: 23.14207\tLR: 0.00030000\t*\n",
      "Steps: 54000\tLoss: 117965.17969\tPPL: 6.46664\tbleu: 23.37123\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lg_lhen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJWjURXjwSnG"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 54000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"{path}/models/lg_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"{path}/joeynmt/models/lg_lhen_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lg_lhen_reverse_transformer\"', f'model_dir: \"models/lg_lhen_reverse_transformer_continued\"').replace(\n",
    "        f'epochs: 30', f'epochs: 11')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CY-9k-G7VMd6",
    "outputId": "5e40ff3f-4703-42f4-ecec-71fe5a5ec9ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lg_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lg_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer/54000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 11                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lg_lhen_reverse_transformer_continued\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_lg_lhen_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zG7hbW9vuvT2",
    "outputId": "b7439c01-ba12-49ec-9fa4-ddb19bcff90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 07:23:58,873 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 07:23:58,949 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 07:24:05,784 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 07:24:06,591 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 07:24:08,060 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 07:24:09,362 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 07:24:09,363 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 07:24:09,762 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 07:24:10.042436: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 07:24:12,294 - INFO - joeynmt.training - Total params: 12151808\n",
      "2021-08-03 07:24:14,846 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer/54000.ckpt\n",
      "2021-08-03 07:24:15,423 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
      "2021-08-03 07:24:15,424 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
      "2021-08-03 07:24:15,424 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 07:24:15,424 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
      "2021-08-03 07:24:15,424 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
      "2021-08-03 07:24:15,425 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
      "2021-08-03 07:24:15,425 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 07:24:15,425 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 07:24:15,425 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 07:24:15,426 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-03 07:24:15,426 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-03 07:24:15,426 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 07:24:15,426 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 07:24:15,427 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer/54000.ckpt\n",
      "2021-08-03 07:24:15,427 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 07:24:15,427 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 07:24:15,427 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 07:24:15,428 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 07:24:15,428 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 07:24:15,428 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 07:24:15,429 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 07:24:15,429 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 07:24:15,429 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 07:24:15,429 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 07:24:15,430 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 07:24:15,430 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 07:24:15,430 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 07:24:15,430 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 07:24:15,431 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 07:24:15,431 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 07:24:15,431 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-03 07:24:15,431 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 07:24:15,431 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 07:24:15,432 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 07:24:15,432 - INFO - joeynmt.helpers - cfg.training.epochs                : 11\n",
      "2021-08-03 07:24:15,432 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-03 07:24:15,432 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-03 07:24:15,433 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 07:24:15,433 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer_continued\n",
      "2021-08-03 07:24:15,433 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 07:24:15,433 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 07:24:15,434 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 07:24:15,434 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 07:24:15,434 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 07:24:15,434 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 07:24:15,435 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 07:24:15,435 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 07:24:15,435 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 07:24:15,435 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 07:24:15,436 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 07:24:15,436 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 07:24:15,436 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 07:24:15,436 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 07:24:15,437 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 07:24:15,437 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 07:24:15,437 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 07:24:15,437 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 07:24:15,438 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 07:24:15,438 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 07:24:15,438 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 07:24:15,438 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 07:24:15,438 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 07:24:15,439 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 07:24:15,439 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 07:24:15,442 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 07:24:15,442 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 07:24:15,443 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 07:24:15,443 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 07:24:15,443 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 07:24:15,444 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 07:24:15,444 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 229609,\n",
      "\tvalid 2349,\n",
      "\ttest 79\n",
      "2021-08-03 07:24:15,444 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-03 07:24:15,444 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 07:24:15,445 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 07:24:15,445 - INFO - joeynmt.helpers - Number of Src words (types): 4264\n",
      "2021-08-03 07:24:15,445 - INFO - joeynmt.helpers - Number of Trg words (types): 4264\n",
      "2021-08-03 07:24:15,446 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4264),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4264))\n",
      "2021-08-03 07:24:15,465 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 07:24:15,466 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 07:25:15,246 - INFO - joeynmt.training - Epoch   1, Step:    54200, Batch Loss:     2.134974, Tokens per Sec:     7116, Lr: 0.000300\n",
      "2021-08-03 07:26:14,603 - INFO - joeynmt.training - Epoch   1, Step:    54400, Batch Loss:     1.907388, Tokens per Sec:     7290, Lr: 0.000300\n",
      "2021-08-03 07:27:04,519 - INFO - joeynmt.training - Epoch   1: total training loss 1157.52\n",
      "2021-08-03 07:27:04,520 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 07:27:14,031 - INFO - joeynmt.training - Epoch   2, Step:    54600, Batch Loss:     1.876415, Tokens per Sec:     6777, Lr: 0.000300\n",
      "2021-08-03 07:28:13,259 - INFO - joeynmt.training - Epoch   2, Step:    54800, Batch Loss:     2.061929, Tokens per Sec:     7259, Lr: 0.000300\n",
      "2021-08-03 07:29:12,372 - INFO - joeynmt.training - Epoch   2, Step:    55000, Batch Loss:     2.190480, Tokens per Sec:     7298, Lr: 0.000300\n",
      "2021-08-03 07:30:11,177 - INFO - joeynmt.training - Epoch   2, Step:    55200, Batch Loss:     1.416245, Tokens per Sec:     7199, Lr: 0.000300\n",
      "2021-08-03 07:31:10,458 - INFO - joeynmt.training - Epoch   2, Step:    55400, Batch Loss:     1.956417, Tokens per Sec:     7230, Lr: 0.000300\n",
      "2021-08-03 07:32:09,969 - INFO - joeynmt.training - Epoch   2, Step:    55600, Batch Loss:     2.256920, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-08-03 07:33:09,077 - INFO - joeynmt.training - Epoch   2, Step:    55800, Batch Loss:     1.795020, Tokens per Sec:     7330, Lr: 0.000300\n",
      "2021-08-03 07:34:08,626 - INFO - joeynmt.training - Epoch   2, Step:    56000, Batch Loss:     1.936045, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-08-03 07:35:51,498 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 07:35:51,499 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 07:35:51,499 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 07:35:52,262 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 07:35:52,262 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 07:35:54,061 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 07:35:54,063 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 07:35:54,063 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 07:35:54,063 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 07:35:54,063 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 07:35:54,065 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 07:35:54,065 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 07:35:54,066 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-03 07:35:54,066 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 07:35:54,066 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 07:35:54,067 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 07:35:54,067 - INFO - joeynmt.training - \tHypothesis: Why do false gods not have a lie ?\n",
      "2021-08-03 07:35:54,067 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 07:35:54,068 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 07:35:54,068 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 07:35:54,068 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 07:35:54,069 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    56000: bleu:  23.16, loss: 117355.3359, ppl:   6.4045, duration: 105.4420s\n",
      "2021-08-03 07:36:53,789 - INFO - joeynmt.training - Epoch   2, Step:    56200, Batch Loss:     1.949908, Tokens per Sec:     7237, Lr: 0.000300\n",
      "2021-08-03 07:37:52,608 - INFO - joeynmt.training - Epoch   2, Step:    56400, Batch Loss:     2.130255, Tokens per Sec:     7245, Lr: 0.000300\n",
      "2021-08-03 07:38:51,200 - INFO - joeynmt.training - Epoch   2, Step:    56600, Batch Loss:     2.054133, Tokens per Sec:     7160, Lr: 0.000300\n",
      "2021-08-03 07:39:50,180 - INFO - joeynmt.training - Epoch   2, Step:    56800, Batch Loss:     2.049974, Tokens per Sec:     7171, Lr: 0.000300\n",
      "2021-08-03 07:40:49,128 - INFO - joeynmt.training - Epoch   2, Step:    57000, Batch Loss:     2.157594, Tokens per Sec:     7229, Lr: 0.000300\n",
      "2021-08-03 07:41:48,317 - INFO - joeynmt.training - Epoch   2, Step:    57200, Batch Loss:     2.122809, Tokens per Sec:     7286, Lr: 0.000300\n",
      "2021-08-03 07:42:47,693 - INFO - joeynmt.training - Epoch   2, Step:    57400, Batch Loss:     2.079001, Tokens per Sec:     7238, Lr: 0.000300\n",
      "2021-08-03 07:42:59,053 - INFO - joeynmt.training - Epoch   2: total training loss 5797.99\n",
      "2021-08-03 07:42:59,054 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 07:43:46,740 - INFO - joeynmt.training - Epoch   3, Step:    57600, Batch Loss:     1.742351, Tokens per Sec:     7225, Lr: 0.000300\n",
      "2021-08-03 07:44:45,733 - INFO - joeynmt.training - Epoch   3, Step:    57800, Batch Loss:     2.051491, Tokens per Sec:     7248, Lr: 0.000300\n",
      "2021-08-03 07:45:44,177 - INFO - joeynmt.training - Epoch   3, Step:    58000, Batch Loss:     2.128270, Tokens per Sec:     7241, Lr: 0.000300\n",
      "2021-08-03 07:47:27,720 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 07:47:27,720 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 07:47:27,720 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 07:47:29,362 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 07:47:29,363 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 07:47:29,363 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 07:47:29,364 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking with us .\n",
      "2021-08-03 07:47:29,364 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 07:47:29,364 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 07:47:29,365 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 07:47:29,365 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-03 07:47:29,365 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 07:47:29,366 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 07:47:29,366 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 07:47:29,366 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unscriptural ?\n",
      "2021-08-03 07:47:29,366 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 07:47:29,367 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 07:47:29,367 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 07:47:29,368 - INFO - joeynmt.training - \tHypothesis: True , Jehovah has dealt with his people in a loving way .\n",
      "2021-08-03 07:47:29,368 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    58000: bleu:  23.59, loss: 117431.3359, ppl:   6.4122, duration: 105.1899s\n",
      "2021-08-03 07:48:28,359 - INFO - joeynmt.training - Epoch   3, Step:    58200, Batch Loss:     2.008332, Tokens per Sec:     7258, Lr: 0.000300\n",
      "2021-08-03 07:49:27,393 - INFO - joeynmt.training - Epoch   3, Step:    58400, Batch Loss:     2.144121, Tokens per Sec:     7295, Lr: 0.000300\n",
      "2021-08-03 07:50:26,607 - INFO - joeynmt.training - Epoch   3, Step:    58600, Batch Loss:     2.072344, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-08-03 07:51:25,970 - INFO - joeynmt.training - Epoch   3, Step:    58800, Batch Loss:     1.833155, Tokens per Sec:     7277, Lr: 0.000300\n",
      "2021-08-03 07:52:24,664 - INFO - joeynmt.training - Epoch   3, Step:    59000, Batch Loss:     2.021747, Tokens per Sec:     7167, Lr: 0.000300\n",
      "2021-08-03 07:53:23,536 - INFO - joeynmt.training - Epoch   3, Step:    59200, Batch Loss:     2.066360, Tokens per Sec:     7202, Lr: 0.000300\n",
      "2021-08-03 07:54:22,599 - INFO - joeynmt.training - Epoch   3, Step:    59400, Batch Loss:     2.134309, Tokens per Sec:     7243, Lr: 0.000300\n",
      "2021-08-03 07:55:20,966 - INFO - joeynmt.training - Epoch   3, Step:    59600, Batch Loss:     1.748480, Tokens per Sec:     7218, Lr: 0.000300\n",
      "2021-08-03 07:56:20,060 - INFO - joeynmt.training - Epoch   3, Step:    59800, Batch Loss:     1.881799, Tokens per Sec:     7311, Lr: 0.000300\n",
      "2021-08-03 07:57:19,060 - INFO - joeynmt.training - Epoch   3, Step:    60000, Batch Loss:     2.156317, Tokens per Sec:     7314, Lr: 0.000300\n",
      "2021-08-03 07:58:57,086 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 07:58:57,086 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 07:58:57,087 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 07:58:57,838 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 07:58:57,839 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 07:58:58,773 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 07:58:58,774 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 07:58:58,774 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 07:58:58,774 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 07:58:58,775 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 07:58:58,775 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 07:58:58,776 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 07:58:58,776 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-03 07:58:58,776 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 07:58:58,777 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 07:58:58,777 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 07:58:58,777 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unbelievable ?\n",
      "2021-08-03 07:58:58,778 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 07:58:58,778 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 07:58:58,779 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 07:58:58,779 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 07:58:58,779 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    60000: bleu:  23.68, loss: 116389.6797, ppl:   6.3074, duration: 99.7179s\n",
      "2021-08-03 07:59:57,466 - INFO - joeynmt.training - Epoch   3, Step:    60200, Batch Loss:     2.026281, Tokens per Sec:     7296, Lr: 0.000300\n",
      "2021-08-03 08:00:31,915 - INFO - joeynmt.training - Epoch   3: total training loss 5778.13\n",
      "2021-08-03 08:00:31,915 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 08:00:56,798 - INFO - joeynmt.training - Epoch   4, Step:    60400, Batch Loss:     1.727922, Tokens per Sec:     7114, Lr: 0.000300\n",
      "2021-08-03 08:01:55,689 - INFO - joeynmt.training - Epoch   4, Step:    60600, Batch Loss:     1.928691, Tokens per Sec:     7230, Lr: 0.000300\n",
      "2021-08-03 08:02:55,049 - INFO - joeynmt.training - Epoch   4, Step:    60800, Batch Loss:     2.091293, Tokens per Sec:     7311, Lr: 0.000300\n",
      "2021-08-03 08:03:54,030 - INFO - joeynmt.training - Epoch   4, Step:    61000, Batch Loss:     2.145411, Tokens per Sec:     7255, Lr: 0.000300\n",
      "2021-08-03 08:04:53,037 - INFO - joeynmt.training - Epoch   4, Step:    61200, Batch Loss:     2.125142, Tokens per Sec:     7255, Lr: 0.000300\n",
      "2021-08-03 08:05:51,518 - INFO - joeynmt.training - Epoch   4, Step:    61400, Batch Loss:     1.223649, Tokens per Sec:     7213, Lr: 0.000300\n",
      "2021-08-03 08:06:50,846 - INFO - joeynmt.training - Epoch   4, Step:    61600, Batch Loss:     1.934053, Tokens per Sec:     7301, Lr: 0.000300\n",
      "2021-08-03 08:07:49,649 - INFO - joeynmt.training - Epoch   4, Step:    61800, Batch Loss:     2.163256, Tokens per Sec:     7254, Lr: 0.000300\n",
      "2021-08-03 08:08:48,848 - INFO - joeynmt.training - Epoch   4, Step:    62000, Batch Loss:     1.834358, Tokens per Sec:     7302, Lr: 0.000300\n",
      "2021-08-03 08:10:28,932 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:10:28,932 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:10:28,932 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:10:29,686 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:10:29,686 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:10:30,533 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:10:30,535 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 08:10:30,535 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 08:10:30,535 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 08:10:30,536 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:10:30,536 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 08:10:30,536 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 08:10:30,537 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the ministry and to read the Scriptures if possible .\n",
      "2021-08-03 08:10:30,537 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:10:30,538 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 08:10:30,538 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 08:10:30,538 - INFO - joeynmt.training - \tHypothesis: Why are false gods unusual ?\n",
      "2021-08-03 08:10:30,538 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:10:30,539 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 08:10:30,539 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 08:10:30,540 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 08:10:30,540 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    62000: bleu:  23.97, loss: 116053.1328, ppl:   6.2739, duration: 101.6914s\n",
      "2021-08-03 08:11:29,801 - INFO - joeynmt.training - Epoch   4, Step:    62200, Batch Loss:     2.044275, Tokens per Sec:     7266, Lr: 0.000300\n",
      "2021-08-03 08:12:28,983 - INFO - joeynmt.training - Epoch   4, Step:    62400, Batch Loss:     1.998006, Tokens per Sec:     7327, Lr: 0.000300\n",
      "2021-08-03 08:13:28,033 - INFO - joeynmt.training - Epoch   4, Step:    62600, Batch Loss:     2.050276, Tokens per Sec:     7242, Lr: 0.000300\n",
      "2021-08-03 08:14:27,097 - INFO - joeynmt.training - Epoch   4, Step:    62800, Batch Loss:     2.025548, Tokens per Sec:     7273, Lr: 0.000300\n",
      "2021-08-03 08:15:26,615 - INFO - joeynmt.training - Epoch   4, Step:    63000, Batch Loss:     2.015141, Tokens per Sec:     7300, Lr: 0.000300\n",
      "2021-08-03 08:16:20,121 - INFO - joeynmt.training - Epoch   4: total training loss 5707.95\n",
      "2021-08-03 08:16:20,122 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-03 08:16:26,229 - INFO - joeynmt.training - Epoch   5, Step:    63200, Batch Loss:     1.961713, Tokens per Sec:     6874, Lr: 0.000300\n",
      "2021-08-03 08:17:24,858 - INFO - joeynmt.training - Epoch   5, Step:    63400, Batch Loss:     1.642421, Tokens per Sec:     7249, Lr: 0.000300\n",
      "2021-08-03 08:18:24,119 - INFO - joeynmt.training - Epoch   5, Step:    63600, Batch Loss:     2.069963, Tokens per Sec:     7251, Lr: 0.000300\n",
      "2021-08-03 08:19:22,503 - INFO - joeynmt.training - Epoch   5, Step:    63800, Batch Loss:     1.975146, Tokens per Sec:     7245, Lr: 0.000300\n",
      "2021-08-03 08:20:21,435 - INFO - joeynmt.training - Epoch   5, Step:    64000, Batch Loss:     1.978279, Tokens per Sec:     7293, Lr: 0.000300\n",
      "2021-08-03 08:22:04,016 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:22:04,016 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:22:04,016 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:22:04,789 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:22:04,789 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:22:05,699 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:22:05,700 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 08:22:05,701 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 08:22:05,701 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 08:22:05,701 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:22:05,702 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 08:22:05,702 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 08:22:05,702 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in the ministry and to study the Scriptures if possible .\n",
      "2021-08-03 08:22:05,702 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:22:05,703 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 08:22:05,703 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 08:22:05,703 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-03 08:22:05,704 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:22:05,704 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 08:22:05,704 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 08:22:05,705 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 08:22:05,705 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    64000: bleu:  23.83, loss: 115649.6250, ppl:   6.2340, duration: 104.2692s\n",
      "2021-08-03 08:23:05,033 - INFO - joeynmt.training - Epoch   5, Step:    64200, Batch Loss:     2.065234, Tokens per Sec:     7307, Lr: 0.000300\n",
      "2021-08-03 08:24:03,755 - INFO - joeynmt.training - Epoch   5, Step:    64400, Batch Loss:     1.973214, Tokens per Sec:     7134, Lr: 0.000300\n",
      "2021-08-03 08:25:02,697 - INFO - joeynmt.training - Epoch   5, Step:    64600, Batch Loss:     1.904945, Tokens per Sec:     7256, Lr: 0.000300\n",
      "2021-08-03 08:26:01,563 - INFO - joeynmt.training - Epoch   5, Step:    64800, Batch Loss:     1.960350, Tokens per Sec:     7219, Lr: 0.000300\n",
      "2021-08-03 08:27:00,683 - INFO - joeynmt.training - Epoch   5, Step:    65000, Batch Loss:     1.910627, Tokens per Sec:     7237, Lr: 0.000300\n",
      "2021-08-03 08:27:59,813 - INFO - joeynmt.training - Epoch   5, Step:    65200, Batch Loss:     1.946978, Tokens per Sec:     7331, Lr: 0.000300\n",
      "2021-08-03 08:28:58,638 - INFO - joeynmt.training - Epoch   5, Step:    65400, Batch Loss:     2.042474, Tokens per Sec:     7246, Lr: 0.000300\n",
      "2021-08-03 08:29:57,998 - INFO - joeynmt.training - Epoch   5, Step:    65600, Batch Loss:     2.248182, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-08-03 08:30:57,203 - INFO - joeynmt.training - Epoch   5, Step:    65800, Batch Loss:     2.348505, Tokens per Sec:     7314, Lr: 0.000300\n",
      "2021-08-03 08:31:56,030 - INFO - joeynmt.training - Epoch   5, Step:    66000, Batch Loss:     1.675930, Tokens per Sec:     7231, Lr: 0.000300\n",
      "2021-08-03 08:33:36,033 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:33:36,034 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:33:36,034 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:33:36,787 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:33:36,787 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:33:37,644 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:33:37,645 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 08:33:37,645 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 08:33:37,645 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 08:33:37,646 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:33:37,648 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 08:33:37,649 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 08:33:37,649 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures as possible .\n",
      "2021-08-03 08:33:37,649 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:33:37,650 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 08:33:37,650 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 08:33:37,650 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 08:33:37,651 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:33:37,651 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 08:33:37,651 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 08:33:37,653 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 08:33:37,653 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    66000: bleu:  24.08, loss: 115222.4531, ppl:   6.1920, duration: 101.6229s\n",
      "2021-08-03 08:33:54,050 - INFO - joeynmt.training - Epoch   5: total training loss 5697.89\n",
      "2021-08-03 08:33:54,051 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-03 08:34:37,118 - INFO - joeynmt.training - Epoch   6, Step:    66200, Batch Loss:     1.914789, Tokens per Sec:     7215, Lr: 0.000300\n",
      "2021-08-03 08:35:35,622 - INFO - joeynmt.training - Epoch   6, Step:    66400, Batch Loss:     1.994754, Tokens per Sec:     7280, Lr: 0.000300\n",
      "2021-08-03 08:36:34,438 - INFO - joeynmt.training - Epoch   6, Step:    66600, Batch Loss:     1.946630, Tokens per Sec:     7220, Lr: 0.000300\n",
      "2021-08-03 08:37:33,748 - INFO - joeynmt.training - Epoch   6, Step:    66800, Batch Loss:     1.826667, Tokens per Sec:     7231, Lr: 0.000300\n",
      "2021-08-03 08:38:33,561 - INFO - joeynmt.training - Epoch   6, Step:    67000, Batch Loss:     1.955985, Tokens per Sec:     7332, Lr: 0.000300\n",
      "2021-08-03 08:39:31,933 - INFO - joeynmt.training - Epoch   6, Step:    67200, Batch Loss:     1.602102, Tokens per Sec:     7225, Lr: 0.000300\n",
      "2021-08-03 08:40:30,821 - INFO - joeynmt.training - Epoch   6, Step:    67400, Batch Loss:     2.153773, Tokens per Sec:     7228, Lr: 0.000300\n",
      "2021-08-03 08:41:29,897 - INFO - joeynmt.training - Epoch   6, Step:    67600, Batch Loss:     1.855829, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-08-03 08:42:28,894 - INFO - joeynmt.training - Epoch   6, Step:    67800, Batch Loss:     1.912695, Tokens per Sec:     7366, Lr: 0.000300\n",
      "2021-08-03 08:43:28,075 - INFO - joeynmt.training - Epoch   6, Step:    68000, Batch Loss:     2.104418, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-08-03 08:45:08,452 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:45:08,453 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:45:08,453 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:45:09,204 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:45:09,205 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:45:10,070 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:45:10,071 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 08:45:10,071 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 08:45:10,072 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 08:45:10,072 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:45:10,073 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 08:45:10,073 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 08:45:10,073 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and striving to read the Scriptures if possible .\n",
      "2021-08-03 08:45:10,073 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:45:10,074 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 08:45:10,074 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 08:45:10,074 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 08:45:10,075 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:45:10,075 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 08:45:10,075 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 08:45:10,076 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 08:45:10,076 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    68000: bleu:  23.95, loss: 114941.3281, ppl:   6.1645, duration: 102.0001s\n",
      "2021-08-03 08:46:09,073 - INFO - joeynmt.training - Epoch   6, Step:    68200, Batch Loss:     1.875711, Tokens per Sec:     7236, Lr: 0.000300\n",
      "2021-08-03 08:47:07,989 - INFO - joeynmt.training - Epoch   6, Step:    68400, Batch Loss:     2.146406, Tokens per Sec:     7222, Lr: 0.000300\n",
      "2021-08-03 08:48:07,162 - INFO - joeynmt.training - Epoch   6, Step:    68600, Batch Loss:     2.256990, Tokens per Sec:     7215, Lr: 0.000300\n",
      "2021-08-03 08:49:06,098 - INFO - joeynmt.training - Epoch   6, Step:    68800, Batch Loss:     2.031236, Tokens per Sec:     7257, Lr: 0.000300\n",
      "2021-08-03 08:49:42,573 - INFO - joeynmt.training - Epoch   6: total training loss 5657.36\n",
      "2021-08-03 08:49:42,573 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-03 08:50:05,454 - INFO - joeynmt.training - Epoch   7, Step:    69000, Batch Loss:     1.802232, Tokens per Sec:     7193, Lr: 0.000300\n",
      "2021-08-03 08:51:04,693 - INFO - joeynmt.training - Epoch   7, Step:    69200, Batch Loss:     1.856143, Tokens per Sec:     7318, Lr: 0.000300\n",
      "2021-08-03 08:52:03,717 - INFO - joeynmt.training - Epoch   7, Step:    69400, Batch Loss:     2.189476, Tokens per Sec:     7278, Lr: 0.000300\n",
      "2021-08-03 08:53:02,529 - INFO - joeynmt.training - Epoch   7, Step:    69600, Batch Loss:     1.993216, Tokens per Sec:     7167, Lr: 0.000300\n",
      "2021-08-03 08:54:01,288 - INFO - joeynmt.training - Epoch   7, Step:    69800, Batch Loss:     2.175827, Tokens per Sec:     7209, Lr: 0.000300\n",
      "2021-08-03 08:55:00,700 - INFO - joeynmt.training - Epoch   7, Step:    70000, Batch Loss:     1.909107, Tokens per Sec:     7321, Lr: 0.000300\n",
      "2021-08-03 08:56:47,616 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 08:56:47,616 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 08:56:47,617 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 08:56:48,395 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 08:56:48,396 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 08:56:49,276 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 08:56:49,278 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 08:56:49,278 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 08:56:49,278 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 08:56:49,278 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 08:56:49,279 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 08:56:49,279 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 08:56:49,280 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the ministry and to read the Scriptures if possible .\n",
      "2021-08-03 08:56:49,280 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 08:56:49,281 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 08:56:49,282 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 08:56:49,282 - INFO - joeynmt.training - \tHypothesis: Why do false gods have unbelievable things ?\n",
      "2021-08-03 08:56:49,283 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 08:56:49,283 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 08:56:49,284 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 08:56:49,284 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 08:56:49,284 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    70000: bleu:  24.35, loss: 114345.6328, ppl:   6.1067, duration: 108.5840s\n",
      "2021-08-03 08:57:48,599 - INFO - joeynmt.training - Epoch   7, Step:    70200, Batch Loss:     1.925089, Tokens per Sec:     7226, Lr: 0.000300\n",
      "2021-08-03 08:58:46,722 - INFO - joeynmt.training - Epoch   7, Step:    70400, Batch Loss:     1.573885, Tokens per Sec:     7211, Lr: 0.000300\n",
      "2021-08-03 08:59:45,701 - INFO - joeynmt.training - Epoch   7, Step:    70600, Batch Loss:     1.946940, Tokens per Sec:     7243, Lr: 0.000300\n",
      "2021-08-03 09:00:44,585 - INFO - joeynmt.training - Epoch   7, Step:    70800, Batch Loss:     1.888242, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-08-03 09:01:43,685 - INFO - joeynmt.training - Epoch   7, Step:    71000, Batch Loss:     1.985286, Tokens per Sec:     7306, Lr: 0.000300\n",
      "2021-08-03 09:02:42,298 - INFO - joeynmt.training - Epoch   7, Step:    71200, Batch Loss:     2.066011, Tokens per Sec:     7147, Lr: 0.000300\n",
      "2021-08-03 09:03:41,353 - INFO - joeynmt.training - Epoch   7, Step:    71400, Batch Loss:     2.008472, Tokens per Sec:     7300, Lr: 0.000300\n",
      "2021-08-03 09:04:40,327 - INFO - joeynmt.training - Epoch   7, Step:    71600, Batch Loss:     1.981403, Tokens per Sec:     7305, Lr: 0.000300\n",
      "2021-08-03 09:05:38,924 - INFO - joeynmt.training - Epoch   7: total training loss 5641.76\n",
      "2021-08-03 09:05:38,925 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-03 09:05:39,643 - INFO - joeynmt.training - Epoch   8, Step:    71800, Batch Loss:     2.040294, Tokens per Sec:     3469, Lr: 0.000300\n",
      "2021-08-03 09:06:38,565 - INFO - joeynmt.training - Epoch   8, Step:    72000, Batch Loss:     1.974707, Tokens per Sec:     7165, Lr: 0.000300\n",
      "2021-08-03 09:08:21,423 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:08:21,424 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:08:21,424 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:08:22,212 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:08:22,213 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:08:23,091 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:08:23,092 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 09:08:23,092 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 09:08:23,092 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 09:08:23,092 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:08:23,093 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 09:08:23,093 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 09:08:23,094 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and striving to read the Scriptures if possible .\n",
      "2021-08-03 09:08:23,094 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:08:23,095 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 09:08:23,095 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 09:08:23,095 - INFO - joeynmt.training - \tHypothesis: Why do false gods have nothing ?\n",
      "2021-08-03 09:08:23,095 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:08:23,096 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 09:08:23,096 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 09:08:23,096 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 09:08:23,097 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    72000: bleu:  24.14, loss: 114159.7578, ppl:   6.0887, duration: 104.5314s\n",
      "2021-08-03 09:09:22,381 - INFO - joeynmt.training - Epoch   8, Step:    72200, Batch Loss:     1.957222, Tokens per Sec:     7244, Lr: 0.000300\n",
      "2021-08-03 09:10:21,372 - INFO - joeynmt.training - Epoch   8, Step:    72400, Batch Loss:     1.923568, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-08-03 09:11:20,817 - INFO - joeynmt.training - Epoch   8, Step:    72600, Batch Loss:     1.996720, Tokens per Sec:     7343, Lr: 0.000300\n",
      "2021-08-03 09:12:19,946 - INFO - joeynmt.training - Epoch   8, Step:    72800, Batch Loss:     2.026127, Tokens per Sec:     7195, Lr: 0.000300\n",
      "2021-08-03 09:13:19,304 - INFO - joeynmt.training - Epoch   8, Step:    73000, Batch Loss:     1.898544, Tokens per Sec:     7307, Lr: 0.000300\n",
      "2021-08-03 09:14:18,198 - INFO - joeynmt.training - Epoch   8, Step:    73200, Batch Loss:     1.881628, Tokens per Sec:     7265, Lr: 0.000300\n",
      "2021-08-03 09:15:17,390 - INFO - joeynmt.training - Epoch   8, Step:    73400, Batch Loss:     1.601482, Tokens per Sec:     7259, Lr: 0.000300\n",
      "2021-08-03 09:16:16,369 - INFO - joeynmt.training - Epoch   8, Step:    73600, Batch Loss:     1.478531, Tokens per Sec:     7270, Lr: 0.000300\n",
      "2021-08-03 09:17:14,865 - INFO - joeynmt.training - Epoch   8, Step:    73800, Batch Loss:     1.999130, Tokens per Sec:     7196, Lr: 0.000300\n",
      "2021-08-03 09:18:13,955 - INFO - joeynmt.training - Epoch   8, Step:    74000, Batch Loss:     1.997713, Tokens per Sec:     7238, Lr: 0.000300\n",
      "2021-08-03 09:19:54,365 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:19:54,366 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:19:54,366 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:19:55,142 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:19:55,143 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:19:55,989 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:19:55,990 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 09:19:55,990 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 09:19:55,991 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 09:19:55,991 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:19:55,992 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 09:19:55,992 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 09:19:55,992 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and strive to read the Scriptures if possible .\n",
      "2021-08-03 09:19:55,992 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:19:55,993 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 09:19:55,993 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 09:19:55,994 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 09:19:55,994 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:19:55,994 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 09:19:55,995 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 09:19:55,995 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 09:19:55,995 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    74000: bleu:  24.10, loss: 113609.7500, ppl:   6.0360, duration: 102.0394s\n",
      "2021-08-03 09:20:55,853 - INFO - joeynmt.training - Epoch   8, Step:    74200, Batch Loss:     2.278201, Tokens per Sec:     7279, Lr: 0.000300\n",
      "2021-08-03 09:21:55,148 - INFO - joeynmt.training - Epoch   8, Step:    74400, Batch Loss:     1.859719, Tokens per Sec:     7208, Lr: 0.000300\n",
      "2021-08-03 09:22:54,197 - INFO - joeynmt.training - Epoch   8, Step:    74600, Batch Loss:     1.940083, Tokens per Sec:     7198, Lr: 0.000300\n",
      "2021-08-03 09:23:15,163 - INFO - joeynmt.training - Epoch   8: total training loss 5603.15\n",
      "2021-08-03 09:23:15,164 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-03 09:23:54,291 - INFO - joeynmt.training - Epoch   9, Step:    74800, Batch Loss:     1.933806, Tokens per Sec:     7240, Lr: 0.000300\n",
      "2021-08-03 09:24:53,591 - INFO - joeynmt.training - Epoch   9, Step:    75000, Batch Loss:     2.012732, Tokens per Sec:     7240, Lr: 0.000300\n",
      "2021-08-03 09:25:52,745 - INFO - joeynmt.training - Epoch   9, Step:    75200, Batch Loss:     1.826425, Tokens per Sec:     7242, Lr: 0.000300\n",
      "2021-08-03 09:26:51,970 - INFO - joeynmt.training - Epoch   9, Step:    75400, Batch Loss:     1.996280, Tokens per Sec:     7150, Lr: 0.000300\n",
      "2021-08-03 09:27:51,351 - INFO - joeynmt.training - Epoch   9, Step:    75600, Batch Loss:     1.976346, Tokens per Sec:     7275, Lr: 0.000300\n",
      "2021-08-03 09:28:49,825 - INFO - joeynmt.training - Epoch   9, Step:    75800, Batch Loss:     1.997365, Tokens per Sec:     7121, Lr: 0.000300\n",
      "2021-08-03 09:29:48,833 - INFO - joeynmt.training - Epoch   9, Step:    76000, Batch Loss:     1.707721, Tokens per Sec:     7249, Lr: 0.000300\n",
      "2021-08-03 09:31:31,575 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:31:31,576 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:31:31,576 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:31:32,358 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:31:32,358 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:31:33,226 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:31:33,226 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 09:31:33,227 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 09:31:33,227 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 09:31:33,227 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:31:33,228 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 09:31:33,228 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 09:31:33,228 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-03 09:31:33,229 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:31:33,229 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 09:31:33,229 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 09:31:33,230 - INFO - joeynmt.training - \tHypothesis: Why do false gods have unique things ?\n",
      "2021-08-03 09:31:33,230 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:31:33,231 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 09:31:33,231 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 09:31:33,231 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 09:31:33,231 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    76000: bleu:  24.41, loss: 113383.9844, ppl:   6.0144, duration: 104.3973s\n",
      "2021-08-03 09:32:32,076 - INFO - joeynmt.training - Epoch   9, Step:    76200, Batch Loss:     1.921217, Tokens per Sec:     7193, Lr: 0.000300\n",
      "2021-08-03 09:33:31,849 - INFO - joeynmt.training - Epoch   9, Step:    76400, Batch Loss:     1.985546, Tokens per Sec:     7284, Lr: 0.000300\n",
      "2021-08-03 09:34:30,987 - INFO - joeynmt.training - Epoch   9, Step:    76600, Batch Loss:     1.873168, Tokens per Sec:     7158, Lr: 0.000300\n",
      "2021-08-03 09:35:30,303 - INFO - joeynmt.training - Epoch   9, Step:    76800, Batch Loss:     1.642908, Tokens per Sec:     7259, Lr: 0.000300\n",
      "2021-08-03 09:36:29,943 - INFO - joeynmt.training - Epoch   9, Step:    77000, Batch Loss:     1.920160, Tokens per Sec:     7299, Lr: 0.000300\n",
      "2021-08-03 09:37:29,164 - INFO - joeynmt.training - Epoch   9, Step:    77200, Batch Loss:     2.369294, Tokens per Sec:     7185, Lr: 0.000300\n",
      "2021-08-03 09:38:28,686 - INFO - joeynmt.training - Epoch   9, Step:    77400, Batch Loss:     2.251699, Tokens per Sec:     7267, Lr: 0.000300\n",
      "2021-08-03 09:39:11,805 - INFO - joeynmt.training - Epoch   9: total training loss 5583.87\n",
      "2021-08-03 09:39:11,806 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-03 09:39:28,131 - INFO - joeynmt.training - Epoch  10, Step:    77600, Batch Loss:     2.070147, Tokens per Sec:     7168, Lr: 0.000300\n",
      "2021-08-03 09:40:27,466 - INFO - joeynmt.training - Epoch  10, Step:    77800, Batch Loss:     2.003402, Tokens per Sec:     7229, Lr: 0.000300\n",
      "2021-08-03 09:41:26,660 - INFO - joeynmt.training - Epoch  10, Step:    78000, Batch Loss:     2.142008, Tokens per Sec:     7272, Lr: 0.000300\n",
      "2021-08-03 09:43:13,542 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:43:13,542 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:43:13,543 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:43:14,328 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:43:14,329 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:43:15,202 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:43:15,203 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 09:43:15,203 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 09:43:15,204 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 09:43:15,204 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:43:15,205 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 09:43:15,205 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 09:43:15,205 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and striving to read the Scriptures if possible .\n",
      "2021-08-03 09:43:15,205 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:43:15,206 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 09:43:15,206 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 09:43:15,206 - INFO - joeynmt.training - \tHypothesis: Why are false gods ?\n",
      "2021-08-03 09:43:15,207 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:43:15,207 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 09:43:15,208 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 09:43:15,208 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 09:43:15,208 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    78000: bleu:  24.58, loss: 113049.4922, ppl:   5.9827, duration: 108.5472s\n",
      "2021-08-03 09:44:14,908 - INFO - joeynmt.training - Epoch  10, Step:    78200, Batch Loss:     2.484359, Tokens per Sec:     7250, Lr: 0.000300\n",
      "2021-08-03 09:45:14,083 - INFO - joeynmt.training - Epoch  10, Step:    78400, Batch Loss:     2.309759, Tokens per Sec:     7134, Lr: 0.000300\n",
      "2021-08-03 09:46:13,790 - INFO - joeynmt.training - Epoch  10, Step:    78600, Batch Loss:     1.835813, Tokens per Sec:     7291, Lr: 0.000300\n",
      "2021-08-03 09:47:12,866 - INFO - joeynmt.training - Epoch  10, Step:    78800, Batch Loss:     1.844000, Tokens per Sec:     7232, Lr: 0.000300\n",
      "2021-08-03 09:48:12,775 - INFO - joeynmt.training - Epoch  10, Step:    79000, Batch Loss:     1.954631, Tokens per Sec:     7294, Lr: 0.000300\n",
      "2021-08-03 09:49:12,262 - INFO - joeynmt.training - Epoch  10, Step:    79200, Batch Loss:     1.916483, Tokens per Sec:     7212, Lr: 0.000300\n",
      "2021-08-03 09:50:10,842 - INFO - joeynmt.training - Epoch  10, Step:    79400, Batch Loss:     2.502870, Tokens per Sec:     7154, Lr: 0.000300\n",
      "2021-08-03 09:51:09,792 - INFO - joeynmt.training - Epoch  10, Step:    79600, Batch Loss:     1.979840, Tokens per Sec:     7283, Lr: 0.000300\n",
      "2021-08-03 09:52:08,159 - INFO - joeynmt.training - Epoch  10, Step:    79800, Batch Loss:     1.974451, Tokens per Sec:     7206, Lr: 0.000300\n",
      "2021-08-03 09:53:07,239 - INFO - joeynmt.training - Epoch  10, Step:    80000, Batch Loss:     1.999018, Tokens per Sec:     7283, Lr: 0.000300\n",
      "2021-08-03 09:54:44,826 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 09:54:44,827 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 09:54:44,827 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 09:54:45,596 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 09:54:45,596 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 09:54:46,453 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 09:54:46,454 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 09:54:46,455 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 09:54:46,455 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 09:54:46,455 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 09:54:46,456 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 09:54:46,456 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 09:54:46,456 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and striving to read the Scriptures if possible .\n",
      "2021-08-03 09:54:46,456 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 09:54:46,457 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 09:54:46,457 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 09:54:46,458 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 09:54:46,458 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 09:54:46,459 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 09:54:46,459 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 09:54:46,460 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 09:54:46,460 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    80000: bleu:  24.43, loss: 112774.7109, ppl:   5.9567, duration: 99.2202s\n",
      "2021-08-03 09:55:45,614 - INFO - joeynmt.training - Epoch  10, Step:    80200, Batch Loss:     1.926679, Tokens per Sec:     7189, Lr: 0.000300\n",
      "2021-08-03 09:56:44,228 - INFO - joeynmt.training - Epoch  10, Step:    80400, Batch Loss:     1.964801, Tokens per Sec:     7259, Lr: 0.000300\n",
      "2021-08-03 09:56:50,349 - INFO - joeynmt.training - Epoch  10: total training loss 5565.11\n",
      "2021-08-03 09:56:50,350 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-03 09:57:43,634 - INFO - joeynmt.training - Epoch  11, Step:    80600, Batch Loss:     2.047417, Tokens per Sec:     7252, Lr: 0.000300\n",
      "2021-08-03 09:58:42,400 - INFO - joeynmt.training - Epoch  11, Step:    80800, Batch Loss:     1.839211, Tokens per Sec:     7263, Lr: 0.000300\n",
      "2021-08-03 09:59:41,932 - INFO - joeynmt.training - Epoch  11, Step:    81000, Batch Loss:     1.902876, Tokens per Sec:     7304, Lr: 0.000300\n",
      "2021-08-03 10:00:41,306 - INFO - joeynmt.training - Epoch  11, Step:    81200, Batch Loss:     1.787791, Tokens per Sec:     7311, Lr: 0.000300\n",
      "2021-08-03 10:01:40,202 - INFO - joeynmt.training - Epoch  11, Step:    81400, Batch Loss:     1.851439, Tokens per Sec:     7228, Lr: 0.000300\n",
      "2021-08-03 10:02:38,666 - INFO - joeynmt.training - Epoch  11, Step:    81600, Batch Loss:     2.047625, Tokens per Sec:     7198, Lr: 0.000300\n",
      "2021-08-03 10:03:37,651 - INFO - joeynmt.training - Epoch  11, Step:    81800, Batch Loss:     1.844200, Tokens per Sec:     7252, Lr: 0.000300\n",
      "2021-08-03 10:04:36,643 - INFO - joeynmt.training - Epoch  11, Step:    82000, Batch Loss:     2.080659, Tokens per Sec:     7230, Lr: 0.000300\n",
      "2021-08-03 10:06:17,655 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:06:17,655 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:06:17,655 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:06:18,433 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:06:18,434 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:06:19,295 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:06:19,296 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 10:06:19,297 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 10:06:19,297 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 10:06:19,297 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:06:19,298 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 10:06:19,298 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 10:06:19,298 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and to read the Scriptures when possible .\n",
      "2021-08-03 10:06:19,299 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:06:19,299 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 10:06:19,299 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 10:06:19,300 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-03 10:06:19,300 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:06:19,301 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 10:06:19,301 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 10:06:19,301 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 10:06:19,301 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    82000: bleu:  24.78, loss: 112510.0234, ppl:   5.9318, duration: 102.6581s\n",
      "2021-08-03 10:07:18,312 - INFO - joeynmt.training - Epoch  11, Step:    82200, Batch Loss:     2.003396, Tokens per Sec:     7200, Lr: 0.000300\n",
      "2021-08-03 10:08:17,727 - INFO - joeynmt.training - Epoch  11, Step:    82400, Batch Loss:     1.853343, Tokens per Sec:     7255, Lr: 0.000300\n",
      "2021-08-03 10:09:17,078 - INFO - joeynmt.training - Epoch  11, Step:    82600, Batch Loss:     1.916318, Tokens per Sec:     7332, Lr: 0.000300\n",
      "2021-08-03 10:10:16,042 - INFO - joeynmt.training - Epoch  11, Step:    82800, Batch Loss:     2.075764, Tokens per Sec:     7286, Lr: 0.000300\n",
      "2021-08-03 10:11:14,600 - INFO - joeynmt.training - Epoch  11, Step:    83000, Batch Loss:     2.226844, Tokens per Sec:     7252, Lr: 0.000300\n",
      "2021-08-03 10:12:13,778 - INFO - joeynmt.training - Epoch  11, Step:    83200, Batch Loss:     2.448031, Tokens per Sec:     7242, Lr: 0.000300\n",
      "2021-08-03 10:12:40,296 - INFO - joeynmt.training - Epoch  11: total training loss 5520.11\n",
      "2021-08-03 10:12:40,297 - INFO - joeynmt.training - Training ended after  11 epochs.\n",
      "2021-08-03 10:12:40,297 - INFO - joeynmt.training - Best validation result (greedy) at step    82000:   5.93 ppl.\n",
      "2021-08-03 10:12:40,328 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
      "2021-08-03 10:12:40,753 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 10:12:40,985 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 10:12:41,052 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe.en)...\n",
      "2021-08-03 10:14:54,191 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:14:54,191 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:14:54,191 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:14:54,935 - INFO - joeynmt.prediction -  dev bleu[13a]:  25.31 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-03 10:14:54,959 - INFO - joeynmt.prediction - Translations saved to: models/lg_lhen_reverse_transformer_continued/00082000.hyps.dev\n",
      "2021-08-03 10:14:54,959 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.en)...\n",
      "2021-08-03 10:15:01,864 - INFO - joeynmt.prediction - test bleu[13a]:  11.55 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-03 10:15:01,869 - INFO - joeynmt.prediction - Translations saved to: models/lg_lhen_reverse_transformer_continued/00082000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_lhen_reload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlJ8i_1H3afn",
    "outputId": "db1b11de-7f86-4f24-fb76-cbdfb2282b9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 56000\tLoss: 117355.33594\tPPL: 6.40453\tbleu: 23.16259\tLR: 0.00030000\t*\n",
      "Steps: 58000\tLoss: 117431.33594\tPPL: 6.41224\tbleu: 23.58560\tLR: 0.00030000\t\n",
      "Steps: 60000\tLoss: 116389.67969\tPPL: 6.30741\tbleu: 23.67653\tLR: 0.00030000\t*\n",
      "Steps: 62000\tLoss: 116053.13281\tPPL: 6.27391\tbleu: 23.96838\tLR: 0.00030000\t*\n",
      "Steps: 64000\tLoss: 115649.62500\tPPL: 6.23398\tbleu: 23.83155\tLR: 0.00030000\t*\n",
      "Steps: 66000\tLoss: 115222.45312\tPPL: 6.19198\tbleu: 24.08203\tLR: 0.00030000\t*\n",
      "Steps: 68000\tLoss: 114941.32812\tPPL: 6.16450\tbleu: 23.94615\tLR: 0.00030000\t*\n",
      "Steps: 70000\tLoss: 114345.63281\tPPL: 6.10667\tbleu: 24.35173\tLR: 0.00030000\t*\n",
      "Steps: 72000\tLoss: 114159.75781\tPPL: 6.08873\tbleu: 24.13757\tLR: 0.00030000\t*\n",
      "Steps: 74000\tLoss: 113609.75000\tPPL: 6.03597\tbleu: 24.10256\tLR: 0.00030000\t*\n",
      "Steps: 76000\tLoss: 113383.98438\tPPL: 6.01445\tbleu: 24.41267\tLR: 0.00030000\t*\n",
      "Steps: 78000\tLoss: 113049.49219\tPPL: 5.98269\tbleu: 24.58095\tLR: 0.00030000\t*\n",
      "Steps: 80000\tLoss: 112774.71094\tPPL: 5.95674\tbleu: 24.43061\tLR: 0.00030000\t*\n",
      "Steps: 82000\tLoss: 112510.02344\tPPL: 5.93184\tbleu: 24.78387\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lg_lhen_reverse_transformer_continued/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_pFGz01l0931",
    "outputId": "6a042738-1295-4155-aa6a-bce71c9c4b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 15:50:21,803 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 15:50:25,406 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 15:50:25,709 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/lg_lhen_reverse_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/translation.bpe.lh_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuR4Oadk2cJ6",
    "outputId": "8d3d25b8-897b-4f3c-aaf4-cb3dab746363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 11.6 43.0/16.2/7.6/3.9 (BP = 0.961 ratio = 0.962 hyp_len = 1936 ref_len = 2013)\n"
     ]
    }
   ],
   "source": [
    "# Luhya BLEU Score\n",
    "!cat \"translation.bpe.lh_en\" | sacrebleu \"test1.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0NaoiGM2yQL",
    "outputId": "b68a2722-821e-4440-fe40-a8a2658455e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 15:50:52,265 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 15:50:55,174 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 15:50:55,437 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/lg_lhen_reverse_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.lg\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/translation.bpe.lg_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQ60czHj3NUK",
    "outputId": "c8a8fc67-d287-437a-b354-1c65f36cca59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
      "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 26.1 58.1/33.7/22.7/16.4 (BP = 0.892 ratio = 0.898 hyp_len = 38371 ref_len = 42737)\n"
     ]
    }
   ],
   "source": [
    "# Luganda BLEU score\n",
    "!cat \"translation.bpe.lg_en\" | sacrebleu \"test2.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNBKJDdVEuEY"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 82000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"{path}/models/lg_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"{path}/joeynmt/models/lg_lhen_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lg_lhen_reverse_transformer\"', f'model_dir: \"models/lg_lhen_reverse_transformer_continued2\"')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "GeNTAuUpFUAn",
    "outputId": "1a84528b-d9b6-40b5-c4e5-e74f32d9ccdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lg_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lg_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued/82000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lg_lhen_reverse_transformer_continued2\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_lg_lhen_reload2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wal6iATCFX9V",
    "outputId": "b800782b-d599-4032-f631-6f32d2e55919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 10:21:27,793 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 10:21:27,825 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 10:21:32,628 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 10:21:32,930 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 10:21:32,990 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 10:21:32,994 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 10:21:32,994 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 10:21:33,238 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 10:21:33.421851: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 10:21:34,858 - INFO - joeynmt.training - Total params: 12151808\n",
      "2021-08-03 10:21:36,928 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued/82000.ckpt\n",
      "2021-08-03 10:21:37,481 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
      "2021-08-03 10:21:37,482 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
      "2021-08-03 10:21:37,482 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 10:21:37,482 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
      "2021-08-03 10:21:37,483 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
      "2021-08-03 10:21:37,483 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
      "2021-08-03 10:21:37,483 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 10:21:37,483 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 10:21:37,484 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 10:21:37,484 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-03 10:21:37,484 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-03 10:21:37,484 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 10:21:37,485 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 10:21:37,485 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued/82000.ckpt\n",
      "2021-08-03 10:21:37,485 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 10:21:37,485 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 10:21:37,485 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 10:21:37,486 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 10:21:37,486 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 10:21:37,486 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 10:21:37,486 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 10:21:37,487 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 10:21:37,487 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 10:21:37,487 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 10:21:37,489 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 10:21:37,489 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 10:21:37,489 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 10:21:37,490 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 10:21:37,490 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 10:21:37,490 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 10:21:37,490 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-03 10:21:37,491 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 10:21:37,491 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 10:21:37,491 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 10:21:37,491 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-08-03 10:21:37,492 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-03 10:21:37,492 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-03 10:21:37,492 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 10:21:37,492 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer_continued2\n",
      "2021-08-03 10:21:37,493 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 10:21:37,493 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 10:21:37,493 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 10:21:37,493 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 10:21:37,494 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 10:21:37,494 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 10:21:37,494 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 10:21:37,495 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 10:21:37,495 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 10:21:37,495 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 10:21:37,495 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 10:21:37,496 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 10:21:37,496 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 10:21:37,496 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 10:21:37,497 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 10:21:37,497 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 10:21:37,497 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 10:21:37,497 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 10:21:37,498 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 10:21:37,498 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 10:21:37,498 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 10:21:37,498 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 10:21:37,499 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 10:21:37,499 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 10:21:37,499 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 10:21:37,499 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 10:21:37,500 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 10:21:37,500 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 10:21:37,500 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 10:21:37,500 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 10:21:37,501 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 10:21:37,501 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 229609,\n",
      "\tvalid 2349,\n",
      "\ttest 79\n",
      "2021-08-03 10:21:37,501 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-03 10:21:37,502 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 10:21:37,502 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 10:21:37,502 - INFO - joeynmt.helpers - Number of Src words (types): 4264\n",
      "2021-08-03 10:21:37,502 - INFO - joeynmt.helpers - Number of Trg words (types): 4264\n",
      "2021-08-03 10:21:37,503 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4264),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4264))\n",
      "2021-08-03 10:21:37,518 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 10:21:37,518 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 10:22:37,653 - INFO - joeynmt.training - Epoch   1, Step:    82200, Batch Loss:     1.982909, Tokens per Sec:     7066, Lr: 0.000300\n",
      "2021-08-03 10:23:37,428 - INFO - joeynmt.training - Epoch   1, Step:    82400, Batch Loss:     1.850042, Tokens per Sec:     7212, Lr: 0.000300\n",
      "2021-08-03 10:24:36,901 - INFO - joeynmt.training - Epoch   1, Step:    82600, Batch Loss:     1.906820, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-08-03 10:25:36,132 - INFO - joeynmt.training - Epoch   1, Step:    82800, Batch Loss:     2.055363, Tokens per Sec:     7253, Lr: 0.000300\n",
      "2021-08-03 10:26:34,917 - INFO - joeynmt.training - Epoch   1, Step:    83000, Batch Loss:     2.247623, Tokens per Sec:     7224, Lr: 0.000300\n",
      "2021-08-03 10:27:34,429 - INFO - joeynmt.training - Epoch   1, Step:    83200, Batch Loss:     2.427109, Tokens per Sec:     7201, Lr: 0.000300\n",
      "2021-08-03 10:28:01,097 - INFO - joeynmt.training - Epoch   1: total training loss 2489.07\n",
      "2021-08-03 10:28:01,097 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 10:28:34,608 - INFO - joeynmt.training - Epoch   2, Step:    83400, Batch Loss:     2.356713, Tokens per Sec:     7052, Lr: 0.000300\n",
      "2021-08-03 10:29:34,067 - INFO - joeynmt.training - Epoch   2, Step:    83600, Batch Loss:     1.624657, Tokens per Sec:     7267, Lr: 0.000300\n",
      "2021-08-03 10:30:32,971 - INFO - joeynmt.training - Epoch   2, Step:    83800, Batch Loss:     1.852390, Tokens per Sec:     7181, Lr: 0.000300\n",
      "2021-08-03 10:31:32,414 - INFO - joeynmt.training - Epoch   2, Step:    84000, Batch Loss:     1.829307, Tokens per Sec:     7196, Lr: 0.000300\n",
      "2021-08-03 10:33:13,582 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:33:13,583 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:33:13,583 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:33:14,333 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:33:14,333 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:33:15,159 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:33:15,160 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 10:33:15,160 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 10:33:15,161 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 10:33:15,161 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:33:15,162 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 10:33:15,162 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 10:33:15,162 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and striving to read the Scriptures if possible .\n",
      "2021-08-03 10:33:15,162 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:33:15,163 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 10:33:15,163 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 10:33:15,164 - INFO - joeynmt.training - \tHypothesis: Why are false gods not unvaluable ?\n",
      "2021-08-03 10:33:15,164 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:33:15,165 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 10:33:15,165 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 10:33:15,165 - INFO - joeynmt.training - \tHypothesis: Granted , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 10:33:15,165 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    84000: bleu:  24.81, loss: 112382.1562, ppl:   5.9199, duration: 102.7511s\n",
      "2021-08-03 10:34:14,545 - INFO - joeynmt.training - Epoch   2, Step:    84200, Batch Loss:     1.579303, Tokens per Sec:     7176, Lr: 0.000300\n",
      "2021-08-03 10:35:14,244 - INFO - joeynmt.training - Epoch   2, Step:    84400, Batch Loss:     1.933869, Tokens per Sec:     7318, Lr: 0.000300\n",
      "2021-08-03 10:36:13,397 - INFO - joeynmt.training - Epoch   2, Step:    84600, Batch Loss:     2.244357, Tokens per Sec:     7155, Lr: 0.000300\n",
      "2021-08-03 10:37:12,668 - INFO - joeynmt.training - Epoch   2, Step:    84800, Batch Loss:     1.630574, Tokens per Sec:     7248, Lr: 0.000300\n",
      "2021-08-03 10:38:11,855 - INFO - joeynmt.training - Epoch   2, Step:    85000, Batch Loss:     1.778556, Tokens per Sec:     7260, Lr: 0.000300\n",
      "2021-08-03 10:39:10,805 - INFO - joeynmt.training - Epoch   2, Step:    85200, Batch Loss:     1.887770, Tokens per Sec:     7239, Lr: 0.000300\n",
      "2021-08-03 10:40:10,337 - INFO - joeynmt.training - Epoch   2, Step:    85400, Batch Loss:     1.832634, Tokens per Sec:     7213, Lr: 0.000300\n",
      "2021-08-03 10:41:09,385 - INFO - joeynmt.training - Epoch   2, Step:    85600, Batch Loss:     1.875345, Tokens per Sec:     7226, Lr: 0.000300\n",
      "2021-08-03 10:42:09,003 - INFO - joeynmt.training - Epoch   2, Step:    85800, Batch Loss:     2.127584, Tokens per Sec:     7304, Lr: 0.000300\n",
      "2021-08-03 10:43:08,129 - INFO - joeynmt.training - Epoch   2, Step:    86000, Batch Loss:     1.939299, Tokens per Sec:     7190, Lr: 0.000300\n",
      "2021-08-03 10:44:48,638 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:44:48,638 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:44:48,638 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:44:49,408 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:44:49,408 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:44:50,734 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:44:50,735 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 10:44:50,735 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 10:44:50,736 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he speaks to us .\n",
      "2021-08-03 10:44:50,736 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:44:50,736 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 10:44:50,737 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 10:44:50,737 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in the ministry and to study the Scriptures when possible .\n",
      "2021-08-03 10:44:50,737 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:44:50,738 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 10:44:50,738 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 10:44:50,738 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 10:44:50,739 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:44:50,739 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 10:44:50,740 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 10:44:50,740 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 10:44:50,740 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    86000: bleu:  24.69, loss: 112053.1172, ppl:   5.8891, duration: 102.6108s\n",
      "2021-08-03 10:45:38,800 - INFO - joeynmt.training - Epoch   2: total training loss 5512.91\n",
      "2021-08-03 10:45:38,801 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 10:45:50,503 - INFO - joeynmt.training - Epoch   3, Step:    86200, Batch Loss:     1.989773, Tokens per Sec:     7065, Lr: 0.000300\n",
      "2021-08-03 10:46:50,326 - INFO - joeynmt.training - Epoch   3, Step:    86400, Batch Loss:     1.930863, Tokens per Sec:     7299, Lr: 0.000300\n",
      "2021-08-03 10:47:49,356 - INFO - joeynmt.training - Epoch   3, Step:    86600, Batch Loss:     2.106696, Tokens per Sec:     7227, Lr: 0.000300\n",
      "2021-08-03 10:48:48,845 - INFO - joeynmt.training - Epoch   3, Step:    86800, Batch Loss:     2.176171, Tokens per Sec:     7211, Lr: 0.000300\n",
      "2021-08-03 10:49:47,990 - INFO - joeynmt.training - Epoch   3, Step:    87000, Batch Loss:     2.521882, Tokens per Sec:     7297, Lr: 0.000300\n",
      "2021-08-03 10:50:47,075 - INFO - joeynmt.training - Epoch   3, Step:    87200, Batch Loss:     1.427858, Tokens per Sec:     7244, Lr: 0.000300\n",
      "2021-08-03 10:51:45,883 - INFO - joeynmt.training - Epoch   3, Step:    87400, Batch Loss:     1.515131, Tokens per Sec:     7238, Lr: 0.000300\n",
      "2021-08-03 10:52:45,083 - INFO - joeynmt.training - Epoch   3, Step:    87600, Batch Loss:     1.839628, Tokens per Sec:     7348, Lr: 0.000300\n",
      "2021-08-03 10:53:44,008 - INFO - joeynmt.training - Epoch   3, Step:    87800, Batch Loss:     1.876494, Tokens per Sec:     7157, Lr: 0.000300\n",
      "2021-08-03 10:54:42,854 - INFO - joeynmt.training - Epoch   3, Step:    88000, Batch Loss:     1.724699, Tokens per Sec:     7268, Lr: 0.000300\n",
      "2021-08-03 10:56:20,855 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 10:56:20,855 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 10:56:20,856 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 10:56:21,612 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 10:56:21,612 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 10:56:22,891 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 10:56:22,892 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 10:56:22,893 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 10:56:22,893 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 10:56:22,893 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 10:56:22,894 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 10:56:22,894 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 10:56:22,895 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and strive to read the Scriptures if possible .\n",
      "2021-08-03 10:56:22,895 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 10:56:22,895 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 10:56:22,896 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 10:56:22,896 - INFO - joeynmt.training - \tHypothesis: Why are false gods unempty ?\n",
      "2021-08-03 10:56:22,896 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 10:56:22,897 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 10:56:22,897 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 10:56:22,897 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 10:56:22,897 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    88000: bleu:  24.90, loss: 112041.8750, ppl:   5.8881, duration: 100.0424s\n",
      "2021-08-03 10:57:21,328 - INFO - joeynmt.training - Epoch   3, Step:    88200, Batch Loss:     1.992943, Tokens per Sec:     7125, Lr: 0.000300\n",
      "2021-08-03 10:58:20,634 - INFO - joeynmt.training - Epoch   3, Step:    88400, Batch Loss:     1.887901, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-08-03 10:59:19,897 - INFO - joeynmt.training - Epoch   3, Step:    88600, Batch Loss:     1.934606, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-08-03 11:00:18,805 - INFO - joeynmt.training - Epoch   3, Step:    88800, Batch Loss:     1.388636, Tokens per Sec:     7263, Lr: 0.000300\n",
      "2021-08-03 11:01:18,296 - INFO - joeynmt.training - Epoch   3, Step:    89000, Batch Loss:     2.044784, Tokens per Sec:     7304, Lr: 0.000300\n",
      "2021-08-03 11:01:27,168 - INFO - joeynmt.training - Epoch   3: total training loss 5484.19\n",
      "2021-08-03 11:01:27,168 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 11:02:17,666 - INFO - joeynmt.training - Epoch   4, Step:    89200, Batch Loss:     1.934940, Tokens per Sec:     7222, Lr: 0.000300\n",
      "2021-08-03 11:03:16,617 - INFO - joeynmt.training - Epoch   4, Step:    89400, Batch Loss:     1.851906, Tokens per Sec:     7206, Lr: 0.000300\n",
      "2021-08-03 11:04:15,554 - INFO - joeynmt.training - Epoch   4, Step:    89600, Batch Loss:     1.893932, Tokens per Sec:     7197, Lr: 0.000300\n",
      "2021-08-03 11:05:14,487 - INFO - joeynmt.training - Epoch   4, Step:    89800, Batch Loss:     1.790471, Tokens per Sec:     7244, Lr: 0.000300\n",
      "2021-08-03 11:06:13,585 - INFO - joeynmt.training - Epoch   4, Step:    90000, Batch Loss:     1.786650, Tokens per Sec:     7226, Lr: 0.000300\n",
      "2021-08-03 11:07:54,250 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:07:54,250 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:07:54,251 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:07:55,024 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 11:07:55,025 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 11:07:55,992 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:07:55,993 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 11:07:55,993 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 11:07:55,993 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 11:07:55,994 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:07:55,994 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 11:07:55,995 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 11:07:55,995 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and striving to read the Scriptures if possible .\n",
      "2021-08-03 11:07:55,995 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:07:55,996 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 11:07:55,996 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 11:07:55,996 - INFO - joeynmt.training - \tHypothesis: Why are false gods not empty ?\n",
      "2021-08-03 11:07:55,997 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:07:55,997 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 11:07:55,998 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 11:07:55,998 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 11:07:55,998 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    90000: bleu:  25.22, loss: 111316.5312, ppl:   5.8209, duration: 102.4128s\n",
      "2021-08-03 11:08:55,844 - INFO - joeynmt.training - Epoch   4, Step:    90200, Batch Loss:     2.297729, Tokens per Sec:     7216, Lr: 0.000300\n",
      "2021-08-03 11:09:55,144 - INFO - joeynmt.training - Epoch   4, Step:    90400, Batch Loss:     1.874710, Tokens per Sec:     7243, Lr: 0.000300\n",
      "2021-08-03 11:10:54,829 - INFO - joeynmt.training - Epoch   4, Step:    90600, Batch Loss:     1.945433, Tokens per Sec:     7298, Lr: 0.000300\n",
      "2021-08-03 11:11:54,250 - INFO - joeynmt.training - Epoch   4, Step:    90800, Batch Loss:     1.895405, Tokens per Sec:     7253, Lr: 0.000300\n",
      "2021-08-03 11:12:53,270 - INFO - joeynmt.training - Epoch   4, Step:    91000, Batch Loss:     2.042323, Tokens per Sec:     7182, Lr: 0.000300\n",
      "2021-08-03 11:13:52,701 - INFO - joeynmt.training - Epoch   4, Step:    91200, Batch Loss:     1.880868, Tokens per Sec:     7260, Lr: 0.000300\n",
      "2021-08-03 11:14:51,982 - INFO - joeynmt.training - Epoch   4, Step:    91400, Batch Loss:     1.956406, Tokens per Sec:     7186, Lr: 0.000300\n",
      "2021-08-03 11:15:50,161 - INFO - joeynmt.training - Epoch   4, Step:    91600, Batch Loss:     1.998935, Tokens per Sec:     7137, Lr: 0.000300\n",
      "2021-08-03 11:16:49,447 - INFO - joeynmt.training - Epoch   4, Step:    91800, Batch Loss:     2.031923, Tokens per Sec:     7239, Lr: 0.000300\n",
      "2021-08-03 11:17:21,181 - INFO - joeynmt.training - Epoch   4: total training loss 5474.46\n",
      "2021-08-03 11:17:21,181 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-03 11:17:49,243 - INFO - joeynmt.training - Epoch   5, Step:    92000, Batch Loss:     1.881847, Tokens per Sec:     7065, Lr: 0.000300\n",
      "2021-08-03 11:19:27,904 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:19:27,904 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:19:27,905 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:19:28,692 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 11:19:28,692 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 11:19:29,561 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:19:29,562 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 11:19:29,563 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 11:19:29,563 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 11:19:29,563 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:19:29,564 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 11:19:29,564 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 11:19:29,564 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and striving to read the Scriptures if possible .\n",
      "2021-08-03 11:19:29,564 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:19:29,565 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 11:19:29,565 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 11:19:29,565 - INFO - joeynmt.training - \tHypothesis: Why are false gods not empty ?\n",
      "2021-08-03 11:19:29,566 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:19:29,566 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 11:19:29,567 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 11:19:29,567 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 11:19:29,567 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    92000: bleu:  25.13, loss: 111122.1641, ppl:   5.8030, duration: 100.3231s\n",
      "2021-08-03 11:20:29,008 - INFO - joeynmt.training - Epoch   5, Step:    92200, Batch Loss:     1.715752, Tokens per Sec:     7137, Lr: 0.000300\n",
      "2021-08-03 11:21:27,937 - INFO - joeynmt.training - Epoch   5, Step:    92400, Batch Loss:     1.969031, Tokens per Sec:     7198, Lr: 0.000300\n",
      "2021-08-03 11:22:27,309 - INFO - joeynmt.training - Epoch   5, Step:    92600, Batch Loss:     2.126497, Tokens per Sec:     7241, Lr: 0.000300\n",
      "2021-08-03 11:23:26,710 - INFO - joeynmt.training - Epoch   5, Step:    92800, Batch Loss:     1.999250, Tokens per Sec:     7203, Lr: 0.000300\n",
      "2021-08-03 11:24:26,125 - INFO - joeynmt.training - Epoch   5, Step:    93000, Batch Loss:     1.135274, Tokens per Sec:     7232, Lr: 0.000300\n",
      "2021-08-03 11:25:25,379 - INFO - joeynmt.training - Epoch   5, Step:    93200, Batch Loss:     1.896207, Tokens per Sec:     7239, Lr: 0.000300\n",
      "2021-08-03 11:26:25,277 - INFO - joeynmt.training - Epoch   5, Step:    93400, Batch Loss:     2.095906, Tokens per Sec:     7359, Lr: 0.000300\n",
      "2021-08-03 11:27:24,735 - INFO - joeynmt.training - Epoch   5, Step:    93600, Batch Loss:     1.926253, Tokens per Sec:     7264, Lr: 0.000300\n",
      "2021-08-03 11:28:24,082 - INFO - joeynmt.training - Epoch   5, Step:    93800, Batch Loss:     1.752009, Tokens per Sec:     7271, Lr: 0.000300\n",
      "2021-08-03 11:29:23,113 - INFO - joeynmt.training - Epoch   5, Step:    94000, Batch Loss:     1.795189, Tokens per Sec:     7200, Lr: 0.000300\n",
      "2021-08-03 11:31:03,335 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:31:03,336 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:31:03,336 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:31:04,128 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 11:31:04,129 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 11:31:05,009 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:31:05,010 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 11:31:05,010 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 11:31:05,010 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 11:31:05,011 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:31:05,011 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 11:31:05,012 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 11:31:05,012 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our efforts to study the Scriptures if possible .\n",
      "2021-08-03 11:31:05,012 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:31:05,013 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 11:31:05,013 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 11:31:05,013 - INFO - joeynmt.training - \tHypothesis: Why are false gods not valueless ?\n",
      "2021-08-03 11:31:05,014 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:31:05,014 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 11:31:05,015 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 11:31:05,015 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 11:31:05,015 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    94000: bleu:  25.21, loss: 110817.2500, ppl:   5.7751, duration: 101.9020s\n",
      "2021-08-03 11:32:04,390 - INFO - joeynmt.training - Epoch   5, Step:    94200, Batch Loss:     1.950770, Tokens per Sec:     7195, Lr: 0.000300\n",
      "2021-08-03 11:33:03,430 - INFO - joeynmt.training - Epoch   5, Step:    94400, Batch Loss:     1.887112, Tokens per Sec:     7130, Lr: 0.000300\n",
      "2021-08-03 11:34:02,995 - INFO - joeynmt.training - Epoch   5, Step:    94600, Batch Loss:     1.979333, Tokens per Sec:     7209, Lr: 0.000300\n",
      "2021-08-03 11:34:56,089 - INFO - joeynmt.training - Epoch   5: total training loss 5446.54\n",
      "2021-08-03 11:34:56,090 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-03 11:35:02,936 - INFO - joeynmt.training - Epoch   6, Step:    94800, Batch Loss:     1.740839, Tokens per Sec:     6807, Lr: 0.000300\n",
      "2021-08-03 11:36:01,746 - INFO - joeynmt.training - Epoch   6, Step:    95000, Batch Loss:     1.724570, Tokens per Sec:     7188, Lr: 0.000300\n",
      "2021-08-03 11:37:01,301 - INFO - joeynmt.training - Epoch   6, Step:    95200, Batch Loss:     2.172776, Tokens per Sec:     7246, Lr: 0.000300\n",
      "2021-08-03 11:38:00,936 - INFO - joeynmt.training - Epoch   6, Step:    95400, Batch Loss:     1.895545, Tokens per Sec:     7208, Lr: 0.000300\n",
      "2021-08-03 11:39:00,329 - INFO - joeynmt.training - Epoch   6, Step:    95600, Batch Loss:     1.807290, Tokens per Sec:     7182, Lr: 0.000300\n",
      "2021-08-03 11:39:59,674 - INFO - joeynmt.training - Epoch   6, Step:    95800, Batch Loss:     1.164297, Tokens per Sec:     7211, Lr: 0.000300\n",
      "2021-08-03 11:40:58,646 - INFO - joeynmt.training - Epoch   6, Step:    96000, Batch Loss:     1.817489, Tokens per Sec:     7290, Lr: 0.000300\n",
      "2021-08-03 11:42:38,796 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:42:38,796 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:42:38,796 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:42:40,425 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:42:40,426 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 11:42:40,427 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 11:42:40,427 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking with us .\n",
      "2021-08-03 11:42:40,427 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:42:40,428 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 11:42:40,428 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 11:42:40,428 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and strive to read the Scriptures if possible .\n",
      "2021-08-03 11:42:40,428 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:42:40,429 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 11:42:40,429 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 11:42:40,430 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-03 11:42:40,430 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:42:40,430 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 11:42:40,431 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 11:42:40,431 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 11:42:40,431 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    96000: bleu:  25.35, loss: 111121.7344, ppl:   5.8030, duration: 101.7848s\n",
      "2021-08-03 11:43:40,090 - INFO - joeynmt.training - Epoch   6, Step:    96200, Batch Loss:     1.847728, Tokens per Sec:     7231, Lr: 0.000300\n",
      "2021-08-03 11:44:39,426 - INFO - joeynmt.training - Epoch   6, Step:    96400, Batch Loss:     1.804183, Tokens per Sec:     7189, Lr: 0.000300\n",
      "2021-08-03 11:45:38,865 - INFO - joeynmt.training - Epoch   6, Step:    96600, Batch Loss:     1.901608, Tokens per Sec:     7200, Lr: 0.000300\n",
      "2021-08-03 11:46:37,822 - INFO - joeynmt.training - Epoch   6, Step:    96800, Batch Loss:     1.974769, Tokens per Sec:     7187, Lr: 0.000300\n",
      "2021-08-03 11:47:37,104 - INFO - joeynmt.training - Epoch   6, Step:    97000, Batch Loss:     1.835107, Tokens per Sec:     7246, Lr: 0.000300\n",
      "2021-08-03 11:48:36,563 - INFO - joeynmt.training - Epoch   6, Step:    97200, Batch Loss:     2.270070, Tokens per Sec:     7289, Lr: 0.000300\n",
      "2021-08-03 11:49:35,707 - INFO - joeynmt.training - Epoch   6, Step:    97400, Batch Loss:     1.706182, Tokens per Sec:     7219, Lr: 0.000300\n",
      "2021-08-03 11:50:35,395 - INFO - joeynmt.training - Epoch   6, Step:    97600, Batch Loss:     1.846217, Tokens per Sec:     7296, Lr: 0.000300\n",
      "2021-08-03 11:50:49,617 - INFO - joeynmt.training - Epoch   6: total training loss 5425.18\n",
      "2021-08-03 11:50:49,617 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-03 11:51:34,990 - INFO - joeynmt.training - Epoch   7, Step:    97800, Batch Loss:     1.997272, Tokens per Sec:     7102, Lr: 0.000300\n",
      "2021-08-03 11:52:34,133 - INFO - joeynmt.training - Epoch   7, Step:    98000, Batch Loss:     2.045078, Tokens per Sec:     7212, Lr: 0.000300\n",
      "2021-08-03 11:54:15,013 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 11:54:15,014 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 11:54:15,014 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 11:54:15,793 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 11:54:15,794 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 11:54:16,769 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 11:54:16,770 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 11:54:16,770 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 11:54:16,770 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 11:54:16,771 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 11:54:16,771 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 11:54:16,772 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 11:54:16,772 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures even when possible .\n",
      "2021-08-03 11:54:16,772 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 11:54:16,773 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 11:54:16,774 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 11:54:16,774 - INFO - joeynmt.training - \tHypothesis: Why do false gods have unrealistic gods ?\n",
      "2021-08-03 11:54:16,775 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 11:54:16,775 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 11:54:16,776 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 11:54:16,776 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 11:54:16,776 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    98000: bleu:  25.39, loss: 110113.3906, ppl:   5.7111, duration: 102.6429s\n",
      "2021-08-03 11:55:16,298 - INFO - joeynmt.training - Epoch   7, Step:    98200, Batch Loss:     1.899132, Tokens per Sec:     7181, Lr: 0.000300\n",
      "2021-08-03 11:56:14,908 - INFO - joeynmt.training - Epoch   7, Step:    98400, Batch Loss:     2.018866, Tokens per Sec:     7262, Lr: 0.000300\n",
      "2021-08-03 11:57:13,770 - INFO - joeynmt.training - Epoch   7, Step:    98600, Batch Loss:     1.740402, Tokens per Sec:     7197, Lr: 0.000300\n",
      "2021-08-03 11:58:13,278 - INFO - joeynmt.training - Epoch   7, Step:    98800, Batch Loss:     2.017418, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-08-03 11:59:12,854 - INFO - joeynmt.training - Epoch   7, Step:    99000, Batch Loss:     1.911111, Tokens per Sec:     7307, Lr: 0.000300\n",
      "2021-08-03 12:00:12,295 - INFO - joeynmt.training - Epoch   7, Step:    99200, Batch Loss:     1.447467, Tokens per Sec:     7271, Lr: 0.000300\n",
      "2021-08-03 12:01:11,338 - INFO - joeynmt.training - Epoch   7, Step:    99400, Batch Loss:     2.039542, Tokens per Sec:     7183, Lr: 0.000300\n",
      "2021-08-03 12:02:10,358 - INFO - joeynmt.training - Epoch   7, Step:    99600, Batch Loss:     2.012197, Tokens per Sec:     7199, Lr: 0.000300\n",
      "2021-08-03 12:03:09,556 - INFO - joeynmt.training - Epoch   7, Step:    99800, Batch Loss:     1.899702, Tokens per Sec:     7203, Lr: 0.000300\n",
      "2021-08-03 12:04:09,379 - INFO - joeynmt.training - Epoch   7, Step:   100000, Batch Loss:     1.968915, Tokens per Sec:     7361, Lr: 0.000300\n",
      "2021-08-03 12:05:48,374 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 12:05:48,375 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 12:05:48,375 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 12:05:49,142 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 12:05:49,143 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 12:05:49,984 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 12:05:49,985 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 12:05:49,986 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 12:05:49,986 - INFO - joeynmt.training - \tHypothesis: When we consider how he manifested faith and imitate him , he is speaking to us .\n",
      "2021-08-03 12:05:49,986 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 12:05:49,987 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 12:05:49,987 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 12:05:49,987 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible by preaching and studying the Scriptures if possible .\n",
      "2021-08-03 12:05:49,988 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 12:05:49,989 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 12:05:49,989 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 12:05:49,989 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-03 12:05:49,989 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 12:05:49,990 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 12:05:49,990 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 12:05:49,990 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 12:05:49,991 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   100000: bleu:  25.49, loss: 110109.1641, ppl:   5.7107, duration: 100.6114s\n",
      "2021-08-03 12:06:49,438 - INFO - joeynmt.training - Epoch   7, Step:   100200, Batch Loss:     1.522249, Tokens per Sec:     7171, Lr: 0.000300\n",
      "2021-08-03 12:07:48,395 - INFO - joeynmt.training - Epoch   7, Step:   100400, Batch Loss:     2.069244, Tokens per Sec:     7184, Lr: 0.000300\n",
      "2021-08-03 12:08:23,912 - INFO - joeynmt.training - Epoch   7: total training loss 5409.28\n",
      "2021-08-03 12:08:23,912 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-03 12:08:47,999 - INFO - joeynmt.training - Epoch   8, Step:   100600, Batch Loss:     1.624563, Tokens per Sec:     7135, Lr: 0.000300\n",
      "2021-08-03 12:09:46,691 - INFO - joeynmt.training - Epoch   8, Step:   100800, Batch Loss:     1.855188, Tokens per Sec:     7193, Lr: 0.000300\n",
      "2021-08-03 12:10:45,686 - INFO - joeynmt.training - Epoch   8, Step:   101000, Batch Loss:     1.885947, Tokens per Sec:     7249, Lr: 0.000300\n",
      "2021-08-03 12:11:44,502 - INFO - joeynmt.training - Epoch   8, Step:   101200, Batch Loss:     1.835236, Tokens per Sec:     7243, Lr: 0.000300\n",
      "2021-08-03 12:12:44,084 - INFO - joeynmt.training - Epoch   8, Step:   101400, Batch Loss:     1.732893, Tokens per Sec:     7291, Lr: 0.000300\n",
      "2021-08-03 12:13:43,079 - INFO - joeynmt.training - Epoch   8, Step:   101600, Batch Loss:     2.476689, Tokens per Sec:     7217, Lr: 0.000300\n",
      "2021-08-03 12:14:42,286 - INFO - joeynmt.training - Epoch   8, Step:   101800, Batch Loss:     1.997998, Tokens per Sec:     7224, Lr: 0.000300\n",
      "2021-08-03 12:15:41,600 - INFO - joeynmt.training - Epoch   8, Step:   102000, Batch Loss:     1.544546, Tokens per Sec:     7217, Lr: 0.000300\n",
      "2021-08-03 12:17:22,675 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 12:17:22,676 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 12:17:22,676 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 12:17:23,425 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 12:17:23,425 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 12:17:24,680 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 12:17:24,681 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 12:17:24,681 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 12:17:24,682 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 12:17:24,682 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 12:17:24,682 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 12:17:24,683 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 12:17:24,683 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 12:17:24,683 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 12:17:24,685 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 12:17:24,685 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 12:17:24,685 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 12:17:24,686 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 12:17:24,686 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 12:17:24,686 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 12:17:24,687 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 12:17:24,687 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   102000: bleu:  25.39, loss: 109891.7109, ppl:   5.6911, duration: 103.0862s\n",
      "2021-08-03 12:18:24,448 - INFO - joeynmt.training - Epoch   8, Step:   102200, Batch Loss:     1.762470, Tokens per Sec:     7272, Lr: 0.000300\n",
      "2021-08-03 12:19:23,766 - INFO - joeynmt.training - Epoch   8, Step:   102400, Batch Loss:     1.830936, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-08-03 12:20:22,712 - INFO - joeynmt.training - Epoch   8, Step:   102600, Batch Loss:     1.854359, Tokens per Sec:     7117, Lr: 0.000300\n",
      "2021-08-03 12:21:21,599 - INFO - joeynmt.training - Epoch   8, Step:   102800, Batch Loss:     1.955661, Tokens per Sec:     7181, Lr: 0.000300\n",
      "2021-08-03 12:22:20,898 - INFO - joeynmt.training - Epoch   8, Step:   103000, Batch Loss:     1.535352, Tokens per Sec:     7323, Lr: 0.000300\n",
      "2021-08-03 12:23:20,047 - INFO - joeynmt.training - Epoch   8, Step:   103200, Batch Loss:     1.352361, Tokens per Sec:     7194, Lr: 0.000300\n",
      "2021-08-03 12:24:17,198 - INFO - joeynmt.training - Epoch   8: total training loss 5399.99\n",
      "2021-08-03 12:24:17,199 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-03 12:24:19,419 - INFO - joeynmt.training - Epoch   9, Step:   103400, Batch Loss:     1.793426, Tokens per Sec:     6449, Lr: 0.000300\n",
      "2021-08-03 12:25:18,136 - INFO - joeynmt.training - Epoch   9, Step:   103600, Batch Loss:     1.843071, Tokens per Sec:     7202, Lr: 0.000300\n",
      "2021-08-03 12:26:17,203 - INFO - joeynmt.training - Epoch   9, Step:   103800, Batch Loss:     1.953061, Tokens per Sec:     7234, Lr: 0.000300\n",
      "2021-08-03 12:27:16,391 - INFO - joeynmt.training - Epoch   9, Step:   104000, Batch Loss:     1.930933, Tokens per Sec:     7219, Lr: 0.000300\n",
      "2021-08-03 12:28:57,726 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 12:28:57,727 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 12:28:57,727 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 12:28:59,304 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 12:28:59,305 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 12:28:59,305 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 12:28:59,306 - INFO - joeynmt.training - \tHypothesis: When we examine how he manifested faith and imitate him , he speaks to us .\n",
      "2021-08-03 12:28:59,306 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 12:28:59,307 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 12:28:59,307 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 12:28:59,307 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 12:28:59,307 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 12:28:59,308 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 12:28:59,308 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 12:28:59,308 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 12:28:59,309 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 12:28:59,309 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 12:28:59,310 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 12:28:59,310 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 12:28:59,310 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   104000: bleu:  25.67, loss: 109975.5156, ppl:   5.6986, duration: 102.9186s\n",
      "2021-08-03 12:29:58,585 - INFO - joeynmt.training - Epoch   9, Step:   104200, Batch Loss:     1.998459, Tokens per Sec:     7247, Lr: 0.000300\n",
      "2021-08-03 12:30:58,151 - INFO - joeynmt.training - Epoch   9, Step:   104400, Batch Loss:     1.725303, Tokens per Sec:     7229, Lr: 0.000300\n",
      "2021-08-03 12:31:57,358 - INFO - joeynmt.training - Epoch   9, Step:   104600, Batch Loss:     1.282885, Tokens per Sec:     7193, Lr: 0.000300\n",
      "2021-08-03 12:32:57,042 - INFO - joeynmt.training - Epoch   9, Step:   104800, Batch Loss:     1.854651, Tokens per Sec:     7321, Lr: 0.000300\n",
      "2021-08-03 12:33:56,356 - INFO - joeynmt.training - Epoch   9, Step:   105000, Batch Loss:     1.899297, Tokens per Sec:     7239, Lr: 0.000300\n",
      "2021-08-03 12:34:55,210 - INFO - joeynmt.training - Epoch   9, Step:   105200, Batch Loss:     1.857529, Tokens per Sec:     7191, Lr: 0.000300\n",
      "2021-08-03 12:35:54,856 - INFO - joeynmt.training - Epoch   9, Step:   105400, Batch Loss:     1.752819, Tokens per Sec:     7249, Lr: 0.000300\n",
      "2021-08-03 12:36:54,171 - INFO - joeynmt.training - Epoch   9, Step:   105600, Batch Loss:     2.005270, Tokens per Sec:     7180, Lr: 0.000300\n",
      "2021-08-03 12:37:53,519 - INFO - joeynmt.training - Epoch   9, Step:   105800, Batch Loss:     1.976420, Tokens per Sec:     7262, Lr: 0.000300\n",
      "2021-08-03 12:38:53,022 - INFO - joeynmt.training - Epoch   9, Step:   106000, Batch Loss:     1.827190, Tokens per Sec:     7162, Lr: 0.000300\n",
      "2021-08-03 12:40:36,150 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 12:40:36,150 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 12:40:36,151 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 12:40:36,951 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 12:40:36,951 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 12:40:37,869 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 12:40:37,871 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 12:40:37,871 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 12:40:37,871 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 12:40:37,871 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 12:40:37,872 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 12:40:37,872 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 12:40:37,873 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 12:40:37,873 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 12:40:37,873 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 12:40:37,874 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 12:40:37,874 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 12:40:37,874 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 12:40:37,875 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 12:40:37,875 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 12:40:37,875 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 12:40:37,876 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   106000: bleu:  25.68, loss: 109685.7109, ppl:   5.6726, duration: 104.8530s\n",
      "2021-08-03 12:41:38,003 - INFO - joeynmt.training - Epoch   9, Step:   106200, Batch Loss:     1.893785, Tokens per Sec:     7198, Lr: 0.000300\n",
      "2021-08-03 12:41:57,715 - INFO - joeynmt.training - Epoch   9: total training loss 5378.25\n",
      "2021-08-03 12:41:57,716 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-03 12:42:37,582 - INFO - joeynmt.training - Epoch  10, Step:   106400, Batch Loss:     1.764149, Tokens per Sec:     7127, Lr: 0.000300\n",
      "2021-08-03 12:43:37,332 - INFO - joeynmt.training - Epoch  10, Step:   106600, Batch Loss:     1.786555, Tokens per Sec:     7141, Lr: 0.000300\n",
      "2021-08-03 12:44:36,833 - INFO - joeynmt.training - Epoch  10, Step:   106800, Batch Loss:     1.754516, Tokens per Sec:     7169, Lr: 0.000300\n",
      "2021-08-03 12:45:36,367 - INFO - joeynmt.training - Epoch  10, Step:   107000, Batch Loss:     1.828571, Tokens per Sec:     7204, Lr: 0.000300\n",
      "2021-08-03 12:46:35,810 - INFO - joeynmt.training - Epoch  10, Step:   107200, Batch Loss:     2.106650, Tokens per Sec:     7143, Lr: 0.000300\n",
      "2021-08-03 12:47:35,529 - INFO - joeynmt.training - Epoch  10, Step:   107400, Batch Loss:     2.213449, Tokens per Sec:     7114, Lr: 0.000300\n",
      "2021-08-03 12:48:35,739 - INFO - joeynmt.training - Epoch  10, Step:   107600, Batch Loss:     2.115571, Tokens per Sec:     7255, Lr: 0.000300\n",
      "2021-08-03 12:49:35,555 - INFO - joeynmt.training - Epoch  10, Step:   107800, Batch Loss:     2.011918, Tokens per Sec:     7232, Lr: 0.000300\n",
      "2021-08-03 12:50:34,986 - INFO - joeynmt.training - Epoch  10, Step:   108000, Batch Loss:     1.883659, Tokens per Sec:     7214, Lr: 0.000300\n",
      "2021-08-03 12:52:18,002 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 12:52:18,003 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 12:52:18,003 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 12:52:18,793 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 12:52:18,793 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 12:52:19,673 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 12:52:19,674 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 12:52:19,675 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 12:52:19,675 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 12:52:19,675 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 12:52:19,676 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 12:52:19,676 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 12:52:19,676 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 12:52:19,677 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 12:52:19,677 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 12:52:19,677 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 12:52:19,678 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 12:52:19,678 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 12:52:19,679 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 12:52:19,679 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 12:52:19,679 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 12:52:19,679 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   108000: bleu:  25.65, loss: 109484.7344, ppl:   5.6546, duration: 104.6925s\n",
      "2021-08-03 12:53:19,344 - INFO - joeynmt.training - Epoch  10, Step:   108200, Batch Loss:     2.024895, Tokens per Sec:     7180, Lr: 0.000300\n",
      "2021-08-03 12:54:18,634 - INFO - joeynmt.training - Epoch  10, Step:   108400, Batch Loss:     1.930173, Tokens per Sec:     7115, Lr: 0.000300\n",
      "2021-08-03 12:55:17,870 - INFO - joeynmt.training - Epoch  10, Step:   108600, Batch Loss:     1.956390, Tokens per Sec:     7321, Lr: 0.000300\n",
      "2021-08-03 12:56:16,874 - INFO - joeynmt.training - Epoch  10, Step:   108800, Batch Loss:     2.014478, Tokens per Sec:     7182, Lr: 0.000300\n",
      "2021-08-03 12:57:16,221 - INFO - joeynmt.training - Epoch  10, Step:   109000, Batch Loss:     2.171731, Tokens per Sec:     7318, Lr: 0.000300\n",
      "2021-08-03 12:57:57,424 - INFO - joeynmt.training - Epoch  10: total training loss 5362.72\n",
      "2021-08-03 12:57:57,425 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-03 12:58:15,925 - INFO - joeynmt.training - Epoch  11, Step:   109200, Batch Loss:     1.884815, Tokens per Sec:     7120, Lr: 0.000300\n",
      "2021-08-03 12:59:15,350 - INFO - joeynmt.training - Epoch  11, Step:   109400, Batch Loss:     1.879578, Tokens per Sec:     7198, Lr: 0.000300\n",
      "2021-08-03 13:00:14,935 - INFO - joeynmt.training - Epoch  11, Step:   109600, Batch Loss:     1.497224, Tokens per Sec:     7255, Lr: 0.000300\n",
      "2021-08-03 13:01:14,289 - INFO - joeynmt.training - Epoch  11, Step:   109800, Batch Loss:     1.957130, Tokens per Sec:     7147, Lr: 0.000300\n",
      "2021-08-03 13:02:13,634 - INFO - joeynmt.training - Epoch  11, Step:   110000, Batch Loss:     1.885615, Tokens per Sec:     7165, Lr: 0.000300\n",
      "2021-08-03 13:03:55,399 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 13:03:55,399 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 13:03:55,400 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 13:03:56,981 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 13:03:56,982 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 13:03:56,982 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 13:03:56,982 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 13:03:56,982 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 13:03:56,983 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 13:03:56,983 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 13:03:56,983 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the ministry and to read the Scriptures if possible .\n",
      "2021-08-03 13:03:56,984 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 13:03:56,984 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 13:03:56,984 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 13:03:56,985 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvaluable ?\n",
      "2021-08-03 13:03:56,985 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 13:03:56,985 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 13:03:56,986 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 13:03:56,986 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 13:03:56,986 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   110000: bleu:  25.61, loss: 109633.0625, ppl:   5.6679, duration: 103.3522s\n",
      "2021-08-03 13:04:56,732 - INFO - joeynmt.training - Epoch  11, Step:   110200, Batch Loss:     2.038805, Tokens per Sec:     7237, Lr: 0.000300\n",
      "2021-08-03 13:05:56,399 - INFO - joeynmt.training - Epoch  11, Step:   110400, Batch Loss:     1.739464, Tokens per Sec:     7345, Lr: 0.000300\n",
      "2021-08-03 13:06:55,476 - INFO - joeynmt.training - Epoch  11, Step:   110600, Batch Loss:     1.830825, Tokens per Sec:     7219, Lr: 0.000300\n",
      "2021-08-03 13:07:54,349 - INFO - joeynmt.training - Epoch  11, Step:   110800, Batch Loss:     1.842246, Tokens per Sec:     7170, Lr: 0.000300\n",
      "2021-08-03 13:08:53,656 - INFO - joeynmt.training - Epoch  11, Step:   111000, Batch Loss:     1.920410, Tokens per Sec:     7231, Lr: 0.000300\n",
      "2021-08-03 13:09:52,858 - INFO - joeynmt.training - Epoch  11, Step:   111200, Batch Loss:     2.078149, Tokens per Sec:     7241, Lr: 0.000300\n",
      "2021-08-03 13:10:51,992 - INFO - joeynmt.training - Epoch  11, Step:   111400, Batch Loss:     1.580852, Tokens per Sec:     7159, Lr: 0.000300\n",
      "2021-08-03 13:11:51,322 - INFO - joeynmt.training - Epoch  11, Step:   111600, Batch Loss:     1.935046, Tokens per Sec:     7177, Lr: 0.000300\n",
      "2021-08-03 13:12:50,524 - INFO - joeynmt.training - Epoch  11, Step:   111800, Batch Loss:     1.964460, Tokens per Sec:     7150, Lr: 0.000300\n",
      "2021-08-03 13:13:49,998 - INFO - joeynmt.training - Epoch  11, Step:   112000, Batch Loss:     1.929425, Tokens per Sec:     7181, Lr: 0.000300\n",
      "2021-08-03 13:15:34,081 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 13:15:34,081 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 13:15:34,081 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 13:15:34,872 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 13:15:34,873 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 13:15:35,761 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 13:15:35,762 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 13:15:35,762 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 13:15:35,763 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 13:15:35,763 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 13:15:35,764 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 13:15:35,764 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 13:15:35,764 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to read the Scriptures if possible .\n",
      "2021-08-03 13:15:35,764 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 13:15:35,766 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 13:15:35,766 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 13:15:35,766 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-03 13:15:35,766 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 13:15:35,767 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 13:15:35,767 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 13:15:35,767 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 13:15:35,768 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   112000: bleu:  25.93, loss: 108607.5781, ppl:   5.5766, duration: 105.7689s\n",
      "2021-08-03 13:15:40,763 - INFO - joeynmt.training - Epoch  11: total training loss 5359.35\n",
      "2021-08-03 13:15:40,764 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-03 13:16:36,008 - INFO - joeynmt.training - Epoch  12, Step:   112200, Batch Loss:     1.873258, Tokens per Sec:     7183, Lr: 0.000300\n",
      "2021-08-03 13:17:35,341 - INFO - joeynmt.training - Epoch  12, Step:   112400, Batch Loss:     2.032217, Tokens per Sec:     7164, Lr: 0.000300\n",
      "2021-08-03 13:18:34,623 - INFO - joeynmt.training - Epoch  12, Step:   112600, Batch Loss:     1.962970, Tokens per Sec:     7271, Lr: 0.000300\n",
      "2021-08-03 13:19:34,061 - INFO - joeynmt.training - Epoch  12, Step:   112800, Batch Loss:     1.851555, Tokens per Sec:     7213, Lr: 0.000300\n",
      "2021-08-03 13:20:34,049 - INFO - joeynmt.training - Epoch  12, Step:   113000, Batch Loss:     1.904353, Tokens per Sec:     7208, Lr: 0.000300\n",
      "2021-08-03 13:21:33,388 - INFO - joeynmt.training - Epoch  12, Step:   113200, Batch Loss:     1.645270, Tokens per Sec:     7244, Lr: 0.000300\n",
      "2021-08-03 13:22:32,757 - INFO - joeynmt.training - Epoch  12, Step:   113400, Batch Loss:     2.007527, Tokens per Sec:     7261, Lr: 0.000300\n",
      "2021-08-03 13:23:31,967 - INFO - joeynmt.training - Epoch  12, Step:   113600, Batch Loss:     1.653642, Tokens per Sec:     7199, Lr: 0.000300\n",
      "2021-08-03 13:24:31,203 - INFO - joeynmt.training - Epoch  12, Step:   113800, Batch Loss:     1.899282, Tokens per Sec:     7216, Lr: 0.000300\n",
      "2021-08-03 13:25:30,117 - INFO - joeynmt.training - Epoch  12, Step:   114000, Batch Loss:     1.673019, Tokens per Sec:     7090, Lr: 0.000300\n",
      "2021-08-03 13:27:12,892 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 13:27:12,893 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 13:27:12,893 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 13:27:14,482 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 13:27:14,483 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 13:27:14,483 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 13:27:14,483 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 13:27:14,484 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 13:27:14,484 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 13:27:14,485 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 13:27:14,485 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to study the Scriptures if possible .\n",
      "2021-08-03 13:27:14,485 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 13:27:14,486 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 13:27:14,486 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 13:27:14,486 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 13:27:14,486 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 13:27:14,487 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 13:27:14,487 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 13:27:14,488 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 13:27:14,488 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   114000: bleu:  25.87, loss: 108877.6719, ppl:   5.6005, duration: 104.3704s\n",
      "2021-08-03 13:28:14,367 - INFO - joeynmt.training - Epoch  12, Step:   114200, Batch Loss:     1.971727, Tokens per Sec:     7188, Lr: 0.000300\n",
      "2021-08-03 13:29:14,007 - INFO - joeynmt.training - Epoch  12, Step:   114400, Batch Loss:     1.892447, Tokens per Sec:     7213, Lr: 0.000300\n",
      "2021-08-03 13:30:13,075 - INFO - joeynmt.training - Epoch  12, Step:   114600, Batch Loss:     1.600461, Tokens per Sec:     7284, Lr: 0.000300\n",
      "2021-08-03 13:31:12,469 - INFO - joeynmt.training - Epoch  12, Step:   114800, Batch Loss:     1.905771, Tokens per Sec:     7302, Lr: 0.000300\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
      "    main()\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/training.py\", line 805, in train\n",
      "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/training.py\", line 427, in train_and_validate\n",
      "    batch_loss += self._train_step(batch)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/training.py\", line 506, in _train_step\n",
      "    batch_loss, _, _, _ = self.model(return_type=\"loss\", **vars(batch))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/model.py\", line 85, in forward\n",
      "    out, _, _, _ = self._encode_decode(**kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/model.py\", line 137, in _encode_decode\n",
      "    trg_mask=trg_mask, **kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/model.py\", line 178, in _decode\n",
      "    **_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt/joeynmt/decoders.py\", line 536, in forward\n",
      "    trg_embed.size(1)).type_as(trg_mask)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_lhen_reload2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "os75b_EKstYb",
    "outputId": "715afe80-6d1f-4533-83a6-b40eeabb1e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 84000\tLoss: 112382.15625\tPPL: 5.91985\tbleu: 24.80845\tLR: 0.00030000\t*\n",
      "Steps: 86000\tLoss: 112053.11719\tPPL: 5.88911\tbleu: 24.69381\tLR: 0.00030000\t*\n",
      "Steps: 88000\tLoss: 112041.87500\tPPL: 5.88806\tbleu: 24.90341\tLR: 0.00030000\t*\n",
      "Steps: 90000\tLoss: 111316.53125\tPPL: 5.82087\tbleu: 25.21723\tLR: 0.00030000\t*\n",
      "Steps: 92000\tLoss: 111122.16406\tPPL: 5.80299\tbleu: 25.12850\tLR: 0.00030000\t*\n",
      "Steps: 94000\tLoss: 110817.25000\tPPL: 5.77506\tbleu: 25.20617\tLR: 0.00030000\t*\n",
      "Steps: 96000\tLoss: 111121.73438\tPPL: 5.80295\tbleu: 25.34566\tLR: 0.00030000\t\n",
      "Steps: 98000\tLoss: 110113.39062\tPPL: 5.71110\tbleu: 25.39224\tLR: 0.00030000\t*\n",
      "Steps: 100000\tLoss: 110109.16406\tPPL: 5.71071\tbleu: 25.49287\tLR: 0.00030000\t*\n",
      "Steps: 102000\tLoss: 109891.71094\tPPL: 5.69110\tbleu: 25.38576\tLR: 0.00030000\t*\n",
      "Steps: 104000\tLoss: 109975.51562\tPPL: 5.69865\tbleu: 25.67211\tLR: 0.00030000\t\n",
      "Steps: 106000\tLoss: 109685.71094\tPPL: 5.67258\tbleu: 25.67861\tLR: 0.00030000\t*\n",
      "Steps: 108000\tLoss: 109484.73438\tPPL: 5.65456\tbleu: 25.64666\tLR: 0.00030000\t*\n",
      "Steps: 110000\tLoss: 109633.06250\tPPL: 5.66785\tbleu: 25.60627\tLR: 0.00030000\t\n",
      "Steps: 112000\tLoss: 108607.57812\tPPL: 5.57662\tbleu: 25.93242\tLR: 0.00030000\t*\n",
      "Steps: 114000\tLoss: 108877.67188\tPPL: 5.60051\tbleu: 25.86711\tLR: 0.00030000\t\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lg_lhen_reverse_transformer_continued2/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp5f07uBi8wo"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 114000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"{path}/models/lg_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"{path}/joeynmt/models/lg_lhen_reverse_transformer_continued2/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lg_lhen_reverse_transformer\"', f'model_dir: \"models/lg_lhen_reverse_transformer_continued3\"').replace(\n",
    "        f'epochs: 30', f'epochs: 18')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XSAGbpSvvIX7",
    "outputId": "e638ab51-83c1-4dd3-adc9-d6a26f58fdf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lg_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lg_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued2/114000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 18                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lg_lhen_reverse_transformer_continued3\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_lg_lhen_reload3.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgr2ksxOkLiW",
    "outputId": "0a1b2f7e-e55c-424e-fa3b-0f046e42e28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 15:59:43,634 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 15:59:43,667 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 15:59:50,582 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 15:59:50,888 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 15:59:52,224 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 15:59:53,275 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 15:59:53,275 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 15:59:53,527 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 15:59:53.780899: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 15:59:56,052 - INFO - joeynmt.training - Total params: 12151808\n",
      "2021-08-03 15:59:58,231 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued2/114000.ckpt\n",
      "2021-08-03 15:59:58,778 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
      "2021-08-03 15:59:58,778 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
      "2021-08-03 15:59:58,779 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 15:59:58,779 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
      "2021-08-03 15:59:58,779 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
      "2021-08-03 15:59:58,780 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
      "2021-08-03 15:59:58,780 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 15:59:58,780 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 15:59:58,780 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 15:59:58,781 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-03 15:59:58,781 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-03 15:59:58,781 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 15:59:58,781 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 15:59:58,781 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued2/114000.ckpt\n",
      "2021-08-03 15:59:58,782 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 15:59:58,782 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 15:59:58,782 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 15:59:58,782 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 15:59:58,783 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 15:59:58,783 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 15:59:58,783 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 15:59:58,784 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 15:59:58,784 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 15:59:58,784 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 15:59:58,784 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 15:59:58,785 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 15:59:58,785 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 15:59:58,785 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 15:59:58,785 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 15:59:58,786 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 15:59:58,786 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-03 15:59:58,786 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 15:59:58,786 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 15:59:58,787 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 15:59:58,787 - INFO - joeynmt.helpers - cfg.training.epochs                : 18\n",
      "2021-08-03 15:59:58,787 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-03 15:59:58,787 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-03 15:59:58,788 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 15:59:58,788 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer_continued3\n",
      "2021-08-03 15:59:58,788 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 15:59:58,788 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 15:59:58,788 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 15:59:58,789 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 15:59:58,789 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 15:59:58,789 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 15:59:58,789 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 15:59:58,790 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 15:59:58,790 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 15:59:58,790 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 15:59:58,790 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 15:59:58,791 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 15:59:58,791 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 15:59:58,791 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 15:59:58,791 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 15:59:58,791 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 15:59:58,792 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 15:59:58,792 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 15:59:58,792 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 15:59:58,792 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 15:59:58,793 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 15:59:58,793 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 15:59:58,793 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 15:59:58,793 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 15:59:58,794 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 15:59:58,794 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 15:59:58,794 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 15:59:58,794 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 15:59:58,795 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 15:59:58,795 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 15:59:58,795 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 15:59:58,797 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 229609,\n",
      "\tvalid 2349,\n",
      "\ttest 79\n",
      "2021-08-03 15:59:58,797 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-03 15:59:58,798 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 15:59:58,798 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 15:59:58,799 - INFO - joeynmt.helpers - Number of Src words (types): 4264\n",
      "2021-08-03 15:59:58,799 - INFO - joeynmt.helpers - Number of Trg words (types): 4264\n",
      "2021-08-03 15:59:58,799 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4264),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4264))\n",
      "2021-08-03 15:59:58,812 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 15:59:58,813 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 16:00:58,224 - INFO - joeynmt.training - Epoch   1, Step:   114200, Batch Loss:     1.989232, Tokens per Sec:     7245, Lr: 0.000300\n",
      "2021-08-03 16:01:56,805 - INFO - joeynmt.training - Epoch   1, Step:   114400, Batch Loss:     1.900258, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-08-03 16:02:55,007 - INFO - joeynmt.training - Epoch   1, Step:   114600, Batch Loss:     1.577399, Tokens per Sec:     7392, Lr: 0.000300\n",
      "2021-08-03 16:03:53,455 - INFO - joeynmt.training - Epoch   1, Step:   114800, Batch Loss:     1.881604, Tokens per Sec:     7420, Lr: 0.000300\n",
      "2021-08-03 16:04:18,179 - INFO - joeynmt.training - Epoch   1: total training loss 1645.46\n",
      "2021-08-03 16:04:18,180 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 16:04:52,190 - INFO - joeynmt.training - Epoch   2, Step:   115000, Batch Loss:     1.875943, Tokens per Sec:     7340, Lr: 0.000300\n",
      "2021-08-03 16:05:50,627 - INFO - joeynmt.training - Epoch   2, Step:   115200, Batch Loss:     1.758408, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-03 16:06:48,772 - INFO - joeynmt.training - Epoch   2, Step:   115400, Batch Loss:     1.921589, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-08-03 16:07:46,305 - INFO - joeynmt.training - Epoch   2, Step:   115600, Batch Loss:     1.305868, Tokens per Sec:     7368, Lr: 0.000300\n",
      "2021-08-03 16:08:44,572 - INFO - joeynmt.training - Epoch   2, Step:   115800, Batch Loss:     1.837663, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-08-03 16:09:42,775 - INFO - joeynmt.training - Epoch   2, Step:   116000, Batch Loss:     1.759400, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-08-03 16:11:21,100 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 16:11:21,101 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 16:11:21,101 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 16:11:23,578 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 16:11:23,582 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 16:11:23,582 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 16:11:23,583 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 16:11:23,583 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 16:11:23,584 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 16:11:23,584 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 16:11:23,584 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in the ministry and to study the Scriptures if possible .\n",
      "2021-08-03 16:11:23,585 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 16:11:23,585 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 16:11:23,586 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 16:11:23,586 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 16:11:23,586 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 16:11:23,587 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 16:11:23,587 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 16:11:23,587 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 16:11:23,588 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   116000: bleu:  25.83, loss: 108834.5234, ppl:   5.5967, duration: 100.8118s\n",
      "2021-08-03 16:12:22,323 - INFO - joeynmt.training - Epoch   2, Step:   116200, Batch Loss:     1.959840, Tokens per Sec:     7302, Lr: 0.000300\n",
      "2021-08-03 16:13:20,525 - INFO - joeynmt.training - Epoch   2, Step:   116400, Batch Loss:     1.472053, Tokens per Sec:     7221, Lr: 0.000300\n",
      "2021-08-03 16:14:18,683 - INFO - joeynmt.training - Epoch   2, Step:   116600, Batch Loss:     1.826522, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-03 16:15:17,255 - INFO - joeynmt.training - Epoch   2, Step:   116800, Batch Loss:     1.951276, Tokens per Sec:     7381, Lr: 0.000300\n",
      "2021-08-03 16:16:15,147 - INFO - joeynmt.training - Epoch   2, Step:   117000, Batch Loss:     2.045261, Tokens per Sec:     7202, Lr: 0.000300\n",
      "2021-08-03 16:17:13,664 - INFO - joeynmt.training - Epoch   2, Step:   117200, Batch Loss:     2.123693, Tokens per Sec:     7366, Lr: 0.000300\n",
      "2021-08-03 16:18:12,105 - INFO - joeynmt.training - Epoch   2, Step:   117400, Batch Loss:     1.822463, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-08-03 16:19:10,738 - INFO - joeynmt.training - Epoch   2, Step:   117600, Batch Loss:     1.977729, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-08-03 16:19:54,663 - INFO - joeynmt.training - Epoch   2: total training loss 5309.71\n",
      "2021-08-03 16:19:54,663 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 16:20:09,312 - INFO - joeynmt.training - Epoch   3, Step:   117800, Batch Loss:     1.691623, Tokens per Sec:     7175, Lr: 0.000300\n",
      "2021-08-03 16:21:07,166 - INFO - joeynmt.training - Epoch   3, Step:   118000, Batch Loss:     1.567780, Tokens per Sec:     7343, Lr: 0.000300\n",
      "2021-08-03 16:22:44,237 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 16:22:44,238 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 16:22:44,238 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 16:22:46,055 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 16:22:46,056 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 16:22:46,056 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 16:22:46,057 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 16:22:46,057 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 16:22:46,058 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 16:22:46,058 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 16:22:46,058 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our zeal and to study the Scriptures if possible .\n",
      "2021-08-03 16:22:46,058 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 16:22:46,059 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 16:22:46,059 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 16:22:46,060 - INFO - joeynmt.training - \tHypothesis: Why are false gods unempty ?\n",
      "2021-08-03 16:22:46,060 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 16:22:46,060 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 16:22:46,061 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 16:22:46,061 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 16:22:46,061 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   118000: bleu:  25.51, loss: 109084.2266, ppl:   5.6188, duration: 98.8944s\n",
      "2021-08-03 16:23:44,663 - INFO - joeynmt.training - Epoch   3, Step:   118200, Batch Loss:     1.768375, Tokens per Sec:     7385, Lr: 0.000300\n",
      "2021-08-03 16:24:42,971 - INFO - joeynmt.training - Epoch   3, Step:   118400, Batch Loss:     1.984208, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-08-03 16:25:41,082 - INFO - joeynmt.training - Epoch   3, Step:   118600, Batch Loss:     1.964507, Tokens per Sec:     7325, Lr: 0.000300\n",
      "2021-08-03 16:26:38,882 - INFO - joeynmt.training - Epoch   3, Step:   118800, Batch Loss:     2.032595, Tokens per Sec:     7291, Lr: 0.000300\n",
      "2021-08-03 16:27:37,309 - INFO - joeynmt.training - Epoch   3, Step:   119000, Batch Loss:     1.802553, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-08-03 16:28:35,585 - INFO - joeynmt.training - Epoch   3, Step:   119200, Batch Loss:     1.944350, Tokens per Sec:     7337, Lr: 0.000300\n",
      "2021-08-03 16:29:33,864 - INFO - joeynmt.training - Epoch   3, Step:   119400, Batch Loss:     1.468867, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-08-03 16:30:31,694 - INFO - joeynmt.training - Epoch   3, Step:   119600, Batch Loss:     1.801188, Tokens per Sec:     7402, Lr: 0.000300\n",
      "2021-08-03 16:31:29,685 - INFO - joeynmt.training - Epoch   3, Step:   119800, Batch Loss:     1.895375, Tokens per Sec:     7380, Lr: 0.000300\n",
      "2021-08-03 16:32:27,539 - INFO - joeynmt.training - Epoch   3, Step:   120000, Batch Loss:     1.749828, Tokens per Sec:     7408, Lr: 0.000300\n",
      "2021-08-03 16:34:03,694 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 16:34:03,695 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 16:34:03,695 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 16:34:04,422 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 16:34:04,423 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 16:34:05,678 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 16:34:05,679 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 16:34:05,679 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 16:34:05,679 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 16:34:05,679 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 16:34:05,680 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 16:34:05,680 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 16:34:05,681 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-03 16:34:05,681 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 16:34:05,681 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 16:34:05,682 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 16:34:05,682 - INFO - joeynmt.training - \tHypothesis: Why are false gods unworthy ?\n",
      "2021-08-03 16:34:05,682 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 16:34:05,683 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 16:34:05,683 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 16:34:05,683 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 16:34:05,683 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   120000: bleu:  25.73, loss: 108191.7891, ppl:   5.5401, duration: 98.1438s\n",
      "2021-08-03 16:35:04,290 - INFO - joeynmt.training - Epoch   3, Step:   120200, Batch Loss:     2.127686, Tokens per Sec:     7377, Lr: 0.000300\n",
      "2021-08-03 16:36:02,706 - INFO - joeynmt.training - Epoch   3, Step:   120400, Batch Loss:     2.112066, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-08-03 16:37:00,857 - INFO - joeynmt.training - Epoch   3, Step:   120600, Batch Loss:     1.827800, Tokens per Sec:     7241, Lr: 0.000300\n",
      "2021-08-03 16:37:07,664 - INFO - joeynmt.training - Epoch   3: total training loss 5309.14\n",
      "2021-08-03 16:37:07,665 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 16:37:59,675 - INFO - joeynmt.training - Epoch   4, Step:   120800, Batch Loss:     1.433787, Tokens per Sec:     7222, Lr: 0.000300\n",
      "2021-08-03 16:38:57,202 - INFO - joeynmt.training - Epoch   4, Step:   121000, Batch Loss:     2.046692, Tokens per Sec:     7351, Lr: 0.000300\n",
      "2021-08-03 16:39:55,375 - INFO - joeynmt.training - Epoch   4, Step:   121200, Batch Loss:     1.964820, Tokens per Sec:     7238, Lr: 0.000300\n",
      "2021-08-03 16:40:54,193 - INFO - joeynmt.training - Epoch   4, Step:   121400, Batch Loss:     1.815398, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-08-03 16:41:52,630 - INFO - joeynmt.training - Epoch   4, Step:   121600, Batch Loss:     1.544557, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-08-03 16:42:50,585 - INFO - joeynmt.training - Epoch   4, Step:   121800, Batch Loss:     1.967447, Tokens per Sec:     7256, Lr: 0.000300\n",
      "2021-08-03 16:43:49,071 - INFO - joeynmt.training - Epoch   4, Step:   122000, Batch Loss:     1.814593, Tokens per Sec:     7275, Lr: 0.000300\n",
      "2021-08-03 16:45:28,092 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 16:45:28,092 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 16:45:28,093 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 16:45:29,721 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 16:45:29,721 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 16:45:29,722 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 16:45:29,722 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 16:45:29,722 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 16:45:29,723 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 16:45:29,723 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 16:45:29,723 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to study the Scriptures if possible .\n",
      "2021-08-03 16:45:29,723 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 16:45:29,724 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 16:45:29,724 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 16:45:29,724 - INFO - joeynmt.training - \tHypothesis: Why do false gods have unvaluable things ?\n",
      "2021-08-03 16:45:29,725 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 16:45:29,725 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 16:45:29,725 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 16:45:29,726 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 16:45:29,726 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   122000: bleu:  25.96, loss: 108538.6016, ppl:   5.5705, duration: 100.6543s\n",
      "2021-08-03 16:46:27,821 - INFO - joeynmt.training - Epoch   4, Step:   122200, Batch Loss:     1.656300, Tokens per Sec:     7300, Lr: 0.000300\n",
      "2021-08-03 16:47:26,413 - INFO - joeynmt.training - Epoch   4, Step:   122400, Batch Loss:     1.493412, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-08-03 16:48:24,436 - INFO - joeynmt.training - Epoch   4, Step:   122600, Batch Loss:     1.749618, Tokens per Sec:     7364, Lr: 0.000300\n",
      "2021-08-03 16:49:22,972 - INFO - joeynmt.training - Epoch   4, Step:   122800, Batch Loss:     1.094648, Tokens per Sec:     7390, Lr: 0.000300\n",
      "2021-08-03 16:50:21,115 - INFO - joeynmt.training - Epoch   4, Step:   123000, Batch Loss:     1.815425, Tokens per Sec:     7256, Lr: 0.000300\n",
      "2021-08-03 16:51:19,894 - INFO - joeynmt.training - Epoch   4, Step:   123200, Batch Loss:     1.830395, Tokens per Sec:     7399, Lr: 0.000300\n",
      "2021-08-03 16:52:18,516 - INFO - joeynmt.training - Epoch   4, Step:   123400, Batch Loss:     1.921788, Tokens per Sec:     7416, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_lhen_reload3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCmBFF5f0UGE",
    "outputId": "3973e11e-7e73-4a65-f174-227d7d9823f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 116000\tLoss: 108834.52344\tPPL: 5.59668\tbleu: 25.82742\tLR: 0.00030000\t\n",
      "Steps: 118000\tLoss: 109084.22656\tPPL: 5.61884\tbleu: 25.51365\tLR: 0.00030000\t\n",
      "Steps: 120000\tLoss: 108191.78906\tPPL: 5.54005\tbleu: 25.73462\tLR: 0.00030000\t*\n",
      "Steps: 122000\tLoss: 108538.60156\tPPL: 5.57054\tbleu: 25.96100\tLR: 0.00030000\t\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lg_lhen_reverse_transformer_continued3/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj534gQZ0yuv"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 122000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"{path}/models/lg_lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"{path}/joeynmt/models/lg_lhen_reverse_transformer_continued3/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lg_lhen_reverse_transformer\"', f'model_dir: \"models/lg_lhen_reverse_transformer_continued4\"').replace(\n",
    "        f'epochs: 30', f'epochs: 16')\n",
    "        \n",
    "with open(\"joeynmt/configs/transformer_{name}_reload4.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "yXTQFkXL1VnH",
    "outputId": "fcfcb61b-9bf5-435c-c69f-2757fe018f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lg_lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lg_lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued3/122000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1000\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 16                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 200\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lg_lhen_reverse_transformer_continued4\"\n",
      "    overwrite: True \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_lg_lhen_reload4.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDEMVVW91mm2",
    "outputId": "db2c59c1-d695-4bf9-a106-2e15ba078d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 21:06:20,797 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 21:06:20,862 - INFO - joeynmt.data - Loading training data...\n",
      "2021-08-03 21:06:26,168 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-08-03 21:06:27,030 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-08-03 21:06:27,766 - INFO - joeynmt.data - Loading test data...\n",
      "2021-08-03 21:06:28,522 - INFO - joeynmt.data - Data loaded.\n",
      "2021-08-03 21:06:28,523 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 21:06:28,902 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 21:06:29.143044: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-03 21:06:30,725 - INFO - joeynmt.training - Total params: 12151808\n",
      "2021-08-03 21:06:41,317 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued3/122000.ckpt\n",
      "2021-08-03 21:06:41,759 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
      "2021-08-03 21:06:41,759 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-03 21:06:41,760 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
      "2021-08-03 21:06:41,761 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-08-03 21:06:41,761 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-08-03 21:06:41,761 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued3/122000.ckpt\n",
      "2021-08-03 21:06:41,761 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-08-03 21:06:41,761 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-08-03 21:06:41,761 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-08-03 21:06:41,761 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-08-03 21:06:41,761 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-08-03 21:06:41,762 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-08-03 21:06:41,762 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-08-03 21:06:41,762 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-08-03 21:06:41,762 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-08-03 21:06:41,762 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-08-03 21:06:41,762 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-08-03 21:06:41,762 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-08-03 21:06:41,762 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-08-03 21:06:41,763 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-08-03 21:06:41,763 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-08-03 21:06:41,763 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-08-03 21:06:41,763 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
      "2021-08-03 21:06:41,763 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-08-03 21:06:41,763 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-08-03 21:06:41,764 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-08-03 21:06:41,764 - INFO - joeynmt.helpers - cfg.training.epochs                : 16\n",
      "2021-08-03 21:06:41,764 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2000\n",
      "2021-08-03 21:06:41,764 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-08-03 21:06:41,764 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-08-03 21:06:41,764 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer_continued4\n",
      "2021-08-03 21:06:41,764 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-08-03 21:06:41,764 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-08-03 21:06:41,765 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-08-03 21:06:41,765 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-08-03 21:06:41,765 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-08-03 21:06:41,765 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-08-03 21:06:41,765 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-08-03 21:06:41,765 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-08-03 21:06:41,765 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-08-03 21:06:41,765 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-08-03 21:06:41,766 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-08-03 21:06:41,766 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-08-03 21:06:41,766 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-08-03 21:06:41,766 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-08-03 21:06:41,766 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-08-03 21:06:41,766 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-08-03 21:06:41,766 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 21:06:41,766 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-08-03 21:06:41,767 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-08-03 21:06:41,767 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-08-03 21:06:41,767 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-08-03 21:06:41,767 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-08-03 21:06:41,767 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-08-03 21:06:41,767 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-08-03 21:06:41,767 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-08-03 21:06:41,767 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-08-03 21:06:41,768 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-08-03 21:06:41,768 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-08-03 21:06:41,768 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-08-03 21:06:41,768 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-08-03 21:06:41,768 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-08-03 21:06:41,768 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 229609,\n",
      "\tvalid 2349,\n",
      "\ttest 79\n",
      "2021-08-03 21:06:41,768 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-08-03 21:06:41,769 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 21:06:41,769 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-08-03 21:06:41,769 - INFO - joeynmt.helpers - Number of Src words (types): 4264\n",
      "2021-08-03 21:06:41,769 - INFO - joeynmt.helpers - Number of Trg words (types): 4264\n",
      "2021-08-03 21:06:41,769 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4264),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4264))\n",
      "2021-08-03 21:06:41,779 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-08-03 21:06:41,779 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-08-03 21:07:07,428 - INFO - joeynmt.training - Epoch   1, Step:   122200, Batch Loss:     1.656443, Tokens per Sec:    16536, Lr: 0.000300\n",
      "2021-08-03 21:07:32,454 - INFO - joeynmt.training - Epoch   1, Step:   122400, Batch Loss:     1.551999, Tokens per Sec:    17352, Lr: 0.000300\n",
      "2021-08-03 21:07:57,451 - INFO - joeynmt.training - Epoch   1, Step:   122600, Batch Loss:     1.755529, Tokens per Sec:    17093, Lr: 0.000300\n",
      "2021-08-03 21:08:23,184 - INFO - joeynmt.training - Epoch   1, Step:   122800, Batch Loss:     1.096614, Tokens per Sec:    16810, Lr: 0.000300\n",
      "2021-08-03 21:08:48,635 - INFO - joeynmt.training - Epoch   1, Step:   123000, Batch Loss:     1.806617, Tokens per Sec:    16577, Lr: 0.000300\n",
      "2021-08-03 21:09:14,725 - INFO - joeynmt.training - Epoch   1, Step:   123200, Batch Loss:     1.854411, Tokens per Sec:    16671, Lr: 0.000300\n",
      "2021-08-03 21:09:40,950 - INFO - joeynmt.training - Epoch   1, Step:   123400, Batch Loss:     1.900206, Tokens per Sec:    16576, Lr: 0.000300\n",
      "2021-08-03 21:09:53,387 - INFO - joeynmt.training - Epoch   1: total training loss 2759.54\n",
      "2021-08-03 21:09:53,387 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-08-03 21:10:07,389 - INFO - joeynmt.training - Epoch   2, Step:   123600, Batch Loss:     1.331426, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-08-03 21:10:33,518 - INFO - joeynmt.training - Epoch   2, Step:   123800, Batch Loss:     1.810602, Tokens per Sec:    16642, Lr: 0.000300\n",
      "2021-08-03 21:10:59,895 - INFO - joeynmt.training - Epoch   2, Step:   124000, Batch Loss:     1.956244, Tokens per Sec:    16147, Lr: 0.000300\n",
      "2021-08-03 21:11:54,871 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:11:54,871 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:11:54,871 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:11:55,538 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:11:55,538 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:11:56,298 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:11:56,299 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:11:56,300 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:11:56,300 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:11:56,300 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:11:56,301 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:11:56,301 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:11:56,301 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to read the Scriptures when possible .\n",
      "2021-08-03 21:11:56,301 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:11:56,301 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:11:56,302 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:11:56,302 - INFO - joeynmt.training - \tHypothesis: Why are false gods unclean ?\n",
      "2021-08-03 21:11:56,302 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:11:56,302 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:11:56,303 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:11:56,303 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:11:56,303 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   124000: bleu:  25.95, loss: 108159.0234, ppl:   5.5372, duration: 56.4071s\n",
      "2021-08-03 21:12:22,817 - INFO - joeynmt.training - Epoch   2, Step:   124200, Batch Loss:     1.832040, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-08-03 21:12:48,819 - INFO - joeynmt.training - Epoch   2, Step:   124400, Batch Loss:     1.891507, Tokens per Sec:    16516, Lr: 0.000300\n",
      "2021-08-03 21:13:15,318 - INFO - joeynmt.training - Epoch   2, Step:   124600, Batch Loss:     1.822603, Tokens per Sec:    16352, Lr: 0.000300\n",
      "2021-08-03 21:13:41,340 - INFO - joeynmt.training - Epoch   2, Step:   124800, Batch Loss:     1.732691, Tokens per Sec:    16345, Lr: 0.000300\n",
      "2021-08-03 21:14:07,445 - INFO - joeynmt.training - Epoch   2, Step:   125000, Batch Loss:     1.945811, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-08-03 21:14:33,411 - INFO - joeynmt.training - Epoch   2, Step:   125200, Batch Loss:     1.879078, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-03 21:14:59,903 - INFO - joeynmt.training - Epoch   2, Step:   125400, Batch Loss:     1.639426, Tokens per Sec:    16209, Lr: 0.000300\n",
      "2021-08-03 21:15:26,047 - INFO - joeynmt.training - Epoch   2, Step:   125600, Batch Loss:     1.762074, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-08-03 21:15:52,241 - INFO - joeynmt.training - Epoch   2, Step:   125800, Batch Loss:     1.893939, Tokens per Sec:    16473, Lr: 0.000300\n",
      "2021-08-03 21:16:18,683 - INFO - joeynmt.training - Epoch   2, Step:   126000, Batch Loss:     2.121164, Tokens per Sec:    16249, Lr: 0.000300\n",
      "2021-08-03 21:17:15,191 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:17:15,192 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:17:15,192 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:17:15,801 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:17:15,801 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:17:16,518 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:17:16,520 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:17:16,520 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:17:16,520 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:17:16,520 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:17:16,521 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:17:16,521 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:17:16,521 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the preaching and to read the Scriptures if possible .\n",
      "2021-08-03 21:17:16,521 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:17:16,522 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:17:16,522 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:17:16,522 - INFO - joeynmt.training - \tHypothesis: Why are false gods unempty ?\n",
      "2021-08-03 21:17:16,523 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:17:16,523 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:17:16,523 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:17:16,523 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:17:16,523 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   126000: bleu:  26.30, loss: 107925.1562, ppl:   5.5167, duration: 57.8402s\n",
      "2021-08-03 21:17:42,845 - INFO - joeynmt.training - Epoch   2, Step:   126200, Batch Loss:     1.910460, Tokens per Sec:    16389, Lr: 0.000300\n",
      "2021-08-03 21:18:04,591 - INFO - joeynmt.training - Epoch   2: total training loss 5280.62\n",
      "2021-08-03 21:18:04,592 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-08-03 21:18:09,685 - INFO - joeynmt.training - Epoch   3, Step:   126400, Batch Loss:     1.990369, Tokens per Sec:    15163, Lr: 0.000300\n",
      "2021-08-03 21:18:35,627 - INFO - joeynmt.training - Epoch   3, Step:   126600, Batch Loss:     1.649308, Tokens per Sec:    16394, Lr: 0.000300\n",
      "2021-08-03 21:19:01,643 - INFO - joeynmt.training - Epoch   3, Step:   126800, Batch Loss:     1.942786, Tokens per Sec:    16290, Lr: 0.000300\n",
      "2021-08-03 21:19:27,938 - INFO - joeynmt.training - Epoch   3, Step:   127000, Batch Loss:     1.976689, Tokens per Sec:    16440, Lr: 0.000300\n",
      "2021-08-03 21:19:54,007 - INFO - joeynmt.training - Epoch   3, Step:   127200, Batch Loss:     1.552537, Tokens per Sec:    16412, Lr: 0.000300\n",
      "2021-08-03 21:20:20,525 - INFO - joeynmt.training - Epoch   3, Step:   127400, Batch Loss:     1.919036, Tokens per Sec:    16431, Lr: 0.000300\n",
      "2021-08-03 21:20:46,412 - INFO - joeynmt.training - Epoch   3, Step:   127600, Batch Loss:     1.362262, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-08-03 21:21:12,675 - INFO - joeynmt.training - Epoch   3, Step:   127800, Batch Loss:     1.946288, Tokens per Sec:    16280, Lr: 0.000300\n",
      "2021-08-03 21:21:38,633 - INFO - joeynmt.training - Epoch   3, Step:   128000, Batch Loss:     1.695765, Tokens per Sec:    16440, Lr: 0.000300\n",
      "2021-08-03 21:22:34,307 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:22:34,307 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:22:34,308 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:22:34,917 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:22:34,917 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:22:35,653 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:22:35,655 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:22:35,655 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:22:35,656 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:22:35,656 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:22:35,656 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:22:35,656 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:22:35,656 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures when possible .\n",
      "2021-08-03 21:22:35,657 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:22:35,657 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:22:35,657 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:22:35,657 - INFO - joeynmt.training - \tHypothesis: Why are false gods unfutile ?\n",
      "2021-08-03 21:22:35,657 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:22:35,658 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:22:35,658 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:22:35,658 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:22:35,658 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   128000: bleu:  26.16, loss: 107705.8125, ppl:   5.4976, duration: 57.0245s\n",
      "2021-08-03 21:23:02,463 - INFO - joeynmt.training - Epoch   3, Step:   128200, Batch Loss:     1.891614, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-08-03 21:23:28,830 - INFO - joeynmt.training - Epoch   3, Step:   128400, Batch Loss:     1.668324, Tokens per Sec:    16627, Lr: 0.000300\n",
      "2021-08-03 21:23:54,867 - INFO - joeynmt.training - Epoch   3, Step:   128600, Batch Loss:     1.828188, Tokens per Sec:    16391, Lr: 0.000300\n",
      "2021-08-03 21:24:21,108 - INFO - joeynmt.training - Epoch   3, Step:   128800, Batch Loss:     1.730095, Tokens per Sec:    16464, Lr: 0.000300\n",
      "2021-08-03 21:24:47,227 - INFO - joeynmt.training - Epoch   3, Step:   129000, Batch Loss:     1.653327, Tokens per Sec:    16464, Lr: 0.000300\n",
      "2021-08-03 21:25:13,526 - INFO - joeynmt.training - Epoch   3, Step:   129200, Batch Loss:     1.957952, Tokens per Sec:    16500, Lr: 0.000300\n",
      "2021-08-03 21:25:18,407 - INFO - joeynmt.training - Epoch   3: total training loss 5269.99\n",
      "2021-08-03 21:25:18,408 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-08-03 21:25:40,215 - INFO - joeynmt.training - Epoch   4, Step:   129400, Batch Loss:     1.811702, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-08-03 21:26:06,599 - INFO - joeynmt.training - Epoch   4, Step:   129600, Batch Loss:     1.861854, Tokens per Sec:    16149, Lr: 0.000300\n",
      "2021-08-03 21:26:32,452 - INFO - joeynmt.training - Epoch   4, Step:   129800, Batch Loss:     1.755808, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-08-03 21:26:58,851 - INFO - joeynmt.training - Epoch   4, Step:   130000, Batch Loss:     2.321795, Tokens per Sec:    16452, Lr: 0.000300\n",
      "2021-08-03 21:27:53,808 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:27:53,809 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:27:53,809 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:27:55,211 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:27:55,213 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:27:55,213 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:27:55,213 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:27:55,213 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:27:55,214 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:27:55,214 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:27:55,214 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 21:27:55,215 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:27:55,215 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:27:55,215 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:27:55,215 - INFO - joeynmt.training - \tHypothesis: Why are false gods unempty ?\n",
      "2021-08-03 21:27:55,215 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:27:55,216 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:27:55,216 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:27:55,216 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:27:55,217 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   130000: bleu:  25.94, loss: 107843.4375, ppl:   5.5096, duration: 56.3652s\n",
      "2021-08-03 21:28:21,638 - INFO - joeynmt.training - Epoch   4, Step:   130200, Batch Loss:     1.797047, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-08-03 21:28:47,831 - INFO - joeynmt.training - Epoch   4, Step:   130400, Batch Loss:     1.867322, Tokens per Sec:    16549, Lr: 0.000300\n",
      "2021-08-03 21:29:14,195 - INFO - joeynmt.training - Epoch   4, Step:   130600, Batch Loss:     1.848432, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-08-03 21:29:40,212 - INFO - joeynmt.training - Epoch   4, Step:   130800, Batch Loss:     1.982236, Tokens per Sec:    16490, Lr: 0.000300\n",
      "2021-08-03 21:30:06,330 - INFO - joeynmt.training - Epoch   4, Step:   131000, Batch Loss:     1.834122, Tokens per Sec:    16326, Lr: 0.000300\n",
      "2021-08-03 21:30:32,560 - INFO - joeynmt.training - Epoch   4, Step:   131200, Batch Loss:     1.863061, Tokens per Sec:    16344, Lr: 0.000300\n",
      "2021-08-03 21:30:59,047 - INFO - joeynmt.training - Epoch   4, Step:   131400, Batch Loss:     1.667349, Tokens per Sec:    16516, Lr: 0.000300\n",
      "2021-08-03 21:31:25,597 - INFO - joeynmt.training - Epoch   4, Step:   131600, Batch Loss:     1.864927, Tokens per Sec:    16279, Lr: 0.000300\n",
      "2021-08-03 21:31:52,129 - INFO - joeynmt.training - Epoch   4, Step:   131800, Batch Loss:     1.809030, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-08-03 21:32:18,725 - INFO - joeynmt.training - Epoch   4, Step:   132000, Batch Loss:     1.878881, Tokens per Sec:    16359, Lr: 0.000300\n",
      "2021-08-03 21:33:13,748 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:33:13,748 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:33:13,748 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:33:14,356 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:33:14,356 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:33:15,159 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:33:15,161 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:33:15,162 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:33:15,162 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:33:15,162 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:33:15,163 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:33:15,163 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:33:15,163 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures if possible .\n",
      "2021-08-03 21:33:15,163 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:33:15,164 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:33:15,164 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:33:15,164 - INFO - joeynmt.training - \tHypothesis: Why are false gods unempty ?\n",
      "2021-08-03 21:33:15,164 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:33:15,165 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:33:15,165 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:33:15,165 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:33:15,166 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   132000: bleu:  26.12, loss: 107375.8203, ppl:   5.4690, duration: 56.4399s\n",
      "2021-08-03 21:33:29,418 - INFO - joeynmt.training - Epoch   4: total training loss 5258.72\n",
      "2021-08-03 21:33:29,418 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-08-03 21:33:41,462 - INFO - joeynmt.training - Epoch   5, Step:   132200, Batch Loss:     2.039705, Tokens per Sec:    15659, Lr: 0.000300\n",
      "2021-08-03 21:34:07,813 - INFO - joeynmt.training - Epoch   5, Step:   132400, Batch Loss:     1.567665, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-03 21:34:33,958 - INFO - joeynmt.training - Epoch   5, Step:   132600, Batch Loss:     1.857572, Tokens per Sec:    16363, Lr: 0.000300\n",
      "2021-08-03 21:35:00,159 - INFO - joeynmt.training - Epoch   5, Step:   132800, Batch Loss:     1.680767, Tokens per Sec:    16547, Lr: 0.000300\n",
      "2021-08-03 21:35:26,574 - INFO - joeynmt.training - Epoch   5, Step:   133000, Batch Loss:     1.790420, Tokens per Sec:    16226, Lr: 0.000300\n",
      "2021-08-03 21:35:52,774 - INFO - joeynmt.training - Epoch   5, Step:   133200, Batch Loss:     1.965859, Tokens per Sec:    16403, Lr: 0.000300\n",
      "2021-08-03 21:36:19,094 - INFO - joeynmt.training - Epoch   5, Step:   133400, Batch Loss:     2.020196, Tokens per Sec:    16311, Lr: 0.000300\n",
      "2021-08-03 21:36:45,307 - INFO - joeynmt.training - Epoch   5, Step:   133600, Batch Loss:     1.913255, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-08-03 21:37:11,548 - INFO - joeynmt.training - Epoch   5, Step:   133800, Batch Loss:     2.058597, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-03 21:37:37,706 - INFO - joeynmt.training - Epoch   5, Step:   134000, Batch Loss:     1.874327, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-08-03 21:38:33,737 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:38:33,737 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:38:33,737 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:38:34,347 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:38:34,347 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:38:35,091 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:38:35,092 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:38:35,092 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:38:35,092 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:38:35,092 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:38:35,093 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:38:35,093 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:38:35,093 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible to preach and to read the Scriptures if possible .\n",
      "2021-08-03 21:38:35,093 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:38:35,094 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:38:35,094 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:38:35,094 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-03 21:38:35,094 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:38:35,095 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:38:35,095 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:38:35,095 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:38:35,095 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   134000: bleu:  26.34, loss: 107292.5938, ppl:   5.4618, duration: 57.3889s\n",
      "2021-08-03 21:39:02,138 - INFO - joeynmt.training - Epoch   5, Step:   134200, Batch Loss:     1.748734, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-08-03 21:39:28,489 - INFO - joeynmt.training - Epoch   5, Step:   134400, Batch Loss:     1.867036, Tokens per Sec:    16491, Lr: 0.000300\n",
      "2021-08-03 21:39:54,588 - INFO - joeynmt.training - Epoch   5, Step:   134600, Batch Loss:     1.445801, Tokens per Sec:    16538, Lr: 0.000300\n",
      "2021-08-03 21:40:21,032 - INFO - joeynmt.training - Epoch   5, Step:   134800, Batch Loss:     1.963336, Tokens per Sec:    16129, Lr: 0.000300\n",
      "2021-08-03 21:40:45,222 - INFO - joeynmt.training - Epoch   5: total training loss 5251.08\n",
      "2021-08-03 21:40:45,223 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-08-03 21:40:47,493 - INFO - joeynmt.training - Epoch   6, Step:   135000, Batch Loss:     1.840823, Tokens per Sec:    15134, Lr: 0.000300\n",
      "2021-08-03 21:41:13,912 - INFO - joeynmt.training - Epoch   6, Step:   135200, Batch Loss:     1.875296, Tokens per Sec:    16152, Lr: 0.000300\n",
      "2021-08-03 21:41:40,233 - INFO - joeynmt.training - Epoch   6, Step:   135400, Batch Loss:     1.502777, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-08-03 21:42:06,452 - INFO - joeynmt.training - Epoch   6, Step:   135600, Batch Loss:     1.737608, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-03 21:42:32,339 - INFO - joeynmt.training - Epoch   6, Step:   135800, Batch Loss:     1.914280, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-08-03 21:42:58,909 - INFO - joeynmt.training - Epoch   6, Step:   136000, Batch Loss:     2.059146, Tokens per Sec:    16306, Lr: 0.000300\n",
      "2021-08-03 21:43:53,065 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:43:53,065 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:43:53,066 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:43:54,416 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:43:54,417 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:43:54,417 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:43:54,417 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:43:54,417 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:43:54,418 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:43:54,418 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:43:54,418 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to study the Scriptures if possible .\n",
      "2021-08-03 21:43:54,418 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:43:54,419 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:43:54,419 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:43:54,419 - INFO - joeynmt.training - \tHypothesis: Why do false gods have a valueless thing ?\n",
      "2021-08-03 21:43:54,419 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:43:54,420 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:43:54,420 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:43:54,420 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:43:54,421 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   136000: bleu:  26.09, loss: 107623.2500, ppl:   5.4904, duration: 55.5112s\n",
      "2021-08-03 21:44:20,984 - INFO - joeynmt.training - Epoch   6, Step:   136200, Batch Loss:     1.842323, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-08-03 21:44:47,076 - INFO - joeynmt.training - Epoch   6, Step:   136400, Batch Loss:     1.768684, Tokens per Sec:    16568, Lr: 0.000300\n",
      "2021-08-03 21:45:13,687 - INFO - joeynmt.training - Epoch   6, Step:   136600, Batch Loss:     1.983741, Tokens per Sec:    16304, Lr: 0.000300\n",
      "2021-08-03 21:45:39,927 - INFO - joeynmt.training - Epoch   6, Step:   136800, Batch Loss:     1.617384, Tokens per Sec:    16587, Lr: 0.000300\n",
      "2021-08-03 21:46:06,134 - INFO - joeynmt.training - Epoch   6, Step:   137000, Batch Loss:     1.723621, Tokens per Sec:    16294, Lr: 0.000300\n",
      "2021-08-03 21:46:32,583 - INFO - joeynmt.training - Epoch   6, Step:   137200, Batch Loss:     1.892359, Tokens per Sec:    16334, Lr: 0.000300\n",
      "2021-08-03 21:46:58,545 - INFO - joeynmt.training - Epoch   6, Step:   137400, Batch Loss:     1.863358, Tokens per Sec:    16287, Lr: 0.000300\n",
      "2021-08-03 21:47:25,165 - INFO - joeynmt.training - Epoch   6, Step:   137600, Batch Loss:     1.930112, Tokens per Sec:    16426, Lr: 0.000300\n",
      "2021-08-03 21:47:51,466 - INFO - joeynmt.training - Epoch   6, Step:   137800, Batch Loss:     1.822146, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-08-03 21:47:59,189 - INFO - joeynmt.training - Epoch   6: total training loss 5241.31\n",
      "2021-08-03 21:47:59,189 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-08-03 21:48:17,840 - INFO - joeynmt.training - Epoch   7, Step:   138000, Batch Loss:     1.821195, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-08-03 21:49:12,087 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:49:12,087 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:49:12,088 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:49:12,727 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:49:12,728 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:49:13,479 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:49:13,480 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:49:13,480 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:49:13,480 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:49:13,480 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:49:13,481 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:49:13,481 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:49:13,482 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with the ministry and to study the Scriptures with people when possible .\n",
      "2021-08-03 21:49:13,482 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:49:13,482 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:49:13,483 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:49:13,483 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 21:49:13,484 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:49:13,484 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:49:13,484 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:49:13,484 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:49:13,485 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   138000: bleu:  25.99, loss: 107152.7734, ppl:   5.4497, duration: 55.6441s\n",
      "2021-08-03 21:49:39,811 - INFO - joeynmt.training - Epoch   7, Step:   138200, Batch Loss:     1.626968, Tokens per Sec:    16312, Lr: 0.000300\n",
      "2021-08-03 21:50:06,238 - INFO - joeynmt.training - Epoch   7, Step:   138400, Batch Loss:     1.968541, Tokens per Sec:    16451, Lr: 0.000300\n",
      "2021-08-03 21:50:32,399 - INFO - joeynmt.training - Epoch   7, Step:   138600, Batch Loss:     1.884246, Tokens per Sec:    16533, Lr: 0.000300\n",
      "2021-08-03 21:50:58,343 - INFO - joeynmt.training - Epoch   7, Step:   138800, Batch Loss:     1.928937, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-03 21:51:24,814 - INFO - joeynmt.training - Epoch   7, Step:   139000, Batch Loss:     1.911805, Tokens per Sec:    16457, Lr: 0.000300\n",
      "2021-08-03 21:51:50,922 - INFO - joeynmt.training - Epoch   7, Step:   139200, Batch Loss:     1.947693, Tokens per Sec:    16620, Lr: 0.000300\n",
      "2021-08-03 21:52:17,278 - INFO - joeynmt.training - Epoch   7, Step:   139400, Batch Loss:     1.783365, Tokens per Sec:    16244, Lr: 0.000300\n",
      "2021-08-03 21:52:43,533 - INFO - joeynmt.training - Epoch   7, Step:   139600, Batch Loss:     1.403991, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-08-03 21:53:09,700 - INFO - joeynmt.training - Epoch   7, Step:   139800, Batch Loss:     1.911497, Tokens per Sec:    16353, Lr: 0.000300\n",
      "2021-08-03 21:53:35,778 - INFO - joeynmt.training - Epoch   7, Step:   140000, Batch Loss:     1.651776, Tokens per Sec:    16641, Lr: 0.000300\n",
      "2021-08-03 21:54:30,704 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:54:30,705 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:54:30,705 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:54:31,313 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:54:31,314 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:54:32,033 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:54:32,033 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:54:32,033 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:54:32,034 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:54:32,034 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:54:32,034 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:54:32,034 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:54:32,034 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 21:54:32,035 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:54:32,035 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:54:32,036 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:54:32,036 - INFO - joeynmt.training - \tHypothesis: Why do false gods futile things ?\n",
      "2021-08-03 21:54:32,036 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:54:32,037 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:54:32,037 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:54:32,037 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:54:32,037 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   140000: bleu:  26.04, loss: 107042.2734, ppl:   5.4402, duration: 56.2590s\n",
      "2021-08-03 21:54:58,836 - INFO - joeynmt.training - Epoch   7, Step:   140200, Batch Loss:     1.805808, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-08-03 21:55:25,067 - INFO - joeynmt.training - Epoch   7, Step:   140400, Batch Loss:     1.707354, Tokens per Sec:    16332, Lr: 0.000300\n",
      "2021-08-03 21:55:50,816 - INFO - joeynmt.training - Epoch   7, Step:   140600, Batch Loss:     1.765666, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-08-03 21:56:08,054 - INFO - joeynmt.training - Epoch   7: total training loss 5223.46\n",
      "2021-08-03 21:56:08,055 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-08-03 21:56:17,602 - INFO - joeynmt.training - Epoch   8, Step:   140800, Batch Loss:     2.185072, Tokens per Sec:    15665, Lr: 0.000300\n",
      "2021-08-03 21:56:43,716 - INFO - joeynmt.training - Epoch   8, Step:   141000, Batch Loss:     2.129261, Tokens per Sec:    16526, Lr: 0.000300\n",
      "2021-08-03 21:57:09,864 - INFO - joeynmt.training - Epoch   8, Step:   141200, Batch Loss:     1.679473, Tokens per Sec:    16339, Lr: 0.000300\n",
      "2021-08-03 21:57:36,005 - INFO - joeynmt.training - Epoch   8, Step:   141400, Batch Loss:     2.018207, Tokens per Sec:    16359, Lr: 0.000300\n",
      "2021-08-03 21:58:02,203 - INFO - joeynmt.training - Epoch   8, Step:   141600, Batch Loss:     1.957709, Tokens per Sec:    16388, Lr: 0.000300\n",
      "2021-08-03 21:58:28,732 - INFO - joeynmt.training - Epoch   8, Step:   141800, Batch Loss:     1.906065, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-08-03 21:58:55,240 - INFO - joeynmt.training - Epoch   8, Step:   142000, Batch Loss:     1.651747, Tokens per Sec:    16465, Lr: 0.000300\n",
      "2021-08-03 21:59:48,447 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 21:59:48,447 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 21:59:48,447 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 21:59:49,102 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 21:59:49,102 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 21:59:49,807 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 21:59:49,808 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 21:59:49,809 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 21:59:49,809 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 21:59:49,809 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 21:59:49,810 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 21:59:49,812 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 21:59:49,812 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to read the Scriptures when possible .\n",
      "2021-08-03 21:59:49,813 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 21:59:49,813 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 21:59:49,813 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 21:59:49,813 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 21:59:49,814 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 21:59:49,814 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 21:59:49,814 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 21:59:49,814 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 21:59:49,815 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   142000: bleu:  26.05, loss: 106838.8125, ppl:   5.4227, duration: 54.5745s\n",
      "2021-08-03 22:00:16,550 - INFO - joeynmt.training - Epoch   8, Step:   142200, Batch Loss:     1.824981, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-08-03 22:00:42,495 - INFO - joeynmt.training - Epoch   8, Step:   142400, Batch Loss:     2.066759, Tokens per Sec:    16292, Lr: 0.000300\n",
      "2021-08-03 22:01:08,840 - INFO - joeynmt.training - Epoch   8, Step:   142600, Batch Loss:     1.886569, Tokens per Sec:    16147, Lr: 0.000300\n",
      "2021-08-03 22:01:35,020 - INFO - joeynmt.training - Epoch   8, Step:   142800, Batch Loss:     1.902909, Tokens per Sec:    16507, Lr: 0.000300\n",
      "2021-08-03 22:02:01,412 - INFO - joeynmt.training - Epoch   8, Step:   143000, Batch Loss:     1.793274, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-08-03 22:02:27,684 - INFO - joeynmt.training - Epoch   8, Step:   143200, Batch Loss:     1.944178, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-08-03 22:02:53,898 - INFO - joeynmt.training - Epoch   8, Step:   143400, Batch Loss:     1.828000, Tokens per Sec:    16320, Lr: 0.000300\n",
      "2021-08-03 22:03:19,832 - INFO - joeynmt.training - Epoch   8: total training loss 5212.12\n",
      "2021-08-03 22:03:19,832 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-08-03 22:03:20,540 - INFO - joeynmt.training - Epoch   9, Step:   143600, Batch Loss:     1.772167, Tokens per Sec:     9092, Lr: 0.000300\n",
      "2021-08-03 22:03:46,832 - INFO - joeynmt.training - Epoch   9, Step:   143800, Batch Loss:     1.927177, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-08-03 22:04:13,043 - INFO - joeynmt.training - Epoch   9, Step:   144000, Batch Loss:     1.823461, Tokens per Sec:    16448, Lr: 0.000300\n",
      "2021-08-03 22:05:08,591 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:05:08,592 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:05:08,592 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:05:09,862 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:05:09,862 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:05:09,863 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:05:09,863 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:05:09,863 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:05:09,863 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:05:09,863 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:05:09,864 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and strive to read the Scriptures when possible .\n",
      "2021-08-03 22:05:09,864 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:05:09,864 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:05:09,864 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:05:09,864 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 22:05:09,864 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:05:09,865 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:05:09,865 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:05:09,865 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:05:09,865 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   144000: bleu:  26.41, loss: 106884.3516, ppl:   5.4266, duration: 56.8220s\n",
      "2021-08-03 22:05:36,212 - INFO - joeynmt.training - Epoch   9, Step:   144200, Batch Loss:     1.241369, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-03 22:06:02,501 - INFO - joeynmt.training - Epoch   9, Step:   144400, Batch Loss:     2.004608, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-08-03 22:06:28,681 - INFO - joeynmt.training - Epoch   9, Step:   144600, Batch Loss:     1.665729, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-08-03 22:06:54,986 - INFO - joeynmt.training - Epoch   9, Step:   144800, Batch Loss:     1.765162, Tokens per Sec:    16442, Lr: 0.000300\n",
      "2021-08-03 22:07:21,603 - INFO - joeynmt.training - Epoch   9, Step:   145000, Batch Loss:     1.908125, Tokens per Sec:    16303, Lr: 0.000300\n",
      "2021-08-03 22:07:47,708 - INFO - joeynmt.training - Epoch   9, Step:   145200, Batch Loss:     1.726377, Tokens per Sec:    16337, Lr: 0.000300\n",
      "2021-08-03 22:08:13,925 - INFO - joeynmt.training - Epoch   9, Step:   145400, Batch Loss:     1.825757, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-03 22:08:40,203 - INFO - joeynmt.training - Epoch   9, Step:   145600, Batch Loss:     1.756021, Tokens per Sec:    16464, Lr: 0.000300\n",
      "2021-08-03 22:09:06,389 - INFO - joeynmt.training - Epoch   9, Step:   145800, Batch Loss:     1.325521, Tokens per Sec:    16390, Lr: 0.000300\n",
      "2021-08-03 22:09:32,655 - INFO - joeynmt.training - Epoch   9, Step:   146000, Batch Loss:     1.669817, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-08-03 22:10:29,149 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:10:29,149 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:10:29,149 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:10:29,763 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 22:10:29,763 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 22:10:30,492 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:10:30,493 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:10:30,493 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:10:30,493 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:10:30,493 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:10:30,494 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:10:30,494 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:10:30,494 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to read the Scriptures when possible .\n",
      "2021-08-03 22:10:30,494 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:10:30,495 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:10:30,495 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:10:30,495 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 22:10:30,495 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:10:30,496 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:10:30,496 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:10:30,496 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:10:30,497 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   146000: bleu:  26.31, loss: 106411.2500, ppl:   5.3861, duration: 57.8409s\n",
      "2021-08-03 22:10:57,376 - INFO - joeynmt.training - Epoch   9, Step:   146200, Batch Loss:     1.910654, Tokens per Sec:    16083, Lr: 0.000300\n",
      "2021-08-03 22:11:23,767 - INFO - joeynmt.training - Epoch   9, Step:   146400, Batch Loss:     2.012855, Tokens per Sec:    16407, Lr: 0.000300\n",
      "2021-08-03 22:11:32,131 - INFO - joeynmt.training - Epoch   9: total training loss 5200.39\n",
      "2021-08-03 22:11:32,132 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-08-03 22:11:50,031 - INFO - joeynmt.training - Epoch  10, Step:   146600, Batch Loss:     1.986761, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-08-03 22:12:16,317 - INFO - joeynmt.training - Epoch  10, Step:   146800, Batch Loss:     1.887537, Tokens per Sec:    16124, Lr: 0.000300\n",
      "2021-08-03 22:12:42,550 - INFO - joeynmt.training - Epoch  10, Step:   147000, Batch Loss:     1.872811, Tokens per Sec:    16509, Lr: 0.000300\n",
      "2021-08-03 22:13:08,956 - INFO - joeynmt.training - Epoch  10, Step:   147200, Batch Loss:     1.895130, Tokens per Sec:    16316, Lr: 0.000300\n",
      "2021-08-03 22:13:35,151 - INFO - joeynmt.training - Epoch  10, Step:   147400, Batch Loss:     1.911656, Tokens per Sec:    16326, Lr: 0.000300\n",
      "2021-08-03 22:14:01,296 - INFO - joeynmt.training - Epoch  10, Step:   147600, Batch Loss:     2.014916, Tokens per Sec:    16415, Lr: 0.000300\n",
      "2021-08-03 22:14:27,571 - INFO - joeynmt.training - Epoch  10, Step:   147800, Batch Loss:     2.048261, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-08-03 22:14:53,980 - INFO - joeynmt.training - Epoch  10, Step:   148000, Batch Loss:     1.616710, Tokens per Sec:    16349, Lr: 0.000300\n",
      "2021-08-03 22:15:47,201 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:15:47,202 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:15:47,202 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:15:48,552 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:15:48,553 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:15:48,553 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:15:48,553 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:15:48,553 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:15:48,554 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:15:48,554 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:15:48,554 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures when possible .\n",
      "2021-08-03 22:15:48,554 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:15:48,555 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:15:48,555 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:15:48,555 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 22:15:48,555 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:15:48,557 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:15:48,557 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:15:48,558 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:15:48,558 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   148000: bleu:  26.08, loss: 106602.9062, ppl:   5.4025, duration: 54.5774s\n",
      "2021-08-03 22:16:14,988 - INFO - joeynmt.training - Epoch  10, Step:   148200, Batch Loss:     1.770807, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-08-03 22:16:41,317 - INFO - joeynmt.training - Epoch  10, Step:   148400, Batch Loss:     1.673093, Tokens per Sec:    16604, Lr: 0.000300\n",
      "2021-08-03 22:17:07,615 - INFO - joeynmt.training - Epoch  10, Step:   148600, Batch Loss:     2.012928, Tokens per Sec:    16292, Lr: 0.000300\n",
      "2021-08-03 22:17:33,682 - INFO - joeynmt.training - Epoch  10, Step:   148800, Batch Loss:     1.892059, Tokens per Sec:    16615, Lr: 0.000300\n",
      "2021-08-03 22:17:59,687 - INFO - joeynmt.training - Epoch  10, Step:   149000, Batch Loss:     1.795140, Tokens per Sec:    16258, Lr: 0.000300\n",
      "2021-08-03 22:18:26,070 - INFO - joeynmt.training - Epoch  10, Step:   149200, Batch Loss:     1.826701, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-08-03 22:18:44,086 - INFO - joeynmt.training - Epoch  10: total training loss 5209.35\n",
      "2021-08-03 22:18:44,086 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-08-03 22:18:52,313 - INFO - joeynmt.training - Epoch  11, Step:   149400, Batch Loss:     1.822729, Tokens per Sec:    15906, Lr: 0.000300\n",
      "2021-08-03 22:19:18,690 - INFO - joeynmt.training - Epoch  11, Step:   149600, Batch Loss:     1.808947, Tokens per Sec:    16251, Lr: 0.000300\n",
      "2021-08-03 22:19:44,737 - INFO - joeynmt.training - Epoch  11, Step:   149800, Batch Loss:     2.290301, Tokens per Sec:    16402, Lr: 0.000300\n",
      "2021-08-03 22:20:10,873 - INFO - joeynmt.training - Epoch  11, Step:   150000, Batch Loss:     1.990949, Tokens per Sec:    16332, Lr: 0.000300\n",
      "2021-08-03 22:21:06,550 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:21:06,551 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:21:06,551 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:21:07,891 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:21:07,892 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:21:07,892 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:21:07,892 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:21:07,892 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:21:07,893 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:21:07,893 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:21:07,893 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to read the Scriptures if possible .\n",
      "2021-08-03 22:21:07,893 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:21:07,894 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:21:07,894 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:21:07,894 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 22:21:07,894 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:21:07,895 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:21:07,895 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:21:07,895 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:21:07,895 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   150000: bleu:  26.36, loss: 106413.7109, ppl:   5.3863, duration: 57.0216s\n",
      "2021-08-03 22:21:34,133 - INFO - joeynmt.training - Epoch  11, Step:   150200, Batch Loss:     1.847177, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-08-03 22:22:00,448 - INFO - joeynmt.training - Epoch  11, Step:   150400, Batch Loss:     2.090734, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-08-03 22:22:26,775 - INFO - joeynmt.training - Epoch  11, Step:   150600, Batch Loss:     1.812304, Tokens per Sec:    16325, Lr: 0.000300\n",
      "2021-08-03 22:22:52,856 - INFO - joeynmt.training - Epoch  11, Step:   150800, Batch Loss:     1.187172, Tokens per Sec:    16448, Lr: 0.000300\n",
      "2021-08-03 22:23:18,987 - INFO - joeynmt.training - Epoch  11, Step:   151000, Batch Loss:     2.109317, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-03 22:23:44,963 - INFO - joeynmt.training - Epoch  11, Step:   151200, Batch Loss:     2.008733, Tokens per Sec:    16381, Lr: 0.000300\n",
      "2021-08-03 22:24:11,289 - INFO - joeynmt.training - Epoch  11, Step:   151400, Batch Loss:     1.860604, Tokens per Sec:    16277, Lr: 0.000300\n",
      "2021-08-03 22:24:37,499 - INFO - joeynmt.training - Epoch  11, Step:   151600, Batch Loss:     1.881942, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-08-03 22:25:03,747 - INFO - joeynmt.training - Epoch  11, Step:   151800, Batch Loss:     1.741353, Tokens per Sec:    16314, Lr: 0.000300\n",
      "2021-08-03 22:25:30,204 - INFO - joeynmt.training - Epoch  11, Step:   152000, Batch Loss:     1.669018, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-08-03 22:26:25,288 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:26:25,289 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:26:25,289 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:26:25,895 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 22:26:25,895 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 22:26:26,623 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:26:26,624 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:26:26,624 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:26:26,624 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:26:26,625 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:26:26,625 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:26:26,625 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:26:26,625 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible in our ministry and to read the Scriptures when possible .\n",
      "2021-08-03 22:26:26,625 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:26:26,626 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:26:26,626 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:26:26,626 - INFO - joeynmt.training - \tHypothesis: Why do false gods have unrealistic things ?\n",
      "2021-08-03 22:26:26,626 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:26:26,627 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:26:26,627 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:26:26,627 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:26:26,627 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   152000: bleu:  26.36, loss: 106248.4453, ppl:   5.3723, duration: 56.4227s\n",
      "2021-08-03 22:26:53,074 - INFO - joeynmt.training - Epoch  11, Step:   152200, Batch Loss:     1.632817, Tokens per Sec:    16181, Lr: 0.000300\n",
      "2021-08-03 22:26:55,512 - INFO - joeynmt.training - Epoch  11: total training loss 5204.46\n",
      "2021-08-03 22:26:55,512 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-08-03 22:27:19,739 - INFO - joeynmt.training - Epoch  12, Step:   152400, Batch Loss:     1.605402, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-08-03 22:27:46,062 - INFO - joeynmt.training - Epoch  12, Step:   152600, Batch Loss:     1.881487, Tokens per Sec:    16406, Lr: 0.000300\n",
      "2021-08-03 22:28:12,334 - INFO - joeynmt.training - Epoch  12, Step:   152800, Batch Loss:     1.673365, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-08-03 22:28:38,486 - INFO - joeynmt.training - Epoch  12, Step:   153000, Batch Loss:     1.881302, Tokens per Sec:    16407, Lr: 0.000300\n",
      "2021-08-03 22:29:04,623 - INFO - joeynmt.training - Epoch  12, Step:   153200, Batch Loss:     1.928901, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-08-03 22:29:30,917 - INFO - joeynmt.training - Epoch  12, Step:   153400, Batch Loss:     1.796511, Tokens per Sec:    16314, Lr: 0.000300\n",
      "2021-08-03 22:29:56,995 - INFO - joeynmt.training - Epoch  12, Step:   153600, Batch Loss:     1.782689, Tokens per Sec:    16541, Lr: 0.000300\n",
      "2021-08-03 22:30:23,432 - INFO - joeynmt.training - Epoch  12, Step:   153800, Batch Loss:     1.923069, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-08-03 22:30:49,468 - INFO - joeynmt.training - Epoch  12, Step:   154000, Batch Loss:     1.909245, Tokens per Sec:    16383, Lr: 0.000300\n",
      "2021-08-03 22:31:46,829 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:31:46,830 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:31:46,830 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:31:47,511 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 22:31:47,511 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 22:31:48,283 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:31:48,285 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:31:48,285 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:31:48,285 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:31:48,285 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:31:48,286 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:31:48,286 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:31:48,286 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to read the Scriptures when possible .\n",
      "2021-08-03 22:31:48,287 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:31:48,287 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:31:48,287 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:31:48,287 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 22:31:48,288 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:31:48,289 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:31:48,289 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:31:48,289 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:31:48,290 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   154000: bleu:  26.45, loss: 106175.3594, ppl:   5.3661, duration: 58.8214s\n",
      "2021-08-03 22:32:15,039 - INFO - joeynmt.training - Epoch  12, Step:   154200, Batch Loss:     1.891865, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-08-03 22:32:41,209 - INFO - joeynmt.training - Epoch  12, Step:   154400, Batch Loss:     1.751739, Tokens per Sec:    16384, Lr: 0.000300\n",
      "2021-08-03 22:33:07,599 - INFO - joeynmt.training - Epoch  12, Step:   154600, Batch Loss:     1.691584, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-08-03 22:33:33,915 - INFO - joeynmt.training - Epoch  12, Step:   154800, Batch Loss:     1.693350, Tokens per Sec:    16659, Lr: 0.000300\n",
      "2021-08-03 22:33:59,945 - INFO - joeynmt.training - Epoch  12, Step:   155000, Batch Loss:     1.839723, Tokens per Sec:    16292, Lr: 0.000300\n",
      "2021-08-03 22:34:12,412 - INFO - joeynmt.training - Epoch  12: total training loss 5189.93\n",
      "2021-08-03 22:34:12,412 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-08-03 22:34:26,706 - INFO - joeynmt.training - Epoch  13, Step:   155200, Batch Loss:     1.812692, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-08-03 22:34:52,830 - INFO - joeynmt.training - Epoch  13, Step:   155400, Batch Loss:     1.824655, Tokens per Sec:    16456, Lr: 0.000300\n",
      "2021-08-03 22:35:19,330 - INFO - joeynmt.training - Epoch  13, Step:   155600, Batch Loss:     1.971860, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-08-03 22:35:45,454 - INFO - joeynmt.training - Epoch  13, Step:   155800, Batch Loss:     1.836444, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-08-03 22:36:11,606 - INFO - joeynmt.training - Epoch  13, Step:   156000, Batch Loss:     1.712360, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-08-03 22:37:07,449 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:37:07,449 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:37:07,449 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:37:08,078 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 22:37:08,078 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 22:37:08,764 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:37:08,765 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:37:08,765 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:37:08,765 - INFO - joeynmt.training - \tHypothesis: When we examine how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:37:08,765 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:37:08,766 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:37:08,766 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:37:08,766 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to study the Scriptures when possible .\n",
      "2021-08-03 22:37:08,766 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:37:08,767 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:37:08,767 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:37:08,767 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 22:37:08,767 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:37:08,768 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:37:08,768 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:37:08,768 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:37:08,768 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   156000: bleu:  26.58, loss: 105833.6641, ppl:   5.3371, duration: 57.1614s\n",
      "2021-08-03 22:37:35,279 - INFO - joeynmt.training - Epoch  13, Step:   156200, Batch Loss:     1.680960, Tokens per Sec:    16421, Lr: 0.000300\n",
      "2021-08-03 22:38:01,748 - INFO - joeynmt.training - Epoch  13, Step:   156400, Batch Loss:     1.728764, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-08-03 22:38:28,018 - INFO - joeynmt.training - Epoch  13, Step:   156600, Batch Loss:     2.048675, Tokens per Sec:    16487, Lr: 0.000300\n",
      "2021-08-03 22:38:54,367 - INFO - joeynmt.training - Epoch  13, Step:   156800, Batch Loss:     1.855140, Tokens per Sec:    16669, Lr: 0.000300\n",
      "2021-08-03 22:39:20,715 - INFO - joeynmt.training - Epoch  13, Step:   157000, Batch Loss:     2.136136, Tokens per Sec:    16345, Lr: 0.000300\n",
      "2021-08-03 22:39:46,951 - INFO - joeynmt.training - Epoch  13, Step:   157200, Batch Loss:     1.735771, Tokens per Sec:    16339, Lr: 0.000300\n",
      "2021-08-03 22:40:13,364 - INFO - joeynmt.training - Epoch  13, Step:   157400, Batch Loss:     1.968183, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-08-03 22:40:39,446 - INFO - joeynmt.training - Epoch  13, Step:   157600, Batch Loss:     1.657146, Tokens per Sec:    16324, Lr: 0.000300\n",
      "2021-08-03 22:41:06,026 - INFO - joeynmt.training - Epoch  13, Step:   157800, Batch Loss:     1.975614, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-08-03 22:41:26,781 - INFO - joeynmt.training - Epoch  13: total training loss 5149.12\n",
      "2021-08-03 22:41:26,781 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-08-03 22:41:33,041 - INFO - joeynmt.training - Epoch  14, Step:   158000, Batch Loss:     1.810629, Tokens per Sec:    15059, Lr: 0.000300\n",
      "2021-08-03 22:42:28,813 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:42:28,814 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:42:28,814 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:42:30,108 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:42:30,109 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:42:30,109 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:42:30,109 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:42:30,109 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:42:30,110 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:42:30,110 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:42:30,110 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to study the Scriptures if possible .\n",
      "2021-08-03 22:42:30,110 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:42:30,111 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:42:30,111 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:42:30,111 - INFO - joeynmt.training - \tHypothesis: Why do false gods futile things ?\n",
      "2021-08-03 22:42:30,111 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:42:30,111 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:42:30,112 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:42:30,112 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:42:30,112 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   158000: bleu:  26.51, loss: 106055.1953, ppl:   5.3559, duration: 57.0705s\n",
      "2021-08-03 22:42:56,827 - INFO - joeynmt.training - Epoch  14, Step:   158200, Batch Loss:     1.674931, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-08-03 22:43:23,301 - INFO - joeynmt.training - Epoch  14, Step:   158400, Batch Loss:     2.041836, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-08-03 22:43:49,865 - INFO - joeynmt.training - Epoch  14, Step:   158600, Batch Loss:     1.916755, Tokens per Sec:    16214, Lr: 0.000300\n",
      "2021-08-03 22:44:16,599 - INFO - joeynmt.training - Epoch  14, Step:   158800, Batch Loss:     1.415155, Tokens per Sec:    16260, Lr: 0.000300\n",
      "2021-08-03 22:44:42,826 - INFO - joeynmt.training - Epoch  14, Step:   159000, Batch Loss:     1.789582, Tokens per Sec:    16566, Lr: 0.000300\n",
      "2021-08-03 22:45:09,312 - INFO - joeynmt.training - Epoch  14, Step:   159200, Batch Loss:     1.863483, Tokens per Sec:    16333, Lr: 0.000300\n",
      "2021-08-03 22:45:35,509 - INFO - joeynmt.training - Epoch  14, Step:   159400, Batch Loss:     1.788127, Tokens per Sec:    16219, Lr: 0.000300\n",
      "2021-08-03 22:46:01,714 - INFO - joeynmt.training - Epoch  14, Step:   159600, Batch Loss:     1.844033, Tokens per Sec:    16378, Lr: 0.000300\n",
      "2021-08-03 22:46:28,215 - INFO - joeynmt.training - Epoch  14, Step:   159800, Batch Loss:     1.888200, Tokens per Sec:    16220, Lr: 0.000300\n",
      "2021-08-03 22:46:54,429 - INFO - joeynmt.training - Epoch  14, Step:   160000, Batch Loss:     1.710334, Tokens per Sec:    16481, Lr: 0.000300\n",
      "2021-08-03 22:47:50,533 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:47:50,533 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:47:50,534 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:47:51,198 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 22:47:51,198 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 22:47:51,965 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:47:51,965 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:47:51,966 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:47:51,966 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:47:51,966 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:47:51,966 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:47:51,967 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:47:51,967 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our preaching and to study the Scriptures as possible .\n",
      "2021-08-03 22:47:51,967 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:47:51,967 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:47:51,968 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:47:51,968 - INFO - joeynmt.training - \tHypothesis: Why are false gods unrealistic ?\n",
      "2021-08-03 22:47:51,968 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:47:51,968 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:47:51,968 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:47:51,969 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:47:51,970 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   160000: bleu:  26.54, loss: 105715.3359, ppl:   5.3272, duration: 57.5402s\n",
      "2021-08-03 22:48:18,605 - INFO - joeynmt.training - Epoch  14, Step:   160200, Batch Loss:     1.970443, Tokens per Sec:    16412, Lr: 0.000300\n",
      "2021-08-03 22:48:44,909 - INFO - joeynmt.training - Epoch  14, Step:   160400, Batch Loss:     1.818333, Tokens per Sec:    16471, Lr: 0.000300\n",
      "2021-08-03 22:49:11,365 - INFO - joeynmt.training - Epoch  14, Step:   160600, Batch Loss:     1.367059, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-08-03 22:49:37,471 - INFO - joeynmt.training - Epoch  14, Step:   160800, Batch Loss:     1.762288, Tokens per Sec:    16231, Lr: 0.000300\n",
      "2021-08-03 22:49:39,779 - INFO - joeynmt.training - Epoch  14: total training loss 5147.54\n",
      "2021-08-03 22:49:39,780 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-08-03 22:50:04,263 - INFO - joeynmt.training - Epoch  15, Step:   161000, Batch Loss:     1.909829, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-08-03 22:50:30,532 - INFO - joeynmt.training - Epoch  15, Step:   161200, Batch Loss:     1.794652, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-08-03 22:50:56,737 - INFO - joeynmt.training - Epoch  15, Step:   161400, Batch Loss:     1.830919, Tokens per Sec:    16397, Lr: 0.000300\n",
      "2021-08-03 22:51:23,186 - INFO - joeynmt.training - Epoch  15, Step:   161600, Batch Loss:     1.789968, Tokens per Sec:    16156, Lr: 0.000300\n",
      "2021-08-03 22:51:49,491 - INFO - joeynmt.training - Epoch  15, Step:   161800, Batch Loss:     1.602324, Tokens per Sec:    16275, Lr: 0.000300\n",
      "2021-08-03 22:52:15,768 - INFO - joeynmt.training - Epoch  15, Step:   162000, Batch Loss:     1.739986, Tokens per Sec:    16302, Lr: 0.000300\n",
      "2021-08-03 22:53:13,246 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:53:13,246 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:53:13,246 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:53:14,914 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:53:14,914 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:53:14,914 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:53:14,915 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:53:14,915 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:53:14,915 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:53:14,915 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:53:14,915 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible with our ministry and to read the Scriptures when possible .\n",
      "2021-08-03 22:53:14,916 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:53:14,916 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:53:14,916 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:53:14,916 - INFO - joeynmt.training - \tHypothesis: Why do false gods futile things ?\n",
      "2021-08-03 22:53:14,916 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:53:14,917 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:53:14,917 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:53:14,917 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:53:14,917 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   162000: bleu:  26.57, loss: 105923.9922, ppl:   5.3448, duration: 59.1488s\n",
      "2021-08-03 22:53:41,487 - INFO - joeynmt.training - Epoch  15, Step:   162200, Batch Loss:     1.874062, Tokens per Sec:    16311, Lr: 0.000300\n",
      "2021-08-03 22:54:07,875 - INFO - joeynmt.training - Epoch  15, Step:   162400, Batch Loss:     1.602963, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-08-03 22:54:34,086 - INFO - joeynmt.training - Epoch  15, Step:   162600, Batch Loss:     1.972818, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-08-03 22:55:00,448 - INFO - joeynmt.training - Epoch  15, Step:   162800, Batch Loss:     1.838421, Tokens per Sec:    16309, Lr: 0.000300\n",
      "2021-08-03 22:55:26,556 - INFO - joeynmt.training - Epoch  15, Step:   163000, Batch Loss:     1.572590, Tokens per Sec:    16302, Lr: 0.000300\n",
      "2021-08-03 22:55:52,879 - INFO - joeynmt.training - Epoch  15, Step:   163200, Batch Loss:     1.978272, Tokens per Sec:    16489, Lr: 0.000300\n",
      "2021-08-03 22:56:19,133 - INFO - joeynmt.training - Epoch  15, Step:   163400, Batch Loss:     1.786705, Tokens per Sec:    16348, Lr: 0.000300\n",
      "2021-08-03 22:56:45,535 - INFO - joeynmt.training - Epoch  15, Step:   163600, Batch Loss:     1.731496, Tokens per Sec:    16511, Lr: 0.000300\n",
      "2021-08-03 22:56:56,522 - INFO - joeynmt.training - Epoch  15: total training loss 5143.68\n",
      "2021-08-03 22:56:56,522 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-08-03 22:57:12,249 - INFO - joeynmt.training - Epoch  16, Step:   163800, Batch Loss:     1.792435, Tokens per Sec:    15754, Lr: 0.000300\n",
      "2021-08-03 22:57:38,631 - INFO - joeynmt.training - Epoch  16, Step:   164000, Batch Loss:     1.745482, Tokens per Sec:    16490, Lr: 0.000300\n",
      "2021-08-03 22:58:34,138 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 22:58:34,139 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 22:58:34,139 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 22:58:35,501 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 22:58:35,502 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 22:58:35,502 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 22:58:35,502 - INFO - joeynmt.training - \tHypothesis: When we consider how he displayed faith and imitate him , he is speaking to us .\n",
      "2021-08-03 22:58:35,502 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 22:58:35,503 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 22:58:35,503 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 22:58:35,503 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we need to use the Bible with our ministry and to read the Scriptures when possible .\n",
      "2021-08-03 22:58:35,503 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 22:58:35,504 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 22:58:35,504 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 22:58:35,504 - INFO - joeynmt.training - \tHypothesis: Why are false gods unvalueless ?\n",
      "2021-08-03 22:58:35,504 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 22:58:35,505 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 22:58:35,505 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 22:58:35,505 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 22:58:35,505 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   164000: bleu:  26.52, loss: 105890.3516, ppl:   5.3419, duration: 56.8737s\n",
      "2021-08-03 22:59:02,141 - INFO - joeynmt.training - Epoch  16, Step:   164200, Batch Loss:     2.014157, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-08-03 22:59:28,520 - INFO - joeynmt.training - Epoch  16, Step:   164400, Batch Loss:     1.827041, Tokens per Sec:    16314, Lr: 0.000300\n",
      "2021-08-03 22:59:54,842 - INFO - joeynmt.training - Epoch  16, Step:   164600, Batch Loss:     1.639144, Tokens per Sec:    16351, Lr: 0.000300\n",
      "2021-08-03 23:00:20,943 - INFO - joeynmt.training - Epoch  16, Step:   164800, Batch Loss:     1.869120, Tokens per Sec:    16299, Lr: 0.000300\n",
      "2021-08-03 23:00:47,084 - INFO - joeynmt.training - Epoch  16, Step:   165000, Batch Loss:     1.920416, Tokens per Sec:    16414, Lr: 0.000300\n",
      "2021-08-03 23:01:13,767 - INFO - joeynmt.training - Epoch  16, Step:   165200, Batch Loss:     1.573626, Tokens per Sec:    16379, Lr: 0.000300\n",
      "2021-08-03 23:01:39,868 - INFO - joeynmt.training - Epoch  16, Step:   165400, Batch Loss:     1.829399, Tokens per Sec:    16169, Lr: 0.000300\n",
      "2021-08-03 23:02:06,273 - INFO - joeynmt.training - Epoch  16, Step:   165600, Batch Loss:     1.877233, Tokens per Sec:    16587, Lr: 0.000300\n",
      "2021-08-03 23:02:32,271 - INFO - joeynmt.training - Epoch  16, Step:   165800, Batch Loss:     1.981257, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-08-03 23:02:58,396 - INFO - joeynmt.training - Epoch  16, Step:   166000, Batch Loss:     1.828788, Tokens per Sec:    16498, Lr: 0.000300\n",
      "2021-08-03 23:03:55,358 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 23:03:55,359 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 23:03:55,359 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 23:03:56,034 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-08-03 23:03:56,034 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-08-03 23:03:57,184 - INFO - joeynmt.training - Example #0\n",
      "2021-08-03 23:03:57,185 - INFO - joeynmt.training - \tSource:     Bwe twekenneenya engeri gye yayolekamu okukkiriza era ne tumukoppa , aba ng’ayogera naffe .\n",
      "2021-08-03 23:03:57,185 - INFO - joeynmt.training - \tReference:  If we learn from his faith and seek to imitate it , then the record of Abel is speaking to us in a very real and effective way .\n",
      "2021-08-03 23:03:57,185 - INFO - joeynmt.training - \tHypothesis: When we examine his faith and imitate him , he is speaking to us .\n",
      "2021-08-03 23:03:57,185 - INFO - joeynmt.training - Example #1\n",
      "2021-08-03 23:03:57,186 - INFO - joeynmt.training - \tSource:     Okufaananako Yesu , tulina okukozesa ennyo Baibuli nga tubuulira n’okufuba okusomera abantu Ebyawandiikibwa bwe kiba kisoboka .\n",
      "2021-08-03 23:03:57,186 - INFO - joeynmt.training - \tReference:  Like Jesus , we rely heavily on the Bible in our ministry and endeavor to feature the Scriptures whenever possible .\n",
      "2021-08-03 23:03:57,186 - INFO - joeynmt.training - \tHypothesis: Like Jesus , we must use the Bible to preach and to read the Scriptures when possible .\n",
      "2021-08-03 23:03:57,186 - INFO - joeynmt.training - Example #2\n",
      "2021-08-03 23:03:57,187 - INFO - joeynmt.training - \tSource:     Lwaki bakatonda ab’obulimba bintu ebitaliimu ?\n",
      "2021-08-03 23:03:57,187 - INFO - joeynmt.training - \tReference:  Why are false gods valueless ?\n",
      "2021-08-03 23:03:57,187 - INFO - joeynmt.training - \tHypothesis: Why do false gods ?\n",
      "2021-08-03 23:03:57,187 - INFO - joeynmt.training - Example #3\n",
      "2021-08-03 23:03:57,188 - INFO - joeynmt.training - \tSource:     Kyo kituufu nti Yakuwa akolagana n’abantu be mu ngeri ey’okwagala .\n",
      "2021-08-03 23:03:57,188 - INFO - joeynmt.training - \tReference:  We can have confidence that Jehovah always deals with his people in a loving way .\n",
      "2021-08-03 23:03:57,188 - INFO - joeynmt.training - \tHypothesis: True , Jehovah is dealing with his people in a loving way .\n",
      "2021-08-03 23:03:57,188 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   166000: bleu:  26.56, loss: 105692.0781, ppl:   5.3252, duration: 58.7913s\n",
      "2021-08-03 23:04:23,578 - INFO - joeynmt.training - Epoch  16, Step:   166200, Batch Loss:     1.664796, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-08-03 23:04:49,679 - INFO - joeynmt.training - Epoch  16, Step:   166400, Batch Loss:     1.892018, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-08-03 23:05:10,912 - INFO - joeynmt.training - Epoch  16: total training loss 5151.29\n",
      "2021-08-03 23:05:10,913 - INFO - joeynmt.training - Training ended after  16 epochs.\n",
      "2021-08-03 23:05:10,913 - INFO - joeynmt.training - Best validation result (greedy) at step   166000:   5.33 ppl.\n",
      "2021-08-03 23:05:10,934 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
      "2021-08-03 23:05:11,298 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 23:05:11,488 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-08-03 23:05:11,555 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe.en)...\n",
      "2021-08-03 23:06:12,108 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-08-03 23:06:12,108 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-08-03 23:06:12,109 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-08-03 23:06:12,726 - INFO - joeynmt.prediction -  dev bleu[13a]:  27.06 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-03 23:06:12,732 - INFO - joeynmt.prediction - Translations saved to: models/lg_lhen_reverse_transformer_continued4/00166000.hyps.dev\n",
      "2021-08-03 23:06:12,732 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.en)...\n",
      "2021-08-03 23:06:15,671 - INFO - joeynmt.prediction - test bleu[13a]:  13.66 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-08-03 23:06:15,675 - INFO - joeynmt.prediction - Translations saved to: models/lg_lhen_reverse_transformer_continued4/00166000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_lhen_reload4.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nL9G7wtudqIC",
    "outputId": "66b98775-cd16-4cc8-d1e0-f12b17266072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 23:23:07,134 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 23:23:11,402 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 23:23:11,614 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/lg_lhen_reverse_transformer_continued4/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/translation2.bpe.lh_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIw66TiSfuYs",
    "outputId": "32392d9d-bf93-4f9a-c22b-4abd8204f801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 13.7 45.8/18.9/9.5/5.3 (BP = 0.949 ratio = 0.950 hyp_len = 1913 ref_len = 2013)\n"
     ]
    }
   ],
   "source": [
    "# Luhya BLEU Score\n",
    "!cat \"translation2.bpe.lh_en\" | sacrebleu \"test1.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uz6Le20zfNI-",
    "outputId": "f8cbdb2f-0e3b-4809-ddd9-80b3385343dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-03 23:23:55,132 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-08-03 23:23:59,342 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-08-03 23:23:59,554 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/lg_lhen_reverse_transformer_continued4/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.lg\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/translation2.bpe.lg_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5hJip2Rg7VL",
    "outputId": "81e72636-688f-4d18-9856-5ef3e3dcbca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
      "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 27.8 59.6/35.6/24.7/18.1 (BP = 0.892 ratio = 0.897 hyp_len = 38353 ref_len = 42737)\n"
     ]
    }
   ],
   "source": [
    "# Luganda BLEU Score\n",
    "!cat \"translation2.bpe.lg_en\" | sacrebleu \"test2.en\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LugandaLuhya_Multilingual_NMT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
