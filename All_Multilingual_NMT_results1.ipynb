{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "All_Multilingual_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZVkNA2C5Yv4z"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EverlynAsiko/Neural_Machine_Translation_for_African_Languages/blob/main/All_Multilingual_NMT_results1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6cJZzBRP1-N"
      },
      "source": [
        "# Multilingual neural machine translation.\n",
        "\n",
        "This is the utilization of more than one pair of language in machine translation. Essentially, one can use several languages pairs of languages in one model to either perform many-to-one, one-to-many or many-to-many translation.\n",
        "\n",
        "For this case, we shall to a many-to-one translation:\n",
        "{Kinyarwanda, Luganda, Luhya} to English. With this model, we do not need any type of special tagging but we concatenate the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4V-O3nJPsAA",
        "outputId": "147de3ce-a8e1-4205-ae1c-afee387028e4"
      },
      "source": [
        "# Linking to drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcRP_CqbRQzj"
      },
      "source": [
        "# Importing needed libraries for preprocessing and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "collapsed": true,
        "id": "grB3V9FhReiZ",
        "outputId": "277b115f-22e2-46b7-b80d-fffb56c40a6e"
      },
      "source": [
        "#@title Default title text\n",
        "# Install Pytorch with GPU support v1.8.0.\n",
        "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 763.5 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7jAsiRLRlMs"
      },
      "source": [
        "# Filtering warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaH6F-u3RrAb"
      },
      "source": [
        "# Loading the drive\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH_IYggrTJKa"
      },
      "source": [
        "# Setting source and target languages\n",
        "source_language = \"en\"\n",
        "target_language = \"lg_rw_lh\"\n",
        "\n",
        "os.environ[\"src\"] = source_language \n",
        "os.environ[\"tgt\"] = target_language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G0mZmUETh-A"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VI9NWxchThje",
        "outputId": "34c569c8-238b-4906-8eb4-ba74f4779160"
      },
      "source": [
        "! head Luganda/train.*\n",
        "! head Luganda/dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Luganda/train.bpe.en <==\n",
            "Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
            "At last , I have the st@@ able family life that I always cr@@ av@@ ed , and I have the loving Father that I always wanted .\n",
            "I was a new husband , only 25 years old and very in@@ experienced , but off we went with confidence in Jehovah .\n",
            "What can you do to show these de@@ a@@ f brothers personal attention ?\n",
            "R@@ ef@@ er@@ r@@ ing to what the rul@@ er@@ ship of God’s Son will accompl@@ ish , Isaiah 9 : 7 says : “ The very z@@ eal of Jehovah of arm@@ ies will do this . ”\n",
            "Jesus is the m@@ igh@@ ti@@ est of all of Jehovah’s spirit sons .\n",
            "The ste@@ ad@@ f@@ ast example set by J@@ ac@@ o@@ b and R@@ ac@@ he@@ l no doubt had a powerful effect on their son Joseph , influ@@ enc@@ ing how he would hand@@ le t@@ ests of his own faith .\n",
            "When s@@ ent@@ enc@@ ing “ the orig@@ in@@ al ser@@ p@@ ent , ” Satan the Devil , God said : “ I shall put en@@ m@@ ity between you and the woman and between your se@@ ed and her se@@ ed . He will br@@ u@@ ise you in the head and you will br@@ u@@ ise him in the h@@ ee@@ l . ”\n",
            "Will this or@@ de@@ al bring David down to S@@ he@@ ol in g@@ ri@@ ef and dis@@ gr@@ ace ?\n",
            "How can Christian love help to strengthen the marriage b@@ ond ?\n",
            "\n",
            "==> Luganda/train.bpe.lg <==\n",
            "Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
            "N@@ ze ne mukyala wange tuli basanyufu , era nk@@ imanyi nti Katonda anj@@ agala nnyo .\n",
            "Mu kiseera ekyo , nnali nn@@ aak@@ aw@@ asa , nga ndi wa myaka 25 gy@@ okka , era nga s@@ ir@@ ina b@@ um@@ any@@ irivu . Naye nnali muk@@ akafu nti Yakuwa yali ajja ku@@ nn@@ yamba .\n",
            "Mu kibiina k@@ yo bwe mu@@ ba nga mulimu bak@@ igg@@ ala , oyinza kukola ki okulaga nti o@@ faayo ku b’@@ oluganda abo ?\n",
            "Isaaya 9 : 7 wal@@ aga nti Omwana wa Katonda y@@ andibadde Kabaka era nti yand@@ ikol@@ edde abantu ebintu ebirungi bingi .\n",
            "Yesu y’@@ as@@ inga obuyinza mu ba@@ ana ba Yakuwa bonna ab’@@ omwoyo .\n",
            "Eky@@ okulabirako ekirungi Yakobo ne L@@ aak@@ e@@ eri kye baat@@ ek@@ awo mu kw@@ oleka obug@@ umiikiriza ky@@ akwata nnyo ku mutabani waabwe Yusufu , era ekyo ky@@ amu@@ yamba nnyo bwe yay@@ olekagana n’@@ embeera ez@@ aag@@ ez@@ esa okukkiriza kwe .\n",
            "Bwe yali as@@ alira omusango “ omus@@ ot@@ a ogw’@@ edda , ” Setaani Omulyolyomi , Katonda yagamba : “ O@@ bul@@ abe n’@@ abu@@ teek@@ anga wakati wo n’@@ omukazi , era ne wakati w’@@ ez@@ zadde l@@ yo n’@@ ez@@ zadde ly’@@ omukazi : ( ez@@ zadde ly’@@ omukazi ) l@@ iri@@ ku@@ be@@ t@@ ent@@ a omutwe , naawe ol@@ ir@@ ib@@ et@@ ent@@ a ekis@@ inzi@@ iro . ”\n",
            "Em@@ beera eno en@@ zibu en@@ e@@ er@@ eetera Dawudi okuk@@ ka em@@ ag@@ om@@ be nga mun@@ aku@@ w@@ avu ?\n",
            "Okwagala kw’@@ Ekikristaayo ku@@ yinza kutya okuny@@ weza obufumbo ?\n",
            "\n",
            "==> Luganda/train.en <==\n",
            "Eventually , however , the truths I learned from the Bible began to sink deeper into my heart . I realized that if I wanted to serve Jehovah , I had to change my political viewpoints and associations .\n",
            "At last , I have the stable family life that I always craved , and I have the loving Father that I always wanted .\n",
            "I was a new husband , only 25 years old and very inexperienced , but off we went with confidence in Jehovah .\n",
            "What can you do to show these deaf brothers personal attention ?\n",
            "Referring to what the rulership of God’s Son will accomplish , Isaiah 9 : 7 says : “ The very zeal of Jehovah of armies will do this . ”\n",
            "Jesus is the mightiest of all of Jehovah’s spirit sons .\n",
            "The steadfast example set by Jacob and Rachel no doubt had a powerful effect on their son Joseph , influencing how he would handle tests of his own faith .\n",
            "When sentencing “ the original serpent , ” Satan the Devil , God said : “ I shall put enmity between you and the woman and between your seed and her seed . He will bruise you in the head and you will bruise him in the heel . ”\n",
            "Will this ordeal bring David down to Sheol in grief and disgrace ?\n",
            "How can Christian love help to strengthen the marriage bond ?\n",
            "\n",
            "==> Luganda/train.lg <==\n",
            "Naye oluvannyuma lw’ekiseera , nnatandika okukolera ku mazima ge nnali njiga , era nnakiraba nti okusobola okuweereza Yakuwa nnalina okuva mu by’obufuzi n’okuleka emikwano emibi gye nnalina .\n",
            "Nze ne mukyala wange tuli basanyufu , era nkimanyi nti Katonda anjagala nnyo .\n",
            "Mu kiseera ekyo , nnali nnaakawasa , nga ndi wa myaka 25 gyokka , era nga sirina bumanyirivu . Naye nnali mukakafu nti Yakuwa yali ajja kunnyamba .\n",
            "Mu kibiina kyo bwe muba nga mulimu bakiggala , oyinza kukola ki okulaga nti ofaayo ku b’oluganda abo ?\n",
            "Isaaya 9 : 7 walaga nti Omwana wa Katonda yandibadde Kabaka era nti yandikoledde abantu ebintu ebirungi bingi .\n",
            "Yesu y’asinga obuyinza mu baana ba Yakuwa bonna ab’omwoyo .\n",
            "Ekyokulabirako ekirungi Yakobo ne Laakeeri kye baatekawo mu kwoleka obugumiikiriza kyakwata nnyo ku mutabani waabwe Yusufu , era ekyo kyamuyamba nnyo bwe yayolekagana n’embeera ezaagezesa okukkiriza kwe .\n",
            "Bwe yali asalira omusango “ omusota ogw’edda , ” Setaani Omulyolyomi , Katonda yagamba : “ Obulabe n’abuteekanga wakati wo n’omukazi , era ne wakati w’ezzadde lyo n’ezzadde ly’omukazi : ( ezzadde ly’omukazi ) lirikubetenta omutwe , naawe oliribetenta ekisinziiro . ”\n",
            "Embeera eno enzibu eneereetera Dawudi okukka emagombe nga munakuwavu ?\n",
            "Okwagala kw’Ekikristaayo kuyinza kutya okunyweza obufumbo ?\n",
            "==> Luganda/dev.bpe.en <==\n",
            "But if , when you are doing good and you su@@ ff@@ er , you end@@ ure it , this is a thing ag@@ re@@ e@@ able with God . ”\n",
            "At his bap@@ tism , Jesus heard a vo@@ ice from heaven say : “ This is my Son , the bel@@ ov@@ ed , whom I have ap@@ proved . ”\n",
            "( b ) How did Jehovah answer Jesus ’ personal requ@@ est about his future ?\n",
            "But Ab@@ ig@@ a@@ il took action to s@@ ave her hou@@ se@@ hold .\n",
            "While anger is not one of God’s d@@ om@@ in@@ ant qualities , he is prov@@ ok@@ ed to righteous ind@@ ign@@ ation by del@@ ib@@ er@@ ate ac@@ ts of in@@ justice , especially when the v@@ ic@@ tim@@ s are v@@ ul@@ n@@ er@@ able ones . ​ — Psalm 10@@ 3 : 6 .\n",
            "TH@@ E TH@@ R@@ E@@ A@@ T : Mic@@ r@@ ob@@ es that live har@@ m@@ less@@ ly in@@ side an an@@ im@@ al can th@@ reat@@ en your health .\n",
            "What does Jehovah exp@@ ect of us in our service to him ?\n",
            "Jesus said that ‘ the Father in heaven gives holy spirit to those as@@ king him . ’\n",
            "9 , 10 . ( a ) What did Jehovah allow the Babyl@@ on@@ ians to do ?\n",
            "( b ) In harmon@@ y with Phili@@ pp@@ ians 1 : 7 , how have Jehovah’s people re@@ ac@@ ted to Satan’s anger ?\n",
            "\n",
            "==> Luganda/dev.bpe.lg <==\n",
            "[ N ] aye bwe muk@@ ola obulungi ne mu@@ b@@ on@@ ya@@ abon@@ y@@ ez@@ ebwa bwe mul@@ ig@@ umiikiriza , ekyo kye kis@@ iimibwa eri Katonda . ”\n",
            "Yesu bwe yali ya@@ ak@@ amala okub@@ atizibwa , y@@ awulira edd@@ oboozi okuva mu ggulu nga l@@ ig@@ amba nti : “ O@@ no ye M@@ wana wange omw@@ agal@@ wa gwe n@@ si@@ ima . ”\n",
            "( b ) Yakuwa y@@ addamu atya ekyo Yesu kye ye@@ es@@ abira ?\n",
            "Naye Ab@@ big@@ ay@@ iri alina kye yak@@ ol@@ awo okusobola okuw@@ onya ab’omu maka ge .\n",
            "Wadde ng’@@ obus@@ ungu si ngeri ya Yakuwa en@@ kulu , as@@ ung@@ u@@ wala singa ab@@ an@@ aku n’@@ abat@@ alina bu@@ yambi bay@@ is@@ ibwa mu ngeri et@@ ali ya bw@@ enkanya . ​ — Zabbuli 10@@ 3 : 6 .\n",
            "O@@ B@@ UL@@ AB@@ E : Obu@@ w@@ uka obumu bus@@ obola okubeera mu bis@@ olo , ne bwe bib@@ a bya w@@ aka , ne bit@@ afuna bu@@ zibu bwonna , naye ate nga bwe bu@@ ku@@ y@@ ingiramu bu@@ kul@@ wa@@ za .\n",
            "Kiki Yakuwa ky’@@ at@@ us@@ uubir@@ amu nga tu@@ mu@@ weereza ?\n",
            "Yesu yagamba nti ‘ Kitaffe ali mu ggulu awa omwoyo omutukuvu abo ab@@ amus@@ aba . ’\n",
            "9 , 10 . ( a ) Kiki Yakuwa kye yak@@ kiriza Ab@@ abab@@ ulooni okukola ?\n",
            "( b ) Nga kitu@@ uk@@ agana ne Aba@@ f@@ iri@@ pi 1 : 7 , abantu ba Yakuwa bak@@ oze ki nga Setaani ab@@ o@@ olek@@ ezza obus@@ ungu bwe ?\n",
            "\n",
            "==> Luganda/dev.en <==\n",
            "But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "But Abigail took action to save her household .\n",
            "While anger is not one of God’s dominant qualities , he is provoked to righteous indignation by deliberate acts of injustice , especially when the victims are vulnerable ones . ​ — Psalm 103 : 6 .\n",
            "THE THREAT : Microbes that live harmlessly inside an animal can threaten your health .\n",
            "What does Jehovah expect of us in our service to him ?\n",
            "Jesus said that ‘ the Father in heaven gives holy spirit to those asking him . ’\n",
            "9 , 10 . ( a ) What did Jehovah allow the Babylonians to do ?\n",
            "( b ) In harmony with Philippians 1 : 7 , how have Jehovah’s people reacted to Satan’s anger ?\n",
            "\n",
            "==> Luganda/dev.lg <==\n",
            "[ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "Wadde ng’obusungu si ngeri ya Yakuwa enkulu , asunguwala singa abanaku n’abatalina buyambi bayisibwa mu ngeri etali ya bwenkanya . ​ — Zabbuli 103 : 6 .\n",
            "OBULABE : Obuwuka obumu busobola okubeera mu bisolo , ne bwe biba bya waka , ne bitafuna buzibu bwonna , naye ate nga bwe bukuyingiramu bukulwaza .\n",
            "Kiki Yakuwa ky’atusuubiramu nga tumuweereza ?\n",
            "Yesu yagamba nti ‘ Kitaffe ali mu ggulu awa omwoyo omutukuvu abo abamusaba . ’\n",
            "9 , 10 . ( a ) Kiki Yakuwa kye yakkiriza Abababulooni okukola ?\n",
            "( b ) Nga kituukagana ne Abafiripi 1 : 7 , abantu ba Yakuwa bakoze ki nga Setaani aboolekezza obusungu bwe ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uMD6Hic-TSCa",
        "outputId": "6750c5c4-637e-4c4a-f06b-fc7c9478dc97"
      },
      "source": [
        "! head Kinyarwanda/train.*\n",
        "! head Kinyarwanda/dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Kinyarwanda/train.bpe.en <==\n",
            "R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
            "You will see the time when God br@@ ings righteous rule to all the earth , und@@ o@@ ing the d@@ am@@ age and inj@@ ust@@ ice brought by human rul@@ er@@ ship .\n",
            "Let us consider f@@ ive reas@@ ons why we should want to follow the Christ .\n",
            "Even in the Bible , the id@@ ea of pers@@ u@@ as@@ ion som@@ et@@ imes has n@@ eg@@ ative con@@ no@@ t@@ ations , den@@ ot@@ ing a cor@@ rup@@ ting or a lead@@ ing as@@ tr@@ ay .\n",
            "For God’s servants to be deliv@@ ered , Satan and his ent@@ ire world@@ wide system of things need to be rem@@ ov@@ ed .\n",
            "I had never heard that name used in my ch@@ urch .\n",
            "S@@ imp@@ ly having authority or a wid@@ er name recogn@@ ition is not the important thing .\n",
            "M@@ ost people do not believe in the spir@@ its .\n",
            "And others are encourag@@ ed to be merc@@ if@@ ul , for merc@@ y beg@@ ets merc@@ y . ​ — Luke 6 : 38 .\n",
            "Like such ro@@ o@@ ts in earth@@ ’s no@@ ur@@ ish@@ ing so@@ il , our m@@ inds and hearts need to del@@ v@@ e exp@@ ans@@ ively into God’s Word and d@@ raw from its life - giving wat@@ ers .\n",
            "\n",
            "==> Kinyarwanda/train.bpe.rw <==\n",
            "A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
            "U@@ z@@ aba uh@@ ari igihe Imana iz@@ ashy@@ iraho ubutegetsi buk@@ iranuka ku isi hose , ik@@ av@@ an@@ aho ibibi n’@@ akar@@ eng@@ ane byat@@ ewe n’@@ ubutegetsi bw’@@ abantu .\n",
            "Nim@@ ucyo dusuzume impamvu esh@@ anu z@@ agombye gutuma tw@@ ifuza guk@@ urikira Kristo .\n",
            "Nd@@ etse no muri Bibiliya , igit@@ ekerezo cyo kw@@ emeza umuntu ikintu , rimwe na rimwe cy@@ umvikana mu buryo bub@@ i , kig@@ as@@ obanura k@@ osh@@ ya , cyangwa kuy@@ ob@@ ya .\n",
            "Kugira ngo abagaragu b’Imana baz@@ ac@@ ung@@ ur@@ we , Satani na gahunda ye y’@@ ibintu yose yo ku isi hose big@@ omba kuv@@ an@@ waho .\n",
            "Mu idini n@@ abag@@ amo sin@@ ari nar@@ igeze n@@ umva bak@@ oresha iryo z@@ ina .\n",
            "G@@ uh@@ abwa ubut@@ ware gusa cyangwa kugira umw@@ anya ukomeye si cyo kintu cy’@@ ingenzi .\n",
            "Abantu benshi ntib@@ emera imy@@ uka .\n",
            "Iyo tug@@ iriye abantu imbabazi na bo bib@@ at@@ era kugira imbabazi , kuko imbabazi zit@@ urwa izindi . — Luka 6 : 38 .\n",
            "Nk’uko iyo m@@ izi ig@@ ab@@ urira ig@@ iti ib@@ iv@@ uye mu but@@ aka buk@@ ung@@ ah@@ aye , ubwenge n’@@ umutima byacu big@@ omba guc@@ eng@@ era mu Ijambo ry’Imana maze bik@@ av@@ om@@ amo amazi atanga ubuzima .\n",
            "\n",
            "==> Kinyarwanda/train.en <==\n",
            "Right after his baptism , he “ went off into Arabia ” ​ — either the Syrian Desert or possibly some quiet place on the Arabian Peninsula that was conducive to meditation .\n",
            "You will see the time when God brings righteous rule to all the earth , undoing the damage and injustice brought by human rulership .\n",
            "Let us consider five reasons why we should want to follow the Christ .\n",
            "Even in the Bible , the idea of persuasion sometimes has negative connotations , denoting a corrupting or a leading astray .\n",
            "For God’s servants to be delivered , Satan and his entire worldwide system of things need to be removed .\n",
            "I had never heard that name used in my church .\n",
            "Simply having authority or a wider name recognition is not the important thing .\n",
            "Most people do not believe in the spirits .\n",
            "And others are encouraged to be merciful , for mercy begets mercy . ​ — Luke 6 : 38 .\n",
            "Like such roots in earth’s nourishing soil , our minds and hearts need to delve expansively into God’s Word and draw from its life - giving waters .\n",
            "\n",
            "==> Kinyarwanda/train.rw <==\n",
            "Ashobora kuba yaragiye ahantu hatuje mu Butayu bwa Siriya cyangwa se wenda ku Mwigimbakirwa wa Arabiya , uri mu burasirazuba bw’Inyanja Itukura , kugira ngo hamufashe gutekereza .\n",
            "Uzaba uhari igihe Imana izashyiraho ubutegetsi bukiranuka ku isi hose , ikavanaho ibibi n’akarengane byatewe n’ubutegetsi bw’abantu .\n",
            "Nimucyo dusuzume impamvu eshanu zagombye gutuma twifuza gukurikira Kristo .\n",
            "Ndetse no muri Bibiliya , igitekerezo cyo kwemeza umuntu ikintu , rimwe na rimwe cyumvikana mu buryo bubi , kigasobanura koshya , cyangwa kuyobya .\n",
            "Kugira ngo abagaragu b’Imana bazacungurwe , Satani na gahunda ye y’ibintu yose yo ku isi hose bigomba kuvanwaho .\n",
            "Mu idini nabagamo sinari narigeze numva bakoresha iryo zina .\n",
            "Guhabwa ubutware gusa cyangwa kugira umwanya ukomeye si cyo kintu cy’ingenzi .\n",
            "Abantu benshi ntibemera imyuka .\n",
            "Iyo tugiriye abantu imbabazi na bo bibatera kugira imbabazi , kuko imbabazi ziturwa izindi . — Luka 6 : 38 .\n",
            "Nk’uko iyo mizi igaburira igiti ibivuye mu butaka bukungahaye , ubwenge n’umutima byacu bigomba gucengera mu Ijambo ry’Imana maze bikavomamo amazi atanga ubuzima .\n",
            "==> Kinyarwanda/dev.bpe.en <==\n",
            "My heart was t@@ ou@@ ched .\n",
            "Consider , however , what was involved in reading a s@@ cro@@ l@@ l .\n",
            "Rather than succ@@ um@@ b to p@@ an@@ ic or des@@ pa@@ ir , we should b@@ ol@@ st@@ er our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "S@@ ad@@ ly , some within the Christian congregation give evidence that , at le@@ ast to a d@@ eg@@ ree , they feel com@@ for@@ table in Satan’s world .\n",
            "Jehovah took into account El@@ ij@@ ah’s l@@ imit@@ ations and dis@@ pat@@ ched an ang@@ el .\n",
            "T@@ ho@@ ugh in@@ her@@ iting imper@@ f@@ ection from Adam , mill@@ ions of other God - f@@ earing humans have follow@@ ed in Jesus ’ f@@ oo@@ t@@ st@@ ep@@ s by keep@@ ing integr@@ ity in the face of sat@@ an@@ ic att@@ ac@@ ks . — 1 Peter 1 : 18 , 19 ; 2 : 19 , 21 .\n",
            "C@@ ould those who later me@@ et the son righ@@ t@@ fully con@@ clud@@ e that he had a bad father or even that he had no father at all ?\n",
            "A@@ ud@@ i@@ ences were moved by the dr@@ ama “ R@@ esp@@ ect Jehovah’s A@@ uth@@ or@@ ity ”\n",
            "Satan has destro@@ y@@ ed coun@@ t@@ less who@@ l@@ es@@ ome , tr@@ us@@ ting relation@@ ship@@ s through in@@ sid@@ ious doub@@ ts pl@@ an@@ ted in that way . ​ — Gal@@ at@@ ians 5 : 7 - 9 .\n",
            "T@@ om , a m@@ ember of the Be@@ the@@ l family in E@@ st@@ on@@ ia , says : “ J@@ ust a b@@ loc@@ k away from Be@@ the@@ l is the se@@ a , and n@@ ear@@ by there is a be@@ aut@@ if@@ ul for@@ est where my wife and I enjoy going for sh@@ ort wal@@ ks .\n",
            "\n",
            "==> Kinyarwanda/dev.bpe.rw <==\n",
            "By@@ ank@@ oze ku mutima .\n",
            "Umw@@ andiko w@@ abaga w@@ anditse mu nk@@ ing@@ i ku ruhande rw’@@ imbere rw’@@ uwo muz@@ ingo .\n",
            "Aho kugira ngo duh@@ ang@@ ay@@ ike cyangwa tw@@ ih@@ ebe , twagombye gukomeza ukwizera duf@@ itiye Imana binyuriye mu gusoma Ijambo ry@@ ayo . — Abaroma 8 : 35 - 39 .\n",
            "Ik@@ ibab@@ aje ni uko hari bamwe mu bagize itorero rya gikristo bag@@ aragaza , mu rugero runaka , ko bag@@ uwe neza muri iyi si ya Satani .\n",
            "Yehova y@@ az@@ irik@@ anye intege nk@@ e za El@@ iya maze amw@@ oh@@ erer@@ eza umum@@ ar@@ ayika .\n",
            "N’ubwo bar@@ az@@ we uk@@ ud@@ at@@ ung@@ ana bitewe n’@@ icyaha cya Adamu , abandi bantu bat@@ inya Imana babarirwa muri za miriyoni , bag@@ eze ikir@@ enge mu cya Yesu bak@@ omeza gush@@ ik@@ ama mu gihe bari bah@@ anganye n’@@ ibit@@ ero bya Satani . ​ —⁠ 1 Petero 1 : 18 , 19 ; 2 : ​ 19 , 21 .\n",
            "Ese byaba bik@@ wiriye kuvuga ko uwo mub@@ yeyi yar@@ eze nabi , tuk@@ aba tw@@ an@@ avuga ko umwana at@@ agira se ?\n",
            "Ab@@ ari mu ikor@@ aniro bak@@ ozwe ku mutima na d@@ ar@@ ame yari ifite umutwe uvuga ngo “ Jya W@@ ubaha Ubut@@ ware bwa Yehova ”\n",
            "Satani yash@@ enye imishyikirano myiza yar@@ ang@@ waga no kw@@ iz@@ er@@ ana abantu bat@@ abar@@ ika bari b@@ af@@ itanye , binyuriye mu bit@@ ekerezo b@@ if@@ if@@ itse byo gushidikanya yagiye ab@@ iba muri ubwo buryo . — Abag@@ al@@ at@@ iya 5 : 7 - 9 .\n",
            "T@@ om , umwe mu bagize umuryango wa Bet@@ eli yo muri Es@@ it@@ on@@ iya , yagize ati “ iyo ur@@ enze inzu imwe gusa uv@@ uye kuri Bet@@ eli uh@@ ita ug@@ era ku ny@@ anja , kandi hafi aho hari ag@@ ashy@@ amba k@@ eza aho jye n’umugore wanjye duk@@ unda kujya gut@@ emb@@ er@@ era ak@@ anya gato .\n",
            "\n",
            "==> Kinyarwanda/dev.en <==\n",
            "My heart was touched .\n",
            "Consider , however , what was involved in reading a scroll .\n",
            "Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "Jehovah took into account Elijah’s limitations and dispatched an angel .\n",
            "Though inheriting imperfection from Adam , millions of other God - fearing humans have followed in Jesus ’ footsteps by keeping integrity in the face of satanic attacks . — 1 Peter 1 : 18 , 19 ; 2 : 19 , 21 .\n",
            "Could those who later meet the son rightfully conclude that he had a bad father or even that he had no father at all ?\n",
            "Audiences were moved by the drama “ Respect Jehovah’s Authority ”\n",
            "Satan has destroyed countless wholesome , trusting relationships through insidious doubts planted in that way . ​ — Galatians 5 : 7 - 9 .\n",
            "Tom , a member of the Bethel family in Estonia , says : “ Just a block away from Bethel is the sea , and nearby there is a beautiful forest where my wife and I enjoy going for short walks .\n",
            "\n",
            "==> Kinyarwanda/dev.rw <==\n",
            "Byankoze ku mutima .\n",
            "Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "Yehova yazirikanye intege nke za Eliya maze amwoherereza umumarayika .\n",
            "N’ubwo barazwe ukudatungana bitewe n’icyaha cya Adamu , abandi bantu batinya Imana babarirwa muri za miriyoni , bageze ikirenge mu cya Yesu bakomeza gushikama mu gihe bari bahanganye n’ibitero bya Satani . ​ —⁠ 1 Petero 1 : 18 , 19 ; 2 : ​ 19 , 21 .\n",
            "Ese byaba bikwiriye kuvuga ko uwo mubyeyi yareze nabi , tukaba twanavuga ko umwana atagira se ?\n",
            "Abari mu ikoraniro bakozwe ku mutima na darame yari ifite umutwe uvuga ngo “ Jya Wubaha Ubutware bwa Yehova ”\n",
            "Satani yashenye imishyikirano myiza yarangwaga no kwizerana abantu batabarika bari bafitanye , binyuriye mu bitekerezo bififitse byo gushidikanya yagiye abiba muri ubwo buryo . — Abagalatiya 5 : 7 - 9 .\n",
            "Tom , umwe mu bagize umuryango wa Beteli yo muri Esitoniya , yagize ati “ iyo urenze inzu imwe gusa uvuye kuri Beteli uhita ugera ku nyanja , kandi hafi aho hari agashyamba keza aho jye n’umugore wanjye dukunda kujya gutemberera akanya gato .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IP5nJ822UGJM",
        "outputId": "3c185b0b-2f44-4cb4-cd01-ca0710a3fe3d"
      },
      "source": [
        "! head Luhyia/train.*\n",
        "! head Luhyia/dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Luhyia/train.bpe.en <==\n",
            "Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
            "If anyone th@@ in@@ ks himself to be a prophet or spirit@@ ual , let him ac@@ knowledge that the things which I write to you are the commandments of the Lord .\n",
            "E@@ very br@@ an@@ ch in Me that does not bear fruit He tak@@ es away ; and every br@@ an@@ ch that be@@ ars fruit He pr@@ un@@ es , that it may bear more fruit .\n",
            "D@@ em@@ et@@ ri@@ us has a good testimony from all , and from the truth its@@ el@@ f . And we also bear witness , and you know that our testimony is true .\n",
            "And supp@@ er being ended , the devil having already put it into the heart of Judas Is@@ c@@ ari@@ ot , Simon ’ s son , to betr@@ ay Him ,\n",
            "im@@ pl@@ or@@ ing us with much ur@@ gen@@ c@@ y that we would receive the gift and the fel@@ low@@ ship of the minis@@ ter@@ ing to the saints .\n",
            "It is written in the prophets , ‘ And they shall all be taught by G@@ od@@ . ’ Therefore everyone who has heard and lear@@ ned from the Father comes to Me .\n",
            "For those who are such do not serve our Lord Jesus Christ , but their own bel@@ ly , and by sm@@ oo@@ th words and fl@@ at@@ ter@@ ing spe@@ e@@ ch dece@@ ive the hearts of the s@@ im@@ ple .\n",
            "So when he had received food , he was streng@@ th@@ ened . Then Saul sp@@ ent some days with the disciples at Damas@@ cus .\n",
            "Therefore if you have not been faithful in the un@@ righteous m@@ am@@ m@@ on , who will comm@@ it to your tr@@ ust the true riches ?\n",
            "\n",
            "==> Luhyia/train.bpe.lh <==\n",
            "Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
            "Omundu yesi ow@@ il@@ olanga mbu , nomu@@ r@@ umwa , wa Nyasaye noho mbu ali neshi@@ haanwa eshia Roho okhuula amany@@ e khandi afuchil@@ ile mbu , aka , emu@@ handi@@ chilanga kano nel@@ ilako elia Omwami .\n",
            "A@@ rem@@ anga buli lis@@ aka , mwisie el@@ il@@ am@@ anga ebiamo ta , ne akh@@ alilanga buli lis@@ aka , eli@@ am@@ anga ebiamo kho mbu , li@@ be lil@@ ayi nil@@ im@@ e@@ eta okhw@@ ama , ebiamo ebinji .\n",
            "Buli mundu amw@@ itsoom@@ injia D@@ em@@ eter@@ io ; ne obw@@ atieli , bwene bu@@ mw@@ itsoom@@ injia . Ne nasi em@@ e@@ et@@ akhwo obuloli , bwanje , ne mumanyile mbu ak@@ emb@@ oola n@@ akatoto . Am@@ ash@@ esio K@@ okhumalil@@ isia ,\n",
            "Yesu nende abeechibe bali nib@@ ali@@ itsanga eshiokhulia , eshia ha@@ muk@@ ol@@ oba . Setani yali namalile okhur@@ a mu Yuda omwana wa Simoni Is@@ ik@@ ari@@ o@@ ti amapaaro k@@ okhukh@@ oba Yes@@ u. ,\n",
            "bakhu@@ s@@ aba nib@@ akhu@@ saaya okhu@@ fuchil@@ ilwa okhus@@ anga , mukhu@@ khon@@ ya abakristo bashi@@ abwe aba Yudea .\n",
            "Ab@@ al@@ akusi , b@@ ahandika mbu , ‘ Buli mundu ali@@ eches@@ ibwa nende , Nyasaye. ’ Kho oyo yesi ou@@ hulilanga aka Papa nende , okhw@@ eka okhurula khuye , yetsa khwisie .\n",
            "Okhuba , abakholanga amakhuwa kario shi@@ bakh@@ alaban@@ ilanga Kristo , Omwami wefwe ta , habula bakh@@ alaban@@ ilanga ts@@ inda , tsiabwe abeene . B@@ ekh@@ oonyelanga amakhuwa kabwe , k@@ okhu@@ ka@@ at@@ ilisia nende ak@@ okhul@@ aha khulwa okhu@@ ka@@ atia , amapaaro k@@ abat@@ eshele .\n",
            "Ne olwa yamala , okhulia eshiokhulia , omubil@@ ikwe kw@@ anyoola amaani . Saulo ayaala Injiili Damas@@ iko Saulo y@@ amenya Damas@@ iko halala nab@@ asuubili khulwa , tsinyanga tsind@@ i@@ iti .\n",
            "Kho , nimul@@ aba ab@@ esi@@ ikwa mubu@@ yinda , bw@@ omushialo shino ta , mwakh@@ aba murie ab@@ esi@@ ikwa , mubu@@ yinda bwatoto ?\n",
            "\n",
            "==> Luhyia/train.en <==\n",
            "Then Pilate entered the Praetorium again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
            "If anyone thinks himself to be a prophet or spiritual , let him acknowledge that the things which I write to you are the commandments of the Lord .\n",
            "Every branch in Me that does not bear fruit He takes away ; and every branch that bears fruit He prunes , that it may bear more fruit .\n",
            "Demetrius has a good testimony from all , and from the truth itself . And we also bear witness , and you know that our testimony is true .\n",
            "And supper being ended , the devil having already put it into the heart of Judas Iscariot , Simon ’ s son , to betray Him ,\n",
            "imploring us with much urgency that we would receive the gift and the fellowship of the ministering to the saints .\n",
            "It is written in the prophets , ‘ And they shall all be taught by God. ’ Therefore everyone who has heard and learned from the Father comes to Me .\n",
            "For those who are such do not serve our Lord Jesus Christ , but their own belly , and by smooth words and flattering speech deceive the hearts of the simple .\n",
            "So when he had received food , he was strengthened . Then Saul spent some days with the disciples at Damascus .\n",
            "Therefore if you have not been faithful in the unrighteous mammon , who will commit to your trust the true riches ?\n",
            "\n",
            "==> Luhyia/train.lh <==\n",
            "Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
            "Omundu yesi owilolanga mbu , nomurumwa , wa Nyasaye noho mbu ali neshihaanwa eshia Roho okhuula amanye khandi afuchilile mbu , aka , emuhandichilanga kano nelilako elia Omwami .\n",
            "Aremanga buli lisaka , mwisie elilamanga ebiamo ta , ne akhalilanga buli lisaka , eliamanga ebiamo kho mbu , libe lilayi nilimeeta okhwama , ebiamo ebinji .\n",
            "Buli mundu amwitsoominjia Demeterio ; ne obwatieli , bwene bumwitsoominjia . Ne nasi emeetakhwo obuloli , bwanje , ne mumanyile mbu akemboola nakatoto . Amashesio Kokhumalilisia ,\n",
            "Yesu nende abeechibe bali nibaliitsanga eshiokhulia , eshia hamukoloba . Setani yali namalile okhura mu Yuda omwana wa Simoni Isikarioti amapaaro kokhukhoba Yesu. ,\n",
            "bakhusaba nibakhusaaya okhufuchililwa okhusanga , mukhukhonya abakristo bashiabwe aba Yudea .\n",
            "Abalakusi , bahandika mbu , ‘ Buli mundu aliechesibwa nende , Nyasaye. ’ Kho oyo yesi ouhulilanga aka Papa nende , okhweka okhurula khuye , yetsa khwisie .\n",
            "Okhuba , abakholanga amakhuwa kario shibakhalabanilanga Kristo , Omwami wefwe ta , habula bakhalabanilanga tsinda , tsiabwe abeene . Bekhoonyelanga amakhuwa kabwe , kokhukaatilisia nende akokhulaha khulwa okhukaatia , amapaaro kabateshele .\n",
            "Ne olwa yamala , okhulia eshiokhulia , omubilikwe kwanyoola amaani . Saulo ayaala Injiili Damasiko Saulo yamenya Damasiko halala nabasuubili khulwa , tsinyanga tsindiiti .\n",
            "Kho , nimulaba abesiikwa mubuyinda , bwomushialo shino ta , mwakhaba murie abesiikwa , mubuyinda bwatoto ?\n",
            "==> Luhyia/dev.bpe.en <==\n",
            "Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put cl@@ ay on my eyes , and I w@@ ash@@ ed , and I see . ”\n",
            "And when she had said these things , she went her way and secre@@ tly called Mary her sis@@ ter , saying , “ The Teacher has come and is cal@@ ling for you . ”\n",
            "T@@ hat day was the P@@ re@@ par@@ ation , and the Sabbath drew near .\n",
            "But when Paul had gathered a bu@@ nd@@ le of st@@ ic@@ ks and laid them on the fire , a vi@@ p@@ er came out because of the he@@ at , and f@@ ast@@ ened on his hand .\n",
            "not given to wine , not vi@@ ol@@ ent , not gre@@ ed@@ y for money , but g@@ ent@@ le , not qu@@ ar@@ rel@@ some , not cov@@ et@@ ous ;\n",
            "and this woman was a wi@@ do@@ w of about ei@@ gh@@ t@@ y-@@ four years , who did not depart from the temple , but serv@@ ed God with f@@ ast@@ ings and pray@@ ers night and day .\n",
            "And not being weak in faith , he did not consi@@ der his own body , already dead ( since he was about a hundred years old ) , and the de@@ ad@@ ness of S@@ ar@@ ah ’ s wom@@ b .\n",
            "In these lay a great multitude of sick people , blind , l@@ ame , par@@ al@@ y@@ zed , wa@@ it@@ ing for the m@@ ov@@ ing of the water .\n",
            "Then he go@@ es and tak@@ es with him seven other spirits more wi@@ cked than himself , and they enter and dwell there ; and the last st@@ ate of that man is wor@@ se than the first . So shall it also be with this wi@@ cked generation . ”\n",
            "So He got into a boat , cr@@ os@@ sed over , and came to His own city .\n",
            "\n",
            "==> Luhyia/dev.bpe.lh <==\n",
            "Abafarisayo nabo nib@@ areeba omundu oyo shinga olwa , yali n@@ any@@ ali@@ ilwe okhulola khandi . Naye nababoolela ari “ Ab@@ ashi@@ le lit@@ oy@@ i khum@@ oni tsi@@ anje , ne nind@@ iy@@ osia mumoni mana bulano enyala okhulola . ”\n",
            "Olwa Mar@@ itsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe m@@ as@@ ili@@ isi , ne , namuboolela ari , “ Omwechesia yets@@ ile ali hano , ne , a@@ khul@@ anganga . ”\n",
            "Yali , inyanga yo@@ khw@@ ire@@ chekha khulwa inyanga eya Isabato e@@ yali niy@@ ili ahambi okhu@@ chaaka .\n",
            "Paulo yab@@ unj@@ elesia olu@@ kh@@ any@@ a , lwe@@ tsi@@ khw@@ i , ne olwa yali n@@ ats@@ ir@@ er@@ anga khu@@ mul@@ ilo , khulwa , olu@@ u@@ ya lw@@ omulilo okwo , inz@@ okha yar@@ ulamwo niy@@ ik@@ any@@ ila , khumu@@ khonokwe .\n",
            "alaba , omum@@ esi kata ow@@ obus@@ olo ta , habula omu@@ h@@ olo khandi ow@@ omulembe , kata ow@@ ala@@ heela amapesa tawe .\n",
            "Khandi y@@ amenya nali omule@@ khwa khulw@@ emiyika , amakhumi mun@@ ane na@@ chi@@ ne . Nebutswa emiyika echio , chiosi , y@@ amenyanga butswa muhekalu . Y@@ enam@@ ilanga , OMWAMI Nyasaye eshilo neshi@@ teere , n@@ ahon@@ ga inzala , nende okhusaaya .\n",
            "Yali ahambi ow@@ emiyika , eshi@@ khumi shilala , nebutswa obusuub@@ ilibwe , shi@@ bw@@ at@@ it@@ iy@@ akhwo kata olwa y@@ apaara khu@@ bul@@ amu , bw@@ omubil@@ ikwe okw@@ ali nik@@ w@@ ah@@ w@@ amwo amaani ta , noho , kata olwa y@@ amanya mbu , S@@ ara nomu@@ ko@@ fu shianyala , okhw@@ ibula tawe .\n",
            "Omuk@@ anda , omukhongo kwabandu , abalwale , abab@@ ofu , abal@@ ema nende , abak@@ wa amak@@ ara , bali nib@@ ak@@ on@@ ile mub@@ ir@@ o@@ ok@@ oola ebio .\n",
            "nishi@@ kal@@ ukh@@ ayo shi@@ tsia okhul@@ anga ebishieno b@@ indi , musafu ebib@@ i muno , nibi@@ ch@@ elela okhum@@ eny@@ amwo . Ne , olunyuma lw@@ okhumw@@ injil@@ amwo , omundu tsana aba obubi , okhushilakhwo shinga olwa yali olw@@ ambeli . Ak@@ o niko , ak@@ atsia okhwikholekha khubandu b@@ olwibulo ol@@ um@@ ayanu , lwa bul@@ an@@ o. ” N@@ y@@ ina Yesu nende abaana babwe ,\n",
            "Yesu yenjila muliaro niy@@ am@@ bu@@ kha ni@@ yoola mwitaala , li@@ ew@@ abwe elia K@@ aper@@ in@@ a@@ umu .\n",
            "\n",
            "==> Luhyia/dev.en <==\n",
            "Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
            "And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
            "That day was the Preparation , and the Sabbath drew near .\n",
            "But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
            "not given to wine , not violent , not greedy for money , but gentle , not quarrelsome , not covetous ;\n",
            "and this woman was a widow of about eighty-four years , who did not depart from the temple , but served God with fastings and prayers night and day .\n",
            "And not being weak in faith , he did not consider his own body , already dead ( since he was about a hundred years old ) , and the deadness of Sarah ’ s womb .\n",
            "In these lay a great multitude of sick people , blind , lame , paralyzed , waiting for the moving of the water .\n",
            "Then he goes and takes with him seven other spirits more wicked than himself , and they enter and dwell there ; and the last state of that man is worse than the first . So shall it also be with this wicked generation . ”\n",
            "So He got into a boat , crossed over , and came to His own city .\n",
            "\n",
            "==> Luhyia/dev.lh <==\n",
            "Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
            "Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
            "Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
            "Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
            "alaba , omumesi kata owobusolo ta , habula omuholo khandi owomulembe , kata owalaheela amapesa tawe .\n",
            "Khandi yamenya nali omulekhwa khulwemiyika , amakhumi munane nachine . Nebutswa emiyika echio , chiosi , yamenyanga butswa muhekalu . Yenamilanga , OMWAMI Nyasaye eshilo neshiteere , nahonga inzala , nende okhusaaya .\n",
            "Yali ahambi owemiyika , eshikhumi shilala , nebutswa obusuubilibwe , shibwatitiyakhwo kata olwa yapaara khubulamu , bwomubilikwe okwali nikwahwamwo amaani ta , noho , kata olwa yamanya mbu , Sara nomukofu shianyala , okhwibula tawe .\n",
            "Omukanda , omukhongo kwabandu , abalwale , ababofu , abalema nende , abakwa amakara , bali nibakonile mubirookoola ebio .\n",
            "nishikalukhayo shitsia okhulanga ebishieno bindi , musafu ebibi muno , nibichelela okhumenyamwo . Ne , olunyuma lwokhumwinjilamwo , omundu tsana aba obubi , okhushilakhwo shinga olwa yali olwambeli . Ako niko , akatsia okhwikholekha khubandu bolwibulo olumayanu , lwa bulano. ” Nyina Yesu nende abaana babwe ,\n",
            "Yesu yenjila muliaro niyambukha niyoola mwitaala , liewabwe elia Kaperinaumu .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_D0BqDaUOF_"
      },
      "source": [
        "pre = '/content/gdrive/Shared drives/NMT_for_African_Language/'\n",
        "# Train data source\n",
        "filenames = [pre+'Luganda/train.en', pre+'Kinyarwanda/train.en',pre+'Luhyia/train.en']\n",
        "\n",
        "# Train data target\n",
        "filenames2 = [pre+'Luganda/train.lg', pre+'Kinyarwanda/train.rw',pre+'Luhyia/train.lh']\n",
        "\n",
        "# Dev data source\n",
        "file1 = [pre+'Luganda/dev.en', pre+'Kinyarwanda/dev.en',pre+'Luhyia/dev.en']\n",
        "\n",
        "# Dev data target\n",
        "file2 = [pre+'Luganda/dev.lg', pre+'Kinyarwanda/dev.rw',pre+'Luhyia/dev.lh']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icPaGn2GlpM-"
      },
      "source": [
        "# Changing to Multilingual directory\n",
        "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5zhE2IimjPs"
      },
      "source": [
        "# Procedure to create concatenated files\n",
        "def create_file(x,filename):\n",
        "  # Open filename in write mode\n",
        "  with open(filename, 'w') as outfile:\n",
        "      for names in x:\n",
        "          # Open each file in read mode\n",
        "          with open(names) as infile:\n",
        "              # read the data and write it in file3\n",
        "              outfile.write(infile.read())\n",
        "          outfile.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay8047V3oyeG"
      },
      "source": [
        "# Creating multilingual files\n",
        "create_file(filenames,'train.en')\n",
        "create_file(filenames2,'train.lg_rw_lh')\n",
        "create_file(file1,'dev.en')\n",
        "create_file(file2,'dev.lg_rw_lh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VouMO0lishSQ"
      },
      "source": [
        "### BPE codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5lK1btbKswMN",
        "outputId": "b36cb336-3977-4ae8-fe18-02136ff79cd6"
      },
      "source": [
        "#! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
            "Collecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 96 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 17.4 MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 38.5 MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "  Downloading pylint-2.9.5-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 44.1 MB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
            "Collecting isort<6,>=4.2.5\n",
            "  Downloading isort-5.9.2-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting astroid<2.7,>=2.6.5\n",
            "  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 45.3 MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
            "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
            "Building wheels for collected packages: joeynmt, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85058 sha256=934b90f3d2308310552524e0f5afd1fea03568a014063fbd68479089e93bfd43\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5i4dzeow/wheels/e2/a9/5c/6dec07e4141ca16dff5bb6c32079519e084a3b38f9497ad88c\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68445 sha256=4ba1f3a146fb9639fb7517ac6060743ef8b312655d21f0f5c378ad0d024d9dec\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
            "Successfully built joeynmt wrapt\n",
            "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
            "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed astroid-2.6.5 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.5 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1DcnZg7ZWNx"
      },
      "source": [
        "#### Baseline BPEs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOYk3OK4rw6d"
      },
      "source": [
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt --output_path vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MihNfJTqjN-8"
      },
      "source": [
        "# Applying BPE to tests\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test1.$src > test.bpe.en1\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test1.lh > test.bpe.lh\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test2.$src > test.bpe.en2\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test2.lg > test.bpe.lg\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test3.$src > test.bpe.en3\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test3.rw > test.bpe.rw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj0HdPZ6t5i3",
        "outputId": "8f1de287-2b54-4263-f016-34579ad77e3b"
      },
      "source": [
        "# Some output\n",
        "! echo \"BPE Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 vocab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BPE Sentences\n",
            "N@@ asi , n@@ ir@@ ee@@ ba end@@ i , ‘ N@@ iwe w@@ ina , Omw@@ ami ? ’ Omw@@ oyo ok@@ wo , n@@ i@@ kum@@ bo@@ ol@@ el@@ a kuri , ‘ N@@ is@@ ie Yesu o@@ wa N@@ az@@ ar@@ eti ow@@ os@@ a@@ and@@ inj@@ ia . ’\n",
            "sh@@ ic@@ hil@@ a , omuk@@ h@@ aan@@ awe omut@@ el@@ wa , ow@@ em@@ iy@@ ika ek@@ hum@@ i n@@ ach@@ ib@@ ili yali n@@ any@@ iranga . N@@ e olwa yali nat@@ s@@ it@@ s@@ anga , aband@@ u , ba@@ mw@@ ib@@ um@@ bak@@ h@@ wo okh@@ ur@@ ula mut@@ s@@ imb@@ eka t@@ si@@ osi .\n",
            "N@@ e olwa kab@@ is@@ ibwa mbu kh@@ uk@@ ho@@ y@@ ile okh@@ ut@@ si@@ ila , m@@ um@@ e@@ eli okh@@ uula I@@ tal@@ ia , bah@@ aana Paul@@ o nende aba@@ bo@@ he , b@@ andi k@@ hum@@ us@@ inj@@ il@@ ili w@@ el@@ ihe J@@ ul@@ i@@ asi ow@@ e@@ ing@@ '@@ anda e@@ ya , esh@@ ir@@ oma ey@@ il@@ angwa mbu , “ I@@ ng@@ '@@ anda ey@@ il@@ ind@@ anga , O@@ mur@@ uc@@ h@@ i . ”\n",
            "Ol@@ uny@@ um@@ ak@@ h@@ wo , aba@@ ku@@ uka be@@ f@@ we , aba@@ bu@@ kul@@ a l@@ ih@@ e@@ ema el@@ o okh@@ ur@@ ula kh@@ u@@ bas@@ abwe , bal@@ ic@@ h@@ inga , okh@@ uula mut@@ s@@ iny@@ anga t@@ s@@ ia Y@@ osh@@ wa n@@ iba@@ bu@@ kul@@ a esh@@ i@@ alo , esh@@ ia amahanga aka N@@ yas@@ aye yal@@ ond@@ anga n@@ ik@@ ar@@ ula im@@ bel@@ i , w@@ abwe . N@@ e li@@ am@@ eny@@ ayo okh@@ uula mut@@ s@@ iny@@ anga t@@ s@@ ia , om@@ ur@@ uc@@ h@@ i D@@ a@@ udi .\n",
            "N@@ e olwa , y@@ enj@@ il@@ am@@ wo , yaba@@ re@@ eba ari , “ Mw@@ ik@@ h@@ uul@@ anga nim@@ uk@@ h@@ up@@ a , t@@ s@@ imb@@ ungu mb@@ ush@@ i@@ ina ? Omw@@ ana un@@ o shi@@ af@@ w@@ ile ta , hab@@ ula , ak@@ on@@ anga but@@ s@@ wa t@@ s@@ ind@@ ool@@ o . ”\n",
            "Combined BPE Vocab\n",
            "Ă@@\n",
            "̄@@\n",
            "epher@@\n",
            "(@@\n",
            "Jerusale@@\n",
            "ḥ\n",
            "ḍ@@\n",
            "rista@@\n",
            "ŋ\n",
            "gyp@@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gSynCGijJUlV",
        "outputId": "4d66d40c-9a75-464c-e897-416cc0949b74"
      },
      "source": [
        "! tail train.*\n",
        "! tail dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.bpe.en <==\n",
            "But if anyone lov@@ es God , this one is known by H@@ im .\n",
            "And the second is like it : ‘ You shall love your neigh@@ b@@ or as yourself . ’\n",
            "until the day in which He was taken up , after He through the H@@ ol@@ y Sp@@ ir@@ it had given command@@ ments to the apostles whom He had cho@@ s@@ en ,\n",
            "For what if some did not believe ? W@@ ill their un@@ beli@@ ef make the faith@@ ful@@ ness of God without ef@@ fect ?\n",
            "And when you go into a hous@@ eho@@ ld , gre@@ et it .\n",
            "And a very great mul@@ t@@ itude sp@@ read their clo@@ th@@ es on the ro@@ ad ; others c@@ ut down br@@ an@@ ch@@ es from the tre@@ es and sp@@ read them on the ro@@ ad .\n",
            "And we heard this vo@@ ice which came from heaven when we were with H@@ im on the holy m@@ ount@@ ain .\n",
            "O@@ r those e@@ igh@@ te@@ en on whom the tower in S@@ il@@ o@@ am f@@ ell and kill@@ ed them , do you think that they were wor@@ se sin@@ n@@ ers than all other men who dw@@ el@@ t in Jerusalem ?\n",
            "For I be@@ ar him witness that he has a great z@@ eal for you , and those who are in L@@ a@@ od@@ ic@@ ea , and those in H@@ i@@ er@@ ap@@ ol@@ is .\n",
            "\n",
            "\n",
            "==> train.bpe.lg_rw_lh <==\n",
            "N@@ eb@@ uts@@ wa , om@@ undu y@@ esi ou@@ he@@ el@@ a N@@ yas@@ aye , omw@@ en@@ oyo N@@ yas@@ aye , yam@@ um@@ anya .\n",
            "El@@ i@@ ak@@ hab@@ ili el@@ ili sh@@ inga el@@ o , nd@@ i@@ el@@ ino mbu , ‘ O@@ he@@ el@@ e ow@@ ash@@ io sh@@ inga olwa w@@ ih@@ e@@ el@@ a , omw@@ ene . ’\n",
            "okh@@ uula , kh@@ uny@@ anga e@@ ya yab@@ ukul@@ il@@ wak@@ h@@ wo n@@ ay@@ il@@ wa mw@@ ikulu . N@@ e , n@@ ash@@ ili okh@@ uy@@ il@@ wa mw@@ ikulu , y@@ ec@@ h@@ es@@ ia aba yali n@@ iy@@ a@@ ah@@ ula , okh@@ uba abar@@ um@@ ebe mub@@ un@@ yali bwa R@@ o@@ ho Omut@@ akat@@ if@@ u@@ . ,\n",
            "N@@ eb@@ uts@@ wa abandi kh@@ ub@@ o sh@@ iba@@ l@@ i , abas@@ uub@@ il@@ wa ta . K@@ ho k@@ ano k@@ ak@@ ham@@ any@@ is@@ ia mbu , N@@ yas@@ aye , shi@@ ali omus@@ uub@@ il@@ wa ta no@@ ho ?\n",
            "N@@ e olwa mw@@ inj@@ ila mun@@ z@@ u mu@@ ba@@ shi@@ es@@ ie , omul@@ embe .\n",
            "Ab@@ and@@ u aban@@ j@@ i n@@ iba@@ ala , eb@@ if@@ wal@@ o bi@@ abwe k@@ hum@@ uh@@ anda , ne abandi n@@ iba@@ rem@@ a amas@@ aka k@@ em@@ is@@ aala n@@ iba@@ ala k@@ hum@@ uh@@ anda ok@@ wo .\n",
            "K@@ h@@ wali n@@ ik@@ h@@ uli , n@@ in@@ aye kh@@ ush@@ ikulu esh@@ it@@ akat@@ if@@ u olwa kh@@ w@@ ah@@ ul@@ ila omwoyo , n@@ i@@ kur@@ ula mw@@ ikulu e@@ wa N@@ yas@@ aye .\n",
            "No@@ ho , m@@ up@@ a@@ aranga mbu , aband@@ u ek@@ hum@@ i nam@@ un@@ ane b@@ om@@ una@@ ara , kw@@ akw@@ ila n@@ iba@@ f@@ wa bo@@ osi mul@@ uk@@ ongo lwa S@@ il@@ o@@ amu , bal@@ i , abon@@ o@@ oni okh@@ ush@@ ila aband@@ u bo@@ osi aba@@ m@@ eny@@ anga mu , Yerusalemu ?\n",
            "E@@ s@@ ie omw@@ ene end@@ i omut@@ erer@@ eri kh@@ ub@@ uk@@ hal@@ aban@@ ib@@ we , obut@@ iny@@ u kh@@ ul@@ weny@@ we nende kh@@ ulwa aband@@ u abal@@ i mu , L@@ a@@ od@@ ik@@ ia nende kh@@ ulwa abo abal@@ i H@@ i@@ er@@ ap@@ ol@@ i .\n",
            "\n",
            "\n",
            "==> train.en <==\n",
            "But if anyone loves God , this one is known by Him .\n",
            "And the second is like it : ‘ You shall love your neighbor as yourself . ’\n",
            "until the day in which He was taken up , after He through the Holy Spirit had given commandments to the apostles whom He had chosen ,\n",
            "For what if some did not believe ? Will their unbelief make the faithfulness of God without effect ?\n",
            "And when you go into a household , greet it .\n",
            "And a very great multitude spread their clothes on the road ; others cut down branches from the trees and spread them on the road .\n",
            "And we heard this voice which came from heaven when we were with Him on the holy mountain .\n",
            "Or those eighteen on whom the tower in Siloam fell and killed them , do you think that they were worse sinners than all other men who dwelt in Jerusalem ?\n",
            "For I bear him witness that he has a great zeal for you , and those who are in Laodicea , and those in Hierapolis .\n",
            "\n",
            "\n",
            "==> train.lg_rw_lh <==\n",
            "Nebutswa , omundu yesi ouheela Nyasaye , omwenoyo Nyasaye , yamumanya .\n",
            "Eliakhabili elili shinga elo , ndielino mbu , ‘ Oheele owashio shinga olwa wiheela , omwene . ’\n",
            "okhuula , khunyanga eya yabukulilwakhwo nayilwa mwikulu . Ne , nashili okhuyilwa mwikulu , yechesia aba yali niyaahula , okhuba abarumebe mubunyali bwa Roho Omutakatifu. ,\n",
            "Nebutswa abandi khubo shibali , abasuubilwa ta . Kho kano kakhamanyisia mbu , Nyasaye , shiali omusuubilwa ta noho ?\n",
            "Ne olwa mwinjila munzu mubashiesie , omulembe .\n",
            "Abandu abanji nibaala , ebifwalo biabwe khumuhanda , ne abandi nibarema amasaka kemisaala nibaala khumuhanda okwo .\n",
            "Khwali nikhuli , ninaye khushikulu eshitakatifu olwa khwahulila omwoyo , nikurula mwikulu ewa Nyasaye .\n",
            "Noho , mupaaranga mbu , abandu ekhumi namunane bomunaara , kwakwila nibafwa boosi mulukongo lwa Siloamu , bali , abonooni okhushila abandu boosi abamenyanga mu , Yerusalemu ?\n",
            "Esie omwene endi omuterereri khubukhalabanibwe , obutinyu khulwenywe nende khulwa abandu abali mu , Laodikia nende khulwa abo abali Hierapoli .\n",
            "\n",
            "==> dev.bpe.en <==\n",
            "I do not say this to con@@ dem@@ n ; for I have said before that you are in our hearts , to d@@ ie together and to live together .\n",
            "So when they were fill@@ ed , He said to His disciples , “ G@@ ather up the fr@@ ag@@ ments that remain , so that nothing is l@@ ost . ”\n",
            "When the D@@ ay of P@@ ent@@ ec@@ ost had fully come , they were all with one acc@@ ord in one place .\n",
            "For it has been declar@@ ed to me concer@@ ning you , my bre@@ th@@ ren , by those of Ch@@ lo@@ e ’ s hous@@ eho@@ ld , that there are cont@@ ent@@ ions among you .\n",
            "For “ who has known the mind of the Lord that he may instruc@@ t H@@ im ? ” But we have the mind of Christ .\n",
            "And do not become id@@ ol@@ at@@ ers as were some of them . As it is written , “ The people s@@ at down to eat and dr@@ in@@ k , and r@@ ose up to pl@@ ay . ”\n",
            "Now f@@ ive of them were wise , and f@@ ive were f@@ ool@@ ish .\n",
            "When He op@@ ened the second se@@ al , I heard the second living cre@@ ature saying , “ C@@ ome and see . ”\n",
            "But let n@@ one of you suf@@ fer as a mur@@ d@@ er@@ er , a th@@ ief , an ev@@ il@@ do@@ er , or as a bus@@ y@@ body in other people ’ s matters .\n",
            "\n",
            "\n",
            "==> dev.bpe.lg_rw_lh <==\n",
            "Sh@@ i@@ emb@@ ool@@ anga k@@ ano , kh@@ ulwa ok@@ hum@@ uk@@ hal@@ ach@@ ila esh@@ i@@ ina ta , okh@@ uba , sh@@ inga , nd@@ am@@ ub@@ ool@@ el@@ a kh@@ ale , m@@ uli aba@@ he@@ el@@ wa mun@@ o kh@@ w@@ if@@ we , ne , kh@@ ub@@ et@@ s@@ anga hal@@ ala buli l@@ w@@ osi , k@@ ata n@@ ik@@ h@@ uba abal@@ amu no@@ ho , n@@ ik@@ h@@ uf@@ wa .\n",
            "N@@ e olwa bo@@ osi bal@@ i n@@ ib@@ e@@ kur@@ e yab@@ ool@@ el@@ a abe@@ ec@@ h@@ ib@@ e ari , “ Muk@@ h@@ ung@@ '@@ as@@ ie ebit@@ onye bit@@ ony@@ ile , bi@@ osi , k@@ ho mbu kh@@ ul@@ es@@ he okh@@ us@@ as@@ i@@ ak@@ h@@ wo esh@@ ind@@ u shi@@ osi shi@@ osi ta@@ we . ”\n",
            "N@@ e olwa iny@@ anga ya P@@ end@@ ek@@ ote y@@ ola , abas@@ uub@@ ili bo@@ osi , bak@@ h@@ ung@@ '@@ ana hab@@ undu hal@@ ala .\n",
            "O@@ kh@@ uba abaana be@@ f@@ we aband@@ u b@@ andi ab@@ om@@ un@@ z@@ u e@@ ya K@@ ul@@ o@@ e ba@@ mb@@ ool@@ el@@ e but@@ s@@ wa , hab@@ ul@@ afu mbu , obus@@ ool@@ o buli h@@ ak@@ ari mw@@ iny@@ we@@ . ,\n",
            "Sh@@ inga Am@@ ah@@ andiko kab@@ ool@@ anga mbu “ N@@ iw@@ i@@ ina o@@ um@@ any@@ ile am@@ ap@@ a@@ aro aka Omw@@ ami ? , N@@ iw@@ i@@ ina ou@@ ny@@ ala ok@@ hum@@ uc@@ hel@@ el@@ a ? ” , N@@ eb@@ uts@@ wa if@@ we kh@@ uli nam@@ ay@@ il@@ il@@ is@@ io aka Kristo .\n",
            "no@@ ho k@@ ata , okh@@ w@@ in@@ am@@ ila eb@@ if@@ wan@@ ani , sh@@ inga bal@@ ala kh@@ ub@@ o bak@@ hol@@ a , ta@@ we . Sh@@ inga olwa Am@@ ah@@ andiko kab@@ ool@@ anga mbu , “ Ab@@ and@@ u , b@@ ek@@ h@@ ala h@@ asi okh@@ ul@@ ia l@@ is@@ abo el@@ i@@ am@@ ala lik@@ al@@ uk@@ h@@ ane okh@@ uba , esh@@ if@@ wab@@ wi esh@@ i@@ ob@@ um@@ e@@ esi nende ob@@ uy@@ il@@ ani . ”\n",
            "Bar@@ ano kh@@ ub@@ o , bal@@ i abay@@ ing@@ wa , ne bar@@ ano b@@ andi bal@@ i aba@@ ch@@ esi .\n",
            "Mana E@@ sh@@ im@@ em@@ e shi@@ el@@ ik@@ ond@@ i n@@ ish@@ i@@ i@@ kul@@ a esh@@ iba@@ lik@@ ho , shi@@ ak@@ hab@@ ili ne nim@@ bul@@ ila esh@@ il@@ on@@ je esh@@ i@@ ak@@ hab@@ ili esh@@ il@@ im@@ w@@ oyo n@@ ish@@ ib@@ o@@ ola sh@@ iri , “ Y@@ it@@ sa ! ”\n",
            "N@@ eb@@ uts@@ wa om@@ undu y@@ esi kh@@ w@@ iny@@ we al@@ any@@ as@@ ibwa sh@@ inga , om@@ uy@@ iri , no@@ ho omw@@ if@@ i , no@@ ho omuk@@ hol@@ i wam@@ ak@@ h@@ uwa am@@ abi , no@@ ho om@@ undu we@@ ind@@ ob@@ oyo ta@@ we .\n",
            "\n",
            "\n",
            "==> dev.en <==\n",
            "I do not say this to condemn ; for I have said before that you are in our hearts , to die together and to live together .\n",
            "So when they were filled , He said to His disciples , “ Gather up the fragments that remain , so that nothing is lost . ”\n",
            "When the Day of Pentecost had fully come , they were all with one accord in one place .\n",
            "For it has been declared to me concerning you , my brethren , by those of Chloe ’ s household , that there are contentions among you .\n",
            "For “ who has known the mind of the Lord that he may instruct Him ? ” But we have the mind of Christ .\n",
            "And do not become idolaters as were some of them . As it is written , “ The people sat down to eat and drink , and rose up to play . ”\n",
            "Now five of them were wise , and five were foolish .\n",
            "When He opened the second seal , I heard the second living creature saying , “ Come and see . ”\n",
            "But let none of you suffer as a murderer , a thief , an evildoer , or as a busybody in other people ’ s matters .\n",
            "\n",
            "\n",
            "==> dev.lg_rw_lh <==\n",
            "Shiemboolanga kano , khulwa okhumukhalachila eshiina ta , okhuba , shinga , ndamuboolela khale , muli abaheelwa muno khwifwe , ne , khubetsanga halala buli lwosi , kata nikhuba abalamu noho , nikhufwa .\n",
            "Ne olwa boosi bali nibekure yaboolela abeechibe ari , “ Mukhung'asie ebitonye bitonyile , biosi , kho mbu khuleshe okhusasiakhwo eshindu shiosi shiosi tawe . ”\n",
            "Ne olwa inyanga ya Pendekote yola , abasuubili boosi , bakhung'ana habundu halala .\n",
            "Okhuba abaana befwe abandu bandi abomunzu eya Kuloe bamboolele butswa , habulafu mbu , obusoolo buli hakari mwinywe. ,\n",
            "Shinga Amahandiko kaboolanga mbu “ Niwiina oumanyile amapaaro aka Omwami ? , Niwiina ounyala okhumuchelela ? ” , Nebutswa ifwe khuli namayililisio aka Kristo .\n",
            "noho kata , okhwinamila ebifwanani , shinga balala khubo bakhola , tawe . Shinga olwa Amahandiko kaboolanga mbu , “ Abandu , bekhala hasi okhulia lisabo eliamala likalukhane okhuba , eshifwabwi eshiobumeesi nende obuyilani . ”\n",
            "Barano khubo , bali abayingwa , ne barano bandi bali abachesi .\n",
            "Mana Eshimeme shielikondi nishiikula eshibalikho , shiakhabili ne nimbulila eshilonje eshiakhabili eshilimwoyo nishiboola shiri , “ Yitsa ! ”\n",
            "Nebutswa omundu yesi khwinywe alanyasibwa shinga , omuyiri , noho omwifi , noho omukholi wamakhuwa amabi , noho omundu weindoboyo tawe .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQIhMMtXYkU8"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StGXJ7RuYmub"
      },
      "source": [
        "### Baseline MNMT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Smh87evI9G",
        "cellView": "code"
      },
      "source": [
        "#@title\n",
        "name = '%s%s' % (target_language, source_language)\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{target_language}{source_language}_reverse_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{target_language}\"\n",
        "    trg: \"{source_language}\"\n",
        "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\"\n",
        "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\"\n",
        "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
        "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 1000\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 200\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_reverse_transformer\"\n",
        "    overwrite: True \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual\", source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej5aVkIwwHu7",
        "outputId": "0b3c12f2-6e62-4f2d-abcc-373c65e5769c"
      },
      "source": [
        "# Train the model\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt$src.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-13 09:47:18,455 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-13 09:47:18,522 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-13 09:47:31,882 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-13 09:47:32,193 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-13 09:47:32,324 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-13 09:47:32,915 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-13 09:47:32,915 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-13 09:47:33,310 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-13 09:47:33.555721: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-13 09:47:37,600 - INFO - joeynmt.training - Total params: 12179456\n",
            "2021-07-13 09:47:48,366 - INFO - joeynmt.helpers - cfg.name                           : lg_rw_lhen_reverse_transformer\n",
            "2021-07-13 09:47:48,366 - INFO - joeynmt.helpers - cfg.data.src                       : lg_rw_lh\n",
            "2021-07-13 09:47:48,367 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-13 09:47:48,367 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\n",
            "2021-07-13 09:47:48,367 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\n",
            "2021-07-13 09:47:48,367 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\n",
            "2021-07-13 09:47:48,367 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-13 09:47:48,367 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-13 09:47:48,368 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-13 09:47:48,368 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-13 09:47:48,368 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-13 09:47:48,368 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-13 09:47:48,368 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-13 09:47:48,368 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-13 09:47:48,368 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-13 09:47:48,368 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-13 09:47:48,369 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-13 09:47:48,369 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-13 09:47:48,369 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-13 09:47:48,369 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-13 09:47:48,369 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-13 09:47:48,369 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-13 09:47:48,369 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-13 09:47:48,370 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-13 09:47:48,370 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-13 09:47:48,370 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-13 09:47:48,370 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-13 09:47:48,370 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-13 09:47:48,370 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-13 09:47:48,370 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-13 09:47:48,370 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-13 09:47:48,371 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-13 09:47:48,371 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-13 09:47:48,371 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-13 09:47:48,371 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-13 09:47:48,371 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-13 09:47:48,371 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-13 09:47:48,371 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_rw_lhen_reverse_transformer\n",
            "2021-07-13 09:47:48,372 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-13 09:47:48,372 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-13 09:47:48,372 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-13 09:47:48,372 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-13 09:47:48,372 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-13 09:47:48,372 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-13 09:47:48,372 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-13 09:47:48,372 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-13 09:47:48,373 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-13 09:47:48,373 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-13 09:47:48,373 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-13 09:47:48,373 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-13 09:47:48,373 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-13 09:47:48,373 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-13 09:47:48,373 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-13 09:47:48,374 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-13 09:47:48,374 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 09:47:48,374 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-13 09:47:48,374 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-13 09:47:48,374 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-13 09:47:48,374 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-13 09:47:48,374 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-13 09:47:48,375 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-13 09:47:48,375 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-13 09:47:48,375 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-13 09:47:48,375 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 09:47:48,375 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-13 09:47:48,375 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-13 09:47:48,375 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-13 09:47:48,375 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-13 09:47:48,376 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-13 09:47:48,377 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 665203,\n",
            "\tvalid 3000,\n",
            "\ttest 1000\n",
            "2021-07-13 09:47:48,380 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ at@@ andika okuk@@ olera ku m@@ azima ge nn@@ ali nj@@ iga , era nn@@ ak@@ ir@@ aba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obuf@@ uzi n’@@ okul@@ eka em@@ ikw@@ ano em@@ ibi gye nn@@ alina .\n",
            "\t[TRG] Ev@@ ent@@ ually , however , the tr@@ uth@@ s I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my polit@@ ical view@@ po@@ in@@ ts and associ@@ ations .\n",
            "2021-07-13 09:47:48,380 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 09:47:48,380 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 09:47:48,380 - INFO - joeynmt.helpers - Number of Src words (types): 4372\n",
            "2021-07-13 09:47:48,381 - INFO - joeynmt.helpers - Number of Trg words (types): 4372\n",
            "2021-07-13 09:47:48,381 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4372),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4372))\n",
            "2021-07-13 09:47:48,391 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-13 09:47:48,392 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-13 09:48:16,165 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.461176, Tokens per Sec:    15508, Lr: 0.000300\n",
            "2021-07-13 09:48:42,980 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.155313, Tokens per Sec:    16338, Lr: 0.000300\n",
            "2021-07-13 09:49:10,337 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.700678, Tokens per Sec:    15816, Lr: 0.000300\n",
            "2021-07-13 09:49:38,325 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.617236, Tokens per Sec:    15655, Lr: 0.000300\n",
            "2021-07-13 09:50:06,550 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.536601, Tokens per Sec:    15429, Lr: 0.000300\n",
            "2021-07-13 09:50:34,254 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.250369, Tokens per Sec:    15627, Lr: 0.000300\n",
            "2021-07-13 09:51:02,013 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.104108, Tokens per Sec:    15495, Lr: 0.000300\n",
            "2021-07-13 09:51:30,028 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.114763, Tokens per Sec:    15472, Lr: 0.000300\n",
            "2021-07-13 09:51:58,035 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.972508, Tokens per Sec:    15513, Lr: 0.000300\n",
            "2021-07-13 09:52:25,895 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.703464, Tokens per Sec:    15676, Lr: 0.000300\n",
            "2021-07-13 09:52:53,636 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.753563, Tokens per Sec:    15605, Lr: 0.000300\n",
            "2021-07-13 09:53:21,706 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.745850, Tokens per Sec:    15601, Lr: 0.000300\n",
            "2021-07-13 09:53:49,824 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.776247, Tokens per Sec:    15598, Lr: 0.000300\n",
            "2021-07-13 09:54:17,700 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.722679, Tokens per Sec:    15396, Lr: 0.000300\n",
            "2021-07-13 09:54:45,601 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.979302, Tokens per Sec:    15617, Lr: 0.000300\n",
            "2021-07-13 09:55:13,523 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.508346, Tokens per Sec:    15595, Lr: 0.000300\n",
            "2021-07-13 09:55:41,666 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.554290, Tokens per Sec:    15857, Lr: 0.000300\n",
            "2021-07-13 09:56:09,435 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.587025, Tokens per Sec:    15525, Lr: 0.000300\n",
            "2021-07-13 09:56:37,161 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     3.689200, Tokens per Sec:    15551, Lr: 0.000300\n",
            "2021-07-13 09:57:04,934 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     3.486194, Tokens per Sec:    15471, Lr: 0.000300\n",
            "2021-07-13 09:57:33,159 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     3.230875, Tokens per Sec:    15867, Lr: 0.000300\n",
            "2021-07-13 09:58:00,945 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     3.344386, Tokens per Sec:    15658, Lr: 0.000300\n",
            "2021-07-13 09:58:28,926 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     3.437128, Tokens per Sec:    15735, Lr: 0.000300\n",
            "2021-07-13 09:58:57,041 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     3.355264, Tokens per Sec:    15775, Lr: 0.000300\n",
            "2021-07-13 09:59:24,742 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     3.459175, Tokens per Sec:    15543, Lr: 0.000300\n",
            "2021-07-13 10:01:25,030 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 10:01:25,031 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 10:01:25,031 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 10:01:26,010 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 10:01:26,011 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 10:01:26,837 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 10:01:26,838 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 10:01:26,838 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 10:01:26,838 - INFO - joeynmt.training - \tHypothesis: [ Both ] is not a source of the source of the source of God , and he will be source of God . ”\n",
            "2021-07-13 10:01:26,839 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 10:01:26,839 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 10:01:26,839 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 10:01:26,839 - INFO - joeynmt.training - \tHypothesis: Jesus was not a source of the resurrection , but he said : “ I am my Father , I am my Father , and I am my Father . ”\n",
            "2021-07-13 10:01:26,840 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 10:01:26,840 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 10:01:26,840 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 10:01:26,840 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah show that Jesus did not do ?\n",
            "2021-07-13 10:01:26,840 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 10:01:26,841 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 10:01:26,841 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 10:01:26,841 - INFO - joeynmt.training - \tHypothesis: But the suggle was a family .\n",
            "2021-07-13 10:01:26,841 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     5000: bleu:   3.54, loss: 294844.9688, ppl:  26.5426, duration: 122.0993s\n",
            "2021-07-13 10:01:55,022 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     3.153459, Tokens per Sec:    15452, Lr: 0.000300\n",
            "2021-07-13 10:02:22,683 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     3.373771, Tokens per Sec:    15623, Lr: 0.000300\n",
            "2021-07-13 10:02:50,573 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     3.276028, Tokens per Sec:    15591, Lr: 0.000300\n",
            "2021-07-13 10:03:18,359 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     3.259942, Tokens per Sec:    15568, Lr: 0.000300\n",
            "2021-07-13 10:03:46,436 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     2.837429, Tokens per Sec:    15671, Lr: 0.000300\n",
            "2021-07-13 10:04:14,378 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     2.878761, Tokens per Sec:    15558, Lr: 0.000300\n",
            "2021-07-13 10:04:42,299 - INFO - joeynmt.training - Epoch   1, Step:     6400, Batch Loss:     3.010859, Tokens per Sec:    15598, Lr: 0.000300\n",
            "2021-07-13 10:05:10,526 - INFO - joeynmt.training - Epoch   1, Step:     6600, Batch Loss:     3.343693, Tokens per Sec:    15821, Lr: 0.000300\n",
            "2021-07-13 10:05:38,536 - INFO - joeynmt.training - Epoch   1, Step:     6800, Batch Loss:     3.434390, Tokens per Sec:    15661, Lr: 0.000300\n",
            "2021-07-13 10:06:06,601 - INFO - joeynmt.training - Epoch   1, Step:     7000, Batch Loss:     3.353863, Tokens per Sec:    15699, Lr: 0.000300\n",
            "2021-07-13 10:06:34,623 - INFO - joeynmt.training - Epoch   1, Step:     7200, Batch Loss:     3.149697, Tokens per Sec:    15636, Lr: 0.000300\n",
            "2021-07-13 10:07:02,316 - INFO - joeynmt.training - Epoch   1, Step:     7400, Batch Loss:     2.910327, Tokens per Sec:    15363, Lr: 0.000300\n",
            "2021-07-13 10:07:30,406 - INFO - joeynmt.training - Epoch   1, Step:     7600, Batch Loss:     3.229632, Tokens per Sec:    15674, Lr: 0.000300\n",
            "2021-07-13 10:07:58,218 - INFO - joeynmt.training - Epoch   1, Step:     7800, Batch Loss:     3.169799, Tokens per Sec:    15473, Lr: 0.000300\n",
            "2021-07-13 10:08:26,248 - INFO - joeynmt.training - Epoch   1, Step:     8000, Batch Loss:     2.809634, Tokens per Sec:    15529, Lr: 0.000300\n",
            "2021-07-13 10:08:54,069 - INFO - joeynmt.training - Epoch   1, Step:     8200, Batch Loss:     3.389301, Tokens per Sec:    15387, Lr: 0.000300\n",
            "2021-07-13 10:09:14,659 - INFO - joeynmt.training - Epoch   1: total training loss 30008.49\n",
            "2021-07-13 10:09:14,659 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-13 10:09:22,978 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     3.117677, Tokens per Sec:    13415, Lr: 0.000300\n",
            "2021-07-13 10:09:50,635 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     3.000438, Tokens per Sec:    15559, Lr: 0.000300\n",
            "2021-07-13 10:10:18,547 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     2.925918, Tokens per Sec:    15421, Lr: 0.000300\n",
            "2021-07-13 10:10:46,593 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     2.849406, Tokens per Sec:    15685, Lr: 0.000300\n",
            "2021-07-13 10:11:14,758 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     2.851726, Tokens per Sec:    15868, Lr: 0.000300\n",
            "2021-07-13 10:11:42,669 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     2.679013, Tokens per Sec:    15433, Lr: 0.000300\n",
            "2021-07-13 10:12:10,115 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     2.962994, Tokens per Sec:    15283, Lr: 0.000300\n",
            "2021-07-13 10:12:38,157 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     2.400512, Tokens per Sec:    15689, Lr: 0.000300\n",
            "2021-07-13 10:13:05,812 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     2.865683, Tokens per Sec:    15441, Lr: 0.000300\n",
            "2021-07-13 10:15:07,707 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 10:15:07,708 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 10:15:07,708 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 10:15:08,721 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 10:15:08,721 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 10:15:09,616 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 10:15:09,617 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 10:15:09,618 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 10:15:09,618 - INFO - joeynmt.training - \tHypothesis: [ BE ] and his own own will be a good way to be safe , ” he is the way of God .\n",
            "2021-07-13 10:15:09,618 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 10:15:09,619 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 10:15:09,619 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 10:15:09,619 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt that he had been resurrected to heaven , he said : “ I am my Father and my Father , and I am my Father . ”\n",
            "2021-07-13 10:15:09,619 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 10:15:09,620 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 10:15:09,620 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 10:15:09,620 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah give his confidence ?\n",
            "2021-07-13 10:15:09,620 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 10:15:09,621 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 10:15:09,621 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 10:15:09,621 - INFO - joeynmt.training - \tHypothesis: But the two of the family was not to be able to make a family .\n",
            "2021-07-13 10:15:09,622 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    10000: bleu:   5.78, loss: 262393.8750, ppl:  18.5021, duration: 123.8090s\n",
            "2021-07-13 10:15:37,828 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     2.840897, Tokens per Sec:    15460, Lr: 0.000300\n",
            "2021-07-13 10:16:05,765 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     3.119127, Tokens per Sec:    15425, Lr: 0.000300\n",
            "2021-07-13 10:16:33,656 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     2.902354, Tokens per Sec:    15601, Lr: 0.000300\n",
            "2021-07-13 10:17:01,728 - INFO - joeynmt.training - Epoch   2, Step:    10800, Batch Loss:     2.837122, Tokens per Sec:    15551, Lr: 0.000300\n",
            "2021-07-13 10:17:29,760 - INFO - joeynmt.training - Epoch   2, Step:    11000, Batch Loss:     2.805578, Tokens per Sec:    15652, Lr: 0.000300\n",
            "2021-07-13 10:17:57,622 - INFO - joeynmt.training - Epoch   2, Step:    11200, Batch Loss:     3.018280, Tokens per Sec:    15722, Lr: 0.000300\n",
            "2021-07-13 10:18:25,329 - INFO - joeynmt.training - Epoch   2, Step:    11400, Batch Loss:     2.104411, Tokens per Sec:    15493, Lr: 0.000300\n",
            "2021-07-13 10:18:53,088 - INFO - joeynmt.training - Epoch   2, Step:    11600, Batch Loss:     2.857669, Tokens per Sec:    15479, Lr: 0.000300\n",
            "2021-07-13 10:19:21,102 - INFO - joeynmt.training - Epoch   2, Step:    11800, Batch Loss:     3.053420, Tokens per Sec:    15644, Lr: 0.000300\n",
            "2021-07-13 10:19:49,140 - INFO - joeynmt.training - Epoch   2, Step:    12000, Batch Loss:     2.928559, Tokens per Sec:    15636, Lr: 0.000300\n",
            "2021-07-13 10:20:17,223 - INFO - joeynmt.training - Epoch   2, Step:    12200, Batch Loss:     2.802934, Tokens per Sec:    15753, Lr: 0.000300\n",
            "2021-07-13 10:20:45,207 - INFO - joeynmt.training - Epoch   2, Step:    12400, Batch Loss:     2.972525, Tokens per Sec:    15625, Lr: 0.000300\n",
            "2021-07-13 10:21:12,870 - INFO - joeynmt.training - Epoch   2, Step:    12600, Batch Loss:     2.736430, Tokens per Sec:    15388, Lr: 0.000300\n",
            "2021-07-13 10:21:40,690 - INFO - joeynmt.training - Epoch   2, Step:    12800, Batch Loss:     2.623102, Tokens per Sec:    15444, Lr: 0.000300\n",
            "2021-07-13 10:22:08,322 - INFO - joeynmt.training - Epoch   2, Step:    13000, Batch Loss:     2.823604, Tokens per Sec:    15541, Lr: 0.000300\n",
            "2021-07-13 10:22:36,268 - INFO - joeynmt.training - Epoch   2, Step:    13200, Batch Loss:     2.821399, Tokens per Sec:    15793, Lr: 0.000300\n",
            "2021-07-13 10:23:03,960 - INFO - joeynmt.training - Epoch   2, Step:    13400, Batch Loss:     2.629812, Tokens per Sec:    15518, Lr: 0.000300\n",
            "2021-07-13 10:23:31,894 - INFO - joeynmt.training - Epoch   2, Step:    13600, Batch Loss:     2.870287, Tokens per Sec:    15822, Lr: 0.000300\n",
            "2021-07-13 10:23:59,935 - INFO - joeynmt.training - Epoch   2, Step:    13800, Batch Loss:     2.662868, Tokens per Sec:    15690, Lr: 0.000300\n",
            "2021-07-13 10:24:28,026 - INFO - joeynmt.training - Epoch   2, Step:    14000, Batch Loss:     3.171040, Tokens per Sec:    15554, Lr: 0.000300\n",
            "2021-07-13 10:24:55,871 - INFO - joeynmt.training - Epoch   2, Step:    14200, Batch Loss:     2.804111, Tokens per Sec:    15595, Lr: 0.000300\n",
            "2021-07-13 10:25:23,543 - INFO - joeynmt.training - Epoch   2, Step:    14400, Batch Loss:     2.507297, Tokens per Sec:    15383, Lr: 0.000300\n",
            "2021-07-13 10:25:51,441 - INFO - joeynmt.training - Epoch   2, Step:    14600, Batch Loss:     2.886579, Tokens per Sec:    15553, Lr: 0.000300\n",
            "2021-07-13 10:26:19,216 - INFO - joeynmt.training - Epoch   2, Step:    14800, Batch Loss:     2.799031, Tokens per Sec:    15562, Lr: 0.000300\n",
            "2021-07-13 10:26:47,251 - INFO - joeynmt.training - Epoch   2, Step:    15000, Batch Loss:     2.485071, Tokens per Sec:    15523, Lr: 0.000300\n",
            "2021-07-13 10:28:41,555 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 10:28:41,555 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 10:28:41,556 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 10:28:42,518 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 10:28:42,518 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 10:28:43,345 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 10:28:43,346 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 10:28:43,346 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 10:28:43,346 - INFO - joeynmt.training - \tHypothesis: [ his ] will be good and forgiving , and he will be able to endure in the face of the patience of God . ”\n",
            "2021-07-13 10:28:43,346 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 10:28:43,347 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 10:28:43,347 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 10:28:43,347 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt that he heard the water of heaven and said : “ I am my Son and my Son . ”\n",
            "2021-07-13 10:28:43,347 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 10:28:43,348 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 10:28:43,348 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 10:28:43,348 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah react to Jesus ’ prayers ?\n",
            "2021-07-13 10:28:43,348 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 10:28:43,348 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 10:28:43,349 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 10:28:43,349 - INFO - joeynmt.training - \tHypothesis: But the two two two two Egyptians would be able to make his family .\n",
            "2021-07-13 10:28:43,349 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    15000: bleu:   8.66, loss: 244500.3125, ppl:  15.1637, duration: 116.0973s\n",
            "2021-07-13 10:29:11,739 - INFO - joeynmt.training - Epoch   2, Step:    15200, Batch Loss:     2.540415, Tokens per Sec:    15473, Lr: 0.000300\n",
            "2021-07-13 10:29:39,478 - INFO - joeynmt.training - Epoch   2, Step:    15400, Batch Loss:     2.969036, Tokens per Sec:    15409, Lr: 0.000300\n",
            "2021-07-13 10:30:07,404 - INFO - joeynmt.training - Epoch   2, Step:    15600, Batch Loss:     2.552953, Tokens per Sec:    15626, Lr: 0.000300\n",
            "2021-07-13 10:30:35,508 - INFO - joeynmt.training - Epoch   2, Step:    15800, Batch Loss:     2.708540, Tokens per Sec:    15770, Lr: 0.000300\n",
            "2021-07-13 10:31:03,193 - INFO - joeynmt.training - Epoch   2, Step:    16000, Batch Loss:     2.573830, Tokens per Sec:    15500, Lr: 0.000300\n",
            "2021-07-13 10:31:31,159 - INFO - joeynmt.training - Epoch   2, Step:    16200, Batch Loss:     2.559863, Tokens per Sec:    15586, Lr: 0.000300\n",
            "2021-07-13 10:31:59,289 - INFO - joeynmt.training - Epoch   2, Step:    16400, Batch Loss:     2.403666, Tokens per Sec:    15818, Lr: 0.000300\n",
            "2021-07-13 10:32:27,155 - INFO - joeynmt.training - Epoch   2, Step:    16600, Batch Loss:     2.269616, Tokens per Sec:    15660, Lr: 0.000300\n",
            "2021-07-13 10:32:42,493 - INFO - joeynmt.training - Epoch   2: total training loss 23064.22\n",
            "2021-07-13 10:32:42,493 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-13 10:32:56,183 - INFO - joeynmt.training - Epoch   3, Step:    16800, Batch Loss:     2.317991, Tokens per Sec:    14552, Lr: 0.000300\n",
            "2021-07-13 10:33:23,913 - INFO - joeynmt.training - Epoch   3, Step:    17000, Batch Loss:     2.608211, Tokens per Sec:    15531, Lr: 0.000300\n",
            "2021-07-13 10:33:51,606 - INFO - joeynmt.training - Epoch   3, Step:    17200, Batch Loss:     2.695565, Tokens per Sec:    15317, Lr: 0.000300\n",
            "2021-07-13 10:34:19,345 - INFO - joeynmt.training - Epoch   3, Step:    17400, Batch Loss:     2.479253, Tokens per Sec:    15589, Lr: 0.000300\n",
            "2021-07-13 10:34:47,039 - INFO - joeynmt.training - Epoch   3, Step:    17600, Batch Loss:     2.139179, Tokens per Sec:    15421, Lr: 0.000300\n",
            "2021-07-13 10:35:15,003 - INFO - joeynmt.training - Epoch   3, Step:    17800, Batch Loss:     2.673035, Tokens per Sec:    15743, Lr: 0.000300\n",
            "2021-07-13 10:35:42,651 - INFO - joeynmt.training - Epoch   3, Step:    18000, Batch Loss:     2.476496, Tokens per Sec:    15483, Lr: 0.000300\n",
            "2021-07-13 10:36:10,225 - INFO - joeynmt.training - Epoch   3, Step:    18200, Batch Loss:     2.617956, Tokens per Sec:    15321, Lr: 0.000300\n",
            "2021-07-13 10:36:38,106 - INFO - joeynmt.training - Epoch   3, Step:    18400, Batch Loss:     2.794114, Tokens per Sec:    15595, Lr: 0.000300\n",
            "2021-07-13 10:37:05,864 - INFO - joeynmt.training - Epoch   3, Step:    18600, Batch Loss:     2.279640, Tokens per Sec:    15547, Lr: 0.000300\n",
            "2021-07-13 10:37:33,940 - INFO - joeynmt.training - Epoch   3, Step:    18800, Batch Loss:     2.433968, Tokens per Sec:    15809, Lr: 0.000300\n",
            "2021-07-13 10:38:01,622 - INFO - joeynmt.training - Epoch   3, Step:    19000, Batch Loss:     2.361533, Tokens per Sec:    15541, Lr: 0.000300\n",
            "2021-07-13 10:38:29,555 - INFO - joeynmt.training - Epoch   3, Step:    19200, Batch Loss:     2.692389, Tokens per Sec:    15732, Lr: 0.000300\n",
            "2021-07-13 10:38:57,508 - INFO - joeynmt.training - Epoch   3, Step:    19400, Batch Loss:     2.674381, Tokens per Sec:    15445, Lr: 0.000300\n",
            "2021-07-13 10:39:25,430 - INFO - joeynmt.training - Epoch   3, Step:    19600, Batch Loss:     2.741727, Tokens per Sec:    15702, Lr: 0.000300\n",
            "2021-07-13 10:39:53,069 - INFO - joeynmt.training - Epoch   3, Step:    19800, Batch Loss:     2.789575, Tokens per Sec:    15527, Lr: 0.000300\n",
            "2021-07-13 10:40:20,891 - INFO - joeynmt.training - Epoch   3, Step:    20000, Batch Loss:     2.548798, Tokens per Sec:    15502, Lr: 0.000300\n",
            "2021-07-13 10:42:10,112 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 10:42:10,113 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 10:42:10,113 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 10:42:11,055 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 10:42:11,055 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 10:42:11,825 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 10:42:11,825 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 10:42:11,826 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 10:42:11,826 - INFO - joeynmt.training - \tHypothesis: [ PRON ] He has given you a good good way to forgive , and he will be called to God . ”\n",
            "2021-07-13 10:42:11,826 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 10:42:11,826 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 10:42:11,827 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 10:42:11,827 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt that he heard the voice of heaven , saying : “ You must love your son , and my son is my son . ”\n",
            "2021-07-13 10:42:11,827 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 10:42:11,827 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 10:42:11,827 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 10:42:11,828 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ?\n",
            "2021-07-13 10:42:11,828 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 10:42:11,828 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 10:42:11,828 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 10:42:11,828 - INFO - joeynmt.training - \tHypothesis: But Absalom was to seek his family .\n",
            "2021-07-13 10:42:11,829 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    20000: bleu:  10.26, loss: 232730.0625, ppl:  13.3034, duration: 110.9370s\n",
            "2021-07-13 10:42:39,937 - INFO - joeynmt.training - Epoch   3, Step:    20200, Batch Loss:     2.737113, Tokens per Sec:    15340, Lr: 0.000300\n",
            "2021-07-13 10:43:07,959 - INFO - joeynmt.training - Epoch   3, Step:    20400, Batch Loss:     2.606394, Tokens per Sec:    15530, Lr: 0.000300\n",
            "2021-07-13 10:43:35,805 - INFO - joeynmt.training - Epoch   3, Step:    20600, Batch Loss:     2.505883, Tokens per Sec:    15599, Lr: 0.000300\n",
            "2021-07-13 10:44:03,825 - INFO - joeynmt.training - Epoch   3, Step:    20800, Batch Loss:     2.496174, Tokens per Sec:    15468, Lr: 0.000300\n",
            "2021-07-13 10:44:31,362 - INFO - joeynmt.training - Epoch   3, Step:    21000, Batch Loss:     2.453705, Tokens per Sec:    15447, Lr: 0.000300\n",
            "2021-07-13 10:44:59,444 - INFO - joeynmt.training - Epoch   3, Step:    21200, Batch Loss:     2.478151, Tokens per Sec:    15719, Lr: 0.000300\n",
            "2021-07-13 10:45:27,093 - INFO - joeynmt.training - Epoch   3, Step:    21400, Batch Loss:     2.338475, Tokens per Sec:    15434, Lr: 0.000300\n",
            "2021-07-13 10:45:54,883 - INFO - joeynmt.training - Epoch   3, Step:    21600, Batch Loss:     2.572525, Tokens per Sec:    15571, Lr: 0.000300\n",
            "2021-07-13 10:46:22,630 - INFO - joeynmt.training - Epoch   3, Step:    21800, Batch Loss:     2.362186, Tokens per Sec:    15613, Lr: 0.000300\n",
            "2021-07-13 10:46:50,486 - INFO - joeynmt.training - Epoch   3, Step:    22000, Batch Loss:     2.522402, Tokens per Sec:    15582, Lr: 0.000300\n",
            "2021-07-13 10:47:18,309 - INFO - joeynmt.training - Epoch   3, Step:    22200, Batch Loss:     2.188221, Tokens per Sec:    15639, Lr: 0.000300\n",
            "2021-07-13 10:47:46,550 - INFO - joeynmt.training - Epoch   3, Step:    22400, Batch Loss:     2.661230, Tokens per Sec:    15809, Lr: 0.000300\n",
            "2021-07-13 10:48:14,492 - INFO - joeynmt.training - Epoch   3, Step:    22600, Batch Loss:     2.448855, Tokens per Sec:    15560, Lr: 0.000300\n",
            "2021-07-13 10:48:42,550 - INFO - joeynmt.training - Epoch   3, Step:    22800, Batch Loss:     2.362969, Tokens per Sec:    15572, Lr: 0.000300\n",
            "2021-07-13 10:49:10,577 - INFO - joeynmt.training - Epoch   3, Step:    23000, Batch Loss:     2.545396, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-13 10:49:38,247 - INFO - joeynmt.training - Epoch   3, Step:    23200, Batch Loss:     2.112744, Tokens per Sec:    15431, Lr: 0.000300\n",
            "2021-07-13 10:50:06,169 - INFO - joeynmt.training - Epoch   3, Step:    23400, Batch Loss:     2.584925, Tokens per Sec:    15648, Lr: 0.000300\n",
            "2021-07-13 10:50:34,104 - INFO - joeynmt.training - Epoch   3, Step:    23600, Batch Loss:     2.148332, Tokens per Sec:    15500, Lr: 0.000300\n",
            "2021-07-13 10:51:02,099 - INFO - joeynmt.training - Epoch   3, Step:    23800, Batch Loss:     2.200854, Tokens per Sec:    15630, Lr: 0.000300\n",
            "2021-07-13 10:51:30,062 - INFO - joeynmt.training - Epoch   3, Step:    24000, Batch Loss:     2.887360, Tokens per Sec:    15435, Lr: 0.000300\n",
            "2021-07-13 10:51:58,086 - INFO - joeynmt.training - Epoch   3, Step:    24200, Batch Loss:     2.764742, Tokens per Sec:    15588, Lr: 0.000300\n",
            "2021-07-13 10:52:26,309 - INFO - joeynmt.training - Epoch   3, Step:    24400, Batch Loss:     2.468667, Tokens per Sec:    15527, Lr: 0.000300\n",
            "2021-07-13 10:52:54,513 - INFO - joeynmt.training - Epoch   3, Step:    24600, Batch Loss:     2.481128, Tokens per Sec:    15623, Lr: 0.000300\n",
            "2021-07-13 10:53:22,787 - INFO - joeynmt.training - Epoch   3, Step:    24800, Batch Loss:     2.443336, Tokens per Sec:    15660, Lr: 0.000300\n",
            "2021-07-13 10:53:50,940 - INFO - joeynmt.training - Epoch   3, Step:    25000, Batch Loss:     2.660616, Tokens per Sec:    15670, Lr: 0.000300\n",
            "2021-07-13 10:55:35,494 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 10:55:35,494 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 10:55:35,495 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 10:55:36,411 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 10:55:36,412 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 10:55:37,421 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 10:55:37,423 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 10:55:37,423 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 10:55:37,423 - INFO - joeynmt.training - \tHypothesis: [ BE ] He has good and forgiving his endurance , and he will be released by God . ”\n",
            "2021-07-13 10:55:37,423 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 10:55:37,424 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 10:55:37,424 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 10:55:37,424 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice of heaven , saying : “ You are my Son , and I have loved you . ”\n",
            "2021-07-13 10:55:37,424 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 10:55:37,425 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 10:55:37,425 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 10:55:37,425 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 10:55:37,425 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 10:55:37,426 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 10:55:37,426 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 10:55:37,426 - INFO - joeynmt.training - \tHypothesis: But Abram had to seek his family .\n",
            "2021-07-13 10:55:37,426 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    25000: bleu:  11.84, loss: 224283.9531, ppl:  12.1108, duration: 106.4859s\n",
            "2021-07-13 10:55:49,329 - INFO - joeynmt.training - Epoch   3: total training loss 20888.24\n",
            "2021-07-13 10:55:49,330 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-13 10:56:06,531 - INFO - joeynmt.training - Epoch   4, Step:    25200, Batch Loss:     1.907555, Tokens per Sec:    14530, Lr: 0.000300\n",
            "2021-07-13 10:56:34,196 - INFO - joeynmt.training - Epoch   4, Step:    25400, Batch Loss:     2.346124, Tokens per Sec:    15714, Lr: 0.000300\n",
            "2021-07-13 10:57:02,144 - INFO - joeynmt.training - Epoch   4, Step:    25600, Batch Loss:     2.510173, Tokens per Sec:    15543, Lr: 0.000300\n",
            "2021-07-13 10:57:29,853 - INFO - joeynmt.training - Epoch   4, Step:    25800, Batch Loss:     2.342729, Tokens per Sec:    15490, Lr: 0.000300\n",
            "2021-07-13 10:57:57,977 - INFO - joeynmt.training - Epoch   4, Step:    26000, Batch Loss:     2.377973, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-13 10:58:25,691 - INFO - joeynmt.training - Epoch   4, Step:    26200, Batch Loss:     2.513595, Tokens per Sec:    15247, Lr: 0.000300\n",
            "2021-07-13 10:58:53,832 - INFO - joeynmt.training - Epoch   4, Step:    26400, Batch Loss:     2.302679, Tokens per Sec:    15674, Lr: 0.000300\n",
            "2021-07-13 10:59:21,724 - INFO - joeynmt.training - Epoch   4, Step:    26600, Batch Loss:     2.273677, Tokens per Sec:    15524, Lr: 0.000300\n",
            "2021-07-13 10:59:49,404 - INFO - joeynmt.training - Epoch   4, Step:    26800, Batch Loss:     2.367591, Tokens per Sec:    15463, Lr: 0.000300\n",
            "2021-07-13 11:00:17,346 - INFO - joeynmt.training - Epoch   4, Step:    27000, Batch Loss:     2.405451, Tokens per Sec:    15804, Lr: 0.000300\n",
            "2021-07-13 11:00:45,166 - INFO - joeynmt.training - Epoch   4, Step:    27200, Batch Loss:     2.583302, Tokens per Sec:    15532, Lr: 0.000300\n",
            "2021-07-13 11:01:12,868 - INFO - joeynmt.training - Epoch   4, Step:    27400, Batch Loss:     2.233958, Tokens per Sec:    15357, Lr: 0.000300\n",
            "2021-07-13 11:01:40,929 - INFO - joeynmt.training - Epoch   4, Step:    27600, Batch Loss:     2.262606, Tokens per Sec:    15648, Lr: 0.000300\n",
            "2021-07-13 11:02:08,713 - INFO - joeynmt.training - Epoch   4, Step:    27800, Batch Loss:     2.799650, Tokens per Sec:    15454, Lr: 0.000300\n",
            "2021-07-13 11:02:36,447 - INFO - joeynmt.training - Epoch   4, Step:    28000, Batch Loss:     2.357432, Tokens per Sec:    15589, Lr: 0.000300\n",
            "2021-07-13 11:03:04,384 - INFO - joeynmt.training - Epoch   4, Step:    28200, Batch Loss:     2.428806, Tokens per Sec:    15595, Lr: 0.000300\n",
            "2021-07-13 11:03:32,276 - INFO - joeynmt.training - Epoch   4, Step:    28400, Batch Loss:     1.950100, Tokens per Sec:    15528, Lr: 0.000300\n",
            "2021-07-13 11:04:00,049 - INFO - joeynmt.training - Epoch   4, Step:    28600, Batch Loss:     2.448566, Tokens per Sec:    15530, Lr: 0.000300\n",
            "2021-07-13 11:04:27,827 - INFO - joeynmt.training - Epoch   4, Step:    28800, Batch Loss:     2.448785, Tokens per Sec:    15384, Lr: 0.000300\n",
            "2021-07-13 11:04:55,924 - INFO - joeynmt.training - Epoch   4, Step:    29000, Batch Loss:     2.622930, Tokens per Sec:    15507, Lr: 0.000300\n",
            "2021-07-13 11:05:24,138 - INFO - joeynmt.training - Epoch   4, Step:    29200, Batch Loss:     2.032394, Tokens per Sec:    15729, Lr: 0.000300\n",
            "2021-07-13 11:05:52,300 - INFO - joeynmt.training - Epoch   4, Step:    29400, Batch Loss:     2.321960, Tokens per Sec:    15632, Lr: 0.000300\n",
            "2021-07-13 11:06:20,536 - INFO - joeynmt.training - Epoch   4, Step:    29600, Batch Loss:     2.383850, Tokens per Sec:    15647, Lr: 0.000300\n",
            "2021-07-13 11:06:48,607 - INFO - joeynmt.training - Epoch   4, Step:    29800, Batch Loss:     2.008789, Tokens per Sec:    15617, Lr: 0.000300\n",
            "2021-07-13 11:07:16,588 - INFO - joeynmt.training - Epoch   4, Step:    30000, Batch Loss:     2.377008, Tokens per Sec:    15540, Lr: 0.000300\n",
            "2021-07-13 11:09:08,995 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:09:08,996 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:09:08,996 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:09:09,958 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:09:09,959 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:09:10,963 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:09:10,964 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 11:09:10,965 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 11:09:10,965 - INFO - joeynmt.training - \tHypothesis: [ He ] has given you good and forgiving one another when he is patient , he is the one who is called God . ”\n",
            "2021-07-13 11:09:10,965 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:09:10,965 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 11:09:10,965 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 11:09:10,966 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ You are my Son , and my son , who is grateful . ”\n",
            "2021-07-13 11:09:10,966 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:09:10,966 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 11:09:10,966 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 11:09:10,966 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 11:09:10,967 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:09:10,967 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 11:09:10,967 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 11:09:10,967 - INFO - joeynmt.training - \tHypothesis: But Absalom had to search for his family .\n",
            "2021-07-13 11:09:10,967 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    30000: bleu:  12.23, loss: 218069.3438, ppl:  11.3021, duration: 114.3786s\n",
            "2021-07-13 11:09:39,044 - INFO - joeynmt.training - Epoch   4, Step:    30200, Batch Loss:     2.537698, Tokens per Sec:    15698, Lr: 0.000300\n",
            "2021-07-13 11:10:06,967 - INFO - joeynmt.training - Epoch   4, Step:    30400, Batch Loss:     2.381675, Tokens per Sec:    15599, Lr: 0.000300\n",
            "2021-07-13 11:10:34,969 - INFO - joeynmt.training - Epoch   4, Step:    30600, Batch Loss:     2.218461, Tokens per Sec:    15641, Lr: 0.000300\n",
            "2021-07-13 11:11:02,982 - INFO - joeynmt.training - Epoch   4, Step:    30800, Batch Loss:     2.313285, Tokens per Sec:    15689, Lr: 0.000300\n",
            "2021-07-13 11:11:30,772 - INFO - joeynmt.training - Epoch   4, Step:    31000, Batch Loss:     2.690371, Tokens per Sec:    15560, Lr: 0.000300\n",
            "2021-07-13 11:11:58,639 - INFO - joeynmt.training - Epoch   4, Step:    31200, Batch Loss:     2.326187, Tokens per Sec:    15442, Lr: 0.000300\n",
            "2021-07-13 11:12:26,433 - INFO - joeynmt.training - Epoch   4, Step:    31400, Batch Loss:     2.390724, Tokens per Sec:    15506, Lr: 0.000300\n",
            "2021-07-13 11:12:54,307 - INFO - joeynmt.training - Epoch   4, Step:    31600, Batch Loss:     2.035066, Tokens per Sec:    15610, Lr: 0.000300\n",
            "2021-07-13 11:13:22,392 - INFO - joeynmt.training - Epoch   4, Step:    31800, Batch Loss:     2.193368, Tokens per Sec:    15395, Lr: 0.000300\n",
            "2021-07-13 11:13:50,903 - INFO - joeynmt.training - Epoch   4, Step:    32000, Batch Loss:     2.091776, Tokens per Sec:    15748, Lr: 0.000300\n",
            "2021-07-13 11:14:18,741 - INFO - joeynmt.training - Epoch   4, Step:    32200, Batch Loss:     2.377280, Tokens per Sec:    15351, Lr: 0.000300\n",
            "2021-07-13 11:14:46,611 - INFO - joeynmt.training - Epoch   4, Step:    32400, Batch Loss:     2.124529, Tokens per Sec:    15530, Lr: 0.000300\n",
            "2021-07-13 11:15:14,569 - INFO - joeynmt.training - Epoch   4, Step:    32600, Batch Loss:     2.477504, Tokens per Sec:    15600, Lr: 0.000300\n",
            "2021-07-13 11:15:42,444 - INFO - joeynmt.training - Epoch   4, Step:    32800, Batch Loss:     2.194011, Tokens per Sec:    15646, Lr: 0.000300\n",
            "2021-07-13 11:16:10,246 - INFO - joeynmt.training - Epoch   4, Step:    33000, Batch Loss:     2.010374, Tokens per Sec:    15826, Lr: 0.000300\n",
            "2021-07-13 11:16:37,979 - INFO - joeynmt.training - Epoch   4, Step:    33200, Batch Loss:     2.326870, Tokens per Sec:    15275, Lr: 0.000300\n",
            "2021-07-13 11:17:06,069 - INFO - joeynmt.training - Epoch   4, Step:    33400, Batch Loss:     2.346767, Tokens per Sec:    15592, Lr: 0.000300\n",
            "2021-07-13 11:17:12,572 - INFO - joeynmt.training - Epoch   4: total training loss 19563.18\n",
            "2021-07-13 11:17:12,572 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-13 11:17:35,312 - INFO - joeynmt.training - Epoch   5, Step:    33600, Batch Loss:     2.569399, Tokens per Sec:    14953, Lr: 0.000300\n",
            "2021-07-13 11:18:03,596 - INFO - joeynmt.training - Epoch   5, Step:    33800, Batch Loss:     2.066869, Tokens per Sec:    15854, Lr: 0.000300\n",
            "2021-07-13 11:18:31,548 - INFO - joeynmt.training - Epoch   5, Step:    34000, Batch Loss:     2.418043, Tokens per Sec:    15651, Lr: 0.000300\n",
            "2021-07-13 11:18:59,563 - INFO - joeynmt.training - Epoch   5, Step:    34200, Batch Loss:     2.189233, Tokens per Sec:    15559, Lr: 0.000300\n",
            "2021-07-13 11:19:27,469 - INFO - joeynmt.training - Epoch   5, Step:    34400, Batch Loss:     1.984988, Tokens per Sec:    15392, Lr: 0.000300\n",
            "2021-07-13 11:19:55,401 - INFO - joeynmt.training - Epoch   5, Step:    34600, Batch Loss:     2.250991, Tokens per Sec:    15393, Lr: 0.000300\n",
            "2021-07-13 11:20:23,644 - INFO - joeynmt.training - Epoch   5, Step:    34800, Batch Loss:     2.303063, Tokens per Sec:    15795, Lr: 0.000300\n",
            "2021-07-13 11:20:51,702 - INFO - joeynmt.training - Epoch   5, Step:    35000, Batch Loss:     2.223382, Tokens per Sec:    15450, Lr: 0.000300\n",
            "2021-07-13 11:22:38,243 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:22:38,243 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:22:38,244 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:22:39,205 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:22:39,205 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:22:40,265 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:22:40,265 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 11:22:40,266 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 11:22:40,266 - INFO - joeynmt.training - \tHypothesis: [ He ] has good and forgiven you , and you will be forgiven to endure , that is the God of God . ”\n",
            "2021-07-13 11:22:40,266 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:22:40,266 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 11:22:40,266 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 11:22:40,267 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ You are my Son , whom I have loved me . ”\n",
            "2021-07-13 11:22:40,267 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:22:40,267 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 11:22:40,267 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 11:22:40,268 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-13 11:22:40,268 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:22:40,268 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 11:22:40,268 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 11:22:40,268 - INFO - joeynmt.training - \tHypothesis: But Absalom did not make the decision to seek his family .\n",
            "2021-07-13 11:22:40,269 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    35000: bleu:  13.58, loss: 212952.4531, ppl:  10.6769, duration: 108.5657s\n",
            "2021-07-13 11:23:08,255 - INFO - joeynmt.training - Epoch   5, Step:    35200, Batch Loss:     2.332809, Tokens per Sec:    15567, Lr: 0.000300\n",
            "2021-07-13 11:23:36,301 - INFO - joeynmt.training - Epoch   5, Step:    35400, Batch Loss:     2.511182, Tokens per Sec:    15782, Lr: 0.000300\n",
            "2021-07-13 11:24:04,175 - INFO - joeynmt.training - Epoch   5, Step:    35600, Batch Loss:     2.355357, Tokens per Sec:    15718, Lr: 0.000300\n",
            "2021-07-13 11:24:32,042 - INFO - joeynmt.training - Epoch   5, Step:    35800, Batch Loss:     2.277037, Tokens per Sec:    15441, Lr: 0.000300\n",
            "2021-07-13 11:25:00,002 - INFO - joeynmt.training - Epoch   5, Step:    36000, Batch Loss:     2.160527, Tokens per Sec:    15594, Lr: 0.000300\n",
            "2021-07-13 11:25:27,540 - INFO - joeynmt.training - Epoch   5, Step:    36200, Batch Loss:     2.197160, Tokens per Sec:    15380, Lr: 0.000300\n",
            "2021-07-13 11:25:55,421 - INFO - joeynmt.training - Epoch   5, Step:    36400, Batch Loss:     2.033073, Tokens per Sec:    15495, Lr: 0.000300\n",
            "2021-07-13 11:26:23,229 - INFO - joeynmt.training - Epoch   5, Step:    36600, Batch Loss:     2.154580, Tokens per Sec:    15337, Lr: 0.000300\n",
            "2021-07-13 11:26:51,023 - INFO - joeynmt.training - Epoch   5, Step:    36800, Batch Loss:     2.172756, Tokens per Sec:    15382, Lr: 0.000300\n",
            "2021-07-13 11:27:18,940 - INFO - joeynmt.training - Epoch   5, Step:    37000, Batch Loss:     2.374693, Tokens per Sec:    15586, Lr: 0.000300\n",
            "2021-07-13 11:27:46,687 - INFO - joeynmt.training - Epoch   5, Step:    37200, Batch Loss:     2.093815, Tokens per Sec:    15499, Lr: 0.000300\n",
            "2021-07-13 11:28:14,573 - INFO - joeynmt.training - Epoch   5, Step:    37400, Batch Loss:     2.137339, Tokens per Sec:    15454, Lr: 0.000300\n",
            "2021-07-13 11:28:42,498 - INFO - joeynmt.training - Epoch   5, Step:    37600, Batch Loss:     2.199673, Tokens per Sec:    15698, Lr: 0.000300\n",
            "2021-07-13 11:29:10,300 - INFO - joeynmt.training - Epoch   5, Step:    37800, Batch Loss:     2.159525, Tokens per Sec:    15553, Lr: 0.000300\n",
            "2021-07-13 11:29:38,162 - INFO - joeynmt.training - Epoch   5, Step:    38000, Batch Loss:     2.238371, Tokens per Sec:    15808, Lr: 0.000300\n",
            "2021-07-13 11:30:05,862 - INFO - joeynmt.training - Epoch   5, Step:    38200, Batch Loss:     2.423992, Tokens per Sec:    15642, Lr: 0.000300\n",
            "2021-07-13 11:30:33,658 - INFO - joeynmt.training - Epoch   5, Step:    38400, Batch Loss:     2.352454, Tokens per Sec:    15463, Lr: 0.000300\n",
            "2021-07-13 11:31:01,507 - INFO - joeynmt.training - Epoch   5, Step:    38600, Batch Loss:     2.180190, Tokens per Sec:    15397, Lr: 0.000300\n",
            "2021-07-13 11:31:29,468 - INFO - joeynmt.training - Epoch   5, Step:    38800, Batch Loss:     2.361583, Tokens per Sec:    15494, Lr: 0.000300\n",
            "2021-07-13 11:31:57,209 - INFO - joeynmt.training - Epoch   5, Step:    39000, Batch Loss:     2.672638, Tokens per Sec:    15486, Lr: 0.000300\n",
            "2021-07-13 11:32:25,038 - INFO - joeynmt.training - Epoch   5, Step:    39200, Batch Loss:     2.320584, Tokens per Sec:    15661, Lr: 0.000300\n",
            "2021-07-13 11:32:52,886 - INFO - joeynmt.training - Epoch   5, Step:    39400, Batch Loss:     2.358700, Tokens per Sec:    15367, Lr: 0.000300\n",
            "2021-07-13 11:33:20,581 - INFO - joeynmt.training - Epoch   5, Step:    39600, Batch Loss:     2.073516, Tokens per Sec:    15654, Lr: 0.000300\n",
            "2021-07-13 11:33:48,505 - INFO - joeynmt.training - Epoch   5, Step:    39800, Batch Loss:     2.218490, Tokens per Sec:    15755, Lr: 0.000300\n",
            "2021-07-13 11:34:16,600 - INFO - joeynmt.training - Epoch   5, Step:    40000, Batch Loss:     2.204461, Tokens per Sec:    15737, Lr: 0.000300\n",
            "2021-07-13 11:35:55,471 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:35:55,471 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:35:55,471 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:35:56,437 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:35:56,438 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:35:57,453 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:35:57,454 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 11:35:57,454 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 11:35:57,454 - INFO - joeynmt.training - \tHypothesis: “ If you have done good and forgive you the trials you endure , this is what is acceptable to God . ”\n",
            "2021-07-13 11:35:57,455 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:35:57,455 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 11:35:57,455 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 11:35:57,455 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ I am my Son , whom I have loved . ”\n",
            "2021-07-13 11:35:57,456 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:35:57,456 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 11:35:57,456 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 11:35:57,456 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus responded ?\n",
            "2021-07-13 11:35:57,456 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:35:57,457 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 11:35:57,457 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 11:35:57,457 - INFO - joeynmt.training - \tHypothesis: But Abram did what he did to seek his family .\n",
            "2021-07-13 11:35:57,457 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    40000: bleu:  13.87, loss: 211624.5000, ppl:  10.5204, duration: 100.8565s\n",
            "2021-07-13 11:36:25,637 - INFO - joeynmt.training - Epoch   5, Step:    40200, Batch Loss:     1.946579, Tokens per Sec:    15411, Lr: 0.000300\n",
            "2021-07-13 11:36:53,370 - INFO - joeynmt.training - Epoch   5, Step:    40400, Batch Loss:     1.952347, Tokens per Sec:    15615, Lr: 0.000300\n",
            "2021-07-13 11:37:21,089 - INFO - joeynmt.training - Epoch   5, Step:    40600, Batch Loss:     2.369299, Tokens per Sec:    15631, Lr: 0.000300\n",
            "2021-07-13 11:37:49,044 - INFO - joeynmt.training - Epoch   5, Step:    40800, Batch Loss:     2.083448, Tokens per Sec:    15584, Lr: 0.000300\n",
            "2021-07-13 11:38:16,931 - INFO - joeynmt.training - Epoch   5, Step:    41000, Batch Loss:     2.257182, Tokens per Sec:    15517, Lr: 0.000300\n",
            "2021-07-13 11:38:44,775 - INFO - joeynmt.training - Epoch   5, Step:    41200, Batch Loss:     2.426999, Tokens per Sec:    15592, Lr: 0.000300\n",
            "2021-07-13 11:39:12,623 - INFO - joeynmt.training - Epoch   5, Step:    41400, Batch Loss:     2.067128, Tokens per Sec:    15548, Lr: 0.000300\n",
            "2021-07-13 11:39:40,677 - INFO - joeynmt.training - Epoch   5, Step:    41600, Batch Loss:     2.229288, Tokens per Sec:    15658, Lr: 0.000300\n",
            "2021-07-13 11:40:08,563 - INFO - joeynmt.training - Epoch   5, Step:    41800, Batch Loss:     2.268388, Tokens per Sec:    15557, Lr: 0.000300\n",
            "2021-07-13 11:40:10,569 - INFO - joeynmt.training - Epoch   5: total training loss 18712.86\n",
            "2021-07-13 11:40:10,569 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-13 11:40:37,596 - INFO - joeynmt.training - Epoch   6, Step:    42000, Batch Loss:     2.121250, Tokens per Sec:    14949, Lr: 0.000300\n",
            "2021-07-13 11:41:05,471 - INFO - joeynmt.training - Epoch   6, Step:    42200, Batch Loss:     2.099199, Tokens per Sec:    15635, Lr: 0.000300\n",
            "2021-07-13 11:41:33,287 - INFO - joeynmt.training - Epoch   6, Step:    42400, Batch Loss:     2.054665, Tokens per Sec:    15487, Lr: 0.000300\n",
            "2021-07-13 11:42:01,259 - INFO - joeynmt.training - Epoch   6, Step:    42600, Batch Loss:     2.118937, Tokens per Sec:    15546, Lr: 0.000300\n",
            "2021-07-13 11:42:29,116 - INFO - joeynmt.training - Epoch   6, Step:    42800, Batch Loss:     2.106054, Tokens per Sec:    15500, Lr: 0.000300\n",
            "2021-07-13 11:42:57,068 - INFO - joeynmt.training - Epoch   6, Step:    43000, Batch Loss:     2.371664, Tokens per Sec:    15470, Lr: 0.000300\n",
            "2021-07-13 11:43:24,754 - INFO - joeynmt.training - Epoch   6, Step:    43200, Batch Loss:     2.393798, Tokens per Sec:    15436, Lr: 0.000300\n",
            "2021-07-13 11:43:52,915 - INFO - joeynmt.training - Epoch   6, Step:    43400, Batch Loss:     2.008492, Tokens per Sec:    15722, Lr: 0.000300\n",
            "2021-07-13 11:44:20,796 - INFO - joeynmt.training - Epoch   6, Step:    43600, Batch Loss:     2.148690, Tokens per Sec:    15624, Lr: 0.000300\n",
            "2021-07-13 11:44:48,836 - INFO - joeynmt.training - Epoch   6, Step:    43800, Batch Loss:     2.173325, Tokens per Sec:    15667, Lr: 0.000300\n",
            "2021-07-13 11:45:16,653 - INFO - joeynmt.training - Epoch   6, Step:    44000, Batch Loss:     2.612678, Tokens per Sec:    15517, Lr: 0.000300\n",
            "2021-07-13 11:45:44,699 - INFO - joeynmt.training - Epoch   6, Step:    44200, Batch Loss:     2.099531, Tokens per Sec:    15517, Lr: 0.000300\n",
            "2021-07-13 11:46:12,829 - INFO - joeynmt.training - Epoch   6, Step:    44400, Batch Loss:     1.963603, Tokens per Sec:    15706, Lr: 0.000300\n",
            "2021-07-13 11:46:40,771 - INFO - joeynmt.training - Epoch   6, Step:    44600, Batch Loss:     2.050816, Tokens per Sec:    15338, Lr: 0.000300\n",
            "2021-07-13 11:47:08,733 - INFO - joeynmt.training - Epoch   6, Step:    44800, Batch Loss:     2.162333, Tokens per Sec:    15579, Lr: 0.000300\n",
            "2021-07-13 11:47:36,669 - INFO - joeynmt.training - Epoch   6, Step:    45000, Batch Loss:     2.268706, Tokens per Sec:    15702, Lr: 0.000300\n",
            "2021-07-13 11:49:29,765 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:49:29,765 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:49:29,765 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:49:30,711 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:49:30,711 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:49:31,657 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:49:31,658 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 11:49:31,659 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 11:49:31,659 - INFO - joeynmt.training - \tHypothesis: “ When you have given you good and forgive you the opportunity to endure , what is pleased to God . ”\n",
            "2021-07-13 11:49:31,659 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:49:31,659 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 11:49:31,660 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 11:49:31,660 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ You have become my Son , my beloved Son . ”\n",
            "2021-07-13 11:49:31,660 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:49:31,660 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 11:49:31,660 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 11:49:31,661 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus responded ?\n",
            "2021-07-13 11:49:31,661 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:49:31,661 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 11:49:31,661 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 11:49:31,661 - INFO - joeynmt.training - \tHypothesis: But Abigai did what he did to seek his family .\n",
            "2021-07-13 11:49:31,662 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    45000: bleu:  13.86, loss: 206207.6406, ppl:   9.9054, duration: 114.9920s\n",
            "2021-07-13 11:49:59,822 - INFO - joeynmt.training - Epoch   6, Step:    45200, Batch Loss:     2.210104, Tokens per Sec:    15560, Lr: 0.000300\n",
            "2021-07-13 11:50:27,416 - INFO - joeynmt.training - Epoch   6, Step:    45400, Batch Loss:     2.257234, Tokens per Sec:    15405, Lr: 0.000300\n",
            "2021-07-13 11:50:55,313 - INFO - joeynmt.training - Epoch   6, Step:    45600, Batch Loss:     2.304919, Tokens per Sec:    15713, Lr: 0.000300\n",
            "2021-07-13 11:51:23,130 - INFO - joeynmt.training - Epoch   6, Step:    45800, Batch Loss:     2.141336, Tokens per Sec:    15566, Lr: 0.000300\n",
            "2021-07-13 11:51:51,135 - INFO - joeynmt.training - Epoch   6, Step:    46000, Batch Loss:     2.129848, Tokens per Sec:    15669, Lr: 0.000300\n",
            "2021-07-13 11:52:19,222 - INFO - joeynmt.training - Epoch   6, Step:    46200, Batch Loss:     2.182387, Tokens per Sec:    15621, Lr: 0.000300\n",
            "2021-07-13 11:52:47,051 - INFO - joeynmt.training - Epoch   6, Step:    46400, Batch Loss:     2.256427, Tokens per Sec:    15355, Lr: 0.000300\n",
            "2021-07-13 11:53:14,804 - INFO - joeynmt.training - Epoch   6, Step:    46600, Batch Loss:     2.280073, Tokens per Sec:    15606, Lr: 0.000300\n",
            "2021-07-13 11:53:43,012 - INFO - joeynmt.training - Epoch   6, Step:    46800, Batch Loss:     2.050454, Tokens per Sec:    15721, Lr: 0.000300\n",
            "2021-07-13 11:54:10,901 - INFO - joeynmt.training - Epoch   6, Step:    47000, Batch Loss:     1.960995, Tokens per Sec:    15771, Lr: 0.000300\n",
            "2021-07-13 11:54:38,759 - INFO - joeynmt.training - Epoch   6, Step:    47200, Batch Loss:     2.257626, Tokens per Sec:    15681, Lr: 0.000300\n",
            "2021-07-13 11:55:06,873 - INFO - joeynmt.training - Epoch   6, Step:    47400, Batch Loss:     2.467994, Tokens per Sec:    15663, Lr: 0.000300\n",
            "2021-07-13 11:55:34,556 - INFO - joeynmt.training - Epoch   6, Step:    47600, Batch Loss:     2.099629, Tokens per Sec:    15636, Lr: 0.000300\n",
            "2021-07-13 11:56:02,318 - INFO - joeynmt.training - Epoch   6, Step:    47800, Batch Loss:     2.139217, Tokens per Sec:    15618, Lr: 0.000300\n",
            "2021-07-13 11:56:30,226 - INFO - joeynmt.training - Epoch   6, Step:    48000, Batch Loss:     2.020444, Tokens per Sec:    15832, Lr: 0.000300\n",
            "2021-07-13 11:56:57,792 - INFO - joeynmt.training - Epoch   6, Step:    48200, Batch Loss:     2.281853, Tokens per Sec:    15391, Lr: 0.000300\n",
            "2021-07-13 11:57:25,692 - INFO - joeynmt.training - Epoch   6, Step:    48400, Batch Loss:     2.569948, Tokens per Sec:    15659, Lr: 0.000300\n",
            "2021-07-13 11:57:53,546 - INFO - joeynmt.training - Epoch   6, Step:    48600, Batch Loss:     2.094375, Tokens per Sec:    15835, Lr: 0.000300\n",
            "2021-07-13 11:58:21,457 - INFO - joeynmt.training - Epoch   6, Step:    48800, Batch Loss:     2.101747, Tokens per Sec:    15593, Lr: 0.000300\n",
            "2021-07-13 11:58:49,299 - INFO - joeynmt.training - Epoch   6, Step:    49000, Batch Loss:     2.211871, Tokens per Sec:    15678, Lr: 0.000300\n",
            "2021-07-13 11:59:17,230 - INFO - joeynmt.training - Epoch   6, Step:    49200, Batch Loss:     1.866729, Tokens per Sec:    15524, Lr: 0.000300\n",
            "2021-07-13 11:59:45,135 - INFO - joeynmt.training - Epoch   6, Step:    49400, Batch Loss:     2.087745, Tokens per Sec:    15654, Lr: 0.000300\n",
            "2021-07-13 12:00:13,014 - INFO - joeynmt.training - Epoch   6, Step:    49600, Batch Loss:     2.108392, Tokens per Sec:    15689, Lr: 0.000300\n",
            "2021-07-13 12:00:40,940 - INFO - joeynmt.training - Epoch   6, Step:    49800, Batch Loss:     2.060336, Tokens per Sec:    15641, Lr: 0.000300\n",
            "2021-07-13 12:01:08,798 - INFO - joeynmt.training - Epoch   6, Step:    50000, Batch Loss:     2.336270, Tokens per Sec:    15464, Lr: 0.000300\n",
            "2021-07-13 12:03:00,795 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:03:00,795 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:03:00,795 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:03:01,755 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:03:01,755 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:03:02,749 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:03:02,750 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:03:02,751 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:03:02,751 - INFO - joeynmt.training - \tHypothesis: [ I ] have made good use and forgiving you , and you will be patient , that is what is pleased to God . ”\n",
            "2021-07-13 12:03:02,751 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:03:02,751 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:03:02,752 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:03:02,752 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ You are my Son , whom I have loved . ”\n",
            "2021-07-13 12:03:02,752 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:03:02,752 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:03:02,753 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:03:02,753 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 12:03:02,753 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:03:02,753 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:03:02,754 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:03:02,754 - INFO - joeynmt.training - \tHypothesis: But Abiga’s choice to save his family .\n",
            "2021-07-13 12:03:02,754 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    50000: bleu:  14.48, loss: 203779.2500, ppl:   9.6415, duration: 113.9555s\n",
            "2021-07-13 12:03:26,102 - INFO - joeynmt.training - Epoch   6: total training loss 18092.91\n",
            "2021-07-13 12:03:26,102 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-13 12:03:32,078 - INFO - joeynmt.training - Epoch   7, Step:    50200, Batch Loss:     2.332927, Tokens per Sec:    12467, Lr: 0.000300\n",
            "2021-07-13 12:04:00,029 - INFO - joeynmt.training - Epoch   7, Step:    50400, Batch Loss:     1.966713, Tokens per Sec:    15579, Lr: 0.000300\n",
            "2021-07-13 12:04:27,797 - INFO - joeynmt.training - Epoch   7, Step:    50600, Batch Loss:     2.490288, Tokens per Sec:    15744, Lr: 0.000300\n",
            "2021-07-13 12:04:55,619 - INFO - joeynmt.training - Epoch   7, Step:    50800, Batch Loss:     2.318108, Tokens per Sec:    15632, Lr: 0.000300\n",
            "2021-07-13 12:05:23,570 - INFO - joeynmt.training - Epoch   7, Step:    51000, Batch Loss:     2.292742, Tokens per Sec:    15539, Lr: 0.000300\n",
            "2021-07-13 12:05:51,337 - INFO - joeynmt.training - Epoch   7, Step:    51200, Batch Loss:     2.259486, Tokens per Sec:    15420, Lr: 0.000300\n",
            "2021-07-13 12:06:19,106 - INFO - joeynmt.training - Epoch   7, Step:    51400, Batch Loss:     1.575248, Tokens per Sec:    15467, Lr: 0.000300\n",
            "2021-07-13 12:06:47,052 - INFO - joeynmt.training - Epoch   7, Step:    51600, Batch Loss:     2.006942, Tokens per Sec:    15464, Lr: 0.000300\n",
            "2021-07-13 12:07:14,960 - INFO - joeynmt.training - Epoch   7, Step:    51800, Batch Loss:     2.015448, Tokens per Sec:    15607, Lr: 0.000300\n",
            "2021-07-13 12:07:42,969 - INFO - joeynmt.training - Epoch   7, Step:    52000, Batch Loss:     1.675578, Tokens per Sec:    15508, Lr: 0.000300\n",
            "2021-07-13 12:08:10,937 - INFO - joeynmt.training - Epoch   7, Step:    52200, Batch Loss:     2.228723, Tokens per Sec:    15479, Lr: 0.000300\n",
            "2021-07-13 12:08:38,759 - INFO - joeynmt.training - Epoch   7, Step:    52400, Batch Loss:     2.076001, Tokens per Sec:    15510, Lr: 0.000300\n",
            "2021-07-13 12:09:06,740 - INFO - joeynmt.training - Epoch   7, Step:    52600, Batch Loss:     2.131361, Tokens per Sec:    15535, Lr: 0.000300\n",
            "2021-07-13 12:09:34,789 - INFO - joeynmt.training - Epoch   7, Step:    52800, Batch Loss:     2.110780, Tokens per Sec:    15455, Lr: 0.000300\n",
            "2021-07-13 12:10:02,922 - INFO - joeynmt.training - Epoch   7, Step:    53000, Batch Loss:     2.188513, Tokens per Sec:    15880, Lr: 0.000300\n",
            "2021-07-13 12:10:30,767 - INFO - joeynmt.training - Epoch   7, Step:    53200, Batch Loss:     2.126120, Tokens per Sec:    15565, Lr: 0.000300\n",
            "2021-07-13 12:10:58,405 - INFO - joeynmt.training - Epoch   7, Step:    53400, Batch Loss:     2.067140, Tokens per Sec:    15437, Lr: 0.000300\n",
            "2021-07-13 12:11:26,253 - INFO - joeynmt.training - Epoch   7, Step:    53600, Batch Loss:     2.401521, Tokens per Sec:    15395, Lr: 0.000300\n",
            "2021-07-13 12:11:53,999 - INFO - joeynmt.training - Epoch   7, Step:    53800, Batch Loss:     2.160207, Tokens per Sec:    15554, Lr: 0.000300\n",
            "2021-07-13 12:12:21,937 - INFO - joeynmt.training - Epoch   7, Step:    54000, Batch Loss:     2.068069, Tokens per Sec:    15613, Lr: 0.000300\n",
            "2021-07-13 12:12:49,720 - INFO - joeynmt.training - Epoch   7, Step:    54200, Batch Loss:     2.154219, Tokens per Sec:    15430, Lr: 0.000300\n",
            "2021-07-13 12:13:17,704 - INFO - joeynmt.training - Epoch   7, Step:    54400, Batch Loss:     2.101769, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-13 12:13:45,831 - INFO - joeynmt.training - Epoch   7, Step:    54600, Batch Loss:     2.178194, Tokens per Sec:    15760, Lr: 0.000300\n",
            "2021-07-13 12:14:13,859 - INFO - joeynmt.training - Epoch   7, Step:    54800, Batch Loss:     1.989254, Tokens per Sec:    15702, Lr: 0.000300\n",
            "2021-07-13 12:14:41,902 - INFO - joeynmt.training - Epoch   7, Step:    55000, Batch Loss:     2.133639, Tokens per Sec:    15638, Lr: 0.000300\n",
            "2021-07-13 12:16:25,813 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:16:25,814 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:16:25,814 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:16:26,747 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:16:26,747 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:16:27,827 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:16:27,828 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:16:27,828 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:16:27,828 - INFO - joeynmt.training - \tHypothesis: “ If you have done good and forgiving you , you will be patient , what is pleased to God . ”\n",
            "2021-07-13 12:16:27,828 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:16:27,829 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:16:27,829 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:16:27,829 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ You are my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 12:16:27,829 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:16:27,830 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:16:27,830 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:16:27,830 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 12:16:27,830 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:16:27,830 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:16:27,831 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:16:27,831 - INFO - joeynmt.training - \tHypothesis: But Abiga’s choice to save his family .\n",
            "2021-07-13 12:16:27,831 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    55000: bleu:  15.55, loss: 200319.6094, ppl:   9.2776, duration: 105.9284s\n",
            "2021-07-13 12:16:55,945 - INFO - joeynmt.training - Epoch   7, Step:    55200, Batch Loss:     1.930794, Tokens per Sec:    15279, Lr: 0.000300\n",
            "2021-07-13 12:17:24,289 - INFO - joeynmt.training - Epoch   7, Step:    55400, Batch Loss:     2.126962, Tokens per Sec:    15955, Lr: 0.000300\n",
            "2021-07-13 12:17:52,200 - INFO - joeynmt.training - Epoch   7, Step:    55600, Batch Loss:     2.325502, Tokens per Sec:    15807, Lr: 0.000300\n",
            "2021-07-13 12:18:19,893 - INFO - joeynmt.training - Epoch   7, Step:    55800, Batch Loss:     2.217985, Tokens per Sec:    15569, Lr: 0.000300\n",
            "2021-07-13 12:18:47,763 - INFO - joeynmt.training - Epoch   7, Step:    56000, Batch Loss:     2.026638, Tokens per Sec:    15635, Lr: 0.000300\n",
            "2021-07-13 12:19:15,449 - INFO - joeynmt.training - Epoch   7, Step:    56200, Batch Loss:     1.885304, Tokens per Sec:    15528, Lr: 0.000300\n",
            "2021-07-13 12:19:43,217 - INFO - joeynmt.training - Epoch   7, Step:    56400, Batch Loss:     2.205940, Tokens per Sec:    15625, Lr: 0.000300\n",
            "2021-07-13 12:20:11,048 - INFO - joeynmt.training - Epoch   7, Step:    56600, Batch Loss:     2.034438, Tokens per Sec:    15479, Lr: 0.000300\n",
            "2021-07-13 12:20:38,731 - INFO - joeynmt.training - Epoch   7, Step:    56800, Batch Loss:     2.119070, Tokens per Sec:    15397, Lr: 0.000300\n",
            "2021-07-13 12:21:06,655 - INFO - joeynmt.training - Epoch   7, Step:    57000, Batch Loss:     2.425586, Tokens per Sec:    15407, Lr: 0.000300\n",
            "2021-07-13 12:21:34,624 - INFO - joeynmt.training - Epoch   7, Step:    57200, Batch Loss:     2.196878, Tokens per Sec:    15531, Lr: 0.000300\n",
            "2021-07-13 12:22:02,834 - INFO - joeynmt.training - Epoch   7, Step:    57400, Batch Loss:     2.174635, Tokens per Sec:    15763, Lr: 0.000300\n",
            "2021-07-13 12:22:30,783 - INFO - joeynmt.training - Epoch   7, Step:    57600, Batch Loss:     2.193587, Tokens per Sec:    15591, Lr: 0.000300\n",
            "2021-07-13 12:22:58,865 - INFO - joeynmt.training - Epoch   7, Step:    57800, Batch Loss:     2.181740, Tokens per Sec:    15591, Lr: 0.000300\n",
            "2021-07-13 12:23:26,833 - INFO - joeynmt.training - Epoch   7, Step:    58000, Batch Loss:     1.955853, Tokens per Sec:    15552, Lr: 0.000300\n",
            "2021-07-13 12:23:54,833 - INFO - joeynmt.training - Epoch   7, Step:    58200, Batch Loss:     2.134248, Tokens per Sec:    15513, Lr: 0.000300\n",
            "2021-07-13 12:24:22,763 - INFO - joeynmt.training - Epoch   7, Step:    58400, Batch Loss:     2.206720, Tokens per Sec:    15609, Lr: 0.000300\n",
            "2021-07-13 12:24:40,467 - INFO - joeynmt.training - Epoch   7: total training loss 17658.21\n",
            "2021-07-13 12:24:40,468 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-13 12:24:51,708 - INFO - joeynmt.training - Epoch   8, Step:    58600, Batch Loss:     2.167394, Tokens per Sec:    13908, Lr: 0.000300\n",
            "2021-07-13 12:25:19,744 - INFO - joeynmt.training - Epoch   8, Step:    58800, Batch Loss:     1.965266, Tokens per Sec:    15803, Lr: 0.000300\n",
            "2021-07-13 12:25:47,634 - INFO - joeynmt.training - Epoch   8, Step:    59000, Batch Loss:     1.888123, Tokens per Sec:    15538, Lr: 0.000300\n",
            "2021-07-13 12:26:15,465 - INFO - joeynmt.training - Epoch   8, Step:    59200, Batch Loss:     2.185579, Tokens per Sec:    15534, Lr: 0.000300\n",
            "2021-07-13 12:26:43,306 - INFO - joeynmt.training - Epoch   8, Step:    59400, Batch Loss:     2.042498, Tokens per Sec:    15444, Lr: 0.000300\n",
            "2021-07-13 12:27:11,270 - INFO - joeynmt.training - Epoch   8, Step:    59600, Batch Loss:     2.284022, Tokens per Sec:    15398, Lr: 0.000300\n",
            "2021-07-13 12:27:39,144 - INFO - joeynmt.training - Epoch   8, Step:    59800, Batch Loss:     2.112876, Tokens per Sec:    15533, Lr: 0.000300\n",
            "2021-07-13 12:28:07,078 - INFO - joeynmt.training - Epoch   8, Step:    60000, Batch Loss:     2.006135, Tokens per Sec:    15541, Lr: 0.000300\n",
            "2021-07-13 12:29:52,345 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:29:52,345 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:29:52,345 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:29:53,295 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:29:53,296 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:29:54,257 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:29:54,258 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:29:54,258 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:29:54,258 - INFO - joeynmt.training - \tHypothesis: “ If you have done good and suffer , you have endured , what is acceptable to God . ”\n",
            "2021-07-13 12:29:54,259 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:29:54,259 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:29:54,259 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:29:54,260 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ I am my beloved Son , whom I have loved you . ”\n",
            "2021-07-13 12:29:54,260 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:29:54,260 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:29:54,260 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:29:54,260 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to what Jesus prayed ?\n",
            "2021-07-13 12:29:54,261 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:29:54,261 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:29:54,261 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:29:54,261 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 12:29:54,262 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    60000: bleu:  15.41, loss: 199524.1250, ppl:   9.1959, duration: 107.1835s\n",
            "2021-07-13 12:30:22,533 - INFO - joeynmt.training - Epoch   8, Step:    60200, Batch Loss:     1.802604, Tokens per Sec:    15526, Lr: 0.000300\n",
            "2021-07-13 12:30:50,555 - INFO - joeynmt.training - Epoch   8, Step:    60400, Batch Loss:     2.024530, Tokens per Sec:    15603, Lr: 0.000300\n",
            "2021-07-13 12:31:18,569 - INFO - joeynmt.training - Epoch   8, Step:    60600, Batch Loss:     1.951461, Tokens per Sec:    15773, Lr: 0.000300\n",
            "2021-07-13 12:31:46,499 - INFO - joeynmt.training - Epoch   8, Step:    60800, Batch Loss:     1.769747, Tokens per Sec:    15527, Lr: 0.000300\n",
            "2021-07-13 12:32:14,178 - INFO - joeynmt.training - Epoch   8, Step:    61000, Batch Loss:     2.162689, Tokens per Sec:    15461, Lr: 0.000300\n",
            "2021-07-13 12:32:41,950 - INFO - joeynmt.training - Epoch   8, Step:    61200, Batch Loss:     2.058253, Tokens per Sec:    15593, Lr: 0.000300\n",
            "2021-07-13 12:33:09,803 - INFO - joeynmt.training - Epoch   8, Step:    61400, Batch Loss:     1.801223, Tokens per Sec:    15442, Lr: 0.000300\n",
            "2021-07-13 12:33:37,686 - INFO - joeynmt.training - Epoch   8, Step:    61600, Batch Loss:     1.926289, Tokens per Sec:    15583, Lr: 0.000300\n",
            "2021-07-13 12:34:05,457 - INFO - joeynmt.training - Epoch   8, Step:    61800, Batch Loss:     2.236172, Tokens per Sec:    15470, Lr: 0.000300\n",
            "2021-07-13 12:34:33,454 - INFO - joeynmt.training - Epoch   8, Step:    62000, Batch Loss:     2.239777, Tokens per Sec:    15594, Lr: 0.000300\n",
            "2021-07-13 12:35:01,515 - INFO - joeynmt.training - Epoch   8, Step:    62200, Batch Loss:     2.073179, Tokens per Sec:    15522, Lr: 0.000300\n",
            "2021-07-13 12:35:29,534 - INFO - joeynmt.training - Epoch   8, Step:    62400, Batch Loss:     2.244740, Tokens per Sec:    15597, Lr: 0.000300\n",
            "2021-07-13 12:35:57,593 - INFO - joeynmt.training - Epoch   8, Step:    62600, Batch Loss:     2.016087, Tokens per Sec:    15607, Lr: 0.000300\n",
            "2021-07-13 12:36:25,556 - INFO - joeynmt.training - Epoch   8, Step:    62800, Batch Loss:     2.042367, Tokens per Sec:    15724, Lr: 0.000300\n",
            "2021-07-13 12:36:53,227 - INFO - joeynmt.training - Epoch   8, Step:    63000, Batch Loss:     1.851046, Tokens per Sec:    15360, Lr: 0.000300\n",
            "2021-07-13 12:37:20,886 - INFO - joeynmt.training - Epoch   8, Step:    63200, Batch Loss:     2.017254, Tokens per Sec:    15449, Lr: 0.000300\n",
            "2021-07-13 12:37:49,001 - INFO - joeynmt.training - Epoch   8, Step:    63400, Batch Loss:     2.001923, Tokens per Sec:    15631, Lr: 0.000300\n",
            "2021-07-13 12:38:16,967 - INFO - joeynmt.training - Epoch   8, Step:    63600, Batch Loss:     2.052901, Tokens per Sec:    15688, Lr: 0.000300\n",
            "2021-07-13 12:38:44,920 - INFO - joeynmt.training - Epoch   8, Step:    63800, Batch Loss:     1.992271, Tokens per Sec:    15511, Lr: 0.000300\n",
            "2021-07-13 12:39:12,723 - INFO - joeynmt.training - Epoch   8, Step:    64000, Batch Loss:     2.295581, Tokens per Sec:    15574, Lr: 0.000300\n",
            "2021-07-13 12:39:40,551 - INFO - joeynmt.training - Epoch   8, Step:    64200, Batch Loss:     2.205100, Tokens per Sec:    15474, Lr: 0.000300\n",
            "2021-07-13 12:40:08,489 - INFO - joeynmt.training - Epoch   8, Step:    64400, Batch Loss:     1.800234, Tokens per Sec:    15530, Lr: 0.000300\n",
            "2021-07-13 12:40:36,252 - INFO - joeynmt.training - Epoch   8, Step:    64600, Batch Loss:     2.150029, Tokens per Sec:    15474, Lr: 0.000300\n",
            "2021-07-13 12:41:03,842 - INFO - joeynmt.training - Epoch   8, Step:    64800, Batch Loss:     2.039857, Tokens per Sec:    15509, Lr: 0.000300\n",
            "2021-07-13 12:41:31,582 - INFO - joeynmt.training - Epoch   8, Step:    65000, Batch Loss:     2.013038, Tokens per Sec:    15376, Lr: 0.000300\n",
            "2021-07-13 12:43:26,884 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:43:26,884 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:43:26,885 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:43:27,856 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:43:27,857 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:43:28,845 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:43:28,847 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:43:28,847 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:43:28,847 - INFO - joeynmt.training - \tHypothesis: “ If you are doing good and suffer , what is acceptable to God . ”\n",
            "2021-07-13 12:43:28,847 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:43:28,848 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:43:28,848 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:43:28,848 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he felt the voice from heaven , saying : “ You are my Son , whom I have loved you . ”\n",
            "2021-07-13 12:43:28,848 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:43:28,848 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:43:28,849 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:43:28,849 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ reply ?\n",
            "2021-07-13 12:43:28,849 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:43:28,849 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:43:28,849 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:43:28,850 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 12:43:28,850 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    65000: bleu:  14.64, loss: 198351.8594, ppl:   9.0768, duration: 117.2678s\n",
            "2021-07-13 12:43:57,264 - INFO - joeynmt.training - Epoch   8, Step:    65200, Batch Loss:     2.148055, Tokens per Sec:    15580, Lr: 0.000300\n",
            "2021-07-13 12:44:25,151 - INFO - joeynmt.training - Epoch   8, Step:    65400, Batch Loss:     2.464573, Tokens per Sec:    15682, Lr: 0.000300\n",
            "2021-07-13 12:44:52,958 - INFO - joeynmt.training - Epoch   8, Step:    65600, Batch Loss:     1.950554, Tokens per Sec:    15736, Lr: 0.000300\n",
            "2021-07-13 12:45:20,949 - INFO - joeynmt.training - Epoch   8, Step:    65800, Batch Loss:     1.963682, Tokens per Sec:    15654, Lr: 0.000300\n",
            "2021-07-13 12:45:48,778 - INFO - joeynmt.training - Epoch   8, Step:    66000, Batch Loss:     1.968447, Tokens per Sec:    15636, Lr: 0.000300\n",
            "2021-07-13 12:46:16,858 - INFO - joeynmt.training - Epoch   8, Step:    66200, Batch Loss:     1.726447, Tokens per Sec:    15722, Lr: 0.000300\n",
            "2021-07-13 12:46:44,600 - INFO - joeynmt.training - Epoch   8, Step:    66400, Batch Loss:     2.215193, Tokens per Sec:    15538, Lr: 0.000300\n",
            "2021-07-13 12:47:12,408 - INFO - joeynmt.training - Epoch   8, Step:    66600, Batch Loss:     2.127146, Tokens per Sec:    15685, Lr: 0.000300\n",
            "2021-07-13 12:47:40,232 - INFO - joeynmt.training - Epoch   8, Step:    66800, Batch Loss:     1.846674, Tokens per Sec:    15542, Lr: 0.000300\n",
            "2021-07-13 12:47:53,704 - INFO - joeynmt.training - Epoch   8: total training loss 17336.64\n",
            "2021-07-13 12:47:53,704 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-13 12:48:09,096 - INFO - joeynmt.training - Epoch   9, Step:    67000, Batch Loss:     2.054338, Tokens per Sec:    14418, Lr: 0.000300\n",
            "2021-07-13 12:48:36,881 - INFO - joeynmt.training - Epoch   9, Step:    67200, Batch Loss:     1.736851, Tokens per Sec:    15697, Lr: 0.000300\n",
            "2021-07-13 12:49:04,643 - INFO - joeynmt.training - Epoch   9, Step:    67400, Batch Loss:     2.193262, Tokens per Sec:    15362, Lr: 0.000300\n",
            "2021-07-13 12:49:32,722 - INFO - joeynmt.training - Epoch   9, Step:    67600, Batch Loss:     1.711835, Tokens per Sec:    15765, Lr: 0.000300\n",
            "2021-07-13 12:50:00,434 - INFO - joeynmt.training - Epoch   9, Step:    67800, Batch Loss:     1.833664, Tokens per Sec:    15639, Lr: 0.000300\n",
            "2021-07-13 12:50:28,344 - INFO - joeynmt.training - Epoch   9, Step:    68000, Batch Loss:     1.898809, Tokens per Sec:    15757, Lr: 0.000300\n",
            "2021-07-13 12:50:56,110 - INFO - joeynmt.training - Epoch   9, Step:    68200, Batch Loss:     1.963271, Tokens per Sec:    15545, Lr: 0.000300\n",
            "2021-07-13 12:51:23,946 - INFO - joeynmt.training - Epoch   9, Step:    68400, Batch Loss:     2.020231, Tokens per Sec:    15579, Lr: 0.000300\n",
            "2021-07-13 12:51:52,031 - INFO - joeynmt.training - Epoch   9, Step:    68600, Batch Loss:     1.684562, Tokens per Sec:    15789, Lr: 0.000300\n",
            "2021-07-13 12:52:19,845 - INFO - joeynmt.training - Epoch   9, Step:    68800, Batch Loss:     2.084334, Tokens per Sec:    15661, Lr: 0.000300\n",
            "2021-07-13 12:52:47,732 - INFO - joeynmt.training - Epoch   9, Step:    69000, Batch Loss:     2.169433, Tokens per Sec:    15557, Lr: 0.000300\n",
            "2021-07-13 12:53:15,137 - INFO - joeynmt.training - Epoch   9, Step:    69200, Batch Loss:     2.001745, Tokens per Sec:    15420, Lr: 0.000300\n",
            "2021-07-13 12:53:42,962 - INFO - joeynmt.training - Epoch   9, Step:    69400, Batch Loss:     1.789792, Tokens per Sec:    15417, Lr: 0.000300\n",
            "2021-07-13 12:54:10,951 - INFO - joeynmt.training - Epoch   9, Step:    69600, Batch Loss:     2.348330, Tokens per Sec:    15638, Lr: 0.000300\n",
            "2021-07-13 12:54:38,534 - INFO - joeynmt.training - Epoch   9, Step:    69800, Batch Loss:     2.312226, Tokens per Sec:    15456, Lr: 0.000300\n",
            "2021-07-13 12:55:06,624 - INFO - joeynmt.training - Epoch   9, Step:    70000, Batch Loss:     2.379194, Tokens per Sec:    15673, Lr: 0.000300\n",
            "2021-07-13 12:56:51,736 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:56:51,736 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:56:51,736 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:56:52,685 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:56:52,685 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:56:53,608 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:56:53,609 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:56:53,609 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:56:53,609 - INFO - joeynmt.training - \tHypothesis: If you have done good and forgive you when you endure , what is acceptable to God . ”\n",
            "2021-07-13 12:56:53,610 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:56:53,610 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:56:53,610 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:56:53,610 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ You are my beloved Son , whom I have loved you . ”\n",
            "2021-07-13 12:56:53,611 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:56:53,611 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:56:53,611 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:56:53,611 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ reply ?\n",
            "2021-07-13 12:56:53,611 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:56:53,612 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:56:53,612 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:56:53,612 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 12:56:53,612 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    70000: bleu:  16.11, loss: 195850.3438, ppl:   8.8278, duration: 106.9884s\n",
            "2021-07-13 12:57:21,838 - INFO - joeynmt.training - Epoch   9, Step:    70200, Batch Loss:     2.002295, Tokens per Sec:    15492, Lr: 0.000300\n",
            "2021-07-13 12:57:49,556 - INFO - joeynmt.training - Epoch   9, Step:    70400, Batch Loss:     1.754699, Tokens per Sec:    15385, Lr: 0.000300\n",
            "2021-07-13 12:58:17,408 - INFO - joeynmt.training - Epoch   9, Step:    70600, Batch Loss:     1.852253, Tokens per Sec:    15730, Lr: 0.000300\n",
            "2021-07-13 12:58:45,111 - INFO - joeynmt.training - Epoch   9, Step:    70800, Batch Loss:     2.214972, Tokens per Sec:    15649, Lr: 0.000300\n",
            "2021-07-13 12:59:12,896 - INFO - joeynmt.training - Epoch   9, Step:    71000, Batch Loss:     2.155617, Tokens per Sec:    15582, Lr: 0.000300\n",
            "2021-07-13 12:59:40,303 - INFO - joeynmt.training - Epoch   9, Step:    71200, Batch Loss:     2.079676, Tokens per Sec:    15448, Lr: 0.000300\n",
            "2021-07-13 13:00:08,325 - INFO - joeynmt.training - Epoch   9, Step:    71400, Batch Loss:     1.873048, Tokens per Sec:    15625, Lr: 0.000300\n",
            "2021-07-13 13:00:36,163 - INFO - joeynmt.training - Epoch   9, Step:    71600, Batch Loss:     2.013733, Tokens per Sec:    15631, Lr: 0.000300\n",
            "2021-07-13 13:01:03,856 - INFO - joeynmt.training - Epoch   9, Step:    71800, Batch Loss:     1.981105, Tokens per Sec:    15536, Lr: 0.000300\n",
            "2021-07-13 13:01:31,657 - INFO - joeynmt.training - Epoch   9, Step:    72000, Batch Loss:     2.140049, Tokens per Sec:    15660, Lr: 0.000300\n",
            "2021-07-13 13:01:59,615 - INFO - joeynmt.training - Epoch   9, Step:    72200, Batch Loss:     2.000607, Tokens per Sec:    15718, Lr: 0.000300\n",
            "2021-07-13 13:02:27,413 - INFO - joeynmt.training - Epoch   9, Step:    72400, Batch Loss:     2.019729, Tokens per Sec:    15609, Lr: 0.000300\n",
            "2021-07-13 13:02:55,373 - INFO - joeynmt.training - Epoch   9, Step:    72600, Batch Loss:     2.109488, Tokens per Sec:    15818, Lr: 0.000300\n",
            "2021-07-13 13:03:23,141 - INFO - joeynmt.training - Epoch   9, Step:    72800, Batch Loss:     1.965007, Tokens per Sec:    15558, Lr: 0.000300\n",
            "2021-07-13 13:03:50,907 - INFO - joeynmt.training - Epoch   9, Step:    73000, Batch Loss:     2.268159, Tokens per Sec:    15621, Lr: 0.000300\n",
            "2021-07-13 13:04:19,017 - INFO - joeynmt.training - Epoch   9, Step:    73200, Batch Loss:     1.881596, Tokens per Sec:    15676, Lr: 0.000300\n",
            "2021-07-13 13:04:47,093 - INFO - joeynmt.training - Epoch   9, Step:    73400, Batch Loss:     1.947551, Tokens per Sec:    15866, Lr: 0.000300\n",
            "2021-07-13 13:05:14,884 - INFO - joeynmt.training - Epoch   9, Step:    73600, Batch Loss:     2.024244, Tokens per Sec:    15500, Lr: 0.000300\n",
            "2021-07-13 13:05:42,720 - INFO - joeynmt.training - Epoch   9, Step:    73800, Batch Loss:     1.821925, Tokens per Sec:    15615, Lr: 0.000300\n",
            "2021-07-13 13:06:10,719 - INFO - joeynmt.training - Epoch   9, Step:    74000, Batch Loss:     2.158430, Tokens per Sec:    15658, Lr: 0.000300\n",
            "2021-07-13 13:06:38,619 - INFO - joeynmt.training - Epoch   9, Step:    74200, Batch Loss:     1.939052, Tokens per Sec:    15431, Lr: 0.000300\n",
            "2021-07-13 13:07:06,292 - INFO - joeynmt.training - Epoch   9, Step:    74400, Batch Loss:     2.200084, Tokens per Sec:    15482, Lr: 0.000300\n",
            "2021-07-13 13:07:33,885 - INFO - joeynmt.training - Epoch   9, Step:    74600, Batch Loss:     1.971853, Tokens per Sec:    15478, Lr: 0.000300\n",
            "2021-07-13 13:08:01,784 - INFO - joeynmt.training - Epoch   9, Step:    74800, Batch Loss:     2.286175, Tokens per Sec:    15667, Lr: 0.000300\n",
            "2021-07-13 13:08:29,618 - INFO - joeynmt.training - Epoch   9, Step:    75000, Batch Loss:     1.944803, Tokens per Sec:    15656, Lr: 0.000300\n",
            "2021-07-13 13:10:07,963 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:10:07,963 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:10:07,964 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:10:08,897 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:10:08,897 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:10:09,933 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:10:09,934 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 13:10:09,935 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 13:10:09,935 - INFO - joeynmt.training - \tHypothesis: If you have done good and forgive you , what is acceptable to God . ”\n",
            "2021-07-13 13:10:09,935 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:10:09,935 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 13:10:09,936 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 13:10:09,936 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ You are my beloved Son , whom I have loved . ”\n",
            "2021-07-13 13:10:09,936 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:10:09,936 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 13:10:09,936 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 13:10:09,937 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ reply ?\n",
            "2021-07-13 13:10:09,937 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:10:09,937 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 13:10:09,937 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 13:10:09,937 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 13:10:09,938 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    75000: bleu:  16.18, loss: 195205.6562, ppl:   8.7647, duration: 100.3194s\n",
            "2021-07-13 13:10:37,926 - INFO - joeynmt.training - Epoch   9, Step:    75200, Batch Loss:     1.900392, Tokens per Sec:    15497, Lr: 0.000300\n",
            "2021-07-13 13:10:47,165 - INFO - joeynmt.training - Epoch   9: total training loss 17036.63\n",
            "2021-07-13 13:10:47,165 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-13 13:11:07,053 - INFO - joeynmt.training - Epoch  10, Step:    75400, Batch Loss:     1.502504, Tokens per Sec:    14857, Lr: 0.000300\n",
            "2021-07-13 13:11:34,820 - INFO - joeynmt.training - Epoch  10, Step:    75600, Batch Loss:     2.116007, Tokens per Sec:    15504, Lr: 0.000300\n",
            "2021-07-13 13:12:02,678 - INFO - joeynmt.training - Epoch  10, Step:    75800, Batch Loss:     2.234359, Tokens per Sec:    15672, Lr: 0.000300\n",
            "2021-07-13 13:12:30,924 - INFO - joeynmt.training - Epoch  10, Step:    76000, Batch Loss:     1.983121, Tokens per Sec:    15801, Lr: 0.000300\n",
            "2021-07-13 13:12:58,687 - INFO - joeynmt.training - Epoch  10, Step:    76200, Batch Loss:     2.051770, Tokens per Sec:    15536, Lr: 0.000300\n",
            "2021-07-13 13:13:26,667 - INFO - joeynmt.training - Epoch  10, Step:    76400, Batch Loss:     1.813955, Tokens per Sec:    15779, Lr: 0.000300\n",
            "2021-07-13 13:13:54,285 - INFO - joeynmt.training - Epoch  10, Step:    76600, Batch Loss:     2.514866, Tokens per Sec:    15568, Lr: 0.000300\n",
            "2021-07-13 13:14:22,231 - INFO - joeynmt.training - Epoch  10, Step:    76800, Batch Loss:     1.859401, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-13 13:14:50,283 - INFO - joeynmt.training - Epoch  10, Step:    77000, Batch Loss:     2.105885, Tokens per Sec:    15787, Lr: 0.000300\n",
            "2021-07-13 13:15:18,308 - INFO - joeynmt.training - Epoch  10, Step:    77200, Batch Loss:     2.367313, Tokens per Sec:    15482, Lr: 0.000300\n",
            "2021-07-13 13:15:46,145 - INFO - joeynmt.training - Epoch  10, Step:    77400, Batch Loss:     2.038007, Tokens per Sec:    15678, Lr: 0.000300\n",
            "2021-07-13 13:16:13,865 - INFO - joeynmt.training - Epoch  10, Step:    77600, Batch Loss:     2.289842, Tokens per Sec:    15633, Lr: 0.000300\n",
            "2021-07-13 13:16:41,787 - INFO - joeynmt.training - Epoch  10, Step:    77800, Batch Loss:     1.857714, Tokens per Sec:    15743, Lr: 0.000300\n",
            "2021-07-13 13:17:09,686 - INFO - joeynmt.training - Epoch  10, Step:    78000, Batch Loss:     2.006835, Tokens per Sec:    15567, Lr: 0.000300\n",
            "2021-07-13 13:17:37,614 - INFO - joeynmt.training - Epoch  10, Step:    78200, Batch Loss:     1.935996, Tokens per Sec:    15729, Lr: 0.000300\n",
            "2021-07-13 13:18:05,573 - INFO - joeynmt.training - Epoch  10, Step:    78400, Batch Loss:     2.090329, Tokens per Sec:    15633, Lr: 0.000300\n",
            "2021-07-13 13:18:33,182 - INFO - joeynmt.training - Epoch  10, Step:    78600, Batch Loss:     2.255298, Tokens per Sec:    15424, Lr: 0.000300\n",
            "2021-07-13 13:19:01,106 - INFO - joeynmt.training - Epoch  10, Step:    78800, Batch Loss:     1.980329, Tokens per Sec:    15548, Lr: 0.000300\n",
            "2021-07-13 13:19:29,211 - INFO - joeynmt.training - Epoch  10, Step:    79000, Batch Loss:     2.127359, Tokens per Sec:    15596, Lr: 0.000300\n",
            "2021-07-13 13:19:57,065 - INFO - joeynmt.training - Epoch  10, Step:    79200, Batch Loss:     1.808896, Tokens per Sec:    15499, Lr: 0.000300\n",
            "2021-07-13 13:20:24,920 - INFO - joeynmt.training - Epoch  10, Step:    79400, Batch Loss:     2.035119, Tokens per Sec:    15540, Lr: 0.000300\n",
            "2021-07-13 13:20:52,886 - INFO - joeynmt.training - Epoch  10, Step:    79600, Batch Loss:     1.957607, Tokens per Sec:    15716, Lr: 0.000300\n",
            "2021-07-13 13:21:20,998 - INFO - joeynmt.training - Epoch  10, Step:    79800, Batch Loss:     2.135190, Tokens per Sec:    15674, Lr: 0.000300\n",
            "2021-07-13 13:21:48,889 - INFO - joeynmt.training - Epoch  10, Step:    80000, Batch Loss:     2.055080, Tokens per Sec:    15653, Lr: 0.000300\n",
            "2021-07-13 13:23:31,347 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:23:31,347 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:23:31,347 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:23:32,271 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:23:32,271 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:23:33,054 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:23:33,055 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 13:23:33,055 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 13:23:33,055 - INFO - joeynmt.training - \tHypothesis: “ If you have done good and forgive you , you will be able to endure , what is acceptable to God . ”\n",
            "2021-07-13 13:23:33,055 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:23:33,056 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 13:23:33,056 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 13:23:33,056 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ You , my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 13:23:33,056 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:23:33,057 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 13:23:33,057 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 13:23:33,057 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ reply ?\n",
            "2021-07-13 13:23:33,057 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:23:33,058 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 13:23:33,058 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 13:23:33,058 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 13:23:33,058 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    80000: bleu:  16.62, loss: 192584.1719, ppl:   8.5129, duration: 104.1689s\n",
            "2021-07-13 13:24:01,278 - INFO - joeynmt.training - Epoch  10, Step:    80200, Batch Loss:     1.965278, Tokens per Sec:    15381, Lr: 0.000300\n",
            "2021-07-13 13:24:29,257 - INFO - joeynmt.training - Epoch  10, Step:    80400, Batch Loss:     1.998758, Tokens per Sec:    15687, Lr: 0.000300\n",
            "2021-07-13 13:24:57,007 - INFO - joeynmt.training - Epoch  10, Step:    80600, Batch Loss:     1.973588, Tokens per Sec:    15521, Lr: 0.000300\n",
            "2021-07-13 13:25:24,478 - INFO - joeynmt.training - Epoch  10, Step:    80800, Batch Loss:     1.929704, Tokens per Sec:    15528, Lr: 0.000300\n",
            "2021-07-13 13:25:52,022 - INFO - joeynmt.training - Epoch  10, Step:    81000, Batch Loss:     1.918842, Tokens per Sec:    15327, Lr: 0.000300\n",
            "2021-07-13 13:26:19,940 - INFO - joeynmt.training - Epoch  10, Step:    81200, Batch Loss:     2.209150, Tokens per Sec:    15733, Lr: 0.000300\n",
            "2021-07-13 13:26:47,614 - INFO - joeynmt.training - Epoch  10, Step:    81400, Batch Loss:     2.069150, Tokens per Sec:    15376, Lr: 0.000300\n",
            "2021-07-13 13:27:15,654 - INFO - joeynmt.training - Epoch  10, Step:    81600, Batch Loss:     2.109392, Tokens per Sec:    15813, Lr: 0.000300\n",
            "2021-07-13 13:27:43,753 - INFO - joeynmt.training - Epoch  10, Step:    81800, Batch Loss:     1.912824, Tokens per Sec:    15720, Lr: 0.000300\n",
            "2021-07-13 13:28:11,648 - INFO - joeynmt.training - Epoch  10, Step:    82000, Batch Loss:     2.014697, Tokens per Sec:    15523, Lr: 0.000300\n",
            "2021-07-13 13:28:39,711 - INFO - joeynmt.training - Epoch  10, Step:    82200, Batch Loss:     2.155798, Tokens per Sec:    15587, Lr: 0.000300\n",
            "2021-07-13 13:29:07,449 - INFO - joeynmt.training - Epoch  10, Step:    82400, Batch Loss:     1.970684, Tokens per Sec:    15464, Lr: 0.000300\n",
            "2021-07-13 13:29:35,326 - INFO - joeynmt.training - Epoch  10, Step:    82600, Batch Loss:     1.889211, Tokens per Sec:    15567, Lr: 0.000300\n",
            "2021-07-13 13:30:03,097 - INFO - joeynmt.training - Epoch  10, Step:    82800, Batch Loss:     1.845640, Tokens per Sec:    15408, Lr: 0.000300\n",
            "2021-07-13 13:30:31,179 - INFO - joeynmt.training - Epoch  10, Step:    83000, Batch Loss:     2.124231, Tokens per Sec:    15774, Lr: 0.000300\n",
            "2021-07-13 13:30:59,233 - INFO - joeynmt.training - Epoch  10, Step:    83200, Batch Loss:     1.977893, Tokens per Sec:    15607, Lr: 0.000300\n",
            "2021-07-13 13:31:27,089 - INFO - joeynmt.training - Epoch  10, Step:    83400, Batch Loss:     2.104967, Tokens per Sec:    15620, Lr: 0.000300\n",
            "2021-07-13 13:31:55,070 - INFO - joeynmt.training - Epoch  10, Step:    83600, Batch Loss:     2.238452, Tokens per Sec:    15415, Lr: 0.000300\n",
            "2021-07-13 13:31:57,793 - INFO - joeynmt.training - Epoch  10: total training loss 16784.61\n",
            "2021-07-13 13:31:57,794 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-13 13:32:24,316 - INFO - joeynmt.training - Epoch  11, Step:    83800, Batch Loss:     2.043761, Tokens per Sec:    14922, Lr: 0.000300\n",
            "2021-07-13 13:32:52,304 - INFO - joeynmt.training - Epoch  11, Step:    84000, Batch Loss:     1.857506, Tokens per Sec:    15672, Lr: 0.000300\n",
            "2021-07-13 13:33:20,302 - INFO - joeynmt.training - Epoch  11, Step:    84200, Batch Loss:     2.151267, Tokens per Sec:    15483, Lr: 0.000300\n",
            "2021-07-13 13:33:48,078 - INFO - joeynmt.training - Epoch  11, Step:    84400, Batch Loss:     1.952946, Tokens per Sec:    15433, Lr: 0.000300\n",
            "2021-07-13 13:34:15,809 - INFO - joeynmt.training - Epoch  11, Step:    84600, Batch Loss:     2.126299, Tokens per Sec:    15478, Lr: 0.000300\n",
            "2021-07-13 13:34:43,772 - INFO - joeynmt.training - Epoch  11, Step:    84800, Batch Loss:     2.020547, Tokens per Sec:    15538, Lr: 0.000300\n",
            "2021-07-13 13:35:11,903 - INFO - joeynmt.training - Epoch  11, Step:    85000, Batch Loss:     2.048020, Tokens per Sec:    15806, Lr: 0.000300\n",
            "2021-07-13 13:36:55,273 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:36:55,273 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:36:55,273 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:36:56,209 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:36:56,210 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:36:57,013 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:36:57,013 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 13:36:57,014 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 13:36:57,014 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-13 13:36:57,014 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:36:57,014 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 13:36:57,015 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 13:36:57,015 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ You are my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 13:36:57,015 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:36:57,015 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 13:36:57,015 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 13:36:57,016 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ reply ?\n",
            "2021-07-13 13:36:57,016 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:36:57,016 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 13:36:57,017 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 13:36:57,017 - INFO - joeynmt.training - \tHypothesis: But Abigael did what he did to save his family .\n",
            "2021-07-13 13:36:57,017 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    85000: bleu:  16.76, loss: 191828.9844, ppl:   8.4417, duration: 105.1134s\n",
            "2021-07-13 13:37:25,420 - INFO - joeynmt.training - Epoch  11, Step:    85200, Batch Loss:     2.105764, Tokens per Sec:    15528, Lr: 0.000300\n",
            "2021-07-13 13:37:53,401 - INFO - joeynmt.training - Epoch  11, Step:    85400, Batch Loss:     1.828444, Tokens per Sec:    15514, Lr: 0.000300\n",
            "2021-07-13 13:38:21,299 - INFO - joeynmt.training - Epoch  11, Step:    85600, Batch Loss:     1.951941, Tokens per Sec:    15685, Lr: 0.000300\n",
            "2021-07-13 13:38:49,340 - INFO - joeynmt.training - Epoch  11, Step:    85800, Batch Loss:     1.994438, Tokens per Sec:    15820, Lr: 0.000300\n",
            "2021-07-13 13:39:17,245 - INFO - joeynmt.training - Epoch  11, Step:    86000, Batch Loss:     1.855183, Tokens per Sec:    15536, Lr: 0.000300\n",
            "2021-07-13 13:39:45,173 - INFO - joeynmt.training - Epoch  11, Step:    86200, Batch Loss:     1.824895, Tokens per Sec:    15637, Lr: 0.000300\n",
            "2021-07-13 13:40:13,065 - INFO - joeynmt.training - Epoch  11, Step:    86400, Batch Loss:     2.015989, Tokens per Sec:    15635, Lr: 0.000300\n",
            "2021-07-13 13:40:40,841 - INFO - joeynmt.training - Epoch  11, Step:    86600, Batch Loss:     1.884056, Tokens per Sec:    15481, Lr: 0.000300\n",
            "2021-07-13 13:41:08,590 - INFO - joeynmt.training - Epoch  11, Step:    86800, Batch Loss:     2.117580, Tokens per Sec:    15540, Lr: 0.000300\n",
            "2021-07-13 13:41:36,698 - INFO - joeynmt.training - Epoch  11, Step:    87000, Batch Loss:     2.108941, Tokens per Sec:    15799, Lr: 0.000300\n",
            "2021-07-13 13:42:04,683 - INFO - joeynmt.training - Epoch  11, Step:    87200, Batch Loss:     1.871780, Tokens per Sec:    15687, Lr: 0.000300\n",
            "2021-07-13 13:42:32,638 - INFO - joeynmt.training - Epoch  11, Step:    87400, Batch Loss:     1.929549, Tokens per Sec:    15649, Lr: 0.000300\n",
            "2021-07-13 13:43:00,472 - INFO - joeynmt.training - Epoch  11, Step:    87600, Batch Loss:     2.105240, Tokens per Sec:    15554, Lr: 0.000300\n",
            "2021-07-13 13:43:28,369 - INFO - joeynmt.training - Epoch  11, Step:    87800, Batch Loss:     1.960453, Tokens per Sec:    15548, Lr: 0.000300\n",
            "2021-07-13 13:43:56,155 - INFO - joeynmt.training - Epoch  11, Step:    88000, Batch Loss:     1.931114, Tokens per Sec:    15620, Lr: 0.000300\n",
            "2021-07-13 13:44:24,068 - INFO - joeynmt.training - Epoch  11, Step:    88200, Batch Loss:     1.883100, Tokens per Sec:    15586, Lr: 0.000300\n",
            "2021-07-13 13:44:51,700 - INFO - joeynmt.training - Epoch  11, Step:    88400, Batch Loss:     1.694307, Tokens per Sec:    15633, Lr: 0.000300\n",
            "2021-07-13 13:45:19,348 - INFO - joeynmt.training - Epoch  11, Step:    88600, Batch Loss:     1.791338, Tokens per Sec:    15604, Lr: 0.000300\n",
            "2021-07-13 13:45:46,890 - INFO - joeynmt.training - Epoch  11, Step:    88800, Batch Loss:     2.059513, Tokens per Sec:    15369, Lr: 0.000300\n",
            "2021-07-13 13:46:14,724 - INFO - joeynmt.training - Epoch  11, Step:    89000, Batch Loss:     1.670146, Tokens per Sec:    15498, Lr: 0.000300\n",
            "2021-07-13 13:46:42,292 - INFO - joeynmt.training - Epoch  11, Step:    89200, Batch Loss:     1.735478, Tokens per Sec:    15403, Lr: 0.000300\n",
            "2021-07-13 13:47:10,226 - INFO - joeynmt.training - Epoch  11, Step:    89400, Batch Loss:     1.774708, Tokens per Sec:    15596, Lr: 0.000300\n",
            "2021-07-13 13:47:38,168 - INFO - joeynmt.training - Epoch  11, Step:    89600, Batch Loss:     2.439178, Tokens per Sec:    15485, Lr: 0.000300\n",
            "2021-07-13 13:48:06,230 - INFO - joeynmt.training - Epoch  11, Step:    89800, Batch Loss:     1.860653, Tokens per Sec:    15760, Lr: 0.000300\n",
            "2021-07-13 13:48:33,742 - INFO - joeynmt.training - Epoch  11, Step:    90000, Batch Loss:     2.457768, Tokens per Sec:    15683, Lr: 0.000300\n",
            "2021-07-13 13:50:18,306 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:50:18,307 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:50:18,307 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:50:19,249 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:50:19,249 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:50:19,994 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:50:19,995 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 13:50:19,995 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 13:50:19,995 - INFO - joeynmt.training - \tHypothesis: “ If you have done good and suffered , what is acceptable to God . ”\n",
            "2021-07-13 13:50:19,995 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:50:19,996 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 13:50:19,996 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 13:50:19,996 - INFO - joeynmt.training - \tHypothesis: After Jesus ’ baptism , he heard the voice from heaven : “ You are my beloved Son , whom I have loved . ”\n",
            "2021-07-13 13:50:19,996 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:50:19,997 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 13:50:19,997 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 13:50:19,997 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ reply ?\n",
            "2021-07-13 13:50:19,997 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:50:19,998 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 13:50:19,998 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 13:50:19,998 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 13:50:19,998 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    90000: bleu:  16.79, loss: 190977.9062, ppl:   8.3622, duration: 106.2556s\n",
            "2021-07-13 13:50:48,183 - INFO - joeynmt.training - Epoch  11, Step:    90200, Batch Loss:     2.013944, Tokens per Sec:    15442, Lr: 0.000300\n",
            "2021-07-13 13:51:15,648 - INFO - joeynmt.training - Epoch  11, Step:    90400, Batch Loss:     2.007235, Tokens per Sec:    15195, Lr: 0.000300\n",
            "2021-07-13 13:51:43,534 - INFO - joeynmt.training - Epoch  11, Step:    90600, Batch Loss:     2.042985, Tokens per Sec:    15569, Lr: 0.000300\n",
            "2021-07-13 13:52:11,485 - INFO - joeynmt.training - Epoch  11, Step:    90800, Batch Loss:     1.970154, Tokens per Sec:    15819, Lr: 0.000300\n",
            "2021-07-13 13:52:39,424 - INFO - joeynmt.training - Epoch  11, Step:    91000, Batch Loss:     1.975912, Tokens per Sec:    15488, Lr: 0.000300\n",
            "2021-07-13 13:53:07,419 - INFO - joeynmt.training - Epoch  11, Step:    91200, Batch Loss:     1.974183, Tokens per Sec:    15603, Lr: 0.000300\n",
            "2021-07-13 13:53:35,307 - INFO - joeynmt.training - Epoch  11, Step:    91400, Batch Loss:     1.957577, Tokens per Sec:    15686, Lr: 0.000300\n",
            "2021-07-13 13:54:03,047 - INFO - joeynmt.training - Epoch  11, Step:    91600, Batch Loss:     1.536381, Tokens per Sec:    15585, Lr: 0.000300\n",
            "2021-07-13 13:54:30,999 - INFO - joeynmt.training - Epoch  11, Step:    91800, Batch Loss:     1.911156, Tokens per Sec:    15729, Lr: 0.000300\n",
            "2021-07-13 13:54:56,507 - INFO - joeynmt.training - Epoch  11: total training loss 16594.49\n",
            "2021-07-13 13:54:56,507 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-13 13:55:00,087 - INFO - joeynmt.training - Epoch  12, Step:    92000, Batch Loss:     1.856047, Tokens per Sec:    11540, Lr: 0.000300\n",
            "2021-07-13 13:55:27,864 - INFO - joeynmt.training - Epoch  12, Step:    92200, Batch Loss:     1.862584, Tokens per Sec:    15593, Lr: 0.000300\n",
            "2021-07-13 13:55:55,763 - INFO - joeynmt.training - Epoch  12, Step:    92400, Batch Loss:     2.260139, Tokens per Sec:    15639, Lr: 0.000300\n",
            "2021-07-13 13:56:23,509 - INFO - joeynmt.training - Epoch  12, Step:    92600, Batch Loss:     2.026763, Tokens per Sec:    15605, Lr: 0.000300\n",
            "2021-07-13 13:56:51,486 - INFO - joeynmt.training - Epoch  12, Step:    92800, Batch Loss:     2.000638, Tokens per Sec:    15653, Lr: 0.000300\n",
            "2021-07-13 13:57:19,324 - INFO - joeynmt.training - Epoch  12, Step:    93000, Batch Loss:     1.972045, Tokens per Sec:    15695, Lr: 0.000300\n",
            "2021-07-13 13:57:46,993 - INFO - joeynmt.training - Epoch  12, Step:    93200, Batch Loss:     2.024569, Tokens per Sec:    15565, Lr: 0.000300\n",
            "2021-07-13 13:58:14,786 - INFO - joeynmt.training - Epoch  12, Step:    93400, Batch Loss:     2.405658, Tokens per Sec:    15611, Lr: 0.000300\n",
            "2021-07-13 13:58:42,706 - INFO - joeynmt.training - Epoch  12, Step:    93600, Batch Loss:     2.029729, Tokens per Sec:    15590, Lr: 0.000300\n",
            "2021-07-13 13:59:10,294 - INFO - joeynmt.training - Epoch  12, Step:    93800, Batch Loss:     2.007556, Tokens per Sec:    15566, Lr: 0.000300\n",
            "2021-07-13 13:59:38,196 - INFO - joeynmt.training - Epoch  12, Step:    94000, Batch Loss:     1.855427, Tokens per Sec:    15558, Lr: 0.000300\n",
            "2021-07-13 14:00:06,307 - INFO - joeynmt.training - Epoch  12, Step:    94200, Batch Loss:     1.898553, Tokens per Sec:    15591, Lr: 0.000300\n",
            "2021-07-13 14:00:34,060 - INFO - joeynmt.training - Epoch  12, Step:    94400, Batch Loss:     1.946700, Tokens per Sec:    15518, Lr: 0.000300\n",
            "2021-07-13 14:01:01,817 - INFO - joeynmt.training - Epoch  12, Step:    94600, Batch Loss:     1.990987, Tokens per Sec:    15538, Lr: 0.000300\n",
            "2021-07-13 14:01:29,760 - INFO - joeynmt.training - Epoch  12, Step:    94800, Batch Loss:     2.333552, Tokens per Sec:    15617, Lr: 0.000300\n",
            "2021-07-13 14:01:57,496 - INFO - joeynmt.training - Epoch  12, Step:    95000, Batch Loss:     1.998091, Tokens per Sec:    15359, Lr: 0.000300\n",
            "2021-07-13 14:03:42,129 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:03:42,130 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:03:42,130 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:03:43,082 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:03:43,083 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:03:43,844 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:03:43,845 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:03:43,846 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:03:43,846 - INFO - joeynmt.training - \tHypothesis: If you have done good and suffered when you endure , this is acceptable to God . ”\n",
            "2021-07-13 14:03:43,846 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:03:43,846 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:03:43,847 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:03:43,847 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ You are my beloved Son , whom I have loved . ”\n",
            "2021-07-13 14:03:43,847 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:03:43,847 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:03:43,847 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:03:43,848 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-13 14:03:43,848 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:03:43,848 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:03:43,848 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:03:43,848 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 14:03:43,849 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    95000: bleu:  17.12, loss: 190402.6719, ppl:   8.3089, duration: 106.3524s\n",
            "2021-07-13 14:04:11,784 - INFO - joeynmt.training - Epoch  12, Step:    95200, Batch Loss:     2.145988, Tokens per Sec:    15250, Lr: 0.000300\n",
            "2021-07-13 14:04:39,906 - INFO - joeynmt.training - Epoch  12, Step:    95400, Batch Loss:     2.184007, Tokens per Sec:    15582, Lr: 0.000300\n",
            "2021-07-13 14:05:07,931 - INFO - joeynmt.training - Epoch  12, Step:    95600, Batch Loss:     1.671423, Tokens per Sec:    15553, Lr: 0.000300\n",
            "2021-07-13 14:05:35,851 - INFO - joeynmt.training - Epoch  12, Step:    95800, Batch Loss:     2.317399, Tokens per Sec:    15670, Lr: 0.000300\n",
            "2021-07-13 14:06:03,786 - INFO - joeynmt.training - Epoch  12, Step:    96000, Batch Loss:     2.176956, Tokens per Sec:    15601, Lr: 0.000300\n",
            "2021-07-13 14:06:31,466 - INFO - joeynmt.training - Epoch  12, Step:    96200, Batch Loss:     1.954070, Tokens per Sec:    15543, Lr: 0.000300\n",
            "2021-07-13 14:06:59,251 - INFO - joeynmt.training - Epoch  12, Step:    96400, Batch Loss:     2.051386, Tokens per Sec:    15527, Lr: 0.000300\n",
            "2021-07-13 14:07:27,157 - INFO - joeynmt.training - Epoch  12, Step:    96600, Batch Loss:     1.950739, Tokens per Sec:    15529, Lr: 0.000300\n",
            "2021-07-13 14:07:55,076 - INFO - joeynmt.training - Epoch  12, Step:    96800, Batch Loss:     1.863124, Tokens per Sec:    15481, Lr: 0.000300\n",
            "2021-07-13 14:08:22,779 - INFO - joeynmt.training - Epoch  12, Step:    97000, Batch Loss:     1.903324, Tokens per Sec:    15497, Lr: 0.000300\n",
            "2021-07-13 14:08:50,465 - INFO - joeynmt.training - Epoch  12, Step:    97200, Batch Loss:     2.117090, Tokens per Sec:    15534, Lr: 0.000300\n",
            "2021-07-13 14:09:18,251 - INFO - joeynmt.training - Epoch  12, Step:    97400, Batch Loss:     1.792558, Tokens per Sec:    15488, Lr: 0.000300\n",
            "2021-07-13 14:09:45,887 - INFO - joeynmt.training - Epoch  12, Step:    97600, Batch Loss:     1.782560, Tokens per Sec:    15606, Lr: 0.000300\n",
            "2021-07-13 14:10:14,146 - INFO - joeynmt.training - Epoch  12, Step:    97800, Batch Loss:     2.319911, Tokens per Sec:    15795, Lr: 0.000300\n",
            "2021-07-13 14:10:42,010 - INFO - joeynmt.training - Epoch  12, Step:    98000, Batch Loss:     1.841925, Tokens per Sec:    15676, Lr: 0.000300\n",
            "2021-07-13 14:11:09,740 - INFO - joeynmt.training - Epoch  12, Step:    98200, Batch Loss:     1.970892, Tokens per Sec:    15493, Lr: 0.000300\n",
            "2021-07-13 14:11:37,587 - INFO - joeynmt.training - Epoch  12, Step:    98400, Batch Loss:     2.042465, Tokens per Sec:    15554, Lr: 0.000300\n",
            "2021-07-13 14:12:05,509 - INFO - joeynmt.training - Epoch  12, Step:    98600, Batch Loss:     1.970418, Tokens per Sec:    15419, Lr: 0.000300\n",
            "2021-07-13 14:12:33,623 - INFO - joeynmt.training - Epoch  12, Step:    98800, Batch Loss:     1.770223, Tokens per Sec:    15753, Lr: 0.000300\n",
            "2021-07-13 14:13:01,460 - INFO - joeynmt.training - Epoch  12, Step:    99000, Batch Loss:     2.039515, Tokens per Sec:    15707, Lr: 0.000300\n",
            "2021-07-13 14:13:29,634 - INFO - joeynmt.training - Epoch  12, Step:    99200, Batch Loss:     1.984795, Tokens per Sec:    15642, Lr: 0.000300\n",
            "2021-07-13 14:13:57,659 - INFO - joeynmt.training - Epoch  12, Step:    99400, Batch Loss:     2.020503, Tokens per Sec:    15375, Lr: 0.000300\n",
            "2021-07-13 14:14:25,636 - INFO - joeynmt.training - Epoch  12, Step:    99600, Batch Loss:     1.712396, Tokens per Sec:    15772, Lr: 0.000300\n",
            "2021-07-13 14:14:53,339 - INFO - joeynmt.training - Epoch  12, Step:    99800, Batch Loss:     2.120977, Tokens per Sec:    15574, Lr: 0.000300\n",
            "2021-07-13 14:15:21,147 - INFO - joeynmt.training - Epoch  12, Step:   100000, Batch Loss:     2.141821, Tokens per Sec:    15695, Lr: 0.000300\n",
            "2021-07-13 14:17:06,753 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:17:06,753 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:17:06,754 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:17:07,709 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:17:07,709 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:17:08,474 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:17:08,475 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:17:08,475 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:17:08,476 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-13 14:17:08,476 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:17:08,476 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:17:08,477 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:17:08,477 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ You , my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 14:17:08,477 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:17:08,477 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:17:08,478 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:17:08,478 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-13 14:17:08,478 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:17:08,478 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:17:08,479 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:17:08,479 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 14:17:08,479 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   100000: bleu:  16.92, loss: 188623.9375, ppl:   8.1462, duration: 107.3313s\n",
            "2021-07-13 14:17:36,666 - INFO - joeynmt.training - Epoch  12, Step:   100200, Batch Loss:     1.767244, Tokens per Sec:    15375, Lr: 0.000300\n",
            "2021-07-13 14:17:58,599 - INFO - joeynmt.training - Epoch  12: total training loss 16434.36\n",
            "2021-07-13 14:17:58,599 - INFO - joeynmt.training - EPOCH 13\n",
            "2021-07-13 14:18:05,588 - INFO - joeynmt.training - Epoch  13, Step:   100400, Batch Loss:     1.900267, Tokens per Sec:    13224, Lr: 0.000300\n",
            "2021-07-13 14:18:33,272 - INFO - joeynmt.training - Epoch  13, Step:   100600, Batch Loss:     2.132473, Tokens per Sec:    15525, Lr: 0.000300\n",
            "2021-07-13 14:19:00,950 - INFO - joeynmt.training - Epoch  13, Step:   100800, Batch Loss:     1.682799, Tokens per Sec:    15490, Lr: 0.000300\n",
            "2021-07-13 14:19:29,043 - INFO - joeynmt.training - Epoch  13, Step:   101000, Batch Loss:     1.790046, Tokens per Sec:    15625, Lr: 0.000300\n",
            "2021-07-13 14:19:57,188 - INFO - joeynmt.training - Epoch  13, Step:   101200, Batch Loss:     1.887922, Tokens per Sec:    15651, Lr: 0.000300\n",
            "2021-07-13 14:20:25,089 - INFO - joeynmt.training - Epoch  13, Step:   101400, Batch Loss:     2.069595, Tokens per Sec:    15744, Lr: 0.000300\n",
            "2021-07-13 14:20:52,729 - INFO - joeynmt.training - Epoch  13, Step:   101600, Batch Loss:     2.124712, Tokens per Sec:    15429, Lr: 0.000300\n",
            "2021-07-13 14:21:20,901 - INFO - joeynmt.training - Epoch  13, Step:   101800, Batch Loss:     1.807776, Tokens per Sec:    15799, Lr: 0.000300\n",
            "2021-07-13 14:21:48,810 - INFO - joeynmt.training - Epoch  13, Step:   102000, Batch Loss:     2.000738, Tokens per Sec:    15669, Lr: 0.000300\n",
            "2021-07-13 14:22:16,526 - INFO - joeynmt.training - Epoch  13, Step:   102200, Batch Loss:     1.820522, Tokens per Sec:    15622, Lr: 0.000300\n",
            "2021-07-13 14:22:44,417 - INFO - joeynmt.training - Epoch  13, Step:   102400, Batch Loss:     1.506176, Tokens per Sec:    15610, Lr: 0.000300\n",
            "2021-07-13 14:23:12,255 - INFO - joeynmt.training - Epoch  13, Step:   102600, Batch Loss:     1.910227, Tokens per Sec:    15524, Lr: 0.000300\n",
            "2021-07-13 14:23:40,379 - INFO - joeynmt.training - Epoch  13, Step:   102800, Batch Loss:     1.908933, Tokens per Sec:    15727, Lr: 0.000300\n",
            "2021-07-13 14:24:08,386 - INFO - joeynmt.training - Epoch  13, Step:   103000, Batch Loss:     1.987267, Tokens per Sec:    15816, Lr: 0.000300\n",
            "2021-07-13 14:24:35,954 - INFO - joeynmt.training - Epoch  13, Step:   103200, Batch Loss:     1.880414, Tokens per Sec:    15515, Lr: 0.000300\n",
            "2021-07-13 14:25:03,968 - INFO - joeynmt.training - Epoch  13, Step:   103400, Batch Loss:     1.865942, Tokens per Sec:    15601, Lr: 0.000300\n",
            "2021-07-13 14:25:31,784 - INFO - joeynmt.training - Epoch  13, Step:   103600, Batch Loss:     1.930695, Tokens per Sec:    15620, Lr: 0.000300\n",
            "2021-07-13 14:25:59,640 - INFO - joeynmt.training - Epoch  13, Step:   103800, Batch Loss:     1.956585, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-13 14:26:27,599 - INFO - joeynmt.training - Epoch  13, Step:   104000, Batch Loss:     2.044868, Tokens per Sec:    15524, Lr: 0.000300\n",
            "2021-07-13 14:26:55,467 - INFO - joeynmt.training - Epoch  13, Step:   104200, Batch Loss:     1.606789, Tokens per Sec:    15565, Lr: 0.000300\n",
            "2021-07-13 14:27:23,423 - INFO - joeynmt.training - Epoch  13, Step:   104400, Batch Loss:     1.906924, Tokens per Sec:    15779, Lr: 0.000300\n",
            "2021-07-13 14:27:51,194 - INFO - joeynmt.training - Epoch  13, Step:   104600, Batch Loss:     1.538698, Tokens per Sec:    15770, Lr: 0.000300\n",
            "2021-07-13 14:28:19,080 - INFO - joeynmt.training - Epoch  13, Step:   104800, Batch Loss:     1.833013, Tokens per Sec:    15754, Lr: 0.000300\n",
            "2021-07-13 14:28:47,078 - INFO - joeynmt.training - Epoch  13, Step:   105000, Batch Loss:     1.880932, Tokens per Sec:    15667, Lr: 0.000300\n",
            "2021-07-13 14:30:29,111 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:30:29,112 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:30:29,112 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:30:30,062 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:30:30,062 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:30:30,839 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:30:30,839 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:30:30,839 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:30:30,840 - INFO - joeynmt.training - \tHypothesis: If you have done good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-13 14:30:30,840 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:30:30,840 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:30:30,840 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:30:30,841 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ You are my beloved Son . ”\n",
            "2021-07-13 14:30:30,841 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:30:30,841 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:30:30,841 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:30:30,842 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-13 14:30:30,842 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:30:30,843 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:30:30,844 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:30:30,844 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 14:30:30,844 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   105000: bleu:  17.43, loss: 188108.0000, ppl:   8.0996, duration: 103.7656s\n",
            "2021-07-13 14:30:58,891 - INFO - joeynmt.training - Epoch  13, Step:   105200, Batch Loss:     2.147126, Tokens per Sec:    15226, Lr: 0.000300\n",
            "2021-07-13 14:31:26,779 - INFO - joeynmt.training - Epoch  13, Step:   105400, Batch Loss:     2.409321, Tokens per Sec:    15540, Lr: 0.000300\n",
            "2021-07-13 14:31:54,675 - INFO - joeynmt.training - Epoch  13, Step:   105600, Batch Loss:     1.827711, Tokens per Sec:    15572, Lr: 0.000300\n",
            "2021-07-13 14:32:22,568 - INFO - joeynmt.training - Epoch  13, Step:   105800, Batch Loss:     1.871163, Tokens per Sec:    15742, Lr: 0.000300\n",
            "2021-07-13 14:32:50,271 - INFO - joeynmt.training - Epoch  13, Step:   106000, Batch Loss:     2.119444, Tokens per Sec:    15609, Lr: 0.000300\n",
            "2021-07-13 14:33:17,990 - INFO - joeynmt.training - Epoch  13, Step:   106200, Batch Loss:     1.603793, Tokens per Sec:    15405, Lr: 0.000300\n",
            "2021-07-13 14:33:45,951 - INFO - joeynmt.training - Epoch  13, Step:   106400, Batch Loss:     1.884851, Tokens per Sec:    15690, Lr: 0.000300\n",
            "2021-07-13 14:34:13,793 - INFO - joeynmt.training - Epoch  13, Step:   106600, Batch Loss:     2.003050, Tokens per Sec:    15492, Lr: 0.000300\n",
            "2021-07-13 14:34:41,345 - INFO - joeynmt.training - Epoch  13, Step:   106800, Batch Loss:     1.832316, Tokens per Sec:    15586, Lr: 0.000300\n",
            "2021-07-13 14:35:08,985 - INFO - joeynmt.training - Epoch  13, Step:   107000, Batch Loss:     2.046747, Tokens per Sec:    15481, Lr: 0.000300\n",
            "2021-07-13 14:35:36,800 - INFO - joeynmt.training - Epoch  13, Step:   107200, Batch Loss:     1.867051, Tokens per Sec:    15594, Lr: 0.000300\n",
            "2021-07-13 14:36:04,615 - INFO - joeynmt.training - Epoch  13, Step:   107400, Batch Loss:     1.954657, Tokens per Sec:    15731, Lr: 0.000300\n",
            "2021-07-13 14:36:32,422 - INFO - joeynmt.training - Epoch  13, Step:   107600, Batch Loss:     1.846236, Tokens per Sec:    15423, Lr: 0.000300\n",
            "2021-07-13 14:37:00,571 - INFO - joeynmt.training - Epoch  13, Step:   107800, Batch Loss:     1.913850, Tokens per Sec:    15765, Lr: 0.000300\n",
            "2021-07-13 14:37:28,687 - INFO - joeynmt.training - Epoch  13, Step:   108000, Batch Loss:     2.088015, Tokens per Sec:    15990, Lr: 0.000300\n",
            "2021-07-13 14:37:56,471 - INFO - joeynmt.training - Epoch  13, Step:   108200, Batch Loss:     1.764895, Tokens per Sec:    15834, Lr: 0.000300\n",
            "2021-07-13 14:38:24,308 - INFO - joeynmt.training - Epoch  13, Step:   108400, Batch Loss:     2.043388, Tokens per Sec:    15407, Lr: 0.000300\n",
            "2021-07-13 14:38:52,144 - INFO - joeynmt.training - Epoch  13, Step:   108600, Batch Loss:     1.671540, Tokens per Sec:    15594, Lr: 0.000300\n",
            "2021-07-13 14:39:07,015 - INFO - joeynmt.training - Epoch  13: total training loss 16229.35\n",
            "2021-07-13 14:39:07,016 - INFO - joeynmt.training - EPOCH 14\n",
            "2021-07-13 14:39:20,941 - INFO - joeynmt.training - Epoch  14, Step:   108800, Batch Loss:     1.825713, Tokens per Sec:    14211, Lr: 0.000300\n",
            "2021-07-13 14:39:48,858 - INFO - joeynmt.training - Epoch  14, Step:   109000, Batch Loss:     2.033951, Tokens per Sec:    15724, Lr: 0.000300\n",
            "2021-07-13 14:40:16,935 - INFO - joeynmt.training - Epoch  14, Step:   109200, Batch Loss:     2.035896, Tokens per Sec:    15817, Lr: 0.000300\n",
            "2021-07-13 14:40:44,975 - INFO - joeynmt.training - Epoch  14, Step:   109400, Batch Loss:     1.738235, Tokens per Sec:    15817, Lr: 0.000300\n",
            "2021-07-13 14:41:12,573 - INFO - joeynmt.training - Epoch  14, Step:   109600, Batch Loss:     1.622285, Tokens per Sec:    15504, Lr: 0.000300\n",
            "2021-07-13 14:41:40,537 - INFO - joeynmt.training - Epoch  14, Step:   109800, Batch Loss:     1.939335, Tokens per Sec:    15642, Lr: 0.000300\n",
            "2021-07-13 14:42:08,180 - INFO - joeynmt.training - Epoch  14, Step:   110000, Batch Loss:     1.662580, Tokens per Sec:    15552, Lr: 0.000300\n",
            "2021-07-13 14:44:03,449 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:44:03,449 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:44:03,449 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:44:05,136 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:44:05,137 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:44:05,137 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:44:05,137 - INFO - joeynmt.training - \tHypothesis: If you have done good and suffer when you endure , that is acceptable to God . ”\n",
            "2021-07-13 14:44:05,137 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:44:05,138 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:44:05,138 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:44:05,138 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ You are my beloved Son , whom I have appreciated . ”\n",
            "2021-07-13 14:44:05,139 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:44:05,139 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:44:05,139 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:44:05,139 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to what Jesus prayed ?\n",
            "2021-07-13 14:44:05,140 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:44:05,141 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:44:05,141 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:44:05,141 - INFO - joeynmt.training - \tHypothesis: But Abigael did what he did to save his family .\n",
            "2021-07-13 14:44:05,141 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   110000: bleu:  16.45, loss: 188434.8438, ppl:   8.1290, duration: 116.9612s\n",
            "2021-07-13 14:44:33,430 - INFO - joeynmt.training - Epoch  14, Step:   110200, Batch Loss:     1.733215, Tokens per Sec:    15479, Lr: 0.000300\n",
            "2021-07-13 14:45:01,410 - INFO - joeynmt.training - Epoch  14, Step:   110400, Batch Loss:     2.017765, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-13 14:45:29,154 - INFO - joeynmt.training - Epoch  14, Step:   110600, Batch Loss:     1.957066, Tokens per Sec:    15520, Lr: 0.000300\n",
            "2021-07-13 14:45:57,096 - INFO - joeynmt.training - Epoch  14, Step:   110800, Batch Loss:     1.922125, Tokens per Sec:    15750, Lr: 0.000300\n",
            "2021-07-13 14:46:25,032 - INFO - joeynmt.training - Epoch  14, Step:   111000, Batch Loss:     1.817099, Tokens per Sec:    15683, Lr: 0.000300\n",
            "2021-07-13 14:46:52,888 - INFO - joeynmt.training - Epoch  14, Step:   111200, Batch Loss:     1.947055, Tokens per Sec:    15623, Lr: 0.000300\n",
            "2021-07-13 14:47:20,704 - INFO - joeynmt.training - Epoch  14, Step:   111400, Batch Loss:     2.109120, Tokens per Sec:    15610, Lr: 0.000300\n",
            "2021-07-13 14:47:48,594 - INFO - joeynmt.training - Epoch  14, Step:   111600, Batch Loss:     2.020448, Tokens per Sec:    15556, Lr: 0.000300\n",
            "2021-07-13 14:48:16,494 - INFO - joeynmt.training - Epoch  14, Step:   111800, Batch Loss:     1.903064, Tokens per Sec:    15604, Lr: 0.000300\n",
            "2021-07-13 14:48:44,153 - INFO - joeynmt.training - Epoch  14, Step:   112000, Batch Loss:     1.785157, Tokens per Sec:    15379, Lr: 0.000300\n",
            "2021-07-13 14:49:12,212 - INFO - joeynmt.training - Epoch  14, Step:   112200, Batch Loss:     2.019038, Tokens per Sec:    15702, Lr: 0.000300\n",
            "2021-07-13 14:49:39,984 - INFO - joeynmt.training - Epoch  14, Step:   112400, Batch Loss:     1.636775, Tokens per Sec:    15568, Lr: 0.000300\n",
            "2021-07-13 14:50:07,821 - INFO - joeynmt.training - Epoch  14, Step:   112600, Batch Loss:     1.927480, Tokens per Sec:    15461, Lr: 0.000300\n",
            "2021-07-13 14:50:35,735 - INFO - joeynmt.training - Epoch  14, Step:   112800, Batch Loss:     1.972640, Tokens per Sec:    15603, Lr: 0.000300\n",
            "2021-07-13 14:51:03,491 - INFO - joeynmt.training - Epoch  14, Step:   113000, Batch Loss:     1.765899, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-13 14:51:31,386 - INFO - joeynmt.training - Epoch  14, Step:   113200, Batch Loss:     2.133249, Tokens per Sec:    15630, Lr: 0.000300\n",
            "2021-07-13 14:51:59,216 - INFO - joeynmt.training - Epoch  14, Step:   113400, Batch Loss:     2.276182, Tokens per Sec:    15529, Lr: 0.000300\n",
            "2021-07-13 14:52:26,960 - INFO - joeynmt.training - Epoch  14, Step:   113600, Batch Loss:     1.953529, Tokens per Sec:    15666, Lr: 0.000300\n",
            "2021-07-13 14:52:54,349 - INFO - joeynmt.training - Epoch  14, Step:   113800, Batch Loss:     1.768600, Tokens per Sec:    15363, Lr: 0.000300\n",
            "2021-07-13 14:53:22,287 - INFO - joeynmt.training - Epoch  14, Step:   114000, Batch Loss:     1.970977, Tokens per Sec:    15667, Lr: 0.000300\n",
            "2021-07-13 14:53:50,047 - INFO - joeynmt.training - Epoch  14, Step:   114200, Batch Loss:     1.874169, Tokens per Sec:    15340, Lr: 0.000300\n",
            "2021-07-13 14:54:17,894 - INFO - joeynmt.training - Epoch  14, Step:   114400, Batch Loss:     1.799548, Tokens per Sec:    15729, Lr: 0.000300\n",
            "2021-07-13 14:54:45,740 - INFO - joeynmt.training - Epoch  14, Step:   114600, Batch Loss:     1.973489, Tokens per Sec:    15611, Lr: 0.000300\n",
            "2021-07-13 14:55:13,795 - INFO - joeynmt.training - Epoch  14, Step:   114800, Batch Loss:     1.905932, Tokens per Sec:    15455, Lr: 0.000300\n",
            "2021-07-13 14:55:41,907 - INFO - joeynmt.training - Epoch  14, Step:   115000, Batch Loss:     1.915919, Tokens per Sec:    15459, Lr: 0.000300\n",
            "2021-07-13 14:57:26,876 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:57:26,876 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:57:26,876 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:57:27,850 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:57:27,851 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:57:28,692 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:57:28,693 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:57:28,693 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:57:28,693 - INFO - joeynmt.training - \tHypothesis: And when you do good and suffer , you will be patient , and what is acceptable to God . ”\n",
            "2021-07-13 14:57:28,693 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:57:28,694 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:57:28,694 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:57:28,694 - INFO - joeynmt.training - \tHypothesis: After Jesus ’ baptism , he heard the voice from heaven : “ You are my beloved Son . ”\n",
            "2021-07-13 14:57:28,694 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:57:28,695 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:57:28,695 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:57:28,695 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 14:57:28,695 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:57:28,696 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:57:28,696 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:57:28,696 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 14:57:28,696 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   115000: bleu:  17.63, loss: 186669.4062, ppl:   7.9710, duration: 106.7884s\n",
            "2021-07-13 14:57:57,128 - INFO - joeynmt.training - Epoch  14, Step:   115200, Batch Loss:     1.998668, Tokens per Sec:    15552, Lr: 0.000300\n",
            "2021-07-13 14:58:25,288 - INFO - joeynmt.training - Epoch  14, Step:   115400, Batch Loss:     1.799327, Tokens per Sec:    15595, Lr: 0.000300\n",
            "2021-07-13 14:58:53,138 - INFO - joeynmt.training - Epoch  14, Step:   115600, Batch Loss:     1.763904, Tokens per Sec:    15701, Lr: 0.000300\n",
            "2021-07-13 14:59:21,211 - INFO - joeynmt.training - Epoch  14, Step:   115800, Batch Loss:     2.066498, Tokens per Sec:    15782, Lr: 0.000300\n",
            "2021-07-13 14:59:49,008 - INFO - joeynmt.training - Epoch  14, Step:   116000, Batch Loss:     2.063868, Tokens per Sec:    15405, Lr: 0.000300\n",
            "2021-07-13 15:00:16,777 - INFO - joeynmt.training - Epoch  14, Step:   116200, Batch Loss:     1.985151, Tokens per Sec:    15567, Lr: 0.000300\n",
            "2021-07-13 15:00:44,800 - INFO - joeynmt.training - Epoch  14, Step:   116400, Batch Loss:     1.751143, Tokens per Sec:    15713, Lr: 0.000300\n",
            "2021-07-13 15:01:12,927 - INFO - joeynmt.training - Epoch  14, Step:   116600, Batch Loss:     2.075837, Tokens per Sec:    15765, Lr: 0.000300\n",
            "2021-07-13 15:01:40,972 - INFO - joeynmt.training - Epoch  14, Step:   116800, Batch Loss:     2.048125, Tokens per Sec:    15592, Lr: 0.000300\n",
            "2021-07-13 15:02:08,739 - INFO - joeynmt.training - Epoch  14, Step:   117000, Batch Loss:     1.991221, Tokens per Sec:    15404, Lr: 0.000300\n",
            "2021-07-13 15:02:17,865 - INFO - joeynmt.training - Epoch  14: total training loss 16118.91\n",
            "2021-07-13 15:02:17,866 - INFO - joeynmt.training - EPOCH 15\n",
            "2021-07-13 15:02:37,671 - INFO - joeynmt.training - Epoch  15, Step:   117200, Batch Loss:     1.904531, Tokens per Sec:    14468, Lr: 0.000300\n",
            "2021-07-13 15:03:05,438 - INFO - joeynmt.training - Epoch  15, Step:   117400, Batch Loss:     1.724779, Tokens per Sec:    15518, Lr: 0.000300\n",
            "2021-07-13 15:03:33,129 - INFO - joeynmt.training - Epoch  15, Step:   117600, Batch Loss:     1.825588, Tokens per Sec:    15670, Lr: 0.000300\n",
            "2021-07-13 15:04:00,972 - INFO - joeynmt.training - Epoch  15, Step:   117800, Batch Loss:     1.871372, Tokens per Sec:    15620, Lr: 0.000300\n",
            "2021-07-13 15:04:28,767 - INFO - joeynmt.training - Epoch  15, Step:   118000, Batch Loss:     1.915806, Tokens per Sec:    15612, Lr: 0.000300\n",
            "2021-07-13 15:04:56,453 - INFO - joeynmt.training - Epoch  15, Step:   118200, Batch Loss:     1.976988, Tokens per Sec:    15428, Lr: 0.000300\n",
            "2021-07-13 15:05:24,545 - INFO - joeynmt.training - Epoch  15, Step:   118400, Batch Loss:     2.113744, Tokens per Sec:    15558, Lr: 0.000300\n",
            "2021-07-13 15:05:52,476 - INFO - joeynmt.training - Epoch  15, Step:   118600, Batch Loss:     1.742146, Tokens per Sec:    15578, Lr: 0.000300\n",
            "2021-07-13 15:06:20,748 - INFO - joeynmt.training - Epoch  15, Step:   118800, Batch Loss:     1.865194, Tokens per Sec:    15738, Lr: 0.000300\n",
            "2021-07-13 15:06:48,533 - INFO - joeynmt.training - Epoch  15, Step:   119000, Batch Loss:     1.797026, Tokens per Sec:    15617, Lr: 0.000300\n",
            "2021-07-13 15:07:16,476 - INFO - joeynmt.training - Epoch  15, Step:   119200, Batch Loss:     1.790494, Tokens per Sec:    15679, Lr: 0.000300\n",
            "2021-07-13 15:07:44,223 - INFO - joeynmt.training - Epoch  15, Step:   119400, Batch Loss:     1.846734, Tokens per Sec:    15403, Lr: 0.000300\n",
            "2021-07-13 15:08:12,389 - INFO - joeynmt.training - Epoch  15, Step:   119600, Batch Loss:     1.868113, Tokens per Sec:    15741, Lr: 0.000300\n",
            "2021-07-13 15:08:40,325 - INFO - joeynmt.training - Epoch  15, Step:   119800, Batch Loss:     1.871774, Tokens per Sec:    15616, Lr: 0.000300\n",
            "2021-07-13 15:09:08,171 - INFO - joeynmt.training - Epoch  15, Step:   120000, Batch Loss:     1.931325, Tokens per Sec:    15552, Lr: 0.000300\n",
            "2021-07-13 15:10:54,715 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:10:54,716 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:10:54,716 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:10:55,661 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:10:55,662 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:10:56,397 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:10:56,398 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 15:10:56,398 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 15:10:56,398 - INFO - joeynmt.training - \tHypothesis: “ If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-13 15:10:56,398 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:10:56,399 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 15:10:56,399 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 15:10:56,399 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 15:10:56,399 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:10:56,399 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 15:10:56,400 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 15:10:56,400 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 15:10:56,400 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:10:56,401 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 15:10:56,401 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 15:10:56,402 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 15:10:56,402 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   120000: bleu:  17.61, loss: 186289.9688, ppl:   7.9374, duration: 108.2303s\n",
            "2021-07-13 15:11:24,557 - INFO - joeynmt.training - Epoch  15, Step:   120200, Batch Loss:     2.012218, Tokens per Sec:    15125, Lr: 0.000300\n",
            "2021-07-13 15:11:52,314 - INFO - joeynmt.training - Epoch  15, Step:   120400, Batch Loss:     1.960992, Tokens per Sec:    15382, Lr: 0.000300\n",
            "2021-07-13 15:12:20,198 - INFO - joeynmt.training - Epoch  15, Step:   120600, Batch Loss:     2.003307, Tokens per Sec:    15655, Lr: 0.000300\n",
            "2021-07-13 15:12:48,054 - INFO - joeynmt.training - Epoch  15, Step:   120800, Batch Loss:     2.038593, Tokens per Sec:    15779, Lr: 0.000300\n",
            "2021-07-13 15:13:16,030 - INFO - joeynmt.training - Epoch  15, Step:   121000, Batch Loss:     1.895109, Tokens per Sec:    15632, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mv2bi0dWfw7"
      },
      "source": [
        "15 epochs completed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJWjURXjwSnG"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 120000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/models/lg_rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_rw_lhen_reverse_transformer\"', f'model_dir: \"models/lg_rw_lhen_reverse_transformer_continued\"').replace(\n",
        "        f'epochs: 30', f'epochs: 15')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY-9k-G7VMd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a3c7ef9f-22c5-4d59-9d8a-95607b24b468"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_lg_rw_lhen_reload.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"lg_rw_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"lg_rw_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer/120000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 15                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/lg_rw_lhen_reverse_transformer_continued\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG7hbW9vuvT2",
        "outputId": "867dcca8-91c3-4b2b-f5b9-2b7d41c8aab2"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_rw_lhen_reload.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-13 15:53:21,970 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-13 15:53:22,042 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-13 15:53:35,796 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-13 15:53:36,469 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-13 15:53:37,293 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-13 15:53:38,048 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-13 15:53:38,049 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-13 15:53:38,431 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-13 15:53:38.688099: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-13 15:53:42,331 - INFO - joeynmt.training - Total params: 12179456\n",
            "2021-07-13 15:53:52,972 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer/120000.ckpt\n",
            "2021-07-13 15:53:57,368 - INFO - joeynmt.helpers - cfg.name                           : lg_rw_lhen_reverse_transformer\n",
            "2021-07-13 15:53:57,368 - INFO - joeynmt.helpers - cfg.data.src                       : lg_rw_lh\n",
            "2021-07-13 15:53:57,368 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-13 15:53:57,368 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\n",
            "2021-07-13 15:53:57,368 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\n",
            "2021-07-13 15:53:57,369 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\n",
            "2021-07-13 15:53:57,369 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-13 15:53:57,369 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-13 15:53:57,369 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-13 15:53:57,369 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-13 15:53:57,369 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-13 15:53:57,369 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-13 15:53:57,369 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer/120000.ckpt\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-13 15:53:57,370 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-13 15:53:57,371 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-13 15:53:57,371 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-13 15:53:57,371 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-13 15:53:57,371 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-13 15:53:57,371 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-13 15:53:57,371 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-13 15:53:57,371 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-13 15:53:57,371 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-13 15:53:57,372 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-13 15:53:57,372 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-13 15:53:57,372 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-13 15:53:57,372 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-13 15:53:57,372 - INFO - joeynmt.helpers - cfg.training.epochs                : 15\n",
            "2021-07-13 15:53:57,372 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-13 15:53:57,372 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-13 15:53:57,373 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-13 15:53:57,373 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_rw_lhen_reverse_transformer_continued\n",
            "2021-07-13 15:53:57,373 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-13 15:53:57,373 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-13 15:53:57,373 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-13 15:53:57,373 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-13 15:53:57,373 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-13 15:53:57,373 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-13 15:53:57,374 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-13 15:53:57,374 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-13 15:53:57,374 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-13 15:53:57,374 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-13 15:53:57,374 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-13 15:53:57,374 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-13 15:53:57,374 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-13 15:53:57,375 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-13 15:53:57,375 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-13 15:53:57,375 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-13 15:53:57,375 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 15:53:57,375 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-13 15:53:57,375 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-13 15:53:57,375 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-13 15:53:57,375 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-13 15:53:57,376 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-13 15:53:57,376 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-13 15:53:57,376 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-13 15:53:57,376 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-13 15:53:57,376 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 15:53:57,376 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-13 15:53:57,376 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-13 15:53:57,376 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-13 15:53:57,377 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-13 15:53:57,377 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-13 15:53:57,377 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 665203,\n",
            "\tvalid 3000,\n",
            "\ttest 1000\n",
            "2021-07-13 15:53:57,377 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ at@@ andika okuk@@ olera ku m@@ azima ge nn@@ ali nj@@ iga , era nn@@ ak@@ ir@@ aba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obuf@@ uzi n’@@ okul@@ eka em@@ ikw@@ ano em@@ ibi gye nn@@ alina .\n",
            "\t[TRG] Ev@@ ent@@ ually , however , the tr@@ uth@@ s I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my polit@@ ical view@@ po@@ in@@ ts and associ@@ ations .\n",
            "2021-07-13 15:53:57,377 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 15:53:57,377 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 15:53:57,378 - INFO - joeynmt.helpers - Number of Src words (types): 4372\n",
            "2021-07-13 15:53:57,378 - INFO - joeynmt.helpers - Number of Trg words (types): 4372\n",
            "2021-07-13 15:53:57,378 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4372),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4372))\n",
            "2021-07-13 15:53:57,387 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-13 15:53:57,388 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-13 15:54:25,297 - INFO - joeynmt.training - Epoch   1, Step:   120200, Batch Loss:     1.999048, Tokens per Sec:    15259, Lr: 0.000300\n",
            "2021-07-13 15:54:51,379 - INFO - joeynmt.training - Epoch   1, Step:   120400, Batch Loss:     1.939702, Tokens per Sec:    16369, Lr: 0.000300\n",
            "2021-07-13 15:55:18,118 - INFO - joeynmt.training - Epoch   1, Step:   120600, Batch Loss:     1.987068, Tokens per Sec:    16326, Lr: 0.000300\n",
            "2021-07-13 15:55:45,362 - INFO - joeynmt.training - Epoch   1, Step:   120800, Batch Loss:     2.053347, Tokens per Sec:    16133, Lr: 0.000300\n",
            "2021-07-13 15:56:12,976 - INFO - joeynmt.training - Epoch   1, Step:   121000, Batch Loss:     1.894834, Tokens per Sec:    15837, Lr: 0.000300\n",
            "2021-07-13 15:56:40,405 - INFO - joeynmt.training - Epoch   1, Step:   121200, Batch Loss:     2.067291, Tokens per Sec:    15888, Lr: 0.000300\n",
            "2021-07-13 15:57:07,757 - INFO - joeynmt.training - Epoch   1, Step:   121400, Batch Loss:     1.874509, Tokens per Sec:    15812, Lr: 0.000300\n",
            "2021-07-13 15:57:35,519 - INFO - joeynmt.training - Epoch   1, Step:   121600, Batch Loss:     1.858049, Tokens per Sec:    16006, Lr: 0.000300\n",
            "2021-07-13 15:58:03,051 - INFO - joeynmt.training - Epoch   1, Step:   121800, Batch Loss:     1.767946, Tokens per Sec:    15883, Lr: 0.000300\n",
            "2021-07-13 15:58:30,377 - INFO - joeynmt.training - Epoch   1, Step:   122000, Batch Loss:     1.865787, Tokens per Sec:    15880, Lr: 0.000300\n",
            "2021-07-13 15:58:57,619 - INFO - joeynmt.training - Epoch   1, Step:   122200, Batch Loss:     2.008422, Tokens per Sec:    15737, Lr: 0.000300\n",
            "2021-07-13 15:59:25,163 - INFO - joeynmt.training - Epoch   1, Step:   122400, Batch Loss:     1.773299, Tokens per Sec:    15814, Lr: 0.000300\n",
            "2021-07-13 15:59:52,886 - INFO - joeynmt.training - Epoch   1, Step:   122600, Batch Loss:     1.792150, Tokens per Sec:    16023, Lr: 0.000300\n",
            "2021-07-13 16:00:20,215 - INFO - joeynmt.training - Epoch   1, Step:   122800, Batch Loss:     2.003268, Tokens per Sec:    15897, Lr: 0.000300\n",
            "2021-07-13 16:00:47,714 - INFO - joeynmt.training - Epoch   1, Step:   123000, Batch Loss:     1.969397, Tokens per Sec:    15801, Lr: 0.000300\n",
            "2021-07-13 16:01:15,140 - INFO - joeynmt.training - Epoch   1, Step:   123200, Batch Loss:     2.087269, Tokens per Sec:    15895, Lr: 0.000300\n",
            "2021-07-13 16:01:42,432 - INFO - joeynmt.training - Epoch   1, Step:   123400, Batch Loss:     1.961835, Tokens per Sec:    15767, Lr: 0.000300\n",
            "2021-07-13 16:02:09,844 - INFO - joeynmt.training - Epoch   1, Step:   123600, Batch Loss:     2.104177, Tokens per Sec:    15801, Lr: 0.000300\n",
            "2021-07-13 16:02:37,306 - INFO - joeynmt.training - Epoch   1, Step:   123800, Batch Loss:     1.726304, Tokens per Sec:    15859, Lr: 0.000300\n",
            "2021-07-13 16:03:04,471 - INFO - joeynmt.training - Epoch   1, Step:   124000, Batch Loss:     1.887681, Tokens per Sec:    15685, Lr: 0.000300\n",
            "2021-07-13 16:03:31,693 - INFO - joeynmt.training - Epoch   1, Step:   124200, Batch Loss:     1.765199, Tokens per Sec:    15772, Lr: 0.000300\n",
            "2021-07-13 16:03:59,403 - INFO - joeynmt.training - Epoch   1, Step:   124400, Batch Loss:     2.047973, Tokens per Sec:    16007, Lr: 0.000300\n",
            "2021-07-13 16:04:26,913 - INFO - joeynmt.training - Epoch   1, Step:   124600, Batch Loss:     1.683761, Tokens per Sec:    15705, Lr: 0.000300\n",
            "2021-07-13 16:04:54,435 - INFO - joeynmt.training - Epoch   1, Step:   124800, Batch Loss:     1.679617, Tokens per Sec:    15731, Lr: 0.000300\n",
            "2021-07-13 16:05:22,010 - INFO - joeynmt.training - Epoch   1, Step:   125000, Batch Loss:     1.829846, Tokens per Sec:    15767, Lr: 0.000300\n",
            "2021-07-13 16:06:58,432 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 16:06:58,432 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 16:06:58,432 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 16:06:59,336 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 16:06:59,336 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 16:07:00,158 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 16:07:00,159 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 16:07:00,159 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 16:07:00,159 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-13 16:07:00,160 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 16:07:00,160 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 16:07:00,160 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 16:07:00,161 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I have loved . ”\n",
            "2021-07-13 16:07:00,161 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 16:07:00,161 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 16:07:00,162 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 16:07:00,162 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-13 16:07:00,162 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 16:07:00,163 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 16:07:00,163 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 16:07:00,163 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 16:07:00,163 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step   125000: bleu:  17.83, loss: 185210.0625, ppl:   7.8427, duration: 98.1524s\n",
            "2021-07-13 16:07:27,502 - INFO - joeynmt.training - Epoch   1, Step:   125200, Batch Loss:     1.818974, Tokens per Sec:    15421, Lr: 0.000300\n",
            "2021-07-13 16:07:54,776 - INFO - joeynmt.training - Epoch   1, Step:   125400, Batch Loss:     1.822160, Tokens per Sec:    15825, Lr: 0.000300\n",
            "2021-07-13 16:08:00,293 - INFO - joeynmt.training - Epoch   1: total training loss 10405.64\n",
            "2021-07-13 16:08:00,294 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-13 16:08:23,330 - INFO - joeynmt.training - Epoch   2, Step:   125600, Batch Loss:     1.943117, Tokens per Sec:    15254, Lr: 0.000300\n",
            "2021-07-13 16:08:50,373 - INFO - joeynmt.training - Epoch   2, Step:   125800, Batch Loss:     2.088959, Tokens per Sec:    15667, Lr: 0.000300\n",
            "2021-07-13 16:09:17,997 - INFO - joeynmt.training - Epoch   2, Step:   126000, Batch Loss:     2.015700, Tokens per Sec:    16055, Lr: 0.000300\n",
            "2021-07-13 16:09:45,379 - INFO - joeynmt.training - Epoch   2, Step:   126200, Batch Loss:     1.766038, Tokens per Sec:    15829, Lr: 0.000300\n",
            "2021-07-13 16:10:12,769 - INFO - joeynmt.training - Epoch   2, Step:   126400, Batch Loss:     2.027717, Tokens per Sec:    16089, Lr: 0.000300\n",
            "2021-07-13 16:10:40,153 - INFO - joeynmt.training - Epoch   2, Step:   126600, Batch Loss:     1.909769, Tokens per Sec:    15762, Lr: 0.000300\n",
            "2021-07-13 16:11:07,738 - INFO - joeynmt.training - Epoch   2, Step:   126800, Batch Loss:     1.995372, Tokens per Sec:    15918, Lr: 0.000300\n",
            "2021-07-13 16:11:35,008 - INFO - joeynmt.training - Epoch   2, Step:   127000, Batch Loss:     1.980807, Tokens per Sec:    15806, Lr: 0.000300\n",
            "2021-07-13 16:12:02,405 - INFO - joeynmt.training - Epoch   2, Step:   127200, Batch Loss:     1.992735, Tokens per Sec:    15914, Lr: 0.000300\n",
            "2021-07-13 16:12:29,779 - INFO - joeynmt.training - Epoch   2, Step:   127400, Batch Loss:     1.786845, Tokens per Sec:    15922, Lr: 0.000300\n",
            "2021-07-13 16:12:57,067 - INFO - joeynmt.training - Epoch   2, Step:   127600, Batch Loss:     1.848117, Tokens per Sec:    15722, Lr: 0.000300\n",
            "2021-07-13 16:13:24,481 - INFO - joeynmt.training - Epoch   2, Step:   127800, Batch Loss:     1.922962, Tokens per Sec:    15772, Lr: 0.000300\n",
            "2021-07-13 16:13:51,735 - INFO - joeynmt.training - Epoch   2, Step:   128000, Batch Loss:     2.014780, Tokens per Sec:    15785, Lr: 0.000300\n",
            "2021-07-13 16:14:18,792 - INFO - joeynmt.training - Epoch   2, Step:   128200, Batch Loss:     1.641980, Tokens per Sec:    15682, Lr: 0.000300\n",
            "2021-07-13 16:14:46,333 - INFO - joeynmt.training - Epoch   2, Step:   128400, Batch Loss:     1.811027, Tokens per Sec:    15876, Lr: 0.000300\n",
            "2021-07-13 16:15:13,706 - INFO - joeynmt.training - Epoch   2, Step:   128600, Batch Loss:     1.723700, Tokens per Sec:    15838, Lr: 0.000300\n",
            "2021-07-13 16:15:40,952 - INFO - joeynmt.training - Epoch   2, Step:   128800, Batch Loss:     1.970090, Tokens per Sec:    15728, Lr: 0.000300\n",
            "2021-07-13 16:16:08,530 - INFO - joeynmt.training - Epoch   2, Step:   129000, Batch Loss:     1.934855, Tokens per Sec:    15971, Lr: 0.000300\n",
            "2021-07-13 16:16:35,946 - INFO - joeynmt.training - Epoch   2, Step:   129200, Batch Loss:     1.310023, Tokens per Sec:    15834, Lr: 0.000300\n",
            "2021-07-13 16:17:03,414 - INFO - joeynmt.training - Epoch   2, Step:   129400, Batch Loss:     1.851458, Tokens per Sec:    15965, Lr: 0.000300\n",
            "2021-07-13 16:17:30,560 - INFO - joeynmt.training - Epoch   2, Step:   129600, Batch Loss:     1.805514, Tokens per Sec:    15850, Lr: 0.000300\n",
            "2021-07-13 16:17:57,664 - INFO - joeynmt.training - Epoch   2, Step:   129800, Batch Loss:     1.830359, Tokens per Sec:    16043, Lr: 0.000300\n",
            "2021-07-13 16:18:25,115 - INFO - joeynmt.training - Epoch   2, Step:   130000, Batch Loss:     1.932215, Tokens per Sec:    15996, Lr: 0.000300\n",
            "2021-07-13 16:19:59,641 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 16:19:59,641 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 16:19:59,642 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 16:20:00,540 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 16:20:00,540 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 16:20:01,319 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 16:20:01,321 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 16:20:01,321 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 16:20:01,322 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-13 16:20:01,322 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 16:20:01,322 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 16:20:01,322 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 16:20:01,323 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 16:20:01,323 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 16:20:01,323 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 16:20:01,323 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 16:20:01,323 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-13 16:20:01,324 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 16:20:01,324 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 16:20:01,324 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 16:20:01,324 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 16:20:01,325 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   130000: bleu:  17.96, loss: 184858.4531, ppl:   7.8121, duration: 96.2086s\n",
            "2021-07-13 16:20:28,750 - INFO - joeynmt.training - Epoch   2, Step:   130200, Batch Loss:     1.805428, Tokens per Sec:    15614, Lr: 0.000300\n",
            "2021-07-13 16:20:55,898 - INFO - joeynmt.training - Epoch   2, Step:   130400, Batch Loss:     2.103298, Tokens per Sec:    15855, Lr: 0.000300\n",
            "2021-07-13 16:21:23,350 - INFO - joeynmt.training - Epoch   2, Step:   130600, Batch Loss:     1.888847, Tokens per Sec:    16148, Lr: 0.000300\n",
            "2021-07-13 16:21:50,457 - INFO - joeynmt.training - Epoch   2, Step:   130800, Batch Loss:     1.925758, Tokens per Sec:    15730, Lr: 0.000300\n",
            "2021-07-13 16:22:17,668 - INFO - joeynmt.training - Epoch   2, Step:   131000, Batch Loss:     1.830573, Tokens per Sec:    15936, Lr: 0.000300\n",
            "2021-07-13 16:22:44,953 - INFO - joeynmt.training - Epoch   2, Step:   131200, Batch Loss:     2.016127, Tokens per Sec:    15953, Lr: 0.000300\n",
            "2021-07-13 16:23:12,079 - INFO - joeynmt.training - Epoch   2, Step:   131400, Batch Loss:     1.680542, Tokens per Sec:    16007, Lr: 0.000300\n",
            "2021-07-13 16:23:39,522 - INFO - joeynmt.training - Epoch   2, Step:   131600, Batch Loss:     1.906636, Tokens per Sec:    15939, Lr: 0.000300\n",
            "2021-07-13 16:24:06,675 - INFO - joeynmt.training - Epoch   2, Step:   131800, Batch Loss:     1.886155, Tokens per Sec:    15922, Lr: 0.000300\n",
            "2021-07-13 16:24:33,832 - INFO - joeynmt.training - Epoch   2, Step:   132000, Batch Loss:     2.007584, Tokens per Sec:    15830, Lr: 0.000300\n",
            "2021-07-13 16:25:00,868 - INFO - joeynmt.training - Epoch   2, Step:   132200, Batch Loss:     2.124438, Tokens per Sec:    15617, Lr: 0.000300\n",
            "2021-07-13 16:25:28,116 - INFO - joeynmt.training - Epoch   2, Step:   132400, Batch Loss:     1.778883, Tokens per Sec:    15989, Lr: 0.000300\n",
            "2021-07-13 16:25:55,344 - INFO - joeynmt.training - Epoch   2, Step:   132600, Batch Loss:     1.789794, Tokens per Sec:    15846, Lr: 0.000300\n",
            "2021-07-13 16:26:22,660 - INFO - joeynmt.training - Epoch   2, Step:   132800, Batch Loss:     1.849859, Tokens per Sec:    16066, Lr: 0.000300\n",
            "2021-07-13 16:26:50,273 - INFO - joeynmt.training - Epoch   2, Step:   133000, Batch Loss:     2.004608, Tokens per Sec:    16146, Lr: 0.000300\n",
            "2021-07-13 16:27:17,266 - INFO - joeynmt.training - Epoch   2, Step:   133200, Batch Loss:     1.950812, Tokens per Sec:    15855, Lr: 0.000300\n",
            "2021-07-13 16:27:44,484 - INFO - joeynmt.training - Epoch   2, Step:   133400, Batch Loss:     1.974890, Tokens per Sec:    15995, Lr: 0.000300\n",
            "2021-07-13 16:28:11,625 - INFO - joeynmt.training - Epoch   2, Step:   133600, Batch Loss:     1.635961, Tokens per Sec:    15741, Lr: 0.000300\n",
            "2021-07-13 16:28:39,205 - INFO - joeynmt.training - Epoch   2, Step:   133800, Batch Loss:     2.036786, Tokens per Sec:    15795, Lr: 0.000300\n",
            "2021-07-13 16:28:42,111 - INFO - joeynmt.training - Epoch   2: total training loss 15938.89\n",
            "2021-07-13 16:28:42,112 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-13 16:29:07,676 - INFO - joeynmt.training - Epoch   3, Step:   134000, Batch Loss:     1.878085, Tokens per Sec:    15129, Lr: 0.000300\n",
            "2021-07-13 16:29:35,119 - INFO - joeynmt.training - Epoch   3, Step:   134200, Batch Loss:     1.930692, Tokens per Sec:    15959, Lr: 0.000300\n",
            "2021-07-13 16:30:02,678 - INFO - joeynmt.training - Epoch   3, Step:   134400, Batch Loss:     1.895838, Tokens per Sec:    15919, Lr: 0.000300\n",
            "2021-07-13 16:30:30,103 - INFO - joeynmt.training - Epoch   3, Step:   134600, Batch Loss:     2.235950, Tokens per Sec:    15976, Lr: 0.000300\n",
            "2021-07-13 16:30:57,503 - INFO - joeynmt.training - Epoch   3, Step:   134800, Batch Loss:     2.020241, Tokens per Sec:    15953, Lr: 0.000300\n",
            "2021-07-13 16:31:24,590 - INFO - joeynmt.training - Epoch   3, Step:   135000, Batch Loss:     1.825475, Tokens per Sec:    15739, Lr: 0.000300\n",
            "2021-07-13 16:33:01,638 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 16:33:01,639 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 16:33:01,639 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 16:33:02,555 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 16:33:02,555 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 16:33:03,413 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 16:33:03,415 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 16:33:03,416 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 16:33:03,416 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-13 16:33:03,416 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 16:33:03,417 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 16:33:03,417 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 16:33:03,417 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 16:33:03,417 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 16:33:03,418 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 16:33:03,418 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 16:33:03,418 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-13 16:33:03,418 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 16:33:03,418 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 16:33:03,418 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 16:33:03,419 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-13 16:33:03,419 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   135000: bleu:  17.77, loss: 184420.8125, ppl:   7.7742, duration: 98.8287s\n",
            "2021-07-13 16:33:31,198 - INFO - joeynmt.training - Epoch   3, Step:   135200, Batch Loss:     1.845129, Tokens per Sec:    15926, Lr: 0.000300\n",
            "2021-07-13 16:33:58,488 - INFO - joeynmt.training - Epoch   3, Step:   135400, Batch Loss:     1.954263, Tokens per Sec:    15924, Lr: 0.000300\n",
            "2021-07-13 16:34:25,936 - INFO - joeynmt.training - Epoch   3, Step:   135600, Batch Loss:     2.060234, Tokens per Sec:    16117, Lr: 0.000300\n",
            "2021-07-13 16:34:53,519 - INFO - joeynmt.training - Epoch   3, Step:   135800, Batch Loss:     1.951561, Tokens per Sec:    16156, Lr: 0.000300\n",
            "2021-07-13 16:35:20,517 - INFO - joeynmt.training - Epoch   3, Step:   136000, Batch Loss:     2.090137, Tokens per Sec:    15759, Lr: 0.000300\n",
            "2021-07-13 16:35:47,815 - INFO - joeynmt.training - Epoch   3, Step:   136200, Batch Loss:     1.694341, Tokens per Sec:    15937, Lr: 0.000300\n",
            "2021-07-13 16:36:15,405 - INFO - joeynmt.training - Epoch   3, Step:   136400, Batch Loss:     2.128018, Tokens per Sec:    16041, Lr: 0.000300\n",
            "2021-07-13 16:36:42,468 - INFO - joeynmt.training - Epoch   3, Step:   136600, Batch Loss:     1.565525, Tokens per Sec:    15829, Lr: 0.000300\n",
            "2021-07-13 16:37:09,569 - INFO - joeynmt.training - Epoch   3, Step:   136800, Batch Loss:     1.872784, Tokens per Sec:    15751, Lr: 0.000300\n",
            "2021-07-13 16:37:36,889 - INFO - joeynmt.training - Epoch   3, Step:   137000, Batch Loss:     2.070091, Tokens per Sec:    15980, Lr: 0.000300\n",
            "2021-07-13 16:38:04,208 - INFO - joeynmt.training - Epoch   3, Step:   137200, Batch Loss:     1.791338, Tokens per Sec:    15934, Lr: 0.000300\n",
            "2021-07-13 16:38:31,524 - INFO - joeynmt.training - Epoch   3, Step:   137400, Batch Loss:     1.917635, Tokens per Sec:    16014, Lr: 0.000300\n",
            "2021-07-13 16:38:58,779 - INFO - joeynmt.training - Epoch   3, Step:   137600, Batch Loss:     1.569116, Tokens per Sec:    16010, Lr: 0.000300\n",
            "2021-07-13 16:39:25,933 - INFO - joeynmt.training - Epoch   3, Step:   137800, Batch Loss:     1.695719, Tokens per Sec:    15823, Lr: 0.000300\n",
            "2021-07-13 16:39:53,245 - INFO - joeynmt.training - Epoch   3, Step:   138000, Batch Loss:     1.853785, Tokens per Sec:    16000, Lr: 0.000300\n",
            "2021-07-13 16:40:20,354 - INFO - joeynmt.training - Epoch   3, Step:   138200, Batch Loss:     1.909434, Tokens per Sec:    15939, Lr: 0.000300\n",
            "2021-07-13 16:40:47,598 - INFO - joeynmt.training - Epoch   3, Step:   138400, Batch Loss:     1.722768, Tokens per Sec:    15805, Lr: 0.000300\n",
            "2021-07-13 16:41:14,821 - INFO - joeynmt.training - Epoch   3, Step:   138600, Batch Loss:     2.075862, Tokens per Sec:    15746, Lr: 0.000300\n",
            "2021-07-13 16:41:42,186 - INFO - joeynmt.training - Epoch   3, Step:   138800, Batch Loss:     1.752767, Tokens per Sec:    15967, Lr: 0.000300\n",
            "2021-07-13 16:42:09,274 - INFO - joeynmt.training - Epoch   3, Step:   139000, Batch Loss:     1.828918, Tokens per Sec:    15855, Lr: 0.000300\n",
            "2021-07-13 16:42:36,613 - INFO - joeynmt.training - Epoch   3, Step:   139200, Batch Loss:     1.902658, Tokens per Sec:    16019, Lr: 0.000300\n",
            "2021-07-13 16:43:03,847 - INFO - joeynmt.training - Epoch   3, Step:   139400, Batch Loss:     1.698028, Tokens per Sec:    15925, Lr: 0.000300\n",
            "2021-07-13 16:43:31,106 - INFO - joeynmt.training - Epoch   3, Step:   139600, Batch Loss:     1.690875, Tokens per Sec:    16089, Lr: 0.000300\n",
            "2021-07-13 16:43:58,390 - INFO - joeynmt.training - Epoch   3, Step:   139800, Batch Loss:     1.993050, Tokens per Sec:    15953, Lr: 0.000300\n",
            "2021-07-13 16:44:25,643 - INFO - joeynmt.training - Epoch   3, Step:   140000, Batch Loss:     1.837261, Tokens per Sec:    15912, Lr: 0.000300\n",
            "2021-07-13 16:45:59,035 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 16:45:59,036 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 16:45:59,036 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 16:45:59,949 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 16:45:59,949 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 16:46:00,744 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 16:46:00,745 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 16:46:00,745 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 16:46:00,745 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-13 16:46:00,745 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 16:46:00,746 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 16:46:00,746 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 16:46:00,746 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-13 16:46:00,746 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 16:46:00,746 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 16:46:00,746 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 16:46:00,747 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 16:46:00,747 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 16:46:00,747 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 16:46:00,747 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 16:46:00,747 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 16:46:00,748 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   140000: bleu:  18.17, loss: 183049.6406, ppl:   7.6565, duration: 95.1040s\n",
            "2021-07-13 16:46:28,368 - INFO - joeynmt.training - Epoch   3, Step:   140200, Batch Loss:     1.964707, Tokens per Sec:    15553, Lr: 0.000300\n",
            "2021-07-13 16:46:55,751 - INFO - joeynmt.training - Epoch   3, Step:   140400, Batch Loss:     1.788109, Tokens per Sec:    15973, Lr: 0.000300\n",
            "2021-07-13 16:47:23,019 - INFO - joeynmt.training - Epoch   3, Step:   140600, Batch Loss:     2.091331, Tokens per Sec:    16106, Lr: 0.000300\n",
            "2021-07-13 16:47:50,190 - INFO - joeynmt.training - Epoch   3, Step:   140800, Batch Loss:     1.833199, Tokens per Sec:    15972, Lr: 0.000300\n",
            "2021-07-13 16:48:17,257 - INFO - joeynmt.training - Epoch   3, Step:   141000, Batch Loss:     1.428391, Tokens per Sec:    15880, Lr: 0.000300\n",
            "2021-07-13 16:48:44,642 - INFO - joeynmt.training - Epoch   3, Step:   141200, Batch Loss:     1.903546, Tokens per Sec:    15995, Lr: 0.000300\n",
            "2021-07-13 16:49:12,025 - INFO - joeynmt.training - Epoch   3, Step:   141400, Batch Loss:     2.027385, Tokens per Sec:    16056, Lr: 0.000300\n",
            "2021-07-13 16:49:39,522 - INFO - joeynmt.training - Epoch   3, Step:   141600, Batch Loss:     1.771055, Tokens per Sec:    16102, Lr: 0.000300\n",
            "2021-07-13 16:50:06,761 - INFO - joeynmt.training - Epoch   3, Step:   141800, Batch Loss:     1.749628, Tokens per Sec:    15950, Lr: 0.000300\n",
            "2021-07-13 16:50:34,090 - INFO - joeynmt.training - Epoch   3, Step:   142000, Batch Loss:     1.954539, Tokens per Sec:    16004, Lr: 0.000300\n",
            "2021-07-13 16:50:57,890 - INFO - joeynmt.training - Epoch   3: total training loss 15788.61\n",
            "2021-07-13 16:50:57,891 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-13 16:51:02,056 - INFO - joeynmt.training - Epoch   4, Step:   142200, Batch Loss:     1.831430, Tokens per Sec:    12585, Lr: 0.000300\n",
            "2021-07-13 16:51:29,228 - INFO - joeynmt.training - Epoch   4, Step:   142400, Batch Loss:     1.924579, Tokens per Sec:    15978, Lr: 0.000300\n",
            "2021-07-13 16:51:56,535 - INFO - joeynmt.training - Epoch   4, Step:   142600, Batch Loss:     1.853722, Tokens per Sec:    16172, Lr: 0.000300\n",
            "2021-07-13 16:52:23,677 - INFO - joeynmt.training - Epoch   4, Step:   142800, Batch Loss:     2.025816, Tokens per Sec:    15939, Lr: 0.000300\n",
            "2021-07-13 16:52:50,602 - INFO - joeynmt.training - Epoch   4, Step:   143000, Batch Loss:     1.432616, Tokens per Sec:    15791, Lr: 0.000300\n",
            "2021-07-13 16:53:17,967 - INFO - joeynmt.training - Epoch   4, Step:   143200, Batch Loss:     1.866131, Tokens per Sec:    16263, Lr: 0.000300\n",
            "2021-07-13 16:53:44,993 - INFO - joeynmt.training - Epoch   4, Step:   143400, Batch Loss:     1.476258, Tokens per Sec:    15952, Lr: 0.000300\n",
            "2021-07-13 16:54:12,048 - INFO - joeynmt.training - Epoch   4, Step:   143600, Batch Loss:     1.699262, Tokens per Sec:    16015, Lr: 0.000300\n",
            "2021-07-13 16:54:39,129 - INFO - joeynmt.training - Epoch   4, Step:   143800, Batch Loss:     2.029446, Tokens per Sec:    16100, Lr: 0.000300\n",
            "2021-07-13 16:55:06,418 - INFO - joeynmt.training - Epoch   4, Step:   144000, Batch Loss:     1.827815, Tokens per Sec:    15982, Lr: 0.000300\n",
            "2021-07-13 16:55:33,373 - INFO - joeynmt.training - Epoch   4, Step:   144200, Batch Loss:     1.983274, Tokens per Sec:    15899, Lr: 0.000300\n",
            "2021-07-13 16:56:00,401 - INFO - joeynmt.training - Epoch   4, Step:   144400, Batch Loss:     1.671059, Tokens per Sec:    15877, Lr: 0.000300\n",
            "2021-07-13 16:56:27,333 - INFO - joeynmt.training - Epoch   4, Step:   144600, Batch Loss:     1.854518, Tokens per Sec:    15902, Lr: 0.000300\n",
            "2021-07-13 16:56:54,456 - INFO - joeynmt.training - Epoch   4, Step:   144800, Batch Loss:     1.920876, Tokens per Sec:    16052, Lr: 0.000300\n",
            "2021-07-13 16:57:21,429 - INFO - joeynmt.training - Epoch   4, Step:   145000, Batch Loss:     1.615796, Tokens per Sec:    15943, Lr: 0.000300\n",
            "2021-07-13 16:58:57,516 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 16:58:57,516 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 16:58:57,516 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 16:58:58,419 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 16:58:58,419 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 16:58:59,475 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 16:58:59,476 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 16:58:59,477 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 16:58:59,477 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , that is acceptable to God . ”\n",
            "2021-07-13 16:58:59,477 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 16:58:59,477 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 16:58:59,477 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 16:58:59,478 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 16:58:59,478 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 16:58:59,478 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 16:58:59,478 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 16:58:59,478 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ reply ?\n",
            "2021-07-13 16:58:59,479 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 16:58:59,479 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 16:58:59,479 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 16:58:59,479 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 16:58:59,480 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   145000: bleu:  18.16, loss: 182512.5781, ppl:   7.6109, duration: 98.0501s\n",
            "2021-07-13 16:59:26,835 - INFO - joeynmt.training - Epoch   4, Step:   145200, Batch Loss:     1.795605, Tokens per Sec:    15808, Lr: 0.000300\n",
            "2021-07-13 16:59:54,040 - INFO - joeynmt.training - Epoch   4, Step:   145400, Batch Loss:     1.956223, Tokens per Sec:    15815, Lr: 0.000300\n",
            "2021-07-13 17:00:21,275 - INFO - joeynmt.training - Epoch   4, Step:   145600, Batch Loss:     2.146206, Tokens per Sec:    16044, Lr: 0.000300\n",
            "2021-07-13 17:00:48,313 - INFO - joeynmt.training - Epoch   4, Step:   145800, Batch Loss:     1.946715, Tokens per Sec:    15893, Lr: 0.000300\n",
            "2021-07-13 17:01:15,669 - INFO - joeynmt.training - Epoch   4, Step:   146000, Batch Loss:     1.749878, Tokens per Sec:    16001, Lr: 0.000300\n",
            "2021-07-13 17:01:42,848 - INFO - joeynmt.training - Epoch   4, Step:   146200, Batch Loss:     1.839411, Tokens per Sec:    15934, Lr: 0.000300\n",
            "2021-07-13 17:02:10,038 - INFO - joeynmt.training - Epoch   4, Step:   146400, Batch Loss:     1.921234, Tokens per Sec:    15974, Lr: 0.000300\n",
            "2021-07-13 17:02:37,111 - INFO - joeynmt.training - Epoch   4, Step:   146600, Batch Loss:     1.828053, Tokens per Sec:    16132, Lr: 0.000300\n",
            "2021-07-13 17:03:04,406 - INFO - joeynmt.training - Epoch   4, Step:   146800, Batch Loss:     2.017777, Tokens per Sec:    16242, Lr: 0.000300\n",
            "2021-07-13 17:03:31,403 - INFO - joeynmt.training - Epoch   4, Step:   147000, Batch Loss:     1.763885, Tokens per Sec:    15817, Lr: 0.000300\n",
            "2021-07-13 17:03:58,693 - INFO - joeynmt.training - Epoch   4, Step:   147200, Batch Loss:     1.809128, Tokens per Sec:    16033, Lr: 0.000300\n",
            "2021-07-13 17:04:26,250 - INFO - joeynmt.training - Epoch   4, Step:   147400, Batch Loss:     1.749104, Tokens per Sec:    16196, Lr: 0.000300\n",
            "2021-07-13 17:04:53,235 - INFO - joeynmt.training - Epoch   4, Step:   147600, Batch Loss:     1.869747, Tokens per Sec:    16037, Lr: 0.000300\n",
            "2021-07-13 17:05:20,374 - INFO - joeynmt.training - Epoch   4, Step:   147800, Batch Loss:     1.957896, Tokens per Sec:    16117, Lr: 0.000300\n",
            "2021-07-13 17:05:47,499 - INFO - joeynmt.training - Epoch   4, Step:   148000, Batch Loss:     1.549477, Tokens per Sec:    15842, Lr: 0.000300\n",
            "2021-07-13 17:06:14,587 - INFO - joeynmt.training - Epoch   4, Step:   148200, Batch Loss:     1.675739, Tokens per Sec:    15993, Lr: 0.000300\n",
            "2021-07-13 17:06:41,552 - INFO - joeynmt.training - Epoch   4, Step:   148400, Batch Loss:     2.048003, Tokens per Sec:    15911, Lr: 0.000300\n",
            "2021-07-13 17:07:08,848 - INFO - joeynmt.training - Epoch   4, Step:   148600, Batch Loss:     1.894330, Tokens per Sec:    16282, Lr: 0.000300\n",
            "2021-07-13 17:07:35,809 - INFO - joeynmt.training - Epoch   4, Step:   148800, Batch Loss:     2.041837, Tokens per Sec:    16001, Lr: 0.000300\n",
            "2021-07-13 17:08:02,675 - INFO - joeynmt.training - Epoch   4, Step:   149000, Batch Loss:     1.879771, Tokens per Sec:    15941, Lr: 0.000300\n",
            "2021-07-13 17:08:29,725 - INFO - joeynmt.training - Epoch   4, Step:   149200, Batch Loss:     1.932968, Tokens per Sec:    16033, Lr: 0.000300\n",
            "2021-07-13 17:08:56,918 - INFO - joeynmt.training - Epoch   4, Step:   149400, Batch Loss:     1.890391, Tokens per Sec:    16216, Lr: 0.000300\n",
            "2021-07-13 17:09:24,053 - INFO - joeynmt.training - Epoch   4, Step:   149600, Batch Loss:     2.073523, Tokens per Sec:    15926, Lr: 0.000300\n",
            "2021-07-13 17:09:51,295 - INFO - joeynmt.training - Epoch   4, Step:   149800, Batch Loss:     1.678521, Tokens per Sec:    16154, Lr: 0.000300\n",
            "2021-07-13 17:10:18,769 - INFO - joeynmt.training - Epoch   4, Step:   150000, Batch Loss:     1.913505, Tokens per Sec:    16173, Lr: 0.000300\n",
            "2021-07-13 17:11:54,064 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 17:11:54,064 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 17:11:54,064 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 17:11:55,013 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 17:11:55,013 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 17:11:56,000 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 17:11:56,001 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 17:11:56,001 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 17:11:56,001 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-13 17:11:56,001 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 17:11:56,002 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 17:11:56,002 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 17:11:56,002 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-13 17:11:56,002 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 17:11:56,003 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 17:11:56,003 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 17:11:56,003 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ response ?\n",
            "2021-07-13 17:11:56,003 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 17:11:56,003 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 17:11:56,003 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 17:11:56,004 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 17:11:56,004 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   150000: bleu:  18.48, loss: 182100.4219, ppl:   7.5761, duration: 97.2341s\n",
            "2021-07-13 17:12:23,372 - INFO - joeynmt.training - Epoch   4, Step:   150200, Batch Loss:     2.047515, Tokens per Sec:    15918, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9053_TiCW0hM"
      },
      "source": [
        "4 epochs completed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPNpviJLxpOZ"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 150000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/models/lg_rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_rw_lhen_reverse_transformer\"', f'model_dir: \"models/lg_rw_lhen_reverse_transformer_continued2\"').replace(\n",
        "        f'epochs: 30', f'epochs: 11')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isr44rkRIiBR",
        "collapsed": true,
        "outputId": "d194c39d-7b57-498b-9969-1df3a3970498"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_lg_rw_lhen_reload2.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"lg_rw_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"lg_rw_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued/150000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 11                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/lg_rw_lhen_reverse_transformer_continued2\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTYZWoNULWn7",
        "outputId": "7ee15da6-c7b1-4f68-e17a-19d5a7578e37"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_rw_lhen_reload2.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 13:01:03,868 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 13:01:03,939 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-14 13:01:21,005 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-14 13:01:22,065 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-14 13:01:24,475 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-14 13:01:26,043 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-14 13:01:26,043 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 13:01:26,475 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-14 13:01:26.790320: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 13:01:30,874 - INFO - joeynmt.training - Total params: 12179456\n",
            "2021-07-14 13:01:39,587 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued/150000.ckpt\n",
            "2021-07-14 13:01:40,151 - INFO - joeynmt.helpers - cfg.name                           : lg_rw_lhen_reverse_transformer\n",
            "2021-07-14 13:01:40,151 - INFO - joeynmt.helpers - cfg.data.src                       : lg_rw_lh\n",
            "2021-07-14 13:01:40,152 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-14 13:01:40,152 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\n",
            "2021-07-14 13:01:40,152 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\n",
            "2021-07-14 13:01:40,152 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\n",
            "2021-07-14 13:01:40,152 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-14 13:01:40,153 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-14 13:01:40,153 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-14 13:01:40,153 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-14 13:01:40,153 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-14 13:01:40,154 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-14 13:01:40,154 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-14 13:01:40,154 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued/150000.ckpt\n",
            "2021-07-14 13:01:40,154 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-14 13:01:40,155 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-14 13:01:40,155 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-14 13:01:40,155 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-14 13:01:40,155 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-14 13:01:40,155 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-14 13:01:40,156 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-14 13:01:40,156 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-14 13:01:40,156 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-14 13:01:40,156 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-14 13:01:40,157 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-14 13:01:40,157 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-14 13:01:40,157 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-14 13:01:40,157 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-14 13:01:40,157 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-14 13:01:40,158 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-14 13:01:40,158 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-14 13:01:40,158 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-14 13:01:40,158 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-14 13:01:40,158 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-14 13:01:40,159 - INFO - joeynmt.helpers - cfg.training.epochs                : 11\n",
            "2021-07-14 13:01:40,159 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-14 13:01:40,159 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-14 13:01:40,159 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-14 13:01:40,160 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_rw_lhen_reverse_transformer_continued2\n",
            "2021-07-14 13:01:40,160 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-14 13:01:40,160 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-14 13:01:40,160 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-14 13:01:40,160 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-14 13:01:40,161 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-14 13:01:40,161 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-14 13:01:40,161 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-14 13:01:40,161 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-14 13:01:40,162 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-14 13:01:40,162 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-14 13:01:40,162 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-14 13:01:40,162 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-14 13:01:40,162 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-14 13:01:40,163 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-14 13:01:40,163 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-14 13:01:40,163 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-14 13:01:40,163 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-14 13:01:40,164 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-14 13:01:40,164 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-14 13:01:40,164 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-14 13:01:40,164 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-14 13:01:40,164 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-14 13:01:40,165 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-14 13:01:40,165 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-14 13:01:40,165 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-14 13:01:40,165 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-14 13:01:40,166 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-14 13:01:40,166 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-14 13:01:40,166 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-14 13:01:40,166 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-14 13:01:40,166 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-14 13:01:40,167 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 665203,\n",
            "\tvalid 3000,\n",
            "\ttest 1000\n",
            "2021-07-14 13:01:40,167 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ at@@ andika okuk@@ olera ku m@@ azima ge nn@@ ali nj@@ iga , era nn@@ ak@@ ir@@ aba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obuf@@ uzi n’@@ okul@@ eka em@@ ikw@@ ano em@@ ibi gye nn@@ alina .\n",
            "\t[TRG] Ev@@ ent@@ ually , however , the tr@@ uth@@ s I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my polit@@ ical view@@ po@@ in@@ ts and associ@@ ations .\n",
            "2021-07-14 13:01:40,167 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-14 13:01:40,168 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-14 13:01:40,168 - INFO - joeynmt.helpers - Number of Src words (types): 4372\n",
            "2021-07-14 13:01:40,168 - INFO - joeynmt.helpers - Number of Trg words (types): 4372\n",
            "2021-07-14 13:01:40,168 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4372),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4372))\n",
            "2021-07-14 13:01:40,183 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-14 13:01:40,184 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-14 13:02:45,782 - INFO - joeynmt.training - Epoch   1, Step:   150200, Batch Loss:     2.032448, Tokens per Sec:     6641, Lr: 0.000300\n",
            "2021-07-14 13:03:47,442 - INFO - joeynmt.training - Epoch   1, Step:   150400, Batch Loss:     1.914239, Tokens per Sec:     7122, Lr: 0.000300\n",
            "2021-07-14 13:04:29,912 - INFO - joeynmt.training - Epoch   1: total training loss 1014.29\n",
            "2021-07-14 13:04:29,913 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-14 13:04:49,503 - INFO - joeynmt.training - Epoch   2, Step:   150600, Batch Loss:     2.282768, Tokens per Sec:     6657, Lr: 0.000300\n",
            "2021-07-14 13:05:50,710 - INFO - joeynmt.training - Epoch   2, Step:   150800, Batch Loss:     1.850985, Tokens per Sec:     7107, Lr: 0.000300\n",
            "2021-07-14 13:06:51,894 - INFO - joeynmt.training - Epoch   2, Step:   151000, Batch Loss:     1.773810, Tokens per Sec:     7044, Lr: 0.000300\n",
            "2021-07-14 13:07:53,007 - INFO - joeynmt.training - Epoch   2, Step:   151200, Batch Loss:     2.026636, Tokens per Sec:     7074, Lr: 0.000300\n",
            "2021-07-14 13:08:54,192 - INFO - joeynmt.training - Epoch   2, Step:   151400, Batch Loss:     1.677393, Tokens per Sec:     6976, Lr: 0.000300\n",
            "2021-07-14 13:09:55,290 - INFO - joeynmt.training - Epoch   2, Step:   151600, Batch Loss:     1.885274, Tokens per Sec:     7052, Lr: 0.000300\n",
            "2021-07-14 13:10:56,443 - INFO - joeynmt.training - Epoch   2, Step:   151800, Batch Loss:     1.914955, Tokens per Sec:     7005, Lr: 0.000300\n",
            "2021-07-14 13:11:57,699 - INFO - joeynmt.training - Epoch   2, Step:   152000, Batch Loss:     1.710456, Tokens per Sec:     7143, Lr: 0.000300\n",
            "2021-07-14 13:12:58,873 - INFO - joeynmt.training - Epoch   2, Step:   152200, Batch Loss:     1.821463, Tokens per Sec:     7075, Lr: 0.000300\n",
            "2021-07-14 13:13:59,748 - INFO - joeynmt.training - Epoch   2, Step:   152400, Batch Loss:     1.978517, Tokens per Sec:     7033, Lr: 0.000300\n",
            "2021-07-14 13:15:00,962 - INFO - joeynmt.training - Epoch   2, Step:   152600, Batch Loss:     1.831816, Tokens per Sec:     7153, Lr: 0.000300\n",
            "2021-07-14 13:16:02,060 - INFO - joeynmt.training - Epoch   2, Step:   152800, Batch Loss:     1.655544, Tokens per Sec:     7150, Lr: 0.000300\n",
            "2021-07-14 13:17:03,161 - INFO - joeynmt.training - Epoch   2, Step:   153000, Batch Loss:     1.776861, Tokens per Sec:     7201, Lr: 0.000300\n",
            "2021-07-14 13:18:04,383 - INFO - joeynmt.training - Epoch   2, Step:   153200, Batch Loss:     1.970587, Tokens per Sec:     7096, Lr: 0.000300\n",
            "2021-07-14 13:19:05,266 - INFO - joeynmt.training - Epoch   2, Step:   153400, Batch Loss:     1.873790, Tokens per Sec:     7012, Lr: 0.000300\n",
            "2021-07-14 13:20:06,541 - INFO - joeynmt.training - Epoch   2, Step:   153600, Batch Loss:     1.776045, Tokens per Sec:     7141, Lr: 0.000300\n",
            "2021-07-14 13:21:08,310 - INFO - joeynmt.training - Epoch   2, Step:   153800, Batch Loss:     1.979102, Tokens per Sec:     7194, Lr: 0.000300\n",
            "2021-07-14 13:22:08,970 - INFO - joeynmt.training - Epoch   2, Step:   154000, Batch Loss:     2.084695, Tokens per Sec:     7067, Lr: 0.000300\n",
            "2021-07-14 13:23:09,493 - INFO - joeynmt.training - Epoch   2, Step:   154200, Batch Loss:     2.083983, Tokens per Sec:     7082, Lr: 0.000300\n",
            "2021-07-14 13:24:10,558 - INFO - joeynmt.training - Epoch   2, Step:   154400, Batch Loss:     1.480398, Tokens per Sec:     7040, Lr: 0.000300\n",
            "2021-07-14 13:25:11,629 - INFO - joeynmt.training - Epoch   2, Step:   154600, Batch Loss:     1.792819, Tokens per Sec:     7118, Lr: 0.000300\n",
            "2021-07-14 13:26:13,110 - INFO - joeynmt.training - Epoch   2, Step:   154800, Batch Loss:     1.481830, Tokens per Sec:     7177, Lr: 0.000300\n",
            "2021-07-14 13:27:14,670 - INFO - joeynmt.training - Epoch   2, Step:   155000, Batch Loss:     2.068580, Tokens per Sec:     7060, Lr: 0.000300\n",
            "2021-07-14 13:30:04,649 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:30:04,650 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:30:04,650 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:30:05,748 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 13:30:05,749 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 13:30:06,726 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:30:06,728 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 13:30:06,728 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 13:30:06,728 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , you will be pained when you endure , that is what is acceptable to God . ”\n",
            "2021-07-14 13:30:06,728 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:30:06,729 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 13:30:06,729 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 13:30:06,729 - INFO - joeynmt.training - \tHypothesis: After Jesus ’ baptism , he heard the voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-14 13:30:06,730 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:30:06,730 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 13:30:06,731 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 13:30:06,731 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-14 13:30:06,731 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:30:06,732 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 13:30:06,732 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 13:30:06,732 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-14 13:30:06,733 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   155000: bleu:  18.18, loss: 181684.3125, ppl:   7.5412, duration: 172.0624s\n",
            "2021-07-14 13:31:08,410 - INFO - joeynmt.training - Epoch   2, Step:   155200, Batch Loss:     1.874179, Tokens per Sec:     7178, Lr: 0.000300\n",
            "2021-07-14 13:32:09,338 - INFO - joeynmt.training - Epoch   2, Step:   155400, Batch Loss:     1.750396, Tokens per Sec:     7103, Lr: 0.000300\n",
            "2021-07-14 13:33:10,822 - INFO - joeynmt.training - Epoch   2, Step:   155600, Batch Loss:     1.893002, Tokens per Sec:     7157, Lr: 0.000300\n",
            "2021-07-14 13:34:12,046 - INFO - joeynmt.training - Epoch   2, Step:   155800, Batch Loss:     1.863680, Tokens per Sec:     7090, Lr: 0.000300\n",
            "2021-07-14 13:35:12,742 - INFO - joeynmt.training - Epoch   2, Step:   156000, Batch Loss:     1.937104, Tokens per Sec:     7067, Lr: 0.000300\n",
            "2021-07-14 13:36:14,008 - INFO - joeynmt.training - Epoch   2, Step:   156200, Batch Loss:     1.696842, Tokens per Sec:     7080, Lr: 0.000300\n",
            "2021-07-14 13:37:15,338 - INFO - joeynmt.training - Epoch   2, Step:   156400, Batch Loss:     2.014809, Tokens per Sec:     7093, Lr: 0.000300\n",
            "2021-07-14 13:38:15,513 - INFO - joeynmt.training - Epoch   2, Step:   156600, Batch Loss:     1.847660, Tokens per Sec:     7011, Lr: 0.000300\n",
            "2021-07-14 13:39:16,736 - INFO - joeynmt.training - Epoch   2, Step:   156800, Batch Loss:     1.818051, Tokens per Sec:     7157, Lr: 0.000300\n",
            "2021-07-14 13:40:17,989 - INFO - joeynmt.training - Epoch   2, Step:   157000, Batch Loss:     1.920402, Tokens per Sec:     7130, Lr: 0.000300\n",
            "2021-07-14 13:41:19,061 - INFO - joeynmt.training - Epoch   2, Step:   157200, Batch Loss:     2.071540, Tokens per Sec:     7096, Lr: 0.000300\n",
            "2021-07-14 13:42:21,135 - INFO - joeynmt.training - Epoch   2, Step:   157400, Batch Loss:     1.920230, Tokens per Sec:     7206, Lr: 0.000300\n",
            "2021-07-14 13:43:22,233 - INFO - joeynmt.training - Epoch   2, Step:   157600, Batch Loss:     1.997393, Tokens per Sec:     7092, Lr: 0.000300\n",
            "2021-07-14 13:44:23,826 - INFO - joeynmt.training - Epoch   2, Step:   157800, Batch Loss:     1.709167, Tokens per Sec:     7196, Lr: 0.000300\n",
            "2021-07-14 13:45:25,100 - INFO - joeynmt.training - Epoch   2, Step:   158000, Batch Loss:     1.937141, Tokens per Sec:     7082, Lr: 0.000300\n",
            "2021-07-14 13:46:26,678 - INFO - joeynmt.training - Epoch   2, Step:   158200, Batch Loss:     1.776510, Tokens per Sec:     7062, Lr: 0.000300\n",
            "2021-07-14 13:47:27,709 - INFO - joeynmt.training - Epoch   2, Step:   158400, Batch Loss:     2.072599, Tokens per Sec:     7090, Lr: 0.000300\n",
            "2021-07-14 13:48:29,021 - INFO - joeynmt.training - Epoch   2, Step:   158600, Batch Loss:     2.017946, Tokens per Sec:     7103, Lr: 0.000300\n",
            "2021-07-14 13:49:30,108 - INFO - joeynmt.training - Epoch   2, Step:   158800, Batch Loss:     1.832272, Tokens per Sec:     7085, Lr: 0.000300\n",
            "2021-07-14 13:50:03,238 - INFO - joeynmt.training - Epoch   2: total training loss 15644.72\n",
            "2021-07-14 13:50:03,239 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-14 13:50:32,005 - INFO - joeynmt.training - Epoch   3, Step:   159000, Batch Loss:     1.848696, Tokens per Sec:     6635, Lr: 0.000300\n",
            "2021-07-14 13:51:33,472 - INFO - joeynmt.training - Epoch   3, Step:   159200, Batch Loss:     2.041929, Tokens per Sec:     7111, Lr: 0.000300\n",
            "2021-07-14 13:52:34,246 - INFO - joeynmt.training - Epoch   3, Step:   159400, Batch Loss:     1.686931, Tokens per Sec:     7079, Lr: 0.000300\n",
            "2021-07-14 13:53:35,800 - INFO - joeynmt.training - Epoch   3, Step:   159600, Batch Loss:     1.839258, Tokens per Sec:     7113, Lr: 0.000300\n",
            "2021-07-14 13:54:37,087 - INFO - joeynmt.training - Epoch   3, Step:   159800, Batch Loss:     1.995748, Tokens per Sec:     7139, Lr: 0.000300\n",
            "2021-07-14 13:55:38,114 - INFO - joeynmt.training - Epoch   3, Step:   160000, Batch Loss:     1.906703, Tokens per Sec:     7143, Lr: 0.000300\n",
            "2021-07-14 13:58:38,144 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:58:38,144 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:58:38,145 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:58:40,159 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:58:40,160 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 13:58:40,160 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 13:58:40,160 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-14 13:58:40,161 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:58:40,161 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 13:58:40,161 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 13:58:40,162 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-14 13:58:40,162 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:58:40,163 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 13:58:40,163 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 13:58:40,164 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-14 13:58:40,164 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:58:40,164 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 13:58:40,165 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 13:58:40,166 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-14 13:58:40,166 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   160000: bleu:  18.36, loss: 181753.3750, ppl:   7.5470, duration: 182.0510s\n",
            "2021-07-14 13:59:41,110 - INFO - joeynmt.training - Epoch   3, Step:   160200, Batch Loss:     2.090101, Tokens per Sec:     7084, Lr: 0.000300\n",
            "2021-07-14 14:00:41,371 - INFO - joeynmt.training - Epoch   3, Step:   160400, Batch Loss:     2.020955, Tokens per Sec:     7012, Lr: 0.000300\n",
            "2021-07-14 14:01:42,494 - INFO - joeynmt.training - Epoch   3, Step:   160600, Batch Loss:     1.811898, Tokens per Sec:     7075, Lr: 0.000300\n",
            "2021-07-14 14:02:43,519 - INFO - joeynmt.training - Epoch   3, Step:   160800, Batch Loss:     1.801417, Tokens per Sec:     7102, Lr: 0.000300\n",
            "2021-07-14 14:03:43,873 - INFO - joeynmt.training - Epoch   3, Step:   161000, Batch Loss:     1.736481, Tokens per Sec:     6985, Lr: 0.000300\n",
            "2021-07-14 14:04:44,584 - INFO - joeynmt.training - Epoch   3, Step:   161200, Batch Loss:     1.970109, Tokens per Sec:     7139, Lr: 0.000300\n",
            "2021-07-14 14:05:45,804 - INFO - joeynmt.training - Epoch   3, Step:   161400, Batch Loss:     1.393309, Tokens per Sec:     7081, Lr: 0.000300\n",
            "2021-07-14 14:06:47,393 - INFO - joeynmt.training - Epoch   3, Step:   161600, Batch Loss:     1.695590, Tokens per Sec:     7121, Lr: 0.000300\n",
            "2021-07-14 14:07:48,544 - INFO - joeynmt.training - Epoch   3, Step:   161800, Batch Loss:     1.850876, Tokens per Sec:     7142, Lr: 0.000300\n",
            "2021-07-14 14:08:49,585 - INFO - joeynmt.training - Epoch   3, Step:   162000, Batch Loss:     1.910527, Tokens per Sec:     7180, Lr: 0.000300\n",
            "2021-07-14 14:09:50,896 - INFO - joeynmt.training - Epoch   3, Step:   162200, Batch Loss:     1.863328, Tokens per Sec:     7094, Lr: 0.000300\n",
            "2021-07-14 14:10:52,248 - INFO - joeynmt.training - Epoch   3, Step:   162400, Batch Loss:     1.845213, Tokens per Sec:     7220, Lr: 0.000300\n",
            "2021-07-14 14:11:53,545 - INFO - joeynmt.training - Epoch   3, Step:   162600, Batch Loss:     1.854849, Tokens per Sec:     7154, Lr: 0.000300\n",
            "2021-07-14 14:12:54,585 - INFO - joeynmt.training - Epoch   3, Step:   162800, Batch Loss:     1.789649, Tokens per Sec:     7100, Lr: 0.000300\n",
            "2021-07-14 14:13:55,786 - INFO - joeynmt.training - Epoch   3, Step:   163000, Batch Loss:     1.740053, Tokens per Sec:     7122, Lr: 0.000300\n",
            "2021-07-14 14:14:57,592 - INFO - joeynmt.training - Epoch   3, Step:   163200, Batch Loss:     1.515326, Tokens per Sec:     7177, Lr: 0.000300\n",
            "2021-07-14 14:15:59,102 - INFO - joeynmt.training - Epoch   3, Step:   163400, Batch Loss:     1.896539, Tokens per Sec:     7049, Lr: 0.000300\n",
            "2021-07-14 14:17:00,215 - INFO - joeynmt.training - Epoch   3, Step:   163600, Batch Loss:     1.838592, Tokens per Sec:     7121, Lr: 0.000300\n",
            "2021-07-14 14:18:01,506 - INFO - joeynmt.training - Epoch   3, Step:   163800, Batch Loss:     1.788909, Tokens per Sec:     7014, Lr: 0.000300\n",
            "2021-07-14 14:19:02,378 - INFO - joeynmt.training - Epoch   3, Step:   164000, Batch Loss:     1.683852, Tokens per Sec:     7079, Lr: 0.000300\n",
            "2021-07-14 14:20:03,733 - INFO - joeynmt.training - Epoch   3, Step:   164200, Batch Loss:     1.951160, Tokens per Sec:     7105, Lr: 0.000300\n",
            "2021-07-14 14:21:04,231 - INFO - joeynmt.training - Epoch   3, Step:   164400, Batch Loss:     2.079143, Tokens per Sec:     7015, Lr: 0.000300\n",
            "2021-07-14 14:22:05,523 - INFO - joeynmt.training - Epoch   3, Step:   164600, Batch Loss:     1.735397, Tokens per Sec:     7203, Lr: 0.000300\n",
            "2021-07-14 14:23:06,556 - INFO - joeynmt.training - Epoch   3, Step:   164800, Batch Loss:     1.902229, Tokens per Sec:     7032, Lr: 0.000300\n",
            "2021-07-14 14:24:07,897 - INFO - joeynmt.training - Epoch   3, Step:   165000, Batch Loss:     1.770995, Tokens per Sec:     7138, Lr: 0.000300\n",
            "2021-07-14 14:26:54,725 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:26:54,726 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:26:54,726 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:26:55,781 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 14:26:55,782 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 14:26:56,711 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:26:56,712 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 14:26:56,712 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 14:26:56,712 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-14 14:26:56,712 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:26:56,713 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 14:26:56,713 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 14:26:56,713 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-14 14:26:56,714 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:26:56,714 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 14:26:56,715 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 14:26:56,715 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to what Jesus prayed ?\n",
            "2021-07-14 14:26:56,715 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:26:56,716 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 14:26:56,716 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 14:26:56,716 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-14 14:26:56,716 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   165000: bleu:  18.38, loss: 180658.2969, ppl:   7.4556, duration: 168.8186s\n",
            "2021-07-14 14:27:57,705 - INFO - joeynmt.training - Epoch   3, Step:   165200, Batch Loss:     2.040919, Tokens per Sec:     7068, Lr: 0.000300\n",
            "2021-07-14 14:28:58,641 - INFO - joeynmt.training - Epoch   3, Step:   165400, Batch Loss:     1.873295, Tokens per Sec:     7024, Lr: 0.000300\n",
            "2021-07-14 14:29:59,815 - INFO - joeynmt.training - Epoch   3, Step:   165600, Batch Loss:     1.865036, Tokens per Sec:     7065, Lr: 0.000300\n",
            "2021-07-14 14:31:01,704 - INFO - joeynmt.training - Epoch   3, Step:   165800, Batch Loss:     1.721196, Tokens per Sec:     7178, Lr: 0.000300\n",
            "2021-07-14 14:32:02,817 - INFO - joeynmt.training - Epoch   3, Step:   166000, Batch Loss:     1.844388, Tokens per Sec:     7182, Lr: 0.000300\n",
            "2021-07-14 14:33:03,464 - INFO - joeynmt.training - Epoch   3, Step:   166200, Batch Loss:     2.062353, Tokens per Sec:     7002, Lr: 0.000300\n",
            "2021-07-14 14:34:04,684 - INFO - joeynmt.training - Epoch   3, Step:   166400, Batch Loss:     1.798673, Tokens per Sec:     7130, Lr: 0.000300\n",
            "2021-07-14 14:35:05,949 - INFO - joeynmt.training - Epoch   3, Step:   166600, Batch Loss:     2.107016, Tokens per Sec:     7115, Lr: 0.000300\n",
            "2021-07-14 14:36:07,009 - INFO - joeynmt.training - Epoch   3, Step:   166800, Batch Loss:     1.878115, Tokens per Sec:     7082, Lr: 0.000300\n",
            "2021-07-14 14:37:08,478 - INFO - joeynmt.training - Epoch   3, Step:   167000, Batch Loss:     1.519411, Tokens per Sec:     7180, Lr: 0.000300\n",
            "2021-07-14 14:38:10,097 - INFO - joeynmt.training - Epoch   3, Step:   167200, Batch Loss:     2.043432, Tokens per Sec:     7131, Lr: 0.000300\n",
            "2021-07-14 14:38:34,219 - INFO - joeynmt.training - Epoch   3: total training loss 15567.49\n",
            "2021-07-14 14:38:34,219 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-14 14:39:11,862 - INFO - joeynmt.training - Epoch   4, Step:   167400, Batch Loss:     1.723746, Tokens per Sec:     6688, Lr: 0.000300\n",
            "2021-07-14 14:40:12,762 - INFO - joeynmt.training - Epoch   4, Step:   167600, Batch Loss:     2.175025, Tokens per Sec:     7046, Lr: 0.000300\n",
            "2021-07-14 14:41:13,693 - INFO - joeynmt.training - Epoch   4, Step:   167800, Batch Loss:     1.865669, Tokens per Sec:     7070, Lr: 0.000300\n",
            "2021-07-14 14:42:14,479 - INFO - joeynmt.training - Epoch   4, Step:   168000, Batch Loss:     1.912126, Tokens per Sec:     7141, Lr: 0.000300\n",
            "2021-07-14 14:43:16,221 - INFO - joeynmt.training - Epoch   4, Step:   168200, Batch Loss:     1.985431, Tokens per Sec:     7208, Lr: 0.000300\n",
            "2021-07-14 14:44:17,581 - INFO - joeynmt.training - Epoch   4, Step:   168400, Batch Loss:     2.132361, Tokens per Sec:     7161, Lr: 0.000300\n",
            "2021-07-14 14:45:18,818 - INFO - joeynmt.training - Epoch   4, Step:   168600, Batch Loss:     1.952556, Tokens per Sec:     7078, Lr: 0.000300\n",
            "2021-07-14 14:46:19,824 - INFO - joeynmt.training - Epoch   4, Step:   168800, Batch Loss:     1.818094, Tokens per Sec:     7114, Lr: 0.000300\n",
            "2021-07-14 14:47:21,273 - INFO - joeynmt.training - Epoch   4, Step:   169000, Batch Loss:     2.092102, Tokens per Sec:     7157, Lr: 0.000300\n",
            "2021-07-14 14:48:22,680 - INFO - joeynmt.training - Epoch   4, Step:   169200, Batch Loss:     1.774138, Tokens per Sec:     7089, Lr: 0.000300\n",
            "2021-07-14 14:49:24,312 - INFO - joeynmt.training - Epoch   4, Step:   169400, Batch Loss:     2.001663, Tokens per Sec:     7167, Lr: 0.000300\n",
            "2021-07-14 14:50:26,261 - INFO - joeynmt.training - Epoch   4, Step:   169600, Batch Loss:     1.958793, Tokens per Sec:     7173, Lr: 0.000300\n",
            "2021-07-14 14:51:27,858 - INFO - joeynmt.training - Epoch   4, Step:   169800, Batch Loss:     1.785385, Tokens per Sec:     7114, Lr: 0.000300\n",
            "2021-07-14 14:52:28,389 - INFO - joeynmt.training - Epoch   4, Step:   170000, Batch Loss:     1.971104, Tokens per Sec:     6968, Lr: 0.000300\n",
            "2021-07-14 14:55:21,582 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:55:21,582 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:55:21,582 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:55:23,499 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:55:23,499 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 14:55:23,501 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 14:55:23,502 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-14 14:55:23,502 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:55:23,503 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 14:55:23,503 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 14:55:23,503 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-14 14:55:23,503 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:55:23,504 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 14:55:23,504 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 14:55:23,505 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-14 14:55:23,505 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:55:23,505 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 14:55:23,506 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 14:55:23,506 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-14 14:55:23,506 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   170000: bleu:  18.64, loss: 180725.3438, ppl:   7.4612, duration: 175.1171s\n",
            "2021-07-14 14:56:25,184 - INFO - joeynmt.training - Epoch   4, Step:   170200, Batch Loss:     2.008619, Tokens per Sec:     7107, Lr: 0.000300\n",
            "2021-07-14 14:57:26,408 - INFO - joeynmt.training - Epoch   4, Step:   170400, Batch Loss:     2.394411, Tokens per Sec:     7127, Lr: 0.000300\n",
            "2021-07-14 14:58:27,196 - INFO - joeynmt.training - Epoch   4, Step:   170600, Batch Loss:     1.600548, Tokens per Sec:     7101, Lr: 0.000300\n",
            "2021-07-14 14:59:27,802 - INFO - joeynmt.training - Epoch   4, Step:   170800, Batch Loss:     1.747916, Tokens per Sec:     7122, Lr: 0.000300\n",
            "2021-07-14 15:00:28,700 - INFO - joeynmt.training - Epoch   4, Step:   171000, Batch Loss:     1.796378, Tokens per Sec:     7014, Lr: 0.000300\n",
            "2021-07-14 15:01:29,873 - INFO - joeynmt.training - Epoch   4, Step:   171200, Batch Loss:     1.808771, Tokens per Sec:     7208, Lr: 0.000300\n",
            "2021-07-14 15:02:30,731 - INFO - joeynmt.training - Epoch   4, Step:   171400, Batch Loss:     1.960550, Tokens per Sec:     7111, Lr: 0.000300\n",
            "2021-07-14 15:03:31,732 - INFO - joeynmt.training - Epoch   4, Step:   171600, Batch Loss:     2.063624, Tokens per Sec:     7135, Lr: 0.000300\n",
            "2021-07-14 15:04:32,381 - INFO - joeynmt.training - Epoch   4, Step:   171800, Batch Loss:     1.932791, Tokens per Sec:     7134, Lr: 0.000300\n",
            "2021-07-14 15:05:33,273 - INFO - joeynmt.training - Epoch   4, Step:   172000, Batch Loss:     1.861553, Tokens per Sec:     7124, Lr: 0.000300\n",
            "2021-07-14 15:06:34,306 - INFO - joeynmt.training - Epoch   4, Step:   172200, Batch Loss:     1.871636, Tokens per Sec:     7141, Lr: 0.000300\n",
            "2021-07-14 15:07:34,961 - INFO - joeynmt.training - Epoch   4, Step:   172400, Batch Loss:     1.789522, Tokens per Sec:     7142, Lr: 0.000300\n",
            "2021-07-14 15:08:36,290 - INFO - joeynmt.training - Epoch   4, Step:   172600, Batch Loss:     1.776520, Tokens per Sec:     7200, Lr: 0.000300\n",
            "2021-07-14 15:09:37,557 - INFO - joeynmt.training - Epoch   4, Step:   172800, Batch Loss:     1.779181, Tokens per Sec:     7226, Lr: 0.000300\n",
            "2021-07-14 15:10:38,871 - INFO - joeynmt.training - Epoch   4, Step:   173000, Batch Loss:     1.872576, Tokens per Sec:     7098, Lr: 0.000300\n",
            "2021-07-14 15:11:39,645 - INFO - joeynmt.training - Epoch   4, Step:   173200, Batch Loss:     1.984432, Tokens per Sec:     7131, Lr: 0.000300\n",
            "2021-07-14 15:12:40,871 - INFO - joeynmt.training - Epoch   4, Step:   173400, Batch Loss:     1.855010, Tokens per Sec:     7135, Lr: 0.000300\n",
            "2021-07-14 15:13:41,276 - INFO - joeynmt.training - Epoch   4, Step:   173600, Batch Loss:     1.783731, Tokens per Sec:     7023, Lr: 0.000300\n",
            "2021-07-14 15:14:42,437 - INFO - joeynmt.training - Epoch   4, Step:   173800, Batch Loss:     2.013283, Tokens per Sec:     7207, Lr: 0.000300\n",
            "2021-07-14 15:15:43,080 - INFO - joeynmt.training - Epoch   4, Step:   174000, Batch Loss:     1.900404, Tokens per Sec:     7123, Lr: 0.000300\n",
            "2021-07-14 15:16:44,180 - INFO - joeynmt.training - Epoch   4, Step:   174200, Batch Loss:     1.747867, Tokens per Sec:     7169, Lr: 0.000300\n",
            "2021-07-14 15:17:44,923 - INFO - joeynmt.training - Epoch   4, Step:   174400, Batch Loss:     1.874531, Tokens per Sec:     7140, Lr: 0.000300\n",
            "2021-07-14 15:18:46,201 - INFO - joeynmt.training - Epoch   4, Step:   174600, Batch Loss:     1.727698, Tokens per Sec:     7032, Lr: 0.000300\n",
            "2021-07-14 15:19:47,486 - INFO - joeynmt.training - Epoch   4, Step:   174800, Batch Loss:     1.759589, Tokens per Sec:     7045, Lr: 0.000300\n",
            "2021-07-14 15:20:48,447 - INFO - joeynmt.training - Epoch   4, Step:   175000, Batch Loss:     1.819186, Tokens per Sec:     7106, Lr: 0.000300\n",
            "2021-07-14 15:23:40,003 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 15:23:40,003 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 15:23:40,003 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 15:23:42,092 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 15:23:42,093 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 15:23:42,093 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 15:23:42,094 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-14 15:23:42,094 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 15:23:42,095 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 15:23:42,095 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 15:23:42,095 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven , saying : “ This is my beloved Son . ”\n",
            "2021-07-14 15:23:42,095 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 15:23:42,096 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 15:23:42,096 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 15:23:42,096 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-14 15:23:42,097 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 15:23:42,097 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 15:23:42,098 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 15:23:42,098 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-14 15:23:42,098 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   175000: bleu:  18.44, loss: 181152.5781, ppl:   7.4967, duration: 173.6504s\n",
            "2021-07-14 15:24:43,460 - INFO - joeynmt.training - Epoch   4, Step:   175200, Batch Loss:     1.935634, Tokens per Sec:     7086, Lr: 0.000300\n",
            "2021-07-14 15:25:44,901 - INFO - joeynmt.training - Epoch   4, Step:   175400, Batch Loss:     1.853116, Tokens per Sec:     7142, Lr: 0.000300\n",
            "2021-07-14 15:26:45,470 - INFO - joeynmt.training - Epoch   4, Step:   175600, Batch Loss:     1.768008, Tokens per Sec:     7027, Lr: 0.000300\n",
            "2021-07-14 15:26:58,605 - INFO - joeynmt.training - Epoch   4: total training loss 15486.81\n",
            "2021-07-14 15:26:58,605 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-14 15:27:47,705 - INFO - joeynmt.training - Epoch   5, Step:   175800, Batch Loss:     1.782302, Tokens per Sec:     6924, Lr: 0.000300\n",
            "2021-07-14 15:28:49,266 - INFO - joeynmt.training - Epoch   5, Step:   176000, Batch Loss:     2.043350, Tokens per Sec:     7179, Lr: 0.000300\n",
            "2021-07-14 15:29:50,439 - INFO - joeynmt.training - Epoch   5, Step:   176200, Batch Loss:     1.844251, Tokens per Sec:     7087, Lr: 0.000300\n",
            "2021-07-14 15:30:51,097 - INFO - joeynmt.training - Epoch   5, Step:   176400, Batch Loss:     1.441276, Tokens per Sec:     7112, Lr: 0.000300\n",
            "2021-07-14 15:31:52,251 - INFO - joeynmt.training - Epoch   5, Step:   176600, Batch Loss:     1.942826, Tokens per Sec:     7075, Lr: 0.000300\n",
            "2021-07-14 15:32:53,495 - INFO - joeynmt.training - Epoch   5, Step:   176800, Batch Loss:     1.571607, Tokens per Sec:     7121, Lr: 0.000300\n",
            "2021-07-14 15:33:54,885 - INFO - joeynmt.training - Epoch   5, Step:   177000, Batch Loss:     2.168520, Tokens per Sec:     7110, Lr: 0.000300\n",
            "2021-07-14 15:34:55,366 - INFO - joeynmt.training - Epoch   5, Step:   177200, Batch Loss:     1.909754, Tokens per Sec:     7003, Lr: 0.000300\n",
            "2021-07-14 15:35:56,055 - INFO - joeynmt.training - Epoch   5, Step:   177400, Batch Loss:     1.641454, Tokens per Sec:     7071, Lr: 0.000300\n",
            "2021-07-14 15:36:57,048 - INFO - joeynmt.training - Epoch   5, Step:   177600, Batch Loss:     1.796056, Tokens per Sec:     7103, Lr: 0.000300\n",
            "2021-07-14 15:37:58,771 - INFO - joeynmt.training - Epoch   5, Step:   177800, Batch Loss:     1.631104, Tokens per Sec:     7183, Lr: 0.000300\n",
            "2021-07-14 15:39:00,229 - INFO - joeynmt.training - Epoch   5, Step:   178000, Batch Loss:     1.760344, Tokens per Sec:     7088, Lr: 0.000300\n",
            "2021-07-14 15:40:00,836 - INFO - joeynmt.training - Epoch   5, Step:   178200, Batch Loss:     1.785866, Tokens per Sec:     7033, Lr: 0.000300\n",
            "2021-07-14 15:41:02,226 - INFO - joeynmt.training - Epoch   5, Step:   178400, Batch Loss:     2.020540, Tokens per Sec:     7128, Lr: 0.000300\n",
            "2021-07-14 15:42:03,533 - INFO - joeynmt.training - Epoch   5, Step:   178600, Batch Loss:     1.805983, Tokens per Sec:     7233, Lr: 0.000300\n",
            "2021-07-14 15:43:04,626 - INFO - joeynmt.training - Epoch   5, Step:   178800, Batch Loss:     2.043449, Tokens per Sec:     7111, Lr: 0.000300\n",
            "2021-07-14 15:44:05,690 - INFO - joeynmt.training - Epoch   5, Step:   179000, Batch Loss:     1.469722, Tokens per Sec:     7091, Lr: 0.000300\n",
            "2021-07-14 15:45:06,740 - INFO - joeynmt.training - Epoch   5, Step:   179200, Batch Loss:     1.892088, Tokens per Sec:     7125, Lr: 0.000300\n",
            "2021-07-14 15:46:08,069 - INFO - joeynmt.training - Epoch   5, Step:   179400, Batch Loss:     1.726150, Tokens per Sec:     7185, Lr: 0.000300\n",
            "2021-07-14 15:47:08,467 - INFO - joeynmt.training - Epoch   5, Step:   179600, Batch Loss:     1.855605, Tokens per Sec:     7074, Lr: 0.000300\n",
            "2021-07-14 15:48:09,936 - INFO - joeynmt.training - Epoch   5, Step:   179800, Batch Loss:     1.805066, Tokens per Sec:     7143, Lr: 0.000300\n",
            "2021-07-14 15:49:10,880 - INFO - joeynmt.training - Epoch   5, Step:   180000, Batch Loss:     1.823140, Tokens per Sec:     7116, Lr: 0.000300\n",
            "2021-07-14 15:51:57,529 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 15:51:57,529 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 15:51:57,529 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 15:51:58,631 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 15:51:58,632 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 15:51:59,561 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 15:51:59,563 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 15:51:59,563 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 15:51:59,563 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , that is acceptable to God . ”\n",
            "2021-07-14 15:51:59,564 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 15:51:59,564 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 15:51:59,565 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 15:51:59,565 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-14 15:51:59,565 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 15:51:59,566 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 15:51:59,566 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 15:51:59,566 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-14 15:51:59,566 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 15:51:59,567 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 15:51:59,567 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 15:51:59,568 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-14 15:51:59,568 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   180000: bleu:  18.47, loss: 179871.6875, ppl:   7.3907, duration: 168.6877s\n",
            "2021-07-14 15:53:01,080 - INFO - joeynmt.training - Epoch   5, Step:   180200, Batch Loss:     1.894934, Tokens per Sec:     7165, Lr: 0.000300\n",
            "2021-07-14 15:54:02,164 - INFO - joeynmt.training - Epoch   5, Step:   180400, Batch Loss:     1.795674, Tokens per Sec:     7117, Lr: 0.000300\n",
            "2021-07-14 15:55:02,620 - INFO - joeynmt.training - Epoch   5, Step:   180600, Batch Loss:     1.906192, Tokens per Sec:     7065, Lr: 0.000300\n",
            "2021-07-14 15:56:03,249 - INFO - joeynmt.training - Epoch   5, Step:   180800, Batch Loss:     1.899315, Tokens per Sec:     6989, Lr: 0.000300\n",
            "2021-07-14 15:57:04,324 - INFO - joeynmt.training - Epoch   5, Step:   181000, Batch Loss:     2.013634, Tokens per Sec:     7154, Lr: 0.000300\n",
            "2021-07-14 15:58:04,723 - INFO - joeynmt.training - Epoch   5, Step:   181200, Batch Loss:     2.477098, Tokens per Sec:     7056, Lr: 0.000300\n",
            "2021-07-14 15:59:05,831 - INFO - joeynmt.training - Epoch   5, Step:   181400, Batch Loss:     1.685785, Tokens per Sec:     7138, Lr: 0.000300\n",
            "2021-07-14 16:00:06,753 - INFO - joeynmt.training - Epoch   5, Step:   181600, Batch Loss:     1.808318, Tokens per Sec:     7062, Lr: 0.000300\n",
            "2021-07-14 16:01:07,780 - INFO - joeynmt.training - Epoch   5, Step:   181800, Batch Loss:     1.907410, Tokens per Sec:     7132, Lr: 0.000300\n",
            "2021-07-14 16:02:08,132 - INFO - joeynmt.training - Epoch   5, Step:   182000, Batch Loss:     1.694341, Tokens per Sec:     7129, Lr: 0.000300\n",
            "2021-07-14 16:03:08,693 - INFO - joeynmt.training - Epoch   5, Step:   182200, Batch Loss:     1.710121, Tokens per Sec:     7128, Lr: 0.000300\n",
            "2021-07-14 16:04:09,948 - INFO - joeynmt.training - Epoch   5, Step:   182400, Batch Loss:     1.956414, Tokens per Sec:     7192, Lr: 0.000300\n",
            "2021-07-14 16:05:10,410 - INFO - joeynmt.training - Epoch   5, Step:   182600, Batch Loss:     1.759477, Tokens per Sec:     7114, Lr: 0.000300\n",
            "2021-07-14 16:06:11,666 - INFO - joeynmt.training - Epoch   5, Step:   182800, Batch Loss:     1.854731, Tokens per Sec:     7167, Lr: 0.000300\n",
            "2021-07-14 16:07:13,348 - INFO - joeynmt.training - Epoch   5, Step:   183000, Batch Loss:     1.840920, Tokens per Sec:     7186, Lr: 0.000300\n",
            "2021-07-14 16:08:14,171 - INFO - joeynmt.training - Epoch   5, Step:   183200, Batch Loss:     1.580146, Tokens per Sec:     7165, Lr: 0.000300\n",
            "2021-07-14 16:09:15,359 - INFO - joeynmt.training - Epoch   5, Step:   183400, Batch Loss:     1.920498, Tokens per Sec:     7146, Lr: 0.000300\n",
            "2021-07-14 16:10:16,219 - INFO - joeynmt.training - Epoch   5, Step:   183600, Batch Loss:     2.001432, Tokens per Sec:     7062, Lr: 0.000300\n",
            "2021-07-14 16:11:17,583 - INFO - joeynmt.training - Epoch   5, Step:   183800, Batch Loss:     2.147940, Tokens per Sec:     7183, Lr: 0.000300\n",
            "2021-07-14 16:12:19,267 - INFO - joeynmt.training - Epoch   5, Step:   184000, Batch Loss:     1.866732, Tokens per Sec:     7135, Lr: 0.000300\n",
            "2021-07-14 16:12:21,745 - INFO - joeynmt.training - Epoch   5: total training loss 15413.03\n",
            "2021-07-14 16:12:21,745 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-14 16:13:21,423 - INFO - joeynmt.training - Epoch   6, Step:   184200, Batch Loss:     1.822504, Tokens per Sec:     7018, Lr: 0.000300\n",
            "2021-07-14 16:14:22,620 - INFO - joeynmt.training - Epoch   6, Step:   184400, Batch Loss:     1.762002, Tokens per Sec:     7170, Lr: 0.000300\n",
            "2021-07-14 16:15:23,929 - INFO - joeynmt.training - Epoch   6, Step:   184600, Batch Loss:     1.922860, Tokens per Sec:     7140, Lr: 0.000300\n",
            "2021-07-14 16:16:25,183 - INFO - joeynmt.training - Epoch   6, Step:   184800, Batch Loss:     1.865876, Tokens per Sec:     7181, Lr: 0.000300\n",
            "2021-07-14 16:17:26,263 - INFO - joeynmt.training - Epoch   6, Step:   185000, Batch Loss:     2.000930, Tokens per Sec:     7145, Lr: 0.000300\n",
            "2021-07-14 16:20:15,413 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 16:20:15,414 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 16:20:15,414 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 16:20:16,457 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 16:20:16,458 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 16:20:17,305 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 16:20:17,306 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 16:20:17,306 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 16:20:17,307 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer , that is acceptable to God . ”\n",
            "2021-07-14 16:20:17,307 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 16:20:17,308 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 16:20:17,308 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 16:20:17,308 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-14 16:20:17,308 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 16:20:17,309 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 16:20:17,309 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 16:20:17,309 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-14 16:20:17,310 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 16:20:17,310 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 16:20:17,310 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 16:20:17,311 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-14 16:20:17,311 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   185000: bleu:  18.45, loss: 179396.9531, ppl:   7.3518, duration: 171.0475s\n",
            "2021-07-14 16:21:18,509 - INFO - joeynmt.training - Epoch   6, Step:   185200, Batch Loss:     1.923651, Tokens per Sec:     7055, Lr: 0.000300\n",
            "2021-07-14 16:22:19,598 - INFO - joeynmt.training - Epoch   6, Step:   185400, Batch Loss:     1.830978, Tokens per Sec:     7177, Lr: 0.000300\n",
            "2021-07-14 16:23:20,201 - INFO - joeynmt.training - Epoch   6, Step:   185600, Batch Loss:     1.841817, Tokens per Sec:     7064, Lr: 0.000300\n",
            "2021-07-14 16:24:21,289 - INFO - joeynmt.training - Epoch   6, Step:   185800, Batch Loss:     1.967232, Tokens per Sec:     7080, Lr: 0.000300\n",
            "2021-07-14 16:25:22,439 - INFO - joeynmt.training - Epoch   6, Step:   186000, Batch Loss:     1.614382, Tokens per Sec:     7067, Lr: 0.000300\n",
            "2021-07-14 16:26:23,915 - INFO - joeynmt.training - Epoch   6, Step:   186200, Batch Loss:     1.901017, Tokens per Sec:     7123, Lr: 0.000300\n",
            "2021-07-14 16:27:25,311 - INFO - joeynmt.training - Epoch   6, Step:   186400, Batch Loss:     1.856735, Tokens per Sec:     7078, Lr: 0.000300\n",
            "2021-07-14 16:28:26,460 - INFO - joeynmt.training - Epoch   6, Step:   186600, Batch Loss:     1.801426, Tokens per Sec:     7111, Lr: 0.000300\n",
            "2021-07-14 16:29:27,646 - INFO - joeynmt.training - Epoch   6, Step:   186800, Batch Loss:     1.880386, Tokens per Sec:     7136, Lr: 0.000300\n",
            "2021-07-14 16:30:28,701 - INFO - joeynmt.training - Epoch   6, Step:   187000, Batch Loss:     1.650575, Tokens per Sec:     7031, Lr: 0.000300\n",
            "2021-07-14 16:31:29,775 - INFO - joeynmt.training - Epoch   6, Step:   187200, Batch Loss:     2.064904, Tokens per Sec:     7064, Lr: 0.000300\n",
            "2021-07-14 16:32:31,359 - INFO - joeynmt.training - Epoch   6, Step:   187400, Batch Loss:     1.508174, Tokens per Sec:     7210, Lr: 0.000300\n",
            "2021-07-14 16:33:32,113 - INFO - joeynmt.training - Epoch   6, Step:   187600, Batch Loss:     1.839873, Tokens per Sec:     7079, Lr: 0.000300\n",
            "2021-07-14 16:34:33,150 - INFO - joeynmt.training - Epoch   6, Step:   187800, Batch Loss:     1.887991, Tokens per Sec:     7090, Lr: 0.000300\n",
            "2021-07-14 16:35:33,938 - INFO - joeynmt.training - Epoch   6, Step:   188000, Batch Loss:     1.748328, Tokens per Sec:     7043, Lr: 0.000300\n",
            "2021-07-14 16:36:35,413 - INFO - joeynmt.training - Epoch   6, Step:   188200, Batch Loss:     1.754286, Tokens per Sec:     7164, Lr: 0.000300\n",
            "2021-07-14 16:37:36,736 - INFO - joeynmt.training - Epoch   6, Step:   188400, Batch Loss:     1.864239, Tokens per Sec:     7115, Lr: 0.000300\n",
            "2021-07-14 16:38:38,044 - INFO - joeynmt.training - Epoch   6, Step:   188600, Batch Loss:     1.741823, Tokens per Sec:     7126, Lr: 0.000300\n",
            "2021-07-14 16:39:39,043 - INFO - joeynmt.training - Epoch   6, Step:   188800, Batch Loss:     1.717481, Tokens per Sec:     7096, Lr: 0.000300\n",
            "2021-07-14 16:40:40,716 - INFO - joeynmt.training - Epoch   6, Step:   189000, Batch Loss:     1.810241, Tokens per Sec:     7132, Lr: 0.000300\n",
            "2021-07-14 16:41:42,379 - INFO - joeynmt.training - Epoch   6, Step:   189200, Batch Loss:     1.915337, Tokens per Sec:     7081, Lr: 0.000300\n",
            "2021-07-14 16:42:43,491 - INFO - joeynmt.training - Epoch   6, Step:   189400, Batch Loss:     2.025949, Tokens per Sec:     7100, Lr: 0.000300\n",
            "2021-07-14 16:43:44,389 - INFO - joeynmt.training - Epoch   6, Step:   189600, Batch Loss:     1.954826, Tokens per Sec:     7114, Lr: 0.000300\n",
            "2021-07-14 16:44:45,388 - INFO - joeynmt.training - Epoch   6, Step:   189800, Batch Loss:     1.875023, Tokens per Sec:     7084, Lr: 0.000300\n",
            "2021-07-14 16:45:46,393 - INFO - joeynmt.training - Epoch   6, Step:   190000, Batch Loss:     1.926401, Tokens per Sec:     7105, Lr: 0.000300\n",
            "2021-07-14 16:48:35,179 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 16:48:35,179 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 16:48:35,179 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 16:48:36,284 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 16:48:36,285 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 16:48:37,362 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 16:48:37,363 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 16:48:37,364 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 16:48:37,364 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-14 16:48:37,364 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 16:48:37,366 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 16:48:37,366 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 16:48:37,366 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-14 16:48:37,367 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 16:48:37,367 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 16:48:37,368 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 16:48:37,368 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-14 16:48:37,368 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 16:48:37,369 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 16:48:37,369 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 16:48:37,369 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-14 16:48:37,370 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   190000: bleu:  18.66, loss: 178744.2344, ppl:   7.2986, duration: 170.9762s\n",
            "2021-07-14 16:49:38,439 - INFO - joeynmt.training - Epoch   6, Step:   190200, Batch Loss:     1.743895, Tokens per Sec:     7033, Lr: 0.000300\n",
            "2021-07-14 16:50:39,481 - INFO - joeynmt.training - Epoch   6, Step:   190400, Batch Loss:     1.862327, Tokens per Sec:     7057, Lr: 0.000300\n",
            "2021-07-14 16:51:40,631 - INFO - joeynmt.training - Epoch   6, Step:   190600, Batch Loss:     1.882970, Tokens per Sec:     7066, Lr: 0.000300\n",
            "2021-07-14 16:52:41,465 - INFO - joeynmt.training - Epoch   6, Step:   190800, Batch Loss:     1.852102, Tokens per Sec:     7100, Lr: 0.000300\n",
            "2021-07-14 16:53:42,720 - INFO - joeynmt.training - Epoch   6, Step:   191000, Batch Loss:     1.729337, Tokens per Sec:     7119, Lr: 0.000300\n",
            "2021-07-14 16:54:43,900 - INFO - joeynmt.training - Epoch   6, Step:   191200, Batch Loss:     1.767326, Tokens per Sec:     7097, Lr: 0.000300\n",
            "2021-07-14 16:55:45,141 - INFO - joeynmt.training - Epoch   6, Step:   191400, Batch Loss:     2.002395, Tokens per Sec:     7088, Lr: 0.000300\n",
            "2021-07-14 16:56:46,847 - INFO - joeynmt.training - Epoch   6, Step:   191600, Batch Loss:     1.962620, Tokens per Sec:     7152, Lr: 0.000300\n",
            "2021-07-14 16:57:47,469 - INFO - joeynmt.training - Epoch   6, Step:   191800, Batch Loss:     1.904222, Tokens per Sec:     7028, Lr: 0.000300\n",
            "2021-07-14 16:58:48,270 - INFO - joeynmt.training - Epoch   6, Step:   192000, Batch Loss:     1.901467, Tokens per Sec:     7038, Lr: 0.000300\n",
            "2021-07-14 16:59:50,036 - INFO - joeynmt.training - Epoch   6, Step:   192200, Batch Loss:     1.718063, Tokens per Sec:     7198, Lr: 0.000300\n",
            "2021-07-14 17:00:42,909 - INFO - joeynmt.training - Epoch   6: total training loss 15359.60\n",
            "2021-07-14 17:00:42,909 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-14 17:00:52,434 - INFO - joeynmt.training - Epoch   7, Step:   192400, Batch Loss:     1.955656, Tokens per Sec:     6163, Lr: 0.000300\n",
            "2021-07-14 17:01:53,227 - INFO - joeynmt.training - Epoch   7, Step:   192600, Batch Loss:     1.963331, Tokens per Sec:     7080, Lr: 0.000300\n",
            "2021-07-14 17:02:54,579 - INFO - joeynmt.training - Epoch   7, Step:   192800, Batch Loss:     1.785544, Tokens per Sec:     7188, Lr: 0.000300\n",
            "2021-07-14 17:03:55,002 - INFO - joeynmt.training - Epoch   7, Step:   193000, Batch Loss:     1.865232, Tokens per Sec:     6961, Lr: 0.000300\n",
            "2021-07-14 17:04:56,151 - INFO - joeynmt.training - Epoch   7, Step:   193200, Batch Loss:     1.836006, Tokens per Sec:     7114, Lr: 0.000300\n",
            "2021-07-14 17:05:57,557 - INFO - joeynmt.training - Epoch   7, Step:   193400, Batch Loss:     1.827652, Tokens per Sec:     7086, Lr: 0.000300\n",
            "2021-07-14 17:06:58,715 - INFO - joeynmt.training - Epoch   7, Step:   193600, Batch Loss:     1.978987, Tokens per Sec:     7066, Lr: 0.000300\n",
            "2021-07-14 17:07:59,724 - INFO - joeynmt.training - Epoch   7, Step:   193800, Batch Loss:     2.090256, Tokens per Sec:     7073, Lr: 0.000300\n",
            "2021-07-14 17:09:00,827 - INFO - joeynmt.training - Epoch   7, Step:   194000, Batch Loss:     1.906271, Tokens per Sec:     7073, Lr: 0.000300\n",
            "2021-07-14 17:10:02,031 - INFO - joeynmt.training - Epoch   7, Step:   194200, Batch Loss:     1.855570, Tokens per Sec:     7155, Lr: 0.000300\n",
            "2021-07-14 17:11:03,330 - INFO - joeynmt.training - Epoch   7, Step:   194400, Batch Loss:     1.773833, Tokens per Sec:     7111, Lr: 0.000300\n",
            "2021-07-14 17:12:04,280 - INFO - joeynmt.training - Epoch   7, Step:   194600, Batch Loss:     2.132886, Tokens per Sec:     7089, Lr: 0.000300\n",
            "2021-07-14 17:13:05,424 - INFO - joeynmt.training - Epoch   7, Step:   194800, Batch Loss:     1.859936, Tokens per Sec:     7175, Lr: 0.000300\n",
            "2021-07-14 17:14:06,203 - INFO - joeynmt.training - Epoch   7, Step:   195000, Batch Loss:     1.931062, Tokens per Sec:     6995, Lr: 0.000300\n",
            "2021-07-14 17:16:59,098 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 17:16:59,099 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 17:16:59,099 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 17:17:00,199 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 17:17:00,199 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 17:17:01,335 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 17:17:01,337 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 17:17:01,338 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 17:17:01,338 - INFO - joeynmt.training - \tHypothesis: And when you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-14 17:17:01,338 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 17:17:01,339 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 17:17:01,339 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 17:17:01,339 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-14 17:17:01,339 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 17:17:01,340 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 17:17:01,340 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 17:17:01,341 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-14 17:17:01,341 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 17:17:01,342 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 17:17:01,342 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 17:17:01,342 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-14 17:17:01,342 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   195000: bleu:  18.89, loss: 178519.0156, ppl:   7.2803, duration: 175.1386s\n",
            "2021-07-14 17:18:01,905 - INFO - joeynmt.training - Epoch   7, Step:   195200, Batch Loss:     1.382552, Tokens per Sec:     7027, Lr: 0.000300\n",
            "2021-07-14 17:19:02,967 - INFO - joeynmt.training - Epoch   7, Step:   195400, Batch Loss:     1.891476, Tokens per Sec:     7112, Lr: 0.000300\n",
            "2021-07-14 17:20:03,330 - INFO - joeynmt.training - Epoch   7, Step:   195600, Batch Loss:     1.715699, Tokens per Sec:     7109, Lr: 0.000300\n",
            "2021-07-14 17:21:04,653 - INFO - joeynmt.training - Epoch   7, Step:   195800, Batch Loss:     1.789329, Tokens per Sec:     7039, Lr: 0.000300\n",
            "2021-07-14 17:22:05,774 - INFO - joeynmt.training - Epoch   7, Step:   196000, Batch Loss:     1.776345, Tokens per Sec:     7074, Lr: 0.000300\n",
            "2021-07-14 17:23:06,083 - INFO - joeynmt.training - Epoch   7, Step:   196200, Batch Loss:     1.790694, Tokens per Sec:     7094, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SRiNVDXW6wf"
      },
      "source": [
        "7 epochs completed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlJ8i_1H3afn",
        "collapsed": true,
        "outputId": "77e2aabd-e267-4d15-d206-322bc9173307"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"joeynmt/models/lg_rw_lhen_reverse_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 5000\tLoss: 294844.96875\tPPL: 26.54260\tbleu: 3.53660\tLR: 0.00030000\t*\n",
            "Steps: 10000\tLoss: 262393.87500\tPPL: 18.50215\tbleu: 5.77667\tLR: 0.00030000\t*\n",
            "Steps: 15000\tLoss: 244500.31250\tPPL: 15.16372\tbleu: 8.66136\tLR: 0.00030000\t*\n",
            "Steps: 20000\tLoss: 232730.06250\tPPL: 13.30337\tbleu: 10.26265\tLR: 0.00030000\t*\n",
            "Steps: 25000\tLoss: 224283.95312\tPPL: 12.11076\tbleu: 11.83755\tLR: 0.00030000\t*\n",
            "Steps: 30000\tLoss: 218069.34375\tPPL: 11.30208\tbleu: 12.22675\tLR: 0.00030000\t*\n",
            "Steps: 35000\tLoss: 212952.45312\tPPL: 10.67693\tbleu: 13.58401\tLR: 0.00030000\t*\n",
            "Steps: 40000\tLoss: 211624.50000\tPPL: 10.52042\tbleu: 13.86809\tLR: 0.00030000\t*\n",
            "Steps: 45000\tLoss: 206207.64062\tPPL: 9.90541\tbleu: 13.85557\tLR: 0.00030000\t*\n",
            "Steps: 50000\tLoss: 203779.25000\tPPL: 9.64150\tbleu: 14.47956\tLR: 0.00030000\t*\n",
            "Steps: 55000\tLoss: 200319.60938\tPPL: 9.27762\tbleu: 15.54748\tLR: 0.00030000\t*\n",
            "Steps: 60000\tLoss: 199524.12500\tPPL: 9.19591\tbleu: 15.40850\tLR: 0.00030000\t*\n",
            "Steps: 65000\tLoss: 198351.85938\tPPL: 9.07681\tbleu: 14.63837\tLR: 0.00030000\t*\n",
            "Steps: 70000\tLoss: 195850.34375\tPPL: 8.82780\tbleu: 16.10626\tLR: 0.00030000\t*\n",
            "Steps: 75000\tLoss: 195205.65625\tPPL: 8.76474\tbleu: 16.18069\tLR: 0.00030000\t*\n",
            "Steps: 80000\tLoss: 192584.17188\tPPL: 8.51292\tbleu: 16.62285\tLR: 0.00030000\t*\n",
            "Steps: 85000\tLoss: 191828.98438\tPPL: 8.44173\tbleu: 16.75802\tLR: 0.00030000\t*\n",
            "Steps: 90000\tLoss: 190977.90625\tPPL: 8.36221\tbleu: 16.79192\tLR: 0.00030000\t*\n",
            "Steps: 95000\tLoss: 190402.67188\tPPL: 8.30889\tbleu: 17.11631\tLR: 0.00030000\t*\n",
            "Steps: 100000\tLoss: 188623.93750\tPPL: 8.14616\tbleu: 16.91725\tLR: 0.00030000\t*\n",
            "Steps: 105000\tLoss: 188108.00000\tPPL: 8.09955\tbleu: 17.43375\tLR: 0.00030000\t*\n",
            "Steps: 110000\tLoss: 188434.84375\tPPL: 8.12904\tbleu: 16.45012\tLR: 0.00030000\t\n",
            "Steps: 115000\tLoss: 186669.40625\tPPL: 7.97101\tbleu: 17.63216\tLR: 0.00030000\t*\n",
            "Steps: 120000\tLoss: 186289.96875\tPPL: 7.93745\tbleu: 17.61446\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWOIl5BILmt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2d5f6340-d754-4841-f074-ca61e9e6e23f"
      },
      "source": [
        "! cat \"joeynmt/models/lg_rw_lhen_reverse_transformer_continued/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 125000\tLoss: 185210.06250\tPPL: 7.84270\tbleu: 17.83400\tLR: 0.00030000\t*\n",
            "Steps: 130000\tLoss: 184858.45312\tPPL: 7.81209\tbleu: 17.96464\tLR: 0.00030000\t*\n",
            "Steps: 135000\tLoss: 184420.81250\tPPL: 7.77417\tbleu: 17.77071\tLR: 0.00030000\t*\n",
            "Steps: 140000\tLoss: 183049.64062\tPPL: 7.65653\tbleu: 18.16814\tLR: 0.00030000\t*\n",
            "Steps: 145000\tLoss: 182512.57812\tPPL: 7.61094\tbleu: 18.16392\tLR: 0.00030000\t*\n",
            "Steps: 150000\tLoss: 182100.42188\tPPL: 7.57613\tbleu: 18.47894\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3KhsVbk4HJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c372e2b-cef8-4604-8349-15e1b82687dd"
      },
      "source": [
        "! cat \"joeynmt/models/lg_rw_lhen_reverse_transformer_continued2/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 155000\tLoss: 181684.31250\tPPL: 7.54116\tbleu: 18.17956\tLR: 0.00030000\t*\n",
            "Steps: 160000\tLoss: 181753.37500\tPPL: 7.54695\tbleu: 18.35824\tLR: 0.00030000\t\n",
            "Steps: 165000\tLoss: 180658.29688\tPPL: 7.45561\tbleu: 18.38394\tLR: 0.00030000\t*\n",
            "Steps: 170000\tLoss: 180725.34375\tPPL: 7.46117\tbleu: 18.63909\tLR: 0.00030000\t\n",
            "Steps: 175000\tLoss: 181152.57812\tPPL: 7.49670\tbleu: 18.44216\tLR: 0.00030000\t\n",
            "Steps: 180000\tLoss: 179871.68750\tPPL: 7.39067\tbleu: 18.46554\tLR: 0.00030000\t*\n",
            "Steps: 185000\tLoss: 179396.95312\tPPL: 7.35176\tbleu: 18.45346\tLR: 0.00030000\t*\n",
            "Steps: 190000\tLoss: 178744.23438\tPPL: 7.29859\tbleu: 18.66191\tLR: 0.00030000\t*\n",
            "Steps: 195000\tLoss: 178519.01562\tPPL: 7.28034\tbleu: 18.88726\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z5r5_ZceaEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a6e84f-58a9-46d2-d7b3-2b68c4178ac6"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt test 'models/lg_rw_lhen_reverse_transformer_continued2/config.yaml'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 21:30:33,594 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 21:30:33,598 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-14 21:30:34,314 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-14 21:30:34,978 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-14 21:30:35,641 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-14 21:30:35,711 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-14 21:30:46,753 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 21:30:47,121 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-14 21:30:47,193 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe.en)...\n",
            "2021-07-14 21:32:29,555 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 21:32:29,555 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 21:32:29,555 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 21:32:30,406 - INFO - joeynmt.prediction -  dev bleu[13a]:  19.21 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-14 21:32:30,406 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.en)...\n",
            "2021-07-14 21:33:14,858 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 21:33:14,859 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 21:33:14,859 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 21:33:15,185 - INFO - joeynmt.prediction - test bleu[13a]:   8.84 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-5brNpqypwA",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def empty_counter(x):\n",
        "  # Opening a file\n",
        "  infile = open(x,\"r\")\n",
        "  empty = []\n",
        "  \n",
        "  for i,line in enumerate(infile):\n",
        "    if not line.strip(): \n",
        "      empty.append(i)\n",
        "\n",
        "  return empty\n",
        "\n",
        "#@title\n",
        "# Reference: https://thispointer.com/python-how-to-delete-specific-lines-in-a-file-in-a-memory-efficient-way/\n",
        "def delete_multiple_lines(original_file, line_numbers):\n",
        "    \"\"\"In a file, delete the lines at line number in given list\"\"\"\n",
        "    is_skipped = False\n",
        "    counter = 0\n",
        "    # Create name of dummy / temporary file\n",
        "    dummy_file = original_file + '.bak'\n",
        "    # Open original file in read only mode and dummy file in write mode\n",
        "    with open(original_file, 'r') as read_obj, open(dummy_file, 'w') as write_obj:\n",
        "        # Line by line copy data from original file to dummy file\n",
        "        for line in read_obj:\n",
        "            # If current line number exist in list then skip copying that line\n",
        "            if counter not in line_numbers:\n",
        "                write_obj.write(line)\n",
        "            else:\n",
        "                is_skipped = True\n",
        "            counter += 1\n",
        "    # If any line is skipped then rename dummy file as original file\n",
        "    if is_skipped:\n",
        "        os.remove(original_file)\n",
        "        os.rename(dummy_file, original_file)\n",
        "    else:\n",
        "        os.remove(dummy_file)\n",
        "\n",
        "x = empty_counter(\"train.lg_rw_lh\")\n",
        "x\n",
        "\n",
        "delete_multiple_lines(\"train.lg_rw_lh\",x)\n",
        "delete_multiple_lines(\"train.en\",x)\n",
        "\n",
        "delete_multiple_lines(\"test3.en\",empty_counter(\"test3.rw\"))\n",
        "\n",
        "delete_multiple_lines(\"test2.en\",empty_counter(\"test2.lg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwojKMFVK6SC"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 195000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/models/lg_rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued2/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_rw_lhen_reverse_transformer\"', f'model_dir: \"models/lg_rw_lhen_reverse_transformer_continued3\"').replace(\n",
        "        f'epochs: 30', f'epochs: 4')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fM-cusyALemG",
        "outputId": "0003cf28-6644-4b8e-ca4e-e73e8f49add3"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_lg_rw_lhen_reload3.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"lg_rw_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"lg_rw_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued2/195000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 4                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/lg_rw_lhen_reverse_transformer_continued3\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcMHW2A-LlcB",
        "collapsed": true,
        "outputId": "8f52bdb0-000e-4e75-9fea-8b867cc9996b"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_rw_lhen_reload3.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-22 09:03:19,993 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-22 09:03:20,030 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-22 09:03:34,833 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-22 09:03:35,151 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-22 09:03:35,275 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-22 09:03:35,290 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-22 09:03:35,291 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-22 09:03:35,529 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-22 09:03:35.788189: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-22 09:03:39,648 - INFO - joeynmt.training - Total params: 12179456\n",
            "2021-07-22 09:03:42,522 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued2/195000.ckpt\n",
            "2021-07-22 09:03:42,991 - INFO - joeynmt.helpers - cfg.name                           : lg_rw_lhen_reverse_transformer\n",
            "2021-07-22 09:03:42,991 - INFO - joeynmt.helpers - cfg.data.src                       : lg_rw_lh\n",
            "2021-07-22 09:03:42,991 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-22 09:03:42,991 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\n",
            "2021-07-22 09:03:42,991 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\n",
            "2021-07-22 09:03:42,992 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\n",
            "2021-07-22 09:03:42,992 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-22 09:03:42,992 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-22 09:03:42,992 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-22 09:03:42,992 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-22 09:03:42,992 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-22 09:03:42,992 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-22 09:03:42,993 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-22 09:03:42,993 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued2/195000.ckpt\n",
            "2021-07-22 09:03:42,993 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-22 09:03:42,993 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-22 09:03:42,993 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-22 09:03:42,993 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-22 09:03:42,993 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-22 09:03:42,993 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-22 09:03:42,994 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-22 09:03:42,994 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-22 09:03:42,994 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-22 09:03:42,994 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-22 09:03:42,994 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-22 09:03:42,994 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-22 09:03:42,994 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-22 09:03:42,995 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-22 09:03:42,995 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-22 09:03:42,995 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-22 09:03:42,995 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-22 09:03:42,995 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-22 09:03:42,995 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-22 09:03:42,995 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-22 09:03:42,996 - INFO - joeynmt.helpers - cfg.training.epochs                : 4\n",
            "2021-07-22 09:03:42,996 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-22 09:03:42,996 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-22 09:03:42,996 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-22 09:03:42,996 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_rw_lhen_reverse_transformer_continued3\n",
            "2021-07-22 09:03:42,996 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-22 09:03:42,997 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-22 09:03:42,997 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-22 09:03:42,997 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-22 09:03:42,997 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-22 09:03:42,997 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-22 09:03:42,997 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-22 09:03:42,997 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-22 09:03:42,998 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-22 09:03:42,998 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-22 09:03:42,998 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-22 09:03:42,998 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-22 09:03:42,998 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-22 09:03:42,998 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-22 09:03:42,998 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-22 09:03:42,999 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-22 09:03:42,999 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-22 09:03:42,999 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-22 09:03:42,999 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-22 09:03:42,999 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-22 09:03:42,999 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-22 09:03:42,999 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-22 09:03:43,000 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-22 09:03:43,000 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-22 09:03:43,000 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-22 09:03:43,000 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-22 09:03:43,000 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-22 09:03:43,000 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-22 09:03:43,000 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-22 09:03:43,001 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-22 09:03:43,001 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-22 09:03:43,002 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 665203,\n",
            "\tvalid 3000,\n",
            "\ttest 1000\n",
            "2021-07-22 09:03:43,003 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ at@@ andika okuk@@ olera ku m@@ azima ge nn@@ ali nj@@ iga , era nn@@ ak@@ ir@@ aba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obuf@@ uzi n’@@ okul@@ eka em@@ ikw@@ ano em@@ ibi gye nn@@ alina .\n",
            "\t[TRG] Ev@@ ent@@ ually , however , the tr@@ uth@@ s I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my polit@@ ical view@@ po@@ in@@ ts and associ@@ ations .\n",
            "2021-07-22 09:03:43,003 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-22 09:03:43,003 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-22 09:03:43,003 - INFO - joeynmt.helpers - Number of Src words (types): 4372\n",
            "2021-07-22 09:03:43,004 - INFO - joeynmt.helpers - Number of Trg words (types): 4372\n",
            "2021-07-22 09:03:43,004 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4372),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4372))\n",
            "2021-07-22 09:03:43,014 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-22 09:03:43,015 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-22 09:04:22,409 - INFO - joeynmt.training - Epoch   1, Step:   195200, Batch Loss:     1.378956, Tokens per Sec:    10803, Lr: 0.000300\n",
            "2021-07-22 09:05:00,334 - INFO - joeynmt.training - Epoch   1, Step:   195400, Batch Loss:     1.881813, Tokens per Sec:    11451, Lr: 0.000300\n",
            "2021-07-22 09:05:37,952 - INFO - joeynmt.training - Epoch   1, Step:   195600, Batch Loss:     1.719947, Tokens per Sec:    11407, Lr: 0.000300\n",
            "2021-07-22 09:06:15,961 - INFO - joeynmt.training - Epoch   1, Step:   195800, Batch Loss:     1.791753, Tokens per Sec:    11357, Lr: 0.000300\n",
            "2021-07-22 09:06:53,785 - INFO - joeynmt.training - Epoch   1, Step:   196000, Batch Loss:     1.781737, Tokens per Sec:    11431, Lr: 0.000300\n",
            "2021-07-22 09:07:31,277 - INFO - joeynmt.training - Epoch   1, Step:   196200, Batch Loss:     1.782590, Tokens per Sec:    11411, Lr: 0.000300\n",
            "2021-07-22 09:08:09,370 - INFO - joeynmt.training - Epoch   1, Step:   196400, Batch Loss:     2.056179, Tokens per Sec:    11508, Lr: 0.000300\n",
            "2021-07-22 09:08:47,458 - INFO - joeynmt.training - Epoch   1, Step:   196600, Batch Loss:     1.805223, Tokens per Sec:    11592, Lr: 0.000300\n",
            "2021-07-22 09:09:25,479 - INFO - joeynmt.training - Epoch   1, Step:   196800, Batch Loss:     1.978162, Tokens per Sec:    11553, Lr: 0.000300\n",
            "2021-07-22 09:10:03,614 - INFO - joeynmt.training - Epoch   1, Step:   197000, Batch Loss:     1.902548, Tokens per Sec:    11319, Lr: 0.000300\n",
            "2021-07-22 09:10:41,563 - INFO - joeynmt.training - Epoch   1, Step:   197200, Batch Loss:     1.696332, Tokens per Sec:    11427, Lr: 0.000300\n",
            "2021-07-22 09:11:19,724 - INFO - joeynmt.training - Epoch   1, Step:   197400, Batch Loss:     1.794845, Tokens per Sec:    11373, Lr: 0.000300\n",
            "2021-07-22 09:11:57,532 - INFO - joeynmt.training - Epoch   1, Step:   197600, Batch Loss:     1.843899, Tokens per Sec:    11295, Lr: 0.000300\n",
            "2021-07-22 09:12:35,826 - INFO - joeynmt.training - Epoch   1, Step:   197800, Batch Loss:     1.813709, Tokens per Sec:    11400, Lr: 0.000300\n",
            "2021-07-22 09:13:14,392 - INFO - joeynmt.training - Epoch   1, Step:   198000, Batch Loss:     1.651821, Tokens per Sec:    11548, Lr: 0.000300\n",
            "2021-07-22 09:13:51,907 - INFO - joeynmt.training - Epoch   1, Step:   198200, Batch Loss:     1.887862, Tokens per Sec:    11272, Lr: 0.000300\n",
            "2021-07-22 09:14:30,436 - INFO - joeynmt.training - Epoch   1, Step:   198400, Batch Loss:     1.912270, Tokens per Sec:    11589, Lr: 0.000300\n",
            "2021-07-22 09:15:08,434 - INFO - joeynmt.training - Epoch   1, Step:   198600, Batch Loss:     1.765242, Tokens per Sec:    11434, Lr: 0.000300\n",
            "2021-07-22 09:15:46,395 - INFO - joeynmt.training - Epoch   1, Step:   198800, Batch Loss:     1.900399, Tokens per Sec:    11325, Lr: 0.000300\n",
            "2021-07-22 09:16:24,629 - INFO - joeynmt.training - Epoch   1, Step:   199000, Batch Loss:     1.759832, Tokens per Sec:    11475, Lr: 0.000300\n",
            "2021-07-22 09:17:02,621 - INFO - joeynmt.training - Epoch   1, Step:   199200, Batch Loss:     2.047400, Tokens per Sec:    11422, Lr: 0.000300\n",
            "2021-07-22 09:17:41,052 - INFO - joeynmt.training - Epoch   1, Step:   199400, Batch Loss:     1.721068, Tokens per Sec:    11517, Lr: 0.000300\n",
            "2021-07-22 09:18:19,380 - INFO - joeynmt.training - Epoch   1, Step:   199600, Batch Loss:     1.702521, Tokens per Sec:    11451, Lr: 0.000300\n",
            "2021-07-22 09:18:57,444 - INFO - joeynmt.training - Epoch   1, Step:   199800, Batch Loss:     1.978569, Tokens per Sec:    11434, Lr: 0.000300\n",
            "2021-07-22 09:19:35,923 - INFO - joeynmt.training - Epoch   1, Step:   200000, Batch Loss:     1.774840, Tokens per Sec:    11556, Lr: 0.000300\n",
            "2021-07-22 09:21:15,910 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 09:21:15,910 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 09:21:15,910 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 09:21:16,935 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 09:21:16,935 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 09:21:17,794 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 09:21:17,795 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 09:21:17,795 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 09:21:17,796 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , that is acceptable to God . ”\n",
            "2021-07-22 09:21:17,796 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 09:21:17,796 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 09:21:17,796 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 09:21:17,797 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 09:21:17,797 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 09:21:17,798 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 09:21:17,798 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 09:21:17,798 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ reaction ?\n",
            "2021-07-22 09:21:17,798 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 09:21:17,799 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 09:21:17,799 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 09:21:17,799 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 09:21:17,799 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step   200000: bleu:  18.96, loss: 178236.9219, ppl:   7.2575, duration: 101.8752s\n",
            "2021-07-22 09:21:56,119 - INFO - joeynmt.training - Epoch   1, Step:   200200, Batch Loss:     1.985119, Tokens per Sec:    11337, Lr: 0.000300\n",
            "2021-07-22 09:22:33,840 - INFO - joeynmt.training - Epoch   1, Step:   200400, Batch Loss:     1.852520, Tokens per Sec:    11376, Lr: 0.000300\n",
            "2021-07-22 09:23:11,958 - INFO - joeynmt.training - Epoch   1, Step:   200600, Batch Loss:     1.755937, Tokens per Sec:    11445, Lr: 0.000300\n",
            "2021-07-22 09:23:39,068 - INFO - joeynmt.training - Epoch   1: total training loss 10501.67\n",
            "2021-07-22 09:23:39,068 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-22 09:23:51,078 - INFO - joeynmt.training - Epoch   2, Step:   200800, Batch Loss:     1.661985, Tokens per Sec:    10376, Lr: 0.000300\n",
            "2021-07-22 09:24:29,185 - INFO - joeynmt.training - Epoch   2, Step:   201000, Batch Loss:     1.737425, Tokens per Sec:    11578, Lr: 0.000300\n",
            "2021-07-22 09:25:07,039 - INFO - joeynmt.training - Epoch   2, Step:   201200, Batch Loss:     1.814020, Tokens per Sec:    11499, Lr: 0.000300\n",
            "2021-07-22 09:25:44,752 - INFO - joeynmt.training - Epoch   2, Step:   201400, Batch Loss:     1.640808, Tokens per Sec:    11560, Lr: 0.000300\n",
            "2021-07-22 09:26:22,539 - INFO - joeynmt.training - Epoch   2, Step:   201600, Batch Loss:     1.543623, Tokens per Sec:    11497, Lr: 0.000300\n",
            "2021-07-22 09:27:00,439 - INFO - joeynmt.training - Epoch   2, Step:   201800, Batch Loss:     1.874627, Tokens per Sec:    11624, Lr: 0.000300\n",
            "2021-07-22 09:27:38,014 - INFO - joeynmt.training - Epoch   2, Step:   202000, Batch Loss:     1.867296, Tokens per Sec:    11523, Lr: 0.000300\n",
            "2021-07-22 09:28:15,712 - INFO - joeynmt.training - Epoch   2, Step:   202200, Batch Loss:     1.749772, Tokens per Sec:    11723, Lr: 0.000300\n",
            "2021-07-22 09:28:53,213 - INFO - joeynmt.training - Epoch   2, Step:   202400, Batch Loss:     1.740538, Tokens per Sec:    11559, Lr: 0.000300\n",
            "2021-07-22 09:29:30,879 - INFO - joeynmt.training - Epoch   2, Step:   202600, Batch Loss:     1.908155, Tokens per Sec:    11728, Lr: 0.000300\n",
            "2021-07-22 09:30:08,711 - INFO - joeynmt.training - Epoch   2, Step:   202800, Batch Loss:     2.162720, Tokens per Sec:    11486, Lr: 0.000300\n",
            "2021-07-22 09:30:46,548 - INFO - joeynmt.training - Epoch   2, Step:   203000, Batch Loss:     1.706066, Tokens per Sec:    11337, Lr: 0.000300\n",
            "2021-07-22 09:31:24,405 - INFO - joeynmt.training - Epoch   2, Step:   203200, Batch Loss:     1.837804, Tokens per Sec:    11389, Lr: 0.000300\n",
            "2021-07-22 09:32:02,106 - INFO - joeynmt.training - Epoch   2, Step:   203400, Batch Loss:     1.763763, Tokens per Sec:    11542, Lr: 0.000300\n",
            "2021-07-22 09:32:39,892 - INFO - joeynmt.training - Epoch   2, Step:   203600, Batch Loss:     1.459446, Tokens per Sec:    11511, Lr: 0.000300\n",
            "2021-07-22 09:33:17,726 - INFO - joeynmt.training - Epoch   2, Step:   203800, Batch Loss:     1.866957, Tokens per Sec:    11455, Lr: 0.000300\n",
            "2021-07-22 09:33:55,704 - INFO - joeynmt.training - Epoch   2, Step:   204000, Batch Loss:     1.688462, Tokens per Sec:    11439, Lr: 0.000300\n",
            "2021-07-22 09:34:33,143 - INFO - joeynmt.training - Epoch   2, Step:   204200, Batch Loss:     1.776409, Tokens per Sec:    11315, Lr: 0.000300\n",
            "2021-07-22 09:35:11,186 - INFO - joeynmt.training - Epoch   2, Step:   204400, Batch Loss:     1.814384, Tokens per Sec:    11332, Lr: 0.000300\n",
            "2021-07-22 09:35:48,842 - INFO - joeynmt.training - Epoch   2, Step:   204600, Batch Loss:     1.937450, Tokens per Sec:    11213, Lr: 0.000300\n",
            "2021-07-22 09:36:27,018 - INFO - joeynmt.training - Epoch   2, Step:   204800, Batch Loss:     1.877841, Tokens per Sec:    11272, Lr: 0.000300\n",
            "2021-07-22 09:37:05,073 - INFO - joeynmt.training - Epoch   2, Step:   205000, Batch Loss:     1.989169, Tokens per Sec:    11459, Lr: 0.000300\n",
            "2021-07-22 09:38:43,880 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 09:38:43,880 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 09:38:43,881 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 09:38:44,879 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 09:38:44,879 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 09:38:45,951 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 09:38:45,952 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 09:38:45,952 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 09:38:45,952 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 09:38:45,952 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 09:38:45,953 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 09:38:45,953 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 09:38:45,953 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 09:38:45,953 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 09:38:45,954 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 09:38:45,954 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 09:38:45,954 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-22 09:38:45,954 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 09:38:45,955 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 09:38:45,955 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 09:38:45,955 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 09:38:45,955 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   205000: bleu:  18.95, loss: 177804.8438, ppl:   7.2227, duration: 100.8817s\n",
            "2021-07-22 09:39:24,329 - INFO - joeynmt.training - Epoch   2, Step:   205200, Batch Loss:     1.845480, Tokens per Sec:    11373, Lr: 0.000300\n",
            "2021-07-22 09:40:02,673 - INFO - joeynmt.training - Epoch   2, Step:   205400, Batch Loss:     1.811384, Tokens per Sec:    11454, Lr: 0.000300\n",
            "2021-07-22 09:40:40,730 - INFO - joeynmt.training - Epoch   2, Step:   205600, Batch Loss:     2.030195, Tokens per Sec:    11449, Lr: 0.000300\n",
            "2021-07-22 09:41:19,084 - INFO - joeynmt.training - Epoch   2, Step:   205800, Batch Loss:     1.560964, Tokens per Sec:    11490, Lr: 0.000300\n",
            "2021-07-22 09:41:56,993 - INFO - joeynmt.training - Epoch   2, Step:   206000, Batch Loss:     1.831841, Tokens per Sec:    11462, Lr: 0.000300\n",
            "2021-07-22 09:42:35,060 - INFO - joeynmt.training - Epoch   2, Step:   206200, Batch Loss:     1.886085, Tokens per Sec:    11647, Lr: 0.000300\n",
            "2021-07-22 09:43:12,556 - INFO - joeynmt.training - Epoch   2, Step:   206400, Batch Loss:     1.723300, Tokens per Sec:    11489, Lr: 0.000300\n",
            "2021-07-22 09:43:50,438 - INFO - joeynmt.training - Epoch   2, Step:   206600, Batch Loss:     1.810144, Tokens per Sec:    11618, Lr: 0.000300\n",
            "2021-07-22 09:44:27,915 - INFO - joeynmt.training - Epoch   2, Step:   206800, Batch Loss:     1.940724, Tokens per Sec:    11594, Lr: 0.000300\n",
            "2021-07-22 09:45:05,502 - INFO - joeynmt.training - Epoch   2, Step:   207000, Batch Loss:     1.648451, Tokens per Sec:    11546, Lr: 0.000300\n",
            "2021-07-22 09:45:42,906 - INFO - joeynmt.training - Epoch   2, Step:   207200, Batch Loss:     1.808409, Tokens per Sec:    11452, Lr: 0.000300\n",
            "2021-07-22 09:46:20,427 - INFO - joeynmt.training - Epoch   2, Step:   207400, Batch Loss:     1.888265, Tokens per Sec:    11561, Lr: 0.000300\n",
            "2021-07-22 09:46:57,937 - INFO - joeynmt.training - Epoch   2, Step:   207600, Batch Loss:     1.697844, Tokens per Sec:    11493, Lr: 0.000300\n",
            "2021-07-22 09:47:35,997 - INFO - joeynmt.training - Epoch   2, Step:   207800, Batch Loss:     1.637012, Tokens per Sec:    11518, Lr: 0.000300\n",
            "2021-07-22 09:48:13,966 - INFO - joeynmt.training - Epoch   2, Step:   208000, Batch Loss:     1.907606, Tokens per Sec:    11525, Lr: 0.000300\n",
            "2021-07-22 09:48:51,991 - INFO - joeynmt.training - Epoch   2, Step:   208200, Batch Loss:     1.776001, Tokens per Sec:    11529, Lr: 0.000300\n",
            "2021-07-22 09:49:29,932 - INFO - joeynmt.training - Epoch   2, Step:   208400, Batch Loss:     1.726356, Tokens per Sec:    11463, Lr: 0.000300\n",
            "2021-07-22 09:50:07,722 - INFO - joeynmt.training - Epoch   2, Step:   208600, Batch Loss:     1.851877, Tokens per Sec:    11584, Lr: 0.000300\n",
            "2021-07-22 09:50:45,666 - INFO - joeynmt.training - Epoch   2, Step:   208800, Batch Loss:     1.891962, Tokens per Sec:    11398, Lr: 0.000300\n",
            "2021-07-22 09:51:23,442 - INFO - joeynmt.training - Epoch   2, Step:   209000, Batch Loss:     1.890008, Tokens per Sec:    11340, Lr: 0.000300\n",
            "2021-07-22 09:51:43,647 - INFO - joeynmt.training - Epoch   2: total training loss 15246.42\n",
            "2021-07-22 09:51:43,648 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-22 09:52:02,273 - INFO - joeynmt.training - Epoch   3, Step:   209200, Batch Loss:     1.808152, Tokens per Sec:    10938, Lr: 0.000300\n",
            "2021-07-22 09:52:39,959 - INFO - joeynmt.training - Epoch   3, Step:   209400, Batch Loss:     2.057272, Tokens per Sec:    11486, Lr: 0.000300\n",
            "2021-07-22 09:53:17,721 - INFO - joeynmt.training - Epoch   3, Step:   209600, Batch Loss:     1.796292, Tokens per Sec:    11450, Lr: 0.000300\n",
            "2021-07-22 09:53:55,056 - INFO - joeynmt.training - Epoch   3, Step:   209800, Batch Loss:     1.830656, Tokens per Sec:    11247, Lr: 0.000300\n",
            "2021-07-22 09:54:32,786 - INFO - joeynmt.training - Epoch   3, Step:   210000, Batch Loss:     1.947336, Tokens per Sec:    11352, Lr: 0.000300\n",
            "2021-07-22 09:56:11,288 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 09:56:11,288 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 09:56:11,288 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 09:56:13,182 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 09:56:13,183 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 09:56:13,183 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 09:56:13,183 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer , that is what is acceptable to God . ”\n",
            "2021-07-22 09:56:13,183 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 09:56:13,184 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 09:56:13,184 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 09:56:13,184 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard a voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 09:56:13,184 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 09:56:13,185 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 09:56:13,185 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 09:56:13,185 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-22 09:56:13,185 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 09:56:13,186 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 09:56:13,186 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 09:56:13,186 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 09:56:13,186 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   210000: bleu:  19.06, loss: 177821.7344, ppl:   7.2241, duration: 100.3999s\n",
            "2021-07-22 09:56:50,893 - INFO - joeynmt.training - Epoch   3, Step:   210200, Batch Loss:     1.778740, Tokens per Sec:    11321, Lr: 0.000300\n",
            "2021-07-22 09:57:28,440 - INFO - joeynmt.training - Epoch   3, Step:   210400, Batch Loss:     1.885357, Tokens per Sec:    11407, Lr: 0.000300\n",
            "2021-07-22 09:58:06,168 - INFO - joeynmt.training - Epoch   3, Step:   210600, Batch Loss:     1.714567, Tokens per Sec:    11502, Lr: 0.000300\n",
            "2021-07-22 09:58:44,353 - INFO - joeynmt.training - Epoch   3, Step:   210800, Batch Loss:     1.836579, Tokens per Sec:    11501, Lr: 0.000300\n",
            "2021-07-22 09:59:22,267 - INFO - joeynmt.training - Epoch   3, Step:   211000, Batch Loss:     1.878945, Tokens per Sec:    11466, Lr: 0.000300\n",
            "2021-07-22 10:00:00,254 - INFO - joeynmt.training - Epoch   3, Step:   211200, Batch Loss:     1.888098, Tokens per Sec:    11433, Lr: 0.000300\n",
            "2021-07-22 10:00:38,501 - INFO - joeynmt.training - Epoch   3, Step:   211400, Batch Loss:     1.903342, Tokens per Sec:    11511, Lr: 0.000300\n",
            "2021-07-22 10:01:16,614 - INFO - joeynmt.training - Epoch   3, Step:   211600, Batch Loss:     1.606817, Tokens per Sec:    11435, Lr: 0.000300\n",
            "2021-07-22 10:01:54,774 - INFO - joeynmt.training - Epoch   3, Step:   211800, Batch Loss:     1.954025, Tokens per Sec:    11453, Lr: 0.000300\n",
            "2021-07-22 10:02:32,580 - INFO - joeynmt.training - Epoch   3, Step:   212000, Batch Loss:     1.909081, Tokens per Sec:    11305, Lr: 0.000300\n",
            "2021-07-22 10:03:10,767 - INFO - joeynmt.training - Epoch   3, Step:   212200, Batch Loss:     1.943258, Tokens per Sec:    11503, Lr: 0.000300\n",
            "2021-07-22 10:03:48,836 - INFO - joeynmt.training - Epoch   3, Step:   212400, Batch Loss:     1.683366, Tokens per Sec:    11429, Lr: 0.000300\n",
            "2021-07-22 10:04:27,033 - INFO - joeynmt.training - Epoch   3, Step:   212600, Batch Loss:     1.911803, Tokens per Sec:    11419, Lr: 0.000300\n",
            "2021-07-22 10:05:04,893 - INFO - joeynmt.training - Epoch   3, Step:   212800, Batch Loss:     1.815352, Tokens per Sec:    11499, Lr: 0.000300\n",
            "2021-07-22 10:05:42,946 - INFO - joeynmt.training - Epoch   3, Step:   213000, Batch Loss:     1.701727, Tokens per Sec:    11338, Lr: 0.000300\n",
            "2021-07-22 10:06:21,162 - INFO - joeynmt.training - Epoch   3, Step:   213200, Batch Loss:     1.787460, Tokens per Sec:    11424, Lr: 0.000300\n",
            "2021-07-22 10:06:59,174 - INFO - joeynmt.training - Epoch   3, Step:   213400, Batch Loss:     2.009297, Tokens per Sec:    11366, Lr: 0.000300\n",
            "2021-07-22 10:07:37,209 - INFO - joeynmt.training - Epoch   3, Step:   213600, Batch Loss:     1.857431, Tokens per Sec:    11229, Lr: 0.000300\n",
            "2021-07-22 10:08:15,248 - INFO - joeynmt.training - Epoch   3, Step:   213800, Batch Loss:     1.739471, Tokens per Sec:    11423, Lr: 0.000300\n",
            "2021-07-22 10:08:53,056 - INFO - joeynmt.training - Epoch   3, Step:   214000, Batch Loss:     1.830510, Tokens per Sec:    11336, Lr: 0.000300\n",
            "2021-07-22 10:09:31,174 - INFO - joeynmt.training - Epoch   3, Step:   214200, Batch Loss:     1.572855, Tokens per Sec:    11452, Lr: 0.000300\n",
            "2021-07-22 10:10:09,177 - INFO - joeynmt.training - Epoch   3, Step:   214400, Batch Loss:     1.626724, Tokens per Sec:    11462, Lr: 0.000300\n",
            "2021-07-22 10:10:47,480 - INFO - joeynmt.training - Epoch   3, Step:   214600, Batch Loss:     2.069488, Tokens per Sec:    11428, Lr: 0.000300\n",
            "2021-07-22 10:11:25,456 - INFO - joeynmt.training - Epoch   3, Step:   214800, Batch Loss:     1.898850, Tokens per Sec:    11397, Lr: 0.000300\n",
            "2021-07-22 10:12:03,419 - INFO - joeynmt.training - Epoch   3, Step:   215000, Batch Loss:     1.825968, Tokens per Sec:    11644, Lr: 0.000300\n",
            "2021-07-22 10:13:48,780 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 10:13:48,781 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 10:13:48,781 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 10:13:49,740 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 10:13:49,741 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 10:13:50,530 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 10:13:50,531 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 10:13:50,531 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 10:13:50,532 - INFO - joeynmt.training - \tHypothesis: “ When you are doing good and suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-22 10:13:50,532 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 10:13:50,532 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 10:13:50,532 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 10:13:50,533 - INFO - joeynmt.training - \tHypothesis: After Jesus ’ baptism , he heard a voice from heaven , saying : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 10:13:50,533 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 10:13:50,533 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 10:13:50,533 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 10:13:50,533 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-22 10:13:50,534 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 10:13:50,534 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 10:13:50,534 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 10:13:50,534 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 10:13:50,535 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   215000: bleu:  18.71, loss: 177669.5938, ppl:   7.2119, duration: 107.1157s\n",
            "2021-07-22 10:14:28,399 - INFO - joeynmt.training - Epoch   3, Step:   215200, Batch Loss:     1.762812, Tokens per Sec:    11109, Lr: 0.000300\n",
            "2021-07-22 10:15:06,050 - INFO - joeynmt.training - Epoch   3, Step:   215400, Batch Loss:     1.803174, Tokens per Sec:    11520, Lr: 0.000300\n",
            "2021-07-22 10:15:44,294 - INFO - joeynmt.training - Epoch   3, Step:   215600, Batch Loss:     1.806947, Tokens per Sec:    11490, Lr: 0.000300\n",
            "2021-07-22 10:16:22,128 - INFO - joeynmt.training - Epoch   3, Step:   215800, Batch Loss:     1.515330, Tokens per Sec:    11456, Lr: 0.000300\n",
            "2021-07-22 10:17:00,140 - INFO - joeynmt.training - Epoch   3, Step:   216000, Batch Loss:     1.950682, Tokens per Sec:    11507, Lr: 0.000300\n",
            "2021-07-22 10:17:38,172 - INFO - joeynmt.training - Epoch   3, Step:   216200, Batch Loss:     1.683542, Tokens per Sec:    11433, Lr: 0.000300\n",
            "2021-07-22 10:18:16,404 - INFO - joeynmt.training - Epoch   3, Step:   216400, Batch Loss:     1.731110, Tokens per Sec:    11548, Lr: 0.000300\n",
            "2021-07-22 10:18:54,177 - INFO - joeynmt.training - Epoch   3, Step:   216600, Batch Loss:     1.740657, Tokens per Sec:    11512, Lr: 0.000300\n",
            "2021-07-22 10:19:32,021 - INFO - joeynmt.training - Epoch   3, Step:   216800, Batch Loss:     1.563819, Tokens per Sec:    11430, Lr: 0.000300\n",
            "2021-07-22 10:20:10,074 - INFO - joeynmt.training - Epoch   3, Step:   217000, Batch Loss:     1.823498, Tokens per Sec:    11505, Lr: 0.000300\n",
            "2021-07-22 10:20:48,254 - INFO - joeynmt.training - Epoch   3, Step:   217200, Batch Loss:     1.856915, Tokens per Sec:    11490, Lr: 0.000300\n",
            "2021-07-22 10:21:26,322 - INFO - joeynmt.training - Epoch   3, Step:   217400, Batch Loss:     1.953613, Tokens per Sec:    11422, Lr: 0.000300\n",
            "2021-07-22 10:21:41,895 - INFO - joeynmt.training - Epoch   3: total training loss 15210.72\n",
            "2021-07-22 10:21:41,896 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-22 10:22:05,464 - INFO - joeynmt.training - Epoch   4, Step:   217600, Batch Loss:     1.349036, Tokens per Sec:    10831, Lr: 0.000300\n",
            "2021-07-22 10:22:43,309 - INFO - joeynmt.training - Epoch   4, Step:   217800, Batch Loss:     1.843897, Tokens per Sec:    11488, Lr: 0.000300\n",
            "2021-07-22 10:23:20,972 - INFO - joeynmt.training - Epoch   4, Step:   218000, Batch Loss:     1.919656, Tokens per Sec:    11492, Lr: 0.000300\n",
            "2021-07-22 10:23:59,008 - INFO - joeynmt.training - Epoch   4, Step:   218200, Batch Loss:     1.900616, Tokens per Sec:    11445, Lr: 0.000300\n",
            "2021-07-22 10:24:37,109 - INFO - joeynmt.training - Epoch   4, Step:   218400, Batch Loss:     1.886723, Tokens per Sec:    11371, Lr: 0.000300\n",
            "2021-07-22 10:25:15,166 - INFO - joeynmt.training - Epoch   4, Step:   218600, Batch Loss:     1.772266, Tokens per Sec:    11541, Lr: 0.000300\n",
            "2021-07-22 10:25:52,899 - INFO - joeynmt.training - Epoch   4, Step:   218800, Batch Loss:     1.768412, Tokens per Sec:    11422, Lr: 0.000300\n",
            "2021-07-22 10:26:30,840 - INFO - joeynmt.training - Epoch   4, Step:   219000, Batch Loss:     1.839059, Tokens per Sec:    11320, Lr: 0.000300\n",
            "2021-07-22 10:27:09,173 - INFO - joeynmt.training - Epoch   4, Step:   219200, Batch Loss:     2.028450, Tokens per Sec:    11486, Lr: 0.000300\n",
            "2021-07-22 10:27:46,901 - INFO - joeynmt.training - Epoch   4, Step:   219400, Batch Loss:     1.837149, Tokens per Sec:    11290, Lr: 0.000300\n",
            "2021-07-22 10:28:25,305 - INFO - joeynmt.training - Epoch   4, Step:   219600, Batch Loss:     1.967171, Tokens per Sec:    11494, Lr: 0.000300\n",
            "2021-07-22 10:29:03,499 - INFO - joeynmt.training - Epoch   4, Step:   219800, Batch Loss:     1.597399, Tokens per Sec:    11465, Lr: 0.000300\n",
            "2021-07-22 10:29:41,692 - INFO - joeynmt.training - Epoch   4, Step:   220000, Batch Loss:     1.571550, Tokens per Sec:    11502, Lr: 0.000300\n",
            "2021-07-22 10:31:24,236 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 10:31:24,236 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 10:31:24,237 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 10:31:25,287 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 10:31:25,287 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 10:31:26,099 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 10:31:26,100 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 10:31:26,100 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 10:31:26,101 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , you endure , what is acceptable to God . ”\n",
            "2021-07-22 10:31:26,101 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 10:31:26,101 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 10:31:26,101 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 10:31:26,102 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 10:31:26,102 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 10:31:26,102 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 10:31:26,102 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 10:31:26,103 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-22 10:31:26,103 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 10:31:26,103 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 10:31:26,103 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 10:31:26,104 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 10:31:26,104 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   220000: bleu:  19.12, loss: 176938.8906, ppl:   7.1535, duration: 104.4114s\n",
            "2021-07-22 10:32:04,604 - INFO - joeynmt.training - Epoch   4, Step:   220200, Batch Loss:     2.002788, Tokens per Sec:    11362, Lr: 0.000300\n",
            "2021-07-22 10:32:42,765 - INFO - joeynmt.training - Epoch   4, Step:   220400, Batch Loss:     1.734076, Tokens per Sec:    11467, Lr: 0.000300\n",
            "2021-07-22 10:33:20,936 - INFO - joeynmt.training - Epoch   4, Step:   220600, Batch Loss:     1.932831, Tokens per Sec:    11323, Lr: 0.000300\n",
            "2021-07-22 10:33:59,256 - INFO - joeynmt.training - Epoch   4, Step:   220800, Batch Loss:     2.065290, Tokens per Sec:    11501, Lr: 0.000300\n",
            "2021-07-22 10:34:37,423 - INFO - joeynmt.training - Epoch   4, Step:   221000, Batch Loss:     1.670607, Tokens per Sec:    11348, Lr: 0.000300\n",
            "2021-07-22 10:35:15,404 - INFO - joeynmt.training - Epoch   4, Step:   221200, Batch Loss:     1.808910, Tokens per Sec:    11421, Lr: 0.000300\n",
            "2021-07-22 10:35:53,366 - INFO - joeynmt.training - Epoch   4, Step:   221400, Batch Loss:     1.864199, Tokens per Sec:    11384, Lr: 0.000300\n",
            "2021-07-22 10:36:31,715 - INFO - joeynmt.training - Epoch   4, Step:   221600, Batch Loss:     1.675121, Tokens per Sec:    11454, Lr: 0.000300\n",
            "2021-07-22 10:37:09,259 - INFO - joeynmt.training - Epoch   4, Step:   221800, Batch Loss:     1.887582, Tokens per Sec:    11336, Lr: 0.000300\n",
            "2021-07-22 10:37:47,658 - INFO - joeynmt.training - Epoch   4, Step:   222000, Batch Loss:     1.624483, Tokens per Sec:    11535, Lr: 0.000300\n",
            "2021-07-22 10:38:25,740 - INFO - joeynmt.training - Epoch   4, Step:   222200, Batch Loss:     2.058238, Tokens per Sec:    11430, Lr: 0.000300\n",
            "2021-07-22 10:39:03,483 - INFO - joeynmt.training - Epoch   4, Step:   222400, Batch Loss:     1.845738, Tokens per Sec:    11523, Lr: 0.000300\n",
            "2021-07-22 10:39:42,040 - INFO - joeynmt.training - Epoch   4, Step:   222600, Batch Loss:     1.805023, Tokens per Sec:    11673, Lr: 0.000300\n",
            "2021-07-22 10:40:19,766 - INFO - joeynmt.training - Epoch   4, Step:   222800, Batch Loss:     1.884607, Tokens per Sec:    11425, Lr: 0.000300\n",
            "2021-07-22 10:40:57,150 - INFO - joeynmt.training - Epoch   4, Step:   223000, Batch Loss:     1.876452, Tokens per Sec:    11651, Lr: 0.000300\n",
            "2021-07-22 10:41:34,273 - INFO - joeynmt.training - Epoch   4, Step:   223200, Batch Loss:     1.806628, Tokens per Sec:    11490, Lr: 0.000300\n",
            "2021-07-22 10:42:11,731 - INFO - joeynmt.training - Epoch   4, Step:   223400, Batch Loss:     1.541729, Tokens per Sec:    11601, Lr: 0.000300\n",
            "2021-07-22 10:42:49,373 - INFO - joeynmt.training - Epoch   4, Step:   223600, Batch Loss:     1.700015, Tokens per Sec:    11645, Lr: 0.000300\n",
            "2021-07-22 10:43:26,910 - INFO - joeynmt.training - Epoch   4, Step:   223800, Batch Loss:     1.978825, Tokens per Sec:    11400, Lr: 0.000300\n",
            "2021-07-22 10:44:04,439 - INFO - joeynmt.training - Epoch   4, Step:   224000, Batch Loss:     1.770328, Tokens per Sec:    11590, Lr: 0.000300\n",
            "2021-07-22 10:44:41,893 - INFO - joeynmt.training - Epoch   4, Step:   224200, Batch Loss:     1.822289, Tokens per Sec:    11308, Lr: 0.000300\n",
            "2021-07-22 10:45:19,780 - INFO - joeynmt.training - Epoch   4, Step:   224400, Batch Loss:     1.688790, Tokens per Sec:    11566, Lr: 0.000300\n",
            "2021-07-22 10:45:57,717 - INFO - joeynmt.training - Epoch   4, Step:   224600, Batch Loss:     1.672861, Tokens per Sec:    11428, Lr: 0.000300\n",
            "2021-07-22 10:46:35,662 - INFO - joeynmt.training - Epoch   4, Step:   224800, Batch Loss:     1.634920, Tokens per Sec:    11555, Lr: 0.000300\n",
            "2021-07-22 10:47:13,707 - INFO - joeynmt.training - Epoch   4, Step:   225000, Batch Loss:     1.852215, Tokens per Sec:    11536, Lr: 0.000300\n",
            "2021-07-22 10:48:52,411 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 10:48:52,412 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 10:48:52,412 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 10:48:53,442 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 10:48:53,443 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 10:48:54,243 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 10:48:54,244 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 10:48:54,244 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 10:48:54,244 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffering , you are endured , that is acceptable to God . ”\n",
            "2021-07-22 10:48:54,244 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 10:48:54,245 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 10:48:54,245 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 10:48:54,245 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 10:48:54,245 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 10:48:54,246 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 10:48:54,246 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 10:48:54,246 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-22 10:48:54,246 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 10:48:54,246 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 10:48:54,247 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 10:48:54,247 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 10:48:54,247 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   225000: bleu:  19.33, loss: 176262.4062, ppl:   7.0999, duration: 100.5395s\n",
            "2021-07-22 10:49:32,544 - INFO - joeynmt.training - Epoch   4, Step:   225200, Batch Loss:     1.919915, Tokens per Sec:    11379, Lr: 0.000300\n",
            "2021-07-22 10:50:10,492 - INFO - joeynmt.training - Epoch   4, Step:   225400, Batch Loss:     1.977975, Tokens per Sec:    11406, Lr: 0.000300\n",
            "2021-07-22 10:50:48,284 - INFO - joeynmt.training - Epoch   4, Step:   225600, Batch Loss:     2.148426, Tokens per Sec:    11393, Lr: 0.000300\n",
            "2021-07-22 10:51:26,086 - INFO - joeynmt.training - Epoch   4, Step:   225800, Batch Loss:     1.902941, Tokens per Sec:    11446, Lr: 0.000300\n",
            "2021-07-22 10:51:33,956 - INFO - joeynmt.training - Epoch   4: total training loss 15140.92\n",
            "2021-07-22 10:51:33,957 - INFO - joeynmt.training - Training ended after   4 epochs.\n",
            "2021-07-22 10:51:33,957 - INFO - joeynmt.training - Best validation result (greedy) at step   225000:   7.10 ppl.\n",
            "2021-07-22 10:51:33,980 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-22 10:51:34,353 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-22 10:51:34,558 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-22 10:51:34,628 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe.en)...\n",
            "2021-07-22 10:53:33,067 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 10:53:33,067 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 10:53:33,067 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 10:53:33,965 - INFO - joeynmt.prediction -  dev bleu[13a]:  19.62 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-22 10:53:33,972 - INFO - joeynmt.prediction - Translations saved to: models/lg_rw_lhen_reverse_transformer_continued3/00225000.hyps.dev\n",
            "2021-07-22 10:53:33,972 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.en)...\n",
            "2021-07-22 10:54:24,132 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 10:54:24,133 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 10:54:24,133 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 10:54:24,480 - INFO - joeynmt.prediction - test bleu[13a]:   8.75 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-22 10:54:24,499 - INFO - joeynmt.prediction - Translations saved to: models/lg_rw_lhen_reverse_transformer_continued3/00225000.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2naTNQCQs-qL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83aab9a8-2c95-4f41-87b7-2bbf70126aa6"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_rw_lhen_reverse_transformer_continued3/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/translation.bpe.lh_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-26 08:04:54,859 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-26 08:05:04,977 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-26 08:05:05,387 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueU0sUrmrKKX",
        "outputId": "dcf4cca8-bfa3-435f-df16-ffff0cb0a515"
      },
      "source": [
        "!cat \"translation.bpe.lh_en\" | sacrebleu \"test1.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 8.8 41.2/14.3/5.6/2.8 (BP = 0.891 ratio = 0.897 hyp_len = 23360 ref_len = 26044)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fGqzRy1tnwp",
        "outputId": "14c039ef-6902-492c-9d12-11062bc78d8c"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_rw_lhen_reverse_transformer_continued3/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.rw\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/translation.bpe.rw_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-26 08:10:58,707 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-26 08:11:01,758 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-26 08:11:02,027 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00YW7ydBvJFf",
        "outputId": "4f317313-5c2b-4a72-8a57-8c98a07a254c"
      },
      "source": [
        "!cat \"translation.bpe.rw_en\" | sacrebleu \"test3.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 36.8 68.0/46.0/34.8/27.4 (BP = 0.884 ratio = 0.890 hyp_len = 37790 ref_len = 42439)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYWnYjdGwxmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9756154a-9b1c-4ec0-c4e8-c3d5e861663e"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_rw_lhen_reverse_transformer_continued3/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.lg\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/translation.bpe.lg_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-26 08:18:24,485 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-26 08:18:27,528 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-26 08:18:27,792 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDpzciacHKNU",
        "outputId": "b033dc16-22af-4db2-ccd0-a97153d8596e"
      },
      "source": [
        "!cat \"translation.bpe.lg_en\" | sacrebleu \"test2.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 35.4 65.6/43.6/32.5/25.3 (BP = 0.905 ratio = 0.909 hyp_len = 39211 ref_len = 43116)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8fsxH27v3C7"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 225000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/models/lg_rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued3/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_rw_lhen_reverse_transformer\"', f'model_dir: \"models/lg_rw_lhen_reverse_transformer_continued4\"')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload4.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khO7Vl0k4pHR",
        "collapsed": true,
        "outputId": "68907b8c-5acb-47b7-8900-a43438b67f8f"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_rw_lhen_reload4.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-22 11:04:01,211 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-22 11:04:01,237 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-22 11:04:14,924 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-22 11:04:15,256 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-22 11:04:15,383 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-22 11:04:15,398 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-22 11:04:15,398 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-22 11:04:15,634 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-22 11:04:15.849913: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-22 11:04:19,270 - INFO - joeynmt.training - Total params: 12179456\n",
            "2021-07-22 11:04:22,079 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued3/225000.ckpt\n",
            "2021-07-22 11:04:22,555 - INFO - joeynmt.helpers - cfg.name                           : lg_rw_lhen_reverse_transformer\n",
            "2021-07-22 11:04:22,555 - INFO - joeynmt.helpers - cfg.data.src                       : lg_rw_lh\n",
            "2021-07-22 11:04:22,555 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-22 11:04:22,556 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\n",
            "2021-07-22 11:04:22,556 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\n",
            "2021-07-22 11:04:22,556 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\n",
            "2021-07-22 11:04:22,556 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-22 11:04:22,556 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-22 11:04:22,556 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-22 11:04:22,556 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-22 11:04:22,557 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-22 11:04:22,557 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-22 11:04:22,557 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-22 11:04:22,557 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued3/225000.ckpt\n",
            "2021-07-22 11:04:22,557 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-22 11:04:22,557 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-22 11:04:22,558 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-22 11:04:22,558 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-22 11:04:22,558 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-22 11:04:22,558 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-22 11:04:22,558 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-22 11:04:22,558 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-22 11:04:22,558 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-22 11:04:22,559 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-22 11:04:22,559 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-22 11:04:22,559 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-22 11:04:22,559 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-22 11:04:22,559 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-22 11:04:22,559 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-22 11:04:22,560 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-22 11:04:22,560 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-22 11:04:22,560 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-22 11:04:22,560 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-22 11:04:22,560 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-22 11:04:22,560 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-22 11:04:22,560 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-22 11:04:22,561 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-22 11:04:22,561 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-22 11:04:22,561 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_rw_lhen_reverse_transformer_continued4\n",
            "2021-07-22 11:04:22,561 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-22 11:04:22,561 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-22 11:04:22,562 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-22 11:04:22,562 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-22 11:04:22,562 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-22 11:04:22,562 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-22 11:04:22,562 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-22 11:04:22,562 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-22 11:04:22,563 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-22 11:04:22,563 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-22 11:04:22,563 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-22 11:04:22,563 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-22 11:04:22,563 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-22 11:04:22,563 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-22 11:04:22,564 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-22 11:04:22,564 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-22 11:04:22,564 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-22 11:04:22,564 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-22 11:04:22,564 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-22 11:04:22,565 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-22 11:04:22,565 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-22 11:04:22,565 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-22 11:04:22,565 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-22 11:04:22,565 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-22 11:04:22,565 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-22 11:04:22,566 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-22 11:04:22,566 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-22 11:04:22,566 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-22 11:04:22,566 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-22 11:04:22,566 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-22 11:04:22,566 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-22 11:04:22,566 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 665203,\n",
            "\tvalid 3000,\n",
            "\ttest 1000\n",
            "2021-07-22 11:04:22,567 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ at@@ andika okuk@@ olera ku m@@ azima ge nn@@ ali nj@@ iga , era nn@@ ak@@ ir@@ aba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obuf@@ uzi n’@@ okul@@ eka em@@ ikw@@ ano em@@ ibi gye nn@@ alina .\n",
            "\t[TRG] Ev@@ ent@@ ually , however , the tr@@ uth@@ s I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my polit@@ ical view@@ po@@ in@@ ts and associ@@ ations .\n",
            "2021-07-22 11:04:22,567 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-22 11:04:22,567 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-22 11:04:22,567 - INFO - joeynmt.helpers - Number of Src words (types): 4372\n",
            "2021-07-22 11:04:22,567 - INFO - joeynmt.helpers - Number of Trg words (types): 4372\n",
            "2021-07-22 11:04:22,568 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4372),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4372))\n",
            "2021-07-22 11:04:22,580 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-22 11:04:22,580 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-22 11:05:04,174 - INFO - joeynmt.training - Epoch   1, Step:   225200, Batch Loss:     1.926449, Tokens per Sec:    10477, Lr: 0.000300\n",
            "2021-07-22 11:05:42,096 - INFO - joeynmt.training - Epoch   1, Step:   225400, Batch Loss:     1.987795, Tokens per Sec:    11414, Lr: 0.000300\n",
            "2021-07-22 11:06:20,049 - INFO - joeynmt.training - Epoch   1, Step:   225600, Batch Loss:     2.146865, Tokens per Sec:    11345, Lr: 0.000300\n",
            "2021-07-22 11:06:58,229 - INFO - joeynmt.training - Epoch   1, Step:   225800, Batch Loss:     1.918326, Tokens per Sec:    11333, Lr: 0.000300\n",
            "2021-07-22 11:07:06,233 - INFO - joeynmt.training - Epoch   1: total training loss 1523.13\n",
            "2021-07-22 11:07:06,233 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-22 11:07:37,706 - INFO - joeynmt.training - Epoch   2, Step:   226000, Batch Loss:     1.877991, Tokens per Sec:    10910, Lr: 0.000300\n",
            "2021-07-22 11:08:15,588 - INFO - joeynmt.training - Epoch   2, Step:   226200, Batch Loss:     1.758515, Tokens per Sec:    11316, Lr: 0.000300\n",
            "2021-07-22 11:08:53,721 - INFO - joeynmt.training - Epoch   2, Step:   226400, Batch Loss:     1.856339, Tokens per Sec:    11280, Lr: 0.000300\n",
            "2021-07-22 11:09:32,057 - INFO - joeynmt.training - Epoch   2, Step:   226600, Batch Loss:     1.588667, Tokens per Sec:    11385, Lr: 0.000300\n",
            "2021-07-22 11:10:09,878 - INFO - joeynmt.training - Epoch   2, Step:   226800, Batch Loss:     1.719516, Tokens per Sec:    11282, Lr: 0.000300\n",
            "2021-07-22 11:10:48,276 - INFO - joeynmt.training - Epoch   2, Step:   227000, Batch Loss:     1.821887, Tokens per Sec:    11553, Lr: 0.000300\n",
            "2021-07-22 11:11:26,349 - INFO - joeynmt.training - Epoch   2, Step:   227200, Batch Loss:     1.725932, Tokens per Sec:    11356, Lr: 0.000300\n",
            "2021-07-22 11:12:04,761 - INFO - joeynmt.training - Epoch   2, Step:   227400, Batch Loss:     1.938259, Tokens per Sec:    11376, Lr: 0.000300\n",
            "2021-07-22 11:12:42,976 - INFO - joeynmt.training - Epoch   2, Step:   227600, Batch Loss:     1.610751, Tokens per Sec:    11357, Lr: 0.000300\n",
            "2021-07-22 11:13:21,255 - INFO - joeynmt.training - Epoch   2, Step:   227800, Batch Loss:     1.985983, Tokens per Sec:    11281, Lr: 0.000300\n",
            "2021-07-22 11:13:59,488 - INFO - joeynmt.training - Epoch   2, Step:   228000, Batch Loss:     1.902937, Tokens per Sec:    11447, Lr: 0.000300\n",
            "2021-07-22 11:14:37,675 - INFO - joeynmt.training - Epoch   2, Step:   228200, Batch Loss:     1.846644, Tokens per Sec:    11405, Lr: 0.000300\n",
            "2021-07-22 11:15:15,772 - INFO - joeynmt.training - Epoch   2, Step:   228400, Batch Loss:     1.913318, Tokens per Sec:    11283, Lr: 0.000300\n",
            "2021-07-22 11:15:53,755 - INFO - joeynmt.training - Epoch   2, Step:   228600, Batch Loss:     1.616751, Tokens per Sec:    11363, Lr: 0.000300\n",
            "2021-07-22 11:16:31,814 - INFO - joeynmt.training - Epoch   2, Step:   228800, Batch Loss:     1.983702, Tokens per Sec:    11604, Lr: 0.000300\n",
            "2021-07-22 11:17:09,447 - INFO - joeynmt.training - Epoch   2, Step:   229000, Batch Loss:     1.751892, Tokens per Sec:    11362, Lr: 0.000300\n",
            "2021-07-22 11:17:47,391 - INFO - joeynmt.training - Epoch   2, Step:   229200, Batch Loss:     1.883160, Tokens per Sec:    11554, Lr: 0.000300\n",
            "2021-07-22 11:18:25,022 - INFO - joeynmt.training - Epoch   2, Step:   229400, Batch Loss:     1.731467, Tokens per Sec:    11462, Lr: 0.000300\n",
            "2021-07-22 11:19:02,942 - INFO - joeynmt.training - Epoch   2, Step:   229600, Batch Loss:     1.378969, Tokens per Sec:    11599, Lr: 0.000300\n",
            "2021-07-22 11:19:40,592 - INFO - joeynmt.training - Epoch   2, Step:   229800, Batch Loss:     1.836981, Tokens per Sec:    11418, Lr: 0.000300\n",
            "2021-07-22 11:20:18,210 - INFO - joeynmt.training - Epoch   2, Step:   230000, Batch Loss:     1.725591, Tokens per Sec:    11475, Lr: 0.000300\n",
            "2021-07-22 11:21:57,892 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 11:21:57,893 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 11:21:57,893 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 11:21:58,921 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 11:21:58,921 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 11:21:59,656 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 11:21:59,657 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 11:21:59,657 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 11:21:59,657 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 11:21:59,657 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 11:21:59,658 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 11:21:59,658 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 11:21:59,659 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 11:21:59,659 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 11:21:59,660 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 11:21:59,660 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 11:21:59,660 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-22 11:21:59,660 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 11:21:59,661 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 11:21:59,661 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 11:21:59,662 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 11:21:59,662 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   230000: bleu:  19.06, loss: 176138.4531, ppl:   7.0901, duration: 101.4518s\n",
            "2021-07-22 11:22:37,702 - INFO - joeynmt.training - Epoch   2, Step:   230200, Batch Loss:     2.154739, Tokens per Sec:    11552, Lr: 0.000300\n",
            "2021-07-22 11:23:15,786 - INFO - joeynmt.training - Epoch   2, Step:   230400, Batch Loss:     1.827179, Tokens per Sec:    11584, Lr: 0.000300\n",
            "2021-07-22 11:23:53,092 - INFO - joeynmt.training - Epoch   2, Step:   230600, Batch Loss:     1.767528, Tokens per Sec:    11347, Lr: 0.000300\n",
            "2021-07-22 11:24:31,303 - INFO - joeynmt.training - Epoch   2, Step:   230800, Batch Loss:     1.851896, Tokens per Sec:    11568, Lr: 0.000300\n",
            "2021-07-22 11:25:09,123 - INFO - joeynmt.training - Epoch   2, Step:   231000, Batch Loss:     1.652893, Tokens per Sec:    11526, Lr: 0.000300\n",
            "2021-07-22 11:25:46,475 - INFO - joeynmt.training - Epoch   2, Step:   231200, Batch Loss:     2.528368, Tokens per Sec:    11272, Lr: 0.000300\n",
            "2021-07-22 11:26:24,428 - INFO - joeynmt.training - Epoch   2, Step:   231400, Batch Loss:     1.754309, Tokens per Sec:    11529, Lr: 0.000300\n",
            "2021-07-22 11:27:02,001 - INFO - joeynmt.training - Epoch   2, Step:   231600, Batch Loss:     1.810864, Tokens per Sec:    11373, Lr: 0.000300\n",
            "2021-07-22 11:27:39,594 - INFO - joeynmt.training - Epoch   2, Step:   231800, Batch Loss:     1.915354, Tokens per Sec:    11547, Lr: 0.000300\n",
            "2021-07-22 11:28:17,409 - INFO - joeynmt.training - Epoch   2, Step:   232000, Batch Loss:     1.925398, Tokens per Sec:    11398, Lr: 0.000300\n",
            "2021-07-22 11:28:55,318 - INFO - joeynmt.training - Epoch   2, Step:   232200, Batch Loss:     1.649088, Tokens per Sec:    11680, Lr: 0.000300\n",
            "2021-07-22 11:29:33,068 - INFO - joeynmt.training - Epoch   2, Step:   232400, Batch Loss:     1.939701, Tokens per Sec:    11516, Lr: 0.000300\n",
            "2021-07-22 11:30:11,054 - INFO - joeynmt.training - Epoch   2, Step:   232600, Batch Loss:     1.510788, Tokens per Sec:    11600, Lr: 0.000300\n",
            "2021-07-22 11:30:48,514 - INFO - joeynmt.training - Epoch   2, Step:   232800, Batch Loss:     1.696860, Tokens per Sec:    11420, Lr: 0.000300\n",
            "2021-07-22 11:31:26,863 - INFO - joeynmt.training - Epoch   2, Step:   233000, Batch Loss:     1.761663, Tokens per Sec:    11611, Lr: 0.000300\n",
            "2021-07-22 11:32:04,772 - INFO - joeynmt.training - Epoch   2, Step:   233200, Batch Loss:     1.719700, Tokens per Sec:    11506, Lr: 0.000300\n",
            "2021-07-22 11:32:42,765 - INFO - joeynmt.training - Epoch   2, Step:   233400, Batch Loss:     1.991185, Tokens per Sec:    11529, Lr: 0.000300\n",
            "2021-07-22 11:33:20,758 - INFO - joeynmt.training - Epoch   2, Step:   233600, Batch Loss:     1.953439, Tokens per Sec:    11554, Lr: 0.000300\n",
            "2021-07-22 11:33:58,865 - INFO - joeynmt.training - Epoch   2, Step:   233800, Batch Loss:     2.015292, Tokens per Sec:    11496, Lr: 0.000300\n",
            "2021-07-22 11:34:36,679 - INFO - joeynmt.training - Epoch   2, Step:   234000, Batch Loss:     1.955307, Tokens per Sec:    11485, Lr: 0.000300\n",
            "2021-07-22 11:35:14,377 - INFO - joeynmt.training - Epoch   2, Step:   234200, Batch Loss:     1.896950, Tokens per Sec:    11387, Lr: 0.000300\n",
            "2021-07-22 11:35:15,907 - INFO - joeynmt.training - Epoch   2: total training loss 15105.96\n",
            "2021-07-22 11:35:15,908 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-22 11:35:53,381 - INFO - joeynmt.training - Epoch   3, Step:   234400, Batch Loss:     2.072881, Tokens per Sec:    11239, Lr: 0.000300\n",
            "2021-07-22 11:36:31,125 - INFO - joeynmt.training - Epoch   3, Step:   234600, Batch Loss:     1.797756, Tokens per Sec:    11460, Lr: 0.000300\n",
            "2021-07-22 11:37:08,975 - INFO - joeynmt.training - Epoch   3, Step:   234800, Batch Loss:     1.725457, Tokens per Sec:    11339, Lr: 0.000300\n",
            "2021-07-22 11:37:46,819 - INFO - joeynmt.training - Epoch   3, Step:   235000, Batch Loss:     1.274674, Tokens per Sec:    11384, Lr: 0.000300\n",
            "2021-07-22 11:39:28,056 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 11:39:28,056 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 11:39:28,057 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 11:39:29,041 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 11:39:29,041 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 11:39:29,828 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 11:39:29,829 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 11:39:29,830 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 11:39:29,830 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 11:39:29,830 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 11:39:29,830 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 11:39:29,831 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 11:39:29,831 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 11:39:29,831 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 11:39:29,831 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 11:39:29,832 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 11:39:29,832 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 11:39:29,832 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 11:39:29,832 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 11:39:29,833 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 11:39:29,833 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to do to save his family .\n",
            "2021-07-22 11:39:29,833 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   235000: bleu:  19.38, loss: 175465.1094, ppl:   7.0372, duration: 103.0134s\n",
            "2021-07-22 11:40:07,895 - INFO - joeynmt.training - Epoch   3, Step:   235200, Batch Loss:     1.216551, Tokens per Sec:    11453, Lr: 0.000300\n",
            "2021-07-22 11:40:45,510 - INFO - joeynmt.training - Epoch   3, Step:   235400, Batch Loss:     1.694842, Tokens per Sec:    11411, Lr: 0.000300\n",
            "2021-07-22 11:41:23,499 - INFO - joeynmt.training - Epoch   3, Step:   235600, Batch Loss:     1.840516, Tokens per Sec:    11428, Lr: 0.000300\n",
            "2021-07-22 11:42:01,206 - INFO - joeynmt.training - Epoch   3, Step:   235800, Batch Loss:     1.974565, Tokens per Sec:    11522, Lr: 0.000300\n",
            "2021-07-22 11:42:39,458 - INFO - joeynmt.training - Epoch   3, Step:   236000, Batch Loss:     1.432238, Tokens per Sec:    11452, Lr: 0.000300\n",
            "2021-07-22 11:43:17,445 - INFO - joeynmt.training - Epoch   3, Step:   236200, Batch Loss:     1.232591, Tokens per Sec:    11443, Lr: 0.000300\n",
            "2021-07-22 11:43:55,630 - INFO - joeynmt.training - Epoch   3, Step:   236400, Batch Loss:     1.886604, Tokens per Sec:    11418, Lr: 0.000300\n",
            "2021-07-22 11:44:33,372 - INFO - joeynmt.training - Epoch   3, Step:   236600, Batch Loss:     1.981254, Tokens per Sec:    11202, Lr: 0.000300\n",
            "2021-07-22 11:45:11,471 - INFO - joeynmt.training - Epoch   3, Step:   236800, Batch Loss:     1.425883, Tokens per Sec:    11176, Lr: 0.000300\n",
            "2021-07-22 11:45:49,255 - INFO - joeynmt.training - Epoch   3, Step:   237000, Batch Loss:     1.674838, Tokens per Sec:    11211, Lr: 0.000300\n",
            "2021-07-22 11:46:27,542 - INFO - joeynmt.training - Epoch   3, Step:   237200, Batch Loss:     1.652919, Tokens per Sec:    11316, Lr: 0.000300\n",
            "2021-07-22 11:47:05,538 - INFO - joeynmt.training - Epoch   3, Step:   237400, Batch Loss:     1.544819, Tokens per Sec:    11349, Lr: 0.000300\n",
            "2021-07-22 11:47:43,990 - INFO - joeynmt.training - Epoch   3, Step:   237600, Batch Loss:     1.817416, Tokens per Sec:    11474, Lr: 0.000300\n",
            "2021-07-22 11:48:21,942 - INFO - joeynmt.training - Epoch   3, Step:   237800, Batch Loss:     1.957423, Tokens per Sec:    11319, Lr: 0.000300\n",
            "2021-07-22 11:49:00,312 - INFO - joeynmt.training - Epoch   3, Step:   238000, Batch Loss:     1.833007, Tokens per Sec:    11375, Lr: 0.000300\n",
            "2021-07-22 11:49:38,646 - INFO - joeynmt.training - Epoch   3, Step:   238200, Batch Loss:     1.794845, Tokens per Sec:    11364, Lr: 0.000300\n",
            "2021-07-22 11:50:16,844 - INFO - joeynmt.training - Epoch   3, Step:   238400, Batch Loss:     1.862085, Tokens per Sec:    11327, Lr: 0.000300\n",
            "2021-07-22 11:50:55,074 - INFO - joeynmt.training - Epoch   3, Step:   238600, Batch Loss:     1.753710, Tokens per Sec:    11426, Lr: 0.000300\n",
            "2021-07-22 11:51:33,177 - INFO - joeynmt.training - Epoch   3, Step:   238800, Batch Loss:     1.807556, Tokens per Sec:    11348, Lr: 0.000300\n",
            "2021-07-22 11:52:11,640 - INFO - joeynmt.training - Epoch   3, Step:   239000, Batch Loss:     1.886092, Tokens per Sec:    11573, Lr: 0.000300\n",
            "2021-07-22 11:52:49,926 - INFO - joeynmt.training - Epoch   3, Step:   239200, Batch Loss:     1.686422, Tokens per Sec:    11527, Lr: 0.000300\n",
            "2021-07-22 11:53:28,007 - INFO - joeynmt.training - Epoch   3, Step:   239400, Batch Loss:     1.715493, Tokens per Sec:    11284, Lr: 0.000300\n",
            "2021-07-22 11:54:06,338 - INFO - joeynmt.training - Epoch   3, Step:   239600, Batch Loss:     1.853810, Tokens per Sec:    11516, Lr: 0.000300\n",
            "2021-07-22 11:54:44,365 - INFO - joeynmt.training - Epoch   3, Step:   239800, Batch Loss:     1.704303, Tokens per Sec:    11411, Lr: 0.000300\n",
            "2021-07-22 11:55:22,210 - INFO - joeynmt.training - Epoch   3, Step:   240000, Batch Loss:     1.921827, Tokens per Sec:    11262, Lr: 0.000300\n",
            "2021-07-22 11:57:06,504 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 11:57:06,504 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 11:57:06,504 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 11:57:08,409 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 11:57:08,409 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 11:57:08,409 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 11:57:08,410 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffering when you endure , that is acceptable to God . ”\n",
            "2021-07-22 11:57:08,410 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 11:57:08,410 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 11:57:08,411 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 11:57:08,411 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 11:57:08,411 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 11:57:08,411 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 11:57:08,411 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 11:57:08,412 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-22 11:57:08,412 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 11:57:08,412 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 11:57:08,412 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 11:57:08,413 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to heal his family .\n",
            "2021-07-22 11:57:08,413 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   240000: bleu:  19.27, loss: 175806.7500, ppl:   7.0640, duration: 106.2024s\n",
            "2021-07-22 11:57:46,478 - INFO - joeynmt.training - Epoch   3, Step:   240200, Batch Loss:     1.746032, Tokens per Sec:    11194, Lr: 0.000300\n",
            "2021-07-22 11:58:24,613 - INFO - joeynmt.training - Epoch   3, Step:   240400, Batch Loss:     1.896366, Tokens per Sec:    11368, Lr: 0.000300\n",
            "2021-07-22 11:59:02,730 - INFO - joeynmt.training - Epoch   3, Step:   240600, Batch Loss:     1.870453, Tokens per Sec:    11300, Lr: 0.000300\n",
            "2021-07-22 11:59:40,673 - INFO - joeynmt.training - Epoch   3, Step:   240800, Batch Loss:     1.723246, Tokens per Sec:    11406, Lr: 0.000300\n",
            "2021-07-22 12:00:18,911 - INFO - joeynmt.training - Epoch   3, Step:   241000, Batch Loss:     1.810650, Tokens per Sec:    11611, Lr: 0.000300\n",
            "2021-07-22 12:00:56,769 - INFO - joeynmt.training - Epoch   3, Step:   241200, Batch Loss:     2.222451, Tokens per Sec:    11492, Lr: 0.000300\n",
            "2021-07-22 12:01:34,778 - INFO - joeynmt.training - Epoch   3, Step:   241400, Batch Loss:     1.871593, Tokens per Sec:    11581, Lr: 0.000300\n",
            "2021-07-22 12:02:12,691 - INFO - joeynmt.training - Epoch   3, Step:   241600, Batch Loss:     1.813050, Tokens per Sec:    11597, Lr: 0.000300\n",
            "2021-07-22 12:02:50,681 - INFO - joeynmt.training - Epoch   3, Step:   241800, Batch Loss:     1.787844, Tokens per Sec:    11625, Lr: 0.000300\n",
            "2021-07-22 12:03:28,606 - INFO - joeynmt.training - Epoch   3, Step:   242000, Batch Loss:     1.504748, Tokens per Sec:    11488, Lr: 0.000300\n",
            "2021-07-22 12:04:06,430 - INFO - joeynmt.training - Epoch   3, Step:   242200, Batch Loss:     1.797738, Tokens per Sec:    11561, Lr: 0.000300\n",
            "2021-07-22 12:04:44,434 - INFO - joeynmt.training - Epoch   3, Step:   242400, Batch Loss:     1.803206, Tokens per Sec:    11617, Lr: 0.000300\n",
            "2021-07-22 12:05:17,353 - INFO - joeynmt.training - Epoch   3: total training loss 15071.20\n",
            "2021-07-22 12:05:17,353 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-22 12:05:23,077 - INFO - joeynmt.training - Epoch   4, Step:   242600, Batch Loss:     1.943365, Tokens per Sec:     9136, Lr: 0.000300\n",
            "2021-07-22 12:06:00,885 - INFO - joeynmt.training - Epoch   4, Step:   242800, Batch Loss:     1.639449, Tokens per Sec:    11309, Lr: 0.000300\n",
            "2021-07-22 12:06:39,016 - INFO - joeynmt.training - Epoch   4, Step:   243000, Batch Loss:     1.816294, Tokens per Sec:    11531, Lr: 0.000300\n",
            "2021-07-22 12:07:17,212 - INFO - joeynmt.training - Epoch   4, Step:   243200, Batch Loss:     1.802520, Tokens per Sec:    11458, Lr: 0.000300\n",
            "2021-07-22 12:07:54,550 - INFO - joeynmt.training - Epoch   4, Step:   243400, Batch Loss:     1.781884, Tokens per Sec:    11276, Lr: 0.000300\n",
            "2021-07-22 12:08:32,558 - INFO - joeynmt.training - Epoch   4, Step:   243600, Batch Loss:     1.742679, Tokens per Sec:    11494, Lr: 0.000300\n",
            "2021-07-22 12:09:10,507 - INFO - joeynmt.training - Epoch   4, Step:   243800, Batch Loss:     1.655298, Tokens per Sec:    11514, Lr: 0.000300\n",
            "2021-07-22 12:09:48,237 - INFO - joeynmt.training - Epoch   4, Step:   244000, Batch Loss:     1.746608, Tokens per Sec:    11570, Lr: 0.000300\n",
            "2021-07-22 12:10:25,698 - INFO - joeynmt.training - Epoch   4, Step:   244200, Batch Loss:     1.632640, Tokens per Sec:    11280, Lr: 0.000300\n",
            "2021-07-22 12:11:03,250 - INFO - joeynmt.training - Epoch   4, Step:   244400, Batch Loss:     2.035437, Tokens per Sec:    11360, Lr: 0.000300\n",
            "2021-07-22 12:11:41,142 - INFO - joeynmt.training - Epoch   4, Step:   244600, Batch Loss:     1.877900, Tokens per Sec:    11470, Lr: 0.000300\n",
            "2021-07-22 12:12:18,700 - INFO - joeynmt.training - Epoch   4, Step:   244800, Batch Loss:     1.836394, Tokens per Sec:    11302, Lr: 0.000300\n",
            "2021-07-22 12:12:56,375 - INFO - joeynmt.training - Epoch   4, Step:   245000, Batch Loss:     1.826137, Tokens per Sec:    11449, Lr: 0.000300\n",
            "2021-07-22 12:14:37,841 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 12:14:37,841 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 12:14:37,842 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 12:14:39,914 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 12:14:39,915 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 12:14:39,915 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 12:14:39,915 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 12:14:39,916 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 12:14:39,916 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 12:14:39,916 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 12:14:39,916 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he listened to the voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 12:14:39,916 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 12:14:39,917 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 12:14:39,917 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 12:14:39,917 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-22 12:14:39,917 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 12:14:39,918 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 12:14:39,918 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 12:14:39,918 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 12:14:39,918 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   245000: bleu:  19.19, loss: 176802.5312, ppl:   7.1427, duration: 103.5433s\n",
            "2021-07-22 12:15:18,157 - INFO - joeynmt.training - Epoch   4, Step:   245200, Batch Loss:     1.788471, Tokens per Sec:    11504, Lr: 0.000300\n",
            "2021-07-22 12:15:56,007 - INFO - joeynmt.training - Epoch   4, Step:   245400, Batch Loss:     1.820503, Tokens per Sec:    11420, Lr: 0.000300\n",
            "2021-07-22 12:16:33,750 - INFO - joeynmt.training - Epoch   4, Step:   245600, Batch Loss:     2.045916, Tokens per Sec:    11431, Lr: 0.000300\n",
            "2021-07-22 12:17:11,638 - INFO - joeynmt.training - Epoch   4, Step:   245800, Batch Loss:     1.990193, Tokens per Sec:    11559, Lr: 0.000300\n",
            "2021-07-22 12:17:49,743 - INFO - joeynmt.training - Epoch   4, Step:   246000, Batch Loss:     1.782934, Tokens per Sec:    11564, Lr: 0.000300\n",
            "2021-07-22 12:18:27,842 - INFO - joeynmt.training - Epoch   4, Step:   246200, Batch Loss:     1.724529, Tokens per Sec:    11504, Lr: 0.000300\n",
            "2021-07-22 12:19:05,706 - INFO - joeynmt.training - Epoch   4, Step:   246400, Batch Loss:     1.775496, Tokens per Sec:    11526, Lr: 0.000300\n",
            "2021-07-22 12:19:43,532 - INFO - joeynmt.training - Epoch   4, Step:   246600, Batch Loss:     1.883387, Tokens per Sec:    11387, Lr: 0.000300\n",
            "2021-07-22 12:20:21,512 - INFO - joeynmt.training - Epoch   4, Step:   246800, Batch Loss:     1.924245, Tokens per Sec:    11458, Lr: 0.000300\n",
            "2021-07-22 12:20:59,123 - INFO - joeynmt.training - Epoch   4, Step:   247000, Batch Loss:     1.757119, Tokens per Sec:    11497, Lr: 0.000300\n",
            "2021-07-22 12:21:36,997 - INFO - joeynmt.training - Epoch   4, Step:   247200, Batch Loss:     1.785273, Tokens per Sec:    11547, Lr: 0.000300\n",
            "2021-07-22 12:22:14,777 - INFO - joeynmt.training - Epoch   4, Step:   247400, Batch Loss:     1.886057, Tokens per Sec:    11286, Lr: 0.000300\n",
            "2021-07-22 12:22:52,963 - INFO - joeynmt.training - Epoch   4, Step:   247600, Batch Loss:     1.779622, Tokens per Sec:    11564, Lr: 0.000300\n",
            "2021-07-22 12:23:30,687 - INFO - joeynmt.training - Epoch   4, Step:   247800, Batch Loss:     1.704783, Tokens per Sec:    11260, Lr: 0.000300\n",
            "2021-07-22 12:24:08,714 - INFO - joeynmt.training - Epoch   4, Step:   248000, Batch Loss:     1.547980, Tokens per Sec:    11596, Lr: 0.000300\n",
            "2021-07-22 12:24:46,739 - INFO - joeynmt.training - Epoch   4, Step:   248200, Batch Loss:     1.872016, Tokens per Sec:    11488, Lr: 0.000300\n",
            "2021-07-22 12:25:24,774 - INFO - joeynmt.training - Epoch   4, Step:   248400, Batch Loss:     1.722243, Tokens per Sec:    11402, Lr: 0.000300\n",
            "2021-07-22 12:26:02,633 - INFO - joeynmt.training - Epoch   4, Step:   248600, Batch Loss:     1.700782, Tokens per Sec:    11545, Lr: 0.000300\n",
            "2021-07-22 12:26:40,646 - INFO - joeynmt.training - Epoch   4, Step:   248800, Batch Loss:     1.816803, Tokens per Sec:    11316, Lr: 0.000300\n",
            "2021-07-22 12:27:18,784 - INFO - joeynmt.training - Epoch   4, Step:   249000, Batch Loss:     1.654459, Tokens per Sec:    11439, Lr: 0.000300\n",
            "2021-07-22 12:27:56,855 - INFO - joeynmt.training - Epoch   4, Step:   249200, Batch Loss:     1.934227, Tokens per Sec:    11307, Lr: 0.000300\n",
            "2021-07-22 12:28:35,213 - INFO - joeynmt.training - Epoch   4, Step:   249400, Batch Loss:     1.835212, Tokens per Sec:    11341, Lr: 0.000300\n",
            "2021-07-22 12:29:13,467 - INFO - joeynmt.training - Epoch   4, Step:   249600, Batch Loss:     1.934903, Tokens per Sec:    11499, Lr: 0.000300\n",
            "2021-07-22 12:29:51,712 - INFO - joeynmt.training - Epoch   4, Step:   249800, Batch Loss:     1.596747, Tokens per Sec:    11320, Lr: 0.000300\n",
            "2021-07-22 12:30:30,167 - INFO - joeynmt.training - Epoch   4, Step:   250000, Batch Loss:     1.769513, Tokens per Sec:    11484, Lr: 0.000300\n",
            "2021-07-22 12:32:16,214 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 12:32:16,215 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 12:32:16,215 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 12:32:17,281 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 12:32:17,282 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 12:32:18,283 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 12:32:18,284 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 12:32:18,284 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 12:32:18,284 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer patiently , that is acceptable to God . ”\n",
            "2021-07-22 12:32:18,284 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 12:32:18,285 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 12:32:18,285 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 12:32:18,285 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 12:32:18,285 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 12:32:18,286 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 12:32:18,286 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 12:32:18,286 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 12:32:18,286 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 12:32:18,287 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 12:32:18,287 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 12:32:18,287 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to heal his family .\n",
            "2021-07-22 12:32:18,287 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   250000: bleu:  19.35, loss: 174842.9688, ppl:   6.9887, duration: 108.1196s\n",
            "2021-07-22 12:32:56,773 - INFO - joeynmt.training - Epoch   4, Step:   250200, Batch Loss:     1.764737, Tokens per Sec:    11257, Lr: 0.000300\n",
            "2021-07-22 12:33:35,095 - INFO - joeynmt.training - Epoch   4, Step:   250400, Batch Loss:     1.930019, Tokens per Sec:    11300, Lr: 0.000300\n",
            "2021-07-22 12:34:13,484 - INFO - joeynmt.training - Epoch   4, Step:   250600, Batch Loss:     1.726048, Tokens per Sec:    11433, Lr: 0.000300\n",
            "2021-07-22 12:34:51,457 - INFO - joeynmt.training - Epoch   4, Step:   250800, Batch Loss:     2.001559, Tokens per Sec:    11291, Lr: 0.000300\n",
            "2021-07-22 12:35:21,146 - INFO - joeynmt.training - Epoch   4: total training loss 15045.47\n",
            "2021-07-22 12:35:21,146 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-22 12:35:30,601 - INFO - joeynmt.training - Epoch   5, Step:   251000, Batch Loss:     1.817905, Tokens per Sec:     9761, Lr: 0.000300\n",
            "2021-07-22 12:36:08,893 - INFO - joeynmt.training - Epoch   5, Step:   251200, Batch Loss:     2.039533, Tokens per Sec:    11461, Lr: 0.000300\n",
            "2021-07-22 12:36:46,914 - INFO - joeynmt.training - Epoch   5, Step:   251400, Batch Loss:     1.685375, Tokens per Sec:    11292, Lr: 0.000300\n",
            "2021-07-22 12:37:24,959 - INFO - joeynmt.training - Epoch   5, Step:   251600, Batch Loss:     1.788464, Tokens per Sec:    11369, Lr: 0.000300\n",
            "2021-07-22 12:38:03,479 - INFO - joeynmt.training - Epoch   5, Step:   251800, Batch Loss:     1.792196, Tokens per Sec:    11477, Lr: 0.000300\n",
            "2021-07-22 12:38:41,536 - INFO - joeynmt.training - Epoch   5, Step:   252000, Batch Loss:     1.916346, Tokens per Sec:    11372, Lr: 0.000300\n",
            "2021-07-22 12:39:19,629 - INFO - joeynmt.training - Epoch   5, Step:   252200, Batch Loss:     1.801448, Tokens per Sec:    11323, Lr: 0.000300\n",
            "2021-07-22 12:39:57,907 - INFO - joeynmt.training - Epoch   5, Step:   252400, Batch Loss:     1.648108, Tokens per Sec:    11329, Lr: 0.000300\n",
            "2021-07-22 12:40:35,945 - INFO - joeynmt.training - Epoch   5, Step:   252600, Batch Loss:     2.196432, Tokens per Sec:    11343, Lr: 0.000300\n",
            "2021-07-22 12:41:14,074 - INFO - joeynmt.training - Epoch   5, Step:   252800, Batch Loss:     1.851251, Tokens per Sec:    11373, Lr: 0.000300\n",
            "2021-07-22 12:41:51,985 - INFO - joeynmt.training - Epoch   5, Step:   253000, Batch Loss:     1.742819, Tokens per Sec:    11462, Lr: 0.000300\n",
            "2021-07-22 12:42:29,656 - INFO - joeynmt.training - Epoch   5, Step:   253200, Batch Loss:     1.642727, Tokens per Sec:    11421, Lr: 0.000300\n",
            "2021-07-22 12:43:07,524 - INFO - joeynmt.training - Epoch   5, Step:   253400, Batch Loss:     1.761334, Tokens per Sec:    11396, Lr: 0.000300\n",
            "2021-07-22 12:43:45,623 - INFO - joeynmt.training - Epoch   5, Step:   253600, Batch Loss:     1.713413, Tokens per Sec:    11427, Lr: 0.000300\n",
            "2021-07-22 12:44:23,679 - INFO - joeynmt.training - Epoch   5, Step:   253800, Batch Loss:     1.749116, Tokens per Sec:    11452, Lr: 0.000300\n",
            "2021-07-22 12:45:01,345 - INFO - joeynmt.training - Epoch   5, Step:   254000, Batch Loss:     1.684671, Tokens per Sec:    11511, Lr: 0.000300\n",
            "2021-07-22 12:45:39,186 - INFO - joeynmt.training - Epoch   5, Step:   254200, Batch Loss:     1.556537, Tokens per Sec:    11550, Lr: 0.000300\n",
            "2021-07-22 12:46:17,144 - INFO - joeynmt.training - Epoch   5, Step:   254400, Batch Loss:     1.759286, Tokens per Sec:    11445, Lr: 0.000300\n",
            "2021-07-22 12:46:54,870 - INFO - joeynmt.training - Epoch   5, Step:   254600, Batch Loss:     1.868843, Tokens per Sec:    11425, Lr: 0.000300\n",
            "2021-07-22 12:47:32,893 - INFO - joeynmt.training - Epoch   5, Step:   254800, Batch Loss:     1.696946, Tokens per Sec:    11490, Lr: 0.000300\n",
            "2021-07-22 12:48:10,288 - INFO - joeynmt.training - Epoch   5, Step:   255000, Batch Loss:     1.812610, Tokens per Sec:    11320, Lr: 0.000300\n",
            "2021-07-22 12:50:00,976 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 12:50:00,977 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 12:50:00,977 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 12:50:02,849 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 12:50:02,850 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 12:50:02,850 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 12:50:02,850 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 12:50:02,851 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 12:50:02,851 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 12:50:02,851 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 12:50:02,852 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 12:50:02,852 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 12:50:02,852 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 12:50:02,853 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 12:50:02,854 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 12:50:02,854 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 12:50:02,855 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 12:50:02,855 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 12:50:02,855 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 12:50:02,856 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   255000: bleu:  19.30, loss: 176045.6094, ppl:   7.0828, duration: 112.5672s\n",
            "2021-07-22 12:50:40,622 - INFO - joeynmt.training - Epoch   5, Step:   255200, Batch Loss:     1.780364, Tokens per Sec:    11270, Lr: 0.000300\n",
            "2021-07-22 12:51:18,661 - INFO - joeynmt.training - Epoch   5, Step:   255400, Batch Loss:     1.675430, Tokens per Sec:    11325, Lr: 0.000300\n",
            "2021-07-22 12:51:56,550 - INFO - joeynmt.training - Epoch   5, Step:   255600, Batch Loss:     1.771081, Tokens per Sec:    11533, Lr: 0.000300\n",
            "2021-07-22 12:52:34,819 - INFO - joeynmt.training - Epoch   5, Step:   255800, Batch Loss:     1.690135, Tokens per Sec:    11526, Lr: 0.000300\n",
            "2021-07-22 12:53:12,449 - INFO - joeynmt.training - Epoch   5, Step:   256000, Batch Loss:     1.894503, Tokens per Sec:    11392, Lr: 0.000300\n",
            "2021-07-22 12:53:50,515 - INFO - joeynmt.training - Epoch   5, Step:   256200, Batch Loss:     1.446183, Tokens per Sec:    11567, Lr: 0.000300\n",
            "2021-07-22 12:54:28,506 - INFO - joeynmt.training - Epoch   5, Step:   256400, Batch Loss:     1.668502, Tokens per Sec:    11493, Lr: 0.000300\n",
            "2021-07-22 12:55:06,473 - INFO - joeynmt.training - Epoch   5, Step:   256600, Batch Loss:     1.699964, Tokens per Sec:    11521, Lr: 0.000300\n",
            "2021-07-22 12:55:44,255 - INFO - joeynmt.training - Epoch   5, Step:   256800, Batch Loss:     1.728142, Tokens per Sec:    11611, Lr: 0.000300\n",
            "2021-07-22 12:56:21,978 - INFO - joeynmt.training - Epoch   5, Step:   257000, Batch Loss:     1.902832, Tokens per Sec:    11437, Lr: 0.000300\n",
            "2021-07-22 12:57:00,062 - INFO - joeynmt.training - Epoch   5, Step:   257200, Batch Loss:     1.936749, Tokens per Sec:    11573, Lr: 0.000300\n",
            "2021-07-22 12:57:37,784 - INFO - joeynmt.training - Epoch   5, Step:   257400, Batch Loss:     1.866749, Tokens per Sec:    11416, Lr: 0.000300\n",
            "2021-07-22 12:58:15,651 - INFO - joeynmt.training - Epoch   5, Step:   257600, Batch Loss:     1.813941, Tokens per Sec:    11520, Lr: 0.000300\n",
            "2021-07-22 12:58:53,420 - INFO - joeynmt.training - Epoch   5, Step:   257800, Batch Loss:     1.748883, Tokens per Sec:    11560, Lr: 0.000300\n",
            "2021-07-22 12:59:31,446 - INFO - joeynmt.training - Epoch   5, Step:   258000, Batch Loss:     1.805568, Tokens per Sec:    11471, Lr: 0.000300\n",
            "2021-07-22 13:00:09,676 - INFO - joeynmt.training - Epoch   5, Step:   258200, Batch Loss:     1.544833, Tokens per Sec:    11497, Lr: 0.000300\n",
            "2021-07-22 13:00:47,370 - INFO - joeynmt.training - Epoch   5, Step:   258400, Batch Loss:     1.708205, Tokens per Sec:    11397, Lr: 0.000300\n",
            "2021-07-22 13:01:25,382 - INFO - joeynmt.training - Epoch   5, Step:   258600, Batch Loss:     1.909428, Tokens per Sec:    11480, Lr: 0.000300\n",
            "2021-07-22 13:02:03,622 - INFO - joeynmt.training - Epoch   5, Step:   258800, Batch Loss:     1.761011, Tokens per Sec:    11685, Lr: 0.000300\n",
            "2021-07-22 13:02:41,816 - INFO - joeynmt.training - Epoch   5, Step:   259000, Batch Loss:     1.524320, Tokens per Sec:    11617, Lr: 0.000300\n",
            "2021-07-22 13:03:19,944 - INFO - joeynmt.training - Epoch   5, Step:   259200, Batch Loss:     1.671522, Tokens per Sec:    11502, Lr: 0.000300\n",
            "2021-07-22 13:03:41,733 - INFO - joeynmt.training - Epoch   5: total training loss 14974.68\n",
            "2021-07-22 13:03:41,733 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-22 13:03:59,007 - INFO - joeynmt.training - Epoch   6, Step:   259400, Batch Loss:     1.789127, Tokens per Sec:    10758, Lr: 0.000300\n",
            "2021-07-22 13:04:37,025 - INFO - joeynmt.training - Epoch   6, Step:   259600, Batch Loss:     1.698020, Tokens per Sec:    11431, Lr: 0.000300\n",
            "2021-07-22 13:05:15,072 - INFO - joeynmt.training - Epoch   6, Step:   259800, Batch Loss:     2.021674, Tokens per Sec:    11568, Lr: 0.000300\n",
            "2021-07-22 13:05:52,925 - INFO - joeynmt.training - Epoch   6, Step:   260000, Batch Loss:     1.971643, Tokens per Sec:    11365, Lr: 0.000300\n",
            "2021-07-22 13:07:32,418 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 13:07:32,419 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 13:07:32,419 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 13:07:33,426 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 13:07:33,427 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 13:07:34,344 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 13:07:34,345 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 13:07:34,345 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 13:07:34,345 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-22 13:07:34,345 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 13:07:34,346 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 13:07:34,346 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 13:07:34,346 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 13:07:34,346 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 13:07:34,347 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 13:07:34,347 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 13:07:34,347 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-22 13:07:34,347 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 13:07:34,348 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 13:07:34,348 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 13:07:34,348 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to heal his family .\n",
            "2021-07-22 13:07:34,348 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   260000: bleu:  19.41, loss: 174760.9531, ppl:   6.9824, duration: 101.4230s\n",
            "2021-07-22 13:08:12,289 - INFO - joeynmt.training - Epoch   6, Step:   260200, Batch Loss:     2.219304, Tokens per Sec:    11192, Lr: 0.000300\n",
            "2021-07-22 13:08:50,269 - INFO - joeynmt.training - Epoch   6, Step:   260400, Batch Loss:     1.537980, Tokens per Sec:    11446, Lr: 0.000300\n",
            "2021-07-22 13:09:28,140 - INFO - joeynmt.training - Epoch   6, Step:   260600, Batch Loss:     1.723193, Tokens per Sec:    11484, Lr: 0.000300\n",
            "2021-07-22 13:10:06,172 - INFO - joeynmt.training - Epoch   6, Step:   260800, Batch Loss:     1.544006, Tokens per Sec:    11546, Lr: 0.000300\n",
            "2021-07-22 13:10:43,847 - INFO - joeynmt.training - Epoch   6, Step:   261000, Batch Loss:     1.739728, Tokens per Sec:    11335, Lr: 0.000300\n",
            "2021-07-22 13:11:21,924 - INFO - joeynmt.training - Epoch   6, Step:   261200, Batch Loss:     1.894162, Tokens per Sec:    11513, Lr: 0.000300\n",
            "2021-07-22 13:11:59,936 - INFO - joeynmt.training - Epoch   6, Step:   261400, Batch Loss:     1.815365, Tokens per Sec:    11625, Lr: 0.000300\n",
            "2021-07-22 13:12:37,582 - INFO - joeynmt.training - Epoch   6, Step:   261600, Batch Loss:     1.774443, Tokens per Sec:    11365, Lr: 0.000300\n",
            "2021-07-22 13:13:15,311 - INFO - joeynmt.training - Epoch   6, Step:   261800, Batch Loss:     1.698605, Tokens per Sec:    11455, Lr: 0.000300\n",
            "2021-07-22 13:13:53,396 - INFO - joeynmt.training - Epoch   6, Step:   262000, Batch Loss:     1.695610, Tokens per Sec:    11574, Lr: 0.000300\n",
            "2021-07-22 13:14:31,380 - INFO - joeynmt.training - Epoch   6, Step:   262200, Batch Loss:     1.661408, Tokens per Sec:    11500, Lr: 0.000300\n",
            "2021-07-22 13:15:09,370 - INFO - joeynmt.training - Epoch   6, Step:   262400, Batch Loss:     1.799635, Tokens per Sec:    11571, Lr: 0.000300\n",
            "2021-07-22 13:15:47,398 - INFO - joeynmt.training - Epoch   6, Step:   262600, Batch Loss:     1.978226, Tokens per Sec:    11501, Lr: 0.000300\n",
            "2021-07-22 13:16:25,180 - INFO - joeynmt.training - Epoch   6, Step:   262800, Batch Loss:     1.723361, Tokens per Sec:    11492, Lr: 0.000300\n",
            "2021-07-22 13:17:02,852 - INFO - joeynmt.training - Epoch   6, Step:   263000, Batch Loss:     1.773881, Tokens per Sec:    11432, Lr: 0.000300\n",
            "2021-07-22 13:17:40,764 - INFO - joeynmt.training - Epoch   6, Step:   263200, Batch Loss:     1.753462, Tokens per Sec:    11444, Lr: 0.000300\n",
            "2021-07-22 13:18:18,429 - INFO - joeynmt.training - Epoch   6, Step:   263400, Batch Loss:     1.905354, Tokens per Sec:    11367, Lr: 0.000300\n",
            "2021-07-22 13:18:56,488 - INFO - joeynmt.training - Epoch   6, Step:   263600, Batch Loss:     1.758078, Tokens per Sec:    11541, Lr: 0.000300\n",
            "2021-07-22 13:19:34,678 - INFO - joeynmt.training - Epoch   6, Step:   263800, Batch Loss:     1.606215, Tokens per Sec:    11485, Lr: 0.000300\n",
            "2021-07-22 13:20:12,515 - INFO - joeynmt.training - Epoch   6, Step:   264000, Batch Loss:     1.857471, Tokens per Sec:    11455, Lr: 0.000300\n",
            "2021-07-22 13:20:50,488 - INFO - joeynmt.training - Epoch   6, Step:   264200, Batch Loss:     1.754544, Tokens per Sec:    11446, Lr: 0.000300\n",
            "2021-07-22 13:21:28,422 - INFO - joeynmt.training - Epoch   6, Step:   264400, Batch Loss:     1.761935, Tokens per Sec:    11462, Lr: 0.000300\n",
            "2021-07-22 13:22:06,578 - INFO - joeynmt.training - Epoch   6, Step:   264600, Batch Loss:     1.841651, Tokens per Sec:    11489, Lr: 0.000300\n",
            "2021-07-22 13:22:44,771 - INFO - joeynmt.training - Epoch   6, Step:   264800, Batch Loss:     1.966191, Tokens per Sec:    11406, Lr: 0.000300\n",
            "2021-07-22 13:23:22,705 - INFO - joeynmt.training - Epoch   6, Step:   265000, Batch Loss:     1.763483, Tokens per Sec:    11365, Lr: 0.000300\n",
            "2021-07-22 13:25:07,016 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 13:25:07,016 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 13:25:07,016 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 13:25:08,055 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 13:25:08,055 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 13:25:08,842 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 13:25:08,843 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 13:25:08,843 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 13:25:08,843 - INFO - joeynmt.training - \tHypothesis: “ When you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 13:25:08,843 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 13:25:08,844 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 13:25:08,844 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 13:25:08,844 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 13:25:08,845 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 13:25:08,845 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 13:25:08,845 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 13:25:08,845 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 13:25:08,845 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 13:25:08,846 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 13:25:08,846 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 13:25:08,846 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to heal his family .\n",
            "2021-07-22 13:25:08,847 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   265000: bleu:  19.54, loss: 174730.6719, ppl:   6.9800, duration: 106.1413s\n",
            "2021-07-22 13:25:47,206 - INFO - joeynmt.training - Epoch   6, Step:   265200, Batch Loss:     1.724399, Tokens per Sec:    11460, Lr: 0.000300\n",
            "2021-07-22 13:26:25,051 - INFO - joeynmt.training - Epoch   6, Step:   265400, Batch Loss:     1.762817, Tokens per Sec:    11427, Lr: 0.000300\n",
            "2021-07-22 13:27:03,059 - INFO - joeynmt.training - Epoch   6, Step:   265600, Batch Loss:     1.679640, Tokens per Sec:    11526, Lr: 0.000300\n",
            "2021-07-22 13:27:41,053 - INFO - joeynmt.training - Epoch   6, Step:   265800, Batch Loss:     1.891563, Tokens per Sec:    11467, Lr: 0.000300\n",
            "2021-07-22 13:28:18,761 - INFO - joeynmt.training - Epoch   6, Step:   266000, Batch Loss:     1.730671, Tokens per Sec:    11311, Lr: 0.000300\n",
            "2021-07-22 13:28:56,548 - INFO - joeynmt.training - Epoch   6, Step:   266200, Batch Loss:     2.038025, Tokens per Sec:    11217, Lr: 0.000300\n",
            "2021-07-22 13:29:34,243 - INFO - joeynmt.training - Epoch   6, Step:   266400, Batch Loss:     1.690242, Tokens per Sec:    11138, Lr: 0.000300\n",
            "2021-07-22 13:30:12,787 - INFO - joeynmt.training - Epoch   6, Step:   266600, Batch Loss:     1.616210, Tokens per Sec:    11458, Lr: 0.000300\n",
            "2021-07-22 13:30:51,108 - INFO - joeynmt.training - Epoch   6, Step:   266800, Batch Loss:     1.781516, Tokens per Sec:    11504, Lr: 0.000300\n",
            "2021-07-22 13:31:29,653 - INFO - joeynmt.training - Epoch   6, Step:   267000, Batch Loss:     1.924235, Tokens per Sec:    11488, Lr: 0.000300\n",
            "2021-07-22 13:32:08,015 - INFO - joeynmt.training - Epoch   6, Step:   267200, Batch Loss:     1.797174, Tokens per Sec:    11269, Lr: 0.000300\n",
            "2021-07-22 13:32:46,216 - INFO - joeynmt.training - Epoch   6, Step:   267400, Batch Loss:     1.767490, Tokens per Sec:    11402, Lr: 0.000300\n",
            "2021-07-22 13:33:23,972 - INFO - joeynmt.training - Epoch   6, Step:   267600, Batch Loss:     1.700487, Tokens per Sec:    11214, Lr: 0.000300\n",
            "2021-07-22 13:33:40,197 - INFO - joeynmt.training - Epoch   6: total training loss 14957.42\n",
            "2021-07-22 13:33:40,198 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-22 13:34:03,192 - INFO - joeynmt.training - Epoch   7, Step:   267800, Batch Loss:     1.996306, Tokens per Sec:    10675, Lr: 0.000300\n",
            "2021-07-22 13:34:41,550 - INFO - joeynmt.training - Epoch   7, Step:   268000, Batch Loss:     1.763109, Tokens per Sec:    11358, Lr: 0.000300\n",
            "2021-07-22 13:35:19,736 - INFO - joeynmt.training - Epoch   7, Step:   268200, Batch Loss:     1.863338, Tokens per Sec:    11360, Lr: 0.000300\n",
            "2021-07-22 13:35:58,180 - INFO - joeynmt.training - Epoch   7, Step:   268400, Batch Loss:     1.820708, Tokens per Sec:    11461, Lr: 0.000300\n",
            "2021-07-22 13:36:36,436 - INFO - joeynmt.training - Epoch   7, Step:   268600, Batch Loss:     1.875271, Tokens per Sec:    11367, Lr: 0.000300\n",
            "2021-07-22 13:37:14,590 - INFO - joeynmt.training - Epoch   7, Step:   268800, Batch Loss:     1.602498, Tokens per Sec:    11371, Lr: 0.000300\n",
            "2021-07-22 13:37:52,862 - INFO - joeynmt.training - Epoch   7, Step:   269000, Batch Loss:     1.866400, Tokens per Sec:    11515, Lr: 0.000300\n",
            "2021-07-22 13:38:31,222 - INFO - joeynmt.training - Epoch   7, Step:   269200, Batch Loss:     1.759356, Tokens per Sec:    11205, Lr: 0.000300\n",
            "2021-07-22 13:39:09,297 - INFO - joeynmt.training - Epoch   7, Step:   269400, Batch Loss:     2.032646, Tokens per Sec:    11355, Lr: 0.000300\n",
            "2021-07-22 13:39:47,365 - INFO - joeynmt.training - Epoch   7, Step:   269600, Batch Loss:     1.909899, Tokens per Sec:    11417, Lr: 0.000300\n",
            "2021-07-22 13:40:25,649 - INFO - joeynmt.training - Epoch   7, Step:   269800, Batch Loss:     1.689970, Tokens per Sec:    11293, Lr: 0.000300\n",
            "2021-07-22 13:41:04,202 - INFO - joeynmt.training - Epoch   7, Step:   270000, Batch Loss:     1.783059, Tokens per Sec:    11484, Lr: 0.000300\n",
            "2021-07-22 13:42:50,078 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 13:42:50,078 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 13:42:50,078 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 13:42:51,105 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 13:42:51,106 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 13:42:51,944 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 13:42:51,945 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 13:42:51,947 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 13:42:51,947 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 13:42:51,947 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 13:42:51,947 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 13:42:51,948 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 13:42:51,948 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus listened to the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 13:42:51,948 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 13:42:51,949 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 13:42:51,949 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 13:42:51,949 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 13:42:51,949 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 13:42:51,950 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 13:42:51,950 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 13:42:51,950 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to heal his family .\n",
            "2021-07-22 13:42:51,950 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   270000: bleu:  19.57, loss: 174549.0469, ppl:   6.9659, duration: 107.7481s\n",
            "2021-07-22 13:43:30,298 - INFO - joeynmt.training - Epoch   7, Step:   270200, Batch Loss:     1.860708, Tokens per Sec:    11278, Lr: 0.000300\n",
            "2021-07-22 13:44:08,423 - INFO - joeynmt.training - Epoch   7, Step:   270400, Batch Loss:     1.973804, Tokens per Sec:    11300, Lr: 0.000300\n",
            "2021-07-22 13:44:46,261 - INFO - joeynmt.training - Epoch   7, Step:   270600, Batch Loss:     1.957717, Tokens per Sec:    11264, Lr: 0.000300\n",
            "2021-07-22 13:45:24,266 - INFO - joeynmt.training - Epoch   7, Step:   270800, Batch Loss:     1.833723, Tokens per Sec:    11242, Lr: 0.000300\n",
            "2021-07-22 13:46:02,724 - INFO - joeynmt.training - Epoch   7, Step:   271000, Batch Loss:     1.813758, Tokens per Sec:    11474, Lr: 0.000300\n",
            "2021-07-22 13:46:41,064 - INFO - joeynmt.training - Epoch   7, Step:   271200, Batch Loss:     1.844837, Tokens per Sec:    11406, Lr: 0.000300\n",
            "2021-07-22 13:47:19,127 - INFO - joeynmt.training - Epoch   7, Step:   271400, Batch Loss:     1.595876, Tokens per Sec:    11351, Lr: 0.000300\n",
            "2021-07-22 13:47:57,389 - INFO - joeynmt.training - Epoch   7, Step:   271600, Batch Loss:     1.775145, Tokens per Sec:    11257, Lr: 0.000300\n",
            "2021-07-22 13:48:35,864 - INFO - joeynmt.training - Epoch   7, Step:   271800, Batch Loss:     1.787536, Tokens per Sec:    11415, Lr: 0.000300\n",
            "2021-07-22 13:49:14,331 - INFO - joeynmt.training - Epoch   7, Step:   272000, Batch Loss:     1.652500, Tokens per Sec:    11426, Lr: 0.000300\n",
            "2021-07-22 13:49:52,525 - INFO - joeynmt.training - Epoch   7, Step:   272200, Batch Loss:     1.786470, Tokens per Sec:    11290, Lr: 0.000300\n",
            "2021-07-22 13:50:30,671 - INFO - joeynmt.training - Epoch   7, Step:   272400, Batch Loss:     1.718359, Tokens per Sec:    11378, Lr: 0.000300\n",
            "2021-07-22 13:51:08,623 - INFO - joeynmt.training - Epoch   7, Step:   272600, Batch Loss:     1.814677, Tokens per Sec:    11349, Lr: 0.000300\n",
            "2021-07-22 13:51:46,756 - INFO - joeynmt.training - Epoch   7, Step:   272800, Batch Loss:     1.830116, Tokens per Sec:    11342, Lr: 0.000300\n",
            "2021-07-22 13:52:24,890 - INFO - joeynmt.training - Epoch   7, Step:   273000, Batch Loss:     1.843997, Tokens per Sec:    11298, Lr: 0.000300\n",
            "2021-07-22 13:53:03,559 - INFO - joeynmt.training - Epoch   7, Step:   273200, Batch Loss:     1.891664, Tokens per Sec:    11498, Lr: 0.000300\n",
            "2021-07-22 13:53:41,087 - INFO - joeynmt.training - Epoch   7, Step:   273400, Batch Loss:     1.872225, Tokens per Sec:    11300, Lr: 0.000300\n",
            "2021-07-22 13:54:18,830 - INFO - joeynmt.training - Epoch   7, Step:   273600, Batch Loss:     2.059786, Tokens per Sec:    11420, Lr: 0.000300\n",
            "2021-07-22 13:54:56,743 - INFO - joeynmt.training - Epoch   7, Step:   273800, Batch Loss:     1.595572, Tokens per Sec:    11463, Lr: 0.000300\n",
            "2021-07-22 13:55:34,608 - INFO - joeynmt.training - Epoch   7, Step:   274000, Batch Loss:     1.829212, Tokens per Sec:    11542, Lr: 0.000300\n",
            "2021-07-22 13:56:12,684 - INFO - joeynmt.training - Epoch   7, Step:   274200, Batch Loss:     1.789400, Tokens per Sec:    11553, Lr: 0.000300\n",
            "2021-07-22 13:56:50,448 - INFO - joeynmt.training - Epoch   7, Step:   274400, Batch Loss:     1.852722, Tokens per Sec:    11634, Lr: 0.000300\n",
            "2021-07-22 13:57:28,204 - INFO - joeynmt.training - Epoch   7, Step:   274600, Batch Loss:     1.776757, Tokens per Sec:    11406, Lr: 0.000300\n",
            "2021-07-22 13:58:05,639 - INFO - joeynmt.training - Epoch   7, Step:   274800, Batch Loss:     2.003685, Tokens per Sec:    11471, Lr: 0.000300\n",
            "2021-07-22 13:58:43,682 - INFO - joeynmt.training - Epoch   7, Step:   275000, Batch Loss:     1.632016, Tokens per Sec:    11427, Lr: 0.000300\n",
            "2021-07-22 14:00:23,973 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 14:00:23,974 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 14:00:23,974 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 14:00:24,973 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 14:00:24,974 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 14:00:25,764 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 14:00:25,766 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 14:00:25,766 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 14:00:25,766 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is God’s approval . ”\n",
            "2021-07-22 14:00:25,766 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 14:00:25,767 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 14:00:25,767 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 14:00:25,767 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 14:00:25,768 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 14:00:25,768 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 14:00:25,768 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 14:00:25,768 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 14:00:25,769 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 14:00:25,769 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 14:00:25,769 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 14:00:25,769 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-22 14:00:25,770 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   275000: bleu:  19.48, loss: 173322.1406, ppl:   6.8715, duration: 102.0875s\n",
            "2021-07-22 14:01:03,818 - INFO - joeynmt.training - Epoch   7, Step:   275200, Batch Loss:     1.780592, Tokens per Sec:    11550, Lr: 0.000300\n",
            "2021-07-22 14:01:41,814 - INFO - joeynmt.training - Epoch   7, Step:   275400, Batch Loss:     1.885650, Tokens per Sec:    11457, Lr: 0.000300\n",
            "2021-07-22 14:02:20,309 - INFO - joeynmt.training - Epoch   7, Step:   275600, Batch Loss:     1.869632, Tokens per Sec:    11557, Lr: 0.000300\n",
            "2021-07-22 14:02:58,063 - INFO - joeynmt.training - Epoch   7, Step:   275800, Batch Loss:     1.711007, Tokens per Sec:    11473, Lr: 0.000300\n",
            "2021-07-22 14:03:36,134 - INFO - joeynmt.training - Epoch   7, Step:   276000, Batch Loss:     1.914200, Tokens per Sec:    11496, Lr: 0.000300\n",
            "2021-07-22 14:03:45,380 - INFO - joeynmt.training - Epoch   7: total training loss 14901.46\n",
            "2021-07-22 14:03:45,381 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-22 14:04:15,184 - INFO - joeynmt.training - Epoch   8, Step:   276200, Batch Loss:     1.826891, Tokens per Sec:    11022, Lr: 0.000300\n",
            "2021-07-22 14:04:53,138 - INFO - joeynmt.training - Epoch   8, Step:   276400, Batch Loss:     1.800717, Tokens per Sec:    11394, Lr: 0.000300\n",
            "2021-07-22 14:05:31,409 - INFO - joeynmt.training - Epoch   8, Step:   276600, Batch Loss:     1.927919, Tokens per Sec:    11505, Lr: 0.000300\n",
            "2021-07-22 14:06:09,194 - INFO - joeynmt.training - Epoch   8, Step:   276800, Batch Loss:     1.846785, Tokens per Sec:    11473, Lr: 0.000300\n",
            "2021-07-22 14:06:46,992 - INFO - joeynmt.training - Epoch   8, Step:   277000, Batch Loss:     1.736681, Tokens per Sec:    11436, Lr: 0.000300\n",
            "2021-07-22 14:07:24,705 - INFO - joeynmt.training - Epoch   8, Step:   277200, Batch Loss:     1.476681, Tokens per Sec:    11502, Lr: 0.000300\n",
            "2021-07-22 14:08:02,722 - INFO - joeynmt.training - Epoch   8, Step:   277400, Batch Loss:     1.960521, Tokens per Sec:    11522, Lr: 0.000300\n",
            "2021-07-22 14:08:40,668 - INFO - joeynmt.training - Epoch   8, Step:   277600, Batch Loss:     1.911899, Tokens per Sec:    11443, Lr: 0.000300\n",
            "2021-07-22 14:09:18,447 - INFO - joeynmt.training - Epoch   8, Step:   277800, Batch Loss:     1.796274, Tokens per Sec:    11451, Lr: 0.000300\n",
            "2021-07-22 14:09:56,473 - INFO - joeynmt.training - Epoch   8, Step:   278000, Batch Loss:     1.712832, Tokens per Sec:    11505, Lr: 0.000300\n",
            "2021-07-22 14:10:34,647 - INFO - joeynmt.training - Epoch   8, Step:   278200, Batch Loss:     2.117544, Tokens per Sec:    11544, Lr: 0.000300\n",
            "2021-07-22 14:11:12,203 - INFO - joeynmt.training - Epoch   8, Step:   278400, Batch Loss:     1.691393, Tokens per Sec:    11315, Lr: 0.000300\n",
            "2021-07-22 14:11:50,069 - INFO - joeynmt.training - Epoch   8, Step:   278600, Batch Loss:     1.783483, Tokens per Sec:    11367, Lr: 0.000300\n",
            "2021-07-22 14:12:27,917 - INFO - joeynmt.training - Epoch   8, Step:   278800, Batch Loss:     1.547861, Tokens per Sec:    11273, Lr: 0.000300\n",
            "2021-07-22 14:13:05,801 - INFO - joeynmt.training - Epoch   8, Step:   279000, Batch Loss:     1.919826, Tokens per Sec:    11361, Lr: 0.000300\n",
            "2021-07-22 14:13:43,649 - INFO - joeynmt.training - Epoch   8, Step:   279200, Batch Loss:     1.780925, Tokens per Sec:    11354, Lr: 0.000300\n",
            "2021-07-22 14:14:21,649 - INFO - joeynmt.training - Epoch   8, Step:   279400, Batch Loss:     1.730086, Tokens per Sec:    11312, Lr: 0.000300\n",
            "2021-07-22 14:14:59,739 - INFO - joeynmt.training - Epoch   8, Step:   279600, Batch Loss:     1.853905, Tokens per Sec:    11540, Lr: 0.000300\n",
            "2021-07-22 14:15:37,948 - INFO - joeynmt.training - Epoch   8, Step:   279800, Batch Loss:     1.855519, Tokens per Sec:    11590, Lr: 0.000300\n",
            "2021-07-22 14:16:15,930 - INFO - joeynmt.training - Epoch   8, Step:   280000, Batch Loss:     1.756903, Tokens per Sec:    11425, Lr: 0.000300\n",
            "2021-07-22 14:17:58,164 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 14:17:58,164 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 14:17:58,164 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 14:17:59,954 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 14:17:59,955 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 14:17:59,955 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 14:17:59,955 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 14:17:59,955 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 14:17:59,956 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 14:17:59,956 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 14:17:59,956 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 14:17:59,956 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 14:17:59,957 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 14:17:59,957 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 14:17:59,957 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayer ?\n",
            "2021-07-22 14:17:59,958 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 14:17:59,958 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 14:17:59,958 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 14:17:59,958 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to heal his family .\n",
            "2021-07-22 14:17:59,959 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   280000: bleu:  19.76, loss: 173443.3125, ppl:   6.8808, duration: 104.0288s\n",
            "2021-07-22 14:18:38,086 - INFO - joeynmt.training - Epoch   8, Step:   280200, Batch Loss:     1.827429, Tokens per Sec:    11444, Lr: 0.000300\n",
            "2021-07-22 14:19:16,465 - INFO - joeynmt.training - Epoch   8, Step:   280400, Batch Loss:     1.853783, Tokens per Sec:    11542, Lr: 0.000300\n",
            "2021-07-22 14:19:54,422 - INFO - joeynmt.training - Epoch   8, Step:   280600, Batch Loss:     1.719094, Tokens per Sec:    11540, Lr: 0.000300\n",
            "2021-07-22 14:20:32,809 - INFO - joeynmt.training - Epoch   8, Step:   280800, Batch Loss:     1.872156, Tokens per Sec:    11531, Lr: 0.000300\n",
            "2021-07-22 14:21:10,353 - INFO - joeynmt.training - Epoch   8, Step:   281000, Batch Loss:     1.789229, Tokens per Sec:    11407, Lr: 0.000300\n",
            "2021-07-22 14:21:48,009 - INFO - joeynmt.training - Epoch   8, Step:   281200, Batch Loss:     1.705842, Tokens per Sec:    11337, Lr: 0.000300\n",
            "2021-07-22 14:22:26,130 - INFO - joeynmt.training - Epoch   8, Step:   281400, Batch Loss:     1.913046, Tokens per Sec:    11523, Lr: 0.000300\n",
            "2021-07-22 14:23:03,871 - INFO - joeynmt.training - Epoch   8, Step:   281600, Batch Loss:     1.545458, Tokens per Sec:    11340, Lr: 0.000300\n",
            "2021-07-22 14:23:41,840 - INFO - joeynmt.training - Epoch   8, Step:   281800, Batch Loss:     1.498487, Tokens per Sec:    11386, Lr: 0.000300\n",
            "2021-07-22 14:24:19,777 - INFO - joeynmt.training - Epoch   8, Step:   282000, Batch Loss:     1.848523, Tokens per Sec:    11368, Lr: 0.000300\n",
            "2021-07-22 14:24:57,643 - INFO - joeynmt.training - Epoch   8, Step:   282200, Batch Loss:     1.713168, Tokens per Sec:    11463, Lr: 0.000300\n",
            "2021-07-22 14:25:35,712 - INFO - joeynmt.training - Epoch   8, Step:   282400, Batch Loss:     1.758288, Tokens per Sec:    11484, Lr: 0.000300\n",
            "2021-07-22 14:26:13,610 - INFO - joeynmt.training - Epoch   8, Step:   282600, Batch Loss:     1.754084, Tokens per Sec:    11412, Lr: 0.000300\n",
            "2021-07-22 14:26:51,485 - INFO - joeynmt.training - Epoch   8, Step:   282800, Batch Loss:     1.878353, Tokens per Sec:    11532, Lr: 0.000300\n",
            "2021-07-22 14:27:29,644 - INFO - joeynmt.training - Epoch   8, Step:   283000, Batch Loss:     1.861628, Tokens per Sec:    11344, Lr: 0.000300\n",
            "2021-07-22 14:28:07,838 - INFO - joeynmt.training - Epoch   8, Step:   283200, Batch Loss:     1.830280, Tokens per Sec:    11407, Lr: 0.000300\n",
            "2021-07-22 14:28:45,960 - INFO - joeynmt.training - Epoch   8, Step:   283400, Batch Loss:     1.721708, Tokens per Sec:    11157, Lr: 0.000300\n",
            "2021-07-22 14:29:24,134 - INFO - joeynmt.training - Epoch   8, Step:   283600, Batch Loss:     1.632341, Tokens per Sec:    11322, Lr: 0.000300\n",
            "2021-07-22 14:30:02,662 - INFO - joeynmt.training - Epoch   8, Step:   283800, Batch Loss:     1.642565, Tokens per Sec:    11563, Lr: 0.000300\n",
            "2021-07-22 14:30:40,802 - INFO - joeynmt.training - Epoch   8, Step:   284000, Batch Loss:     1.696803, Tokens per Sec:    11329, Lr: 0.000300\n",
            "2021-07-22 14:31:18,992 - INFO - joeynmt.training - Epoch   8, Step:   284200, Batch Loss:     1.615243, Tokens per Sec:    11400, Lr: 0.000300\n",
            "2021-07-22 14:31:56,920 - INFO - joeynmt.training - Epoch   8, Step:   284400, Batch Loss:     1.820651, Tokens per Sec:    11279, Lr: 0.000300\n",
            "2021-07-22 14:32:01,280 - INFO - joeynmt.training - Epoch   8: total training loss 14880.26\n",
            "2021-07-22 14:32:01,281 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-22 14:32:36,612 - INFO - joeynmt.training - Epoch   9, Step:   284600, Batch Loss:     1.783882, Tokens per Sec:    11121, Lr: 0.000300\n",
            "2021-07-22 14:33:14,854 - INFO - joeynmt.training - Epoch   9, Step:   284800, Batch Loss:     1.627496, Tokens per Sec:    11411, Lr: 0.000300\n",
            "2021-07-22 14:33:53,014 - INFO - joeynmt.training - Epoch   9, Step:   285000, Batch Loss:     1.910340, Tokens per Sec:    11424, Lr: 0.000300\n",
            "2021-07-22 14:35:33,952 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 14:35:33,952 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 14:35:33,952 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 14:35:34,963 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 14:35:34,963 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 14:35:35,855 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 14:35:35,856 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 14:35:35,856 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 14:35:35,856 - INFO - joeynmt.training - \tHypothesis: “ If you do good and suffer , that is acceptable to God . ”\n",
            "2021-07-22 14:35:35,856 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 14:35:35,857 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 14:35:35,857 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 14:35:35,857 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-22 14:35:35,857 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 14:35:35,858 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 14:35:35,858 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 14:35:35,858 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-22 14:35:35,858 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 14:35:35,859 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 14:35:35,859 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 14:35:35,859 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-22 14:35:35,859 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   285000: bleu:  19.74, loss: 172925.5781, ppl:   6.8413, duration: 102.8451s\n",
            "2021-07-22 14:36:14,076 - INFO - joeynmt.training - Epoch   9, Step:   285200, Batch Loss:     1.998722, Tokens per Sec:    11376, Lr: 0.000300\n",
            "2021-07-22 14:36:52,239 - INFO - joeynmt.training - Epoch   9, Step:   285400, Batch Loss:     1.755602, Tokens per Sec:    11381, Lr: 0.000300\n",
            "2021-07-22 14:37:30,512 - INFO - joeynmt.training - Epoch   9, Step:   285600, Batch Loss:     1.741899, Tokens per Sec:    11446, Lr: 0.000300\n",
            "2021-07-22 14:38:08,674 - INFO - joeynmt.training - Epoch   9, Step:   285800, Batch Loss:     1.864711, Tokens per Sec:    11344, Lr: 0.000300\n",
            "2021-07-22 14:38:46,900 - INFO - joeynmt.training - Epoch   9, Step:   286000, Batch Loss:     1.844294, Tokens per Sec:    11338, Lr: 0.000300\n",
            "2021-07-22 14:39:25,153 - INFO - joeynmt.training - Epoch   9, Step:   286200, Batch Loss:     1.760174, Tokens per Sec:    11377, Lr: 0.000300\n",
            "2021-07-22 14:40:03,181 - INFO - joeynmt.training - Epoch   9, Step:   286400, Batch Loss:     1.895733, Tokens per Sec:    11391, Lr: 0.000300\n",
            "2021-07-22 14:40:41,094 - INFO - joeynmt.training - Epoch   9, Step:   286600, Batch Loss:     1.848095, Tokens per Sec:    11315, Lr: 0.000300\n",
            "2021-07-22 14:41:19,393 - INFO - joeynmt.training - Epoch   9, Step:   286800, Batch Loss:     1.810245, Tokens per Sec:    11366, Lr: 0.000300\n",
            "2021-07-22 14:41:57,489 - INFO - joeynmt.training - Epoch   9, Step:   287000, Batch Loss:     1.867702, Tokens per Sec:    11295, Lr: 0.000300\n",
            "2021-07-22 14:42:35,465 - INFO - joeynmt.training - Epoch   9, Step:   287200, Batch Loss:     1.746566, Tokens per Sec:    11355, Lr: 0.000300\n",
            "2021-07-22 14:43:13,512 - INFO - joeynmt.training - Epoch   9, Step:   287400, Batch Loss:     1.699254, Tokens per Sec:    11239, Lr: 0.000300\n",
            "2021-07-22 14:43:51,878 - INFO - joeynmt.training - Epoch   9, Step:   287600, Batch Loss:     1.767940, Tokens per Sec:    11430, Lr: 0.000300\n",
            "2021-07-22 14:44:29,423 - INFO - joeynmt.training - Epoch   9, Step:   287800, Batch Loss:     1.918015, Tokens per Sec:    11146, Lr: 0.000300\n",
            "2021-07-22 14:45:08,093 - INFO - joeynmt.training - Epoch   9, Step:   288000, Batch Loss:     1.724064, Tokens per Sec:    11554, Lr: 0.000300\n",
            "2021-07-22 14:45:46,390 - INFO - joeynmt.training - Epoch   9, Step:   288200, Batch Loss:     1.709152, Tokens per Sec:    11410, Lr: 0.000300\n",
            "2021-07-22 14:46:24,496 - INFO - joeynmt.training - Epoch   9, Step:   288400, Batch Loss:     1.777395, Tokens per Sec:    11384, Lr: 0.000300\n",
            "2021-07-22 14:47:02,433 - INFO - joeynmt.training - Epoch   9, Step:   288600, Batch Loss:     1.787453, Tokens per Sec:    11565, Lr: 0.000300\n",
            "2021-07-22 14:47:40,167 - INFO - joeynmt.training - Epoch   9, Step:   288800, Batch Loss:     1.863405, Tokens per Sec:    11273, Lr: 0.000300\n",
            "2021-07-22 14:48:17,739 - INFO - joeynmt.training - Epoch   9, Step:   289000, Batch Loss:     1.850011, Tokens per Sec:    11523, Lr: 0.000300\n",
            "2021-07-22 14:48:55,520 - INFO - joeynmt.training - Epoch   9, Step:   289200, Batch Loss:     1.795310, Tokens per Sec:    11367, Lr: 0.000300\n",
            "2021-07-22 14:49:33,424 - INFO - joeynmt.training - Epoch   9, Step:   289400, Batch Loss:     1.717906, Tokens per Sec:    11413, Lr: 0.000300\n",
            "2021-07-22 14:50:11,133 - INFO - joeynmt.training - Epoch   9, Step:   289600, Batch Loss:     1.636317, Tokens per Sec:    11476, Lr: 0.000300\n",
            "2021-07-22 14:50:48,913 - INFO - joeynmt.training - Epoch   9, Step:   289800, Batch Loss:     1.947913, Tokens per Sec:    11671, Lr: 0.000300\n",
            "2021-07-22 14:51:26,683 - INFO - joeynmt.training - Epoch   9, Step:   290000, Batch Loss:     1.448106, Tokens per Sec:    11554, Lr: 0.000300\n",
            "2021-07-22 14:53:08,421 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 14:53:08,422 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 14:53:08,422 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 14:53:10,214 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 14:53:10,215 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 14:53:10,215 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 14:53:10,215 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , this is acceptable to God . ”\n",
            "2021-07-22 14:53:10,216 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 14:53:10,216 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 14:53:10,216 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 14:53:10,216 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-22 14:53:10,217 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 14:53:10,217 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 14:53:10,217 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 14:53:10,217 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-22 14:53:10,218 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 14:53:10,219 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 14:53:10,220 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 14:53:10,220 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to heal his family .\n",
            "2021-07-22 14:53:10,220 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   290000: bleu:  19.66, loss: 174335.8906, ppl:   6.9494, duration: 103.5370s\n",
            "2021-07-22 14:53:48,202 - INFO - joeynmt.training - Epoch   9, Step:   290200, Batch Loss:     1.733405, Tokens per Sec:    11346, Lr: 0.000300\n",
            "2021-07-22 14:54:26,197 - INFO - joeynmt.training - Epoch   9, Step:   290400, Batch Loss:     1.775679, Tokens per Sec:    11466, Lr: 0.000300\n",
            "2021-07-22 14:55:04,155 - INFO - joeynmt.training - Epoch   9, Step:   290600, Batch Loss:     1.758473, Tokens per Sec:    11282, Lr: 0.000300\n",
            "2021-07-22 14:55:42,484 - INFO - joeynmt.training - Epoch   9, Step:   290800, Batch Loss:     1.621799, Tokens per Sec:    11614, Lr: 0.000300\n",
            "2021-07-22 14:56:20,564 - INFO - joeynmt.training - Epoch   9, Step:   291000, Batch Loss:     1.494309, Tokens per Sec:    11322, Lr: 0.000300\n",
            "2021-07-22 14:56:58,726 - INFO - joeynmt.training - Epoch   9, Step:   291200, Batch Loss:     1.869135, Tokens per Sec:    11367, Lr: 0.000300\n",
            "2021-07-22 14:57:37,146 - INFO - joeynmt.training - Epoch   9, Step:   291400, Batch Loss:     1.871836, Tokens per Sec:    11501, Lr: 0.000300\n",
            "2021-07-22 14:58:15,283 - INFO - joeynmt.training - Epoch   9, Step:   291600, Batch Loss:     2.263675, Tokens per Sec:    11465, Lr: 0.000300\n",
            "2021-07-22 14:58:53,405 - INFO - joeynmt.training - Epoch   9, Step:   291800, Batch Loss:     1.844944, Tokens per Sec:    11457, Lr: 0.000300\n",
            "2021-07-22 14:59:31,686 - INFO - joeynmt.training - Epoch   9, Step:   292000, Batch Loss:     1.766505, Tokens per Sec:    11451, Lr: 0.000300\n",
            "2021-07-22 15:00:09,967 - INFO - joeynmt.training - Epoch   9, Step:   292200, Batch Loss:     1.660431, Tokens per Sec:    11424, Lr: 0.000300\n",
            "2021-07-22 15:00:48,053 - INFO - joeynmt.training - Epoch   9, Step:   292400, Batch Loss:     1.783529, Tokens per Sec:    11333, Lr: 0.000300\n",
            "2021-07-22 15:01:25,788 - INFO - joeynmt.training - Epoch   9, Step:   292600, Batch Loss:     1.784964, Tokens per Sec:    11388, Lr: 0.000300\n",
            "2021-07-22 15:02:01,689 - INFO - joeynmt.training - Epoch   9: total training loss 14848.01\n",
            "2021-07-22 15:02:01,690 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-22 15:02:04,510 - INFO - joeynmt.training - Epoch  10, Step:   292800, Batch Loss:     1.792415, Tokens per Sec:     7209, Lr: 0.000300\n",
            "2021-07-22 15:02:42,238 - INFO - joeynmt.training - Epoch  10, Step:   293000, Batch Loss:     1.872385, Tokens per Sec:    11499, Lr: 0.000300\n",
            "2021-07-22 15:03:19,961 - INFO - joeynmt.training - Epoch  10, Step:   293200, Batch Loss:     1.477139, Tokens per Sec:    11422, Lr: 0.000300\n",
            "2021-07-22 15:03:57,669 - INFO - joeynmt.training - Epoch  10, Step:   293400, Batch Loss:     1.783547, Tokens per Sec:    11599, Lr: 0.000300\n",
            "2021-07-22 15:04:35,535 - INFO - joeynmt.training - Epoch  10, Step:   293600, Batch Loss:     1.767450, Tokens per Sec:    11451, Lr: 0.000300\n",
            "2021-07-22 15:05:13,273 - INFO - joeynmt.training - Epoch  10, Step:   293800, Batch Loss:     1.638185, Tokens per Sec:    11484, Lr: 0.000300\n",
            "2021-07-22 15:05:51,261 - INFO - joeynmt.training - Epoch  10, Step:   294000, Batch Loss:     1.563496, Tokens per Sec:    11397, Lr: 0.000300\n",
            "2021-07-22 15:06:29,236 - INFO - joeynmt.training - Epoch  10, Step:   294200, Batch Loss:     1.692357, Tokens per Sec:    11514, Lr: 0.000300\n",
            "2021-07-22 15:07:07,373 - INFO - joeynmt.training - Epoch  10, Step:   294400, Batch Loss:     1.976164, Tokens per Sec:    11501, Lr: 0.000300\n",
            "2021-07-22 15:07:45,085 - INFO - joeynmt.training - Epoch  10, Step:   294600, Batch Loss:     1.765275, Tokens per Sec:    11410, Lr: 0.000300\n",
            "2021-07-22 15:08:23,274 - INFO - joeynmt.training - Epoch  10, Step:   294800, Batch Loss:     1.834064, Tokens per Sec:    11388, Lr: 0.000300\n",
            "2021-07-22 15:09:01,275 - INFO - joeynmt.training - Epoch  10, Step:   295000, Batch Loss:     1.678851, Tokens per Sec:    11406, Lr: 0.000300\n",
            "2021-07-22 15:10:45,013 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 15:10:45,013 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 15:10:45,014 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 15:10:46,826 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 15:10:46,826 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 15:10:46,827 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 15:10:46,827 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , this is acceptable to God . ”\n",
            "2021-07-22 15:10:46,827 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 15:10:46,828 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 15:10:46,828 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 15:10:46,828 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he listened to the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-22 15:10:46,828 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 15:10:46,829 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 15:10:46,829 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 15:10:46,830 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 15:10:46,830 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 15:10:46,830 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 15:10:46,830 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 15:10:46,831 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 15:10:46,831 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   295000: bleu:  19.32, loss: 174723.4844, ppl:   6.9794, duration: 105.5550s\n",
            "2021-07-22 15:11:24,794 - INFO - joeynmt.training - Epoch  10, Step:   295200, Batch Loss:     1.692021, Tokens per Sec:    11150, Lr: 0.000300\n",
            "2021-07-22 15:12:03,022 - INFO - joeynmt.training - Epoch  10, Step:   295400, Batch Loss:     1.762576, Tokens per Sec:    11442, Lr: 0.000300\n",
            "2021-07-22 15:12:41,301 - INFO - joeynmt.training - Epoch  10, Step:   295600, Batch Loss:     1.508859, Tokens per Sec:    11480, Lr: 0.000300\n",
            "2021-07-22 15:13:19,383 - INFO - joeynmt.training - Epoch  10, Step:   295800, Batch Loss:     1.796783, Tokens per Sec:    11403, Lr: 0.000300\n",
            "2021-07-22 15:13:57,081 - INFO - joeynmt.training - Epoch  10, Step:   296000, Batch Loss:     1.832733, Tokens per Sec:    11552, Lr: 0.000300\n",
            "2021-07-22 15:14:34,680 - INFO - joeynmt.training - Epoch  10, Step:   296200, Batch Loss:     1.798081, Tokens per Sec:    11445, Lr: 0.000300\n",
            "2021-07-22 15:15:12,228 - INFO - joeynmt.training - Epoch  10, Step:   296400, Batch Loss:     1.856982, Tokens per Sec:    11459, Lr: 0.000300\n",
            "2021-07-22 15:15:50,197 - INFO - joeynmt.training - Epoch  10, Step:   296600, Batch Loss:     2.086270, Tokens per Sec:    11487, Lr: 0.000300\n",
            "2021-07-22 15:16:27,880 - INFO - joeynmt.training - Epoch  10, Step:   296800, Batch Loss:     1.888381, Tokens per Sec:    11538, Lr: 0.000300\n",
            "2021-07-22 15:17:05,880 - INFO - joeynmt.training - Epoch  10, Step:   297000, Batch Loss:     1.652842, Tokens per Sec:    11395, Lr: 0.000300\n",
            "2021-07-22 15:17:43,467 - INFO - joeynmt.training - Epoch  10, Step:   297200, Batch Loss:     1.764919, Tokens per Sec:    11508, Lr: 0.000300\n",
            "2021-07-22 15:18:21,381 - INFO - joeynmt.training - Epoch  10, Step:   297400, Batch Loss:     1.717755, Tokens per Sec:    11334, Lr: 0.000300\n",
            "2021-07-22 15:18:58,965 - INFO - joeynmt.training - Epoch  10, Step:   297600, Batch Loss:     1.837582, Tokens per Sec:    11308, Lr: 0.000300\n",
            "2021-07-22 15:19:36,914 - INFO - joeynmt.training - Epoch  10, Step:   297800, Batch Loss:     1.827255, Tokens per Sec:    11493, Lr: 0.000300\n",
            "2021-07-22 15:20:15,007 - INFO - joeynmt.training - Epoch  10, Step:   298000, Batch Loss:     1.777551, Tokens per Sec:    11552, Lr: 0.000300\n",
            "2021-07-22 15:20:52,838 - INFO - joeynmt.training - Epoch  10, Step:   298200, Batch Loss:     2.332761, Tokens per Sec:    11398, Lr: 0.000300\n",
            "2021-07-22 15:21:30,877 - INFO - joeynmt.training - Epoch  10, Step:   298400, Batch Loss:     1.483568, Tokens per Sec:    11474, Lr: 0.000300\n",
            "2021-07-22 15:22:09,138 - INFO - joeynmt.training - Epoch  10, Step:   298600, Batch Loss:     1.654029, Tokens per Sec:    11440, Lr: 0.000300\n",
            "2021-07-22 15:22:47,053 - INFO - joeynmt.training - Epoch  10, Step:   298800, Batch Loss:     1.867580, Tokens per Sec:    11454, Lr: 0.000300\n",
            "2021-07-22 15:23:24,931 - INFO - joeynmt.training - Epoch  10, Step:   299000, Batch Loss:     1.762583, Tokens per Sec:    11434, Lr: 0.000300\n",
            "2021-07-22 15:24:02,846 - INFO - joeynmt.training - Epoch  10, Step:   299200, Batch Loss:     1.412941, Tokens per Sec:    11425, Lr: 0.000300\n",
            "2021-07-22 15:24:40,949 - INFO - joeynmt.training - Epoch  10, Step:   299400, Batch Loss:     1.665403, Tokens per Sec:    11537, Lr: 0.000300\n",
            "2021-07-22 15:25:18,944 - INFO - joeynmt.training - Epoch  10, Step:   299600, Batch Loss:     1.660075, Tokens per Sec:    11535, Lr: 0.000300\n",
            "2021-07-22 15:25:56,689 - INFO - joeynmt.training - Epoch  10, Step:   299800, Batch Loss:     2.064706, Tokens per Sec:    11403, Lr: 0.000300\n",
            "2021-07-22 15:26:34,754 - INFO - joeynmt.training - Epoch  10, Step:   300000, Batch Loss:     1.744968, Tokens per Sec:    11266, Lr: 0.000300\n",
            "2021-07-22 15:28:15,720 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 15:28:15,720 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 15:28:15,720 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 15:28:16,736 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 15:28:16,736 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 15:28:17,844 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 15:28:17,845 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 15:28:17,845 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 15:28:17,846 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 15:28:17,846 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 15:28:17,846 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 15:28:17,846 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 15:28:17,847 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-22 15:28:17,847 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 15:28:17,847 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 15:28:17,847 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 15:28:17,848 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 15:28:17,848 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 15:28:17,848 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 15:28:17,848 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 15:28:17,849 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 15:28:17,849 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   300000: bleu:  19.68, loss: 172826.5000, ppl:   6.8338, duration: 103.0940s\n",
            "2021-07-22 15:28:56,212 - INFO - joeynmt.training - Epoch  10, Step:   300200, Batch Loss:     1.684999, Tokens per Sec:    11497, Lr: 0.000300\n",
            "2021-07-22 15:29:34,273 - INFO - joeynmt.training - Epoch  10, Step:   300400, Batch Loss:     1.922709, Tokens per Sec:    11476, Lr: 0.000300\n",
            "2021-07-22 15:30:12,796 - INFO - joeynmt.training - Epoch  10, Step:   300600, Batch Loss:     1.539547, Tokens per Sec:    11428, Lr: 0.000300\n",
            "2021-07-22 15:30:50,751 - INFO - joeynmt.training - Epoch  10, Step:   300800, Batch Loss:     1.883710, Tokens per Sec:    11419, Lr: 0.000300\n",
            "2021-07-22 15:31:28,907 - INFO - joeynmt.training - Epoch  10, Step:   301000, Batch Loss:     1.797315, Tokens per Sec:    11393, Lr: 0.000300\n",
            "2021-07-22 15:31:59,737 - INFO - joeynmt.training - Epoch  10: total training loss 14822.36\n",
            "2021-07-22 15:31:59,738 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-22 15:32:08,188 - INFO - joeynmt.training - Epoch  11, Step:   301200, Batch Loss:     1.659798, Tokens per Sec:     9789, Lr: 0.000300\n",
            "2021-07-22 15:32:46,493 - INFO - joeynmt.training - Epoch  11, Step:   301400, Batch Loss:     1.776539, Tokens per Sec:    11520, Lr: 0.000300\n",
            "2021-07-22 15:33:24,537 - INFO - joeynmt.training - Epoch  11, Step:   301600, Batch Loss:     1.667103, Tokens per Sec:    11280, Lr: 0.000300\n",
            "2021-07-22 15:34:02,436 - INFO - joeynmt.training - Epoch  11, Step:   301800, Batch Loss:     1.731614, Tokens per Sec:    11281, Lr: 0.000300\n",
            "2021-07-22 15:34:40,993 - INFO - joeynmt.training - Epoch  11, Step:   302000, Batch Loss:     1.741449, Tokens per Sec:    11414, Lr: 0.000300\n",
            "2021-07-22 15:35:19,062 - INFO - joeynmt.training - Epoch  11, Step:   302200, Batch Loss:     1.876025, Tokens per Sec:    11381, Lr: 0.000300\n",
            "2021-07-22 15:35:56,836 - INFO - joeynmt.training - Epoch  11, Step:   302400, Batch Loss:     1.447183, Tokens per Sec:    11367, Lr: 0.000300\n",
            "2021-07-22 15:36:34,996 - INFO - joeynmt.training - Epoch  11, Step:   302600, Batch Loss:     1.986482, Tokens per Sec:    11495, Lr: 0.000300\n",
            "2021-07-22 15:37:13,285 - INFO - joeynmt.training - Epoch  11, Step:   302800, Batch Loss:     1.747161, Tokens per Sec:    11549, Lr: 0.000300\n",
            "2021-07-22 15:37:51,067 - INFO - joeynmt.training - Epoch  11, Step:   303000, Batch Loss:     1.880520, Tokens per Sec:    11308, Lr: 0.000300\n",
            "2021-07-22 15:38:29,428 - INFO - joeynmt.training - Epoch  11, Step:   303200, Batch Loss:     1.769035, Tokens per Sec:    11478, Lr: 0.000300\n",
            "2021-07-22 15:39:07,662 - INFO - joeynmt.training - Epoch  11, Step:   303400, Batch Loss:     1.876043, Tokens per Sec:    11408, Lr: 0.000300\n",
            "2021-07-22 15:39:45,760 - INFO - joeynmt.training - Epoch  11, Step:   303600, Batch Loss:     1.348014, Tokens per Sec:    11475, Lr: 0.000300\n",
            "2021-07-22 15:40:23,925 - INFO - joeynmt.training - Epoch  11, Step:   303800, Batch Loss:     1.697364, Tokens per Sec:    11519, Lr: 0.000300\n",
            "2021-07-22 15:41:01,420 - INFO - joeynmt.training - Epoch  11, Step:   304000, Batch Loss:     1.772413, Tokens per Sec:    11459, Lr: 0.000300\n",
            "2021-07-22 15:41:38,876 - INFO - joeynmt.training - Epoch  11, Step:   304200, Batch Loss:     1.998520, Tokens per Sec:    11419, Lr: 0.000300\n",
            "2021-07-22 15:42:16,757 - INFO - joeynmt.training - Epoch  11, Step:   304400, Batch Loss:     1.772530, Tokens per Sec:    11542, Lr: 0.000300\n",
            "2021-07-22 15:42:54,674 - INFO - joeynmt.training - Epoch  11, Step:   304600, Batch Loss:     1.817806, Tokens per Sec:    11606, Lr: 0.000300\n",
            "2021-07-22 15:43:32,297 - INFO - joeynmt.training - Epoch  11, Step:   304800, Batch Loss:     1.613508, Tokens per Sec:    11503, Lr: 0.000300\n",
            "2021-07-22 15:44:10,294 - INFO - joeynmt.training - Epoch  11, Step:   305000, Batch Loss:     1.483415, Tokens per Sec:    11538, Lr: 0.000300\n",
            "2021-07-22 15:45:49,108 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 15:45:49,108 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 15:45:49,109 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 15:45:50,754 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 15:45:50,755 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 15:45:50,755 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 15:45:50,755 - INFO - joeynmt.training - \tHypothesis: “ If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 15:45:50,755 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 15:45:50,756 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 15:45:50,756 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 15:45:50,756 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-22 15:45:50,756 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 15:45:50,757 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 15:45:50,757 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 15:45:50,757 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-22 15:45:50,757 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 15:45:50,758 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 15:45:50,758 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 15:45:50,758 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-22 15:45:50,758 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   305000: bleu:  19.81, loss: 172889.5312, ppl:   6.8385, duration: 100.4638s\n",
            "2021-07-22 15:46:28,849 - INFO - joeynmt.training - Epoch  11, Step:   305200, Batch Loss:     1.767248, Tokens per Sec:    11613, Lr: 0.000300\n",
            "2021-07-22 15:47:06,413 - INFO - joeynmt.training - Epoch  11, Step:   305400, Batch Loss:     1.840811, Tokens per Sec:    11559, Lr: 0.000300\n",
            "2021-07-22 15:47:44,131 - INFO - joeynmt.training - Epoch  11, Step:   305600, Batch Loss:     1.841953, Tokens per Sec:    11588, Lr: 0.000300\n",
            "2021-07-22 15:48:21,899 - INFO - joeynmt.training - Epoch  11, Step:   305800, Batch Loss:     1.548337, Tokens per Sec:    11679, Lr: 0.000300\n",
            "2021-07-22 15:48:59,375 - INFO - joeynmt.training - Epoch  11, Step:   306000, Batch Loss:     1.804376, Tokens per Sec:    11739, Lr: 0.000300\n",
            "2021-07-22 15:49:36,666 - INFO - joeynmt.training - Epoch  11, Step:   306200, Batch Loss:     1.772542, Tokens per Sec:    11539, Lr: 0.000300\n",
            "2021-07-22 15:50:14,299 - INFO - joeynmt.training - Epoch  11, Step:   306400, Batch Loss:     1.885505, Tokens per Sec:    11637, Lr: 0.000300\n",
            "2021-07-22 15:50:51,522 - INFO - joeynmt.training - Epoch  11, Step:   306600, Batch Loss:     1.718812, Tokens per Sec:    11596, Lr: 0.000300\n",
            "2021-07-22 15:51:28,943 - INFO - joeynmt.training - Epoch  11, Step:   306800, Batch Loss:     1.758229, Tokens per Sec:    11516, Lr: 0.000300\n",
            "2021-07-22 15:52:06,273 - INFO - joeynmt.training - Epoch  11, Step:   307000, Batch Loss:     1.771751, Tokens per Sec:    11436, Lr: 0.000300\n",
            "2021-07-22 15:52:43,676 - INFO - joeynmt.training - Epoch  11, Step:   307200, Batch Loss:     1.731827, Tokens per Sec:    11402, Lr: 0.000300\n",
            "2021-07-22 15:53:21,276 - INFO - joeynmt.training - Epoch  11, Step:   307400, Batch Loss:     1.557166, Tokens per Sec:    11670, Lr: 0.000300\n",
            "2021-07-22 15:53:58,555 - INFO - joeynmt.training - Epoch  11, Step:   307600, Batch Loss:     1.914866, Tokens per Sec:    11457, Lr: 0.000300\n",
            "2021-07-22 15:54:36,043 - INFO - joeynmt.training - Epoch  11, Step:   307800, Batch Loss:     1.757137, Tokens per Sec:    11576, Lr: 0.000300\n",
            "2021-07-22 15:55:13,705 - INFO - joeynmt.training - Epoch  11, Step:   308000, Batch Loss:     1.776015, Tokens per Sec:    11659, Lr: 0.000300\n",
            "2021-07-22 15:55:50,918 - INFO - joeynmt.training - Epoch  11, Step:   308200, Batch Loss:     1.752159, Tokens per Sec:    11546, Lr: 0.000300\n",
            "2021-07-22 15:56:28,270 - INFO - joeynmt.training - Epoch  11, Step:   308400, Batch Loss:     1.805988, Tokens per Sec:    11513, Lr: 0.000300\n",
            "2021-07-22 15:57:05,850 - INFO - joeynmt.training - Epoch  11, Step:   308600, Batch Loss:     1.834422, Tokens per Sec:    11702, Lr: 0.000300\n",
            "2021-07-22 15:57:43,160 - INFO - joeynmt.training - Epoch  11, Step:   308800, Batch Loss:     1.687329, Tokens per Sec:    11477, Lr: 0.000300\n",
            "2021-07-22 15:58:20,514 - INFO - joeynmt.training - Epoch  11, Step:   309000, Batch Loss:     1.794101, Tokens per Sec:    11565, Lr: 0.000300\n",
            "2021-07-22 15:58:57,938 - INFO - joeynmt.training - Epoch  11, Step:   309200, Batch Loss:     1.631149, Tokens per Sec:    11672, Lr: 0.000300\n",
            "2021-07-22 15:59:35,513 - INFO - joeynmt.training - Epoch  11, Step:   309400, Batch Loss:     1.898452, Tokens per Sec:    11551, Lr: 0.000300\n",
            "2021-07-22 15:59:59,079 - INFO - joeynmt.training - Epoch  11: total training loss 14778.14\n",
            "2021-07-22 15:59:59,079 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-22 16:00:14,855 - INFO - joeynmt.training - Epoch  12, Step:   309600, Batch Loss:     1.703374, Tokens per Sec:    10928, Lr: 0.000300\n",
            "2021-07-22 16:00:52,807 - INFO - joeynmt.training - Epoch  12, Step:   309800, Batch Loss:     1.902723, Tokens per Sec:    11559, Lr: 0.000300\n",
            "2021-07-22 16:01:30,522 - INFO - joeynmt.training - Epoch  12, Step:   310000, Batch Loss:     1.859381, Tokens per Sec:    11584, Lr: 0.000300\n",
            "2021-07-22 16:03:08,500 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 16:03:08,501 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 16:03:08,501 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 16:03:09,483 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 16:03:09,487 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 16:03:10,254 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 16:03:10,254 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-22 16:03:10,255 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-22 16:03:10,255 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-22 16:03:10,255 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 16:03:10,255 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-22 16:03:10,256 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-22 16:03:10,256 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-22 16:03:10,256 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 16:03:10,257 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-22 16:03:10,258 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-22 16:03:10,258 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-22 16:03:10,258 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 16:03:10,259 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-22 16:03:10,259 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-22 16:03:10,259 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-22 16:03:10,259 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   310000: bleu:  19.82, loss: 172501.8906, ppl:   6.8091, duration: 99.7371s\n",
            "2021-07-22 16:03:47,775 - INFO - joeynmt.training - Epoch  12, Step:   310200, Batch Loss:     1.789080, Tokens per Sec:    11418, Lr: 0.000300\n",
            "2021-07-22 16:04:25,120 - INFO - joeynmt.training - Epoch  12, Step:   310400, Batch Loss:     1.895018, Tokens per Sec:    11378, Lr: 0.000300\n",
            "2021-07-22 16:05:02,646 - INFO - joeynmt.training - Epoch  12, Step:   310600, Batch Loss:     1.668324, Tokens per Sec:    11632, Lr: 0.000300\n",
            "2021-07-22 16:05:40,496 - INFO - joeynmt.training - Epoch  12, Step:   310800, Batch Loss:     1.833601, Tokens per Sec:    11669, Lr: 0.000300\n",
            "2021-07-22 16:06:18,104 - INFO - joeynmt.training - Epoch  12, Step:   311000, Batch Loss:     1.729050, Tokens per Sec:    11487, Lr: 0.000300\n",
            "2021-07-22 16:06:55,543 - INFO - joeynmt.training - Epoch  12, Step:   311200, Batch Loss:     1.645526, Tokens per Sec:    11594, Lr: 0.000300\n",
            "2021-07-22 16:07:33,243 - INFO - joeynmt.training - Epoch  12, Step:   311400, Batch Loss:     1.698112, Tokens per Sec:    11591, Lr: 0.000300\n",
            "2021-07-22 16:08:10,720 - INFO - joeynmt.training - Epoch  12, Step:   311600, Batch Loss:     1.604482, Tokens per Sec:    11556, Lr: 0.000300\n",
            "2021-07-22 16:08:48,203 - INFO - joeynmt.training - Epoch  12, Step:   311800, Batch Loss:     1.812811, Tokens per Sec:    11526, Lr: 0.000300\n",
            "2021-07-22 16:09:26,035 - INFO - joeynmt.training - Epoch  12, Step:   312000, Batch Loss:     1.962079, Tokens per Sec:    11655, Lr: 0.000300\n",
            "2021-07-22 16:10:03,477 - INFO - joeynmt.training - Epoch  12, Step:   312200, Batch Loss:     1.773021, Tokens per Sec:    11596, Lr: 0.000300\n",
            "2021-07-22 16:10:40,708 - INFO - joeynmt.training - Epoch  12, Step:   312400, Batch Loss:     1.835696, Tokens per Sec:    11582, Lr: 0.000300\n",
            "2021-07-22 16:11:18,430 - INFO - joeynmt.training - Epoch  12, Step:   312600, Batch Loss:     1.802531, Tokens per Sec:    11555, Lr: 0.000300\n",
            "2021-07-22 16:11:55,971 - INFO - joeynmt.training - Epoch  12, Step:   312800, Batch Loss:     2.036805, Tokens per Sec:    11630, Lr: 0.000300\n",
            "2021-07-22 16:12:33,542 - INFO - joeynmt.training - Epoch  12, Step:   313000, Batch Loss:     1.531015, Tokens per Sec:    11739, Lr: 0.000300\n",
            "2021-07-22 16:13:10,955 - INFO - joeynmt.training - Epoch  12, Step:   313200, Batch Loss:     1.879487, Tokens per Sec:    11620, Lr: 0.000300\n",
            "2021-07-22 16:13:48,344 - INFO - joeynmt.training - Epoch  12, Step:   313400, Batch Loss:     1.575255, Tokens per Sec:    11538, Lr: 0.000300\n",
            "2021-07-22 16:14:25,922 - INFO - joeynmt.training - Epoch  12, Step:   313600, Batch Loss:     1.763769, Tokens per Sec:    11571, Lr: 0.000300\n",
            "2021-07-22 16:15:03,122 - INFO - joeynmt.training - Epoch  12, Step:   313800, Batch Loss:     1.692902, Tokens per Sec:    11706, Lr: 0.000300\n",
            "2021-07-22 16:15:40,448 - INFO - joeynmt.training - Epoch  12, Step:   314000, Batch Loss:     1.626425, Tokens per Sec:    11562, Lr: 0.000300\n",
            "2021-07-22 16:16:17,390 - INFO - joeynmt.training - Epoch  12, Step:   314200, Batch Loss:     1.768458, Tokens per Sec:    11559, Lr: 0.000300\n",
            "2021-07-22 16:16:55,142 - INFO - joeynmt.training - Epoch  12, Step:   314400, Batch Loss:     1.740640, Tokens per Sec:    11680, Lr: 0.000300\n",
            "2021-07-22 16:17:32,627 - INFO - joeynmt.training - Epoch  12, Step:   314600, Batch Loss:     1.917290, Tokens per Sec:    11474, Lr: 0.000300\n",
            "2021-07-22 16:18:10,500 - INFO - joeynmt.training - Epoch  12, Step:   314800, Batch Loss:     1.648596, Tokens per Sec:    11809, Lr: 0.000300\n",
            "2021-07-22 16:18:47,620 - INFO - joeynmt.training - Epoch  12, Step:   315000, Batch Loss:     1.801753, Tokens per Sec:    11380, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCrMtoDih8q5"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 310000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/models/lg_rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued4/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_rw_lhen_reverse_transformer\"', f'model_dir: \"models/lg_rw_lhen_reverse_transformer_continued5\"').replace(\n",
        "        f'epochs: 30', f'epochs: 18')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload5.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oSzcj8K7iV-e",
        "outputId": "5f9a43d3-77e1-4110-e265-ad195cb96097"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_lg_rw_lhen_reload5.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"lg_rw_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"lg_rw_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued4/310000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 18                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/lg_rw_lhen_reverse_transformer_continued5\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E2ZhqIzij9A",
        "collapsed": true,
        "outputId": "5c504c97-f7a0-40c1-b196-a14649e51e13"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_rw_lhen_reload5.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-25 07:21:54,318 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-25 07:21:54,392 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-25 07:22:09,628 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-25 07:22:10,160 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-25 07:22:10,868 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-25 07:22:11,674 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-25 07:22:11,674 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-25 07:22:12,053 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-25 07:22:12.301664: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-25 07:22:16,034 - INFO - joeynmt.training - Total params: 12179456\n",
            "2021-07-25 07:22:26,804 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued4/310000.ckpt\n",
            "2021-07-25 07:22:27,304 - INFO - joeynmt.helpers - cfg.name                           : lg_rw_lhen_reverse_transformer\n",
            "2021-07-25 07:22:27,304 - INFO - joeynmt.helpers - cfg.data.src                       : lg_rw_lh\n",
            "2021-07-25 07:22:27,304 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-25 07:22:27,305 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\n",
            "2021-07-25 07:22:27,305 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\n",
            "2021-07-25 07:22:27,305 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\n",
            "2021-07-25 07:22:27,305 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-25 07:22:27,305 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-25 07:22:27,305 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-25 07:22:27,306 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-25 07:22:27,306 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-25 07:22:27,306 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-25 07:22:27,306 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-25 07:22:27,306 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued4/310000.ckpt\n",
            "2021-07-25 07:22:27,306 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-25 07:22:27,306 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-25 07:22:27,307 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-25 07:22:27,307 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-25 07:22:27,307 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-25 07:22:27,307 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-25 07:22:27,307 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-25 07:22:27,307 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-25 07:22:27,307 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-25 07:22:27,307 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-25 07:22:27,308 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-25 07:22:27,308 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-25 07:22:27,308 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-25 07:22:27,308 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-25 07:22:27,308 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-25 07:22:27,308 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-25 07:22:27,308 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-25 07:22:27,309 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-25 07:22:27,309 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-25 07:22:27,309 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-25 07:22:27,309 - INFO - joeynmt.helpers - cfg.training.epochs                : 18\n",
            "2021-07-25 07:22:27,309 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-25 07:22:27,309 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-25 07:22:27,309 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-25 07:22:27,309 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_rw_lhen_reverse_transformer_continued5\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-25 07:22:27,310 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-25 07:22:27,311 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-25 07:22:27,311 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-25 07:22:27,311 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-25 07:22:27,311 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-25 07:22:27,311 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-25 07:22:27,311 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-25 07:22:27,311 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-25 07:22:27,312 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-25 07:22:27,312 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-25 07:22:27,312 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-25 07:22:27,312 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-25 07:22:27,312 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-25 07:22:27,312 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-25 07:22:27,312 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-25 07:22:27,312 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-25 07:22:27,313 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-25 07:22:27,313 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-25 07:22:27,313 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-25 07:22:27,313 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-25 07:22:27,313 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-25 07:22:27,313 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-25 07:22:27,313 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-25 07:22:27,313 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 665203,\n",
            "\tvalid 3000,\n",
            "\ttest 1000\n",
            "2021-07-25 07:22:27,314 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ at@@ andika okuk@@ olera ku m@@ azima ge nn@@ ali nj@@ iga , era nn@@ ak@@ ir@@ aba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obuf@@ uzi n’@@ okul@@ eka em@@ ikw@@ ano em@@ ibi gye nn@@ alina .\n",
            "\t[TRG] Ev@@ ent@@ ually , however , the tr@@ uth@@ s I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my polit@@ ical view@@ po@@ in@@ ts and associ@@ ations .\n",
            "2021-07-25 07:22:27,314 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-25 07:22:27,314 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-25 07:22:27,314 - INFO - joeynmt.helpers - Number of Src words (types): 4372\n",
            "2021-07-25 07:22:27,314 - INFO - joeynmt.helpers - Number of Trg words (types): 4372\n",
            "2021-07-25 07:22:27,315 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4372),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4372))\n",
            "2021-07-25 07:22:27,330 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-25 07:22:27,330 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-25 07:22:55,002 - INFO - joeynmt.training - Epoch   1, Step:   310200, Batch Loss:     1.786153, Tokens per Sec:    15481, Lr: 0.000300\n",
            "2021-07-25 07:23:21,328 - INFO - joeynmt.training - Epoch   1, Step:   310400, Batch Loss:     1.894172, Tokens per Sec:    16141, Lr: 0.000300\n",
            "2021-07-25 07:23:49,011 - INFO - joeynmt.training - Epoch   1, Step:   310600, Batch Loss:     1.649000, Tokens per Sec:    15768, Lr: 0.000300\n",
            "2021-07-25 07:24:17,305 - INFO - joeynmt.training - Epoch   1, Step:   310800, Batch Loss:     1.832309, Tokens per Sec:    15611, Lr: 0.000300\n",
            "2021-07-25 07:24:45,353 - INFO - joeynmt.training - Epoch   1, Step:   311000, Batch Loss:     1.727797, Tokens per Sec:    15402, Lr: 0.000300\n",
            "2021-07-25 07:25:13,156 - INFO - joeynmt.training - Epoch   1, Step:   311200, Batch Loss:     1.659445, Tokens per Sec:    15613, Lr: 0.000300\n",
            "2021-07-25 07:25:41,286 - INFO - joeynmt.training - Epoch   1, Step:   311400, Batch Loss:     1.710641, Tokens per Sec:    15534, Lr: 0.000300\n",
            "2021-07-25 07:26:09,218 - INFO - joeynmt.training - Epoch   1, Step:   311600, Batch Loss:     1.585213, Tokens per Sec:    15505, Lr: 0.000300\n",
            "2021-07-25 07:26:37,201 - INFO - joeynmt.training - Epoch   1, Step:   311800, Batch Loss:     1.799054, Tokens per Sec:    15440, Lr: 0.000300\n",
            "2021-07-25 07:27:05,211 - INFO - joeynmt.training - Epoch   1, Step:   312000, Batch Loss:     1.922455, Tokens per Sec:    15742, Lr: 0.000300\n",
            "2021-07-25 07:27:33,307 - INFO - joeynmt.training - Epoch   1, Step:   312200, Batch Loss:     1.793887, Tokens per Sec:    15453, Lr: 0.000300\n",
            "2021-07-25 07:28:00,996 - INFO - joeynmt.training - Epoch   1, Step:   312400, Batch Loss:     1.860323, Tokens per Sec:    15573, Lr: 0.000300\n",
            "2021-07-25 07:28:29,088 - INFO - joeynmt.training - Epoch   1, Step:   312600, Batch Loss:     1.786446, Tokens per Sec:    15517, Lr: 0.000300\n",
            "2021-07-25 07:28:57,264 - INFO - joeynmt.training - Epoch   1, Step:   312800, Batch Loss:     2.032360, Tokens per Sec:    15496, Lr: 0.000300\n",
            "2021-07-25 07:29:25,349 - INFO - joeynmt.training - Epoch   1, Step:   313000, Batch Loss:     1.503693, Tokens per Sec:    15704, Lr: 0.000300\n",
            "2021-07-25 07:29:53,403 - INFO - joeynmt.training - Epoch   1, Step:   313200, Batch Loss:     1.896587, Tokens per Sec:    15496, Lr: 0.000300\n",
            "2021-07-25 07:30:21,312 - INFO - joeynmt.training - Epoch   1, Step:   313400, Batch Loss:     1.574652, Tokens per Sec:    15457, Lr: 0.000300\n",
            "2021-07-25 07:30:49,454 - INFO - joeynmt.training - Epoch   1, Step:   313600, Batch Loss:     1.792218, Tokens per Sec:    15451, Lr: 0.000300\n",
            "2021-07-25 07:31:17,218 - INFO - joeynmt.training - Epoch   1, Step:   313800, Batch Loss:     1.696384, Tokens per Sec:    15684, Lr: 0.000300\n",
            "2021-07-25 07:31:44,898 - INFO - joeynmt.training - Epoch   1, Step:   314000, Batch Loss:     1.659785, Tokens per Sec:    15591, Lr: 0.000300\n",
            "2021-07-25 07:32:12,514 - INFO - joeynmt.training - Epoch   1, Step:   314200, Batch Loss:     1.759678, Tokens per Sec:    15462, Lr: 0.000300\n",
            "2021-07-25 07:32:40,734 - INFO - joeynmt.training - Epoch   1, Step:   314400, Batch Loss:     1.753106, Tokens per Sec:    15626, Lr: 0.000300\n",
            "2021-07-25 07:33:08,584 - INFO - joeynmt.training - Epoch   1, Step:   314600, Batch Loss:     1.918351, Tokens per Sec:    15444, Lr: 0.000300\n",
            "2021-07-25 07:33:36,930 - INFO - joeynmt.training - Epoch   1, Step:   314800, Batch Loss:     1.677056, Tokens per Sec:    15777, Lr: 0.000300\n",
            "2021-07-25 07:34:04,567 - INFO - joeynmt.training - Epoch   1, Step:   315000, Batch Loss:     1.788612, Tokens per Sec:    15285, Lr: 0.000300\n",
            "2021-07-25 07:35:41,171 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 07:35:41,171 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 07:35:41,172 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 07:35:43,024 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 07:35:43,024 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 07:35:43,024 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 07:35:43,025 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 07:35:43,025 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 07:35:43,025 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 07:35:43,025 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 07:35:43,026 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-25 07:35:43,026 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 07:35:43,027 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 07:35:43,027 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 07:35:43,027 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 07:35:43,028 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 07:35:43,028 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 07:35:43,028 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 07:35:43,028 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 07:35:43,029 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step   315000: bleu:  19.90, loss: 172863.5000, ppl:   6.8366, duration: 98.4615s\n",
            "2021-07-25 07:36:11,452 - INFO - joeynmt.training - Epoch   1, Step:   315200, Batch Loss:     2.000349, Tokens per Sec:    14871, Lr: 0.000300\n",
            "2021-07-25 07:36:39,449 - INFO - joeynmt.training - Epoch   1, Step:   315400, Batch Loss:     1.711303, Tokens per Sec:    15317, Lr: 0.000300\n",
            "2021-07-25 07:37:07,548 - INFO - joeynmt.training - Epoch   1, Step:   315600, Batch Loss:     1.596503, Tokens per Sec:    15607, Lr: 0.000300\n",
            "2021-07-25 07:37:35,440 - INFO - joeynmt.training - Epoch   1, Step:   315800, Batch Loss:     1.792459, Tokens per Sec:    15307, Lr: 0.000300\n",
            "2021-07-25 07:38:03,691 - INFO - joeynmt.training - Epoch   1, Step:   316000, Batch Loss:     1.526453, Tokens per Sec:    15678, Lr: 0.000300\n",
            "2021-07-25 07:38:31,736 - INFO - joeynmt.training - Epoch   1, Step:   316200, Batch Loss:     1.931884, Tokens per Sec:    15529, Lr: 0.000300\n",
            "2021-07-25 07:38:59,455 - INFO - joeynmt.training - Epoch   1, Step:   316400, Batch Loss:     1.871911, Tokens per Sec:    15543, Lr: 0.000300\n",
            "2021-07-25 07:39:27,375 - INFO - joeynmt.training - Epoch   1, Step:   316600, Batch Loss:     1.540784, Tokens per Sec:    15639, Lr: 0.000300\n",
            "2021-07-25 07:39:55,311 - INFO - joeynmt.training - Epoch   1, Step:   316800, Batch Loss:     1.863331, Tokens per Sec:    15662, Lr: 0.000300\n",
            "2021-07-25 07:40:23,287 - INFO - joeynmt.training - Epoch   1, Step:   317000, Batch Loss:     1.807732, Tokens per Sec:    15491, Lr: 0.000300\n",
            "2021-07-25 07:40:51,314 - INFO - joeynmt.training - Epoch   1, Step:   317200, Batch Loss:     1.755871, Tokens per Sec:    15497, Lr: 0.000300\n",
            "2021-07-25 07:41:19,301 - INFO - joeynmt.training - Epoch   1, Step:   317400, Batch Loss:     1.879276, Tokens per Sec:    15665, Lr: 0.000300\n",
            "2021-07-25 07:41:47,292 - INFO - joeynmt.training - Epoch   1, Step:   317600, Batch Loss:     1.709944, Tokens per Sec:    15426, Lr: 0.000300\n",
            "2021-07-25 07:42:15,143 - INFO - joeynmt.training - Epoch   1, Step:   317800, Batch Loss:     2.188637, Tokens per Sec:    15453, Lr: 0.000300\n",
            "2021-07-25 07:42:28,071 - INFO - joeynmt.training - Epoch   1: total training loss 13922.28\n",
            "2021-07-25 07:42:28,072 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-25 07:42:44,013 - INFO - joeynmt.training - Epoch   2, Step:   318000, Batch Loss:     1.317869, Tokens per Sec:    14490, Lr: 0.000300\n",
            "2021-07-25 07:43:12,077 - INFO - joeynmt.training - Epoch   2, Step:   318200, Batch Loss:     1.811093, Tokens per Sec:    15598, Lr: 0.000300\n",
            "2021-07-25 07:43:40,110 - INFO - joeynmt.training - Epoch   2, Step:   318400, Batch Loss:     1.837853, Tokens per Sec:    15620, Lr: 0.000300\n",
            "2021-07-25 07:44:07,992 - INFO - joeynmt.training - Epoch   2, Step:   318600, Batch Loss:     1.725348, Tokens per Sec:    15590, Lr: 0.000300\n",
            "2021-07-25 07:44:36,213 - INFO - joeynmt.training - Epoch   2, Step:   318800, Batch Loss:     1.811977, Tokens per Sec:    15568, Lr: 0.000300\n",
            "2021-07-25 07:45:04,078 - INFO - joeynmt.training - Epoch   2, Step:   319000, Batch Loss:     1.772054, Tokens per Sec:    15512, Lr: 0.000300\n",
            "2021-07-25 07:45:32,016 - INFO - joeynmt.training - Epoch   2, Step:   319200, Batch Loss:     1.490385, Tokens per Sec:    15429, Lr: 0.000300\n",
            "2021-07-25 07:45:59,936 - INFO - joeynmt.training - Epoch   2, Step:   319400, Batch Loss:     1.683203, Tokens per Sec:    15446, Lr: 0.000300\n",
            "2021-07-25 07:46:28,004 - INFO - joeynmt.training - Epoch   2, Step:   319600, Batch Loss:     1.849474, Tokens per Sec:    15717, Lr: 0.000300\n",
            "2021-07-25 07:46:56,100 - INFO - joeynmt.training - Epoch   2, Step:   319800, Batch Loss:     1.637504, Tokens per Sec:    15377, Lr: 0.000300\n",
            "2021-07-25 07:47:23,964 - INFO - joeynmt.training - Epoch   2, Step:   320000, Batch Loss:     1.264368, Tokens per Sec:    15550, Lr: 0.000300\n",
            "2021-07-25 07:49:02,459 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 07:49:02,459 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 07:49:02,459 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 07:49:03,369 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 07:49:03,370 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 07:49:04,435 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 07:49:04,436 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 07:49:04,436 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 07:49:04,436 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 07:49:04,436 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 07:49:04,437 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 07:49:04,437 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 07:49:04,437 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-25 07:49:04,437 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 07:49:04,438 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 07:49:04,438 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 07:49:04,438 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-25 07:49:04,438 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 07:49:04,439 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 07:49:04,439 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 07:49:04,439 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-25 07:49:04,439 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   320000: bleu:  20.08, loss: 171868.3438, ppl:   6.7613, duration: 100.4746s\n",
            "2021-07-25 07:49:32,587 - INFO - joeynmt.training - Epoch   2, Step:   320200, Batch Loss:     1.942271, Tokens per Sec:    15406, Lr: 0.000300\n",
            "2021-07-25 07:50:00,452 - INFO - joeynmt.training - Epoch   2, Step:   320400, Batch Loss:     1.686894, Tokens per Sec:    15478, Lr: 0.000300\n",
            "2021-07-25 07:50:28,403 - INFO - joeynmt.training - Epoch   2, Step:   320600, Batch Loss:     1.696239, Tokens per Sec:    15632, Lr: 0.000300\n",
            "2021-07-25 07:50:56,440 - INFO - joeynmt.training - Epoch   2, Step:   320800, Batch Loss:     1.791502, Tokens per Sec:    15408, Lr: 0.000300\n",
            "2021-07-25 07:51:24,128 - INFO - joeynmt.training - Epoch   2, Step:   321000, Batch Loss:     1.801547, Tokens per Sec:    15501, Lr: 0.000300\n",
            "2021-07-25 07:51:52,260 - INFO - joeynmt.training - Epoch   2, Step:   321200, Batch Loss:     1.703184, Tokens per Sec:    15500, Lr: 0.000300\n",
            "2021-07-25 07:52:20,253 - INFO - joeynmt.training - Epoch   2, Step:   321400, Batch Loss:     1.829814, Tokens per Sec:    15615, Lr: 0.000300\n",
            "2021-07-25 07:52:48,615 - INFO - joeynmt.training - Epoch   2, Step:   321600, Batch Loss:     1.822328, Tokens per Sec:    15296, Lr: 0.000300\n",
            "2021-07-25 07:53:16,554 - INFO - joeynmt.training - Epoch   2, Step:   321800, Batch Loss:     1.741412, Tokens per Sec:    15324, Lr: 0.000300\n",
            "2021-07-25 07:53:44,589 - INFO - joeynmt.training - Epoch   2, Step:   322000, Batch Loss:     1.672051, Tokens per Sec:    15613, Lr: 0.000300\n",
            "2021-07-25 07:54:12,707 - INFO - joeynmt.training - Epoch   2, Step:   322200, Batch Loss:     1.911581, Tokens per Sec:    15479, Lr: 0.000300\n",
            "2021-07-25 07:54:41,098 - INFO - joeynmt.training - Epoch   2, Step:   322400, Batch Loss:     1.790899, Tokens per Sec:    15526, Lr: 0.000300\n",
            "2021-07-25 07:55:08,931 - INFO - joeynmt.training - Epoch   2, Step:   322600, Batch Loss:     1.947911, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-25 07:55:36,957 - INFO - joeynmt.training - Epoch   2, Step:   322800, Batch Loss:     2.201395, Tokens per Sec:    15347, Lr: 0.000300\n",
            "2021-07-25 07:56:04,655 - INFO - joeynmt.training - Epoch   2, Step:   323000, Batch Loss:     2.170336, Tokens per Sec:    15515, Lr: 0.000300\n",
            "2021-07-25 07:56:32,626 - INFO - joeynmt.training - Epoch   2, Step:   323200, Batch Loss:     1.838372, Tokens per Sec:    15605, Lr: 0.000300\n",
            "2021-07-25 07:57:00,559 - INFO - joeynmt.training - Epoch   2, Step:   323400, Batch Loss:     1.711408, Tokens per Sec:    15632, Lr: 0.000300\n",
            "2021-07-25 07:57:28,450 - INFO - joeynmt.training - Epoch   2, Step:   323600, Batch Loss:     2.135782, Tokens per Sec:    15707, Lr: 0.000300\n",
            "2021-07-25 07:57:56,269 - INFO - joeynmt.training - Epoch   2, Step:   323800, Batch Loss:     1.866992, Tokens per Sec:    15197, Lr: 0.000300\n",
            "2021-07-25 07:58:24,382 - INFO - joeynmt.training - Epoch   2, Step:   324000, Batch Loss:     1.664475, Tokens per Sec:    15754, Lr: 0.000300\n",
            "2021-07-25 07:58:52,323 - INFO - joeynmt.training - Epoch   2, Step:   324200, Batch Loss:     1.805687, Tokens per Sec:    15448, Lr: 0.000300\n",
            "2021-07-25 07:59:20,631 - INFO - joeynmt.training - Epoch   2, Step:   324400, Batch Loss:     1.845873, Tokens per Sec:    15501, Lr: 0.000300\n",
            "2021-07-25 07:59:48,914 - INFO - joeynmt.training - Epoch   2, Step:   324600, Batch Loss:     1.904454, Tokens per Sec:    15684, Lr: 0.000300\n",
            "2021-07-25 08:00:16,705 - INFO - joeynmt.training - Epoch   2, Step:   324800, Batch Loss:     1.660970, Tokens per Sec:    15682, Lr: 0.000300\n",
            "2021-07-25 08:00:44,775 - INFO - joeynmt.training - Epoch   2, Step:   325000, Batch Loss:     1.664759, Tokens per Sec:    15502, Lr: 0.000300\n",
            "2021-07-25 08:02:28,044 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:02:28,044 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:02:28,045 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:02:29,791 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:02:29,792 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:02:29,792 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:02:29,792 - INFO - joeynmt.training - \tHypothesis: If you do good and suffering , that is acceptable to God . ”\n",
            "2021-07-25 08:02:29,793 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:02:29,793 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:02:29,793 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:02:29,794 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-25 08:02:29,794 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:02:29,794 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:02:29,794 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:02:29,795 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 08:02:29,795 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:02:29,795 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:02:29,795 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:02:29,796 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 08:02:29,796 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   325000: bleu:  19.56, loss: 173408.6094, ppl:   6.8781, duration: 105.0207s\n",
            "2021-07-25 08:02:58,082 - INFO - joeynmt.training - Epoch   2, Step:   325200, Batch Loss:     1.858728, Tokens per Sec:    15393, Lr: 0.000300\n",
            "2021-07-25 08:03:26,014 - INFO - joeynmt.training - Epoch   2, Step:   325400, Batch Loss:     1.710192, Tokens per Sec:    15603, Lr: 0.000300\n",
            "2021-07-25 08:03:54,014 - INFO - joeynmt.training - Epoch   2, Step:   325600, Batch Loss:     1.690607, Tokens per Sec:    15479, Lr: 0.000300\n",
            "2021-07-25 08:04:22,097 - INFO - joeynmt.training - Epoch   2, Step:   325800, Batch Loss:     1.493922, Tokens per Sec:    15524, Lr: 0.000300\n",
            "2021-07-25 08:04:50,057 - INFO - joeynmt.training - Epoch   2, Step:   326000, Batch Loss:     1.863224, Tokens per Sec:    15719, Lr: 0.000300\n",
            "2021-07-25 08:05:17,610 - INFO - joeynmt.training - Epoch   2, Step:   326200, Batch Loss:     1.614251, Tokens per Sec:    15502, Lr: 0.000300\n",
            "2021-07-25 08:05:25,207 - INFO - joeynmt.training - Epoch   2: total training loss 14713.34\n",
            "2021-07-25 08:05:25,208 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-25 08:05:46,838 - INFO - joeynmt.training - Epoch   3, Step:   326400, Batch Loss:     1.616742, Tokens per Sec:    14895, Lr: 0.000300\n",
            "2021-07-25 08:06:14,476 - INFO - joeynmt.training - Epoch   3, Step:   326600, Batch Loss:     1.795981, Tokens per Sec:    15558, Lr: 0.000300\n",
            "2021-07-25 08:06:42,655 - INFO - joeynmt.training - Epoch   3, Step:   326800, Batch Loss:     1.526122, Tokens per Sec:    15500, Lr: 0.000300\n",
            "2021-07-25 08:07:10,488 - INFO - joeynmt.training - Epoch   3, Step:   327000, Batch Loss:     1.855294, Tokens per Sec:    15728, Lr: 0.000300\n",
            "2021-07-25 08:07:38,217 - INFO - joeynmt.training - Epoch   3, Step:   327200, Batch Loss:     1.533221, Tokens per Sec:    15462, Lr: 0.000300\n",
            "2021-07-25 08:08:06,125 - INFO - joeynmt.training - Epoch   3, Step:   327400, Batch Loss:     1.602861, Tokens per Sec:    15745, Lr: 0.000300\n",
            "2021-07-25 08:08:33,968 - INFO - joeynmt.training - Epoch   3, Step:   327600, Batch Loss:     1.723433, Tokens per Sec:    15603, Lr: 0.000300\n",
            "2021-07-25 08:09:01,743 - INFO - joeynmt.training - Epoch   3, Step:   327800, Batch Loss:     2.044179, Tokens per Sec:    15417, Lr: 0.000300\n",
            "2021-07-25 08:09:29,359 - INFO - joeynmt.training - Epoch   3, Step:   328000, Batch Loss:     1.768658, Tokens per Sec:    15661, Lr: 0.000300\n",
            "2021-07-25 08:09:57,481 - INFO - joeynmt.training - Epoch   3, Step:   328200, Batch Loss:     1.752195, Tokens per Sec:    15521, Lr: 0.000300\n",
            "2021-07-25 08:10:25,371 - INFO - joeynmt.training - Epoch   3, Step:   328400, Batch Loss:     1.705764, Tokens per Sec:    15487, Lr: 0.000300\n",
            "2021-07-25 08:10:53,087 - INFO - joeynmt.training - Epoch   3, Step:   328600, Batch Loss:     1.787549, Tokens per Sec:    15522, Lr: 0.000300\n",
            "2021-07-25 08:11:20,967 - INFO - joeynmt.training - Epoch   3, Step:   328800, Batch Loss:     1.680366, Tokens per Sec:    15783, Lr: 0.000300\n",
            "2021-07-25 08:11:49,017 - INFO - joeynmt.training - Epoch   3, Step:   329000, Batch Loss:     1.912503, Tokens per Sec:    15681, Lr: 0.000300\n",
            "2021-07-25 08:12:16,763 - INFO - joeynmt.training - Epoch   3, Step:   329200, Batch Loss:     1.500397, Tokens per Sec:    15766, Lr: 0.000300\n",
            "2021-07-25 08:12:44,839 - INFO - joeynmt.training - Epoch   3, Step:   329400, Batch Loss:     1.762566, Tokens per Sec:    15389, Lr: 0.000300\n",
            "2021-07-25 08:13:12,566 - INFO - joeynmt.training - Epoch   3, Step:   329600, Batch Loss:     1.724256, Tokens per Sec:    15431, Lr: 0.000300\n",
            "2021-07-25 08:13:40,609 - INFO - joeynmt.training - Epoch   3, Step:   329800, Batch Loss:     1.487715, Tokens per Sec:    15476, Lr: 0.000300\n",
            "2021-07-25 08:14:08,765 - INFO - joeynmt.training - Epoch   3, Step:   330000, Batch Loss:     1.912207, Tokens per Sec:    15516, Lr: 0.000300\n",
            "2021-07-25 08:15:47,165 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:15:47,166 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:15:47,166 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:15:48,079 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 08:15:48,079 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 08:15:49,054 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:15:49,055 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:15:49,055 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:15:49,055 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 08:15:49,056 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:15:49,056 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:15:49,056 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:15:49,056 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he listened to the voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-25 08:15:49,057 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:15:49,058 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:15:49,058 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:15:49,058 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-25 08:15:49,058 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:15:49,059 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:15:49,059 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:15:49,059 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-25 08:15:49,059 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   330000: bleu:  19.88, loss: 171447.0781, ppl:   6.7297, duration: 100.2941s\n",
            "2021-07-25 08:16:17,376 - INFO - joeynmt.training - Epoch   3, Step:   330200, Batch Loss:     1.782699, Tokens per Sec:    15522, Lr: 0.000300\n",
            "2021-07-25 08:16:45,632 - INFO - joeynmt.training - Epoch   3, Step:   330400, Batch Loss:     1.650016, Tokens per Sec:    15535, Lr: 0.000300\n",
            "2021-07-25 08:17:13,709 - INFO - joeynmt.training - Epoch   3, Step:   330600, Batch Loss:     1.775083, Tokens per Sec:    15449, Lr: 0.000300\n",
            "2021-07-25 08:17:42,008 - INFO - joeynmt.training - Epoch   3, Step:   330800, Batch Loss:     1.850569, Tokens per Sec:    15269, Lr: 0.000300\n",
            "2021-07-25 08:18:10,178 - INFO - joeynmt.training - Epoch   3, Step:   331000, Batch Loss:     1.736793, Tokens per Sec:    15555, Lr: 0.000300\n",
            "2021-07-25 08:18:38,265 - INFO - joeynmt.training - Epoch   3, Step:   331200, Batch Loss:     1.923443, Tokens per Sec:    15485, Lr: 0.000300\n",
            "2021-07-25 08:19:05,977 - INFO - joeynmt.training - Epoch   3, Step:   331400, Batch Loss:     1.623843, Tokens per Sec:    15277, Lr: 0.000300\n",
            "2021-07-25 08:19:34,102 - INFO - joeynmt.training - Epoch   3, Step:   331600, Batch Loss:     1.830133, Tokens per Sec:    16031, Lr: 0.000300\n",
            "2021-07-25 08:20:02,140 - INFO - joeynmt.training - Epoch   3, Step:   331800, Batch Loss:     1.639699, Tokens per Sec:    15450, Lr: 0.000300\n",
            "2021-07-25 08:20:30,225 - INFO - joeynmt.training - Epoch   3, Step:   332000, Batch Loss:     1.791615, Tokens per Sec:    15801, Lr: 0.000300\n",
            "2021-07-25 08:20:58,287 - INFO - joeynmt.training - Epoch   3, Step:   332200, Batch Loss:     1.737480, Tokens per Sec:    15445, Lr: 0.000300\n",
            "2021-07-25 08:21:26,119 - INFO - joeynmt.training - Epoch   3, Step:   332400, Batch Loss:     1.655465, Tokens per Sec:    15478, Lr: 0.000300\n",
            "2021-07-25 08:21:53,650 - INFO - joeynmt.training - Epoch   3, Step:   332600, Batch Loss:     1.723263, Tokens per Sec:    15424, Lr: 0.000300\n",
            "2021-07-25 08:22:21,690 - INFO - joeynmt.training - Epoch   3, Step:   332800, Batch Loss:     1.623961, Tokens per Sec:    15643, Lr: 0.000300\n",
            "2021-07-25 08:22:50,007 - INFO - joeynmt.training - Epoch   3, Step:   333000, Batch Loss:     1.914970, Tokens per Sec:    15508, Lr: 0.000300\n",
            "2021-07-25 08:23:17,733 - INFO - joeynmt.training - Epoch   3, Step:   333200, Batch Loss:     1.719465, Tokens per Sec:    15409, Lr: 0.000300\n",
            "2021-07-25 08:23:45,793 - INFO - joeynmt.training - Epoch   3, Step:   333400, Batch Loss:     1.874891, Tokens per Sec:    15431, Lr: 0.000300\n",
            "2021-07-25 08:24:13,548 - INFO - joeynmt.training - Epoch   3, Step:   333600, Batch Loss:     1.797975, Tokens per Sec:    15640, Lr: 0.000300\n",
            "2021-07-25 08:24:41,453 - INFO - joeynmt.training - Epoch   3, Step:   333800, Batch Loss:     1.817281, Tokens per Sec:    15502, Lr: 0.000300\n",
            "2021-07-25 08:25:09,260 - INFO - joeynmt.training - Epoch   3, Step:   334000, Batch Loss:     1.911787, Tokens per Sec:    15538, Lr: 0.000300\n",
            "2021-07-25 08:25:36,885 - INFO - joeynmt.training - Epoch   3, Step:   334200, Batch Loss:     1.928591, Tokens per Sec:    15323, Lr: 0.000300\n",
            "2021-07-25 08:26:05,101 - INFO - joeynmt.training - Epoch   3, Step:   334400, Batch Loss:     1.866715, Tokens per Sec:    15580, Lr: 0.000300\n",
            "2021-07-25 08:26:33,068 - INFO - joeynmt.training - Epoch   3, Step:   334600, Batch Loss:     1.713952, Tokens per Sec:    15446, Lr: 0.000300\n",
            "2021-07-25 08:26:36,325 - INFO - joeynmt.training - Epoch   3: total training loss 14710.47\n",
            "2021-07-25 08:26:36,326 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-25 08:27:01,765 - INFO - joeynmt.training - Epoch   4, Step:   334800, Batch Loss:     1.598734, Tokens per Sec:    15071, Lr: 0.000300\n",
            "2021-07-25 08:27:29,560 - INFO - joeynmt.training - Epoch   4, Step:   335000, Batch Loss:     1.686210, Tokens per Sec:    15457, Lr: 0.000300\n",
            "2021-07-25 08:29:04,418 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:29:04,418 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:29:04,419 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:29:05,986 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:29:05,987 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:29:05,987 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:29:05,987 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 08:29:05,987 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:29:05,988 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:29:05,988 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:29:05,988 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he listened to the voice from heaven : “ This is my beloved Son whom I am approved . ”\n",
            "2021-07-25 08:29:05,988 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:29:05,989 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:29:05,989 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:29:05,989 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 08:29:05,989 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:29:05,989 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:29:05,990 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:29:05,990 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 08:29:05,990 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   335000: bleu:  20.04, loss: 171485.7500, ppl:   6.7326, duration: 96.4293s\n",
            "2021-07-25 08:29:33,436 - INFO - joeynmt.training - Epoch   4, Step:   335200, Batch Loss:     1.708188, Tokens per Sec:    15482, Lr: 0.000300\n",
            "2021-07-25 08:30:00,968 - INFO - joeynmt.training - Epoch   4, Step:   335400, Batch Loss:     1.522818, Tokens per Sec:    15499, Lr: 0.000300\n",
            "2021-07-25 08:30:28,197 - INFO - joeynmt.training - Epoch   4, Step:   335600, Batch Loss:     1.813693, Tokens per Sec:    15742, Lr: 0.000300\n",
            "2021-07-25 08:30:55,883 - INFO - joeynmt.training - Epoch   4, Step:   335800, Batch Loss:     1.857084, Tokens per Sec:    15648, Lr: 0.000300\n",
            "2021-07-25 08:31:23,317 - INFO - joeynmt.training - Epoch   4, Step:   336000, Batch Loss:     1.623146, Tokens per Sec:    16003, Lr: 0.000300\n",
            "2021-07-25 08:31:50,971 - INFO - joeynmt.training - Epoch   4, Step:   336200, Batch Loss:     2.002198, Tokens per Sec:    15727, Lr: 0.000300\n",
            "2021-07-25 08:32:18,653 - INFO - joeynmt.training - Epoch   4, Step:   336400, Batch Loss:     1.714280, Tokens per Sec:    15611, Lr: 0.000300\n",
            "2021-07-25 08:32:46,204 - INFO - joeynmt.training - Epoch   4, Step:   336600, Batch Loss:     1.850590, Tokens per Sec:    15718, Lr: 0.000300\n",
            "2021-07-25 08:33:13,777 - INFO - joeynmt.training - Epoch   4, Step:   336800, Batch Loss:     1.812397, Tokens per Sec:    15823, Lr: 0.000300\n",
            "2021-07-25 08:33:41,632 - INFO - joeynmt.training - Epoch   4, Step:   337000, Batch Loss:     1.642775, Tokens per Sec:    15593, Lr: 0.000300\n",
            "2021-07-25 08:34:09,245 - INFO - joeynmt.training - Epoch   4, Step:   337200, Batch Loss:     1.583516, Tokens per Sec:    15797, Lr: 0.000300\n",
            "2021-07-25 08:34:36,944 - INFO - joeynmt.training - Epoch   4, Step:   337400, Batch Loss:     1.728069, Tokens per Sec:    15502, Lr: 0.000300\n",
            "2021-07-25 08:35:04,599 - INFO - joeynmt.training - Epoch   4, Step:   337600, Batch Loss:     1.746362, Tokens per Sec:    15736, Lr: 0.000300\n",
            "2021-07-25 08:35:32,180 - INFO - joeynmt.training - Epoch   4, Step:   337800, Batch Loss:     1.856914, Tokens per Sec:    15875, Lr: 0.000300\n",
            "2021-07-25 08:35:59,759 - INFO - joeynmt.training - Epoch   4, Step:   338000, Batch Loss:     1.713488, Tokens per Sec:    15629, Lr: 0.000300\n",
            "2021-07-25 08:36:27,905 - INFO - joeynmt.training - Epoch   4, Step:   338200, Batch Loss:     1.767510, Tokens per Sec:    15717, Lr: 0.000300\n",
            "2021-07-25 08:36:56,103 - INFO - joeynmt.training - Epoch   4, Step:   338400, Batch Loss:     1.500335, Tokens per Sec:    15451, Lr: 0.000300\n",
            "2021-07-25 08:37:24,265 - INFO - joeynmt.training - Epoch   4, Step:   338600, Batch Loss:     1.772257, Tokens per Sec:    15936, Lr: 0.000300\n",
            "2021-07-25 08:37:51,824 - INFO - joeynmt.training - Epoch   4, Step:   338800, Batch Loss:     1.864499, Tokens per Sec:    15525, Lr: 0.000300\n",
            "2021-07-25 08:38:19,693 - INFO - joeynmt.training - Epoch   4, Step:   339000, Batch Loss:     1.775658, Tokens per Sec:    15600, Lr: 0.000300\n",
            "2021-07-25 08:38:47,904 - INFO - joeynmt.training - Epoch   4, Step:   339200, Batch Loss:     1.714832, Tokens per Sec:    15714, Lr: 0.000300\n",
            "2021-07-25 08:39:15,815 - INFO - joeynmt.training - Epoch   4, Step:   339400, Batch Loss:     1.883550, Tokens per Sec:    15762, Lr: 0.000300\n",
            "2021-07-25 08:39:44,086 - INFO - joeynmt.training - Epoch   4, Step:   339600, Batch Loss:     1.736570, Tokens per Sec:    15552, Lr: 0.000300\n",
            "2021-07-25 08:40:11,727 - INFO - joeynmt.training - Epoch   4, Step:   339800, Batch Loss:     1.791444, Tokens per Sec:    15584, Lr: 0.000300\n",
            "2021-07-25 08:40:39,896 - INFO - joeynmt.training - Epoch   4, Step:   340000, Batch Loss:     1.688226, Tokens per Sec:    15732, Lr: 0.000300\n",
            "2021-07-25 08:42:13,825 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:42:13,826 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:42:13,826 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:42:15,485 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:42:15,486 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:42:15,486 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:42:15,486 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , that is acceptable to God . ”\n",
            "2021-07-25 08:42:15,486 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:42:15,487 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:42:15,487 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:42:15,487 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 08:42:15,487 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:42:15,488 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:42:15,488 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:42:15,488 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 08:42:15,488 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:42:15,489 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:42:15,489 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:42:15,489 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 08:42:15,489 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   340000: bleu:  19.88, loss: 171482.8906, ppl:   6.7324, duration: 95.5929s\n",
            "2021-07-25 08:42:43,554 - INFO - joeynmt.training - Epoch   4, Step:   340200, Batch Loss:     1.963149, Tokens per Sec:    15533, Lr: 0.000300\n",
            "2021-07-25 08:43:11,603 - INFO - joeynmt.training - Epoch   4, Step:   340400, Batch Loss:     1.935392, Tokens per Sec:    15585, Lr: 0.000300\n",
            "2021-07-25 08:43:39,197 - INFO - joeynmt.training - Epoch   4, Step:   340600, Batch Loss:     1.681592, Tokens per Sec:    15338, Lr: 0.000300\n",
            "2021-07-25 08:44:06,825 - INFO - joeynmt.training - Epoch   4, Step:   340800, Batch Loss:     1.714866, Tokens per Sec:    15784, Lr: 0.000300\n",
            "2021-07-25 08:44:34,768 - INFO - joeynmt.training - Epoch   4, Step:   341000, Batch Loss:     1.735217, Tokens per Sec:    15576, Lr: 0.000300\n",
            "2021-07-25 08:45:02,358 - INFO - joeynmt.training - Epoch   4, Step:   341200, Batch Loss:     2.077015, Tokens per Sec:    15626, Lr: 0.000300\n",
            "2021-07-25 08:45:30,062 - INFO - joeynmt.training - Epoch   4, Step:   341400, Batch Loss:     1.795003, Tokens per Sec:    15513, Lr: 0.000300\n",
            "2021-07-25 08:45:57,966 - INFO - joeynmt.training - Epoch   4, Step:   341600, Batch Loss:     1.667498, Tokens per Sec:    15691, Lr: 0.000300\n",
            "2021-07-25 08:46:25,677 - INFO - joeynmt.training - Epoch   4, Step:   341800, Batch Loss:     1.748292, Tokens per Sec:    15631, Lr: 0.000300\n",
            "2021-07-25 08:46:53,593 - INFO - joeynmt.training - Epoch   4, Step:   342000, Batch Loss:     1.954593, Tokens per Sec:    15523, Lr: 0.000300\n",
            "2021-07-25 08:47:21,691 - INFO - joeynmt.training - Epoch   4, Step:   342200, Batch Loss:     1.835224, Tokens per Sec:    15828, Lr: 0.000300\n",
            "2021-07-25 08:47:49,440 - INFO - joeynmt.training - Epoch   4, Step:   342400, Batch Loss:     1.844136, Tokens per Sec:    15426, Lr: 0.000300\n",
            "2021-07-25 08:48:17,085 - INFO - joeynmt.training - Epoch   4, Step:   342600, Batch Loss:     1.839807, Tokens per Sec:    15510, Lr: 0.000300\n",
            "2021-07-25 08:48:45,092 - INFO - joeynmt.training - Epoch   4, Step:   342800, Batch Loss:     1.799744, Tokens per Sec:    15678, Lr: 0.000300\n",
            "2021-07-25 08:49:10,920 - INFO - joeynmt.training - Epoch   4: total training loss 14667.58\n",
            "2021-07-25 08:49:10,921 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-25 08:49:13,573 - INFO - joeynmt.training - Epoch   5, Step:   343000, Batch Loss:     1.818339, Tokens per Sec:     9803, Lr: 0.000300\n",
            "2021-07-25 08:49:41,435 - INFO - joeynmt.training - Epoch   5, Step:   343200, Batch Loss:     1.772598, Tokens per Sec:    15598, Lr: 0.000300\n",
            "2021-07-25 08:50:09,208 - INFO - joeynmt.training - Epoch   5, Step:   343400, Batch Loss:     1.800159, Tokens per Sec:    15761, Lr: 0.000300\n",
            "2021-07-25 08:50:37,213 - INFO - joeynmt.training - Epoch   5, Step:   343600, Batch Loss:     1.824041, Tokens per Sec:    15701, Lr: 0.000300\n",
            "2021-07-25 08:51:05,209 - INFO - joeynmt.training - Epoch   5, Step:   343800, Batch Loss:     1.579475, Tokens per Sec:    15796, Lr: 0.000300\n",
            "2021-07-25 08:51:33,075 - INFO - joeynmt.training - Epoch   5, Step:   344000, Batch Loss:     1.715859, Tokens per Sec:    15708, Lr: 0.000300\n",
            "2021-07-25 08:52:01,023 - INFO - joeynmt.training - Epoch   5, Step:   344200, Batch Loss:     1.640589, Tokens per Sec:    15415, Lr: 0.000300\n",
            "2021-07-25 08:52:28,788 - INFO - joeynmt.training - Epoch   5, Step:   344400, Batch Loss:     1.775513, Tokens per Sec:    15804, Lr: 0.000300\n",
            "2021-07-25 08:52:56,800 - INFO - joeynmt.training - Epoch   5, Step:   344600, Batch Loss:     1.818298, Tokens per Sec:    15525, Lr: 0.000300\n",
            "2021-07-25 08:53:24,527 - INFO - joeynmt.training - Epoch   5, Step:   344800, Batch Loss:     1.798254, Tokens per Sec:    15726, Lr: 0.000300\n",
            "2021-07-25 08:53:52,584 - INFO - joeynmt.training - Epoch   5, Step:   345000, Batch Loss:     1.842729, Tokens per Sec:    15750, Lr: 0.000300\n",
            "2021-07-25 08:55:32,835 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:55:32,835 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:55:32,836 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:55:33,826 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 08:55:33,827 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 08:55:34,607 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:55:34,607 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:55:34,608 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:55:34,608 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 08:55:34,608 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:55:34,608 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:55:34,609 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:55:34,609 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 08:55:34,609 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:55:34,609 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:55:34,610 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:55:34,610 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 08:55:34,610 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:55:34,611 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:55:34,611 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:55:34,611 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 08:55:34,611 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   345000: bleu:  20.04, loss: 171212.5938, ppl:   6.7122, duration: 102.0265s\n",
            "2021-07-25 08:56:02,717 - INFO - joeynmt.training - Epoch   5, Step:   345200, Batch Loss:     1.387383, Tokens per Sec:    15550, Lr: 0.000300\n",
            "2021-07-25 08:56:30,202 - INFO - joeynmt.training - Epoch   5, Step:   345400, Batch Loss:     1.739125, Tokens per Sec:    15531, Lr: 0.000300\n",
            "2021-07-25 08:56:57,890 - INFO - joeynmt.training - Epoch   5, Step:   345600, Batch Loss:     1.799055, Tokens per Sec:    15551, Lr: 0.000300\n",
            "2021-07-25 08:57:25,526 - INFO - joeynmt.training - Epoch   5, Step:   345800, Batch Loss:     1.743907, Tokens per Sec:    15764, Lr: 0.000300\n",
            "2021-07-25 08:57:53,429 - INFO - joeynmt.training - Epoch   5, Step:   346000, Batch Loss:     1.678441, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-25 08:58:21,169 - INFO - joeynmt.training - Epoch   5, Step:   346200, Batch Loss:     1.549335, Tokens per Sec:    15586, Lr: 0.000300\n",
            "2021-07-25 08:58:49,110 - INFO - joeynmt.training - Epoch   5, Step:   346400, Batch Loss:     1.592615, Tokens per Sec:    15709, Lr: 0.000300\n",
            "2021-07-25 08:59:17,085 - INFO - joeynmt.training - Epoch   5, Step:   346600, Batch Loss:     1.821871, Tokens per Sec:    15515, Lr: 0.000300\n",
            "2021-07-25 08:59:44,852 - INFO - joeynmt.training - Epoch   5, Step:   346800, Batch Loss:     1.596559, Tokens per Sec:    15682, Lr: 0.000300\n",
            "2021-07-25 09:00:12,641 - INFO - joeynmt.training - Epoch   5, Step:   347000, Batch Loss:     1.792665, Tokens per Sec:    15841, Lr: 0.000300\n",
            "2021-07-25 09:00:40,359 - INFO - joeynmt.training - Epoch   5, Step:   347200, Batch Loss:     1.653060, Tokens per Sec:    15703, Lr: 0.000300\n",
            "2021-07-25 09:01:08,080 - INFO - joeynmt.training - Epoch   5, Step:   347400, Batch Loss:     2.006492, Tokens per Sec:    15564, Lr: 0.000300\n",
            "2021-07-25 09:01:35,537 - INFO - joeynmt.training - Epoch   5, Step:   347600, Batch Loss:     1.691470, Tokens per Sec:    15633, Lr: 0.000300\n",
            "2021-07-25 09:02:03,144 - INFO - joeynmt.training - Epoch   5, Step:   347800, Batch Loss:     1.780120, Tokens per Sec:    15482, Lr: 0.000300\n",
            "2021-07-25 09:02:30,877 - INFO - joeynmt.training - Epoch   5, Step:   348000, Batch Loss:     1.740463, Tokens per Sec:    15663, Lr: 0.000300\n",
            "2021-07-25 09:02:58,755 - INFO - joeynmt.training - Epoch   5, Step:   348200, Batch Loss:     1.642200, Tokens per Sec:    15538, Lr: 0.000300\n",
            "2021-07-25 09:03:26,468 - INFO - joeynmt.training - Epoch   5, Step:   348400, Batch Loss:     1.890720, Tokens per Sec:    15760, Lr: 0.000300\n",
            "2021-07-25 09:03:54,196 - INFO - joeynmt.training - Epoch   5, Step:   348600, Batch Loss:     1.762097, Tokens per Sec:    15568, Lr: 0.000300\n",
            "2021-07-25 09:04:22,116 - INFO - joeynmt.training - Epoch   5, Step:   348800, Batch Loss:     1.869457, Tokens per Sec:    15641, Lr: 0.000300\n",
            "2021-07-25 09:04:49,798 - INFO - joeynmt.training - Epoch   5, Step:   349000, Batch Loss:     1.850888, Tokens per Sec:    15640, Lr: 0.000300\n",
            "2021-07-25 09:05:17,452 - INFO - joeynmt.training - Epoch   5, Step:   349200, Batch Loss:     1.879314, Tokens per Sec:    15660, Lr: 0.000300\n",
            "2021-07-25 09:05:45,163 - INFO - joeynmt.training - Epoch   5, Step:   349400, Batch Loss:     1.809568, Tokens per Sec:    15559, Lr: 0.000300\n",
            "2021-07-25 09:06:12,869 - INFO - joeynmt.training - Epoch   5, Step:   349600, Batch Loss:     1.602496, Tokens per Sec:    15604, Lr: 0.000300\n",
            "2021-07-25 09:06:41,048 - INFO - joeynmt.training - Epoch   5, Step:   349800, Batch Loss:     1.306333, Tokens per Sec:    15601, Lr: 0.000300\n",
            "2021-07-25 09:07:08,842 - INFO - joeynmt.training - Epoch   5, Step:   350000, Batch Loss:     1.656397, Tokens per Sec:    15726, Lr: 0.000300\n",
            "2021-07-25 09:08:46,276 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:08:46,276 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:08:46,277 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:08:47,254 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 09:08:47,254 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 09:08:48,022 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:08:48,022 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:08:48,022 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:08:48,023 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , that is what is acceptable to God . ”\n",
            "2021-07-25 09:08:48,023 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:08:48,023 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:08:48,023 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:08:48,024 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he listened to the voice from heaven , saying : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 09:08:48,024 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:08:48,024 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:08:48,024 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:08:48,025 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 09:08:48,025 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:08:48,025 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:08:48,025 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:08:48,025 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 09:08:48,026 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   350000: bleu:  20.11, loss: 171060.1719, ppl:   6.7008, duration: 99.1832s\n",
            "2021-07-25 09:09:16,233 - INFO - joeynmt.training - Epoch   5, Step:   350200, Batch Loss:     1.776142, Tokens per Sec:    15623, Lr: 0.000300\n",
            "2021-07-25 09:09:43,833 - INFO - joeynmt.training - Epoch   5, Step:   350400, Batch Loss:     1.647796, Tokens per Sec:    15490, Lr: 0.000300\n",
            "2021-07-25 09:10:11,852 - INFO - joeynmt.training - Epoch   5, Step:   350600, Batch Loss:     1.596314, Tokens per Sec:    15669, Lr: 0.000300\n",
            "2021-07-25 09:10:39,767 - INFO - joeynmt.training - Epoch   5, Step:   350800, Batch Loss:     1.687937, Tokens per Sec:    15636, Lr: 0.000300\n",
            "2021-07-25 09:11:07,524 - INFO - joeynmt.training - Epoch   5, Step:   351000, Batch Loss:     1.828550, Tokens per Sec:    15804, Lr: 0.000300\n",
            "2021-07-25 09:11:35,627 - INFO - joeynmt.training - Epoch   5, Step:   351200, Batch Loss:     1.726051, Tokens per Sec:    15525, Lr: 0.000300\n",
            "2021-07-25 09:11:55,620 - INFO - joeynmt.training - Epoch   5: total training loss 14640.70\n",
            "2021-07-25 09:11:55,620 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-25 09:12:04,041 - INFO - joeynmt.training - Epoch   6, Step:   351400, Batch Loss:     1.556322, Tokens per Sec:    13871, Lr: 0.000300\n",
            "2021-07-25 09:12:32,145 - INFO - joeynmt.training - Epoch   6, Step:   351600, Batch Loss:     1.623038, Tokens per Sec:    15655, Lr: 0.000300\n",
            "2021-07-25 09:13:00,013 - INFO - joeynmt.training - Epoch   6, Step:   351800, Batch Loss:     1.921432, Tokens per Sec:    15764, Lr: 0.000300\n",
            "2021-07-25 09:13:27,656 - INFO - joeynmt.training - Epoch   6, Step:   352000, Batch Loss:     1.571009, Tokens per Sec:    15586, Lr: 0.000300\n",
            "2021-07-25 09:13:55,659 - INFO - joeynmt.training - Epoch   6, Step:   352200, Batch Loss:     1.653166, Tokens per Sec:    15568, Lr: 0.000300\n",
            "2021-07-25 09:14:23,297 - INFO - joeynmt.training - Epoch   6, Step:   352400, Batch Loss:     1.570343, Tokens per Sec:    15872, Lr: 0.000300\n",
            "2021-07-25 09:14:51,312 - INFO - joeynmt.training - Epoch   6, Step:   352600, Batch Loss:     1.742044, Tokens per Sec:    15727, Lr: 0.000300\n",
            "2021-07-25 09:15:18,931 - INFO - joeynmt.training - Epoch   6, Step:   352800, Batch Loss:     1.717849, Tokens per Sec:    15518, Lr: 0.000300\n",
            "2021-07-25 09:15:46,639 - INFO - joeynmt.training - Epoch   6, Step:   353000, Batch Loss:     1.783083, Tokens per Sec:    15553, Lr: 0.000300\n",
            "2021-07-25 09:16:14,263 - INFO - joeynmt.training - Epoch   6, Step:   353200, Batch Loss:     1.844173, Tokens per Sec:    15657, Lr: 0.000300\n",
            "2021-07-25 09:16:41,970 - INFO - joeynmt.training - Epoch   6, Step:   353400, Batch Loss:     1.431176, Tokens per Sec:    15373, Lr: 0.000300\n",
            "2021-07-25 09:17:09,708 - INFO - joeynmt.training - Epoch   6, Step:   353600, Batch Loss:     1.863098, Tokens per Sec:    15608, Lr: 0.000300\n",
            "2021-07-25 09:17:37,590 - INFO - joeynmt.training - Epoch   6, Step:   353800, Batch Loss:     1.873533, Tokens per Sec:    15643, Lr: 0.000300\n",
            "2021-07-25 09:18:05,462 - INFO - joeynmt.training - Epoch   6, Step:   354000, Batch Loss:     1.751210, Tokens per Sec:    15665, Lr: 0.000300\n",
            "2021-07-25 09:18:33,308 - INFO - joeynmt.training - Epoch   6, Step:   354200, Batch Loss:     1.422541, Tokens per Sec:    15588, Lr: 0.000300\n",
            "2021-07-25 09:19:01,167 - INFO - joeynmt.training - Epoch   6, Step:   354400, Batch Loss:     1.780854, Tokens per Sec:    15590, Lr: 0.000300\n",
            "2021-07-25 09:19:28,967 - INFO - joeynmt.training - Epoch   6, Step:   354600, Batch Loss:     1.911590, Tokens per Sec:    15682, Lr: 0.000300\n",
            "2021-07-25 09:19:56,909 - INFO - joeynmt.training - Epoch   6, Step:   354800, Batch Loss:     1.795120, Tokens per Sec:    15564, Lr: 0.000300\n",
            "2021-07-25 09:20:24,304 - INFO - joeynmt.training - Epoch   6, Step:   355000, Batch Loss:     1.817188, Tokens per Sec:    15703, Lr: 0.000300\n",
            "2021-07-25 09:22:05,474 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:22:05,474 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:22:05,474 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:22:07,104 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:22:07,105 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:22:07,105 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:22:07,105 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , that is acceptable to God . ”\n",
            "2021-07-25 09:22:07,105 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:22:07,106 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:22:07,106 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:22:07,106 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he listened to the voice from heaven : “ This is my beloved Son whom I am approved . ”\n",
            "2021-07-25 09:22:07,107 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:22:07,107 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:22:07,107 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:22:07,107 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 09:22:07,107 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:22:07,108 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:22:07,108 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:22:07,108 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 09:22:07,108 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   355000: bleu:  20.25, loss: 171558.7969, ppl:   6.7381, duration: 102.8040s\n",
            "2021-07-25 09:22:35,156 - INFO - joeynmt.training - Epoch   6, Step:   355200, Batch Loss:     1.551220, Tokens per Sec:    15182, Lr: 0.000300\n",
            "2021-07-25 09:23:02,951 - INFO - joeynmt.training - Epoch   6, Step:   355400, Batch Loss:     2.246491, Tokens per Sec:    15739, Lr: 0.000300\n",
            "2021-07-25 09:23:30,721 - INFO - joeynmt.training - Epoch   6, Step:   355600, Batch Loss:     1.899591, Tokens per Sec:    15823, Lr: 0.000300\n",
            "2021-07-25 09:23:58,250 - INFO - joeynmt.training - Epoch   6, Step:   355800, Batch Loss:     1.946037, Tokens per Sec:    15359, Lr: 0.000300\n",
            "2021-07-25 09:24:25,749 - INFO - joeynmt.training - Epoch   6, Step:   356000, Batch Loss:     1.537315, Tokens per Sec:    15645, Lr: 0.000300\n",
            "2021-07-25 09:24:53,566 - INFO - joeynmt.training - Epoch   6, Step:   356200, Batch Loss:     1.846909, Tokens per Sec:    15603, Lr: 0.000300\n",
            "2021-07-25 09:25:21,380 - INFO - joeynmt.training - Epoch   6, Step:   356400, Batch Loss:     1.697544, Tokens per Sec:    15778, Lr: 0.000300\n",
            "2021-07-25 09:25:49,440 - INFO - joeynmt.training - Epoch   6, Step:   356600, Batch Loss:     1.944575, Tokens per Sec:    15683, Lr: 0.000300\n",
            "2021-07-25 09:26:17,320 - INFO - joeynmt.training - Epoch   6, Step:   356800, Batch Loss:     1.785957, Tokens per Sec:    15567, Lr: 0.000300\n",
            "2021-07-25 09:26:45,156 - INFO - joeynmt.training - Epoch   6, Step:   357000, Batch Loss:     1.829836, Tokens per Sec:    15381, Lr: 0.000300\n",
            "2021-07-25 09:27:12,921 - INFO - joeynmt.training - Epoch   6, Step:   357200, Batch Loss:     1.667762, Tokens per Sec:    15644, Lr: 0.000300\n",
            "2021-07-25 09:27:40,819 - INFO - joeynmt.training - Epoch   6, Step:   357400, Batch Loss:     1.798594, Tokens per Sec:    15455, Lr: 0.000300\n",
            "2021-07-25 09:28:08,750 - INFO - joeynmt.training - Epoch   6, Step:   357600, Batch Loss:     1.819208, Tokens per Sec:    15977, Lr: 0.000300\n",
            "2021-07-25 09:28:36,582 - INFO - joeynmt.training - Epoch   6, Step:   357800, Batch Loss:     1.947672, Tokens per Sec:    15638, Lr: 0.000300\n",
            "2021-07-25 09:29:04,404 - INFO - joeynmt.training - Epoch   6, Step:   358000, Batch Loss:     1.620129, Tokens per Sec:    15723, Lr: 0.000300\n",
            "2021-07-25 09:29:32,107 - INFO - joeynmt.training - Epoch   6, Step:   358200, Batch Loss:     1.988767, Tokens per Sec:    15521, Lr: 0.000300\n",
            "2021-07-25 09:29:59,911 - INFO - joeynmt.training - Epoch   6, Step:   358400, Batch Loss:     1.725660, Tokens per Sec:    15466, Lr: 0.000300\n",
            "2021-07-25 09:30:27,590 - INFO - joeynmt.training - Epoch   6, Step:   358600, Batch Loss:     1.823871, Tokens per Sec:    15754, Lr: 0.000300\n",
            "2021-07-25 09:30:55,613 - INFO - joeynmt.training - Epoch   6, Step:   358800, Batch Loss:     1.441772, Tokens per Sec:    15475, Lr: 0.000300\n",
            "2021-07-25 09:31:23,263 - INFO - joeynmt.training - Epoch   6, Step:   359000, Batch Loss:     1.846083, Tokens per Sec:    15574, Lr: 0.000300\n",
            "2021-07-25 09:31:51,420 - INFO - joeynmt.training - Epoch   6, Step:   359200, Batch Loss:     1.641346, Tokens per Sec:    15886, Lr: 0.000300\n",
            "2021-07-25 09:32:19,333 - INFO - joeynmt.training - Epoch   6, Step:   359400, Batch Loss:     1.815680, Tokens per Sec:    15398, Lr: 0.000300\n",
            "2021-07-25 09:32:47,615 - INFO - joeynmt.training - Epoch   6, Step:   359600, Batch Loss:     1.663640, Tokens per Sec:    15403, Lr: 0.000300\n",
            "2021-07-25 09:33:03,917 - INFO - joeynmt.training - Epoch   6: total training loss 14644.23\n",
            "2021-07-25 09:33:03,917 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-25 09:33:16,518 - INFO - joeynmt.training - Epoch   7, Step:   359800, Batch Loss:     1.754205, Tokens per Sec:    14423, Lr: 0.000300\n",
            "2021-07-25 09:33:44,322 - INFO - joeynmt.training - Epoch   7, Step:   360000, Batch Loss:     1.575358, Tokens per Sec:    15408, Lr: 0.000300\n",
            "2021-07-25 09:35:18,368 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:35:18,369 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:35:18,369 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:35:20,065 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:35:20,066 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:35:20,066 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:35:20,066 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , this is acceptable to God . ”\n",
            "2021-07-25 09:35:20,066 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:35:20,067 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:35:20,067 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:35:20,067 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I am approved . ”\n",
            "2021-07-25 09:35:20,067 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:35:20,068 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:35:20,068 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:35:20,068 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 09:35:20,068 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:35:20,068 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:35:20,069 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:35:20,069 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 09:35:20,069 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   360000: bleu:  19.80, loss: 171861.8281, ppl:   6.7608, duration: 95.7465s\n",
            "2021-07-25 09:35:48,491 - INFO - joeynmt.training - Epoch   7, Step:   360200, Batch Loss:     1.792199, Tokens per Sec:    15434, Lr: 0.000300\n",
            "2021-07-25 09:36:16,588 - INFO - joeynmt.training - Epoch   7, Step:   360400, Batch Loss:     1.532472, Tokens per Sec:    15710, Lr: 0.000300\n",
            "2021-07-25 09:36:44,611 - INFO - joeynmt.training - Epoch   7, Step:   360600, Batch Loss:     1.559147, Tokens per Sec:    15614, Lr: 0.000300\n",
            "2021-07-25 09:37:12,771 - INFO - joeynmt.training - Epoch   7, Step:   360800, Batch Loss:     1.851383, Tokens per Sec:    15255, Lr: 0.000300\n",
            "2021-07-25 09:37:40,507 - INFO - joeynmt.training - Epoch   7, Step:   361000, Batch Loss:     1.715540, Tokens per Sec:    15376, Lr: 0.000300\n",
            "2021-07-25 09:38:08,462 - INFO - joeynmt.training - Epoch   7, Step:   361200, Batch Loss:     1.737316, Tokens per Sec:    15716, Lr: 0.000300\n",
            "2021-07-25 09:38:36,663 - INFO - joeynmt.training - Epoch   7, Step:   361400, Batch Loss:     1.723753, Tokens per Sec:    15506, Lr: 0.000300\n",
            "2021-07-25 09:39:04,728 - INFO - joeynmt.training - Epoch   7, Step:   361600, Batch Loss:     1.818663, Tokens per Sec:    15514, Lr: 0.000300\n",
            "2021-07-25 09:39:33,045 - INFO - joeynmt.training - Epoch   7, Step:   361800, Batch Loss:     1.785910, Tokens per Sec:    15614, Lr: 0.000300\n",
            "2021-07-25 09:40:00,991 - INFO - joeynmt.training - Epoch   7, Step:   362000, Batch Loss:     1.865456, Tokens per Sec:    15630, Lr: 0.000300\n",
            "2021-07-25 09:40:29,035 - INFO - joeynmt.training - Epoch   7, Step:   362200, Batch Loss:     1.787630, Tokens per Sec:    15654, Lr: 0.000300\n",
            "2021-07-25 09:40:56,867 - INFO - joeynmt.training - Epoch   7, Step:   362400, Batch Loss:     1.684216, Tokens per Sec:    15159, Lr: 0.000300\n",
            "2021-07-25 09:41:24,705 - INFO - joeynmt.training - Epoch   7, Step:   362600, Batch Loss:     1.843484, Tokens per Sec:    15605, Lr: 0.000300\n",
            "2021-07-25 09:41:52,580 - INFO - joeynmt.training - Epoch   7, Step:   362800, Batch Loss:     1.851140, Tokens per Sec:    15546, Lr: 0.000300\n",
            "2021-07-25 09:42:20,259 - INFO - joeynmt.training - Epoch   7, Step:   363000, Batch Loss:     1.903583, Tokens per Sec:    15371, Lr: 0.000300\n",
            "2021-07-25 09:42:48,222 - INFO - joeynmt.training - Epoch   7, Step:   363200, Batch Loss:     1.725241, Tokens per Sec:    15663, Lr: 0.000300\n",
            "2021-07-25 09:43:16,472 - INFO - joeynmt.training - Epoch   7, Step:   363400, Batch Loss:     1.605452, Tokens per Sec:    15503, Lr: 0.000300\n",
            "2021-07-25 09:43:44,708 - INFO - joeynmt.training - Epoch   7, Step:   363600, Batch Loss:     1.932237, Tokens per Sec:    15652, Lr: 0.000300\n",
            "2021-07-25 09:44:12,285 - INFO - joeynmt.training - Epoch   7, Step:   363800, Batch Loss:     1.675137, Tokens per Sec:    15339, Lr: 0.000300\n",
            "2021-07-25 09:44:40,657 - INFO - joeynmt.training - Epoch   7, Step:   364000, Batch Loss:     1.753984, Tokens per Sec:    15198, Lr: 0.000300\n",
            "2021-07-25 09:45:08,739 - INFO - joeynmt.training - Epoch   7, Step:   364200, Batch Loss:     2.037714, Tokens per Sec:    15707, Lr: 0.000300\n",
            "2021-07-25 09:45:37,058 - INFO - joeynmt.training - Epoch   7, Step:   364400, Batch Loss:     1.751251, Tokens per Sec:    15565, Lr: 0.000300\n",
            "2021-07-25 09:46:05,202 - INFO - joeynmt.training - Epoch   7, Step:   364600, Batch Loss:     1.932082, Tokens per Sec:    15542, Lr: 0.000300\n",
            "2021-07-25 09:46:33,404 - INFO - joeynmt.training - Epoch   7, Step:   364800, Batch Loss:     1.741522, Tokens per Sec:    15572, Lr: 0.000300\n",
            "2021-07-25 09:47:01,515 - INFO - joeynmt.training - Epoch   7, Step:   365000, Batch Loss:     1.523510, Tokens per Sec:    15429, Lr: 0.000300\n",
            "2021-07-25 09:48:39,190 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:48:39,191 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:48:39,191 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:48:40,113 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 09:48:40,113 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 09:48:40,878 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:48:40,880 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:48:40,880 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:48:40,880 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 09:48:40,881 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:48:40,881 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:48:40,881 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:48:40,881 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus listened to the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 09:48:40,882 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:48:40,882 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:48:40,882 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:48:40,882 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 09:48:40,883 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:48:40,883 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:48:40,884 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:48:40,884 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 09:48:40,884 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   365000: bleu:  20.24, loss: 170904.7188, ppl:   6.6893, duration: 99.3693s\n",
            "2021-07-25 09:49:09,043 - INFO - joeynmt.training - Epoch   7, Step:   365200, Batch Loss:     1.769844, Tokens per Sec:    15518, Lr: 0.000300\n",
            "2021-07-25 09:49:37,340 - INFO - joeynmt.training - Epoch   7, Step:   365400, Batch Loss:     1.792868, Tokens per Sec:    15622, Lr: 0.000300\n",
            "2021-07-25 09:50:05,467 - INFO - joeynmt.training - Epoch   7, Step:   365600, Batch Loss:     1.653702, Tokens per Sec:    15580, Lr: 0.000300\n",
            "2021-07-25 09:50:33,189 - INFO - joeynmt.training - Epoch   7, Step:   365800, Batch Loss:     1.752983, Tokens per Sec:    15459, Lr: 0.000300\n",
            "2021-07-25 09:51:00,803 - INFO - joeynmt.training - Epoch   7, Step:   366000, Batch Loss:     1.894372, Tokens per Sec:    15262, Lr: 0.000300\n",
            "2021-07-25 09:51:28,608 - INFO - joeynmt.training - Epoch   7, Step:   366200, Batch Loss:     1.757573, Tokens per Sec:    15600, Lr: 0.000300\n",
            "2021-07-25 09:51:56,882 - INFO - joeynmt.training - Epoch   7, Step:   366400, Batch Loss:     1.865590, Tokens per Sec:    15506, Lr: 0.000300\n",
            "2021-07-25 09:52:24,913 - INFO - joeynmt.training - Epoch   7, Step:   366600, Batch Loss:     1.766293, Tokens per Sec:    15549, Lr: 0.000300\n",
            "2021-07-25 09:52:53,158 - INFO - joeynmt.training - Epoch   7, Step:   366800, Batch Loss:     1.781580, Tokens per Sec:    15470, Lr: 0.000300\n",
            "2021-07-25 09:53:20,934 - INFO - joeynmt.training - Epoch   7, Step:   367000, Batch Loss:     1.588145, Tokens per Sec:    15625, Lr: 0.000300\n",
            "2021-07-25 09:53:49,051 - INFO - joeynmt.training - Epoch   7, Step:   367200, Batch Loss:     1.741751, Tokens per Sec:    15714, Lr: 0.000300\n",
            "2021-07-25 09:54:16,920 - INFO - joeynmt.training - Epoch   7, Step:   367400, Batch Loss:     1.503993, Tokens per Sec:    15456, Lr: 0.000300\n",
            "2021-07-25 09:54:44,795 - INFO - joeynmt.training - Epoch   7, Step:   367600, Batch Loss:     1.609423, Tokens per Sec:    15359, Lr: 0.000300\n",
            "2021-07-25 09:55:12,489 - INFO - joeynmt.training - Epoch   7, Step:   367800, Batch Loss:     1.830505, Tokens per Sec:    15567, Lr: 0.000300\n",
            "2021-07-25 09:55:40,640 - INFO - joeynmt.training - Epoch   7, Step:   368000, Batch Loss:     1.775305, Tokens per Sec:    15529, Lr: 0.000300\n",
            "2021-07-25 09:55:51,571 - INFO - joeynmt.training - Epoch   7: total training loss 14596.63\n",
            "2021-07-25 09:55:51,572 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-25 09:56:09,413 - INFO - joeynmt.training - Epoch   8, Step:   368200, Batch Loss:     1.626328, Tokens per Sec:    14808, Lr: 0.000300\n",
            "2021-07-25 09:56:37,549 - INFO - joeynmt.training - Epoch   8, Step:   368400, Batch Loss:     1.726122, Tokens per Sec:    15588, Lr: 0.000300\n",
            "2021-07-25 09:57:05,595 - INFO - joeynmt.training - Epoch   8, Step:   368600, Batch Loss:     1.727558, Tokens per Sec:    15465, Lr: 0.000300\n",
            "2021-07-25 09:57:33,399 - INFO - joeynmt.training - Epoch   8, Step:   368800, Batch Loss:     1.654895, Tokens per Sec:    15438, Lr: 0.000300\n",
            "2021-07-25 09:58:01,767 - INFO - joeynmt.training - Epoch   8, Step:   369000, Batch Loss:     1.745895, Tokens per Sec:    15564, Lr: 0.000300\n",
            "2021-07-25 09:58:29,467 - INFO - joeynmt.training - Epoch   8, Step:   369200, Batch Loss:     1.780856, Tokens per Sec:    15507, Lr: 0.000300\n",
            "2021-07-25 09:58:57,729 - INFO - joeynmt.training - Epoch   8, Step:   369400, Batch Loss:     1.793978, Tokens per Sec:    15543, Lr: 0.000300\n",
            "2021-07-25 09:59:25,961 - INFO - joeynmt.training - Epoch   8, Step:   369600, Batch Loss:     1.716591, Tokens per Sec:    15562, Lr: 0.000300\n",
            "2021-07-25 09:59:53,725 - INFO - joeynmt.training - Epoch   8, Step:   369800, Batch Loss:     1.717909, Tokens per Sec:    15735, Lr: 0.000300\n",
            "2021-07-25 10:00:21,733 - INFO - joeynmt.training - Epoch   8, Step:   370000, Batch Loss:     1.764867, Tokens per Sec:    15765, Lr: 0.000300\n",
            "2021-07-25 10:01:57,421 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:01:57,422 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:01:57,422 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:01:58,322 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 10:01:58,322 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 10:01:59,126 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:01:59,127 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:01:59,127 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:01:59,128 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer , this is acceptable to God . ”\n",
            "2021-07-25 10:01:59,128 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:01:59,128 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:01:59,128 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:01:59,129 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 10:01:59,129 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:01:59,129 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:01:59,130 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:01:59,130 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 10:01:59,130 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:01:59,131 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:01:59,132 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:01:59,132 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to heal his family .\n",
            "2021-07-25 10:01:59,132 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   370000: bleu:  20.17, loss: 169840.3281, ppl:   6.6106, duration: 97.3987s\n",
            "2021-07-25 10:02:27,296 - INFO - joeynmt.training - Epoch   8, Step:   370200, Batch Loss:     1.635627, Tokens per Sec:    15365, Lr: 0.000300\n",
            "2021-07-25 10:02:55,691 - INFO - joeynmt.training - Epoch   8, Step:   370400, Batch Loss:     1.831865, Tokens per Sec:    15626, Lr: 0.000300\n",
            "2021-07-25 10:03:23,334 - INFO - joeynmt.training - Epoch   8, Step:   370600, Batch Loss:     1.560406, Tokens per Sec:    15466, Lr: 0.000300\n",
            "2021-07-25 10:03:50,963 - INFO - joeynmt.training - Epoch   8, Step:   370800, Batch Loss:     1.662526, Tokens per Sec:    15358, Lr: 0.000300\n",
            "2021-07-25 10:04:18,914 - INFO - joeynmt.training - Epoch   8, Step:   371000, Batch Loss:     1.391505, Tokens per Sec:    15360, Lr: 0.000300\n",
            "2021-07-25 10:04:46,987 - INFO - joeynmt.training - Epoch   8, Step:   371200, Batch Loss:     1.791182, Tokens per Sec:    15617, Lr: 0.000300\n",
            "2021-07-25 10:05:15,234 - INFO - joeynmt.training - Epoch   8, Step:   371400, Batch Loss:     1.852754, Tokens per Sec:    15672, Lr: 0.000300\n",
            "2021-07-25 10:05:43,257 - INFO - joeynmt.training - Epoch   8, Step:   371600, Batch Loss:     1.830805, Tokens per Sec:    15436, Lr: 0.000300\n",
            "2021-07-25 10:06:11,155 - INFO - joeynmt.training - Epoch   8, Step:   371800, Batch Loss:     1.564597, Tokens per Sec:    15504, Lr: 0.000300\n",
            "2021-07-25 10:06:39,811 - INFO - joeynmt.training - Epoch   8, Step:   372000, Batch Loss:     1.721229, Tokens per Sec:    15602, Lr: 0.000300\n",
            "2021-07-25 10:07:07,726 - INFO - joeynmt.training - Epoch   8, Step:   372200, Batch Loss:     1.761502, Tokens per Sec:    15558, Lr: 0.000300\n",
            "2021-07-25 10:07:35,647 - INFO - joeynmt.training - Epoch   8, Step:   372400, Batch Loss:     1.558015, Tokens per Sec:    15438, Lr: 0.000300\n",
            "2021-07-25 10:08:03,742 - INFO - joeynmt.training - Epoch   8, Step:   372600, Batch Loss:     1.852246, Tokens per Sec:    15594, Lr: 0.000300\n",
            "2021-07-25 10:08:31,532 - INFO - joeynmt.training - Epoch   8, Step:   372800, Batch Loss:     1.775344, Tokens per Sec:    15354, Lr: 0.000300\n",
            "2021-07-25 10:08:59,941 - INFO - joeynmt.training - Epoch   8, Step:   373000, Batch Loss:     1.826104, Tokens per Sec:    15516, Lr: 0.000300\n",
            "2021-07-25 10:09:27,900 - INFO - joeynmt.training - Epoch   8, Step:   373200, Batch Loss:     1.940658, Tokens per Sec:    15480, Lr: 0.000300\n",
            "2021-07-25 10:09:55,954 - INFO - joeynmt.training - Epoch   8, Step:   373400, Batch Loss:     1.507903, Tokens per Sec:    15274, Lr: 0.000300\n",
            "2021-07-25 10:10:24,155 - INFO - joeynmt.training - Epoch   8, Step:   373600, Batch Loss:     1.692706, Tokens per Sec:    15468, Lr: 0.000300\n",
            "2021-07-25 10:10:52,044 - INFO - joeynmt.training - Epoch   8, Step:   373800, Batch Loss:     1.630314, Tokens per Sec:    15420, Lr: 0.000300\n",
            "2021-07-25 10:11:20,133 - INFO - joeynmt.training - Epoch   8, Step:   374000, Batch Loss:     1.572070, Tokens per Sec:    15597, Lr: 0.000300\n",
            "2021-07-25 10:11:48,343 - INFO - joeynmt.training - Epoch   8, Step:   374200, Batch Loss:     1.598910, Tokens per Sec:    15533, Lr: 0.000300\n",
            "2021-07-25 10:12:16,451 - INFO - joeynmt.training - Epoch   8, Step:   374400, Batch Loss:     1.690888, Tokens per Sec:    15418, Lr: 0.000300\n",
            "2021-07-25 10:12:44,750 - INFO - joeynmt.training - Epoch   8, Step:   374600, Batch Loss:     1.747649, Tokens per Sec:    15480, Lr: 0.000300\n",
            "2021-07-25 10:13:12,657 - INFO - joeynmt.training - Epoch   8, Step:   374800, Batch Loss:     1.924666, Tokens per Sec:    15560, Lr: 0.000300\n",
            "2021-07-25 10:13:40,899 - INFO - joeynmt.training - Epoch   8, Step:   375000, Batch Loss:     1.943475, Tokens per Sec:    15415, Lr: 0.000300\n",
            "2021-07-25 10:15:19,847 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:15:19,848 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:15:19,848 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:15:21,542 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:15:21,543 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:15:21,543 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:15:21,544 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 10:15:21,544 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:15:21,544 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:15:21,545 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:15:21,545 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 10:15:21,545 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:15:21,545 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:15:21,545 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:15:21,546 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 10:15:21,546 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:15:21,546 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:15:21,546 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:15:21,547 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 10:15:21,547 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   375000: bleu:  20.40, loss: 169987.6094, ppl:   6.6214, duration: 100.6468s\n",
            "2021-07-25 10:15:50,299 - INFO - joeynmt.training - Epoch   8, Step:   375200, Batch Loss:     2.020640, Tokens per Sec:    15503, Lr: 0.000300\n",
            "2021-07-25 10:16:18,172 - INFO - joeynmt.training - Epoch   8, Step:   375400, Batch Loss:     1.858822, Tokens per Sec:    15340, Lr: 0.000300\n",
            "2021-07-25 10:16:46,069 - INFO - joeynmt.training - Epoch   8, Step:   375600, Batch Loss:     1.674677, Tokens per Sec:    14998, Lr: 0.000300\n",
            "2021-07-25 10:17:14,218 - INFO - joeynmt.training - Epoch   8, Step:   375800, Batch Loss:     1.891805, Tokens per Sec:    15759, Lr: 0.000300\n",
            "2021-07-25 10:17:42,479 - INFO - joeynmt.training - Epoch   8, Step:   376000, Batch Loss:     1.811897, Tokens per Sec:    15286, Lr: 0.000300\n",
            "2021-07-25 10:18:10,440 - INFO - joeynmt.training - Epoch   8, Step:   376200, Batch Loss:     1.533211, Tokens per Sec:    15537, Lr: 0.000300\n",
            "2021-07-25 10:18:38,437 - INFO - joeynmt.training - Epoch   8, Step:   376400, Batch Loss:     1.797469, Tokens per Sec:    15470, Lr: 0.000300\n",
            "2021-07-25 10:18:43,740 - INFO - joeynmt.training - Epoch   8: total training loss 14570.13\n",
            "2021-07-25 10:18:43,741 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-25 10:19:07,652 - INFO - joeynmt.training - Epoch   9, Step:   376600, Batch Loss:     1.730441, Tokens per Sec:    14778, Lr: 0.000300\n",
            "2021-07-25 10:19:35,597 - INFO - joeynmt.training - Epoch   9, Step:   376800, Batch Loss:     1.647170, Tokens per Sec:    15623, Lr: 0.000300\n",
            "2021-07-25 10:20:03,698 - INFO - joeynmt.training - Epoch   9, Step:   377000, Batch Loss:     1.711056, Tokens per Sec:    15522, Lr: 0.000300\n",
            "2021-07-25 10:20:31,685 - INFO - joeynmt.training - Epoch   9, Step:   377200, Batch Loss:     1.975657, Tokens per Sec:    15440, Lr: 0.000300\n",
            "2021-07-25 10:20:59,956 - INFO - joeynmt.training - Epoch   9, Step:   377400, Batch Loss:     1.838456, Tokens per Sec:    15445, Lr: 0.000300\n",
            "2021-07-25 10:21:28,073 - INFO - joeynmt.training - Epoch   9, Step:   377600, Batch Loss:     1.463900, Tokens per Sec:    15369, Lr: 0.000300\n",
            "2021-07-25 10:21:56,192 - INFO - joeynmt.training - Epoch   9, Step:   377800, Batch Loss:     1.741187, Tokens per Sec:    15590, Lr: 0.000300\n",
            "2021-07-25 10:22:24,274 - INFO - joeynmt.training - Epoch   9, Step:   378000, Batch Loss:     1.911325, Tokens per Sec:    15437, Lr: 0.000300\n",
            "2021-07-25 10:22:52,247 - INFO - joeynmt.training - Epoch   9, Step:   378200, Batch Loss:     1.798914, Tokens per Sec:    15504, Lr: 0.000300\n",
            "2021-07-25 10:23:20,284 - INFO - joeynmt.training - Epoch   9, Step:   378400, Batch Loss:     1.979452, Tokens per Sec:    15655, Lr: 0.000300\n",
            "2021-07-25 10:23:48,292 - INFO - joeynmt.training - Epoch   9, Step:   378600, Batch Loss:     1.773230, Tokens per Sec:    15223, Lr: 0.000300\n",
            "2021-07-25 10:24:16,246 - INFO - joeynmt.training - Epoch   9, Step:   378800, Batch Loss:     1.627988, Tokens per Sec:    15558, Lr: 0.000300\n",
            "2021-07-25 10:24:44,351 - INFO - joeynmt.training - Epoch   9, Step:   379000, Batch Loss:     1.658525, Tokens per Sec:    15465, Lr: 0.000300\n",
            "2021-07-25 10:25:12,610 - INFO - joeynmt.training - Epoch   9, Step:   379200, Batch Loss:     1.749103, Tokens per Sec:    15494, Lr: 0.000300\n",
            "2021-07-25 10:25:40,740 - INFO - joeynmt.training - Epoch   9, Step:   379400, Batch Loss:     1.405456, Tokens per Sec:    15514, Lr: 0.000300\n",
            "2021-07-25 10:26:08,626 - INFO - joeynmt.training - Epoch   9, Step:   379600, Batch Loss:     1.863284, Tokens per Sec:    15289, Lr: 0.000300\n",
            "2021-07-25 10:26:36,770 - INFO - joeynmt.training - Epoch   9, Step:   379800, Batch Loss:     1.839019, Tokens per Sec:    15271, Lr: 0.000300\n",
            "2021-07-25 10:27:04,636 - INFO - joeynmt.training - Epoch   9, Step:   380000, Batch Loss:     1.530110, Tokens per Sec:    15627, Lr: 0.000300\n",
            "2021-07-25 10:28:41,471 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:28:41,472 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:28:41,472 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:28:43,299 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:28:43,300 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:28:43,300 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:28:43,300 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 10:28:43,300 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:28:43,301 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:28:43,301 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:28:43,301 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus listened to the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 10:28:43,301 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:28:43,302 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:28:43,302 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:28:43,302 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 10:28:43,303 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:28:43,303 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:28:43,303 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:28:43,303 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-25 10:28:43,304 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   380000: bleu:  20.20, loss: 170140.4688, ppl:   6.6327, duration: 98.6669s\n",
            "2021-07-25 10:29:11,836 - INFO - joeynmt.training - Epoch   9, Step:   380200, Batch Loss:     1.653847, Tokens per Sec:    15438, Lr: 0.000300\n",
            "2021-07-25 10:29:39,737 - INFO - joeynmt.training - Epoch   9, Step:   380400, Batch Loss:     1.784710, Tokens per Sec:    15442, Lr: 0.000300\n",
            "2021-07-25 10:30:07,885 - INFO - joeynmt.training - Epoch   9, Step:   380600, Batch Loss:     1.827813, Tokens per Sec:    15463, Lr: 0.000300\n",
            "2021-07-25 10:30:36,013 - INFO - joeynmt.training - Epoch   9, Step:   380800, Batch Loss:     1.792095, Tokens per Sec:    15523, Lr: 0.000300\n",
            "2021-07-25 10:31:04,136 - INFO - joeynmt.training - Epoch   9, Step:   381000, Batch Loss:     1.801947, Tokens per Sec:    15410, Lr: 0.000300\n",
            "2021-07-25 10:31:32,264 - INFO - joeynmt.training - Epoch   9, Step:   381200, Batch Loss:     1.689375, Tokens per Sec:    15636, Lr: 0.000300\n",
            "2021-07-25 10:32:00,585 - INFO - joeynmt.training - Epoch   9, Step:   381400, Batch Loss:     1.836794, Tokens per Sec:    15402, Lr: 0.000300\n",
            "2021-07-25 10:32:28,736 - INFO - joeynmt.training - Epoch   9, Step:   381600, Batch Loss:     1.576727, Tokens per Sec:    15203, Lr: 0.000300\n",
            "2021-07-25 10:32:56,974 - INFO - joeynmt.training - Epoch   9, Step:   381800, Batch Loss:     1.761226, Tokens per Sec:    15342, Lr: 0.000300\n",
            "2021-07-25 10:33:25,116 - INFO - joeynmt.training - Epoch   9, Step:   382000, Batch Loss:     1.693016, Tokens per Sec:    15432, Lr: 0.000300\n",
            "2021-07-25 10:33:53,163 - INFO - joeynmt.training - Epoch   9, Step:   382200, Batch Loss:     1.799535, Tokens per Sec:    15520, Lr: 0.000300\n",
            "2021-07-25 10:34:21,219 - INFO - joeynmt.training - Epoch   9, Step:   382400, Batch Loss:     1.720375, Tokens per Sec:    15566, Lr: 0.000300\n",
            "2021-07-25 10:34:49,285 - INFO - joeynmt.training - Epoch   9, Step:   382600, Batch Loss:     1.964527, Tokens per Sec:    15252, Lr: 0.000300\n",
            "2021-07-25 10:35:17,030 - INFO - joeynmt.training - Epoch   9, Step:   382800, Batch Loss:     1.768796, Tokens per Sec:    15491, Lr: 0.000300\n",
            "2021-07-25 10:35:45,121 - INFO - joeynmt.training - Epoch   9, Step:   383000, Batch Loss:     1.803612, Tokens per Sec:    15480, Lr: 0.000300\n",
            "2021-07-25 10:36:13,380 - INFO - joeynmt.training - Epoch   9, Step:   383200, Batch Loss:     1.713632, Tokens per Sec:    15779, Lr: 0.000300\n",
            "2021-07-25 10:36:41,198 - INFO - joeynmt.training - Epoch   9, Step:   383400, Batch Loss:     1.945100, Tokens per Sec:    15512, Lr: 0.000300\n",
            "2021-07-25 10:37:09,345 - INFO - joeynmt.training - Epoch   9, Step:   383600, Batch Loss:     1.941757, Tokens per Sec:    15417, Lr: 0.000300\n",
            "2021-07-25 10:37:37,494 - INFO - joeynmt.training - Epoch   9, Step:   383800, Batch Loss:     1.979131, Tokens per Sec:    15411, Lr: 0.000300\n",
            "2021-07-25 10:38:05,325 - INFO - joeynmt.training - Epoch   9, Step:   384000, Batch Loss:     1.550660, Tokens per Sec:    15565, Lr: 0.000300\n",
            "2021-07-25 10:38:33,642 - INFO - joeynmt.training - Epoch   9, Step:   384200, Batch Loss:     1.547441, Tokens per Sec:    15633, Lr: 0.000300\n",
            "2021-07-25 10:39:01,749 - INFO - joeynmt.training - Epoch   9, Step:   384400, Batch Loss:     1.537447, Tokens per Sec:    15700, Lr: 0.000300\n",
            "2021-07-25 10:39:29,750 - INFO - joeynmt.training - Epoch   9, Step:   384600, Batch Loss:     1.937250, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-25 10:39:57,463 - INFO - joeynmt.training - Epoch   9: total training loss 14554.90\n",
            "2021-07-25 10:39:57,464 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-25 10:39:58,757 - INFO - joeynmt.training - Epoch  10, Step:   384800, Batch Loss:     1.750989, Tokens per Sec:     3087, Lr: 0.000300\n",
            "2021-07-25 10:40:26,703 - INFO - joeynmt.training - Epoch  10, Step:   385000, Batch Loss:     1.693347, Tokens per Sec:    15426, Lr: 0.000300\n",
            "2021-07-25 10:42:06,534 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:42:06,534 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:42:06,534 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:42:08,192 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:42:08,193 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:42:08,193 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:42:08,193 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer , that is acceptable to God . ”\n",
            "2021-07-25 10:42:08,193 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:42:08,194 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:42:08,194 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:42:08,194 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 10:42:08,194 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:42:08,195 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:42:08,195 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:42:08,195 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 10:42:08,195 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:42:08,195 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:42:08,196 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:42:08,196 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 10:42:08,196 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   385000: bleu:  20.47, loss: 170455.7812, ppl:   6.6560, duration: 101.4928s\n",
            "2021-07-25 10:42:36,346 - INFO - joeynmt.training - Epoch  10, Step:   385200, Batch Loss:     1.673365, Tokens per Sec:    15485, Lr: 0.000300\n",
            "2021-07-25 10:43:04,548 - INFO - joeynmt.training - Epoch  10, Step:   385400, Batch Loss:     1.739585, Tokens per Sec:    15301, Lr: 0.000300\n",
            "2021-07-25 10:43:32,728 - INFO - joeynmt.training - Epoch  10, Step:   385600, Batch Loss:     1.834735, Tokens per Sec:    15216, Lr: 0.000300\n",
            "2021-07-25 10:44:01,095 - INFO - joeynmt.training - Epoch  10, Step:   385800, Batch Loss:     1.452468, Tokens per Sec:    15685, Lr: 0.000300\n",
            "2021-07-25 10:44:29,268 - INFO - joeynmt.training - Epoch  10, Step:   386000, Batch Loss:     1.245206, Tokens per Sec:    15264, Lr: 0.000300\n",
            "2021-07-25 10:44:57,297 - INFO - joeynmt.training - Epoch  10, Step:   386200, Batch Loss:     1.317218, Tokens per Sec:    15475, Lr: 0.000300\n",
            "2021-07-25 10:45:25,467 - INFO - joeynmt.training - Epoch  10, Step:   386400, Batch Loss:     1.870803, Tokens per Sec:    15509, Lr: 0.000300\n",
            "2021-07-25 10:45:53,906 - INFO - joeynmt.training - Epoch  10, Step:   386600, Batch Loss:     1.813028, Tokens per Sec:    15438, Lr: 0.000300\n",
            "2021-07-25 10:46:22,032 - INFO - joeynmt.training - Epoch  10, Step:   386800, Batch Loss:     1.743053, Tokens per Sec:    15534, Lr: 0.000300\n",
            "2021-07-25 10:46:50,293 - INFO - joeynmt.training - Epoch  10, Step:   387000, Batch Loss:     1.935555, Tokens per Sec:    15468, Lr: 0.000300\n",
            "2021-07-25 10:47:18,353 - INFO - joeynmt.training - Epoch  10, Step:   387200, Batch Loss:     1.579918, Tokens per Sec:    15500, Lr: 0.000300\n",
            "2021-07-25 10:47:46,405 - INFO - joeynmt.training - Epoch  10, Step:   387400, Batch Loss:     1.660413, Tokens per Sec:    15251, Lr: 0.000300\n",
            "2021-07-25 10:48:14,753 - INFO - joeynmt.training - Epoch  10, Step:   387600, Batch Loss:     1.656533, Tokens per Sec:    15399, Lr: 0.000300\n",
            "2021-07-25 10:48:42,858 - INFO - joeynmt.training - Epoch  10, Step:   387800, Batch Loss:     1.809951, Tokens per Sec:    15197, Lr: 0.000300\n",
            "2021-07-25 10:49:11,022 - INFO - joeynmt.training - Epoch  10, Step:   388000, Batch Loss:     1.664060, Tokens per Sec:    15526, Lr: 0.000300\n",
            "2021-07-25 10:49:39,368 - INFO - joeynmt.training - Epoch  10, Step:   388200, Batch Loss:     1.817521, Tokens per Sec:    15375, Lr: 0.000300\n",
            "2021-07-25 10:50:07,434 - INFO - joeynmt.training - Epoch  10, Step:   388400, Batch Loss:     1.712452, Tokens per Sec:    15620, Lr: 0.000300\n",
            "2021-07-25 10:50:35,439 - INFO - joeynmt.training - Epoch  10, Step:   388600, Batch Loss:     1.786706, Tokens per Sec:    15650, Lr: 0.000300\n",
            "2021-07-25 10:51:03,708 - INFO - joeynmt.training - Epoch  10, Step:   388800, Batch Loss:     1.737546, Tokens per Sec:    15541, Lr: 0.000300\n",
            "2021-07-25 10:51:31,779 - INFO - joeynmt.training - Epoch  10, Step:   389000, Batch Loss:     1.807056, Tokens per Sec:    15602, Lr: 0.000300\n",
            "2021-07-25 10:51:59,902 - INFO - joeynmt.training - Epoch  10, Step:   389200, Batch Loss:     1.658099, Tokens per Sec:    15083, Lr: 0.000300\n",
            "2021-07-25 10:52:27,870 - INFO - joeynmt.training - Epoch  10, Step:   389400, Batch Loss:     1.959164, Tokens per Sec:    15819, Lr: 0.000300\n",
            "2021-07-25 10:52:55,854 - INFO - joeynmt.training - Epoch  10, Step:   389600, Batch Loss:     1.477308, Tokens per Sec:    15181, Lr: 0.000300\n",
            "2021-07-25 10:53:23,854 - INFO - joeynmt.training - Epoch  10, Step:   389800, Batch Loss:     1.609306, Tokens per Sec:    15530, Lr: 0.000300\n",
            "2021-07-25 10:53:51,897 - INFO - joeynmt.training - Epoch  10, Step:   390000, Batch Loss:     1.771886, Tokens per Sec:    15333, Lr: 0.000300\n",
            "2021-07-25 10:55:28,929 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:55:28,929 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:55:28,929 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:55:30,650 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:55:30,650 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:55:30,650 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:55:30,652 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 10:55:30,652 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:55:30,652 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:55:30,653 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:55:30,653 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 10:55:30,653 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:55:30,653 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:55:30,654 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:55:30,654 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-25 10:55:30,654 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:55:30,654 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:55:30,654 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:55:30,655 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 10:55:30,655 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   390000: bleu:  20.24, loss: 169896.9062, ppl:   6.6147, duration: 98.7579s\n",
            "2021-07-25 10:55:59,256 - INFO - joeynmt.training - Epoch  10, Step:   390200, Batch Loss:     1.790083, Tokens per Sec:    15361, Lr: 0.000300\n",
            "2021-07-25 10:56:27,371 - INFO - joeynmt.training - Epoch  10, Step:   390400, Batch Loss:     1.707799, Tokens per Sec:    15540, Lr: 0.000300\n",
            "2021-07-25 10:56:55,395 - INFO - joeynmt.training - Epoch  10, Step:   390600, Batch Loss:     1.591784, Tokens per Sec:    15355, Lr: 0.000300\n",
            "2021-07-25 10:57:23,475 - INFO - joeynmt.training - Epoch  10, Step:   390800, Batch Loss:     1.787011, Tokens per Sec:    15570, Lr: 0.000300\n",
            "2021-07-25 10:57:51,948 - INFO - joeynmt.training - Epoch  10, Step:   391000, Batch Loss:     1.792277, Tokens per Sec:    15490, Lr: 0.000300\n",
            "2021-07-25 10:58:19,743 - INFO - joeynmt.training - Epoch  10, Step:   391200, Batch Loss:     1.468178, Tokens per Sec:    15353, Lr: 0.000300\n",
            "2021-07-25 10:58:47,843 - INFO - joeynmt.training - Epoch  10, Step:   391400, Batch Loss:     1.857662, Tokens per Sec:    15541, Lr: 0.000300\n",
            "2021-07-25 10:59:16,054 - INFO - joeynmt.training - Epoch  10, Step:   391600, Batch Loss:     1.826201, Tokens per Sec:    15442, Lr: 0.000300\n",
            "2021-07-25 10:59:44,336 - INFO - joeynmt.training - Epoch  10, Step:   391800, Batch Loss:     1.942386, Tokens per Sec:    15523, Lr: 0.000300\n",
            "2021-07-25 11:00:12,416 - INFO - joeynmt.training - Epoch  10, Step:   392000, Batch Loss:     1.603606, Tokens per Sec:    15526, Lr: 0.000300\n",
            "2021-07-25 11:00:40,979 - INFO - joeynmt.training - Epoch  10, Step:   392200, Batch Loss:     1.800975, Tokens per Sec:    15535, Lr: 0.000300\n",
            "2021-07-25 11:01:08,903 - INFO - joeynmt.training - Epoch  10, Step:   392400, Batch Loss:     1.899039, Tokens per Sec:    15530, Lr: 0.000300\n",
            "2021-07-25 11:01:37,210 - INFO - joeynmt.training - Epoch  10, Step:   392600, Batch Loss:     1.929571, Tokens per Sec:    15509, Lr: 0.000300\n",
            "2021-07-25 11:02:05,343 - INFO - joeynmt.training - Epoch  10, Step:   392800, Batch Loss:     1.947145, Tokens per Sec:    15471, Lr: 0.000300\n",
            "2021-07-25 11:02:33,231 - INFO - joeynmt.training - Epoch  10, Step:   393000, Batch Loss:     1.643533, Tokens per Sec:    15433, Lr: 0.000300\n",
            "2021-07-25 11:02:54,991 - INFO - joeynmt.training - Epoch  10: total training loss 14531.61\n",
            "2021-07-25 11:02:54,992 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-25 11:03:02,330 - INFO - joeynmt.training - Epoch  11, Step:   393200, Batch Loss:     1.776247, Tokens per Sec:    13149, Lr: 0.000300\n",
            "2021-07-25 11:03:30,274 - INFO - joeynmt.training - Epoch  11, Step:   393400, Batch Loss:     1.658803, Tokens per Sec:    15582, Lr: 0.000300\n",
            "2021-07-25 11:03:58,610 - INFO - joeynmt.training - Epoch  11, Step:   393600, Batch Loss:     1.647475, Tokens per Sec:    15251, Lr: 0.000300\n",
            "2021-07-25 11:04:26,912 - INFO - joeynmt.training - Epoch  11, Step:   393800, Batch Loss:     1.874912, Tokens per Sec:    15449, Lr: 0.000300\n",
            "2021-07-25 11:04:54,799 - INFO - joeynmt.training - Epoch  11, Step:   394000, Batch Loss:     1.667160, Tokens per Sec:    15361, Lr: 0.000300\n",
            "2021-07-25 11:05:22,640 - INFO - joeynmt.training - Epoch  11, Step:   394200, Batch Loss:     1.660895, Tokens per Sec:    15313, Lr: 0.000300\n",
            "2021-07-25 11:05:50,747 - INFO - joeynmt.training - Epoch  11, Step:   394400, Batch Loss:     1.720029, Tokens per Sec:    15314, Lr: 0.000300\n",
            "2021-07-25 11:06:18,964 - INFO - joeynmt.training - Epoch  11, Step:   394600, Batch Loss:     1.410970, Tokens per Sec:    15691, Lr: 0.000300\n",
            "2021-07-25 11:06:47,181 - INFO - joeynmt.training - Epoch  11, Step:   394800, Batch Loss:     1.796595, Tokens per Sec:    15225, Lr: 0.000300\n",
            "2021-07-25 11:07:14,840 - INFO - joeynmt.training - Epoch  11, Step:   395000, Batch Loss:     1.745158, Tokens per Sec:    15558, Lr: 0.000300\n",
            "2021-07-25 11:08:51,696 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 11:08:51,696 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 11:08:51,696 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 11:08:52,660 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 11:08:52,660 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 11:08:53,449 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 11:08:53,450 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 11:08:53,450 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 11:08:53,451 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer in your endurance , this is acceptable to God . ”\n",
            "2021-07-25 11:08:53,451 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 11:08:53,451 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 11:08:53,451 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 11:08:53,452 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 11:08:53,452 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 11:08:53,452 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 11:08:53,452 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 11:08:53,453 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 11:08:53,453 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 11:08:53,453 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 11:08:53,453 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 11:08:53,454 - INFO - joeynmt.training - \tHypothesis: But Abigail did what to save his family .\n",
            "2021-07-25 11:08:53,454 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   395000: bleu:  20.13, loss: 169478.0938, ppl:   6.5840, duration: 98.6135s\n",
            "2021-07-25 11:09:21,854 - INFO - joeynmt.training - Epoch  11, Step:   395200, Batch Loss:     1.599469, Tokens per Sec:    15470, Lr: 0.000300\n",
            "2021-07-25 11:09:49,889 - INFO - joeynmt.training - Epoch  11, Step:   395400, Batch Loss:     1.729736, Tokens per Sec:    15339, Lr: 0.000300\n",
            "2021-07-25 11:10:18,142 - INFO - joeynmt.training - Epoch  11, Step:   395600, Batch Loss:     1.401297, Tokens per Sec:    15787, Lr: 0.000300\n",
            "2021-07-25 11:10:45,969 - INFO - joeynmt.training - Epoch  11, Step:   395800, Batch Loss:     1.699075, Tokens per Sec:    15354, Lr: 0.000300\n",
            "2021-07-25 11:11:14,075 - INFO - joeynmt.training - Epoch  11, Step:   396000, Batch Loss:     1.861390, Tokens per Sec:    15735, Lr: 0.000300\n",
            "2021-07-25 11:11:42,418 - INFO - joeynmt.training - Epoch  11, Step:   396200, Batch Loss:     1.814138, Tokens per Sec:    15548, Lr: 0.000300\n",
            "2021-07-25 11:12:10,497 - INFO - joeynmt.training - Epoch  11, Step:   396400, Batch Loss:     1.637445, Tokens per Sec:    15592, Lr: 0.000300\n",
            "2021-07-25 11:12:38,640 - INFO - joeynmt.training - Epoch  11, Step:   396600, Batch Loss:     1.884283, Tokens per Sec:    15307, Lr: 0.000300\n",
            "2021-07-25 11:13:06,667 - INFO - joeynmt.training - Epoch  11, Step:   396800, Batch Loss:     1.763947, Tokens per Sec:    15437, Lr: 0.000300\n",
            "2021-07-25 11:13:34,995 - INFO - joeynmt.training - Epoch  11, Step:   397000, Batch Loss:     1.793804, Tokens per Sec:    15708, Lr: 0.000300\n",
            "2021-07-25 11:14:03,287 - INFO - joeynmt.training - Epoch  11, Step:   397200, Batch Loss:     1.496741, Tokens per Sec:    15392, Lr: 0.000300\n",
            "2021-07-25 11:14:31,452 - INFO - joeynmt.training - Epoch  11, Step:   397400, Batch Loss:     1.605193, Tokens per Sec:    15497, Lr: 0.000300\n",
            "2021-07-25 11:14:59,572 - INFO - joeynmt.training - Epoch  11, Step:   397600, Batch Loss:     1.875651, Tokens per Sec:    15366, Lr: 0.000300\n",
            "2021-07-25 11:15:28,004 - INFO - joeynmt.training - Epoch  11, Step:   397800, Batch Loss:     1.733422, Tokens per Sec:    15440, Lr: 0.000300\n",
            "2021-07-25 11:15:56,145 - INFO - joeynmt.training - Epoch  11, Step:   398000, Batch Loss:     1.797443, Tokens per Sec:    15576, Lr: 0.000300\n",
            "2021-07-25 11:16:24,492 - INFO - joeynmt.training - Epoch  11, Step:   398200, Batch Loss:     1.723911, Tokens per Sec:    15413, Lr: 0.000300\n",
            "2021-07-25 11:16:52,787 - INFO - joeynmt.training - Epoch  11, Step:   398400, Batch Loss:     1.880173, Tokens per Sec:    15533, Lr: 0.000300\n",
            "2021-07-25 11:17:20,762 - INFO - joeynmt.training - Epoch  11, Step:   398600, Batch Loss:     1.830818, Tokens per Sec:    15237, Lr: 0.000300\n",
            "2021-07-25 11:17:49,214 - INFO - joeynmt.training - Epoch  11, Step:   398800, Batch Loss:     1.750681, Tokens per Sec:    15313, Lr: 0.000300\n",
            "2021-07-25 11:18:17,262 - INFO - joeynmt.training - Epoch  11, Step:   399000, Batch Loss:     1.611536, Tokens per Sec:    15631, Lr: 0.000300\n",
            "2021-07-25 11:18:45,587 - INFO - joeynmt.training - Epoch  11, Step:   399200, Batch Loss:     1.996399, Tokens per Sec:    15323, Lr: 0.000300\n",
            "2021-07-25 11:19:13,782 - INFO - joeynmt.training - Epoch  11, Step:   399400, Batch Loss:     1.789150, Tokens per Sec:    15348, Lr: 0.000300\n",
            "2021-07-25 11:19:41,688 - INFO - joeynmt.training - Epoch  11, Step:   399600, Batch Loss:     1.915553, Tokens per Sec:    15359, Lr: 0.000300\n",
            "2021-07-25 11:20:10,020 - INFO - joeynmt.training - Epoch  11, Step:   399800, Batch Loss:     1.876915, Tokens per Sec:    15378, Lr: 0.000300\n",
            "2021-07-25 11:20:38,072 - INFO - joeynmt.training - Epoch  11, Step:   400000, Batch Loss:     1.788456, Tokens per Sec:    15381, Lr: 0.000300\n",
            "2021-07-25 11:22:16,347 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 11:22:16,347 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 11:22:16,347 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 11:22:18,343 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 11:22:18,344 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 11:22:18,345 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 11:22:18,345 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 11:22:18,345 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 11:22:18,346 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 11:22:18,346 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 11:22:18,346 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 11:22:18,347 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 11:22:18,347 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 11:22:18,347 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 11:22:18,347 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 11:22:18,348 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 11:22:18,348 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 11:22:18,348 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 11:22:18,348 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 11:22:18,349 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   400000: bleu:  20.30, loss: 170198.5469, ppl:   6.6369, duration: 100.2766s\n",
            "2021-07-25 11:22:46,776 - INFO - joeynmt.training - Epoch  11, Step:   400200, Batch Loss:     1.559185, Tokens per Sec:    15157, Lr: 0.000300\n",
            "2021-07-25 11:23:14,994 - INFO - joeynmt.training - Epoch  11, Step:   400400, Batch Loss:     1.785907, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-25 11:23:43,224 - INFO - joeynmt.training - Epoch  11, Step:   400600, Batch Loss:     1.705628, Tokens per Sec:    15386, Lr: 0.000300\n",
            "2021-07-25 11:24:11,247 - INFO - joeynmt.training - Epoch  11, Step:   400800, Batch Loss:     1.659258, Tokens per Sec:    15413, Lr: 0.000300\n",
            "2021-07-25 11:24:39,426 - INFO - joeynmt.training - Epoch  11, Step:   401000, Batch Loss:     1.621269, Tokens per Sec:    15536, Lr: 0.000300\n",
            "2021-07-25 11:25:07,778 - INFO - joeynmt.training - Epoch  11, Step:   401200, Batch Loss:     1.655039, Tokens per Sec:    15458, Lr: 0.000300\n",
            "2021-07-25 11:25:35,860 - INFO - joeynmt.training - Epoch  11, Step:   401400, Batch Loss:     1.751055, Tokens per Sec:    15444, Lr: 0.000300\n",
            "2021-07-25 11:25:51,981 - INFO - joeynmt.training - Epoch  11: total training loss 14508.89\n",
            "2021-07-25 11:25:51,981 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-25 11:26:05,328 - INFO - joeynmt.training - Epoch  12, Step:   401600, Batch Loss:     1.681550, Tokens per Sec:    14400, Lr: 0.000300\n",
            "2021-07-25 11:26:33,678 - INFO - joeynmt.training - Epoch  12, Step:   401800, Batch Loss:     1.751800, Tokens per Sec:    15419, Lr: 0.000300\n",
            "2021-07-25 11:27:01,926 - INFO - joeynmt.training - Epoch  12, Step:   402000, Batch Loss:     1.913545, Tokens per Sec:    15663, Lr: 0.000300\n",
            "2021-07-25 11:27:30,249 - INFO - joeynmt.training - Epoch  12, Step:   402200, Batch Loss:     1.672216, Tokens per Sec:    15360, Lr: 0.000300\n",
            "2021-07-25 11:27:58,554 - INFO - joeynmt.training - Epoch  12, Step:   402400, Batch Loss:     1.803737, Tokens per Sec:    15728, Lr: 0.000300\n",
            "2021-07-25 11:28:26,229 - INFO - joeynmt.training - Epoch  12, Step:   402600, Batch Loss:     1.593535, Tokens per Sec:    15072, Lr: 0.000300\n",
            "2021-07-25 11:28:54,570 - INFO - joeynmt.training - Epoch  12, Step:   402800, Batch Loss:     1.837335, Tokens per Sec:    15584, Lr: 0.000300\n",
            "2021-07-25 11:29:22,687 - INFO - joeynmt.training - Epoch  12, Step:   403000, Batch Loss:     1.527372, Tokens per Sec:    15577, Lr: 0.000300\n",
            "2021-07-25 11:29:50,898 - INFO - joeynmt.training - Epoch  12, Step:   403200, Batch Loss:     1.773101, Tokens per Sec:    15157, Lr: 0.000300\n",
            "2021-07-25 11:30:18,766 - INFO - joeynmt.training - Epoch  12, Step:   403400, Batch Loss:     1.913149, Tokens per Sec:    15588, Lr: 0.000300\n",
            "2021-07-25 11:30:46,956 - INFO - joeynmt.training - Epoch  12, Step:   403600, Batch Loss:     1.809215, Tokens per Sec:    15213, Lr: 0.000300\n",
            "2021-07-25 11:31:14,835 - INFO - joeynmt.training - Epoch  12, Step:   403800, Batch Loss:     1.872952, Tokens per Sec:    15249, Lr: 0.000300\n",
            "2021-07-25 11:31:42,997 - INFO - joeynmt.training - Epoch  12, Step:   404000, Batch Loss:     1.765910, Tokens per Sec:    15686, Lr: 0.000300\n",
            "2021-07-25 11:32:10,884 - INFO - joeynmt.training - Epoch  12, Step:   404200, Batch Loss:     1.672186, Tokens per Sec:    15119, Lr: 0.000300\n",
            "2021-07-25 11:32:39,200 - INFO - joeynmt.training - Epoch  12, Step:   404400, Batch Loss:     1.708478, Tokens per Sec:    15450, Lr: 0.000300\n",
            "2021-07-25 11:33:07,281 - INFO - joeynmt.training - Epoch  12, Step:   404600, Batch Loss:     1.717571, Tokens per Sec:    15556, Lr: 0.000300\n",
            "2021-07-25 11:33:35,545 - INFO - joeynmt.training - Epoch  12, Step:   404800, Batch Loss:     1.730076, Tokens per Sec:    15481, Lr: 0.000300\n",
            "2021-07-25 11:34:03,750 - INFO - joeynmt.training - Epoch  12, Step:   405000, Batch Loss:     1.704338, Tokens per Sec:    15652, Lr: 0.000300\n",
            "2021-07-25 11:35:40,146 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 11:35:40,146 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 11:35:40,147 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 11:35:41,854 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 11:35:41,855 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 11:35:41,855 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 11:35:41,855 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 11:35:41,855 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 11:35:41,856 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 11:35:41,856 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 11:35:41,856 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven , saying : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 11:35:41,856 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 11:35:41,857 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 11:35:41,857 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 11:35:41,857 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 11:35:41,857 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 11:35:41,858 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 11:35:41,858 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 11:35:41,858 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 11:35:41,859 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   405000: bleu:  20.07, loss: 170464.1562, ppl:   6.6566, duration: 98.1077s\n",
            "2021-07-25 11:36:10,224 - INFO - joeynmt.training - Epoch  12, Step:   405200, Batch Loss:     1.600050, Tokens per Sec:    15370, Lr: 0.000300\n",
            "2021-07-25 11:36:38,081 - INFO - joeynmt.training - Epoch  12, Step:   405400, Batch Loss:     1.361120, Tokens per Sec:    15478, Lr: 0.000300\n",
            "2021-07-25 11:37:06,370 - INFO - joeynmt.training - Epoch  12, Step:   405600, Batch Loss:     1.627803, Tokens per Sec:    15513, Lr: 0.000300\n",
            "2021-07-25 11:37:34,510 - INFO - joeynmt.training - Epoch  12, Step:   405800, Batch Loss:     1.905958, Tokens per Sec:    15365, Lr: 0.000300\n",
            "2021-07-25 11:38:02,643 - INFO - joeynmt.training - Epoch  12, Step:   406000, Batch Loss:     1.823896, Tokens per Sec:    15503, Lr: 0.000300\n",
            "2021-07-25 11:38:30,987 - INFO - joeynmt.training - Epoch  12, Step:   406200, Batch Loss:     1.674957, Tokens per Sec:    15405, Lr: 0.000300\n",
            "2021-07-25 11:38:58,937 - INFO - joeynmt.training - Epoch  12, Step:   406400, Batch Loss:     1.548277, Tokens per Sec:    15428, Lr: 0.000300\n",
            "2021-07-25 11:39:27,074 - INFO - joeynmt.training - Epoch  12, Step:   406600, Batch Loss:     1.740196, Tokens per Sec:    15518, Lr: 0.000300\n",
            "2021-07-25 11:39:55,263 - INFO - joeynmt.training - Epoch  12, Step:   406800, Batch Loss:     1.706881, Tokens per Sec:    15519, Lr: 0.000300\n",
            "2021-07-25 11:40:23,253 - INFO - joeynmt.training - Epoch  12, Step:   407000, Batch Loss:     1.808792, Tokens per Sec:    15623, Lr: 0.000300\n",
            "2021-07-25 11:40:51,480 - INFO - joeynmt.training - Epoch  12, Step:   407200, Batch Loss:     1.695645, Tokens per Sec:    15327, Lr: 0.000300\n",
            "2021-07-25 11:41:19,537 - INFO - joeynmt.training - Epoch  12, Step:   407400, Batch Loss:     1.712610, Tokens per Sec:    15653, Lr: 0.000300\n",
            "2021-07-25 11:41:47,570 - INFO - joeynmt.training - Epoch  12, Step:   407600, Batch Loss:     1.668464, Tokens per Sec:    15389, Lr: 0.000300\n",
            "2021-07-25 11:42:15,624 - INFO - joeynmt.training - Epoch  12, Step:   407800, Batch Loss:     1.626849, Tokens per Sec:    15467, Lr: 0.000300\n",
            "2021-07-25 11:42:43,743 - INFO - joeynmt.training - Epoch  12, Step:   408000, Batch Loss:     1.372851, Tokens per Sec:    15589, Lr: 0.000300\n",
            "2021-07-25 11:43:12,117 - INFO - joeynmt.training - Epoch  12, Step:   408200, Batch Loss:     1.547628, Tokens per Sec:    15309, Lr: 0.000300\n",
            "2021-07-25 11:43:40,334 - INFO - joeynmt.training - Epoch  12, Step:   408400, Batch Loss:     1.850259, Tokens per Sec:    15555, Lr: 0.000300\n",
            "2021-07-25 11:44:08,195 - INFO - joeynmt.training - Epoch  12, Step:   408600, Batch Loss:     1.580584, Tokens per Sec:    15519, Lr: 0.000300\n",
            "2021-07-25 11:44:36,631 - INFO - joeynmt.training - Epoch  12, Step:   408800, Batch Loss:     1.700757, Tokens per Sec:    15627, Lr: 0.000300\n",
            "2021-07-25 11:45:04,320 - INFO - joeynmt.training - Epoch  12, Step:   409000, Batch Loss:     1.401367, Tokens per Sec:    15445, Lr: 0.000300\n",
            "2021-07-25 11:45:32,328 - INFO - joeynmt.training - Epoch  12, Step:   409200, Batch Loss:     1.517099, Tokens per Sec:    15509, Lr: 0.000300\n",
            "2021-07-25 11:46:00,489 - INFO - joeynmt.training - Epoch  12, Step:   409400, Batch Loss:     1.778548, Tokens per Sec:    15546, Lr: 0.000300\n",
            "2021-07-25 11:46:28,142 - INFO - joeynmt.training - Epoch  12, Step:   409600, Batch Loss:     1.719754, Tokens per Sec:    15417, Lr: 0.000300\n",
            "2021-07-25 11:46:56,147 - INFO - joeynmt.training - Epoch  12, Step:   409800, Batch Loss:     1.806879, Tokens per Sec:    15223, Lr: 0.000300\n",
            "2021-07-25 11:47:06,465 - INFO - joeynmt.training - Epoch  12: total training loss 14489.82\n",
            "2021-07-25 11:47:06,466 - INFO - joeynmt.training - EPOCH 13\n",
            "2021-07-25 11:47:25,239 - INFO - joeynmt.training - Epoch  13, Step:   410000, Batch Loss:     1.679957, Tokens per Sec:    14937, Lr: 0.000300\n",
            "2021-07-25 11:49:04,195 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 11:49:04,195 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 11:49:04,195 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 11:49:05,084 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 11:49:05,084 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 11:49:05,797 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 11:49:05,797 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 11:49:05,798 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 11:49:05,798 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 11:49:05,798 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 11:49:05,799 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 11:49:05,799 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 11:49:05,799 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus listened to the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 11:49:05,799 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 11:49:05,799 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 11:49:05,800 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 11:49:05,800 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 11:49:05,800 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 11:49:05,800 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 11:49:05,800 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 11:49:05,801 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 11:49:05,801 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   410000: bleu:  20.47, loss: 169254.9844, ppl:   6.5677, duration: 100.5611s\n",
            "2021-07-25 11:49:33,819 - INFO - joeynmt.training - Epoch  13, Step:   410200, Batch Loss:     1.750970, Tokens per Sec:    15363, Lr: 0.000300\n",
            "2021-07-25 11:50:01,699 - INFO - joeynmt.training - Epoch  13, Step:   410400, Batch Loss:     1.771542, Tokens per Sec:    15685, Lr: 0.000300\n",
            "2021-07-25 11:50:29,562 - INFO - joeynmt.training - Epoch  13, Step:   410600, Batch Loss:     1.794050, Tokens per Sec:    15618, Lr: 0.000300\n",
            "2021-07-25 11:50:57,429 - INFO - joeynmt.training - Epoch  13, Step:   410800, Batch Loss:     1.885667, Tokens per Sec:    15646, Lr: 0.000300\n",
            "2021-07-25 11:51:25,417 - INFO - joeynmt.training - Epoch  13, Step:   411000, Batch Loss:     1.676577, Tokens per Sec:    15796, Lr: 0.000300\n",
            "2021-07-25 11:51:53,434 - INFO - joeynmt.training - Epoch  13, Step:   411200, Batch Loss:     1.409280, Tokens per Sec:    15421, Lr: 0.000300\n",
            "2021-07-25 11:52:21,492 - INFO - joeynmt.training - Epoch  13, Step:   411400, Batch Loss:     1.780429, Tokens per Sec:    15773, Lr: 0.000300\n",
            "2021-07-25 11:52:49,402 - INFO - joeynmt.training - Epoch  13, Step:   411600, Batch Loss:     1.799583, Tokens per Sec:    15390, Lr: 0.000300\n",
            "2021-07-25 11:53:17,629 - INFO - joeynmt.training - Epoch  13, Step:   411800, Batch Loss:     1.694058, Tokens per Sec:    15568, Lr: 0.000300\n",
            "2021-07-25 11:53:45,546 - INFO - joeynmt.training - Epoch  13, Step:   412000, Batch Loss:     1.674580, Tokens per Sec:    15463, Lr: 0.000300\n",
            "2021-07-25 11:54:13,629 - INFO - joeynmt.training - Epoch  13, Step:   412200, Batch Loss:     1.640266, Tokens per Sec:    15602, Lr: 0.000300\n",
            "2021-07-25 11:54:41,808 - INFO - joeynmt.training - Epoch  13, Step:   412400, Batch Loss:     1.809676, Tokens per Sec:    15537, Lr: 0.000300\n",
            "2021-07-25 11:55:09,592 - INFO - joeynmt.training - Epoch  13, Step:   412600, Batch Loss:     1.867793, Tokens per Sec:    15442, Lr: 0.000300\n",
            "2021-07-25 11:55:37,741 - INFO - joeynmt.training - Epoch  13, Step:   412800, Batch Loss:     1.693559, Tokens per Sec:    15478, Lr: 0.000300\n",
            "2021-07-25 11:56:05,401 - INFO - joeynmt.training - Epoch  13, Step:   413000, Batch Loss:     1.904437, Tokens per Sec:    15491, Lr: 0.000300\n",
            "2021-07-25 11:56:33,435 - INFO - joeynmt.training - Epoch  13, Step:   413200, Batch Loss:     1.670769, Tokens per Sec:    15689, Lr: 0.000300\n",
            "2021-07-25 11:57:01,362 - INFO - joeynmt.training - Epoch  13, Step:   413400, Batch Loss:     1.772124, Tokens per Sec:    15315, Lr: 0.000300\n",
            "2021-07-25 11:57:29,124 - INFO - joeynmt.training - Epoch  13, Step:   413600, Batch Loss:     1.685820, Tokens per Sec:    15517, Lr: 0.000300\n",
            "2021-07-25 11:57:57,125 - INFO - joeynmt.training - Epoch  13, Step:   413800, Batch Loss:     1.846695, Tokens per Sec:    15353, Lr: 0.000300\n",
            "2021-07-25 11:58:25,087 - INFO - joeynmt.training - Epoch  13, Step:   414000, Batch Loss:     1.259524, Tokens per Sec:    15580, Lr: 0.000300\n",
            "2021-07-25 11:58:53,022 - INFO - joeynmt.training - Epoch  13, Step:   414200, Batch Loss:     1.830280, Tokens per Sec:    15429, Lr: 0.000300\n",
            "2021-07-25 11:59:21,218 - INFO - joeynmt.training - Epoch  13, Step:   414400, Batch Loss:     1.821242, Tokens per Sec:    15543, Lr: 0.000300\n",
            "2021-07-25 11:59:49,064 - INFO - joeynmt.training - Epoch  13, Step:   414600, Batch Loss:     1.739408, Tokens per Sec:    15400, Lr: 0.000300\n",
            "2021-07-25 12:00:16,826 - INFO - joeynmt.training - Epoch  13, Step:   414800, Batch Loss:     1.685975, Tokens per Sec:    15625, Lr: 0.000300\n",
            "2021-07-25 12:00:44,935 - INFO - joeynmt.training - Epoch  13, Step:   415000, Batch Loss:     1.628381, Tokens per Sec:    15528, Lr: 0.000300\n",
            "2021-07-25 12:02:20,701 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 12:02:20,701 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 12:02:20,702 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 12:02:22,291 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 12:02:22,291 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 12:02:22,292 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 12:02:22,292 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer patiently , this is acceptable to God . ”\n",
            "2021-07-25 12:02:22,292 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 12:02:22,292 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 12:02:22,293 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 12:02:22,293 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he listened to the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 12:02:22,293 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 12:02:22,293 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 12:02:22,294 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 12:02:22,294 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 12:02:22,294 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 12:02:22,294 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 12:02:22,294 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 12:02:22,295 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 12:02:22,295 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   415000: bleu:  20.25, loss: 169662.5781, ppl:   6.5975, duration: 97.3588s\n",
            "2021-07-25 12:02:50,613 - INFO - joeynmt.training - Epoch  13, Step:   415200, Batch Loss:     1.907037, Tokens per Sec:    15454, Lr: 0.000300\n",
            "2021-07-25 12:03:18,633 - INFO - joeynmt.training - Epoch  13, Step:   415400, Batch Loss:     1.958926, Tokens per Sec:    15585, Lr: 0.000300\n",
            "2021-07-25 12:03:46,365 - INFO - joeynmt.training - Epoch  13, Step:   415600, Batch Loss:     1.826783, Tokens per Sec:    15428, Lr: 0.000300\n",
            "2021-07-25 12:04:14,278 - INFO - joeynmt.training - Epoch  13, Step:   415800, Batch Loss:     1.451082, Tokens per Sec:    15677, Lr: 0.000300\n",
            "2021-07-25 12:04:41,810 - INFO - joeynmt.training - Epoch  13, Step:   416000, Batch Loss:     1.630362, Tokens per Sec:    15541, Lr: 0.000300\n",
            "2021-07-25 12:05:09,969 - INFO - joeynmt.training - Epoch  13, Step:   416200, Batch Loss:     1.514358, Tokens per Sec:    15843, Lr: 0.000300\n",
            "2021-07-25 12:05:38,033 - INFO - joeynmt.training - Epoch  13, Step:   416400, Batch Loss:     1.940400, Tokens per Sec:    15567, Lr: 0.000300\n",
            "2021-07-25 12:06:05,674 - INFO - joeynmt.training - Epoch  13, Step:   416600, Batch Loss:     1.646456, Tokens per Sec:    15669, Lr: 0.000300\n",
            "2021-07-25 12:06:33,503 - INFO - joeynmt.training - Epoch  13, Step:   416800, Batch Loss:     1.853826, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-25 12:07:01,415 - INFO - joeynmt.training - Epoch  13, Step:   417000, Batch Loss:     1.576202, Tokens per Sec:    15681, Lr: 0.000300\n",
            "2021-07-25 12:07:29,204 - INFO - joeynmt.training - Epoch  13, Step:   417200, Batch Loss:     1.513662, Tokens per Sec:    15766, Lr: 0.000300\n",
            "2021-07-25 12:07:57,072 - INFO - joeynmt.training - Epoch  13, Step:   417400, Batch Loss:     1.531815, Tokens per Sec:    15648, Lr: 0.000300\n",
            "2021-07-25 12:08:24,979 - INFO - joeynmt.training - Epoch  13, Step:   417600, Batch Loss:     2.195671, Tokens per Sec:    15716, Lr: 0.000300\n",
            "2021-07-25 12:08:52,923 - INFO - joeynmt.training - Epoch  13, Step:   417800, Batch Loss:     1.678440, Tokens per Sec:    15483, Lr: 0.000300\n",
            "2021-07-25 12:09:20,365 - INFO - joeynmt.training - Epoch  13, Step:   418000, Batch Loss:     1.721342, Tokens per Sec:    15439, Lr: 0.000300\n",
            "2021-07-25 12:09:48,072 - INFO - joeynmt.training - Epoch  13, Step:   418200, Batch Loss:     1.740464, Tokens per Sec:    15209, Lr: 0.000300\n",
            "2021-07-25 12:09:53,748 - INFO - joeynmt.training - Epoch  13: total training loss 14488.24\n",
            "2021-07-25 12:09:53,748 - INFO - joeynmt.training - EPOCH 14\n",
            "2021-07-25 12:10:16,809 - INFO - joeynmt.training - Epoch  14, Step:   418400, Batch Loss:     1.560829, Tokens per Sec:    14962, Lr: 0.000300\n",
            "2021-07-25 12:10:44,884 - INFO - joeynmt.training - Epoch  14, Step:   418600, Batch Loss:     1.748705, Tokens per Sec:    15619, Lr: 0.000300\n",
            "2021-07-25 12:11:12,790 - INFO - joeynmt.training - Epoch  14, Step:   418800, Batch Loss:     1.702081, Tokens per Sec:    15762, Lr: 0.000300\n",
            "2021-07-25 12:11:40,789 - INFO - joeynmt.training - Epoch  14, Step:   419000, Batch Loss:     1.616185, Tokens per Sec:    15589, Lr: 0.000300\n",
            "2021-07-25 12:12:08,523 - INFO - joeynmt.training - Epoch  14, Step:   419200, Batch Loss:     1.625817, Tokens per Sec:    15871, Lr: 0.000300\n",
            "2021-07-25 12:12:36,014 - INFO - joeynmt.training - Epoch  14, Step:   419400, Batch Loss:     1.756504, Tokens per Sec:    15361, Lr: 0.000300\n",
            "2021-07-25 12:13:04,186 - INFO - joeynmt.training - Epoch  14, Step:   419600, Batch Loss:     1.665785, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-25 12:13:31,894 - INFO - joeynmt.training - Epoch  14, Step:   419800, Batch Loss:     1.941758, Tokens per Sec:    15644, Lr: 0.000300\n",
            "2021-07-25 12:14:00,021 - INFO - joeynmt.training - Epoch  14, Step:   420000, Batch Loss:     1.735174, Tokens per Sec:    15665, Lr: 0.000300\n",
            "2021-07-25 12:15:37,936 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 12:15:37,937 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 12:15:37,937 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 12:15:39,525 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 12:15:39,526 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 12:15:39,526 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 12:15:39,526 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 12:15:39,526 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 12:15:39,527 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 12:15:39,527 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 12:15:39,527 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus listened to the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 12:15:39,527 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 12:15:39,528 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 12:15:39,528 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 12:15:39,528 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-25 12:15:39,528 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 12:15:39,529 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 12:15:39,529 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 12:15:39,529 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 12:15:39,529 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   420000: bleu:  20.28, loss: 169806.0156, ppl:   6.6080, duration: 99.5077s\n",
            "2021-07-25 12:16:07,489 - INFO - joeynmt.training - Epoch  14, Step:   420200, Batch Loss:     1.907660, Tokens per Sec:    15569, Lr: 0.000300\n",
            "2021-07-25 12:16:35,583 - INFO - joeynmt.training - Epoch  14, Step:   420400, Batch Loss:     1.708840, Tokens per Sec:    15612, Lr: 0.000300\n",
            "2021-07-25 12:17:03,261 - INFO - joeynmt.training - Epoch  14, Step:   420600, Batch Loss:     1.720490, Tokens per Sec:    15679, Lr: 0.000300\n",
            "2021-07-25 12:17:31,104 - INFO - joeynmt.training - Epoch  14, Step:   420800, Batch Loss:     1.754700, Tokens per Sec:    15493, Lr: 0.000300\n",
            "2021-07-25 12:17:59,172 - INFO - joeynmt.training - Epoch  14, Step:   421000, Batch Loss:     1.594261, Tokens per Sec:    15730, Lr: 0.000300\n",
            "2021-07-25 12:18:26,829 - INFO - joeynmt.training - Epoch  14, Step:   421200, Batch Loss:     1.694231, Tokens per Sec:    15613, Lr: 0.000300\n",
            "2021-07-25 12:18:54,827 - INFO - joeynmt.training - Epoch  14, Step:   421400, Batch Loss:     1.703088, Tokens per Sec:    15713, Lr: 0.000300\n",
            "2021-07-25 12:19:22,555 - INFO - joeynmt.training - Epoch  14, Step:   421600, Batch Loss:     1.607548, Tokens per Sec:    15696, Lr: 0.000300\n",
            "2021-07-25 12:19:50,456 - INFO - joeynmt.training - Epoch  14, Step:   421800, Batch Loss:     1.711293, Tokens per Sec:    15442, Lr: 0.000300\n",
            "2021-07-25 12:20:18,315 - INFO - joeynmt.training - Epoch  14, Step:   422000, Batch Loss:     1.692153, Tokens per Sec:    15717, Lr: 0.000300\n",
            "2021-07-25 12:20:45,815 - INFO - joeynmt.training - Epoch  14, Step:   422200, Batch Loss:     1.717974, Tokens per Sec:    15559, Lr: 0.000300\n",
            "2021-07-25 12:21:13,834 - INFO - joeynmt.training - Epoch  14, Step:   422400, Batch Loss:     1.807395, Tokens per Sec:    15726, Lr: 0.000300\n",
            "2021-07-25 12:21:41,648 - INFO - joeynmt.training - Epoch  14, Step:   422600, Batch Loss:     1.676408, Tokens per Sec:    15507, Lr: 0.000300\n",
            "2021-07-25 12:22:09,448 - INFO - joeynmt.training - Epoch  14, Step:   422800, Batch Loss:     1.610534, Tokens per Sec:    15846, Lr: 0.000300\n",
            "2021-07-25 12:22:37,278 - INFO - joeynmt.training - Epoch  14, Step:   423000, Batch Loss:     1.739219, Tokens per Sec:    15503, Lr: 0.000300\n",
            "2021-07-25 12:23:04,853 - INFO - joeynmt.training - Epoch  14, Step:   423200, Batch Loss:     1.734380, Tokens per Sec:    15512, Lr: 0.000300\n",
            "2021-07-25 12:23:33,044 - INFO - joeynmt.training - Epoch  14, Step:   423400, Batch Loss:     1.753998, Tokens per Sec:    15933, Lr: 0.000300\n",
            "2021-07-25 12:24:00,869 - INFO - joeynmt.training - Epoch  14, Step:   423600, Batch Loss:     1.834929, Tokens per Sec:    15609, Lr: 0.000300\n",
            "2021-07-25 12:24:28,602 - INFO - joeynmt.training - Epoch  14, Step:   423800, Batch Loss:     1.837472, Tokens per Sec:    15485, Lr: 0.000300\n",
            "2021-07-25 12:24:56,711 - INFO - joeynmt.training - Epoch  14, Step:   424000, Batch Loss:     1.753605, Tokens per Sec:    15486, Lr: 0.000300\n",
            "2021-07-25 12:25:24,607 - INFO - joeynmt.training - Epoch  14, Step:   424200, Batch Loss:     1.800586, Tokens per Sec:    15715, Lr: 0.000300\n",
            "2021-07-25 12:25:52,926 - INFO - joeynmt.training - Epoch  14, Step:   424400, Batch Loss:     1.685056, Tokens per Sec:    15306, Lr: 0.000300\n",
            "2021-07-25 12:26:21,031 - INFO - joeynmt.training - Epoch  14, Step:   424600, Batch Loss:     1.730384, Tokens per Sec:    15395, Lr: 0.000300\n",
            "2021-07-25 12:26:49,068 - INFO - joeynmt.training - Epoch  14, Step:   424800, Batch Loss:     1.556512, Tokens per Sec:    15736, Lr: 0.000300\n",
            "2021-07-25 12:27:16,789 - INFO - joeynmt.training - Epoch  14, Step:   425000, Batch Loss:     1.955348, Tokens per Sec:    15602, Lr: 0.000300\n",
            "2021-07-25 12:28:56,011 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 12:28:56,011 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 12:28:56,011 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 12:28:56,907 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 12:28:56,908 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 12:28:57,684 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 12:28:57,685 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 12:28:57,685 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 12:28:57,685 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 12:28:57,685 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 12:28:57,686 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 12:28:57,686 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 12:28:57,686 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 12:28:57,687 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 12:28:57,687 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 12:28:57,687 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 12:28:57,687 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 12:28:57,688 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 12:28:57,688 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 12:28:57,688 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 12:28:57,688 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to heal his family .\n",
            "2021-07-25 12:28:57,689 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   425000: bleu:  20.53, loss: 168639.8750, ppl:   6.5229, duration: 100.8991s\n",
            "2021-07-25 12:29:25,682 - INFO - joeynmt.training - Epoch  14, Step:   425200, Batch Loss:     1.689082, Tokens per Sec:    15497, Lr: 0.000300\n",
            "2021-07-25 12:29:53,600 - INFO - joeynmt.training - Epoch  14, Step:   425400, Batch Loss:     1.901285, Tokens per Sec:    15513, Lr: 0.000300\n",
            "2021-07-25 12:30:20,971 - INFO - joeynmt.training - Epoch  14, Step:   425600, Batch Loss:     1.832677, Tokens per Sec:    15501, Lr: 0.000300\n",
            "2021-07-25 12:30:48,563 - INFO - joeynmt.training - Epoch  14, Step:   425800, Batch Loss:     1.661691, Tokens per Sec:    15486, Lr: 0.000300\n",
            "2021-07-25 12:31:16,241 - INFO - joeynmt.training - Epoch  14, Step:   426000, Batch Loss:     1.754638, Tokens per Sec:    15606, Lr: 0.000300\n",
            "2021-07-25 12:31:43,892 - INFO - joeynmt.training - Epoch  14, Step:   426200, Batch Loss:     1.837963, Tokens per Sec:    15472, Lr: 0.000300\n",
            "2021-07-25 12:32:11,821 - INFO - joeynmt.training - Epoch  14, Step:   426400, Batch Loss:     1.822493, Tokens per Sec:    15608, Lr: 0.000300\n",
            "2021-07-25 12:32:39,810 - INFO - joeynmt.training - Epoch  14, Step:   426600, Batch Loss:     1.714252, Tokens per Sec:    15585, Lr: 0.000300\n",
            "2021-07-25 12:32:39,969 - INFO - joeynmt.training - Epoch  14: total training loss 14469.84\n",
            "2021-07-25 12:32:39,969 - INFO - joeynmt.training - EPOCH 15\n",
            "2021-07-25 12:33:08,556 - INFO - joeynmt.training - Epoch  15, Step:   426800, Batch Loss:     1.643699, Tokens per Sec:    15319, Lr: 0.000300\n",
            "2021-07-25 12:33:36,538 - INFO - joeynmt.training - Epoch  15, Step:   427000, Batch Loss:     1.765372, Tokens per Sec:    15602, Lr: 0.000300\n",
            "2021-07-25 12:34:04,169 - INFO - joeynmt.training - Epoch  15, Step:   427200, Batch Loss:     1.838518, Tokens per Sec:    15513, Lr: 0.000300\n",
            "2021-07-25 12:34:31,843 - INFO - joeynmt.training - Epoch  15, Step:   427400, Batch Loss:     1.659788, Tokens per Sec:    15537, Lr: 0.000300\n",
            "2021-07-25 12:34:59,882 - INFO - joeynmt.training - Epoch  15, Step:   427600, Batch Loss:     1.674279, Tokens per Sec:    15718, Lr: 0.000300\n",
            "2021-07-25 12:35:27,485 - INFO - joeynmt.training - Epoch  15, Step:   427800, Batch Loss:     1.562981, Tokens per Sec:    15771, Lr: 0.000300\n",
            "2021-07-25 12:35:55,441 - INFO - joeynmt.training - Epoch  15, Step:   428000, Batch Loss:     1.592795, Tokens per Sec:    15395, Lr: 0.000300\n",
            "2021-07-25 12:36:23,210 - INFO - joeynmt.training - Epoch  15, Step:   428200, Batch Loss:     1.680805, Tokens per Sec:    15718, Lr: 0.000300\n",
            "2021-07-25 12:36:50,915 - INFO - joeynmt.training - Epoch  15, Step:   428400, Batch Loss:     1.742430, Tokens per Sec:    15422, Lr: 0.000300\n",
            "2021-07-25 12:37:18,622 - INFO - joeynmt.training - Epoch  15, Step:   428600, Batch Loss:     1.594318, Tokens per Sec:    15542, Lr: 0.000300\n",
            "2021-07-25 12:37:46,674 - INFO - joeynmt.training - Epoch  15, Step:   428800, Batch Loss:     1.682533, Tokens per Sec:    15521, Lr: 0.000300\n",
            "2021-07-25 12:38:14,550 - INFO - joeynmt.training - Epoch  15, Step:   429000, Batch Loss:     1.830147, Tokens per Sec:    15492, Lr: 0.000300\n",
            "2021-07-25 12:38:42,491 - INFO - joeynmt.training - Epoch  15, Step:   429200, Batch Loss:     1.743986, Tokens per Sec:    15479, Lr: 0.000300\n",
            "2021-07-25 12:39:10,241 - INFO - joeynmt.training - Epoch  15, Step:   429400, Batch Loss:     1.726807, Tokens per Sec:    15490, Lr: 0.000300\n",
            "2021-07-25 12:39:38,229 - INFO - joeynmt.training - Epoch  15, Step:   429600, Batch Loss:     1.640945, Tokens per Sec:    15595, Lr: 0.000300\n",
            "2021-07-25 12:40:05,918 - INFO - joeynmt.training - Epoch  15, Step:   429800, Batch Loss:     1.560322, Tokens per Sec:    15630, Lr: 0.000300\n",
            "2021-07-25 12:40:33,780 - INFO - joeynmt.training - Epoch  15, Step:   430000, Batch Loss:     1.452919, Tokens per Sec:    15731, Lr: 0.000300\n",
            "2021-07-25 12:42:09,569 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 12:42:09,569 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 12:42:09,569 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 12:42:11,227 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 12:42:11,228 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 12:42:11,228 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 12:42:11,228 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 12:42:11,228 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 12:42:11,229 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 12:42:11,229 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 12:42:11,229 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 12:42:11,229 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 12:42:11,230 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 12:42:11,230 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 12:42:11,230 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 12:42:11,230 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 12:42:11,231 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 12:42:11,231 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 12:42:11,231 - INFO - joeynmt.training - \tHypothesis: But Abigail took action to save his family .\n",
            "2021-07-25 12:42:11,231 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   430000: bleu:  20.42, loss: 168904.2031, ppl:   6.5421, duration: 97.4503s\n",
            "2021-07-25 12:42:39,291 - INFO - joeynmt.training - Epoch  15, Step:   430200, Batch Loss:     1.788331, Tokens per Sec:    15362, Lr: 0.000300\n",
            "2021-07-25 12:43:07,217 - INFO - joeynmt.training - Epoch  15, Step:   430400, Batch Loss:     1.703028, Tokens per Sec:    15404, Lr: 0.000300\n",
            "2021-07-25 12:43:35,240 - INFO - joeynmt.training - Epoch  15, Step:   430600, Batch Loss:     1.834640, Tokens per Sec:    15492, Lr: 0.000300\n",
            "2021-07-25 12:44:02,963 - INFO - joeynmt.training - Epoch  15, Step:   430800, Batch Loss:     1.648949, Tokens per Sec:    15451, Lr: 0.000300\n",
            "2021-07-25 12:44:31,042 - INFO - joeynmt.training - Epoch  15, Step:   431000, Batch Loss:     1.823755, Tokens per Sec:    15751, Lr: 0.000300\n",
            "2021-07-25 12:44:58,969 - INFO - joeynmt.training - Epoch  15, Step:   431200, Batch Loss:     1.936656, Tokens per Sec:    15661, Lr: 0.000300\n",
            "2021-07-25 12:45:26,661 - INFO - joeynmt.training - Epoch  15, Step:   431400, Batch Loss:     1.627831, Tokens per Sec:    15721, Lr: 0.000300\n",
            "2021-07-25 12:45:54,907 - INFO - joeynmt.training - Epoch  15, Step:   431600, Batch Loss:     1.667482, Tokens per Sec:    15454, Lr: 0.000300\n",
            "2021-07-25 12:46:22,892 - INFO - joeynmt.training - Epoch  15, Step:   431800, Batch Loss:     1.760683, Tokens per Sec:    15554, Lr: 0.000300\n",
            "2021-07-25 12:46:51,141 - INFO - joeynmt.training - Epoch  15, Step:   432000, Batch Loss:     1.758043, Tokens per Sec:    15344, Lr: 0.000300\n",
            "2021-07-25 12:47:19,315 - INFO - joeynmt.training - Epoch  15, Step:   432200, Batch Loss:     1.912188, Tokens per Sec:    15542, Lr: 0.000300\n",
            "2021-07-25 12:47:47,781 - INFO - joeynmt.training - Epoch  15, Step:   432400, Batch Loss:     1.925429, Tokens per Sec:    15689, Lr: 0.000300\n",
            "2021-07-25 12:48:15,812 - INFO - joeynmt.training - Epoch  15, Step:   432600, Batch Loss:     1.917771, Tokens per Sec:    15215, Lr: 0.000300\n",
            "2021-07-25 12:48:44,065 - INFO - joeynmt.training - Epoch  15, Step:   432800, Batch Loss:     1.612914, Tokens per Sec:    15276, Lr: 0.000300\n",
            "2021-07-25 12:49:12,202 - INFO - joeynmt.training - Epoch  15, Step:   433000, Batch Loss:     1.964212, Tokens per Sec:    15634, Lr: 0.000300\n",
            "2021-07-25 12:49:40,599 - INFO - joeynmt.training - Epoch  15, Step:   433200, Batch Loss:     1.809345, Tokens per Sec:    15418, Lr: 0.000300\n",
            "2021-07-25 12:50:08,574 - INFO - joeynmt.training - Epoch  15, Step:   433400, Batch Loss:     1.708125, Tokens per Sec:    15711, Lr: 0.000300\n",
            "2021-07-25 12:50:36,700 - INFO - joeynmt.training - Epoch  15, Step:   433600, Batch Loss:     1.529135, Tokens per Sec:    15297, Lr: 0.000300\n",
            "2021-07-25 12:51:04,704 - INFO - joeynmt.training - Epoch  15, Step:   433800, Batch Loss:     1.739855, Tokens per Sec:    15692, Lr: 0.000300\n",
            "2021-07-25 12:51:32,433 - INFO - joeynmt.training - Epoch  15, Step:   434000, Batch Loss:     1.687138, Tokens per Sec:    15365, Lr: 0.000300\n",
            "2021-07-25 12:52:00,341 - INFO - joeynmt.training - Epoch  15, Step:   434200, Batch Loss:     1.720583, Tokens per Sec:    15275, Lr: 0.000300\n",
            "2021-07-25 12:52:28,061 - INFO - joeynmt.training - Epoch  15, Step:   434400, Batch Loss:     1.650496, Tokens per Sec:    15694, Lr: 0.000300\n",
            "2021-07-25 12:52:56,320 - INFO - joeynmt.training - Epoch  15, Step:   434600, Batch Loss:     1.891453, Tokens per Sec:    15501, Lr: 0.000300\n",
            "2021-07-25 12:53:24,124 - INFO - joeynmt.training - Epoch  15, Step:   434800, Batch Loss:     1.791530, Tokens per Sec:    15579, Lr: 0.000300\n",
            "2021-07-25 12:53:48,183 - INFO - joeynmt.training - Epoch  15: total training loss 14455.59\n",
            "2021-07-25 12:53:48,184 - INFO - joeynmt.training - EPOCH 16\n",
            "2021-07-25 12:53:53,215 - INFO - joeynmt.training - Epoch  16, Step:   435000, Batch Loss:     1.663306, Tokens per Sec:    12042, Lr: 0.000300\n",
            "2021-07-25 12:55:28,471 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 12:55:28,471 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 12:55:28,471 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 12:55:29,441 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 12:55:29,442 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 12:55:30,237 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 12:55:30,238 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 12:55:30,238 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 12:55:30,238 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 12:55:30,238 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 12:55:30,239 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 12:55:30,239 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 12:55:30,239 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 12:55:30,239 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 12:55:30,240 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 12:55:30,240 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 12:55:30,240 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 12:55:30,240 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 12:55:30,241 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 12:55:30,241 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 12:55:30,241 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
            "2021-07-25 12:55:30,241 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   435000: bleu:  20.54, loss: 168495.3125, ppl:   6.5124, duration: 97.0256s\n",
            "2021-07-25 12:55:58,371 - INFO - joeynmt.training - Epoch  16, Step:   435200, Batch Loss:     1.426962, Tokens per Sec:    15471, Lr: 0.000300\n",
            "2021-07-25 12:56:26,335 - INFO - joeynmt.training - Epoch  16, Step:   435400, Batch Loss:     1.490728, Tokens per Sec:    15871, Lr: 0.000300\n",
            "2021-07-25 12:56:54,654 - INFO - joeynmt.training - Epoch  16, Step:   435600, Batch Loss:     1.902932, Tokens per Sec:    15866, Lr: 0.000300\n",
            "2021-07-25 12:57:22,327 - INFO - joeynmt.training - Epoch  16, Step:   435800, Batch Loss:     2.033774, Tokens per Sec:    15787, Lr: 0.000300\n",
            "2021-07-25 12:57:50,363 - INFO - joeynmt.training - Epoch  16, Step:   436000, Batch Loss:     1.588009, Tokens per Sec:    15616, Lr: 0.000300\n",
            "2021-07-25 12:58:18,169 - INFO - joeynmt.training - Epoch  16, Step:   436200, Batch Loss:     1.939404, Tokens per Sec:    15546, Lr: 0.000300\n",
            "2021-07-25 12:58:46,233 - INFO - joeynmt.training - Epoch  16, Step:   436400, Batch Loss:     1.968183, Tokens per Sec:    15411, Lr: 0.000300\n",
            "2021-07-25 12:59:14,153 - INFO - joeynmt.training - Epoch  16, Step:   436600, Batch Loss:     1.843514, Tokens per Sec:    15264, Lr: 0.000300\n",
            "2021-07-25 12:59:42,125 - INFO - joeynmt.training - Epoch  16, Step:   436800, Batch Loss:     1.457779, Tokens per Sec:    15446, Lr: 0.000300\n",
            "2021-07-25 13:00:09,933 - INFO - joeynmt.training - Epoch  16, Step:   437000, Batch Loss:     1.554703, Tokens per Sec:    15673, Lr: 0.000300\n",
            "2021-07-25 13:00:38,242 - INFO - joeynmt.training - Epoch  16, Step:   437200, Batch Loss:     1.710443, Tokens per Sec:    15698, Lr: 0.000300\n",
            "2021-07-25 13:01:05,861 - INFO - joeynmt.training - Epoch  16, Step:   437400, Batch Loss:     1.571870, Tokens per Sec:    15261, Lr: 0.000300\n",
            "2021-07-25 13:01:33,434 - INFO - joeynmt.training - Epoch  16, Step:   437600, Batch Loss:     2.039142, Tokens per Sec:    15529, Lr: 0.000300\n",
            "2021-07-25 13:02:01,350 - INFO - joeynmt.training - Epoch  16, Step:   437800, Batch Loss:     1.694200, Tokens per Sec:    15589, Lr: 0.000300\n",
            "2021-07-25 13:02:29,314 - INFO - joeynmt.training - Epoch  16, Step:   438000, Batch Loss:     1.676336, Tokens per Sec:    15631, Lr: 0.000300\n",
            "2021-07-25 13:02:57,461 - INFO - joeynmt.training - Epoch  16, Step:   438200, Batch Loss:     1.742146, Tokens per Sec:    15615, Lr: 0.000300\n",
            "2021-07-25 13:03:25,226 - INFO - joeynmt.training - Epoch  16, Step:   438400, Batch Loss:     1.737757, Tokens per Sec:    15678, Lr: 0.000300\n",
            "2021-07-25 13:03:53,272 - INFO - joeynmt.training - Epoch  16, Step:   438600, Batch Loss:     1.663355, Tokens per Sec:    15454, Lr: 0.000300\n",
            "2021-07-25 13:04:21,083 - INFO - joeynmt.training - Epoch  16, Step:   438800, Batch Loss:     1.856871, Tokens per Sec:    15419, Lr: 0.000300\n",
            "2021-07-25 13:04:48,811 - INFO - joeynmt.training - Epoch  16, Step:   439000, Batch Loss:     1.649889, Tokens per Sec:    15461, Lr: 0.000300\n",
            "2021-07-25 13:05:16,904 - INFO - joeynmt.training - Epoch  16, Step:   439200, Batch Loss:     1.803917, Tokens per Sec:    15407, Lr: 0.000300\n",
            "2021-07-25 13:05:44,897 - INFO - joeynmt.training - Epoch  16, Step:   439400, Batch Loss:     2.088764, Tokens per Sec:    15328, Lr: 0.000300\n",
            "2021-07-25 13:06:13,013 - INFO - joeynmt.training - Epoch  16, Step:   439600, Batch Loss:     1.924254, Tokens per Sec:    15700, Lr: 0.000300\n",
            "2021-07-25 13:06:40,978 - INFO - joeynmt.training - Epoch  16, Step:   439800, Batch Loss:     1.940149, Tokens per Sec:    15567, Lr: 0.000300\n",
            "2021-07-25 13:07:08,728 - INFO - joeynmt.training - Epoch  16, Step:   440000, Batch Loss:     1.693139, Tokens per Sec:    15591, Lr: 0.000300\n",
            "2021-07-25 13:08:46,856 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 13:08:46,857 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 13:08:46,857 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 13:08:48,559 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 13:08:48,560 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 13:08:48,560 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 13:08:48,560 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 13:08:48,560 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 13:08:48,561 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 13:08:48,561 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 13:08:48,561 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 13:08:48,562 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 13:08:48,562 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 13:08:48,562 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 13:08:48,562 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
            "2021-07-25 13:08:48,563 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 13:08:48,563 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 13:08:48,563 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 13:08:48,563 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to heal his family .\n",
            "2021-07-25 13:08:48,564 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   440000: bleu:  20.41, loss: 169407.3281, ppl:   6.5788, duration: 99.8352s\n",
            "2021-07-25 13:09:16,680 - INFO - joeynmt.training - Epoch  16, Step:   440200, Batch Loss:     1.748371, Tokens per Sec:    15692, Lr: 0.000300\n",
            "2021-07-25 13:09:44,566 - INFO - joeynmt.training - Epoch  16, Step:   440400, Batch Loss:     1.106771, Tokens per Sec:    15821, Lr: 0.000300\n",
            "2021-07-25 13:10:12,957 - INFO - joeynmt.training - Epoch  16, Step:   440600, Batch Loss:     1.560965, Tokens per Sec:    15779, Lr: 0.000300\n",
            "2021-07-25 13:10:41,045 - INFO - joeynmt.training - Epoch  16, Step:   440800, Batch Loss:     1.911243, Tokens per Sec:    15585, Lr: 0.000300\n",
            "2021-07-25 13:11:08,821 - INFO - joeynmt.training - Epoch  16, Step:   441000, Batch Loss:     1.929245, Tokens per Sec:    15544, Lr: 0.000300\n",
            "2021-07-25 13:11:36,882 - INFO - joeynmt.training - Epoch  16, Step:   441200, Batch Loss:     1.580867, Tokens per Sec:    15429, Lr: 0.000300\n",
            "2021-07-25 13:12:04,689 - INFO - joeynmt.training - Epoch  16, Step:   441400, Batch Loss:     1.646990, Tokens per Sec:    15583, Lr: 0.000300\n",
            "2021-07-25 13:12:32,721 - INFO - joeynmt.training - Epoch  16, Step:   441600, Batch Loss:     1.896563, Tokens per Sec:    15372, Lr: 0.000300\n",
            "2021-07-25 13:13:00,529 - INFO - joeynmt.training - Epoch  16, Step:   441800, Batch Loss:     1.733858, Tokens per Sec:    15487, Lr: 0.000300\n",
            "2021-07-25 13:13:28,617 - INFO - joeynmt.training - Epoch  16, Step:   442000, Batch Loss:     1.669731, Tokens per Sec:    15704, Lr: 0.000300\n",
            "2021-07-25 13:13:56,300 - INFO - joeynmt.training - Epoch  16, Step:   442200, Batch Loss:     1.617889, Tokens per Sec:    15491, Lr: 0.000300\n",
            "2021-07-25 13:14:24,251 - INFO - joeynmt.training - Epoch  16, Step:   442400, Batch Loss:     1.710153, Tokens per Sec:    15648, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1xyUA3xBzEf"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 450000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/models/lg_rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued5/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_rw_lhen_reverse_transformer\"', f'model_dir: \"models/lg_rw_lhen_reverse_transformer_continued6\"').replace(\n",
        "        f'epochs: 30', f'epochs: 2')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload6.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QF61fu_DE0L",
        "collapsed": true,
        "outputId": "7098a50b-abb6-4f89-8592-e78f201a86fc"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_lg_rw_lhen_reload6.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"lg_rw_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"lg_rw_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued5/450000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 2                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/lg_rw_lhen_reverse_transformer_continued6\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3471Bq5DElU",
        "outputId": "8b534ed3-0f92-4bc9-de13-f8aa814af722"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_rw_lhen_reload6.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-25 14:33:18,079 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-25 14:33:18,166 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-25 14:33:36,505 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-25 14:33:37,627 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-25 14:33:41,987 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-25 14:33:43,510 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-25 14:33:43,510 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-25 14:33:43,930 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-25 14:33:44.177548: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-25 14:33:48,669 - INFO - joeynmt.training - Total params: 12179456\n",
            "2021-07-25 14:33:57,385 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued5/450000.ckpt\n",
            "2021-07-25 14:34:01,803 - INFO - joeynmt.helpers - cfg.name                           : lg_rw_lhen_reverse_transformer\n",
            "2021-07-25 14:34:01,803 - INFO - joeynmt.helpers - cfg.data.src                       : lg_rw_lh\n",
            "2021-07-25 14:34:01,803 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-25 14:34:01,804 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/train.bpe\n",
            "2021-07-25 14:34:01,804 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe\n",
            "2021-07-25 14:34:01,804 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe\n",
            "2021-07-25 14:34:01,804 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-25 14:34:01,805 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-25 14:34:01,805 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-25 14:34:01,805 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-25 14:34:01,805 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/vocab.txt\n",
            "2021-07-25 14:34:01,806 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-25 14:34:01,806 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-25 14:34:01,806 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/joeynmt/models/lg_rw_lhen_reverse_transformer_continued5/450000.ckpt\n",
            "2021-07-25 14:34:01,806 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-25 14:34:01,806 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-25 14:34:01,807 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-25 14:34:01,807 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-25 14:34:01,807 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-25 14:34:01,807 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-25 14:34:01,808 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-25 14:34:01,808 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-25 14:34:01,808 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-25 14:34:01,808 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-25 14:34:01,809 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-25 14:34:01,809 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-25 14:34:01,809 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-25 14:34:01,809 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-25 14:34:01,810 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-25 14:34:01,810 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-25 14:34:01,810 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-25 14:34:01,810 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-25 14:34:01,811 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-25 14:34:01,811 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-25 14:34:01,811 - INFO - joeynmt.helpers - cfg.training.epochs                : 2\n",
            "2021-07-25 14:34:01,811 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-25 14:34:01,811 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-25 14:34:01,812 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-25 14:34:01,812 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_rw_lhen_reverse_transformer_continued6\n",
            "2021-07-25 14:34:01,812 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-25 14:34:01,812 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-25 14:34:01,813 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-25 14:34:01,813 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-25 14:34:01,813 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-25 14:34:01,813 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-25 14:34:01,814 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-25 14:34:01,814 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-25 14:34:01,814 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-25 14:34:01,814 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-25 14:34:01,815 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-25 14:34:01,819 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-25 14:34:01,819 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-25 14:34:01,819 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-25 14:34:01,819 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-25 14:34:01,820 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-25 14:34:01,820 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-25 14:34:01,820 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-25 14:34:01,820 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-25 14:34:01,821 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-25 14:34:01,821 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-25 14:34:01,821 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-25 14:34:01,821 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-25 14:34:01,822 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-25 14:34:01,822 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-25 14:34:01,823 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-25 14:34:01,823 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-25 14:34:01,823 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-25 14:34:01,823 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-25 14:34:01,824 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-25 14:34:01,824 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-25 14:34:01,824 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 665203,\n",
            "\tvalid 3000,\n",
            "\ttest 1000\n",
            "2021-07-25 14:34:01,824 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ at@@ andika okuk@@ olera ku m@@ azima ge nn@@ ali nj@@ iga , era nn@@ ak@@ ir@@ aba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obuf@@ uzi n’@@ okul@@ eka em@@ ikw@@ ano em@@ ibi gye nn@@ alina .\n",
            "\t[TRG] Ev@@ ent@@ ually , however , the tr@@ uth@@ s I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my polit@@ ical view@@ po@@ in@@ ts and associ@@ ations .\n",
            "2021-07-25 14:34:01,825 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-25 14:34:01,825 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-25 14:34:01,825 - INFO - joeynmt.helpers - Number of Src words (types): 4372\n",
            "2021-07-25 14:34:01,826 - INFO - joeynmt.helpers - Number of Trg words (types): 4372\n",
            "2021-07-25 14:34:01,826 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4372),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4372))\n",
            "2021-07-25 14:34:01,843 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-25 14:34:01,844 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-25 14:35:04,702 - INFO - joeynmt.training - Epoch   1, Step:   450200, Batch Loss:     1.870718, Tokens per Sec:     6898, Lr: 0.000300\n",
            "2021-07-25 14:36:04,163 - INFO - joeynmt.training - Epoch   1, Step:   450400, Batch Loss:     2.054698, Tokens per Sec:     7359, Lr: 0.000300\n",
            "2021-07-25 14:37:03,087 - INFO - joeynmt.training - Epoch   1, Step:   450600, Batch Loss:     1.967670, Tokens per Sec:     7345, Lr: 0.000300\n",
            "2021-07-25 14:38:02,201 - INFO - joeynmt.training - Epoch   1, Step:   450800, Batch Loss:     1.874038, Tokens per Sec:     7260, Lr: 0.000300\n",
            "2021-07-25 14:39:01,635 - INFO - joeynmt.training - Epoch   1, Step:   451000, Batch Loss:     1.388085, Tokens per Sec:     7328, Lr: 0.000300\n",
            "2021-07-25 14:40:01,142 - INFO - joeynmt.training - Epoch   1, Step:   451200, Batch Loss:     1.757794, Tokens per Sec:     7356, Lr: 0.000300\n",
            "2021-07-25 14:40:59,663 - INFO - joeynmt.training - Epoch   1, Step:   451400, Batch Loss:     1.795303, Tokens per Sec:     7170, Lr: 0.000300\n",
            "2021-07-25 14:41:59,361 - INFO - joeynmt.training - Epoch   1, Step:   451600, Batch Loss:     1.586428, Tokens per Sec:     7306, Lr: 0.000300\n",
            "2021-07-25 14:42:26,331 - INFO - joeynmt.training - Epoch   1: total training loss 2918.57\n",
            "2021-07-25 14:42:26,331 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-25 14:42:59,544 - INFO - joeynmt.training - Epoch   2, Step:   451800, Batch Loss:     1.579963, Tokens per Sec:     7031, Lr: 0.000300\n",
            "2021-07-25 14:43:58,887 - INFO - joeynmt.training - Epoch   2, Step:   452000, Batch Loss:     1.632106, Tokens per Sec:     7239, Lr: 0.000300\n",
            "2021-07-25 14:44:58,806 - INFO - joeynmt.training - Epoch   2, Step:   452200, Batch Loss:     1.666543, Tokens per Sec:     7363, Lr: 0.000300\n",
            "2021-07-25 14:45:57,961 - INFO - joeynmt.training - Epoch   2, Step:   452400, Batch Loss:     1.572192, Tokens per Sec:     7283, Lr: 0.000300\n",
            "2021-07-25 14:46:57,493 - INFO - joeynmt.training - Epoch   2, Step:   452600, Batch Loss:     1.747883, Tokens per Sec:     7263, Lr: 0.000300\n",
            "2021-07-25 14:47:56,896 - INFO - joeynmt.training - Epoch   2, Step:   452800, Batch Loss:     1.479399, Tokens per Sec:     7321, Lr: 0.000300\n",
            "2021-07-25 14:48:56,364 - INFO - joeynmt.training - Epoch   2, Step:   453000, Batch Loss:     1.851855, Tokens per Sec:     7302, Lr: 0.000300\n",
            "2021-07-25 14:49:55,501 - INFO - joeynmt.training - Epoch   2, Step:   453200, Batch Loss:     1.894862, Tokens per Sec:     7312, Lr: 0.000300\n",
            "2021-07-25 14:50:55,419 - INFO - joeynmt.training - Epoch   2, Step:   453400, Batch Loss:     1.721789, Tokens per Sec:     7434, Lr: 0.000300\n",
            "2021-07-25 14:51:54,773 - INFO - joeynmt.training - Epoch   2, Step:   453600, Batch Loss:     1.812233, Tokens per Sec:     7327, Lr: 0.000300\n",
            "2021-07-25 14:52:54,371 - INFO - joeynmt.training - Epoch   2, Step:   453800, Batch Loss:     1.475904, Tokens per Sec:     7348, Lr: 0.000300\n",
            "2021-07-25 14:53:54,343 - INFO - joeynmt.training - Epoch   2, Step:   454000, Batch Loss:     1.450416, Tokens per Sec:     7409, Lr: 0.000300\n",
            "2021-07-25 14:54:53,978 - INFO - joeynmt.training - Epoch   2, Step:   454200, Batch Loss:     1.795487, Tokens per Sec:     7319, Lr: 0.000300\n",
            "2021-07-25 14:55:53,351 - INFO - joeynmt.training - Epoch   2, Step:   454400, Batch Loss:     1.618222, Tokens per Sec:     7278, Lr: 0.000300\n",
            "2021-07-25 14:56:52,198 - INFO - joeynmt.training - Epoch   2, Step:   454600, Batch Loss:     1.345832, Tokens per Sec:     7221, Lr: 0.000300\n",
            "2021-07-25 14:57:51,776 - INFO - joeynmt.training - Epoch   2, Step:   454800, Batch Loss:     1.789911, Tokens per Sec:     7337, Lr: 0.000300\n",
            "2021-07-25 14:58:50,556 - INFO - joeynmt.training - Epoch   2, Step:   455000, Batch Loss:     1.745071, Tokens per Sec:     7243, Lr: 0.000300\n",
            "2021-07-25 15:01:38,997 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 15:01:38,997 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 15:01:38,998 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 15:01:41,074 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 15:01:41,075 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 15:01:41,075 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 15:01:41,075 - INFO - joeynmt.training - \tHypothesis: “ If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 15:01:41,076 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 15:01:41,076 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 15:01:41,076 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 15:01:41,077 - INFO - joeynmt.training - \tHypothesis: After his baptism , Jesus heard a voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 15:01:41,077 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 15:01:41,078 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 15:01:41,078 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 15:01:41,078 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 15:01:41,078 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 15:01:41,079 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 15:01:41,079 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 15:01:41,080 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to heal his family .\n",
            "2021-07-25 15:01:41,080 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   455000: bleu:  20.47, loss: 168458.3438, ppl:   6.5097, duration: 170.5236s\n",
            "2021-07-25 15:02:40,306 - INFO - joeynmt.training - Epoch   2, Step:   455200, Batch Loss:     1.776727, Tokens per Sec:     7286, Lr: 0.000300\n",
            "2021-07-25 15:03:39,659 - INFO - joeynmt.training - Epoch   2, Step:   455400, Batch Loss:     1.825340, Tokens per Sec:     7255, Lr: 0.000300\n",
            "2021-07-25 15:04:38,976 - INFO - joeynmt.training - Epoch   2, Step:   455600, Batch Loss:     1.723213, Tokens per Sec:     7398, Lr: 0.000300\n",
            "2021-07-25 15:05:38,275 - INFO - joeynmt.training - Epoch   2, Step:   455800, Batch Loss:     1.689160, Tokens per Sec:     7342, Lr: 0.000300\n",
            "2021-07-25 15:06:37,638 - INFO - joeynmt.training - Epoch   2, Step:   456000, Batch Loss:     1.738349, Tokens per Sec:     7365, Lr: 0.000300\n",
            "2021-07-25 15:07:36,730 - INFO - joeynmt.training - Epoch   2, Step:   456200, Batch Loss:     1.587106, Tokens per Sec:     7323, Lr: 0.000300\n",
            "2021-07-25 15:08:36,441 - INFO - joeynmt.training - Epoch   2, Step:   456400, Batch Loss:     1.885710, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-25 15:09:35,909 - INFO - joeynmt.training - Epoch   2, Step:   456600, Batch Loss:     1.722090, Tokens per Sec:     7342, Lr: 0.000300\n",
            "2021-07-25 15:10:35,956 - INFO - joeynmt.training - Epoch   2, Step:   456800, Batch Loss:     1.963807, Tokens per Sec:     7403, Lr: 0.000300\n",
            "2021-07-25 15:11:35,590 - INFO - joeynmt.training - Epoch   2, Step:   457000, Batch Loss:     1.857085, Tokens per Sec:     7381, Lr: 0.000300\n",
            "2021-07-25 15:12:35,130 - INFO - joeynmt.training - Epoch   2, Step:   457200, Batch Loss:     1.823772, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-25 15:13:34,408 - INFO - joeynmt.training - Epoch   2, Step:   457400, Batch Loss:     1.801693, Tokens per Sec:     7304, Lr: 0.000300\n",
            "2021-07-25 15:14:33,469 - INFO - joeynmt.training - Epoch   2, Step:   457600, Batch Loss:     1.618417, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-25 15:15:32,939 - INFO - joeynmt.training - Epoch   2, Step:   457800, Batch Loss:     1.654106, Tokens per Sec:     7338, Lr: 0.000300\n",
            "2021-07-25 15:16:32,505 - INFO - joeynmt.training - Epoch   2, Step:   458000, Batch Loss:     1.261650, Tokens per Sec:     7385, Lr: 0.000300\n",
            "2021-07-25 15:17:31,521 - INFO - joeynmt.training - Epoch   2, Step:   458200, Batch Loss:     1.724920, Tokens per Sec:     7329, Lr: 0.000300\n",
            "2021-07-25 15:18:30,715 - INFO - joeynmt.training - Epoch   2, Step:   458400, Batch Loss:     1.782241, Tokens per Sec:     7330, Lr: 0.000300\n",
            "2021-07-25 15:19:29,356 - INFO - joeynmt.training - Epoch   2, Step:   458600, Batch Loss:     1.392652, Tokens per Sec:     7187, Lr: 0.000300\n",
            "2021-07-25 15:20:29,216 - INFO - joeynmt.training - Epoch   2, Step:   458800, Batch Loss:     1.767805, Tokens per Sec:     7332, Lr: 0.000300\n",
            "2021-07-25 15:21:28,251 - INFO - joeynmt.training - Epoch   2, Step:   459000, Batch Loss:     1.424583, Tokens per Sec:     7360, Lr: 0.000300\n",
            "2021-07-25 15:22:26,878 - INFO - joeynmt.training - Epoch   2, Step:   459200, Batch Loss:     1.680749, Tokens per Sec:     7248, Lr: 0.000300\n",
            "2021-07-25 15:23:26,110 - INFO - joeynmt.training - Epoch   2, Step:   459400, Batch Loss:     1.716934, Tokens per Sec:     7231, Lr: 0.000300\n",
            "2021-07-25 15:24:25,310 - INFO - joeynmt.training - Epoch   2, Step:   459600, Batch Loss:     1.776877, Tokens per Sec:     7219, Lr: 0.000300\n",
            "2021-07-25 15:25:24,490 - INFO - joeynmt.training - Epoch   2, Step:   459800, Batch Loss:     1.699983, Tokens per Sec:     7244, Lr: 0.000300\n",
            "2021-07-25 15:26:23,738 - INFO - joeynmt.training - Epoch   2, Step:   460000, Batch Loss:     1.686639, Tokens per Sec:     7372, Lr: 0.000300\n",
            "2021-07-25 15:29:14,067 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 15:29:14,067 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 15:29:14,068 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 15:29:16,152 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 15:29:16,153 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 15:29:16,153 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 15:29:16,153 - INFO - joeynmt.training - \tHypothesis: “ If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 15:29:16,154 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 15:29:16,154 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 15:29:16,155 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 15:29:16,155 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 15:29:16,155 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 15:29:16,156 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 15:29:16,156 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 15:29:16,156 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 15:29:16,156 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 15:29:16,157 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 15:29:16,157 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 15:29:16,157 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to heal his family .\n",
            "2021-07-25 15:29:16,158 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   460000: bleu:  20.45, loss: 168577.0625, ppl:   6.5183, duration: 172.4187s\n",
            "2021-07-25 15:29:34,504 - INFO - joeynmt.training - Epoch   2: total training loss 14412.55\n",
            "2021-07-25 15:29:34,504 - INFO - joeynmt.training - Training ended after   2 epochs.\n",
            "2021-07-25 15:29:34,505 - INFO - joeynmt.training - Best validation result (greedy) at step   450000:   6.49 ppl.\n",
            "2021-07-25 15:29:34,531 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
            "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
            "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual/joeynmt/joeynmt/training.py\", line 822, in train\n",
            "    datasets=datasets_to_test)\n",
            "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual/joeynmt/joeynmt/prediction.py\", line 307, in test\n",
            "    model_checkpoint = load_checkpoint(ckpt, use_cuda=use_cuda)\n",
            "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual/joeynmt/joeynmt/helpers.py\", line 278, in load_checkpoint\n",
            "    assert os.path.isfile(path), \"Checkpoint %s not found\" % path\n",
            "AssertionError: Checkpoint models/lg_rw_lhen_reverse_transformer_continued6/450000.ckpt not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKW9fdMyDEfk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu6AbiCHFZZC",
        "outputId": "6b404ebc-76e3-4e78-8b67-a3b5047edfdc"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt test 'models/lg_rw_lhen_reverse_transformer_continued6/config.yaml'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-26 09:33:10,321 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-26 09:33:10,339 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-26 09:33:11,544 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-26 09:33:13,098 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-26 09:33:14,166 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-26 09:33:14,231 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-26 09:33:23,372 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-26 09:33:23,780 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-26 09:33:23,864 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/dev.bpe.en)...\n",
            "2021-07-26 09:37:18,554 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-26 09:37:18,555 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-26 09:37:18,555 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-26 09:37:19,657 - INFO - joeynmt.prediction -  dev bleu[13a]:  20.81 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-26 09:37:19,657 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.en)...\n",
            "2021-07-26 09:39:06,760 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-26 09:39:06,761 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-26 09:39:06,761 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-26 09:39:07,158 - INFO - joeynmt.prediction - test bleu[13a]:  10.21 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHKUlyc6FZZE",
        "outputId": "679ee499-c835-4d5e-e2f1-bab53da34b80"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_rw_lhen_reverse_transformer_continued6/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/translation2.bpe.lh_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-26 09:39:09,755 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-26 09:39:12,630 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-26 09:39:12,889 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JA1Gj8jFZZF",
        "outputId": "4a97a037-d9b3-413b-8c6d-08b7bd921377"
      },
      "source": [
        "!cat \"translation2.bpe.lh_en\" | sacrebleu \"test1.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 10.2 41.6/15.1/6.5/3.4 (BP = 0.944 ratio = 0.946 hyp_len = 24628 ref_len = 26044)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LnWchoVFZZG",
        "outputId": "d0fc2b85-a686-4d1f-b6aa-2df024d43329"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_rw_lhen_reverse_transformer_continued6/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.rw\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/translation2.bpe.rw_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-26 09:41:04,443 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-26 09:41:07,329 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-26 09:41:07,598 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKy0juN2FZZH",
        "outputId": "eda74840-af3d-4b21-9ff1-0d57b0f38839"
      },
      "source": [
        "!cat \"translation2.bpe.rw_en\" | sacrebleu \"test3.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 38.2 69.0/47.4/36.5/29.1 (BP = 0.885 ratio = 0.891 hyp_len = 37818 ref_len = 42439)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg9qtYOJFZZI",
        "outputId": "f0599236-68e0-4863-c2d2-3a8919842146"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_rw_lhen_reverse_transformer_continued6/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/test.bpe.lg\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/translation2.bpe.lg_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-26 09:42:55,100 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-26 09:42:57,936 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-26 09:42:58,207 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0w7YiaPFZZK",
        "outputId": "bfebe8a9-c24c-437f-d349-0e8302bd1adc"
      },
      "source": [
        "!cat \"translation2.bpe.lg_en\" | sacrebleu \"test2.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 37.1 66.9/45.4/34.3/27.1 (BP = 0.904 ratio = 0.909 hyp_len = 39177 ref_len = 43116)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}