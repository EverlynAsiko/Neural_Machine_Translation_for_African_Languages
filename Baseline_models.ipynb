{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EverlynAsiko/Neural_Machine_Translation_for_African_Languages/blob/main/Baseline_models_results1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cz8WB8I_MZlH"
   },
   "source": [
    "# Summary of Baseline Models \n",
    "\n",
    "**Overview:**\n",
    "1. Text preprocessing\n",
    "2. Inputs of the transformer\n",
    "3. Workings of a transformer: *Submitted write up*\n",
    "4. Results of baseline models\n",
    "\n",
    "Codes are adapted from Masakhane reverse model notebook: https://github.com/masakhane-io/masakhane-mt/blob/master/starter_notebook_into_English_training.ipynb\n",
    "\n",
    "Changes made include:\n",
    "1. Additional models.\n",
    "2. Generating csv from dataframes created.\n",
    "3. Using of checkpoint to resume training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS8ZWHMS7JLs"
   },
   "source": [
    "#### Setting up locations and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1yiVhhB6l_p",
    "outputId": "cb743096-3ef6-41bb-8ed2-843681061fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Linking to drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NUnPw-P7-Uz"
   },
   "outputs": [],
   "source": [
    "# Importing needed libraries for preprocessing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "iXfU_ii3__jA",
    "outputId": "4da017d5-d1ce-4f0c-bf03-0b00e9f8041d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.8.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "# Install Pytorch with GPU support v1.8.0.\n",
    "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0QiN_EHEE-s"
   },
   "outputs": [],
   "source": [
    "# Filtering warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2_ah4uX8JyW"
   },
   "outputs": [],
   "source": [
    "# Loading the drive\n",
    "import os\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0v_iKryEpww"
   },
   "outputs": [],
   "source": [
    "# Setting source and target languages\n",
    "source_language = \"en\"\n",
    "target_language1 = \"lg\"\n",
    "target_language2 = \"rw\"\n",
    "target_language3 = \"lh\"\n",
    "\n",
    "os.environ[\"src\"] = source_language \n",
    "os.environ[\"tgt1\"] = target_language1\n",
    "os.environ[\"tgt2\"] = target_language2\n",
    "os.environ[\"tgt3\"] = target_language3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djKD3lcy7Amu"
   },
   "source": [
    "# Getting Data\n",
    "\n",
    "JW300 to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk6qPrUeDg2L"
   },
   "outputs": [],
   "source": [
    "# Installing package to retrieve datasets\n",
    "! pip install opustools-pkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ZrPmbdBKji"
   },
   "source": [
    "## Luganda   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfSeE3eeb8E"
   },
   "source": [
    "### Turning data from JW300 to dataframe\n",
    "\n",
    "**Do not rerun**: Load pandas dataframe instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvWydhNxLNR_"
   },
   "outputs": [],
   "source": [
    "# Changing to Luganda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wv3vUXFDBJLv",
    "outputId": "0be987c5-ff26-4ef8-9d4c-46d1643fb9aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-lg.xml.gz not found. The following files are available for downloading:\n",
      "\n",
      "   3 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/en-lg.xml.gz\n",
      " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/en.zip\n",
      "  22 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/lg.zip\n",
      "\n",
      " 288 MB Total size\n",
      "./JW300_latest_xml_en-lg.xml.gz ... 100% of 3 MB\n",
      "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
      "./JW300_latest_xml_lg.zip ... 100% of 22 MB\n"
     ]
    }
   ],
   "source": [
    "# Downloading our corpus\n",
    "! opus_read -d JW300 -s $src -t $tgt1 -wm moses -w jw300.$src jw300.$tgt1 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqgcbvV3BHm2"
   },
   "outputs": [],
   "source": [
    "# extract the corpus file\n",
    "! gunzip JW300_latest_xml_$src-$tgt1.xml.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MP4TiZ0UPbzv",
    "outputId": "e97430c9-2ec7-40f6-edc9-b85321a928a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-14 10:19:53--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 277791 (271K) [text/plain]\n",
      "Saving to: ‘test.en-any.en.1’\n",
      "\n",
      "\r",
      "test.en-any.en.1      0%[                    ]       0  --.-KB/s               \r",
      "test.en-any.en.1    100%[===================>] 271.28K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-05-14 10:19:53 (8.11 MB/s) - ‘test.en-any.en.1’ saved [277791/277791]\n",
      "\n",
      "--2021-05-14 10:19:54--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-lg.en\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 204598 (200K) [text/plain]\n",
      "Saving to: ‘test.en-lg.en’\n",
      "\n",
      "test.en-lg.en       100%[===================>] 199.80K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-05-14 10:19:54 (10.4 MB/s) - ‘test.en-lg.en’ saved [204598/204598]\n",
      "\n",
      "--2021-05-14 10:19:54--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-lg.lg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 223728 (218K) [text/plain]\n",
      "Saving to: ‘test.en-lg.lg’\n",
      "\n",
      "test.en-lg.lg       100%[===================>] 218.48K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-05-14 10:19:54 (10.1 MB/s) - ‘test.en-lg.lg’ saved [223728/223728]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading test set\n",
    "# Download the global test set.\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
    "\n",
    "# Specific test set\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$tgt1.en \n",
    "! mv test.en-$tgt1.en test.en\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$tgt1.$tgt1 \n",
    "! mv test.en-$tgt1.$tgt1 test.$tgt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beTSnRB_D2VE",
    "outputId": "d960dcd5-d75c-4d51-b322-182f62fbfaa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3571 global test sentences to filter from the training/dev data.\n"
     ]
    }
   ],
   "source": [
    "# Read the test data to filter from train and dev splits.\n",
    "# Store english portion in set for quick filtering checks.\n",
    "en_test_sents = set()\n",
    "filter_test_sents = \"test.en-any.en\"\n",
    "j = 0\n",
    "with open(filter_test_sents) as f:\n",
    "  for line in f:\n",
    "    en_test_sents.add(line.strip())\n",
    "    j += 1\n",
    "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "qsEsi86zAv9W",
    "outputId": "a1b5d2a7-258a-4716-faac-3e7ab4765c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and skipped 5229/254723 lines since contained in test set.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "      <td>Akatabo kano tekatundibwa .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "      <td>OMUTWE OGULI KUNGULU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "      <td>Bayibuli yamalirizibwa okuwandiikibwa emyaka n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_sentence                                    target_sentence\n",
       "0                 This publication is not for sale .                        Akatabo kano tekatundibwa .\n",
       "1                                      COVER SUBJECT                               OMUTWE OGULI KUNGULU\n",
       "2  The Bible was completed about two thousand yea...  Bayibuli yamalirizibwa okuwandiikibwa emyaka n..."
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TMX file to dataframe\n",
    "source_file = 'jw300.' + source_language\n",
    "target_file = 'jw300.' + target_language1\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
    "with open(source_file) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Skip sentences that are contained in the test set.\n",
    "        if line.strip() not in en_test_sents:\n",
    "            source.append(line.strip())\n",
    "        else:\n",
    "            skip_lines.append(i)             \n",
    "with open(target_file) as f:\n",
    "    for j, line in enumerate(f):\n",
    "        # Only add to corpus if corresponding source was not skipped.\n",
    "        if j not in skip_lines:\n",
    "            target.append(line.strip())\n",
    "    \n",
    "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
    "    \n",
    "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_1XnM7LZ_6F"
   },
   "outputs": [],
   "source": [
    "# Luganda training set\n",
    "df.to_csv('Luganda.csv',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGimJSBX4PJ5"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "GZsYvkKf5M15",
    "outputId": "3e5f39d8-698f-43f1-f69d-11c7ff1814ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "      <td>Akatabo kano tekatundibwa .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "      <td>OMUTWE OGULI KUNGULU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "      <td>Bayibuli yamalirizibwa okuwandiikibwa emyaka n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_sentence                                    target_sentence\n",
       "0                 This publication is not for sale .                        Akatabo kano tekatundibwa .\n",
       "1                                      COVER SUBJECT                               OMUTWE OGULI KUNGULU\n",
       "2  The Bible was completed about two thousand yea...  Bayibuli yamalirizibwa okuwandiikibwa emyaka n..."
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lug = pd.read_csv(\"Luganda.csv\")\n",
    "lug.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kbxd6FvEYtz"
   },
   "outputs": [],
   "source": [
    "# drop duplicate translations\n",
    "df_pp = lug.drop_duplicates()\n",
    "\n",
    "# drop conflicting translations\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Shuffle the data to remove bias in dev set selection.\n",
    "df_pp = df_pp.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRtw68PW7oQF"
   },
   "outputs": [],
   "source": [
    "# reset the index of the training set after previous filtering\n",
    "df_pp.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU2ApSnZDWYl"
   },
   "outputs": [],
   "source": [
    "df_pp.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2wSBFDn_3RM",
    "outputId": "6f309933-08e1-4055-dfcc-3dcbf658460f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index              0\n",
       "source_sentence    0\n",
       "target_sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsDo5Kh6-8nA"
   },
   "outputs": [],
   "source": [
    "# Splitting train and validation set\n",
    "num_valid = 1000\n",
    "\n",
    "dev = df_pp.tail(num_valid) \n",
    "stripped = df_pp.drop(df_pp.tail(num_valid).index)\n",
    "\n",
    "# Creating files for luganda and english\n",
    "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language1, \"w\") as trg_file:\n",
    "  for index, row in stripped.iterrows():\n",
    "    try:\n",
    "      src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "      trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "    except TypeError:\n",
    "      print(index,row[\"target_sentence\"])\n",
    "    \n",
    "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language1, \"w\") as trg_file:\n",
    "  for index, row in dev.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GnjVaB_5a2a",
    "outputId": "edfc8fcc-a3ea-40f6-e003-2f07807c5aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.bpe.en <==\n",
      "Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "At last , I have the st@@ able family life that I always cr@@ av@@ ed , and I have the loving Father that I always wanted .\n",
      "I was a new husband , only 25 years old and very in@@ experienced , but off we went with confidence in Jehovah .\n",
      "What can you do to show these de@@ a@@ f brothers personal attention ?\n",
      "R@@ ef@@ er@@ r@@ ing to what the rul@@ er@@ ship of God’s Son will accompl@@ ish , Isaiah 9 : 7 says : “ The very z@@ eal of Jehovah of arm@@ ies will do this . ”\n",
      "Jesus is the m@@ igh@@ ti@@ est of all of Jehovah’s spirit sons .\n",
      "The ste@@ ad@@ f@@ ast example set by J@@ ac@@ o@@ b and R@@ ac@@ he@@ l no doubt had a powerful effect on their son Joseph , influ@@ enc@@ ing how he would hand@@ le t@@ ests of his own faith .\n",
      "When s@@ ent@@ enc@@ ing “ the orig@@ in@@ al ser@@ p@@ ent , ” Satan the Devil , God said : “ I shall put en@@ m@@ ity between you and the woman and between your se@@ ed and her se@@ ed . He will br@@ u@@ ise you in the head and you will br@@ u@@ ise him in the h@@ ee@@ l . ”\n",
      "Will this or@@ de@@ al bring David down to S@@ he@@ ol in g@@ ri@@ ef and dis@@ gr@@ ace ?\n",
      "How can Christian love help to strengthen the marriage b@@ ond ?\n",
      "\n",
      "==> train.bpe.lg <==\n",
      "Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "N@@ ze ne mukyala wange tuli basanyufu , era nk@@ imanyi nti Katonda anj@@ agala nnyo .\n",
      "Mu kiseera ekyo , nnali nn@@ aak@@ aw@@ asa , nga ndi wa myaka 25 gy@@ okka , era nga s@@ ir@@ ina b@@ um@@ any@@ irivu . Naye nnali muk@@ akafu nti Yakuwa yali ajja ku@@ nn@@ yamba .\n",
      "Mu kibiina k@@ yo bwe mu@@ ba nga mulimu bak@@ igg@@ ala , oyinza kukola ki okulaga nti o@@ faayo ku b’@@ oluganda abo ?\n",
      "Isaaya 9 : 7 wal@@ aga nti Omwana wa Katonda y@@ andibadde Kabaka era nti yand@@ ikol@@ edde abantu ebintu ebirungi bingi .\n",
      "Yesu y’@@ as@@ inga obuyinza mu ba@@ ana ba Yakuwa bonna ab’@@ omwoyo .\n",
      "Eky@@ okulabirako ekirungi Yakobo ne L@@ aak@@ e@@ eri kye baat@@ ek@@ awo mu kw@@ oleka obug@@ umiikiriza ky@@ akwata nnyo ku mutabani waabwe Yusufu , era ekyo ky@@ amu@@ yamba nnyo bwe yay@@ olekagana n’@@ embeera ez@@ aag@@ ez@@ esa okukkiriza kwe .\n",
      "Bwe yali as@@ alira omusango “ omus@@ ot@@ a ogw’@@ edda , ” Setaani Omulyolyomi , Katonda yagamba : “ O@@ bul@@ abe n’@@ abu@@ teek@@ anga wakati wo n’@@ omukazi , era ne wakati w’@@ ez@@ zadde l@@ yo n’@@ ez@@ zadde ly’@@ omukazi : ( ez@@ zadde ly’@@ omukazi ) l@@ iri@@ ku@@ be@@ t@@ ent@@ a omutwe , naawe ol@@ ir@@ ib@@ et@@ ent@@ a ekis@@ inzi@@ iro . ”\n",
      "Em@@ beera eno en@@ zibu en@@ e@@ er@@ eetera Dawudi okuk@@ ka em@@ ag@@ om@@ be nga mun@@ aku@@ w@@ avu ?\n",
      "Okwagala kw’@@ Ekikristaayo ku@@ yinza kutya okuny@@ weza obufumbo ?\n",
      "\n",
      "==> train.en <==\n",
      "Eventually , however , the truths I learned from the Bible began to sink deeper into my heart . I realized that if I wanted to serve Jehovah , I had to change my political viewpoints and associations .\n",
      "At last , I have the stable family life that I always craved , and I have the loving Father that I always wanted .\n",
      "I was a new husband , only 25 years old and very inexperienced , but off we went with confidence in Jehovah .\n",
      "What can you do to show these deaf brothers personal attention ?\n",
      "Referring to what the rulership of God’s Son will accomplish , Isaiah 9 : 7 says : “ The very zeal of Jehovah of armies will do this . ”\n",
      "Jesus is the mightiest of all of Jehovah’s spirit sons .\n",
      "The steadfast example set by Jacob and Rachel no doubt had a powerful effect on their son Joseph , influencing how he would handle tests of his own faith .\n",
      "When sentencing “ the original serpent , ” Satan the Devil , God said : “ I shall put enmity between you and the woman and between your seed and her seed . He will bruise you in the head and you will bruise him in the heel . ”\n",
      "Will this ordeal bring David down to Sheol in grief and disgrace ?\n",
      "How can Christian love help to strengthen the marriage bond ?\n",
      "\n",
      "==> train.lg <==\n",
      "Naye oluvannyuma lw’ekiseera , nnatandika okukolera ku mazima ge nnali njiga , era nnakiraba nti okusobola okuweereza Yakuwa nnalina okuva mu by’obufuzi n’okuleka emikwano emibi gye nnalina .\n",
      "Nze ne mukyala wange tuli basanyufu , era nkimanyi nti Katonda anjagala nnyo .\n",
      "Mu kiseera ekyo , nnali nnaakawasa , nga ndi wa myaka 25 gyokka , era nga sirina bumanyirivu . Naye nnali mukakafu nti Yakuwa yali ajja kunnyamba .\n",
      "Mu kibiina kyo bwe muba nga mulimu bakiggala , oyinza kukola ki okulaga nti ofaayo ku b’oluganda abo ?\n",
      "Isaaya 9 : 7 walaga nti Omwana wa Katonda yandibadde Kabaka era nti yandikoledde abantu ebintu ebirungi bingi .\n",
      "Yesu y’asinga obuyinza mu baana ba Yakuwa bonna ab’omwoyo .\n",
      "Ekyokulabirako ekirungi Yakobo ne Laakeeri kye baatekawo mu kwoleka obugumiikiriza kyakwata nnyo ku mutabani waabwe Yusufu , era ekyo kyamuyamba nnyo bwe yayolekagana n’embeera ezaagezesa okukkiriza kwe .\n",
      "Bwe yali asalira omusango “ omusota ogw’edda , ” Setaani Omulyolyomi , Katonda yagamba : “ Obulabe n’abuteekanga wakati wo n’omukazi , era ne wakati w’ezzadde lyo n’ezzadde ly’omukazi : ( ezzadde ly’omukazi ) lirikubetenta omutwe , naawe oliribetenta ekisinziiro . ”\n",
      "Embeera eno enzibu eneereetera Dawudi okukka emagombe nga munakuwavu ?\n",
      "Okwagala kw’Ekikristaayo kuyinza kutya okunyweza obufumbo ?\n",
      "==> dev.bpe.en <==\n",
      "But if , when you are doing good and you su@@ ff@@ er , you end@@ ure it , this is a thing ag@@ re@@ e@@ able with God . ”\n",
      "At his bap@@ tism , Jesus heard a vo@@ ice from heaven say : “ This is my Son , the bel@@ ov@@ ed , whom I have ap@@ proved . ”\n",
      "( b ) How did Jehovah answer Jesus ’ personal requ@@ est about his future ?\n",
      "But Ab@@ ig@@ a@@ il took action to s@@ ave her hou@@ se@@ hold .\n",
      "While anger is not one of God’s d@@ om@@ in@@ ant qualities , he is prov@@ ok@@ ed to righteous ind@@ ign@@ ation by del@@ ib@@ er@@ ate ac@@ ts of in@@ justice , especially when the v@@ ic@@ tim@@ s are v@@ ul@@ n@@ er@@ able ones . ​ — Psalm 10@@ 3 : 6 .\n",
      "TH@@ E TH@@ R@@ E@@ A@@ T : Mic@@ r@@ ob@@ es that live har@@ m@@ less@@ ly in@@ side an an@@ im@@ al can th@@ reat@@ en your health .\n",
      "What does Jehovah exp@@ ect of us in our service to him ?\n",
      "Jesus said that ‘ the Father in heaven gives holy spirit to those as@@ king him . ’\n",
      "9 , 10 . ( a ) What did Jehovah allow the Babyl@@ on@@ ians to do ?\n",
      "( b ) In harmon@@ y with Phili@@ pp@@ ians 1 : 7 , how have Jehovah’s people re@@ ac@@ ted to Satan’s anger ?\n",
      "\n",
      "==> dev.bpe.lg <==\n",
      "[ N ] aye bwe muk@@ ola obulungi ne mu@@ b@@ on@@ ya@@ abon@@ y@@ ez@@ ebwa bwe mul@@ ig@@ umiikiriza , ekyo kye kis@@ iimibwa eri Katonda . ”\n",
      "Yesu bwe yali ya@@ ak@@ amala okub@@ atizibwa , y@@ awulira edd@@ oboozi okuva mu ggulu nga l@@ ig@@ amba nti : “ O@@ no ye M@@ wana wange omw@@ agal@@ wa gwe n@@ si@@ ima . ”\n",
      "( b ) Yakuwa y@@ addamu atya ekyo Yesu kye ye@@ es@@ abira ?\n",
      "Naye Ab@@ big@@ ay@@ iri alina kye yak@@ ol@@ awo okusobola okuw@@ onya ab’omu maka ge .\n",
      "Wadde ng’@@ obus@@ ungu si ngeri ya Yakuwa en@@ kulu , as@@ ung@@ u@@ wala singa ab@@ an@@ aku n’@@ abat@@ alina bu@@ yambi bay@@ is@@ ibwa mu ngeri et@@ ali ya bw@@ enkanya . ​ — Zabbuli 10@@ 3 : 6 .\n",
      "O@@ B@@ UL@@ AB@@ E : Obu@@ w@@ uka obumu bus@@ obola okubeera mu bis@@ olo , ne bwe bib@@ a bya w@@ aka , ne bit@@ afuna bu@@ zibu bwonna , naye ate nga bwe bu@@ ku@@ y@@ ingiramu bu@@ kul@@ wa@@ za .\n",
      "Kiki Yakuwa ky’@@ at@@ us@@ uubir@@ amu nga tu@@ mu@@ weereza ?\n",
      "Yesu yagamba nti ‘ Kitaffe ali mu ggulu awa omwoyo omutukuvu abo ab@@ amus@@ aba . ’\n",
      "9 , 10 . ( a ) Kiki Yakuwa kye yak@@ kiriza Ab@@ abab@@ ulooni okukola ?\n",
      "( b ) Nga kitu@@ uk@@ agana ne Aba@@ f@@ iri@@ pi 1 : 7 , abantu ba Yakuwa bak@@ oze ki nga Setaani ab@@ o@@ olek@@ ezza obus@@ ungu bwe ?\n",
      "\n",
      "==> dev.en <==\n",
      "But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "But Abigail took action to save her household .\n",
      "While anger is not one of God’s dominant qualities , he is provoked to righteous indignation by deliberate acts of injustice , especially when the victims are vulnerable ones . ​ — Psalm 103 : 6 .\n",
      "THE THREAT : Microbes that live harmlessly inside an animal can threaten your health .\n",
      "What does Jehovah expect of us in our service to him ?\n",
      "Jesus said that ‘ the Father in heaven gives holy spirit to those asking him . ’\n",
      "9 , 10 . ( a ) What did Jehovah allow the Babylonians to do ?\n",
      "( b ) In harmony with Philippians 1 : 7 , how have Jehovah’s people reacted to Satan’s anger ?\n",
      "\n",
      "==> dev.lg <==\n",
      "[ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "Wadde ng’obusungu si ngeri ya Yakuwa enkulu , asunguwala singa abanaku n’abatalina buyambi bayisibwa mu ngeri etali ya bwenkanya . ​ — Zabbuli 103 : 6 .\n",
      "OBULABE : Obuwuka obumu busobola okubeera mu bisolo , ne bwe biba bya waka , ne bitafuna buzibu bwonna , naye ate nga bwe bukuyingiramu bukulwaza .\n",
      "Kiki Yakuwa ky’atusuubiramu nga tumuweereza ?\n",
      "Yesu yagamba nti ‘ Kitaffe ali mu ggulu awa omwoyo omutukuvu abo abamusaba . ’\n",
      "9 , 10 . ( a ) Kiki Yakuwa kye yakkiriza Abababulooni okukola ?\n",
      "( b ) Nga kituukagana ne Abafiripi 1 : 7 , abantu ba Yakuwa bakoze ki nga Setaani aboolekezza obusungu bwe ?\n"
     ]
    }
   ],
   "source": [
    "! head train.*\n",
    "! head dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "o49g7MLK9nyR",
    "outputId": "87afe5e8-3f76-4da4-fd17-418cb8eb9a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Luganda/joeynmt\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8a/064b4077e3d793f877e3b77aa64f56fa49a4d37236a53f78ee28be009a16/numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3MB 198kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.0.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/50/84184d6230686e230c464f0dd4ff32eada2756b4a0b9cefec68b88d1d580/torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 33.7MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 35.9MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/dd/7b8e2f0ed47cc46ba2f6144df4f32de3614d6f56ca07b7f308d40cfa2dfb/pylint-2.9.3-py3-none-any.whl (372kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 48.9MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Collecting astroid<2.7,>=2.6.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/bc/ee4ca5a31fbc1f9cec6df52170baa151f82cbf4f4989de87c7f94a28a958/astroid-2.6.2-py3-none-any.whl (228kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 43.4MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting isort<6,>=4.2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/6f/5adde6e4d9e745a39fa0fed2ae0ca8667df95fee50c516370767dde7e000/isort-5.9.2-py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 46.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Collecting lazy-object-proxy>=1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/b0/f055db25fd68ab4859832a887c8b304274fc12dd5a3f8e83e61250733aeb/lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
      "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 32.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.4.1)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-cp37-none-any.whl size=85058 sha256=57531da123606eef09d98c36a58f70f295cf480e0ded7aa4966799374294a76d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vc6fz6ka/wheels/ae/1c/88/0bb16b41740b5172282fc37c550c63391fa0c6cd8193fc70c4\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68395 sha256=287db94f13be4c6efff198caf6f0b5e07bb6e6209cfb98448156c73c923fc326\n",
      "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
      "Successfully built joeynmt wrapt\n",
      "\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.20.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.5.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.5.0 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, torchtext, portalocker, sacrebleu, subword-nmt, pyyaml, wrapt, lazy-object-proxy, typed-ast, astroid, mccabe, isort, pylint, six, joeynmt\n",
      "  Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "Successfully installed astroid-2.6.2 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.3 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_BanTZM93fw"
   },
   "outputs": [],
   "source": [
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt1 -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt1\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt1 < train.$tgt1 > train.bpe.$tgt1\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt1 < dev.$tgt1 > dev.bpe.$tgt1\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt1 < test.$tgt1 > test.bpe.$tgt1\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt1 --output_path vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8fPfOMmAVW4",
    "outputId": "4a49ca45-d2ef-4784-ed2b-fb8540732ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Luganda Sentences\n",
      "Eng@@ abo enn@@ ene ey’@@ okukkiriza ( Laba akat@@ undu 12 - 14 )\n",
      "En@@ k@@ of@@ i@@ ira ey’@@ obul@@ ok@@ ozi ( Laba akat@@ undu 15 - 18 )\n",
      "N@@ kir@@ abye nti abantu bak@@ wat@@ ibwako nnyo bwe bak@@ iraba nti oy@@ agala nnyo Bayibuli era nti ok@@ ola kyonna ekis@@ oboka oku@@ bayamba . ”\n",
      "E@@ kit@@ ala eky’@@ omwoyo ( Laba akat@@ undu 19 - 20 )\n",
      "Yakuwa asobola okutuyamba okul@@ wanyisa omul@@ abe oyo ne tu@@ mu@@ w@@ angula !\n",
      "Combined BPE Vocab\n",
      "(@@\n",
      "Ó@@\n",
      "taayo\n",
      "\\\n",
      "meet@@\n",
      "uld\n",
      "Prover@@\n",
      "”@@\n",
      "ö\n",
      "ŋ\n"
     ]
    }
   ],
   "source": [
    "# Some output\n",
    "! echo \"BPE Luganda Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt1\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V42gvMwCBO73"
   },
   "source": [
    "## Kinyarwanda  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzBBzWMalTpE"
   },
   "source": [
    "### Turning data from JW300 to dataframe\n",
    "\n",
    "**Do not rerun**: Load pandas dataframe instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpcQDoQvZyFf"
   },
   "outputs": [],
   "source": [
    "# Changing to Kinyarwanda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qu20Nfj7BStY",
    "outputId": "e3617a69-3905-4143-86b3-c6339a971751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-rw.xml.gz not found. The following files are available for downloading:\n",
      "\n",
      "   5 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/en-rw.xml.gz\n",
      " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/en.zip\n",
      "  48 MB https://object.pouta.csc.fi/OPUS-JW300/v1b/xml/rw.zip\n",
      "\n",
      " 316 MB Total size\n",
      "./JW300_latest_xml_en-rw.xml.gz ... 100% of 5 MB\n",
      "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
      "./JW300_latest_xml_rw.zip ... 100% of 48 MB\n"
     ]
    }
   ],
   "source": [
    "# Downloading our corpus\n",
    "! opus_read -d JW300 -s $src -t $tgt2 -wm moses -w jw300.$src jw300.$tgt2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CE90Jq4BUIc"
   },
   "outputs": [],
   "source": [
    "# extract the corpus file\n",
    "! gunzip JW300_latest_xml_$src-$tgt2.xml.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JF3uxL80dSF4",
    "outputId": "e1223d28-690b-481e-8dd0-1e0f5958de45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-14 10:39:30--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 277791 (271K) [text/plain]\n",
      "Saving to: ‘test.en-any.en’\n",
      "\n",
      "\r",
      "test.en-any.en        0%[                    ]       0  --.-KB/s               \r",
      "test.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-05-14 10:39:30 (10.6 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
      "\n",
      "--2021-05-14 10:39:30--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-rw.en\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 202456 (198K) [text/plain]\n",
      "Saving to: ‘test.en-rw.en’\n",
      "\n",
      "test.en-rw.en       100%[===================>] 197.71K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-05-14 10:39:31 (9.47 MB/s) - ‘test.en-rw.en’ saved [202456/202456]\n",
      "\n",
      "--2021-05-14 10:39:31--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-rw.rw\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 206496 (202K) [text/plain]\n",
      "Saving to: ‘test.en-rw.rw’\n",
      "\n",
      "test.en-rw.rw       100%[===================>] 201.66K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-05-14 10:39:31 (9.83 MB/s) - ‘test.en-rw.rw’ saved [206496/206496]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading test set\n",
    "# Download the global test set.\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
    "\n",
    "# Specific test set\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$tgt2.en \n",
    "! mv test.en-$tgt2.en test.en\n",
    "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$tgt2.$tgt2 \n",
    "! mv test.en-$tgt2.$tgt2 test.$tgt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "CEriPsWQdbbz",
    "outputId": "07da95f0-4329-40b0-8a37-2769310ada76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data and skipped 5825/483984 lines since contained in test set.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Deaf Praise Jehovah</td>\n",
       "      <td>Ibipfamatwi Bisingiza Yehova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BY AWAKE !</td>\n",
       "      <td>BY AWAKE !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CORRESPONDENT IN NIGERIA</td>\n",
       "      <td>CORRESPONDENT IN NIGERIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source_sentence               target_sentence\n",
       "0   The Deaf Praise Jehovah  Ibipfamatwi Bisingiza Yehova\n",
       "1                BY AWAKE !                    BY AWAKE !\n",
       "2  CORRESPONDENT IN NIGERIA      CORRESPONDENT IN NIGERIA"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TMX file to dataframe\n",
    "source_file = 'jw300.' + source_language\n",
    "target_file = 'jw300.' + target_language2\n",
    "\n",
    "source = []\n",
    "target = []\n",
    "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
    "with open(source_file) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # Skip sentences that are contained in the test set.\n",
    "        if line.strip() not in en_test_sents:\n",
    "            source.append(line.strip())\n",
    "        else:\n",
    "            skip_lines.append(i)             \n",
    "with open(target_file) as f:\n",
    "    for j, line in enumerate(f):\n",
    "        # Only add to corpus if corresponding source was not skipped.\n",
    "        if j not in skip_lines:\n",
    "            target.append(line.strip())\n",
    "    \n",
    "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
    "    \n",
    "df2 = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xev-wuzFdyMj"
   },
   "outputs": [],
   "source": [
    "# Kinyarwanda training set\n",
    "df2.to_csv('Kinyarwanda.csv',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBqgEGvO4ZVO"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "ecfAcP7E6jE4",
    "outputId": "0882062c-a2b3-4a75-9974-e77d1a908dc8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>target_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Deaf Praise Jehovah</td>\n",
       "      <td>Ibipfamatwi Bisingiza Yehova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BY AWAKE !</td>\n",
       "      <td>BY AWAKE !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CORRESPONDENT IN NIGERIA</td>\n",
       "      <td>CORRESPONDENT IN NIGERIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            source_sentence               target_sentence\n",
       "0   The Deaf Praise Jehovah  Ibipfamatwi Bisingiza Yehova\n",
       "1                BY AWAKE !                    BY AWAKE !\n",
       "2  CORRESPONDENT IN NIGERIA      CORRESPONDENT IN NIGERIA"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwa = pd.read_csv(\"Kinyarwanda.csv\")\n",
    "rwa.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44E_RLf16jE5"
   },
   "outputs": [],
   "source": [
    "# drop duplicate translations\n",
    "df_pp = rwa.drop_duplicates()\n",
    "\n",
    "# drop conflicting translations\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Shuffle the data to remove bias in dev set selection.\n",
    "df_pp = df_pp.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQNOmwQo6jE6"
   },
   "outputs": [],
   "source": [
    "# reset the index of the training set after previous filtering\n",
    "df_pp.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dBLmIWEHTnX"
   },
   "outputs": [],
   "source": [
    "df_pp.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7ZVTmQDG4UR",
    "outputId": "0e6f2ac9-9885-4662-e447-83c8d8dd19c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_sentence    0\n",
       "target_sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5MG_TDd6jE7"
   },
   "outputs": [],
   "source": [
    "# Splitting train and validation set\n",
    "num_valid = 1000\n",
    "\n",
    "dev = df_pp.tail(num_valid) \n",
    "stripped = df_pp.drop(df_pp.tail(num_valid).index)\n",
    "\n",
    "# Creating files for luganda and english\n",
    "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language2, \"w\") as trg_file:\n",
    "  for index, row in stripped.iterrows():\n",
    "    try:\n",
    "      src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "      trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "    except TypeError:\n",
    "      print(index,row[\"target_sentence\"])\n",
    "    \n",
    "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language2, \"w\") as trg_file:\n",
    "  for index, row in dev.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ST0TsRf6jE8",
    "outputId": "c0e469be-3ca8-4ed5-a00d-cfbd74a1973a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.bpe.en <==\n",
      "R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "You will see the time when God br@@ ings righteous rule to all the earth , und@@ o@@ ing the d@@ am@@ age and inj@@ ust@@ ice brought by human rul@@ er@@ ship .\n",
      "Let us consider f@@ ive reas@@ ons why we should want to follow the Christ .\n",
      "Even in the Bible , the id@@ ea of pers@@ u@@ as@@ ion som@@ et@@ imes has n@@ eg@@ ative con@@ no@@ t@@ ations , den@@ ot@@ ing a cor@@ rup@@ ting or a lead@@ ing as@@ tr@@ ay .\n",
      "For God’s servants to be deliv@@ ered , Satan and his ent@@ ire world@@ wide system of things need to be rem@@ ov@@ ed .\n",
      "I had never heard that name used in my ch@@ urch .\n",
      "S@@ imp@@ ly having authority or a wid@@ er name recogn@@ ition is not the important thing .\n",
      "M@@ ost people do not believe in the spir@@ its .\n",
      "And others are encourag@@ ed to be merc@@ if@@ ul , for merc@@ y beg@@ ets merc@@ y . ​ — Luke 6 : 38 .\n",
      "Like such ro@@ o@@ ts in earth@@ ’s no@@ ur@@ ish@@ ing so@@ il , our m@@ inds and hearts need to del@@ v@@ e exp@@ ans@@ ively into God’s Word and d@@ raw from its life - giving wat@@ ers .\n",
      "\n",
      "==> train.bpe.rw <==\n",
      "A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "U@@ z@@ aba uh@@ ari igihe Imana iz@@ ashy@@ iraho ubutegetsi buk@@ iranuka ku isi hose , ik@@ av@@ an@@ aho ibibi n’@@ akar@@ eng@@ ane byat@@ ewe n’@@ ubutegetsi bw’@@ abantu .\n",
      "Nim@@ ucyo dusuzume impamvu esh@@ anu z@@ agombye gutuma tw@@ ifuza guk@@ urikira Kristo .\n",
      "Nd@@ etse no muri Bibiliya , igit@@ ekerezo cyo kw@@ emeza umuntu ikintu , rimwe na rimwe cy@@ umvikana mu buryo bub@@ i , kig@@ as@@ obanura k@@ osh@@ ya , cyangwa kuy@@ ob@@ ya .\n",
      "Kugira ngo abagaragu b’Imana baz@@ ac@@ ung@@ ur@@ we , Satani na gahunda ye y’@@ ibintu yose yo ku isi hose big@@ omba kuv@@ an@@ waho .\n",
      "Mu idini n@@ abag@@ amo sin@@ ari nar@@ igeze n@@ umva bak@@ oresha iryo z@@ ina .\n",
      "G@@ uh@@ abwa ubut@@ ware gusa cyangwa kugira umw@@ anya ukomeye si cyo kintu cy’@@ ingenzi .\n",
      "Abantu benshi ntib@@ emera imy@@ uka .\n",
      "Iyo tug@@ iriye abantu imbabazi na bo bib@@ at@@ era kugira imbabazi , kuko imbabazi zit@@ urwa izindi . — Luka 6 : 38 .\n",
      "Nk’uko iyo m@@ izi ig@@ ab@@ urira ig@@ iti ib@@ iv@@ uye mu but@@ aka buk@@ ung@@ ah@@ aye , ubwenge n’@@ umutima byacu big@@ omba guc@@ eng@@ era mu Ijambo ry’Imana maze bik@@ av@@ om@@ amo amazi atanga ubuzima .\n",
      "\n",
      "==> train.en <==\n",
      "Right after his baptism , he “ went off into Arabia ” ​ — either the Syrian Desert or possibly some quiet place on the Arabian Peninsula that was conducive to meditation .\n",
      "You will see the time when God brings righteous rule to all the earth , undoing the damage and injustice brought by human rulership .\n",
      "Let us consider five reasons why we should want to follow the Christ .\n",
      "Even in the Bible , the idea of persuasion sometimes has negative connotations , denoting a corrupting or a leading astray .\n",
      "For God’s servants to be delivered , Satan and his entire worldwide system of things need to be removed .\n",
      "I had never heard that name used in my church .\n",
      "Simply having authority or a wider name recognition is not the important thing .\n",
      "Most people do not believe in the spirits .\n",
      "And others are encouraged to be merciful , for mercy begets mercy . ​ — Luke 6 : 38 .\n",
      "Like such roots in earth’s nourishing soil , our minds and hearts need to delve expansively into God’s Word and draw from its life - giving waters .\n",
      "\n",
      "==> train.rw <==\n",
      "Ashobora kuba yaragiye ahantu hatuje mu Butayu bwa Siriya cyangwa se wenda ku Mwigimbakirwa wa Arabiya , uri mu burasirazuba bw’Inyanja Itukura , kugira ngo hamufashe gutekereza .\n",
      "Uzaba uhari igihe Imana izashyiraho ubutegetsi bukiranuka ku isi hose , ikavanaho ibibi n’akarengane byatewe n’ubutegetsi bw’abantu .\n",
      "Nimucyo dusuzume impamvu eshanu zagombye gutuma twifuza gukurikira Kristo .\n",
      "Ndetse no muri Bibiliya , igitekerezo cyo kwemeza umuntu ikintu , rimwe na rimwe cyumvikana mu buryo bubi , kigasobanura koshya , cyangwa kuyobya .\n",
      "Kugira ngo abagaragu b’Imana bazacungurwe , Satani na gahunda ye y’ibintu yose yo ku isi hose bigomba kuvanwaho .\n",
      "Mu idini nabagamo sinari narigeze numva bakoresha iryo zina .\n",
      "Guhabwa ubutware gusa cyangwa kugira umwanya ukomeye si cyo kintu cy’ingenzi .\n",
      "Abantu benshi ntibemera imyuka .\n",
      "Iyo tugiriye abantu imbabazi na bo bibatera kugira imbabazi , kuko imbabazi ziturwa izindi . — Luka 6 : 38 .\n",
      "Nk’uko iyo mizi igaburira igiti ibivuye mu butaka bukungahaye , ubwenge n’umutima byacu bigomba gucengera mu Ijambo ry’Imana maze bikavomamo amazi atanga ubuzima .\n",
      "==> dev.bpe.en <==\n",
      "My heart was t@@ ou@@ ched .\n",
      "Consider , however , what was involved in reading a s@@ cro@@ l@@ l .\n",
      "Rather than succ@@ um@@ b to p@@ an@@ ic or des@@ pa@@ ir , we should b@@ ol@@ st@@ er our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "S@@ ad@@ ly , some within the Christian congregation give evidence that , at le@@ ast to a d@@ eg@@ ree , they feel com@@ for@@ table in Satan’s world .\n",
      "Jehovah took into account El@@ ij@@ ah’s l@@ imit@@ ations and dis@@ pat@@ ched an ang@@ el .\n",
      "T@@ ho@@ ugh in@@ her@@ iting imper@@ f@@ ection from Adam , mill@@ ions of other God - f@@ earing humans have follow@@ ed in Jesus ’ f@@ oo@@ t@@ st@@ ep@@ s by keep@@ ing integr@@ ity in the face of sat@@ an@@ ic att@@ ac@@ ks . — 1 Peter 1 : 18 , 19 ; 2 : 19 , 21 .\n",
      "C@@ ould those who later me@@ et the son righ@@ t@@ fully con@@ clud@@ e that he had a bad father or even that he had no father at all ?\n",
      "A@@ ud@@ i@@ ences were moved by the dr@@ ama “ R@@ esp@@ ect Jehovah’s A@@ uth@@ or@@ ity ”\n",
      "Satan has destro@@ y@@ ed coun@@ t@@ less who@@ l@@ es@@ ome , tr@@ us@@ ting relation@@ ship@@ s through in@@ sid@@ ious doub@@ ts pl@@ an@@ ted in that way . ​ — Gal@@ at@@ ians 5 : 7 - 9 .\n",
      "T@@ om , a m@@ ember of the Be@@ the@@ l family in E@@ st@@ on@@ ia , says : “ J@@ ust a b@@ loc@@ k away from Be@@ the@@ l is the se@@ a , and n@@ ear@@ by there is a be@@ aut@@ if@@ ul for@@ est where my wife and I enjoy going for sh@@ ort wal@@ ks .\n",
      "\n",
      "==> dev.bpe.rw <==\n",
      "By@@ ank@@ oze ku mutima .\n",
      "Umw@@ andiko w@@ abaga w@@ anditse mu nk@@ ing@@ i ku ruhande rw’@@ imbere rw’@@ uwo muz@@ ingo .\n",
      "Aho kugira ngo duh@@ ang@@ ay@@ ike cyangwa tw@@ ih@@ ebe , twagombye gukomeza ukwizera duf@@ itiye Imana binyuriye mu gusoma Ijambo ry@@ ayo . — Abaroma 8 : 35 - 39 .\n",
      "Ik@@ ibab@@ aje ni uko hari bamwe mu bagize itorero rya gikristo bag@@ aragaza , mu rugero runaka , ko bag@@ uwe neza muri iyi si ya Satani .\n",
      "Yehova y@@ az@@ irik@@ anye intege nk@@ e za El@@ iya maze amw@@ oh@@ erer@@ eza umum@@ ar@@ ayika .\n",
      "N’ubwo bar@@ az@@ we uk@@ ud@@ at@@ ung@@ ana bitewe n’@@ icyaha cya Adamu , abandi bantu bat@@ inya Imana babarirwa muri za miriyoni , bag@@ eze ikir@@ enge mu cya Yesu bak@@ omeza gush@@ ik@@ ama mu gihe bari bah@@ anganye n’@@ ibit@@ ero bya Satani . ​ —⁠ 1 Petero 1 : 18 , 19 ; 2 : ​ 19 , 21 .\n",
      "Ese byaba bik@@ wiriye kuvuga ko uwo mub@@ yeyi yar@@ eze nabi , tuk@@ aba tw@@ an@@ avuga ko umwana at@@ agira se ?\n",
      "Ab@@ ari mu ikor@@ aniro bak@@ ozwe ku mutima na d@@ ar@@ ame yari ifite umutwe uvuga ngo “ Jya W@@ ubaha Ubut@@ ware bwa Yehova ”\n",
      "Satani yash@@ enye imishyikirano myiza yar@@ ang@@ waga no kw@@ iz@@ er@@ ana abantu bat@@ abar@@ ika bari b@@ af@@ itanye , binyuriye mu bit@@ ekerezo b@@ if@@ if@@ itse byo gushidikanya yagiye ab@@ iba muri ubwo buryo . — Abag@@ al@@ at@@ iya 5 : 7 - 9 .\n",
      "T@@ om , umwe mu bagize umuryango wa Bet@@ eli yo muri Es@@ it@@ on@@ iya , yagize ati “ iyo ur@@ enze inzu imwe gusa uv@@ uye kuri Bet@@ eli uh@@ ita ug@@ era ku ny@@ anja , kandi hafi aho hari ag@@ ashy@@ amba k@@ eza aho jye n’umugore wanjye duk@@ unda kujya gut@@ emb@@ er@@ era ak@@ anya gato .\n",
      "\n",
      "==> dev.en <==\n",
      "My heart was touched .\n",
      "Consider , however , what was involved in reading a scroll .\n",
      "Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "Jehovah took into account Elijah’s limitations and dispatched an angel .\n",
      "Though inheriting imperfection from Adam , millions of other God - fearing humans have followed in Jesus ’ footsteps by keeping integrity in the face of satanic attacks . — 1 Peter 1 : 18 , 19 ; 2 : 19 , 21 .\n",
      "Could those who later meet the son rightfully conclude that he had a bad father or even that he had no father at all ?\n",
      "Audiences were moved by the drama “ Respect Jehovah’s Authority ”\n",
      "Satan has destroyed countless wholesome , trusting relationships through insidious doubts planted in that way . ​ — Galatians 5 : 7 - 9 .\n",
      "Tom , a member of the Bethel family in Estonia , says : “ Just a block away from Bethel is the sea , and nearby there is a beautiful forest where my wife and I enjoy going for short walks .\n",
      "\n",
      "==> dev.rw <==\n",
      "Byankoze ku mutima .\n",
      "Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "Yehova yazirikanye intege nke za Eliya maze amwoherereza umumarayika .\n",
      "N’ubwo barazwe ukudatungana bitewe n’icyaha cya Adamu , abandi bantu batinya Imana babarirwa muri za miriyoni , bageze ikirenge mu cya Yesu bakomeza gushikama mu gihe bari bahanganye n’ibitero bya Satani . ​ —⁠ 1 Petero 1 : 18 , 19 ; 2 : ​ 19 , 21 .\n",
      "Ese byaba bikwiriye kuvuga ko uwo mubyeyi yareze nabi , tukaba twanavuga ko umwana atagira se ?\n",
      "Abari mu ikoraniro bakozwe ku mutima na darame yari ifite umutwe uvuga ngo “ Jya Wubaha Ubutware bwa Yehova ”\n",
      "Satani yashenye imishyikirano myiza yarangwaga no kwizerana abantu batabarika bari bafitanye , binyuriye mu bitekerezo bififitse byo gushidikanya yagiye abiba muri ubwo buryo . — Abagalatiya 5 : 7 - 9 .\n",
      "Tom , umwe mu bagize umuryango wa Beteli yo muri Esitoniya , yagize ati “ iyo urenze inzu imwe gusa uvuye kuri Beteli uhita ugera ku nyanja , kandi hafi aho hari agashyamba keza aho jye n’umugore wanjye dukunda kujya gutemberera akanya gato .\n"
     ]
    }
   ],
   "source": [
    "! head train.*\n",
    "! head dev.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ie8EzVjj6jE-",
    "outputId": "30b1e3e6-d523-4db9-f381-308908fe6692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8a/064b4077e3d793f877e3b77aa64f56fa49a4d37236a53f78ee28be009a16/numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3MB 200kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (56.1.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.4.1)\n",
      "Collecting torchtext==0.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/50/84184d6230686e230c464f0dd4ff32eada2756b4a0b9cefec68b88d1d580/torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 24.3MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 47.6MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/f0/9705d6ec002876bc20b6923cbdeeca82569a895fc214211562580e946079/pylint-2.8.2-py3-none-any.whl (357kB)\n",
      "\u001b[K     |████████████████████████████████| 358kB 45.6MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.30.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (2.0.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.12.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Collecting mccabe<0.7,>=0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
      "Collecting isort<6,>=4.2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/47/0ec3ec948b7b3a0ba44e62adede4dca8b5985ba6aaee59998bed0916bd17/isort-5.8.0-py3-none-any.whl (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 56.0MB/s \n",
      "\u001b[?25hCollecting astroid<2.7,>=2.5.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/82/a61df6c2d68f3ae3ad1afa0d2e5ba5cfb7386eb80cffb453def7c5757271/astroid-2.5.6-py3-none-any.whl (219kB)\n",
      "\u001b[K     |████████████████████████████████| 225kB 57.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2020.12.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 48.2MB/s \n",
      "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/b0/f055db25fd68ab4859832a887c8b304274fc12dd5a3f8e83e61250733aeb/lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.4.1)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-cp37-none-any.whl size=85042 sha256=d8c9649900c1eb81d428d9375b05914d52f4596529af16776efe2a77d9691941\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ktwaqfu3/wheels/cf/8f/4a/ae21bc283e97ae1a567dbb2ff86c81cafd0a203cc6163abfe7\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68384 sha256=84891c91369f05ff106591585589ef21c3c3e924af2284dea8469b09774726ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
      "Successfully built joeynmt wrapt\n",
      "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, torchtext, portalocker, sacrebleu, subword-nmt, pyyaml, mccabe, isort, typed-ast, lazy-object-proxy, wrapt, astroid, pylint, six, joeynmt\n",
      "  Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Found existing installation: torchtext 0.9.1\n",
      "    Uninstalling torchtext-0.9.1:\n",
      "      Successfully uninstalled torchtext-0.9.1\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "Successfully installed astroid-2.5.6 isort-5.8.0 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.8.2 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rrv8B5fz6jE_"
   },
   "outputs": [],
   "source": [
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt2 -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt2\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt2 < train.$tgt2 > train.bpe.$tgt2\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt2 < dev.$tgt2 > dev.bpe.$tgt2\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt2 < test.$tgt2 > test.bpe.$tgt2\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt2 --output_path vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6yj6p4s6jFA",
    "outputId": "35aa0fc5-5557-4e57-c11e-25c05bacc8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Kinyarwanda Sentences\n",
      "I@@ ng@@ abo n@@ ini yo kwizera ( Reba p@@ aragar@@ af@@ u ya 12 - 14 )\n",
      "I@@ ng@@ of@@ ero y’@@ agak@@ iza ( Reba p@@ aragar@@ af@@ u ya 15 - 18 )\n",
      "N@@ abonye ko iyo abantu babona ko ukunda gukoresha Bibiliya kandi ug@@ akora ib@@ ish@@ oboka byose kugira ngo ub@@ afashe , bak@@ ira neza ubutumwa . ”\n",
      "I@@ nk@@ ota y’@@ umwuka ( Reba p@@ aragar@@ af@@ u ya 19 - 20 )\n",
      "Ariko Yehova adufasha k@@ umur@@ wanya , tuk@@ am@@ ut@@ sinda !\n",
      "Combined BPE Vocab\n",
      "Ê@@\n",
      "̆\n",
      "ahamu\n",
      "ʺ\n",
      "⁄\n",
      "ointed\n",
      "Ă@@\n",
      "̄@@\n",
      "ḥ\n",
      "Ā@@\n"
     ]
    }
   ],
   "source": [
    "# Some output\n",
    "! echo \"BPE Kinyarwanda Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt2\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcIZiMnYTUwH"
   },
   "source": [
    "## Luhyia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOckEV2ylfUb"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_OHPxeVUBu9"
   },
   "outputs": [],
   "source": [
    "# Changing to Luhyia directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "XkuQpTrCXtRw",
    "outputId": "2da789bc-ce80-479f-9d99-9ef8b42a8665"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7949</th>\n",
       "      <td>Ne omundu yesi naba narusiakhwo likhuwa liosi...</td>\n",
       "      <td>and if anyone takes away from the words of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7950</th>\n",
       "      <td>Ulia ourusinjia obuloli khumakhuwa kano koosi...</td>\n",
       "      <td>He who testifies to these things says, “Surel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>Obukoosia obwa Omwami Yesu bube khubandu ba N...</td>\n",
       "      <td>The grace of our Lord Jesus Christ be with yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0                                                  1\n",
       "7949   Ne omundu yesi naba narusiakhwo likhuwa liosi...   and if anyone takes away from the words of th...\n",
       "7950   Ulia ourusinjia obuloli khumakhuwa kano koosi...   He who testifies to these things says, “Surel...\n",
       "7951   Obukoosia obwa Omwami Yesu bube khubandu ba N...   The grace of our Lord Jesus Christ be with yo..."
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luh = pd.read_csv(\"Luhya.csv\")\n",
    "luh.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-svP6rWOPys",
    "outputId": "43b25c4e-8569-4cdb-9b42-4a651369f260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "9dM1rDdnOXp2",
    "outputId": "0f710168-3e98-47f8-f0f3-3d4e37d72006"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_sentence</th>\n",
       "      <th>source_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7950</th>\n",
       "      <td>Ulia ourusinjia obuloli khumakhuwa kano koosi ...</td>\n",
       "      <td>He who testifies to these things says , “ Sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>Obukoosia obwa Omwami Yesu bube khubandu ba Ny...</td>\n",
       "      <td>The grace of our Lord Jesus Christ be with you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        target_sentence                                    source_sentence\n",
       "7950  Ulia ourusinjia obuloli khumakhuwa kano koosi ...  He who testifies to these things says , “ Sure...\n",
       "7951  Obukoosia obwa Omwami Yesu bube khubandu ba Ny...  The grace of our Lord Jesus Christ be with you..."
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luh['target_sentence'] = luh['0'].apply(lambda x: ' '.join(word_tokenize(x)))\n",
    "luh['source_sentence'] = luh['1'].apply(lambda x: ' '.join(word_tokenize(x)))\n",
    "luh = luh.drop(['0', '1'], axis = 1)\n",
    "luh.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lg47qSrrYiiR"
   },
   "outputs": [],
   "source": [
    "#luh.rename(columns = {'0' : 'target_sentence', '1' : 'source_sentence'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJOVwI2dYBE1"
   },
   "outputs": [],
   "source": [
    "# drop duplicate translations\n",
    "df_pp = luh.drop_duplicates()\n",
    "\n",
    "# drop conflicting translations\n",
    "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
    "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
    "\n",
    "# Shuffle the data to remove bias in dev set selection.\n",
    "df_pp = df_pp.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bV_lgFo4Y-Cz"
   },
   "outputs": [],
   "source": [
    "# reset the index of the training set after previous filtering\n",
    "df_pp.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5jkIxRnZXF3"
   },
   "outputs": [],
   "source": [
    "# Splitting train and validation set\n",
    "num_valid = 1000\n",
    "\n",
    "dev = df_pp.tail(num_valid) \n",
    "stripped = df_pp.drop(df_pp.tail(num_valid).index)\n",
    "test = stripped.tail(num_valid)\n",
    "stripped2 = stripped.drop(stripped.tail(num_valid).index)\n",
    "\n",
    "# Creating files for luhyia and english\n",
    "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language3, \"w\") as trg_file:\n",
    "  for index, row in stripped2.iterrows():\n",
    "    try:\n",
    "      src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "      trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "    except TypeError:\n",
    "      print(index,row[\"target_sentence\"])\n",
    "\n",
    "# Dev   \n",
    "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language3, \"w\") as trg_file:\n",
    "  for index, row in dev.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
    "\n",
    "# Test\n",
    "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language3, \"w\") as trg_file:\n",
    "  for index, row in test.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "    trg_file.write(row[\"target_sentence\"]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgJ5EqjBZmnU",
    "outputId": "296cfe47-5a8d-4fba-cd9e-a7f3079144bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> train.bpe.en <==\n",
      " T@@ hat day was the P@@ re@@ par@@ ation, and the Sab@@ b@@ ath drew ne@@ ar@@ .\n",
      " Behold, I am coming qui@@ ck@@ l@@ y@@ ! H@@ old fast what you ha@@ ve, that no one may take your crow@@ n.\n",
      " The next day, because he wan@@ ted to know for certain why he was acc@@ us@@ ed by the Jews, he r@@ ele@@ as@@ ed him from his bond@@ s, and commanded the chief priests and all their coun@@ ci@@ l to appear@@ , and brought Paul down and set him before them. \n",
      "\n",
      " This He said, sig@@ ni@@ f@@ ying by what death He would di@@ e.\n",
      " Then they said to the wom@@ an, “@@ Now we believ@@ e, not because of what you said, for we our@@ selves have heard Him and we know that this is indeed the Christ, the Sa@@ vi@@ or of the worl@@ d.”\n",
      " But rej@@ ect prof@@ ane and old wi@@ ves@@ ’ f@@ ab@@ les, and ex@@ er@@ c@@ ise your@@ self toward god@@ lin@@ es@@ s.\n",
      " It is written in the prophe@@ ts, ‘@@ And they shall all be taught by God@@ .’ Therefore everyone who has heard and lear@@ ned from the Father comes to Me.\n",
      " Then out of the sm@@ oke lo@@ c@@ us@@ ts came upon the ear@@ th. And to them was given pow@@ er, as the s@@ cor@@ pi@@ ons of the earth have pow@@ er.\n",
      " Jesus said to him, “R@@ is@@ e, take up your b@@ ed and wal@@ k@@ .”\n",
      "\n",
      "==> train.bpe.lh <==\n",
      " Y@@ ali,@@ inyanga yo@@ khwi@@ re@@ chekha khulwa inyanga eya Is@@ aba@@ to ey@@ ali niy@@ ili ahambi okhu@@ chaak@@ a. \n",
      " N@@ di@@ itsanga bwangu o@@ hand@@ e khu aka oli nin@@ ako, kho mbu, omundu yesi yesi,@@ alab@@ uk@@ ul@@ akhwo olu@@ si@@ mb@@ il@@ wo tawe. \n",
      " Omu@@ s@@ injilili wab@@ elihe oyo y@@ enya okhumanya eshi@@ chila,@@ Abayahudi nib@@ enj@@ ililanga Paulo itookh@@ o. Kho iny@@ anga,@@ yal@@ ondakhwo yab@@ ol@@ ola em@@ iny@@ olol@@ o echia bali nibab@@ oy@@ ile,@@ Paulo mana nal@@ aka abesaaliisi aba@@ khongo nende ab@@ eshiin@@ a,@@ boosi okhw@@ aka@@ ana. Mana nay@@ ila Paulo namu@@ s@@ injisia imbeli,@@ w@@ abwe.\n",
      "\n",
      " Y@@ aboola ako khulw@@ okhumany@@ ia,@@ shinga lw@@ okhuf@@ wakh@@ we khw@@ itsa okhuba@@ . \n",
      " Kho nibaboolela omukhasi oyo bari, “I@@ fwe,@@ shikhu@@ suu@@ bile khulwa okhubela aka iwe okhubool@@ ile ta habula khu@@ suubi@@ ile shichila mbu, abeene khw@@ its@@ ile,@@ nikhu@@ hulila na@@ ya@@ ala, ne bulano khumanyile mbu, niye,@@ omu@@ honia w@@ abandu boosi@@ .” Yesu ahonia omwana w@@ omus@@ esi@@ a,\n",
      " Nebutswa,@@ wi@@ h@@ any@@ e okhurula khu@@ tsing@@ ano tsi@@ ab@@ u@@ ts@@ wa ets@@ il@@ akhoy@@ e@@ ele,@@ ta, w@@ ina@@ sie okhwe@@ ka amakhuwa amalayi k@@ obulamu obwa,@@ eshik@@ rist@@ o. \n",
      " Ab@@ al@@ akusi@@ ,@@ ba@@ hand@@ ika mbu, ‘@@ Buli mundu ali@@ e@@ chesi@@ bwa nende,@@ Nyasaye@@ .’ Kho oyo yesi ou@@ hulilanga aka Papa nende,@@ okhwe@@ ka okhurula khuy@@ e, yetsa khw@@ isie. \n",
      " Ne tsi@@ si@@ che nitsi@@ rula mu@@ mw@@ osi nitsi@@ ba khushialo nitsi@@ helesi@@ bwe obunyali obu@@ fwana shinga obwa amak@@ ati@@ a,@@ k@@ eshial@@ o. \n",
      " Yesu namuboolela ari, “S@@ injila wit@@ u@@ ushe omuk@@ e@@ kw@@ o,@@ mana o@@ chend@@ e.” \n",
      "\n",
      "==> train.en <==\n",
      "Then Pilate entered the Praetorium again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "If anyone thinks himself to be a prophet or spiritual , let him acknowledge that the things which I write to you are the commandments of the Lord .\n",
      "Every branch in Me that does not bear fruit He takes away ; and every branch that bears fruit He prunes , that it may bear more fruit .\n",
      "Demetrius has a good testimony from all , and from the truth itself . And we also bear witness , and you know that our testimony is true .\n",
      "And supper being ended , the devil having already put it into the heart of Judas Iscariot , Simon ’ s son , to betray Him ,\n",
      "imploring us with much urgency that we would receive the gift and the fellowship of the ministering to the saints .\n",
      "It is written in the prophets , ‘ And they shall all be taught by God. ’ Therefore everyone who has heard and learned from the Father comes to Me .\n",
      "For those who are such do not serve our Lord Jesus Christ , but their own belly , and by smooth words and flattering speech deceive the hearts of the simple .\n",
      "So when he had received food , he was strengthened . Then Saul spent some days with the disciples at Damascus .\n",
      "Therefore if you have not been faithful in the unrighteous mammon , who will commit to your trust the true riches ?\n",
      "\n",
      "==> train.lh <==\n",
      "Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "Omundu yesi owilolanga mbu , nomurumwa , wa Nyasaye noho mbu ali neshihaanwa eshia Roho okhuula amanye khandi afuchilile mbu , aka , emuhandichilanga kano nelilako elia Omwami .\n",
      "Aremanga buli lisaka , mwisie elilamanga ebiamo ta , ne akhalilanga buli lisaka , eliamanga ebiamo kho mbu , libe lilayi nilimeeta okhwama , ebiamo ebinji .\n",
      "Buli mundu amwitsoominjia Demeterio ; ne obwatieli , bwene bumwitsoominjia . Ne nasi emeetakhwo obuloli , bwanje , ne mumanyile mbu akemboola nakatoto . Amashesio Kokhumalilisia ,\n",
      "Yesu nende abeechibe bali nibaliitsanga eshiokhulia , eshia hamukoloba . Setani yali namalile okhura mu Yuda omwana wa Simoni Isikarioti amapaaro kokhukhoba Yesu. ,\n",
      "bakhusaba nibakhusaaya okhufuchililwa okhusanga , mukhukhonya abakristo bashiabwe aba Yudea .\n",
      "Abalakusi , bahandika mbu , ‘ Buli mundu aliechesibwa nende , Nyasaye. ’ Kho oyo yesi ouhulilanga aka Papa nende , okhweka okhurula khuye , yetsa khwisie .\n",
      "Okhuba , abakholanga amakhuwa kario shibakhalabanilanga Kristo , Omwami wefwe ta , habula bakhalabanilanga tsinda , tsiabwe abeene . Bekhoonyelanga amakhuwa kabwe , kokhukaatilisia nende akokhulaha khulwa okhukaatia , amapaaro kabateshele .\n",
      "Ne olwa yamala , okhulia eshiokhulia , omubilikwe kwanyoola amaani . Saulo ayaala Injiili Damasiko Saulo yamenya Damasiko halala nabasuubili khulwa , tsinyanga tsindiiti .\n",
      "Kho , nimulaba abesiikwa mubuyinda , bwomushialo shino ta , mwakhaba murie abesiikwa , mubuyinda bwatoto ?\n",
      "==> dev.bpe.en <==\n",
      " They brought him who for@@ mer@@ ly was blind to the Pharise@@ es.\n",
      " And whoever li@@ ves and believ@@ es in Me shall never di@@ e. Do you believe this@@ ?”\n",
      " Then he took it dow@@ n, w@@ ra@@ pp@@ ed it in lin@@ en, and laid it in a tom@@ b that was he@@ w@@ n out of the ro@@ ck@@ , where no one had ever la@@ in be@@ for@@ e.\n",
      " Now when they had es@@ ca@@ pe@@ d, they then found out that the is@@ land was called M@@ al@@ ta@@ .\n",
      " Nevertheles@@ s she will be sa@@ ved in chil@@ d@@ be@@ ar@@ ing if they continu@@ e in faith, love, and hol@@ in@@ es@@ s, with self@@ -@@ con@@ tr@@ ol@@ . \n",
      "\n",
      " and this woman was a wi@@ do@@ w of about ei@@ ght@@ y@@ -@@ four year@@ s, who did not depar@@ t from the temple, but ser@@ ved God with f@@ ast@@ ings and pray@@ ers night and day@@ .\n",
      " (@@ as it is writt@@ en, “I have made you a father of many nati@@ on@@ s@@ ”@@ ) in the presence of Him whom he believ@@ ed@@ —@@ God, who gives life to the dead and call@@ s those things which do not ex@@ is@@ t as though they di@@ d;\n",
      " Now there is in Jerusalem by the S@@ he@@ ep G@@ ate a p@@ ool@@ , which is called in H@@ eb@@ re@@ w@@ , Beth@@ es@@ d@@ a, having five p@@ or@@ ch@@ es.\n",
      " Then he go@@ es and tak@@ es with him seven other spir@@ its more wi@@ cked than himself, and they enter and dwell ther@@ e; and the last st@@ ate of that man is wor@@ se than the first@@ . So shall it also be with this wi@@ cked gener@@ ation@@ .”\n",
      "\n",
      "==> dev.bpe.lh <==\n",
      " Kho niba@@ yila omundu owali omubo@@ fu oyo khu,@@ Abafari@@ sa@@ yo@@ . \n",
      " Ne yesi yesi,@@ ou@@ m@@ enya ne nas@@ uu@@ bila mw@@ isie, shi@@ ali@@ fwa tawe. O@@ suubil@@ a,@@ ako@@ ?” \n",
      " Mana nak@@ u@@ ru@@ sia,@@ khumusal@@ aba, nak@@ u@@ f@@ im@@ ba@@ khwo is@@ anda ye@@ ik@@ it@@ ani, ne,@@ nak@@ u@@ yab@@ ila mu@@ ng'@@ ani, ey@@ ali niya@@ yab@@ wa mul@@ w@@ anda y@@ omundu yesi yali nashili okhu@@ yab@@ il@@ w@@ amwo tawe. \n",
      " Olwa khwali khul@@ uk@@ uku ni@@ khwi@@ hon@@ oko@@ os@@ he khw@@ amala ni@@ khumany@@ a mbu, eshi@@ khala@@ chinga eshi@@ o,@@ shil@@ angwa mbu M@@ al@@ it@@ a. \n",
      " Nebutswa omukhasi al@@ ah@@ oni@@ bwa khulw@@ okhwi@@ bul@@ a,@@ aba@@ ana, naba natsi@@ ililila okhw@@ if@@ w@@ ila mubusuubili mubu@@ heel@@ i, nobu@@ takati@@ fu nende obw@@ it@@ e@@ mb@@ elesi@@ .\n",
      "\n",
      " Khandi yam@@ enya nali omule@@ khwa khulwemi@@ y@@ ik@@ a,@@ amakhumi mu@@ n@@ ane na@@ chin@@ e. Nebutswa emi@@ yika echi@@ o,@@ chi@@ osi, yam@@ enyanga butswa muhekalu@@ . Y@@ enam@@ ilanga,@@ OMWAMI Nyasaye eshilo neshi@@ te@@ er@@ e, nah@@ onga inz@@ ala,@@ nende okhusa@@ aya@@ . \n",
      " shinga olwa Amahandik@@ o,@@ kaboolanga mbu, “E@@ khu@@ kholile iwe okhuba s@@ amw@@ ana,@@ Am@@ ahanga am@@ anj@@ i.” Kho obusuubisie buno nobul@@ ayi imbeli,@@ wa Nyasaye, owa Aburahamu ya@@ su@@ bil@@ amwo oul@@ amu@@ s@@ injia,@@ aba@@ fu, khandi owa li@@ khuw@@ ali@@ e li@@ kholanga ebil@@ aliho t@@ a,okhu@@ ba@@ ho. \n",
      " N@@ ali e@@ bwen@@ eyo, li@@ ali@@ yo l@@ iti@@ ba,@@ el@@ il@@ angwa mulu@@ he@@ bur@@ ania mbu, B@@ etsi@@ z@@ at@@ sa, aham@@ bi,@@ khushi@@ li@@ bwa shil@@ angwa mbu, Eshi@@ amak@@ on@@ di@@ . L@@ iti@@ ba,@@ li@@ amaatsi elo liali nebi@@ ro@@ ok@@ oola bir@@ ano. \n",
      " nishi@@ kalu@@ kh@@ ayo shi@@ tsia okhul@@ anga ebishieno b@@ ind@@ i,@@ musafu ebi@@ bi muno, nibi@@ chel@@ ela okhum@@ eny@@ amw@@ o. Ne,@@ olunyuma lw@@ okhumw@@ injil@@ amw@@ o, omundu tsana aba obu@@ bi,@@ okhushila@@ khwo shinga olwa yali olw@@ amb@@ eli. A@@ ko nik@@ o,@@ ak@@ atsia okhwi@@ kholekha khubandu b@@ olwibulo ol@@ um@@ ayan@@ u,@@ lwa bul@@ ano@@ .” N@@ y@@ ina Yesu nende abaana bab@@ we, \n",
      "\n",
      "==> dev.en <==\n",
      "Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "That day was the Preparation , and the Sabbath drew near .\n",
      "But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "not given to wine , not violent , not greedy for money , but gentle , not quarrelsome , not covetous ;\n",
      "and this woman was a widow of about eighty-four years , who did not depart from the temple , but served God with fastings and prayers night and day .\n",
      "And not being weak in faith , he did not consider his own body , already dead ( since he was about a hundred years old ) , and the deadness of Sarah ’ s womb .\n",
      "In these lay a great multitude of sick people , blind , lame , paralyzed , waiting for the moving of the water .\n",
      "Then he goes and takes with him seven other spirits more wicked than himself , and they enter and dwell there ; and the last state of that man is worse than the first . So shall it also be with this wicked generation . ”\n",
      "So He got into a boat , crossed over , and came to His own city .\n",
      "\n",
      "==> dev.lh <==\n",
      "Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "alaba , omumesi kata owobusolo ta , habula omuholo khandi owomulembe , kata owalaheela amapesa tawe .\n",
      "Khandi yamenya nali omulekhwa khulwemiyika , amakhumi munane nachine . Nebutswa emiyika echio , chiosi , yamenyanga butswa muhekalu . Yenamilanga , OMWAMI Nyasaye eshilo neshiteere , nahonga inzala , nende okhusaaya .\n",
      "Yali ahambi owemiyika , eshikhumi shilala , nebutswa obusuubilibwe , shibwatitiyakhwo kata olwa yapaara khubulamu , bwomubilikwe okwali nikwahwamwo amaani ta , noho , kata olwa yamanya mbu , Sara nomukofu shianyala , okhwibula tawe .\n",
      "Omukanda , omukhongo kwabandu , abalwale , ababofu , abalema nende , abakwa amakara , bali nibakonile mubirookoola ebio .\n",
      "nishikalukhayo shitsia okhulanga ebishieno bindi , musafu ebibi muno , nibichelela okhumenyamwo . Ne , olunyuma lwokhumwinjilamwo , omundu tsana aba obubi , okhushilakhwo shinga olwa yali olwambeli . Ako niko , akatsia okhwikholekha khubandu bolwibulo olumayanu , lwa bulano. ” Nyina Yesu nende abaana babwe ,\n",
      "Yesu yenjila muliaro niyambukha niyoola mwitaala , liewabwe elia Kaperinaumu .\n",
      "==> test.bpe.en <==\n",
      " They brought him who for@@ mer@@ ly was blind to the Pharise@@ es.\n",
      " And whoever li@@ ves and believ@@ es in Me shall never di@@ e. Do you believe this@@ ?”\n",
      " Then he took it dow@@ n, w@@ ra@@ pp@@ ed it in lin@@ en, and laid it in a tom@@ b that was he@@ w@@ n out of the ro@@ ck@@ , where no one had ever la@@ in be@@ for@@ e.\n",
      " Now when they had es@@ ca@@ pe@@ d, they then found out that the is@@ land was called M@@ al@@ ta@@ .\n",
      " Nevertheles@@ s she will be sa@@ ved in chil@@ d@@ be@@ ar@@ ing if they continu@@ e in faith, love, and hol@@ in@@ es@@ s, with self@@ -@@ con@@ tr@@ ol@@ . \n",
      "\n",
      " and this woman was a wi@@ do@@ w of about ei@@ ght@@ y@@ -@@ four year@@ s, who did not depar@@ t from the temple, but ser@@ ved God with f@@ ast@@ ings and pray@@ ers night and day@@ .\n",
      " (@@ as it is writt@@ en, “I have made you a father of many nati@@ on@@ s@@ ”@@ ) in the presence of Him whom he believ@@ ed@@ —@@ God, who gives life to the dead and call@@ s those things which do not ex@@ is@@ t as though they di@@ d;\n",
      " Now there is in Jerusalem by the S@@ he@@ ep G@@ ate a p@@ ool@@ , which is called in H@@ eb@@ re@@ w@@ , Beth@@ es@@ d@@ a, having five p@@ or@@ ch@@ es.\n",
      " Then he go@@ es and tak@@ es with him seven other spir@@ its more wi@@ cked than himself, and they enter and dwell ther@@ e; and the last st@@ ate of that man is wor@@ se than the first@@ . So shall it also be with this wi@@ cked gener@@ ation@@ .”\n",
      "\n",
      "==> test.bpe.lh <==\n",
      " Kho niba@@ yila omundu owali omubo@@ fu oyo khu,@@ Abafari@@ sa@@ yo@@ . \n",
      " Ne yesi yesi,@@ ou@@ m@@ enya ne nas@@ uu@@ bila mw@@ isie, shi@@ ali@@ fwa tawe. O@@ suubil@@ a,@@ ako@@ ?” \n",
      " Mana nak@@ u@@ ru@@ sia,@@ khumusal@@ aba, nak@@ u@@ f@@ im@@ ba@@ khwo is@@ anda ye@@ ik@@ it@@ ani, ne,@@ nak@@ u@@ yab@@ ila mu@@ ng'@@ ani, ey@@ ali niya@@ yab@@ wa mul@@ w@@ anda y@@ omundu yesi yali nashili okhu@@ yab@@ il@@ w@@ amwo tawe. \n",
      " Olwa khwali khul@@ uk@@ uku ni@@ khwi@@ hon@@ oko@@ os@@ he khw@@ amala ni@@ khumany@@ a mbu, eshi@@ khala@@ chinga eshi@@ o,@@ shil@@ angwa mbu M@@ al@@ it@@ a. \n",
      " Nebutswa omukhasi al@@ ah@@ oni@@ bwa khulw@@ okhwi@@ bul@@ a,@@ aba@@ ana, naba natsi@@ ililila okhw@@ if@@ w@@ ila mubusuubili mubu@@ heel@@ i, nobu@@ takati@@ fu nende obw@@ it@@ e@@ mb@@ elesi@@ .\n",
      "\n",
      " Khandi yam@@ enya nali omule@@ khwa khulwemi@@ y@@ ik@@ a,@@ amakhumi mu@@ n@@ ane na@@ chin@@ e. Nebutswa emi@@ yika echi@@ o,@@ chi@@ osi, yam@@ enyanga butswa muhekalu@@ . Y@@ enam@@ ilanga,@@ OMWAMI Nyasaye eshilo neshi@@ te@@ er@@ e, nah@@ onga inz@@ ala,@@ nende okhusa@@ aya@@ . \n",
      " shinga olwa Amahandik@@ o,@@ kaboolanga mbu, “E@@ khu@@ kholile iwe okhuba s@@ amw@@ ana,@@ Am@@ ahanga am@@ anj@@ i.” Kho obusuubisie buno nobul@@ ayi imbeli,@@ wa Nyasaye, owa Aburahamu ya@@ su@@ bil@@ amwo oul@@ amu@@ s@@ injia,@@ aba@@ fu, khandi owa li@@ khuw@@ ali@@ e li@@ kholanga ebil@@ aliho t@@ a,okhu@@ ba@@ ho. \n",
      " N@@ ali e@@ bwen@@ eyo, li@@ ali@@ yo l@@ iti@@ ba,@@ el@@ il@@ angwa mulu@@ he@@ bur@@ ania mbu, B@@ etsi@@ z@@ at@@ sa, aham@@ bi,@@ khushi@@ li@@ bwa shil@@ angwa mbu, Eshi@@ amak@@ on@@ di@@ . L@@ iti@@ ba,@@ li@@ amaatsi elo liali nebi@@ ro@@ ok@@ oola bir@@ ano. \n",
      " nishi@@ kalu@@ kh@@ ayo shi@@ tsia okhul@@ anga ebishieno b@@ ind@@ i,@@ musafu ebi@@ bi muno, nibi@@ chel@@ ela okhum@@ eny@@ amw@@ o. Ne,@@ olunyuma lw@@ okhumw@@ injil@@ amw@@ o, omundu tsana aba obu@@ bi,@@ okhushila@@ khwo shinga olwa yali olw@@ amb@@ eli. A@@ ko nik@@ o,@@ ak@@ atsia okhwi@@ kholekha khubandu b@@ olwibulo ol@@ um@@ ayan@@ u,@@ lwa bul@@ ano@@ .” N@@ y@@ ina Yesu nende abaana bab@@ we, \n",
      "\n",
      "==> test.en <==\n",
      "Inasmuch then as the children have partaken of flesh and blood , He Himself likewise shared in the same , that through death He might destroy him who had the power of death , that is , the devil ,\n",
      "So all this was done that it might be fulfilled which was spoken by the Lord through the prophet , saying :\n",
      "Now the men who held Jesus mocked Him and beat Him .\n",
      "So you ought to have deposited my money with the bankers , and at my coming I would have received back my own with interest .\n",
      "Assuredly , I say to you , wherever this gospel is preached in the whole world , what this woman has done will also be told as a memorial to her . ”\n",
      "And Mary Magdalene was there , and the other Mary , sitting opposite the tomb .\n",
      "To the saints and faithful brethren in Christ who are in Colosse : Grace to you and peace from God our Father and the Lord Jesus Christ .\n",
      "but to wear sandals , and not to put on two tunics .\n",
      "And when I wanted to know the reason they accused him , I brought him before their council .\n",
      "For though by this time you ought to be teachers , you need someone to teach you again the first principles of the oracles of God ; and you have come to need milk and not solid food .\n",
      "\n",
      "==> test.lh <==\n",
      "Ne abaana , shinga omwene abalanganga ario , nabandu , abali nomubili okwoburulanda , Yesu omwene yekhola , shinga abo nasanga ninabo mumubili okwoburulanda. , Yakhola ario kho mbu okhubirira mukhufwakhwe anyale , okhusasia Setani , ouli nobunyali bwokhwira .\n",
      "Ako koosi kekholekha khulwa okhuusia aka OMWAMI , Nyasaye yali niyaboola okhubirira khumulakusi ari\n",
      "Abandu abali nibamutilile Yesu bamunyokhoosia khandi , nibamunywekha .\n",
      "Wali nokhoyile okhukholela italanda yanje inganga , ne , mukhuchelela khwanje ndakhanyoolile italanda yanje , nende injelesio yayo .\n",
      "Emuboolela toto mbu , hosi hosi aha , amakhuwa amalayi aka Nyasaye kano kakhayaalwe , mushialo , amakhuwa komukhasi uno akholile okhuula , kakharumbulwe khulwa okhumwitsulila ye. ” Yuda afuchilila okhukhoba Yesu\n",
      "Mariamu Magadalene , nende Mariamu undi baliho nibekhale nibahengane nende , ing'ani . Abelihe baliinda hashilindwa ,\n",
      "Yule khuba Kristo nende , abaana befwe abasuubili mu Kristo aba ebu Kolosai. , Nyasaye ouli sefwe amuhelesia obukoosia nende , omulembe . Amasaayo akokhuupa Orio ,\n",
      "Nebutswa muchendelenje tsingato khandi , mwifwalenje ingubo indala , shimulachinga eyindi tawe. ” ,\n",
      "Ne olwa ndenya , okhumanya eshiabali nibamwinjililangakhwo itookho ndamuyila khubeshiina shiabwe .\n",
      "Mwabele nebise ebihela , inywe okhuba abeechesia , nebutswa mushienyanga , omundu okhumwechesia ameeko akambeli akoburume bwa Nyasaye . Olwa muba nimwenyekhanga okhulia , ebiliibwa ebimilanu , nilwo olwa mushinywetusanga , amabeele .\n"
     ]
    }
   ],
   "source": [
    "! head train.*\n",
    "! head dev.*\n",
    "! head test.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "AgQOGzLyZwKT",
    "outputId": "45fe4d89-b041-4440-e040-1a14a90d5d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8a/064b4077e3d793f877e3b77aa64f56fa49a4d37236a53f78ee28be009a16/numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3MB 196kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/50/84184d6230686e230c464f0dd4ff32eada2756b4a0b9cefec68b88d1d580/torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 20.9MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 29.6MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/dd/7b8e2f0ed47cc46ba2f6144df4f32de3614d6f56ca07b7f308d40cfa2dfb/pylint-2.9.3-py3-none-any.whl (372kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 34.6MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Collecting isort<6,>=4.2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/6f/5adde6e4d9e745a39fa0fed2ae0ca8667df95fee50c516370767dde7e000/isort-5.9.2-py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 38.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting mccabe<0.7,>=0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
      "Collecting astroid<2.7,>=2.6.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/bc/ee4ca5a31fbc1f9cec6df52170baa151f82cbf4f4989de87c7f94a28a958/astroid-2.6.2-py3-none-any.whl (228kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 40.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Collecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 34.1MB/s \n",
      "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/b0/f055db25fd68ab4859832a887c8b304274fc12dd5a3f8e83e61250733aeb/lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-cp37-none-any.whl size=85058 sha256=593972ffd594d980b57fdad5835199db5f536f4891a0d2837747d077697d4a9f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5rup3daz/wheels/c0/35/d9/e092219ce4f1be75b87f52e253b59d0161fb62ec8f61a98c1d\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68398 sha256=bef607a57e289377ab6d0b63312c19cc52ce120739eaec5fe7fd028ee85be959\n",
      "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
      "Successfully built joeynmt wrapt\n",
      "\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.20.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.5.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.5.0 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, torchtext, portalocker, sacrebleu, subword-nmt, pyyaml, isort, mccabe, typed-ast, lazy-object-proxy, wrapt, astroid, pylint, six, joeynmt\n",
      "  Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "Successfully installed astroid-2.6.2 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.3 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7pdwWNLaLS6"
   },
   "outputs": [],
   "source": [
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt3 -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt3\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt3 < train.$tgt3 > train.bpe.$tgt3\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt3 < dev.$tgt3 > dev.bpe.$tgt3\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt3 < test.$tgt3 > test.bpe.$tgt3\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt3 --output_path vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYFWd4nDbgD2",
    "outputId": "2df6b864-11a9-48d5-f0c1-4278bc3c3fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Luhya Sentences\n",
      "N@@ asi , ni@@ reeba endi , ‘ N@@ iwe wina , Omwami ? ’ Omw@@ oyo okwo , nik@@ umb@@ ool@@ ela kuri , ‘ N@@ isie Yesu owa Nazaret@@ i ow@@ os@@ a@@ and@@ injia . ’\n",
      "shichila , omukh@@ a@@ anawe omut@@ elwa , ow@@ emiyika ekhumi na@@ chi@@ bili yali n@@ any@@ ir@@ anga . Ne olwa yali na@@ tsitsanga , abandu , bam@@ wi@@ bu@@ mb@@ akhwo okhurula mu@@ tsimb@@ eka tsiosi .\n",
      "Ne olwa kab@@ isibwa mbu khu@@ khoyile okhu@@ tsi@@ ila , mum@@ eeli okhuula I@@ tal@@ ia , ba@@ haana Paulo nende abab@@ ohe , bandi khumu@@ s@@ injilili w@@ elihe J@@ ul@@ i@@ asi owe@@ ing'@@ anda eya , eshi@@ r@@ oma ey@@ il@@ angwa mbu , “ I@@ ng'@@ anda ey@@ il@@ ind@@ anga , Omuruchi . ”\n",
      "Ol@@ uny@@ um@@ akhwo , abakuuka befwe , abab@@ ukula li@@ he@@ ema elo okhurula khub@@ as@@ abwe , bali@@ chinga , okhuula mutsinyanga tsia Yos@@ h@@ wa nibab@@ ukula eshialo , eshia amahanga aka Nyasaye yal@@ ondanga nik@@ arula imbeli , wabwe . Ne li@@ am@@ eny@@ ayo okhuula mutsinyanga tsia , omuruchi Daudi .\n",
      "Ne olwa , y@@ enj@@ ilamwo , yab@@ areeba ari , “ M@@ wi@@ khu@@ ulanga nimu@@ khu@@ pa , tsim@@ bu@@ ng@@ u mbushiina ? Omwana uno shi@@ afw@@ ile ta , habula , ak@@ on@@ anga butswa tsind@@ oolo . ”\n",
      "Combined BPE Vocab\n",
      "Eshim@@\n",
      "haaka\n",
      "chin@@\n",
      "Mari@@\n",
      "okoya\n",
      "injel@@\n",
      "tside\n",
      "unjila\n",
      "ing'ini\n",
      "q@@\n"
     ]
    }
   ],
   "source": [
    "# Some output\n",
    "! echo \"BPE Luhya Sentences\"\n",
    "! tail -n 5 test.bpe.$tgt3\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9VOJvLygjfG"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLaDEY0U6kzN"
   },
   "source": [
    "## Luganda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piNsl_221dNI"
   },
   "outputs": [],
   "source": [
    "# Changing to Luganda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "V0kQqdMr1xIw",
    "outputId": "3279d055-2ec3-4be9-9670-f5dd298811aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Luganda/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Requirement already satisfied: numpy==1.20.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.20.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.9.0)\n",
      "Requirement already satisfied: sacrebleu>=1.3.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.5.1)\n",
      "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.3.7)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (5.4.1)\n",
      "Requirement already satisfied: pylint in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.9.5)\n",
      "Requirement already satisfied: six==1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.12.0)\n",
      "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.6->joeynmt==1.3) (2.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Requirement already satisfied: astroid<2.7,>=2.6.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (2.6.5)\n",
      "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (5.9.2)\n",
      "Requirement already satisfied: typed-ast<1.5,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.5->pylint->joeynmt==1.3) (1.4.3)\n",
      "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.7,>=2.6.5->pylint->joeynmt==1.3) (1.6.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85058 sha256=ee244f622f96330fbe7605e3786a0bfdcad815b0a4712d40a0e1bfc6e32b8332\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-i96seoa9/wheels/b8/3e/ec/4da3b842b3679715f7cd3b4065c087c62dd0fcb0ab5f55b80c\n",
      "Successfully built joeynmt\n",
      "Installing collected packages: joeynmt\n",
      "  Attempting uninstall: joeynmt\n",
      "    Found existing installation: joeynmt 1.3\n",
      "    Uninstalling joeynmt-1.3:\n",
      "      Successfully uninstalled joeynmt-1.3\n",
      "Successfully installed joeynmt-1.3\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "M4pO-ZQogqZU"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language1, source_language)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language1}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language1}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"joeynmt/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 3000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer\"\n",
    "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\", source_language=source_language, target_language1=target_language1)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fq3kiHP1Bj6p",
    "outputId": "503ff9f6-ef90-4501-bd52-ff710fe45bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-26 09:05:19,702 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-26 09:05:19,730 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-26 09:05:24,612 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-26 09:05:24,917 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-26 09:05:24,946 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-26 09:05:24,998 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-26 09:05:24,998 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-26 09:05:25,255 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-26 09:05:25.431993: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-26 09:05:26,860 - INFO - joeynmt.training - Total params: 12152064\n",
      "2021-07-26 09:05:29,079 - INFO - joeynmt.helpers - cfg.name                           : lgen_reverse_transformer\n",
      "2021-07-26 09:05:29,079 - INFO - joeynmt.helpers - cfg.data.src                       : lg\n",
      "2021-07-26 09:05:29,080 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-26 09:05:29,080 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\n",
      "2021-07-26 09:05:29,080 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\n",
      "2021-07-26 09:05:29,080 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\n",
      "2021-07-26 09:05:29,081 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-26 09:05:29,081 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-26 09:05:29,081 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-26 09:05:29,081 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-07-26 09:05:29,082 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-07-26 09:05:29,082 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-26 09:05:29,082 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-26 09:05:29,082 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-26 09:05:29,083 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-26 09:05:29,083 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-26 09:05:29,083 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-26 09:05:29,083 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-26 09:05:29,084 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-26 09:05:29,084 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-26 09:05:29,084 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-26 09:05:29,084 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-26 09:05:29,085 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-26 09:05:29,085 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-26 09:05:29,085 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-26 09:05:29,085 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-26 09:05:29,086 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-26 09:05:29,086 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-26 09:05:29,086 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-26 09:05:29,086 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-26 09:05:29,087 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-26 09:05:29,087 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-26 09:05:29,087 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-26 09:05:29,087 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-26 09:05:29,088 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 3000\n",
      "2021-07-26 09:05:29,088 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-26 09:05:29,088 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-26 09:05:29,088 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lgen_reverse_transformer\n",
      "2021-07-26 09:05:29,089 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-26 09:05:29,089 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-26 09:05:29,089 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-26 09:05:29,089 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-26 09:05:29,090 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-26 09:05:29,090 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-26 09:05:29,090 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-26 09:05:29,090 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-26 09:05:29,091 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-26 09:05:29,091 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-26 09:05:29,091 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-26 09:05:29,091 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-26 09:05:29,092 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-26 09:05:29,092 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-26 09:05:29,092 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-26 09:05:29,092 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-26 09:05:29,093 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-26 09:05:29,093 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-26 09:05:29,093 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-26 09:05:29,093 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-26 09:05:29,094 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-26 09:05:29,094 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-26 09:05:29,094 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-26 09:05:29,094 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-26 09:05:29,095 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-26 09:05:29,095 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-26 09:05:29,095 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-26 09:05:29,095 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-26 09:05:29,096 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-26 09:05:29,096 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-26 09:05:29,096 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-26 09:05:29,096 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 225657,\n",
      "\tvalid 1000,\n",
      "\ttest 2692\n",
      "2021-07-26 09:05:29,097 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-07-26 09:05:29,097 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-07-26 09:05:29,097 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-07-26 09:05:29,098 - INFO - joeynmt.helpers - Number of Src words (types): 4265\n",
      "2021-07-26 09:05:29,098 - INFO - joeynmt.helpers - Number of Trg words (types): 4265\n",
      "2021-07-26 09:05:29,098 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4265),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4265))\n",
      "2021-07-26 09:05:29,114 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-26 09:05:29,115 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-26 09:05:58,672 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.645333, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-07-26 09:06:27,272 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.243737, Tokens per Sec:     7332, Lr: 0.000300\n",
      "2021-07-26 09:06:56,150 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.184949, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-07-26 09:07:24,871 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     4.996638, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-07-26 09:07:53,912 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.917653, Tokens per Sec:     7375, Lr: 0.000300\n",
      "2021-07-26 09:08:22,991 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.393587, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-07-26 09:08:51,879 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.768122, Tokens per Sec:     7331, Lr: 0.000300\n",
      "2021-07-26 09:09:20,621 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.606078, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-07-26 09:09:49,456 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.610054, Tokens per Sec:     7530, Lr: 0.000300\n",
      "2021-07-26 09:10:18,358 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.488424, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-07-26 09:10:47,244 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     4.381368, Tokens per Sec:     7497, Lr: 0.000300\n",
      "2021-07-26 09:11:16,007 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.238816, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-07-26 09:11:44,432 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.234028, Tokens per Sec:     7394, Lr: 0.000300\n",
      "2021-07-26 09:12:12,786 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.413239, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-07-26 09:12:41,906 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.803061, Tokens per Sec:     7591, Lr: 0.000300\n",
      "2021-07-26 09:13:10,607 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.489617, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-07-26 09:13:39,567 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.088424, Tokens per Sec:     7489, Lr: 0.000300\n",
      "2021-07-26 09:14:07,964 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.787845, Tokens per Sec:     7543, Lr: 0.000300\n",
      "2021-07-26 09:14:37,093 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.017128, Tokens per Sec:     7550, Lr: 0.000300\n",
      "2021-07-26 09:15:05,972 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.649893, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-07-26 09:15:35,009 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.748681, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-07-26 09:16:03,793 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.602364, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-07-26 09:16:32,689 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.227541, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-07-26 09:17:01,450 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.825097, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-07-26 09:17:30,387 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     3.817817, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-07-26 09:17:59,357 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.897349, Tokens per Sec:     7532, Lr: 0.000300\n",
      "2021-07-26 09:18:28,339 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.736707, Tokens per Sec:     7598, Lr: 0.000300\n",
      "2021-07-26 09:18:50,730 - INFO - joeynmt.training - Epoch   1: total training loss 11842.43\n",
      "2021-07-26 09:18:50,731 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-26 09:18:57,539 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     3.720803, Tokens per Sec:     6874, Lr: 0.000300\n",
      "2021-07-26 09:19:26,443 - INFO - joeynmt.training - Epoch   2, Step:     2900, Batch Loss:     3.375562, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-07-26 09:19:55,257 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     3.412509, Tokens per Sec:     7397, Lr: 0.000300\n",
      "2021-07-26 09:20:51,667 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 09:20:51,667 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 09:20:51,668 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 09:20:51,975 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 09:20:51,975 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 09:20:52,800 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 09:20:52,802 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 09:20:52,802 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 09:20:52,802 - INFO - joeynmt.training - \tHypothesis: [ I am not not not to be a person , but he will be a person , and he will be a person . ”\n",
      "2021-07-26 09:20:52,802 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 09:20:52,804 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 09:20:52,804 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 09:20:52,804 - INFO - joeynmt.training - \tHypothesis: Jesus was a man , and he was a man who was a man who said : “ I am my Father , and I am my own own own . ”\n",
      "2021-07-26 09:20:52,805 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 09:20:52,805 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 09:20:52,806 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 09:20:52,806 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah show that he did not be ?\n",
      "2021-07-26 09:20:52,806 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 09:20:52,807 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 09:20:52,807 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 09:20:52,807 - INFO - joeynmt.training - \tHypothesis: But he was a child to be a family to be a family .\n",
      "2021-07-26 09:20:52,807 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     3000: bleu:   4.65, loss: 88693.9453, ppl:  28.2407, duration: 57.5498s\n",
      "2021-07-26 09:21:22,108 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     3.492141, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-07-26 09:21:51,195 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     3.547958, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-07-26 09:22:20,389 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     3.449842, Tokens per Sec:     7607, Lr: 0.000300\n",
      "2021-07-26 09:22:48,913 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     3.551685, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-07-26 09:23:18,069 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     3.104010, Tokens per Sec:     7529, Lr: 0.000300\n",
      "2021-07-26 09:23:47,296 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     3.382248, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-07-26 09:24:15,576 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     2.980936, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-07-26 09:24:44,276 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.600156, Tokens per Sec:     7350, Lr: 0.000300\n",
      "2021-07-26 09:25:13,182 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     3.453096, Tokens per Sec:     7529, Lr: 0.000300\n",
      "2021-07-26 09:25:41,831 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     3.215676, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-07-26 09:26:10,971 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     3.416002, Tokens per Sec:     7651, Lr: 0.000300\n",
      "2021-07-26 09:26:39,074 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     2.696465, Tokens per Sec:     7305, Lr: 0.000300\n",
      "2021-07-26 09:27:07,933 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     3.678245, Tokens per Sec:     7538, Lr: 0.000300\n",
      "2021-07-26 09:27:36,304 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     3.293608, Tokens per Sec:     7308, Lr: 0.000300\n",
      "2021-07-26 09:28:05,520 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     3.241782, Tokens per Sec:     7635, Lr: 0.000300\n",
      "2021-07-26 09:28:33,876 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     3.157339, Tokens per Sec:     7484, Lr: 0.000300\n",
      "2021-07-26 09:29:03,118 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     3.295664, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-26 09:29:31,674 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     3.333448, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-07-26 09:30:00,200 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     3.250522, Tokens per Sec:     7462, Lr: 0.000300\n",
      "2021-07-26 09:30:28,946 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     3.220361, Tokens per Sec:     7440, Lr: 0.000300\n",
      "2021-07-26 09:30:57,906 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     3.187941, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-07-26 09:31:26,382 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.659781, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-07-26 09:31:55,285 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     2.998628, Tokens per Sec:     7626, Lr: 0.000300\n",
      "2021-07-26 09:32:24,140 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     3.320992, Tokens per Sec:     7515, Lr: 0.000300\n",
      "2021-07-26 09:32:52,996 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     3.225784, Tokens per Sec:     7488, Lr: 0.000300\n",
      "2021-07-26 09:33:09,014 - INFO - joeynmt.training - Epoch   2: total training loss 9023.39\n",
      "2021-07-26 09:33:09,014 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-26 09:33:21,921 - INFO - joeynmt.training - Epoch   3, Step:     5600, Batch Loss:     3.007073, Tokens per Sec:     7126, Lr: 0.000300\n",
      "2021-07-26 09:33:50,732 - INFO - joeynmt.training - Epoch   3, Step:     5700, Batch Loss:     3.335918, Tokens per Sec:     7598, Lr: 0.000300\n",
      "2021-07-26 09:34:19,460 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     3.235142, Tokens per Sec:     7477, Lr: 0.000300\n",
      "2021-07-26 09:34:47,875 - INFO - joeynmt.training - Epoch   3, Step:     5900, Batch Loss:     2.869432, Tokens per Sec:     7443, Lr: 0.000300\n",
      "2021-07-26 09:35:16,645 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     2.993548, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-07-26 09:36:01,189 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 09:36:01,190 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 09:36:01,190 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 09:36:01,488 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 09:36:01,489 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 09:36:02,329 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 09:36:02,330 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 09:36:02,331 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 09:36:02,331 - INFO - joeynmt.training - \tHypothesis: [ Sam ] If you are not a source of the source of God , he has been a perfect way to be true God . ”\n",
      "2021-07-26 09:36:02,331 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 09:36:02,332 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 09:36:02,332 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 09:36:02,332 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he was baptized in heaven , saying : “ I am my Father , O my Father , I am my father . ”\n",
      "2021-07-26 09:36:02,333 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 09:36:02,333 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 09:36:02,334 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 09:36:02,334 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ?\n",
      "2021-07-26 09:36:02,334 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 09:36:02,335 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 09:36:02,335 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 09:36:02,335 - INFO - joeynmt.training - \tHypothesis: But the Gospels had to be a family .\n",
      "2021-07-26 09:36:02,335 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     6000: bleu:   9.57, loss: 74349.9375, ppl:  16.4525, duration: 45.6901s\n",
      "2021-07-26 09:36:31,183 - INFO - joeynmt.training - Epoch   3, Step:     6100, Batch Loss:     2.485568, Tokens per Sec:     7380, Lr: 0.000300\n",
      "2021-07-26 09:36:59,842 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     3.015359, Tokens per Sec:     7576, Lr: 0.000300\n",
      "2021-07-26 09:37:28,831 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     2.881807, Tokens per Sec:     7555, Lr: 0.000300\n",
      "2021-07-26 09:37:57,569 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     3.018582, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-07-26 09:38:26,276 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     2.815252, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-07-26 09:38:54,991 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     3.017027, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-07-26 09:39:24,104 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     3.210854, Tokens per Sec:     7662, Lr: 0.000300\n",
      "2021-07-26 09:39:52,691 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     3.156859, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-07-26 09:40:21,497 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     3.280516, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-26 09:40:50,676 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     2.869543, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-07-26 09:41:19,057 - INFO - joeynmt.training - Epoch   3, Step:     7100, Batch Loss:     3.266522, Tokens per Sec:     7394, Lr: 0.000300\n",
      "2021-07-26 09:41:47,596 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     3.018093, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-07-26 09:42:16,408 - INFO - joeynmt.training - Epoch   3, Step:     7300, Batch Loss:     2.606842, Tokens per Sec:     7538, Lr: 0.000300\n",
      "2021-07-26 09:42:45,230 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     2.947232, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-07-26 09:43:13,883 - INFO - joeynmt.training - Epoch   3, Step:     7500, Batch Loss:     2.967185, Tokens per Sec:     7509, Lr: 0.000300\n",
      "2021-07-26 09:43:42,614 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     3.052156, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-07-26 09:44:11,027 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     3.017172, Tokens per Sec:     7190, Lr: 0.000300\n",
      "2021-07-26 09:44:39,764 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     2.982334, Tokens per Sec:     7559, Lr: 0.000300\n",
      "2021-07-26 09:45:08,540 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     2.963676, Tokens per Sec:     7512, Lr: 0.000300\n",
      "2021-07-26 09:45:37,550 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     2.950655, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-26 09:46:06,090 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     2.963842, Tokens per Sec:     7561, Lr: 0.000300\n",
      "2021-07-26 09:46:34,853 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     2.932530, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-07-26 09:47:03,908 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     2.686626, Tokens per Sec:     7641, Lr: 0.000300\n",
      "2021-07-26 09:47:12,823 - INFO - joeynmt.training - Epoch   3: total training loss 8073.90\n",
      "2021-07-26 09:47:12,823 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-26 09:47:32,924 - INFO - joeynmt.training - Epoch   4, Step:     8400, Batch Loss:     2.890346, Tokens per Sec:     7215, Lr: 0.000300\n",
      "2021-07-26 09:48:01,842 - INFO - joeynmt.training - Epoch   4, Step:     8500, Batch Loss:     2.806733, Tokens per Sec:     7480, Lr: 0.000300\n",
      "2021-07-26 09:48:30,650 - INFO - joeynmt.training - Epoch   4, Step:     8600, Batch Loss:     2.858665, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-07-26 09:48:59,768 - INFO - joeynmt.training - Epoch   4, Step:     8700, Batch Loss:     2.512944, Tokens per Sec:     7665, Lr: 0.000300\n",
      "2021-07-26 09:49:28,247 - INFO - joeynmt.training - Epoch   4, Step:     8800, Batch Loss:     2.426760, Tokens per Sec:     7263, Lr: 0.000300\n",
      "2021-07-26 09:49:57,105 - INFO - joeynmt.training - Epoch   4, Step:     8900, Batch Loss:     2.808520, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-07-26 09:50:26,223 - INFO - joeynmt.training - Epoch   4, Step:     9000, Batch Loss:     2.684134, Tokens per Sec:     7560, Lr: 0.000300\n",
      "2021-07-26 09:51:09,233 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 09:51:09,234 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 09:51:09,234 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 09:51:09,536 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 09:51:09,536 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 09:51:10,370 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 09:51:10,372 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 09:51:10,372 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 09:51:10,372 - INFO - joeynmt.training - \tHypothesis: [ Sam ] the good and the sincere ones and the sincere ones , that is patient . ”\n",
      "2021-07-26 09:51:10,372 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 09:51:10,373 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 09:51:10,373 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 09:51:10,374 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt when he was baptized in heaven in heaven : “ You are my love for me . ”\n",
      "2021-07-26 09:51:10,374 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 09:51:10,375 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 09:51:10,375 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 09:51:10,375 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ?\n",
      "2021-07-26 09:51:10,375 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 09:51:10,376 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 09:51:10,376 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 09:51:10,376 - INFO - joeynmt.training - \tHypothesis: But the Memorial was a source of the family .\n",
      "2021-07-26 09:51:10,377 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     9000: bleu:  13.31, loss: 67064.0703, ppl:  12.5040, duration: 44.1535s\n",
      "2021-07-26 09:51:39,376 - INFO - joeynmt.training - Epoch   4, Step:     9100, Batch Loss:     2.633294, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-07-26 09:52:08,066 - INFO - joeynmt.training - Epoch   4, Step:     9200, Batch Loss:     2.760155, Tokens per Sec:     7613, Lr: 0.000300\n",
      "2021-07-26 09:52:36,935 - INFO - joeynmt.training - Epoch   4, Step:     9300, Batch Loss:     2.274561, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-07-26 09:53:06,046 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     2.716780, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-07-26 09:53:34,962 - INFO - joeynmt.training - Epoch   4, Step:     9500, Batch Loss:     2.822780, Tokens per Sec:     7440, Lr: 0.000300\n",
      "2021-07-26 09:54:03,425 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     2.757871, Tokens per Sec:     7474, Lr: 0.000300\n",
      "2021-07-26 09:54:32,413 - INFO - joeynmt.training - Epoch   4, Step:     9700, Batch Loss:     2.927676, Tokens per Sec:     7552, Lr: 0.000300\n",
      "2021-07-26 09:55:01,166 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     2.547737, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-07-26 09:55:29,971 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     3.014037, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-07-26 09:55:58,362 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     2.705755, Tokens per Sec:     7315, Lr: 0.000300\n",
      "2021-07-26 09:56:26,673 - INFO - joeynmt.training - Epoch   4, Step:    10100, Batch Loss:     2.768971, Tokens per Sec:     7342, Lr: 0.000300\n",
      "2021-07-26 09:56:55,310 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     2.839161, Tokens per Sec:     7450, Lr: 0.000300\n",
      "2021-07-26 09:57:24,081 - INFO - joeynmt.training - Epoch   4, Step:    10300, Batch Loss:     2.062434, Tokens per Sec:     7567, Lr: 0.000300\n",
      "2021-07-26 09:57:52,862 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     2.811274, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-07-26 09:58:21,739 - INFO - joeynmt.training - Epoch   4, Step:    10500, Batch Loss:     2.673349, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-07-26 09:58:50,137 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     2.753890, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-07-26 09:59:19,068 - INFO - joeynmt.training - Epoch   4, Step:    10700, Batch Loss:     2.564809, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-07-26 09:59:47,986 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     2.688453, Tokens per Sec:     7589, Lr: 0.000300\n",
      "2021-07-26 10:00:16,749 - INFO - joeynmt.training - Epoch   4, Step:    10900, Batch Loss:     2.654485, Tokens per Sec:     7537, Lr: 0.000300\n",
      "2021-07-26 10:00:45,275 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     2.649977, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-07-26 10:01:13,992 - INFO - joeynmt.training - Epoch   4, Step:    11100, Batch Loss:     2.655881, Tokens per Sec:     7609, Lr: 0.000300\n",
      "2021-07-26 10:01:15,722 - INFO - joeynmt.training - Epoch   4: total training loss 7527.89\n",
      "2021-07-26 10:01:15,723 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-26 10:01:42,641 - INFO - joeynmt.training - Epoch   5, Step:    11200, Batch Loss:     2.507202, Tokens per Sec:     7085, Lr: 0.000300\n",
      "2021-07-26 10:02:11,808 - INFO - joeynmt.training - Epoch   5, Step:    11300, Batch Loss:     2.653426, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-07-26 10:02:40,284 - INFO - joeynmt.training - Epoch   5, Step:    11400, Batch Loss:     2.745528, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-07-26 10:03:09,119 - INFO - joeynmt.training - Epoch   5, Step:    11500, Batch Loss:     2.419349, Tokens per Sec:     7431, Lr: 0.000300\n",
      "2021-07-26 10:03:37,897 - INFO - joeynmt.training - Epoch   5, Step:    11600, Batch Loss:     2.272877, Tokens per Sec:     7562, Lr: 0.000300\n",
      "2021-07-26 10:04:06,865 - INFO - joeynmt.training - Epoch   5, Step:    11700, Batch Loss:     2.718756, Tokens per Sec:     7573, Lr: 0.000300\n",
      "2021-07-26 10:04:35,557 - INFO - joeynmt.training - Epoch   5, Step:    11800, Batch Loss:     2.674410, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-07-26 10:05:04,457 - INFO - joeynmt.training - Epoch   5, Step:    11900, Batch Loss:     2.684710, Tokens per Sec:     7521, Lr: 0.000300\n",
      "2021-07-26 10:05:33,195 - INFO - joeynmt.training - Epoch   5, Step:    12000, Batch Loss:     2.451450, Tokens per Sec:     7361, Lr: 0.000300\n",
      "2021-07-26 10:06:15,412 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 10:06:15,413 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 10:06:15,413 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 10:06:15,703 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 10:06:15,704 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 10:06:16,631 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 10:06:16,632 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 10:06:16,632 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 10:06:16,633 - INFO - joeynmt.training - \tHypothesis: [ I ] have a good way and the sort of suffering , and it is patient , that is God’s approval . ”\n",
      "2021-07-26 10:06:16,633 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 10:06:16,636 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 10:06:16,636 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 10:06:16,637 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt voice from heaven , saying : “ You are my Son . ”\n",
      "2021-07-26 10:06:16,637 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 10:06:16,638 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 10:06:16,638 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 10:06:16,638 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus to his prayers ?\n",
      "2021-07-26 10:06:16,639 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 10:06:16,640 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 10:06:16,640 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 10:06:16,641 - INFO - joeynmt.training - \tHypothesis: But Absalom was to be able to be saved to the family .\n",
      "2021-07-26 10:06:16,641 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    12000: bleu:  16.22, loss: 62883.1406, ppl:  10.6821, duration: 43.4456s\n",
      "2021-07-26 10:06:45,417 - INFO - joeynmt.training - Epoch   5, Step:    12100, Batch Loss:     2.638687, Tokens per Sec:     7355, Lr: 0.000300\n",
      "2021-07-26 10:07:14,513 - INFO - joeynmt.training - Epoch   5, Step:    12200, Batch Loss:     2.958011, Tokens per Sec:     7618, Lr: 0.000300\n",
      "2021-07-26 10:07:43,252 - INFO - joeynmt.training - Epoch   5, Step:    12300, Batch Loss:     2.627488, Tokens per Sec:     7350, Lr: 0.000300\n",
      "2021-07-26 10:08:12,291 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     2.614484, Tokens per Sec:     7412, Lr: 0.000300\n",
      "2021-07-26 10:08:40,975 - INFO - joeynmt.training - Epoch   5, Step:    12500, Batch Loss:     2.384458, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-07-26 10:09:09,760 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     2.750264, Tokens per Sec:     7313, Lr: 0.000300\n",
      "2021-07-26 10:09:38,644 - INFO - joeynmt.training - Epoch   5, Step:    12700, Batch Loss:     2.529809, Tokens per Sec:     7624, Lr: 0.000300\n",
      "2021-07-26 10:10:07,044 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     2.386715, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-07-26 10:10:36,266 - INFO - joeynmt.training - Epoch   5, Step:    12900, Batch Loss:     2.966787, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-07-26 10:11:04,700 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     2.459232, Tokens per Sec:     7354, Lr: 0.000300\n",
      "2021-07-26 10:11:33,472 - INFO - joeynmt.training - Epoch   5, Step:    13100, Batch Loss:     2.555809, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-07-26 10:12:01,971 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     2.475599, Tokens per Sec:     7556, Lr: 0.000300\n",
      "2021-07-26 10:12:30,825 - INFO - joeynmt.training - Epoch   5, Step:    13300, Batch Loss:     2.655097, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-07-26 10:12:59,530 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     2.303262, Tokens per Sec:     7567, Lr: 0.000300\n",
      "2021-07-26 10:13:28,012 - INFO - joeynmt.training - Epoch   5, Step:    13500, Batch Loss:     2.288608, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-07-26 10:13:57,164 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     2.430839, Tokens per Sec:     7638, Lr: 0.000300\n",
      "2021-07-26 10:14:26,109 - INFO - joeynmt.training - Epoch   5, Step:    13700, Batch Loss:     2.713632, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-07-26 10:14:54,836 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     2.534387, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-07-26 10:15:19,791 - INFO - joeynmt.training - Epoch   5: total training loss 7161.90\n",
      "2021-07-26 10:15:19,791 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-26 10:15:23,952 - INFO - joeynmt.training - Epoch   6, Step:    13900, Batch Loss:     2.428725, Tokens per Sec:     6614, Lr: 0.000300\n",
      "2021-07-26 10:15:52,860 - INFO - joeynmt.training - Epoch   6, Step:    14000, Batch Loss:     2.743247, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-07-26 10:16:21,781 - INFO - joeynmt.training - Epoch   6, Step:    14100, Batch Loss:     2.410871, Tokens per Sec:     7610, Lr: 0.000300\n",
      "2021-07-26 10:16:50,293 - INFO - joeynmt.training - Epoch   6, Step:    14200, Batch Loss:     2.671276, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-07-26 10:17:18,878 - INFO - joeynmt.training - Epoch   6, Step:    14300, Batch Loss:     2.464463, Tokens per Sec:     7324, Lr: 0.000300\n",
      "2021-07-26 10:17:47,832 - INFO - joeynmt.training - Epoch   6, Step:    14400, Batch Loss:     2.609931, Tokens per Sec:     7528, Lr: 0.000300\n",
      "2021-07-26 10:18:16,691 - INFO - joeynmt.training - Epoch   6, Step:    14500, Batch Loss:     2.804386, Tokens per Sec:     7489, Lr: 0.000300\n",
      "2021-07-26 10:18:45,369 - INFO - joeynmt.training - Epoch   6, Step:    14600, Batch Loss:     2.418613, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-07-26 10:19:14,045 - INFO - joeynmt.training - Epoch   6, Step:    14700, Batch Loss:     2.485555, Tokens per Sec:     7435, Lr: 0.000300\n",
      "2021-07-26 10:19:43,002 - INFO - joeynmt.training - Epoch   6, Step:    14800, Batch Loss:     2.805578, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-07-26 10:20:11,677 - INFO - joeynmt.training - Epoch   6, Step:    14900, Batch Loss:     2.727644, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-07-26 10:20:40,458 - INFO - joeynmt.training - Epoch   6, Step:    15000, Batch Loss:     2.339522, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-07-26 10:21:32,786 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 10:21:32,787 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 10:21:32,787 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 10:21:33,090 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 10:21:33,090 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 10:21:33,934 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 10:21:33,936 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 10:21:33,936 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 10:21:33,936 - INFO - joeynmt.training - \tHypothesis: When I have been able to make good good and to be saved , he is determined to be patient , what is pleased to God . ”\n",
      "2021-07-26 10:21:33,937 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 10:21:33,937 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 10:21:33,938 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 10:21:33,938 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt voice from heaven , saying : “ This is my Son , my beloved Son . ”\n",
      "2021-07-26 10:21:33,938 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 10:21:33,939 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 10:21:33,939 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 10:21:33,939 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus to his prayers ?\n",
      "2021-07-26 10:21:33,939 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 10:21:33,940 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 10:21:33,940 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 10:21:33,941 - INFO - joeynmt.training - \tHypothesis: But Abigai had to be able to give his family .\n",
      "2021-07-26 10:21:33,941 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    15000: bleu:  17.65, loss: 59963.2461, ppl:   9.5696, duration: 53.4826s\n",
      "2021-07-26 10:22:02,720 - INFO - joeynmt.training - Epoch   6, Step:    15100, Batch Loss:     2.598195, Tokens per Sec:     7538, Lr: 0.000300\n",
      "2021-07-26 10:22:31,751 - INFO - joeynmt.training - Epoch   6, Step:    15200, Batch Loss:     2.562017, Tokens per Sec:     7576, Lr: 0.000300\n",
      "2021-07-26 10:23:00,487 - INFO - joeynmt.training - Epoch   6, Step:    15300, Batch Loss:     2.626724, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-07-26 10:23:29,064 - INFO - joeynmt.training - Epoch   6, Step:    15400, Batch Loss:     2.358276, Tokens per Sec:     7632, Lr: 0.000300\n",
      "2021-07-26 10:23:57,837 - INFO - joeynmt.training - Epoch   6, Step:    15500, Batch Loss:     2.476883, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-26 10:24:26,405 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     2.771842, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-07-26 10:24:55,052 - INFO - joeynmt.training - Epoch   6, Step:    15700, Batch Loss:     2.438410, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-07-26 10:25:23,595 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     2.460068, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-07-26 10:25:52,141 - INFO - joeynmt.training - Epoch   6, Step:    15900, Batch Loss:     2.766406, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-07-26 10:26:20,806 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     2.357742, Tokens per Sec:     7336, Lr: 0.000300\n",
      "2021-07-26 10:26:49,374 - INFO - joeynmt.training - Epoch   6, Step:    16100, Batch Loss:     2.038245, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-07-26 10:27:18,221 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     2.641125, Tokens per Sec:     7502, Lr: 0.000300\n",
      "2021-07-26 10:27:47,171 - INFO - joeynmt.training - Epoch   6, Step:    16300, Batch Loss:     2.485439, Tokens per Sec:     7462, Lr: 0.000300\n",
      "2021-07-26 10:28:15,629 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     2.612849, Tokens per Sec:     7356, Lr: 0.000300\n",
      "2021-07-26 10:28:44,384 - INFO - joeynmt.training - Epoch   6, Step:    16500, Batch Loss:     2.275110, Tokens per Sec:     7606, Lr: 0.000300\n",
      "2021-07-26 10:29:13,091 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     2.504718, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-07-26 10:29:32,600 - INFO - joeynmt.training - Epoch   6: total training loss 6889.89\n",
      "2021-07-26 10:29:32,601 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-26 10:29:41,722 - INFO - joeynmt.training - Epoch   7, Step:    16700, Batch Loss:     1.761453, Tokens per Sec:     6998, Lr: 0.000300\n",
      "2021-07-26 10:30:10,709 - INFO - joeynmt.training - Epoch   7, Step:    16800, Batch Loss:     2.604397, Tokens per Sec:     7568, Lr: 0.000300\n",
      "2021-07-26 10:30:39,267 - INFO - joeynmt.training - Epoch   7, Step:    16900, Batch Loss:     2.418684, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-07-26 10:31:08,004 - INFO - joeynmt.training - Epoch   7, Step:    17000, Batch Loss:     2.542174, Tokens per Sec:     7435, Lr: 0.000300\n",
      "2021-07-26 10:31:36,626 - INFO - joeynmt.training - Epoch   7, Step:    17100, Batch Loss:     2.440198, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-07-26 10:32:05,572 - INFO - joeynmt.training - Epoch   7, Step:    17200, Batch Loss:     2.435988, Tokens per Sec:     7622, Lr: 0.000300\n",
      "2021-07-26 10:32:34,376 - INFO - joeynmt.training - Epoch   7, Step:    17300, Batch Loss:     2.616367, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-07-26 10:33:02,928 - INFO - joeynmt.training - Epoch   7, Step:    17400, Batch Loss:     2.487426, Tokens per Sec:     7361, Lr: 0.000300\n",
      "2021-07-26 10:33:31,958 - INFO - joeynmt.training - Epoch   7, Step:    17500, Batch Loss:     2.545205, Tokens per Sec:     7513, Lr: 0.000300\n",
      "2021-07-26 10:34:00,592 - INFO - joeynmt.training - Epoch   7, Step:    17600, Batch Loss:     2.414364, Tokens per Sec:     7459, Lr: 0.000300\n",
      "2021-07-26 10:34:29,260 - INFO - joeynmt.training - Epoch   7, Step:    17700, Batch Loss:     2.622598, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-07-26 10:34:58,026 - INFO - joeynmt.training - Epoch   7, Step:    17800, Batch Loss:     2.352153, Tokens per Sec:     7555, Lr: 0.000300\n",
      "2021-07-26 10:35:26,554 - INFO - joeynmt.training - Epoch   7, Step:    17900, Batch Loss:     2.362286, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 10:35:54,890 - INFO - joeynmt.training - Epoch   7, Step:    18000, Batch Loss:     2.557360, Tokens per Sec:     7368, Lr: 0.000300\n",
      "2021-07-26 10:36:37,602 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 10:36:37,602 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 10:36:37,603 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 10:36:37,893 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 10:36:37,894 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 10:36:38,714 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 10:36:38,715 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 10:36:38,715 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 10:36:38,716 - INFO - joeynmt.training - \tHypothesis: [ I ] have hated you with good and suffer when you are patient , that is God’s approval . ”\n",
      "2021-07-26 10:36:38,716 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 10:36:38,717 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 10:36:38,717 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 10:36:38,717 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ You are my Son , my beloved Son . ”\n",
      "2021-07-26 10:36:38,717 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 10:36:38,718 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 10:36:38,718 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 10:36:38,718 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer that ?\n",
      "2021-07-26 10:36:38,719 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 10:36:38,719 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 10:36:38,720 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 10:36:38,720 - INFO - joeynmt.training - \tHypothesis: But Abigail had to be able to save his family .\n",
      "2021-07-26 10:36:38,720 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    18000: bleu:  18.89, loss: 57535.5312, ppl:   8.7333, duration: 43.8299s\n",
      "2021-07-26 10:37:07,686 - INFO - joeynmt.training - Epoch   7, Step:    18100, Batch Loss:     2.403290, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-07-26 10:37:36,501 - INFO - joeynmt.training - Epoch   7, Step:    18200, Batch Loss:     1.895131, Tokens per Sec:     7536, Lr: 0.000300\n",
      "2021-07-26 10:38:05,047 - INFO - joeynmt.training - Epoch   7, Step:    18300, Batch Loss:     2.258241, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-07-26 10:38:34,085 - INFO - joeynmt.training - Epoch   7, Step:    18400, Batch Loss:     2.406015, Tokens per Sec:     7622, Lr: 0.000300\n",
      "2021-07-26 10:39:02,709 - INFO - joeynmt.training - Epoch   7, Step:    18500, Batch Loss:     2.446435, Tokens per Sec:     7340, Lr: 0.000300\n",
      "2021-07-26 10:39:31,682 - INFO - joeynmt.training - Epoch   7, Step:    18600, Batch Loss:     2.409521, Tokens per Sec:     7418, Lr: 0.000300\n",
      "2021-07-26 10:40:00,881 - INFO - joeynmt.training - Epoch   7, Step:    18700, Batch Loss:     2.507662, Tokens per Sec:     7397, Lr: 0.000300\n",
      "2021-07-26 10:40:29,761 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     2.547053, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-07-26 10:40:58,256 - INFO - joeynmt.training - Epoch   7, Step:    18900, Batch Loss:     2.490245, Tokens per Sec:     7556, Lr: 0.000300\n",
      "2021-07-26 10:41:27,071 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     1.788163, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-07-26 10:41:56,018 - INFO - joeynmt.training - Epoch   7, Step:    19100, Batch Loss:     2.388378, Tokens per Sec:     7592, Lr: 0.000300\n",
      "2021-07-26 10:42:25,261 - INFO - joeynmt.training - Epoch   7, Step:    19200, Batch Loss:     2.193986, Tokens per Sec:     7524, Lr: 0.000300\n",
      "2021-07-26 10:42:53,862 - INFO - joeynmt.training - Epoch   7, Step:    19300, Batch Loss:     2.537448, Tokens per Sec:     7435, Lr: 0.000300\n",
      "2021-07-26 10:43:22,536 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     3.145289, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-07-26 10:43:36,742 - INFO - joeynmt.training - Epoch   7: total training loss 6654.64\n",
      "2021-07-26 10:43:36,743 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-26 10:43:51,741 - INFO - joeynmt.training - Epoch   8, Step:    19500, Batch Loss:     2.343517, Tokens per Sec:     7189, Lr: 0.000300\n",
      "2021-07-26 10:44:20,482 - INFO - joeynmt.training - Epoch   8, Step:    19600, Batch Loss:     2.322498, Tokens per Sec:     7584, Lr: 0.000300\n",
      "2021-07-26 10:44:49,274 - INFO - joeynmt.training - Epoch   8, Step:    19700, Batch Loss:     2.042448, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-07-26 10:45:18,309 - INFO - joeynmt.training - Epoch   8, Step:    19800, Batch Loss:     2.040627, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-07-26 10:45:47,178 - INFO - joeynmt.training - Epoch   8, Step:    19900, Batch Loss:     2.010125, Tokens per Sec:     7572, Lr: 0.000300\n",
      "2021-07-26 10:46:16,113 - INFO - joeynmt.training - Epoch   8, Step:    20000, Batch Loss:     2.432374, Tokens per Sec:     7527, Lr: 0.000300\n",
      "2021-07-26 10:46:45,072 - INFO - joeynmt.training - Epoch   8, Step:    20100, Batch Loss:     2.425000, Tokens per Sec:     7489, Lr: 0.000300\n",
      "2021-07-26 10:47:13,946 - INFO - joeynmt.training - Epoch   8, Step:    20200, Batch Loss:     2.435176, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-07-26 10:47:42,655 - INFO - joeynmt.training - Epoch   8, Step:    20300, Batch Loss:     2.112259, Tokens per Sec:     7449, Lr: 0.000300\n",
      "2021-07-26 10:48:11,274 - INFO - joeynmt.training - Epoch   8, Step:    20400, Batch Loss:     2.397408, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-07-26 10:48:40,346 - INFO - joeynmt.training - Epoch   8, Step:    20500, Batch Loss:     2.332777, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-07-26 10:49:09,121 - INFO - joeynmt.training - Epoch   8, Step:    20600, Batch Loss:     2.324663, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-07-26 10:49:37,782 - INFO - joeynmt.training - Epoch   8, Step:    20700, Batch Loss:     2.410271, Tokens per Sec:     7349, Lr: 0.000300\n",
      "2021-07-26 10:50:06,416 - INFO - joeynmt.training - Epoch   8, Step:    20800, Batch Loss:     2.425361, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-07-26 10:50:35,713 - INFO - joeynmt.training - Epoch   8, Step:    20900, Batch Loss:     2.101175, Tokens per Sec:     7594, Lr: 0.000300\n",
      "2021-07-26 10:51:04,478 - INFO - joeynmt.training - Epoch   8, Step:    21000, Batch Loss:     2.287614, Tokens per Sec:     7610, Lr: 0.000300\n",
      "2021-07-26 10:51:53,830 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 10:51:53,831 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 10:51:53,831 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 10:51:54,126 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 10:51:54,127 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 10:51:54,971 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 10:51:54,972 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 10:51:54,972 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 10:51:54,972 - INFO - joeynmt.training - \tHypothesis: [ I ] heet his good and bad , and he will suffer when he endured , that is what is pleasing to God . ”\n",
      "2021-07-26 10:51:54,972 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 10:51:54,973 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 10:51:54,974 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 10:51:54,974 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son . ”\n",
      "2021-07-26 10:51:54,974 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 10:51:54,975 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 10:51:54,975 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 10:51:54,975 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
      "2021-07-26 10:51:54,976 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 10:51:54,976 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 10:51:54,977 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 10:51:54,977 - INFO - joeynmt.training - \tHypothesis: But Abigail did not have to cope with his family .\n",
      "2021-07-26 10:51:54,977 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    21000: bleu:  19.65, loss: 55978.5742, ppl:   8.2359, duration: 50.4987s\n",
      "2021-07-26 10:52:24,141 - INFO - joeynmt.training - Epoch   8, Step:    21100, Batch Loss:     2.356542, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-07-26 10:52:52,689 - INFO - joeynmt.training - Epoch   8, Step:    21200, Batch Loss:     2.333567, Tokens per Sec:     7230, Lr: 0.000300\n",
      "2021-07-26 10:53:21,369 - INFO - joeynmt.training - Epoch   8, Step:    21300, Batch Loss:     2.122726, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-07-26 10:53:50,361 - INFO - joeynmt.training - Epoch   8, Step:    21400, Batch Loss:     2.338438, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-07-26 10:54:19,164 - INFO - joeynmt.training - Epoch   8, Step:    21500, Batch Loss:     2.265886, Tokens per Sec:     7385, Lr: 0.000300\n",
      "2021-07-26 10:54:47,939 - INFO - joeynmt.training - Epoch   8, Step:    21600, Batch Loss:     2.099747, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-07-26 10:55:17,093 - INFO - joeynmt.training - Epoch   8, Step:    21700, Batch Loss:     2.205622, Tokens per Sec:     7527, Lr: 0.000300\n",
      "2021-07-26 10:55:46,091 - INFO - joeynmt.training - Epoch   8, Step:    21800, Batch Loss:     2.347333, Tokens per Sec:     7546, Lr: 0.000300\n",
      "2021-07-26 10:56:14,915 - INFO - joeynmt.training - Epoch   8, Step:    21900, Batch Loss:     2.304391, Tokens per Sec:     7433, Lr: 0.000300\n",
      "2021-07-26 10:56:43,764 - INFO - joeynmt.training - Epoch   8, Step:    22000, Batch Loss:     2.250231, Tokens per Sec:     7337, Lr: 0.000300\n",
      "2021-07-26 10:57:12,655 - INFO - joeynmt.training - Epoch   8, Step:    22100, Batch Loss:     2.341195, Tokens per Sec:     7616, Lr: 0.000300\n",
      "2021-07-26 10:57:41,375 - INFO - joeynmt.training - Epoch   8, Step:    22200, Batch Loss:     1.927120, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-07-26 10:57:47,878 - INFO - joeynmt.training - Epoch   8: total training loss 6463.60\n",
      "2021-07-26 10:57:47,879 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-26 10:58:10,373 - INFO - joeynmt.training - Epoch   9, Step:    22300, Batch Loss:     2.206626, Tokens per Sec:     7230, Lr: 0.000300\n",
      "2021-07-26 10:58:39,273 - INFO - joeynmt.training - Epoch   9, Step:    22400, Batch Loss:     2.037513, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-07-26 10:59:08,144 - INFO - joeynmt.training - Epoch   9, Step:    22500, Batch Loss:     2.089836, Tokens per Sec:     7467, Lr: 0.000300\n",
      "2021-07-26 10:59:37,206 - INFO - joeynmt.training - Epoch   9, Step:    22600, Batch Loss:     2.348567, Tokens per Sec:     7479, Lr: 0.000300\n",
      "2021-07-26 11:00:05,919 - INFO - joeynmt.training - Epoch   9, Step:    22700, Batch Loss:     2.132382, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-07-26 11:00:34,983 - INFO - joeynmt.training - Epoch   9, Step:    22800, Batch Loss:     2.324955, Tokens per Sec:     7568, Lr: 0.000300\n",
      "2021-07-26 11:01:03,836 - INFO - joeynmt.training - Epoch   9, Step:    22900, Batch Loss:     2.354295, Tokens per Sec:     7470, Lr: 0.000300\n",
      "2021-07-26 11:01:32,443 - INFO - joeynmt.training - Epoch   9, Step:    23000, Batch Loss:     2.485590, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 11:02:01,274 - INFO - joeynmt.training - Epoch   9, Step:    23100, Batch Loss:     2.145563, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-07-26 11:02:30,302 - INFO - joeynmt.training - Epoch   9, Step:    23200, Batch Loss:     2.199680, Tokens per Sec:     7555, Lr: 0.000300\n",
      "2021-07-26 11:02:58,781 - INFO - joeynmt.training - Epoch   9, Step:    23300, Batch Loss:     2.165966, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-07-26 11:03:27,637 - INFO - joeynmt.training - Epoch   9, Step:    23400, Batch Loss:     2.134458, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-07-26 11:03:56,247 - INFO - joeynmt.training - Epoch   9, Step:    23500, Batch Loss:     2.257554, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-26 11:04:24,988 - INFO - joeynmt.training - Epoch   9, Step:    23600, Batch Loss:     2.479053, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-07-26 11:04:53,743 - INFO - joeynmt.training - Epoch   9, Step:    23700, Batch Loss:     2.564270, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-07-26 11:05:22,659 - INFO - joeynmt.training - Epoch   9, Step:    23800, Batch Loss:     2.517916, Tokens per Sec:     7533, Lr: 0.000300\n",
      "2021-07-26 11:05:51,430 - INFO - joeynmt.training - Epoch   9, Step:    23900, Batch Loss:     2.240697, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-07-26 11:06:20,211 - INFO - joeynmt.training - Epoch   9, Step:    24000, Batch Loss:     2.442295, Tokens per Sec:     7515, Lr: 0.000300\n",
      "2021-07-26 11:07:10,583 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 11:07:10,583 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 11:07:10,584 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 11:07:10,892 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 11:07:10,893 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 11:07:11,764 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 11:07:11,765 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 11:07:11,765 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 11:07:11,767 - INFO - joeynmt.training - \tHypothesis: [ I ] gave him a good and the sufficient death when you endure , what is pleased to God . ”\n",
      "2021-07-26 11:07:11,767 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 11:07:11,767 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 11:07:11,768 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 11:07:11,768 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ This is my Son , my beloved Son . ”\n",
      "2021-07-26 11:07:11,768 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 11:07:11,769 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 11:07:11,769 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 11:07:11,769 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed for ?\n",
      "2021-07-26 11:07:11,770 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 11:07:11,770 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 11:07:11,770 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 11:07:11,771 - INFO - joeynmt.training - \tHypothesis: But Abigail had to cope with his family .\n",
      "2021-07-26 11:07:11,771 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    24000: bleu:  20.36, loss: 54671.2148, ppl:   7.8401, duration: 51.5593s\n",
      "2021-07-26 11:07:40,797 - INFO - joeynmt.training - Epoch   9, Step:    24100, Batch Loss:     2.212999, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 11:08:09,688 - INFO - joeynmt.training - Epoch   9, Step:    24200, Batch Loss:     2.312100, Tokens per Sec:     7580, Lr: 0.000300\n",
      "2021-07-26 11:08:38,569 - INFO - joeynmt.training - Epoch   9, Step:    24300, Batch Loss:     2.167359, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-07-26 11:09:07,169 - INFO - joeynmt.training - Epoch   9, Step:    24400, Batch Loss:     2.411500, Tokens per Sec:     7430, Lr: 0.000300\n",
      "2021-07-26 11:09:36,135 - INFO - joeynmt.training - Epoch   9, Step:    24500, Batch Loss:     2.207213, Tokens per Sec:     7577, Lr: 0.000300\n",
      "2021-07-26 11:10:04,534 - INFO - joeynmt.training - Epoch   9, Step:    24600, Batch Loss:     2.304932, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-07-26 11:10:33,109 - INFO - joeynmt.training - Epoch   9, Step:    24700, Batch Loss:     2.241929, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-07-26 11:11:02,305 - INFO - joeynmt.training - Epoch   9, Step:    24800, Batch Loss:     2.551213, Tokens per Sec:     7640, Lr: 0.000300\n",
      "2021-07-26 11:11:30,797 - INFO - joeynmt.training - Epoch   9, Step:    24900, Batch Loss:     2.198394, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-07-26 11:11:59,427 - INFO - joeynmt.training - Epoch   9: total training loss 6316.25\n",
      "2021-07-26 11:11:59,428 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-26 11:12:00,078 - INFO - joeynmt.training - Epoch  10, Step:    25000, Batch Loss:     2.187106, Tokens per Sec:     2609, Lr: 0.000300\n",
      "2021-07-26 11:12:28,393 - INFO - joeynmt.training - Epoch  10, Step:    25100, Batch Loss:     1.641145, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-07-26 11:12:57,387 - INFO - joeynmt.training - Epoch  10, Step:    25200, Batch Loss:     2.272965, Tokens per Sec:     7458, Lr: 0.000300\n",
      "2021-07-26 11:13:26,040 - INFO - joeynmt.training - Epoch  10, Step:    25300, Batch Loss:     2.174825, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-07-26 11:13:54,972 - INFO - joeynmt.training - Epoch  10, Step:    25400, Batch Loss:     2.370770, Tokens per Sec:     7564, Lr: 0.000300\n",
      "2021-07-26 11:14:23,478 - INFO - joeynmt.training - Epoch  10, Step:    25500, Batch Loss:     2.321938, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-07-26 11:14:52,312 - INFO - joeynmt.training - Epoch  10, Step:    25600, Batch Loss:     2.291353, Tokens per Sec:     7593, Lr: 0.000300\n",
      "2021-07-26 11:15:21,006 - INFO - joeynmt.training - Epoch  10, Step:    25700, Batch Loss:     2.133200, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-07-26 11:15:49,506 - INFO - joeynmt.training - Epoch  10, Step:    25800, Batch Loss:     1.866095, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-07-26 11:16:18,252 - INFO - joeynmt.training - Epoch  10, Step:    25900, Batch Loss:     2.014867, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-07-26 11:16:46,996 - INFO - joeynmt.training - Epoch  10, Step:    26000, Batch Loss:     2.291138, Tokens per Sec:     7578, Lr: 0.000300\n",
      "2021-07-26 11:17:15,676 - INFO - joeynmt.training - Epoch  10, Step:    26100, Batch Loss:     2.345072, Tokens per Sec:     7534, Lr: 0.000300\n",
      "2021-07-26 11:17:44,318 - INFO - joeynmt.training - Epoch  10, Step:    26200, Batch Loss:     2.381633, Tokens per Sec:     7560, Lr: 0.000300\n",
      "2021-07-26 11:18:12,825 - INFO - joeynmt.training - Epoch  10, Step:    26300, Batch Loss:     2.598255, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-07-26 11:18:41,651 - INFO - joeynmt.training - Epoch  10, Step:    26400, Batch Loss:     2.067266, Tokens per Sec:     7550, Lr: 0.000300\n",
      "2021-07-26 11:19:10,317 - INFO - joeynmt.training - Epoch  10, Step:    26500, Batch Loss:     2.118950, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-07-26 11:19:39,212 - INFO - joeynmt.training - Epoch  10, Step:    26600, Batch Loss:     2.469600, Tokens per Sec:     7558, Lr: 0.000300\n",
      "2021-07-26 11:20:08,100 - INFO - joeynmt.training - Epoch  10, Step:    26700, Batch Loss:     2.386669, Tokens per Sec:     7490, Lr: 0.000300\n",
      "2021-07-26 11:20:36,780 - INFO - joeynmt.training - Epoch  10, Step:    26800, Batch Loss:     2.281086, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 11:21:05,480 - INFO - joeynmt.training - Epoch  10, Step:    26900, Batch Loss:     2.239864, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-07-26 11:21:34,442 - INFO - joeynmt.training - Epoch  10, Step:    27000, Batch Loss:     2.312567, Tokens per Sec:     7568, Lr: 0.000300\n",
      "2021-07-26 11:22:12,029 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 11:22:12,030 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 11:22:12,030 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 11:22:12,318 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 11:22:12,318 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 11:22:13,168 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 11:22:13,170 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 11:22:13,170 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 11:22:13,170 - INFO - joeynmt.training - \tHypothesis: [ I ] hates what is good and how to deal with the suffering you endurance , that is what is acceptable to God . ”\n",
      "2021-07-26 11:22:13,170 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 11:22:13,171 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 11:22:13,171 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 11:22:13,172 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son . ”\n",
      "2021-07-26 11:22:13,172 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 11:22:13,173 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 11:22:13,173 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 11:22:13,174 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-26 11:22:13,174 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 11:22:13,174 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 11:22:13,175 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 11:22:13,175 - INFO - joeynmt.training - \tHypothesis: But Abigail had what he had to cope with his family .\n",
      "2021-07-26 11:22:13,175 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    27000: bleu:  21.12, loss: 53323.7656, ppl:   7.4522, duration: 38.7327s\n",
      "2021-07-26 11:22:41,989 - INFO - joeynmt.training - Epoch  10, Step:    27100, Batch Loss:     1.532561, Tokens per Sec:     7315, Lr: 0.000300\n",
      "2021-07-26 11:23:10,412 - INFO - joeynmt.training - Epoch  10, Step:    27200, Batch Loss:     2.388293, Tokens per Sec:     7448, Lr: 0.000300\n",
      "2021-07-26 11:23:39,284 - INFO - joeynmt.training - Epoch  10, Step:    27300, Batch Loss:     2.278142, Tokens per Sec:     7645, Lr: 0.000300\n",
      "2021-07-26 11:24:08,406 - INFO - joeynmt.training - Epoch  10, Step:    27400, Batch Loss:     2.071080, Tokens per Sec:     7592, Lr: 0.000300\n",
      "2021-07-26 11:24:37,115 - INFO - joeynmt.training - Epoch  10, Step:    27500, Batch Loss:     2.092319, Tokens per Sec:     7498, Lr: 0.000300\n",
      "2021-07-26 11:25:05,641 - INFO - joeynmt.training - Epoch  10, Step:    27600, Batch Loss:     2.240831, Tokens per Sec:     7334, Lr: 0.000300\n",
      "2021-07-26 11:25:34,120 - INFO - joeynmt.training - Epoch  10, Step:    27700, Batch Loss:     2.557180, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-07-26 11:25:58,141 - INFO - joeynmt.training - Epoch  10: total training loss 6205.50\n",
      "2021-07-26 11:25:58,142 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-26 11:26:03,026 - INFO - joeynmt.training - Epoch  11, Step:    27800, Batch Loss:     2.290077, Tokens per Sec:     6574, Lr: 0.000300\n",
      "2021-07-26 11:26:31,856 - INFO - joeynmt.training - Epoch  11, Step:    27900, Batch Loss:     2.379818, Tokens per Sec:     7586, Lr: 0.000300\n",
      "2021-07-26 11:27:00,387 - INFO - joeynmt.training - Epoch  11, Step:    28000, Batch Loss:     2.303751, Tokens per Sec:     7501, Lr: 0.000300\n",
      "2021-07-26 11:27:28,566 - INFO - joeynmt.training - Epoch  11, Step:    28100, Batch Loss:     2.354140, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-07-26 11:27:57,580 - INFO - joeynmt.training - Epoch  11, Step:    28200, Batch Loss:     2.282348, Tokens per Sec:     7530, Lr: 0.000300\n",
      "2021-07-26 11:28:26,019 - INFO - joeynmt.training - Epoch  11, Step:    28300, Batch Loss:     2.214448, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-26 11:28:54,563 - INFO - joeynmt.training - Epoch  11, Step:    28400, Batch Loss:     2.181980, Tokens per Sec:     7423, Lr: 0.000300\n",
      "2021-07-26 11:29:23,362 - INFO - joeynmt.training - Epoch  11, Step:    28500, Batch Loss:     2.066350, Tokens per Sec:     7590, Lr: 0.000300\n",
      "2021-07-26 11:29:52,068 - INFO - joeynmt.training - Epoch  11, Step:    28600, Batch Loss:     1.653663, Tokens per Sec:     7477, Lr: 0.000300\n",
      "2021-07-26 11:30:20,797 - INFO - joeynmt.training - Epoch  11, Step:    28700, Batch Loss:     2.341865, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-07-26 11:30:49,513 - INFO - joeynmt.training - Epoch  11, Step:    28800, Batch Loss:     2.040517, Tokens per Sec:     7545, Lr: 0.000300\n",
      "2021-07-26 11:31:18,392 - INFO - joeynmt.training - Epoch  11, Step:    28900, Batch Loss:     2.623536, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-07-26 11:31:47,282 - INFO - joeynmt.training - Epoch  11, Step:    29000, Batch Loss:     2.435895, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-26 11:32:16,123 - INFO - joeynmt.training - Epoch  11, Step:    29100, Batch Loss:     2.048183, Tokens per Sec:     7602, Lr: 0.000300\n",
      "2021-07-26 11:32:44,892 - INFO - joeynmt.training - Epoch  11, Step:    29200, Batch Loss:     2.494687, Tokens per Sec:     7609, Lr: 0.000300\n",
      "2021-07-26 11:33:13,324 - INFO - joeynmt.training - Epoch  11, Step:    29300, Batch Loss:     2.227759, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-07-26 11:33:42,335 - INFO - joeynmt.training - Epoch  11, Step:    29400, Batch Loss:     2.178921, Tokens per Sec:     7523, Lr: 0.000300\n",
      "2021-07-26 11:34:11,013 - INFO - joeynmt.training - Epoch  11, Step:    29500, Batch Loss:     2.225682, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-07-26 11:34:39,841 - INFO - joeynmt.training - Epoch  11, Step:    29600, Batch Loss:     2.086894, Tokens per Sec:     7661, Lr: 0.000300\n",
      "2021-07-26 11:35:08,551 - INFO - joeynmt.training - Epoch  11, Step:    29700, Batch Loss:     2.186594, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-07-26 11:35:36,974 - INFO - joeynmt.training - Epoch  11, Step:    29800, Batch Loss:     2.050544, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-26 11:36:05,493 - INFO - joeynmt.training - Epoch  11, Step:    29900, Batch Loss:     2.023152, Tokens per Sec:     7544, Lr: 0.000300\n",
      "2021-07-26 11:36:33,869 - INFO - joeynmt.training - Epoch  11, Step:    30000, Batch Loss:     2.183887, Tokens per Sec:     7346, Lr: 0.000300\n",
      "2021-07-26 11:37:14,656 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 11:37:14,657 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 11:37:14,657 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 11:37:14,949 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 11:37:14,950 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 11:37:15,824 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 11:37:15,825 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 11:37:15,825 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 11:37:15,826 - INFO - joeynmt.training - \tHypothesis: When you hate you to do good and to suffer when you endure , what is pleasing to God . ”\n",
      "2021-07-26 11:37:15,826 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 11:37:15,827 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 11:37:15,827 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 11:37:15,827 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son . ”\n",
      "2021-07-26 11:37:15,827 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 11:37:15,828 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 11:37:15,828 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 11:37:15,828 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
      "2021-07-26 11:37:15,829 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 11:37:15,830 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 11:37:15,831 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 11:37:15,831 - INFO - joeynmt.training - \tHypothesis: But Abigail had to cope with his family .\n",
      "2021-07-26 11:37:15,831 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    30000: bleu:  22.08, loss: 52226.2969, ppl:   7.1504, duration: 41.9615s\n",
      "2021-07-26 11:37:44,490 - INFO - joeynmt.training - Epoch  11, Step:    30100, Batch Loss:     2.055101, Tokens per Sec:     7433, Lr: 0.000300\n",
      "2021-07-26 11:38:12,799 - INFO - joeynmt.training - Epoch  11, Step:    30200, Batch Loss:     2.302558, Tokens per Sec:     7381, Lr: 0.000300\n",
      "2021-07-26 11:38:41,070 - INFO - joeynmt.training - Epoch  11, Step:    30300, Batch Loss:     2.162619, Tokens per Sec:     7374, Lr: 0.000300\n",
      "2021-07-26 11:39:09,983 - INFO - joeynmt.training - Epoch  11, Step:    30400, Batch Loss:     1.837585, Tokens per Sec:     7545, Lr: 0.000300\n",
      "2021-07-26 11:39:38,705 - INFO - joeynmt.training - Epoch  11, Step:    30500, Batch Loss:     1.972793, Tokens per Sec:     7480, Lr: 0.000300\n",
      "2021-07-26 11:39:58,422 - INFO - joeynmt.training - Epoch  11: total training loss 6096.47\n",
      "2021-07-26 11:39:58,422 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-26 11:40:07,749 - INFO - joeynmt.training - Epoch  12, Step:    30600, Batch Loss:     2.486087, Tokens per Sec:     7128, Lr: 0.000300\n",
      "2021-07-26 11:40:36,424 - INFO - joeynmt.training - Epoch  12, Step:    30700, Batch Loss:     2.189083, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-07-26 11:41:05,233 - INFO - joeynmt.training - Epoch  12, Step:    30800, Batch Loss:     2.358224, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-07-26 11:41:33,924 - INFO - joeynmt.training - Epoch  12, Step:    30900, Batch Loss:     1.465980, Tokens per Sec:     7481, Lr: 0.000300\n",
      "2021-07-26 11:42:02,632 - INFO - joeynmt.training - Epoch  12, Step:    31000, Batch Loss:     2.276451, Tokens per Sec:     7570, Lr: 0.000300\n",
      "2021-07-26 11:42:31,080 - INFO - joeynmt.training - Epoch  12, Step:    31100, Batch Loss:     2.174437, Tokens per Sec:     7509, Lr: 0.000300\n",
      "2021-07-26 11:42:59,924 - INFO - joeynmt.training - Epoch  12, Step:    31200, Batch Loss:     2.231035, Tokens per Sec:     7448, Lr: 0.000300\n",
      "2021-07-26 11:43:28,565 - INFO - joeynmt.training - Epoch  12, Step:    31300, Batch Loss:     2.061615, Tokens per Sec:     7431, Lr: 0.000300\n",
      "2021-07-26 11:43:57,433 - INFO - joeynmt.training - Epoch  12, Step:    31400, Batch Loss:     2.295869, Tokens per Sec:     7482, Lr: 0.000300\n",
      "2021-07-26 11:44:26,026 - INFO - joeynmt.training - Epoch  12, Step:    31500, Batch Loss:     2.091147, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-07-26 11:44:54,860 - INFO - joeynmt.training - Epoch  12, Step:    31600, Batch Loss:     1.936735, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-07-26 11:45:23,437 - INFO - joeynmt.training - Epoch  12, Step:    31700, Batch Loss:     2.185218, Tokens per Sec:     7431, Lr: 0.000300\n",
      "2021-07-26 11:45:52,380 - INFO - joeynmt.training - Epoch  12, Step:    31800, Batch Loss:     1.957277, Tokens per Sec:     7557, Lr: 0.000300\n",
      "2021-07-26 11:46:21,221 - INFO - joeynmt.training - Epoch  12, Step:    31900, Batch Loss:     2.209002, Tokens per Sec:     7580, Lr: 0.000300\n",
      "2021-07-26 11:46:50,028 - INFO - joeynmt.training - Epoch  12, Step:    32000, Batch Loss:     2.341983, Tokens per Sec:     7350, Lr: 0.000300\n",
      "2021-07-26 11:47:18,920 - INFO - joeynmt.training - Epoch  12, Step:    32100, Batch Loss:     1.728532, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-07-26 11:47:47,192 - INFO - joeynmt.training - Epoch  12, Step:    32200, Batch Loss:     2.121246, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-07-26 11:48:15,903 - INFO - joeynmt.training - Epoch  12, Step:    32300, Batch Loss:     2.078593, Tokens per Sec:     7654, Lr: 0.000300\n",
      "2021-07-26 11:48:44,608 - INFO - joeynmt.training - Epoch  12, Step:    32400, Batch Loss:     2.280890, Tokens per Sec:     7477, Lr: 0.000300\n",
      "2021-07-26 11:49:13,338 - INFO - joeynmt.training - Epoch  12, Step:    32500, Batch Loss:     2.268495, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-07-26 11:49:42,309 - INFO - joeynmt.training - Epoch  12, Step:    32600, Batch Loss:     1.350316, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-07-26 11:50:10,709 - INFO - joeynmt.training - Epoch  12, Step:    32700, Batch Loss:     2.255740, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-07-26 11:50:39,632 - INFO - joeynmt.training - Epoch  12, Step:    32800, Batch Loss:     2.260751, Tokens per Sec:     7595, Lr: 0.000300\n",
      "2021-07-26 11:51:08,316 - INFO - joeynmt.training - Epoch  12, Step:    32900, Batch Loss:     2.085211, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-07-26 11:51:36,916 - INFO - joeynmt.training - Epoch  12, Step:    33000, Batch Loss:     2.040622, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-07-26 11:52:20,943 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 11:52:20,943 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 11:52:20,943 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 11:52:21,238 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 11:52:21,238 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 11:52:22,116 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 11:52:22,118 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 11:52:22,118 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 11:52:22,118 - INFO - joeynmt.training - \tHypothesis: [ I ] hate what you do good and be tortured when you endure , what is acceptable to God . ”\n",
      "2021-07-26 11:52:22,118 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 11:52:22,119 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 11:52:22,119 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 11:52:22,119 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I have love . ”\n",
      "2021-07-26 11:52:22,120 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 11:52:22,120 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 11:52:22,121 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 11:52:22,121 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-26 11:52:22,121 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 11:52:22,122 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 11:52:22,122 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 11:52:22,122 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 11:52:22,123 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    33000: bleu:  22.61, loss: 51357.4102, ppl:   6.9202, duration: 45.2058s\n",
      "2021-07-26 11:52:51,055 - INFO - joeynmt.training - Epoch  12, Step:    33100, Batch Loss:     2.377635, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-07-26 11:53:19,712 - INFO - joeynmt.training - Epoch  12, Step:    33200, Batch Loss:     2.030387, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-07-26 11:53:48,422 - INFO - joeynmt.training - Epoch  12, Step:    33300, Batch Loss:     2.390424, Tokens per Sec:     7593, Lr: 0.000300\n",
      "2021-07-26 11:54:03,176 - INFO - joeynmt.training - Epoch  12: total training loss 6000.37\n",
      "2021-07-26 11:54:03,177 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-26 11:54:17,387 - INFO - joeynmt.training - Epoch  13, Step:    33400, Batch Loss:     2.113958, Tokens per Sec:     7295, Lr: 0.000300\n",
      "2021-07-26 11:54:46,249 - INFO - joeynmt.training - Epoch  13, Step:    33500, Batch Loss:     2.172502, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-07-26 11:55:15,188 - INFO - joeynmt.training - Epoch  13, Step:    33600, Batch Loss:     2.274713, Tokens per Sec:     7440, Lr: 0.000300\n",
      "2021-07-26 11:55:44,009 - INFO - joeynmt.training - Epoch  13, Step:    33700, Batch Loss:     2.313342, Tokens per Sec:     7471, Lr: 0.000300\n",
      "2021-07-26 11:56:12,793 - INFO - joeynmt.training - Epoch  13, Step:    33800, Batch Loss:     2.095268, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-07-26 11:56:41,871 - INFO - joeynmt.training - Epoch  13, Step:    33900, Batch Loss:     1.403889, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-07-26 11:57:10,637 - INFO - joeynmt.training - Epoch  13, Step:    34000, Batch Loss:     2.257070, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-07-26 11:57:39,704 - INFO - joeynmt.training - Epoch  13, Step:    34100, Batch Loss:     1.904447, Tokens per Sec:     7649, Lr: 0.000300\n",
      "2021-07-26 11:58:08,229 - INFO - joeynmt.training - Epoch  13, Step:    34200, Batch Loss:     2.036323, Tokens per Sec:     7469, Lr: 0.000300\n",
      "2021-07-26 11:58:36,933 - INFO - joeynmt.training - Epoch  13, Step:    34300, Batch Loss:     2.150992, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-07-26 11:59:05,748 - INFO - joeynmt.training - Epoch  13, Step:    34400, Batch Loss:     2.077031, Tokens per Sec:     7521, Lr: 0.000300\n",
      "2021-07-26 11:59:34,756 - INFO - joeynmt.training - Epoch  13, Step:    34500, Batch Loss:     1.983286, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-07-26 12:00:03,583 - INFO - joeynmt.training - Epoch  13, Step:    34600, Batch Loss:     2.242967, Tokens per Sec:     7418, Lr: 0.000300\n",
      "2021-07-26 12:00:32,418 - INFO - joeynmt.training - Epoch  13, Step:    34700, Batch Loss:     2.156404, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-07-26 12:01:00,992 - INFO - joeynmt.training - Epoch  13, Step:    34800, Batch Loss:     2.056391, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-07-26 12:01:29,944 - INFO - joeynmt.training - Epoch  13, Step:    34900, Batch Loss:     2.068444, Tokens per Sec:     7465, Lr: 0.000300\n",
      "2021-07-26 12:01:58,990 - INFO - joeynmt.training - Epoch  13, Step:    35000, Batch Loss:     2.201875, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-26 12:02:27,681 - INFO - joeynmt.training - Epoch  13, Step:    35100, Batch Loss:     2.273809, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 12:02:56,673 - INFO - joeynmt.training - Epoch  13, Step:    35200, Batch Loss:     1.945420, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-07-26 12:03:25,460 - INFO - joeynmt.training - Epoch  13, Step:    35300, Batch Loss:     2.111287, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-07-26 12:03:54,145 - INFO - joeynmt.training - Epoch  13, Step:    35400, Batch Loss:     2.333118, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-07-26 12:04:23,054 - INFO - joeynmt.training - Epoch  13, Step:    35500, Batch Loss:     2.165594, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-07-26 12:04:52,124 - INFO - joeynmt.training - Epoch  13, Step:    35600, Batch Loss:     2.032499, Tokens per Sec:     7585, Lr: 0.000300\n",
      "2021-07-26 12:05:21,069 - INFO - joeynmt.training - Epoch  13, Step:    35700, Batch Loss:     1.579888, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-07-26 12:05:49,632 - INFO - joeynmt.training - Epoch  13, Step:    35800, Batch Loss:     2.137000, Tokens per Sec:     7372, Lr: 0.000300\n",
      "2021-07-26 12:06:18,448 - INFO - joeynmt.training - Epoch  13, Step:    35900, Batch Loss:     2.355359, Tokens per Sec:     7447, Lr: 0.000300\n",
      "2021-07-26 12:06:47,455 - INFO - joeynmt.training - Epoch  13, Step:    36000, Batch Loss:     2.143820, Tokens per Sec:     7512, Lr: 0.000300\n",
      "2021-07-26 12:07:28,970 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 12:07:28,971 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 12:07:28,971 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 12:07:29,266 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 12:07:29,267 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 12:07:30,259 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 12:07:30,260 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 12:07:30,260 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 12:07:30,261 - INFO - joeynmt.training - \tHypothesis: [ I ] hate you with goodness and the suffering you endure , what is acceptable to God . ”\n",
      "2021-07-26 12:07:30,261 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 12:07:30,262 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 12:07:30,262 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 12:07:30,262 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son . ”\n",
      "2021-07-26 12:07:30,262 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 12:07:30,263 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 12:07:30,263 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 12:07:30,264 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-26 12:07:30,264 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 12:07:30,264 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 12:07:30,265 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 12:07:30,265 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 12:07:30,265 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    36000: bleu:  22.44, loss: 51089.7461, ppl:   6.8507, duration: 42.8096s\n",
      "2021-07-26 12:07:59,141 - INFO - joeynmt.training - Epoch  13, Step:    36100, Batch Loss:     2.168601, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-26 12:08:07,026 - INFO - joeynmt.training - Epoch  13: total training loss 5902.03\n",
      "2021-07-26 12:08:07,027 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-26 12:08:28,331 - INFO - joeynmt.training - Epoch  14, Step:    36200, Batch Loss:     2.232477, Tokens per Sec:     7310, Lr: 0.000300\n",
      "2021-07-26 12:08:57,037 - INFO - joeynmt.training - Epoch  14, Step:    36300, Batch Loss:     1.793957, Tokens per Sec:     7527, Lr: 0.000300\n",
      "2021-07-26 12:09:25,617 - INFO - joeynmt.training - Epoch  14, Step:    36400, Batch Loss:     1.765066, Tokens per Sec:     7283, Lr: 0.000300\n",
      "2021-07-26 12:09:54,645 - INFO - joeynmt.training - Epoch  14, Step:    36500, Batch Loss:     2.287955, Tokens per Sec:     7546, Lr: 0.000300\n",
      "2021-07-26 12:10:23,499 - INFO - joeynmt.training - Epoch  14, Step:    36600, Batch Loss:     2.221428, Tokens per Sec:     7500, Lr: 0.000300\n",
      "2021-07-26 12:10:52,539 - INFO - joeynmt.training - Epoch  14, Step:    36700, Batch Loss:     2.159569, Tokens per Sec:     7569, Lr: 0.000300\n",
      "2021-07-26 12:11:21,216 - INFO - joeynmt.training - Epoch  14, Step:    36800, Batch Loss:     2.258425, Tokens per Sec:     7592, Lr: 0.000300\n",
      "2021-07-26 12:11:49,901 - INFO - joeynmt.training - Epoch  14, Step:    36900, Batch Loss:     1.709311, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-07-26 12:12:18,518 - INFO - joeynmt.training - Epoch  14, Step:    37000, Batch Loss:     2.429209, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-07-26 12:12:47,735 - INFO - joeynmt.training - Epoch  14, Step:    37100, Batch Loss:     2.103665, Tokens per Sec:     7666, Lr: 0.000300\n",
      "2021-07-26 12:13:16,740 - INFO - joeynmt.training - Epoch  14, Step:    37200, Batch Loss:     2.333305, Tokens per Sec:     7311, Lr: 0.000300\n",
      "2021-07-26 12:13:45,670 - INFO - joeynmt.training - Epoch  14, Step:    37300, Batch Loss:     2.029353, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-07-26 12:14:14,310 - INFO - joeynmt.training - Epoch  14, Step:    37400, Batch Loss:     2.363436, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-07-26 12:14:43,126 - INFO - joeynmt.training - Epoch  14, Step:    37500, Batch Loss:     1.827143, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-07-26 12:15:11,862 - INFO - joeynmt.training - Epoch  14, Step:    37600, Batch Loss:     2.122773, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-07-26 12:15:40,563 - INFO - joeynmt.training - Epoch  14, Step:    37700, Batch Loss:     2.289938, Tokens per Sec:     7477, Lr: 0.000300\n",
      "2021-07-26 12:16:09,605 - INFO - joeynmt.training - Epoch  14, Step:    37800, Batch Loss:     2.343534, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-07-26 12:16:38,587 - INFO - joeynmt.training - Epoch  14, Step:    37900, Batch Loss:     2.295751, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-07-26 12:17:07,213 - INFO - joeynmt.training - Epoch  14, Step:    38000, Batch Loss:     2.240843, Tokens per Sec:     7366, Lr: 0.000300\n",
      "2021-07-26 12:17:36,115 - INFO - joeynmt.training - Epoch  14, Step:    38100, Batch Loss:     2.161843, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-07-26 12:18:04,984 - INFO - joeynmt.training - Epoch  14, Step:    38200, Batch Loss:     1.594986, Tokens per Sec:     7477, Lr: 0.000300\n",
      "2021-07-26 12:18:33,660 - INFO - joeynmt.training - Epoch  14, Step:    38300, Batch Loss:     1.711588, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-07-26 12:19:02,629 - INFO - joeynmt.training - Epoch  14, Step:    38400, Batch Loss:     2.218065, Tokens per Sec:     7408, Lr: 0.000300\n",
      "2021-07-26 12:19:31,255 - INFO - joeynmt.training - Epoch  14, Step:    38500, Batch Loss:     2.187172, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-07-26 12:20:00,457 - INFO - joeynmt.training - Epoch  14, Step:    38600, Batch Loss:     2.175019, Tokens per Sec:     7631, Lr: 0.000300\n",
      "2021-07-26 12:20:29,457 - INFO - joeynmt.training - Epoch  14, Step:    38700, Batch Loss:     2.123714, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-07-26 12:20:58,306 - INFO - joeynmt.training - Epoch  14, Step:    38800, Batch Loss:     2.155331, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-07-26 12:21:26,794 - INFO - joeynmt.training - Epoch  14, Step:    38900, Batch Loss:     2.078152, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-26 12:21:29,938 - INFO - joeynmt.training - Epoch  14: total training loss 5842.99\n",
      "2021-07-26 12:21:29,939 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-26 12:21:56,022 - INFO - joeynmt.training - Epoch  15, Step:    39000, Batch Loss:     2.149140, Tokens per Sec:     7409, Lr: 0.000300\n",
      "2021-07-26 12:22:37,754 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 12:22:37,754 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 12:22:37,755 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 12:22:38,048 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 12:22:38,050 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 12:22:39,320 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 12:22:39,322 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 12:22:39,322 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 12:22:39,322 - INFO - joeynmt.training - \tHypothesis: [ I ] hate you with good and suffer when you endure , that is what is acceptable to God . ”\n",
      "2021-07-26 12:22:39,323 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 12:22:39,324 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 12:22:39,324 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 12:22:39,324 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I have not felt . ”\n",
      "2021-07-26 12:22:39,324 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 12:22:39,325 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 12:22:39,325 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 12:22:39,325 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-26 12:22:39,326 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 12:22:39,326 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 12:22:39,327 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 12:22:39,327 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 12:22:39,327 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    39000: bleu:  22.84, loss: 50230.0469, ppl:   6.6325, duration: 43.3045s\n",
      "2021-07-26 12:23:08,613 - INFO - joeynmt.training - Epoch  15, Step:    39100, Batch Loss:     2.370478, Tokens per Sec:     7536, Lr: 0.000300\n",
      "2021-07-26 12:23:37,432 - INFO - joeynmt.training - Epoch  15, Step:    39200, Batch Loss:     2.169816, Tokens per Sec:     7470, Lr: 0.000300\n",
      "2021-07-26 12:24:06,162 - INFO - joeynmt.training - Epoch  15, Step:    39300, Batch Loss:     1.867424, Tokens per Sec:     7571, Lr: 0.000300\n",
      "2021-07-26 12:24:35,129 - INFO - joeynmt.training - Epoch  15, Step:    39400, Batch Loss:     1.702603, Tokens per Sec:     7504, Lr: 0.000300\n",
      "2021-07-26 12:25:04,057 - INFO - joeynmt.training - Epoch  15, Step:    39500, Batch Loss:     2.118268, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-07-26 12:25:32,512 - INFO - joeynmt.training - Epoch  15, Step:    39600, Batch Loss:     1.961862, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-07-26 12:26:01,390 - INFO - joeynmt.training - Epoch  15, Step:    39700, Batch Loss:     2.413154, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-07-26 12:26:30,183 - INFO - joeynmt.training - Epoch  15, Step:    39800, Batch Loss:     2.123913, Tokens per Sec:     7316, Lr: 0.000300\n",
      "2021-07-26 12:26:59,236 - INFO - joeynmt.training - Epoch  15, Step:    39900, Batch Loss:     2.144727, Tokens per Sec:     7605, Lr: 0.000300\n",
      "2021-07-26 12:27:28,112 - INFO - joeynmt.training - Epoch  15, Step:    40000, Batch Loss:     2.295805, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-07-26 12:27:56,896 - INFO - joeynmt.training - Epoch  15, Step:    40100, Batch Loss:     2.161407, Tokens per Sec:     7487, Lr: 0.000300\n",
      "2021-07-26 12:28:25,849 - INFO - joeynmt.training - Epoch  15, Step:    40200, Batch Loss:     2.112871, Tokens per Sec:     7536, Lr: 0.000300\n",
      "2021-07-26 12:28:54,503 - INFO - joeynmt.training - Epoch  15, Step:    40300, Batch Loss:     2.060434, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-07-26 12:29:23,239 - INFO - joeynmt.training - Epoch  15, Step:    40400, Batch Loss:     1.857062, Tokens per Sec:     7346, Lr: 0.000300\n",
      "2021-07-26 12:29:51,748 - INFO - joeynmt.training - Epoch  15, Step:    40500, Batch Loss:     2.173730, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-07-26 12:30:20,021 - INFO - joeynmt.training - Epoch  15, Step:    40600, Batch Loss:     2.205156, Tokens per Sec:     7251, Lr: 0.000300\n",
      "2021-07-26 12:30:48,764 - INFO - joeynmt.training - Epoch  15, Step:    40700, Batch Loss:     2.229622, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-07-26 12:31:17,673 - INFO - joeynmt.training - Epoch  15, Step:    40800, Batch Loss:     2.065975, Tokens per Sec:     7582, Lr: 0.000300\n",
      "2021-07-26 12:31:46,058 - INFO - joeynmt.training - Epoch  15, Step:    40900, Batch Loss:     2.057958, Tokens per Sec:     7312, Lr: 0.000300\n",
      "2021-07-26 12:32:14,960 - INFO - joeynmt.training - Epoch  15, Step:    41000, Batch Loss:     2.135720, Tokens per Sec:     7535, Lr: 0.000300\n",
      "2021-07-26 12:32:44,011 - INFO - joeynmt.training - Epoch  15, Step:    41100, Batch Loss:     2.496487, Tokens per Sec:     7583, Lr: 0.000300\n",
      "2021-07-26 12:33:12,760 - INFO - joeynmt.training - Epoch  15, Step:    41200, Batch Loss:     2.171288, Tokens per Sec:     7521, Lr: 0.000300\n",
      "2021-07-26 12:33:41,316 - INFO - joeynmt.training - Epoch  15, Step:    41300, Batch Loss:     1.961317, Tokens per Sec:     7351, Lr: 0.000300\n",
      "2021-07-26 12:34:10,407 - INFO - joeynmt.training - Epoch  15, Step:    41400, Batch Loss:     1.992177, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-07-26 12:34:39,286 - INFO - joeynmt.training - Epoch  15, Step:    41500, Batch Loss:     2.248327, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-26 12:35:08,052 - INFO - joeynmt.training - Epoch  15, Step:    41600, Batch Loss:     1.852574, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-07-26 12:35:33,814 - INFO - joeynmt.training - Epoch  15: total training loss 5770.59\n",
      "2021-07-26 12:35:33,815 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-26 12:35:37,377 - INFO - joeynmt.training - Epoch  16, Step:    41700, Batch Loss:     2.122212, Tokens per Sec:     6705, Lr: 0.000300\n",
      "2021-07-26 12:36:06,363 - INFO - joeynmt.training - Epoch  16, Step:    41800, Batch Loss:     2.150849, Tokens per Sec:     7380, Lr: 0.000300\n",
      "2021-07-26 12:36:34,817 - INFO - joeynmt.training - Epoch  16, Step:    41900, Batch Loss:     2.090874, Tokens per Sec:     7372, Lr: 0.000300\n",
      "2021-07-26 12:37:03,711 - INFO - joeynmt.training - Epoch  16, Step:    42000, Batch Loss:     2.088238, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-07-26 12:37:44,910 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 12:37:44,910 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 12:37:44,910 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 12:37:45,214 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 12:37:45,215 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 12:37:46,486 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 12:37:46,487 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 12:37:46,488 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 12:37:46,488 - INFO - joeynmt.training - \tHypothesis: [ I ] hated you with good and suffering when you endure , that is what is acceptable to God . ”\n",
      "2021-07-26 12:37:46,488 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 12:37:46,489 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 12:37:46,489 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 12:37:46,489 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I have not approved . ”\n",
      "2021-07-26 12:37:46,490 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 12:37:46,490 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 12:37:46,490 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 12:37:46,491 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-26 12:37:46,491 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 12:37:46,491 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 12:37:46,492 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 12:37:46,492 - INFO - joeynmt.training - \tHypothesis: But Abigail had to cope with his family .\n",
      "2021-07-26 12:37:46,492 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    42000: bleu:  23.15, loss: 49693.3789, ppl:   6.4997, duration: 42.7807s\n",
      "2021-07-26 12:38:15,952 - INFO - joeynmt.training - Epoch  16, Step:    42100, Batch Loss:     2.085593, Tokens per Sec:     7498, Lr: 0.000300\n",
      "2021-07-26 12:38:44,903 - INFO - joeynmt.training - Epoch  16, Step:    42200, Batch Loss:     2.204512, Tokens per Sec:     7442, Lr: 0.000300\n",
      "2021-07-26 12:39:13,642 - INFO - joeynmt.training - Epoch  16, Step:    42300, Batch Loss:     2.083750, Tokens per Sec:     7472, Lr: 0.000300\n",
      "2021-07-26 12:39:42,361 - INFO - joeynmt.training - Epoch  16, Step:    42400, Batch Loss:     1.592878, Tokens per Sec:     7501, Lr: 0.000300\n",
      "2021-07-26 12:40:11,278 - INFO - joeynmt.training - Epoch  16, Step:    42500, Batch Loss:     1.929005, Tokens per Sec:     7525, Lr: 0.000300\n",
      "2021-07-26 12:40:40,098 - INFO - joeynmt.training - Epoch  16, Step:    42600, Batch Loss:     1.980670, Tokens per Sec:     7471, Lr: 0.000300\n",
      "2021-07-26 12:41:09,018 - INFO - joeynmt.training - Epoch  16, Step:    42700, Batch Loss:     2.285913, Tokens per Sec:     7552, Lr: 0.000300\n",
      "2021-07-26 12:41:37,601 - INFO - joeynmt.training - Epoch  16, Step:    42800, Batch Loss:     2.078414, Tokens per Sec:     7480, Lr: 0.000300\n",
      "2021-07-26 12:42:06,562 - INFO - joeynmt.training - Epoch  16, Step:    42900, Batch Loss:     2.181463, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-07-26 12:42:35,153 - INFO - joeynmt.training - Epoch  16, Step:    43000, Batch Loss:     2.298463, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-07-26 12:43:03,730 - INFO - joeynmt.training - Epoch  16, Step:    43100, Batch Loss:     2.722935, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-07-26 12:43:32,723 - INFO - joeynmt.training - Epoch  16, Step:    43200, Batch Loss:     2.014916, Tokens per Sec:     7490, Lr: 0.000300\n",
      "2021-07-26 12:44:01,588 - INFO - joeynmt.training - Epoch  16, Step:    43300, Batch Loss:     2.140022, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-07-26 12:44:30,373 - INFO - joeynmt.training - Epoch  16, Step:    43400, Batch Loss:     1.990444, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-07-26 12:44:59,230 - INFO - joeynmt.training - Epoch  16, Step:    43500, Batch Loss:     2.075642, Tokens per Sec:     7476, Lr: 0.000300\n",
      "2021-07-26 12:45:28,185 - INFO - joeynmt.training - Epoch  16, Step:    43600, Batch Loss:     2.282245, Tokens per Sec:     7520, Lr: 0.000300\n",
      "2021-07-26 12:45:56,903 - INFO - joeynmt.training - Epoch  16, Step:    43700, Batch Loss:     1.577462, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-07-26 12:46:25,766 - INFO - joeynmt.training - Epoch  16, Step:    43800, Batch Loss:     1.887846, Tokens per Sec:     7585, Lr: 0.000300\n",
      "2021-07-26 12:46:54,450 - INFO - joeynmt.training - Epoch  16, Step:    43900, Batch Loss:     2.193843, Tokens per Sec:     7497, Lr: 0.000300\n",
      "2021-07-26 12:47:23,315 - INFO - joeynmt.training - Epoch  16, Step:    44000, Batch Loss:     1.885657, Tokens per Sec:     7426, Lr: 0.000300\n",
      "2021-07-26 12:47:52,229 - INFO - joeynmt.training - Epoch  16, Step:    44100, Batch Loss:     1.979505, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-07-26 12:48:21,048 - INFO - joeynmt.training - Epoch  16, Step:    44200, Batch Loss:     1.884426, Tokens per Sec:     7575, Lr: 0.000300\n",
      "2021-07-26 12:48:49,949 - INFO - joeynmt.training - Epoch  16, Step:    44300, Batch Loss:     2.083270, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-07-26 12:49:18,471 - INFO - joeynmt.training - Epoch  16, Step:    44400, Batch Loss:     2.096588, Tokens per Sec:     7470, Lr: 0.000300\n",
      "2021-07-26 12:49:36,765 - INFO - joeynmt.training - Epoch  16: total training loss 5707.23\n",
      "2021-07-26 12:49:36,766 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-26 12:49:47,633 - INFO - joeynmt.training - Epoch  17, Step:    44500, Batch Loss:     2.088279, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-07-26 12:50:16,503 - INFO - joeynmt.training - Epoch  17, Step:    44600, Batch Loss:     2.033863, Tokens per Sec:     7540, Lr: 0.000300\n",
      "2021-07-26 12:50:45,473 - INFO - joeynmt.training - Epoch  17, Step:    44700, Batch Loss:     2.082065, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-07-26 12:51:14,152 - INFO - joeynmt.training - Epoch  17, Step:    44800, Batch Loss:     1.984848, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-07-26 12:51:43,126 - INFO - joeynmt.training - Epoch  17, Step:    44900, Batch Loss:     2.033497, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-07-26 12:52:11,741 - INFO - joeynmt.training - Epoch  17, Step:    45000, Batch Loss:     1.595673, Tokens per Sec:     7377, Lr: 0.000300\n",
      "2021-07-26 12:52:50,873 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 12:52:50,874 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 12:52:50,874 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 12:52:51,169 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 12:52:51,170 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 12:52:52,013 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 12:52:52,015 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 12:52:52,015 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 12:52:52,015 - INFO - joeynmt.training - \tHypothesis: [ I ] hate what you do good and suffering when you endure , that is what is acceptable to God . ”\n",
      "2021-07-26 12:52:52,015 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 12:52:52,016 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 12:52:52,016 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 12:52:52,017 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son . ”\n",
      "2021-07-26 12:52:52,017 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 12:52:52,018 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 12:52:52,018 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 12:52:52,018 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-26 12:52:52,018 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 12:52:52,019 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 12:52:52,019 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 12:52:52,019 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 12:52:52,020 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    45000: bleu:  23.46, loss: 49138.1602, ppl:   6.3652, duration: 40.2784s\n",
      "2021-07-26 12:53:20,883 - INFO - joeynmt.training - Epoch  17, Step:    45100, Batch Loss:     2.126430, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-07-26 12:53:49,810 - INFO - joeynmt.training - Epoch  17, Step:    45200, Batch Loss:     2.256122, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-07-26 12:54:18,625 - INFO - joeynmt.training - Epoch  17, Step:    45300, Batch Loss:     2.162363, Tokens per Sec:     7481, Lr: 0.000300\n",
      "2021-07-26 12:54:47,403 - INFO - joeynmt.training - Epoch  17, Step:    45400, Batch Loss:     1.885909, Tokens per Sec:     7555, Lr: 0.000300\n",
      "2021-07-26 12:55:16,033 - INFO - joeynmt.training - Epoch  17, Step:    45500, Batch Loss:     1.765555, Tokens per Sec:     7501, Lr: 0.000300\n",
      "2021-07-26 12:55:44,830 - INFO - joeynmt.training - Epoch  17, Step:    45600, Batch Loss:     2.066626, Tokens per Sec:     7580, Lr: 0.000300\n",
      "2021-07-26 12:56:13,934 - INFO - joeynmt.training - Epoch  17, Step:    45700, Batch Loss:     2.255318, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-07-26 12:56:42,647 - INFO - joeynmt.training - Epoch  17, Step:    45800, Batch Loss:     2.072179, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-07-26 12:57:11,390 - INFO - joeynmt.training - Epoch  17, Step:    45900, Batch Loss:     2.167685, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-07-26 12:57:40,028 - INFO - joeynmt.training - Epoch  17, Step:    46000, Batch Loss:     2.219167, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-07-26 12:58:08,832 - INFO - joeynmt.training - Epoch  17, Step:    46100, Batch Loss:     2.142012, Tokens per Sec:     7408, Lr: 0.000300\n",
      "2021-07-26 12:58:37,923 - INFO - joeynmt.training - Epoch  17, Step:    46200, Batch Loss:     1.859210, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 12:59:06,820 - INFO - joeynmt.training - Epoch  17, Step:    46300, Batch Loss:     1.698101, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-07-26 12:59:35,924 - INFO - joeynmt.training - Epoch  17, Step:    46400, Batch Loss:     1.845995, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-07-26 13:00:04,752 - INFO - joeynmt.training - Epoch  17, Step:    46500, Batch Loss:     2.182780, Tokens per Sec:     7479, Lr: 0.000300\n",
      "2021-07-26 13:00:33,628 - INFO - joeynmt.training - Epoch  17, Step:    46600, Batch Loss:     2.073927, Tokens per Sec:     7440, Lr: 0.000300\n",
      "2021-07-26 13:01:02,318 - INFO - joeynmt.training - Epoch  17, Step:    46700, Batch Loss:     2.006617, Tokens per Sec:     7528, Lr: 0.000300\n",
      "2021-07-26 13:01:31,026 - INFO - joeynmt.training - Epoch  17, Step:    46800, Batch Loss:     2.383046, Tokens per Sec:     7563, Lr: 0.000300\n",
      "2021-07-26 13:02:00,144 - INFO - joeynmt.training - Epoch  17, Step:    46900, Batch Loss:     2.154267, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-07-26 13:02:28,999 - INFO - joeynmt.training - Epoch  17, Step:    47000, Batch Loss:     1.866927, Tokens per Sec:     7545, Lr: 0.000300\n",
      "2021-07-26 13:02:57,797 - INFO - joeynmt.training - Epoch  17, Step:    47100, Batch Loss:     2.217887, Tokens per Sec:     7471, Lr: 0.000300\n",
      "2021-07-26 13:03:26,469 - INFO - joeynmt.training - Epoch  17, Step:    47200, Batch Loss:     2.530421, Tokens per Sec:     7318, Lr: 0.000300\n",
      "2021-07-26 13:03:37,293 - INFO - joeynmt.training - Epoch  17: total training loss 5649.70\n",
      "2021-07-26 13:03:37,293 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-26 13:03:55,565 - INFO - joeynmt.training - Epoch  18, Step:    47300, Batch Loss:     1.770466, Tokens per Sec:     7368, Lr: 0.000300\n",
      "2021-07-26 13:04:24,518 - INFO - joeynmt.training - Epoch  18, Step:    47400, Batch Loss:     2.192914, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-07-26 13:04:53,016 - INFO - joeynmt.training - Epoch  18, Step:    47500, Batch Loss:     2.133373, Tokens per Sec:     7328, Lr: 0.000300\n",
      "2021-07-26 13:05:22,046 - INFO - joeynmt.training - Epoch  18, Step:    47600, Batch Loss:     2.078179, Tokens per Sec:     7551, Lr: 0.000300\n",
      "2021-07-26 13:05:50,905 - INFO - joeynmt.training - Epoch  18, Step:    47700, Batch Loss:     1.641423, Tokens per Sec:     7397, Lr: 0.000300\n",
      "2021-07-26 13:06:19,247 - INFO - joeynmt.training - Epoch  18, Step:    47800, Batch Loss:     1.649249, Tokens per Sec:     7529, Lr: 0.000300\n",
      "2021-07-26 13:06:48,026 - INFO - joeynmt.training - Epoch  18, Step:    47900, Batch Loss:     1.973821, Tokens per Sec:     7550, Lr: 0.000300\n",
      "2021-07-26 13:07:17,033 - INFO - joeynmt.training - Epoch  18, Step:    48000, Batch Loss:     1.983115, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 13:08:01,133 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 13:08:01,133 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 13:08:01,133 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 13:08:01,430 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 13:08:01,431 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 13:08:02,297 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 13:08:02,299 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 13:08:02,299 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 13:08:02,300 - INFO - joeynmt.training - \tHypothesis: “ If you hate what you do good and suffer when you endure , that is what is acceptable to God . ”\n",
      "2021-07-26 13:08:02,300 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 13:08:02,300 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 13:08:02,301 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 13:08:02,301 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son , whom I have not approved . ”\n",
      "2021-07-26 13:08:02,301 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 13:08:02,302 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 13:08:02,302 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 13:08:02,303 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-26 13:08:02,303 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 13:08:02,304 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 13:08:02,304 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 13:08:02,304 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 13:08:02,304 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    48000: bleu:  23.69, loss: 48693.7891, ppl:   6.2596, duration: 45.2714s\n",
      "2021-07-26 13:08:31,479 - INFO - joeynmt.training - Epoch  18, Step:    48100, Batch Loss:     1.899022, Tokens per Sec:     7485, Lr: 0.000300\n",
      "2021-07-26 13:09:00,216 - INFO - joeynmt.training - Epoch  18, Step:    48200, Batch Loss:     2.163074, Tokens per Sec:     7380, Lr: 0.000300\n",
      "2021-07-26 13:09:28,904 - INFO - joeynmt.training - Epoch  18, Step:    48300, Batch Loss:     1.936351, Tokens per Sec:     7335, Lr: 0.000300\n",
      "2021-07-26 13:09:57,559 - INFO - joeynmt.training - Epoch  18, Step:    48400, Batch Loss:     2.042736, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-07-26 13:10:26,387 - INFO - joeynmt.training - Epoch  18, Step:    48500, Batch Loss:     2.019398, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-07-26 13:10:55,080 - INFO - joeynmt.training - Epoch  18, Step:    48600, Batch Loss:     2.118577, Tokens per Sec:     7335, Lr: 0.000300\n",
      "2021-07-26 13:11:24,035 - INFO - joeynmt.training - Epoch  18, Step:    48700, Batch Loss:     2.073985, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-07-26 13:11:52,677 - INFO - joeynmt.training - Epoch  18, Step:    48800, Batch Loss:     2.113564, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-07-26 13:12:21,545 - INFO - joeynmt.training - Epoch  18, Step:    48900, Batch Loss:     2.025542, Tokens per Sec:     7537, Lr: 0.000300\n",
      "2021-07-26 13:12:50,450 - INFO - joeynmt.training - Epoch  18, Step:    49000, Batch Loss:     1.884762, Tokens per Sec:     7523, Lr: 0.000300\n",
      "2021-07-26 13:13:19,260 - INFO - joeynmt.training - Epoch  18, Step:    49100, Batch Loss:     2.276309, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-07-26 13:13:48,312 - INFO - joeynmt.training - Epoch  18, Step:    49200, Batch Loss:     2.170623, Tokens per Sec:     7519, Lr: 0.000300\n",
      "2021-07-26 13:14:17,189 - INFO - joeynmt.training - Epoch  18, Step:    49300, Batch Loss:     2.019201, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-07-26 13:14:46,003 - INFO - joeynmt.training - Epoch  18, Step:    49400, Batch Loss:     2.019001, Tokens per Sec:     7556, Lr: 0.000300\n",
      "2021-07-26 13:15:15,147 - INFO - joeynmt.training - Epoch  18, Step:    49500, Batch Loss:     2.306019, Tokens per Sec:     7480, Lr: 0.000300\n",
      "2021-07-26 13:15:44,193 - INFO - joeynmt.training - Epoch  18, Step:    49600, Batch Loss:     1.944485, Tokens per Sec:     7462, Lr: 0.000300\n",
      "2021-07-26 13:16:12,953 - INFO - joeynmt.training - Epoch  18, Step:    49700, Batch Loss:     1.997455, Tokens per Sec:     7449, Lr: 0.000300\n",
      "2021-07-26 13:16:41,730 - INFO - joeynmt.training - Epoch  18, Step:    49800, Batch Loss:     1.916501, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-07-26 13:17:10,213 - INFO - joeynmt.training - Epoch  18, Step:    49900, Batch Loss:     2.120594, Tokens per Sec:     7342, Lr: 0.000300\n",
      "2021-07-26 13:17:39,174 - INFO - joeynmt.training - Epoch  18, Step:    50000, Batch Loss:     2.074792, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-07-26 13:17:44,465 - INFO - joeynmt.training - Epoch  18: total training loss 5613.01\n",
      "2021-07-26 13:17:44,465 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-26 13:18:08,607 - INFO - joeynmt.training - Epoch  19, Step:    50100, Batch Loss:     2.328489, Tokens per Sec:     7354, Lr: 0.000300\n",
      "2021-07-26 13:18:37,550 - INFO - joeynmt.training - Epoch  19, Step:    50200, Batch Loss:     2.147028, Tokens per Sec:     7538, Lr: 0.000300\n",
      "2021-07-26 13:19:06,493 - INFO - joeynmt.training - Epoch  19, Step:    50300, Batch Loss:     1.861427, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-07-26 13:19:35,008 - INFO - joeynmt.training - Epoch  19, Step:    50400, Batch Loss:     1.970137, Tokens per Sec:     7274, Lr: 0.000300\n",
      "2021-07-26 13:20:03,777 - INFO - joeynmt.training - Epoch  19, Step:    50500, Batch Loss:     2.065535, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-07-26 13:20:32,841 - INFO - joeynmt.training - Epoch  19, Step:    50600, Batch Loss:     1.666221, Tokens per Sec:     7595, Lr: 0.000300\n",
      "2021-07-26 13:21:01,594 - INFO - joeynmt.training - Epoch  19, Step:    50700, Batch Loss:     2.092873, Tokens per Sec:     7467, Lr: 0.000300\n",
      "2021-07-26 13:21:30,357 - INFO - joeynmt.training - Epoch  19, Step:    50800, Batch Loss:     2.112618, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-07-26 13:21:59,514 - INFO - joeynmt.training - Epoch  19, Step:    50900, Batch Loss:     2.083097, Tokens per Sec:     7488, Lr: 0.000300\n",
      "2021-07-26 13:22:28,094 - INFO - joeynmt.training - Epoch  19, Step:    51000, Batch Loss:     1.746421, Tokens per Sec:     7286, Lr: 0.000300\n",
      "2021-07-26 13:23:06,111 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 13:23:06,111 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 13:23:06,111 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 13:23:06,413 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 13:23:06,413 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 13:23:07,690 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 13:23:07,691 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 13:23:07,691 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 13:23:07,692 - INFO - joeynmt.training - \tHypothesis: If you hate what is good and suffer when you endure , what is acceptable to God . ”\n",
      "2021-07-26 13:23:07,692 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 13:23:07,693 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 13:23:07,693 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 13:23:07,693 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-26 13:23:07,693 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 13:23:07,694 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 13:23:07,694 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 13:23:07,695 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-26 13:23:07,695 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 13:23:07,696 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 13:23:07,696 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 13:23:07,697 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to cure his family .\n",
      "2021-07-26 13:23:07,697 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    51000: bleu:  23.92, loss: 48376.3750, ppl:   6.1852, duration: 39.6021s\n",
      "2021-07-26 13:23:37,287 - INFO - joeynmt.training - Epoch  19, Step:    51100, Batch Loss:     1.372160, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-07-26 13:24:05,953 - INFO - joeynmt.training - Epoch  19, Step:    51200, Batch Loss:     2.025077, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-07-26 13:24:35,024 - INFO - joeynmt.training - Epoch  19, Step:    51300, Batch Loss:     1.971806, Tokens per Sec:     7531, Lr: 0.000300\n",
      "2021-07-26 13:25:03,559 - INFO - joeynmt.training - Epoch  19, Step:    51400, Batch Loss:     2.148551, Tokens per Sec:     7337, Lr: 0.000300\n",
      "2021-07-26 13:25:32,135 - INFO - joeynmt.training - Epoch  19, Step:    51500, Batch Loss:     2.039721, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-07-26 13:26:00,638 - INFO - joeynmt.training - Epoch  19, Step:    51600, Batch Loss:     1.960381, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-07-26 13:26:29,724 - INFO - joeynmt.training - Epoch  19, Step:    51700, Batch Loss:     2.064970, Tokens per Sec:     7478, Lr: 0.000300\n",
      "2021-07-26 13:26:58,472 - INFO - joeynmt.training - Epoch  19, Step:    51800, Batch Loss:     2.155165, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-07-26 13:27:26,999 - INFO - joeynmt.training - Epoch  19, Step:    51900, Batch Loss:     1.957552, Tokens per Sec:     7308, Lr: 0.000300\n",
      "2021-07-26 13:27:55,730 - INFO - joeynmt.training - Epoch  19, Step:    52000, Batch Loss:     1.875953, Tokens per Sec:     7370, Lr: 0.000300\n",
      "2021-07-26 13:28:24,649 - INFO - joeynmt.training - Epoch  19, Step:    52100, Batch Loss:     2.079758, Tokens per Sec:     7517, Lr: 0.000300\n",
      "2021-07-26 13:28:53,409 - INFO - joeynmt.training - Epoch  19, Step:    52200, Batch Loss:     2.076124, Tokens per Sec:     7417, Lr: 0.000300\n",
      "2021-07-26 13:29:22,374 - INFO - joeynmt.training - Epoch  19, Step:    52300, Batch Loss:     2.202668, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-07-26 13:29:50,862 - INFO - joeynmt.training - Epoch  19, Step:    52400, Batch Loss:     2.188830, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 13:30:19,791 - INFO - joeynmt.training - Epoch  19, Step:    52500, Batch Loss:     1.929811, Tokens per Sec:     7637, Lr: 0.000300\n",
      "2021-07-26 13:30:48,691 - INFO - joeynmt.training - Epoch  19, Step:    52600, Batch Loss:     2.084716, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-07-26 13:31:17,818 - INFO - joeynmt.training - Epoch  19, Step:    52700, Batch Loss:     2.310627, Tokens per Sec:     7464, Lr: 0.000300\n",
      "2021-07-26 13:31:46,665 - INFO - joeynmt.training - Epoch  19: total training loss 5573.52\n",
      "2021-07-26 13:31:46,670 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-26 13:31:47,319 - INFO - joeynmt.training - Epoch  20, Step:    52800, Batch Loss:     1.788493, Tokens per Sec:     2289, Lr: 0.000300\n",
      "2021-07-26 13:32:16,083 - INFO - joeynmt.training - Epoch  20, Step:    52900, Batch Loss:     2.293375, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-07-26 13:32:44,942 - INFO - joeynmt.training - Epoch  20, Step:    53000, Batch Loss:     2.011954, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-07-26 13:33:13,826 - INFO - joeynmt.training - Epoch  20, Step:    53100, Batch Loss:     1.995098, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-07-26 13:33:42,294 - INFO - joeynmt.training - Epoch  20, Step:    53200, Batch Loss:     2.151518, Tokens per Sec:     7372, Lr: 0.000300\n",
      "2021-07-26 13:34:11,320 - INFO - joeynmt.training - Epoch  20, Step:    53300, Batch Loss:     2.015138, Tokens per Sec:     7552, Lr: 0.000300\n",
      "2021-07-26 13:34:40,042 - INFO - joeynmt.training - Epoch  20, Step:    53400, Batch Loss:     1.996987, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-07-26 13:35:08,779 - INFO - joeynmt.training - Epoch  20, Step:    53500, Batch Loss:     2.101033, Tokens per Sec:     7331, Lr: 0.000300\n",
      "2021-07-26 13:35:37,559 - INFO - joeynmt.training - Epoch  20, Step:    53600, Batch Loss:     2.106184, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-07-26 13:36:06,324 - INFO - joeynmt.training - Epoch  20, Step:    53700, Batch Loss:     1.997796, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-07-26 13:36:35,005 - INFO - joeynmt.training - Epoch  20, Step:    53800, Batch Loss:     1.940995, Tokens per Sec:     7368, Lr: 0.000300\n",
      "2021-07-26 13:37:04,023 - INFO - joeynmt.training - Epoch  20, Step:    53900, Batch Loss:     2.066491, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-07-26 13:37:32,540 - INFO - joeynmt.training - Epoch  20, Step:    54000, Batch Loss:     2.075776, Tokens per Sec:     7330, Lr: 0.000300\n",
      "2021-07-26 13:38:13,026 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 13:38:13,026 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 13:38:13,027 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 13:38:13,335 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 13:38:13,335 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 13:38:14,190 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 13:38:14,191 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 13:38:14,192 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 13:38:14,192 - INFO - joeynmt.training - \tHypothesis: When you hate what you are doing good and suffer when you endure , that is acceptable to God . ”\n",
      "2021-07-26 13:38:14,192 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 13:38:14,193 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 13:38:14,193 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 13:38:14,193 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have not appreciated . ”\n",
      "2021-07-26 13:38:14,194 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 13:38:14,194 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 13:38:14,195 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 13:38:14,195 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-26 13:38:14,195 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 13:38:14,196 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 13:38:14,196 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 13:38:14,196 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 13:38:14,198 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    54000: bleu:  24.08, loss: 48173.9219, ppl:   6.1382, duration: 41.6566s\n",
      "2021-07-26 13:38:43,955 - INFO - joeynmt.training - Epoch  20, Step:    54100, Batch Loss:     2.087806, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-07-26 13:39:13,035 - INFO - joeynmt.training - Epoch  20, Step:    54200, Batch Loss:     2.125611, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-07-26 13:39:41,754 - INFO - joeynmt.training - Epoch  20, Step:    54300, Batch Loss:     1.927937, Tokens per Sec:     7430, Lr: 0.000300\n",
      "2021-07-26 13:40:10,362 - INFO - joeynmt.training - Epoch  20, Step:    54400, Batch Loss:     2.006349, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-07-26 13:40:39,358 - INFO - joeynmt.training - Epoch  20, Step:    54500, Batch Loss:     1.557709, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-07-26 13:41:08,065 - INFO - joeynmt.training - Epoch  20, Step:    54600, Batch Loss:     2.035212, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-07-26 13:41:36,940 - INFO - joeynmt.training - Epoch  20, Step:    54700, Batch Loss:     2.157003, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-07-26 13:42:05,791 - INFO - joeynmt.training - Epoch  20, Step:    54800, Batch Loss:     2.051990, Tokens per Sec:     7530, Lr: 0.000300\n",
      "2021-07-26 13:42:34,789 - INFO - joeynmt.training - Epoch  20, Step:    54900, Batch Loss:     1.797747, Tokens per Sec:     7506, Lr: 0.000300\n",
      "2021-07-26 13:43:03,706 - INFO - joeynmt.training - Epoch  20, Step:    55000, Batch Loss:     1.897557, Tokens per Sec:     7562, Lr: 0.000300\n",
      "2021-07-26 13:43:32,505 - INFO - joeynmt.training - Epoch  20, Step:    55100, Batch Loss:     2.212094, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-07-26 13:44:01,064 - INFO - joeynmt.training - Epoch  20, Step:    55200, Batch Loss:     2.066965, Tokens per Sec:     7369, Lr: 0.000300\n",
      "2021-07-26 13:44:29,800 - INFO - joeynmt.training - Epoch  20, Step:    55300, Batch Loss:     2.118442, Tokens per Sec:     7571, Lr: 0.000300\n",
      "2021-07-26 13:44:58,613 - INFO - joeynmt.training - Epoch  20, Step:    55400, Batch Loss:     2.032940, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-07-26 13:45:27,521 - INFO - joeynmt.training - Epoch  20, Step:    55500, Batch Loss:     2.139765, Tokens per Sec:     7553, Lr: 0.000300\n",
      "2021-07-26 13:45:50,144 - INFO - joeynmt.training - Epoch  20: total training loss 5533.05\n",
      "2021-07-26 13:45:50,145 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-26 13:45:56,166 - INFO - joeynmt.training - Epoch  21, Step:    55600, Batch Loss:     2.081931, Tokens per Sec:     6406, Lr: 0.000300\n",
      "2021-07-26 13:46:25,033 - INFO - joeynmt.training - Epoch  21, Step:    55700, Batch Loss:     1.913334, Tokens per Sec:     7670, Lr: 0.000300\n",
      "2021-07-26 13:46:54,103 - INFO - joeynmt.training - Epoch  21, Step:    55800, Batch Loss:     2.027750, Tokens per Sec:     7576, Lr: 0.000300\n",
      "2021-07-26 13:47:23,097 - INFO - joeynmt.training - Epoch  21, Step:    55900, Batch Loss:     2.006148, Tokens per Sec:     7493, Lr: 0.000300\n",
      "2021-07-26 13:47:51,756 - INFO - joeynmt.training - Epoch  21, Step:    56000, Batch Loss:     1.966112, Tokens per Sec:     7613, Lr: 0.000300\n",
      "2021-07-26 13:48:20,330 - INFO - joeynmt.training - Epoch  21, Step:    56100, Batch Loss:     2.117944, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-07-26 13:48:49,161 - INFO - joeynmt.training - Epoch  21, Step:    56200, Batch Loss:     1.673303, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-07-26 13:49:18,079 - INFO - joeynmt.training - Epoch  21, Step:    56300, Batch Loss:     1.399519, Tokens per Sec:     7529, Lr: 0.000300\n",
      "2021-07-26 13:49:47,071 - INFO - joeynmt.training - Epoch  21, Step:    56400, Batch Loss:     2.084628, Tokens per Sec:     7529, Lr: 0.000300\n",
      "2021-07-26 13:50:15,761 - INFO - joeynmt.training - Epoch  21, Step:    56500, Batch Loss:     1.930622, Tokens per Sec:     7486, Lr: 0.000300\n",
      "2021-07-26 13:50:44,869 - INFO - joeynmt.training - Epoch  21, Step:    56600, Batch Loss:     2.116616, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-07-26 13:51:13,729 - INFO - joeynmt.training - Epoch  21, Step:    56700, Batch Loss:     1.865439, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-07-26 13:51:42,621 - INFO - joeynmt.training - Epoch  21, Step:    56800, Batch Loss:     1.952018, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-07-26 13:52:11,423 - INFO - joeynmt.training - Epoch  21, Step:    56900, Batch Loss:     2.035693, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-07-26 13:52:40,087 - INFO - joeynmt.training - Epoch  21, Step:    57000, Batch Loss:     2.098150, Tokens per Sec:     7338, Lr: 0.000300\n",
      "2021-07-26 13:53:25,389 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 13:53:25,390 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 13:53:25,390 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 13:53:25,698 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 13:53:25,698 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 13:53:26,578 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 13:53:26,579 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 13:53:26,579 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 13:53:26,579 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is acceptable to God . ”\n",
      "2021-07-26 13:53:26,580 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 13:53:26,580 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 13:53:26,581 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 13:53:26,581 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son . ”\n",
      "2021-07-26 13:53:26,581 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 13:53:26,582 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 13:53:26,582 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 13:53:26,582 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-26 13:53:26,582 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 13:53:26,583 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 13:53:26,584 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 13:53:26,584 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 13:53:26,584 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    57000: bleu:  24.49, loss: 47796.1289, ppl:   6.0515, duration: 46.4964s\n",
      "2021-07-26 13:53:55,670 - INFO - joeynmt.training - Epoch  21, Step:    57100, Batch Loss:     1.961094, Tokens per Sec:     7336, Lr: 0.000300\n",
      "2021-07-26 13:54:24,412 - INFO - joeynmt.training - Epoch  21, Step:    57200, Batch Loss:     1.865238, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-07-26 13:54:53,149 - INFO - joeynmt.training - Epoch  21, Step:    57300, Batch Loss:     2.056646, Tokens per Sec:     7467, Lr: 0.000300\n",
      "2021-07-26 13:55:21,957 - INFO - joeynmt.training - Epoch  21, Step:    57400, Batch Loss:     1.918413, Tokens per Sec:     7480, Lr: 0.000300\n",
      "2021-07-26 13:55:50,672 - INFO - joeynmt.training - Epoch  21, Step:    57500, Batch Loss:     2.315016, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-07-26 13:56:19,547 - INFO - joeynmt.training - Epoch  21, Step:    57600, Batch Loss:     2.100782, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-07-26 13:56:48,457 - INFO - joeynmt.training - Epoch  21, Step:    57700, Batch Loss:     2.042735, Tokens per Sec:     7524, Lr: 0.000300\n",
      "2021-07-26 13:57:16,976 - INFO - joeynmt.training - Epoch  21, Step:    57800, Batch Loss:     1.977585, Tokens per Sec:     7265, Lr: 0.000300\n",
      "2021-07-26 13:57:46,122 - INFO - joeynmt.training - Epoch  21, Step:    57900, Batch Loss:     2.143362, Tokens per Sec:     7555, Lr: 0.000300\n",
      "2021-07-26 13:58:14,986 - INFO - joeynmt.training - Epoch  21, Step:    58000, Batch Loss:     2.019017, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-07-26 13:58:43,726 - INFO - joeynmt.training - Epoch  21, Step:    58100, Batch Loss:     1.678339, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-07-26 13:59:12,583 - INFO - joeynmt.training - Epoch  21, Step:    58200, Batch Loss:     2.083037, Tokens per Sec:     7545, Lr: 0.000300\n",
      "2021-07-26 13:59:41,287 - INFO - joeynmt.training - Epoch  21, Step:    58300, Batch Loss:     2.024454, Tokens per Sec:     7411, Lr: 0.000300\n",
      "2021-07-26 13:59:56,607 - INFO - joeynmt.training - Epoch  21: total training loss 5477.83\n",
      "2021-07-26 13:59:56,608 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-26 14:00:10,591 - INFO - joeynmt.training - Epoch  22, Step:    58400, Batch Loss:     1.923078, Tokens per Sec:     7336, Lr: 0.000300\n",
      "2021-07-26 14:00:39,408 - INFO - joeynmt.training - Epoch  22, Step:    58500, Batch Loss:     1.895946, Tokens per Sec:     7581, Lr: 0.000300\n",
      "2021-07-26 14:01:08,128 - INFO - joeynmt.training - Epoch  22, Step:    58600, Batch Loss:     2.041906, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-07-26 14:01:37,005 - INFO - joeynmt.training - Epoch  22, Step:    58700, Batch Loss:     1.787464, Tokens per Sec:     7484, Lr: 0.000300\n",
      "2021-07-26 14:02:05,531 - INFO - joeynmt.training - Epoch  22, Step:    58800, Batch Loss:     1.530533, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-07-26 14:02:33,809 - INFO - joeynmt.training - Epoch  22, Step:    58900, Batch Loss:     1.976023, Tokens per Sec:     7378, Lr: 0.000300\n",
      "2021-07-26 14:03:02,889 - INFO - joeynmt.training - Epoch  22, Step:    59000, Batch Loss:     2.175054, Tokens per Sec:     7466, Lr: 0.000300\n",
      "2021-07-26 14:03:31,257 - INFO - joeynmt.training - Epoch  22, Step:    59100, Batch Loss:     2.049866, Tokens per Sec:     7322, Lr: 0.000300\n",
      "2021-07-26 14:03:59,874 - INFO - joeynmt.training - Epoch  22, Step:    59200, Batch Loss:     2.021554, Tokens per Sec:     7536, Lr: 0.000300\n",
      "2021-07-26 14:04:28,431 - INFO - joeynmt.training - Epoch  22, Step:    59300, Batch Loss:     2.034420, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-07-26 14:04:57,234 - INFO - joeynmt.training - Epoch  22, Step:    59400, Batch Loss:     2.072811, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-07-26 14:05:26,042 - INFO - joeynmt.training - Epoch  22, Step:    59500, Batch Loss:     1.999832, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-07-26 14:05:54,970 - INFO - joeynmt.training - Epoch  22, Step:    59600, Batch Loss:     1.730961, Tokens per Sec:     7517, Lr: 0.000300\n",
      "2021-07-26 14:06:23,423 - INFO - joeynmt.training - Epoch  22, Step:    59700, Batch Loss:     1.927494, Tokens per Sec:     7582, Lr: 0.000300\n",
      "2021-07-26 14:06:51,423 - INFO - joeynmt.training - Epoch  22, Step:    59800, Batch Loss:     1.508904, Tokens per Sec:     7209, Lr: 0.000300\n",
      "2021-07-26 14:07:20,000 - INFO - joeynmt.training - Epoch  22, Step:    59900, Batch Loss:     1.941673, Tokens per Sec:     7562, Lr: 0.000300\n",
      "2021-07-26 14:07:48,249 - INFO - joeynmt.training - Epoch  22, Step:    60000, Batch Loss:     2.259316, Tokens per Sec:     7465, Lr: 0.000300\n",
      "2021-07-26 14:08:30,053 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 14:08:30,058 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 14:08:30,058 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 14:08:30,364 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 14:08:30,364 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 14:08:31,235 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 14:08:31,236 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 14:08:31,236 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 14:08:31,236 - INFO - joeynmt.training - \tHypothesis: If you do good , you are doing good and sufficient when you endure , that is what is acceptable to God . ”\n",
      "2021-07-26 14:08:31,237 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 14:08:31,238 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 14:08:31,238 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 14:08:31,238 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son , whom I have not approved . ”\n",
      "2021-07-26 14:08:31,238 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 14:08:31,239 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 14:08:31,239 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 14:08:31,240 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-26 14:08:31,240 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 14:08:31,241 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 14:08:31,241 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 14:08:31,241 - INFO - joeynmt.training - \tHypothesis: But Abigail had what he did to save his family .\n",
      "2021-07-26 14:08:31,241 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step    60000: bleu:  24.62, loss: 47380.0977, ppl:   5.9574, duration: 42.9918s\n",
      "2021-07-26 14:09:00,622 - INFO - joeynmt.training - Epoch  22, Step:    60100, Batch Loss:     2.065078, Tokens per Sec:     7517, Lr: 0.000300\n",
      "2021-07-26 14:09:29,188 - INFO - joeynmt.training - Epoch  22, Step:    60200, Batch Loss:     2.147618, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-07-26 14:09:57,998 - INFO - joeynmt.training - Epoch  22, Step:    60300, Batch Loss:     2.090430, Tokens per Sec:     7465, Lr: 0.000300\n",
      "2021-07-26 14:10:26,649 - INFO - joeynmt.training - Epoch  22, Step:    60400, Batch Loss:     2.074636, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-07-26 14:10:55,717 - INFO - joeynmt.training - Epoch  22, Step:    60500, Batch Loss:     2.047791, Tokens per Sec:     7381, Lr: 0.000300\n",
      "2021-07-26 14:11:24,534 - INFO - joeynmt.training - Epoch  22, Step:    60600, Batch Loss:     1.883388, Tokens per Sec:     7563, Lr: 0.000300\n",
      "2021-07-26 14:11:53,278 - INFO - joeynmt.training - Epoch  22, Step:    60700, Batch Loss:     2.053013, Tokens per Sec:     7402, Lr: 0.000300\n",
      "2021-07-26 14:12:21,940 - INFO - joeynmt.training - Epoch  22, Step:    60800, Batch Loss:     2.179003, Tokens per Sec:     7511, Lr: 0.000300\n",
      "2021-07-26 14:12:50,800 - INFO - joeynmt.training - Epoch  22, Step:    60900, Batch Loss:     1.769090, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-07-26 14:13:19,603 - INFO - joeynmt.training - Epoch  22, Step:    61000, Batch Loss:     2.242237, Tokens per Sec:     7565, Lr: 0.000300\n",
      "2021-07-26 14:13:48,624 - INFO - joeynmt.training - Epoch  22, Step:    61100, Batch Loss:     1.868538, Tokens per Sec:     7579, Lr: 0.000300\n",
      "2021-07-26 14:13:59,367 - INFO - joeynmt.training - Epoch  22: total training loss 5463.76\n",
      "2021-07-26 14:13:59,368 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-26 14:14:18,023 - INFO - joeynmt.training - Epoch  23, Step:    61200, Batch Loss:     1.973916, Tokens per Sec:     7339, Lr: 0.000300\n",
      "2021-07-26 14:14:46,345 - INFO - joeynmt.training - Epoch  23, Step:    61300, Batch Loss:     1.948509, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-07-26 14:15:15,319 - INFO - joeynmt.training - Epoch  23, Step:    61400, Batch Loss:     2.034348, Tokens per Sec:     7552, Lr: 0.000300\n",
      "2021-07-26 14:15:43,829 - INFO - joeynmt.training - Epoch  23, Step:    61500, Batch Loss:     2.102164, Tokens per Sec:     7285, Lr: 0.000300\n",
      "2021-07-26 14:16:12,895 - INFO - joeynmt.training - Epoch  23, Step:    61600, Batch Loss:     1.826344, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-26 14:16:41,762 - INFO - joeynmt.training - Epoch  23, Step:    61700, Batch Loss:     2.183228, Tokens per Sec:     7508, Lr: 0.000300\n",
      "2021-07-26 14:17:10,363 - INFO - joeynmt.training - Epoch  23, Step:    61800, Batch Loss:     2.011687, Tokens per Sec:     7559, Lr: 0.000300\n",
      "2021-07-26 14:17:39,162 - INFO - joeynmt.training - Epoch  23, Step:    61900, Batch Loss:     2.157290, Tokens per Sec:     7318, Lr: 0.000300\n",
      "2021-07-26 14:18:08,043 - INFO - joeynmt.training - Epoch  23, Step:    62000, Batch Loss:     1.987885, Tokens per Sec:     7518, Lr: 0.000300\n",
      "2021-07-26 14:18:36,663 - INFO - joeynmt.training - Epoch  23, Step:    62100, Batch Loss:     1.857024, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-07-26 14:19:05,741 - INFO - joeynmt.training - Epoch  23, Step:    62200, Batch Loss:     1.890147, Tokens per Sec:     7554, Lr: 0.000300\n",
      "2021-07-26 14:19:33,934 - INFO - joeynmt.training - Epoch  23, Step:    62300, Batch Loss:     1.884822, Tokens per Sec:     7345, Lr: 0.000300\n",
      "2021-07-26 14:20:02,985 - INFO - joeynmt.training - Epoch  23, Step:    62400, Batch Loss:     1.596832, Tokens per Sec:     7515, Lr: 0.000300\n",
      "2021-07-26 14:20:31,893 - INFO - joeynmt.training - Epoch  23, Step:    62500, Batch Loss:     1.911330, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-07-26 14:21:00,398 - INFO - joeynmt.training - Epoch  23, Step:    62600, Batch Loss:     1.833601, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-07-26 14:21:29,139 - INFO - joeynmt.training - Epoch  23, Step:    62700, Batch Loss:     2.005106, Tokens per Sec:     7486, Lr: 0.000300\n",
      "2021-07-26 14:21:57,726 - INFO - joeynmt.training - Epoch  23, Step:    62800, Batch Loss:     1.966973, Tokens per Sec:     7526, Lr: 0.000300\n",
      "2021-07-26 14:22:26,586 - INFO - joeynmt.training - Epoch  23, Step:    62900, Batch Loss:     2.109457, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-07-26 14:22:55,578 - INFO - joeynmt.training - Epoch  23, Step:    63000, Batch Loss:     1.995321, Tokens per Sec:     7492, Lr: 0.000300\n",
      "2021-07-26 14:23:33,898 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 14:23:33,899 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 14:23:33,899 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 14:23:34,211 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 14:23:34,211 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 14:23:35,085 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 14:23:35,086 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 14:23:35,087 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 14:23:35,087 - INFO - joeynmt.training - \tHypothesis: [ I ] hated you with good works and suffer when you endure , that is acceptable to God . ”\n",
      "2021-07-26 14:23:35,087 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 14:23:35,088 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 14:23:35,088 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 14:23:35,088 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son whom I have approved . ”\n",
      "2021-07-26 14:23:35,089 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 14:23:35,089 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 14:23:35,090 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 14:23:35,090 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
      "2021-07-26 14:23:35,090 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 14:23:35,091 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 14:23:35,091 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 14:23:35,091 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 14:23:35,091 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    63000: bleu:  24.86, loss: 47135.6172, ppl:   5.9028, duration: 39.5126s\n",
      "2021-07-26 14:24:03,976 - INFO - joeynmt.training - Epoch  23, Step:    63100, Batch Loss:     1.923231, Tokens per Sec:     7489, Lr: 0.000300\n",
      "2021-07-26 14:24:32,656 - INFO - joeynmt.training - Epoch  23, Step:    63200, Batch Loss:     2.135990, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-07-26 14:25:01,454 - INFO - joeynmt.training - Epoch  23, Step:    63300, Batch Loss:     1.145102, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-07-26 14:25:30,561 - INFO - joeynmt.training - Epoch  23, Step:    63400, Batch Loss:     1.420908, Tokens per Sec:     7704, Lr: 0.000300\n",
      "2021-07-26 14:25:58,929 - INFO - joeynmt.training - Epoch  23, Step:    63500, Batch Loss:     2.126811, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-07-26 14:26:27,564 - INFO - joeynmt.training - Epoch  23, Step:    63600, Batch Loss:     2.127270, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-07-26 14:26:56,459 - INFO - joeynmt.training - Epoch  23, Step:    63700, Batch Loss:     2.133939, Tokens per Sec:     7463, Lr: 0.000300\n",
      "2021-07-26 14:27:24,922 - INFO - joeynmt.training - Epoch  23, Step:    63800, Batch Loss:     1.989081, Tokens per Sec:     7349, Lr: 0.000300\n",
      "2021-07-26 14:27:53,879 - INFO - joeynmt.training - Epoch  23, Step:    63900, Batch Loss:     1.863776, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-07-26 14:27:58,725 - INFO - joeynmt.training - Epoch  23: total training loss 5422.22\n",
      "2021-07-26 14:27:58,726 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-26 14:28:23,241 - INFO - joeynmt.training - Epoch  24, Step:    64000, Batch Loss:     1.983339, Tokens per Sec:     7274, Lr: 0.000300\n",
      "2021-07-26 14:28:51,825 - INFO - joeynmt.training - Epoch  24, Step:    64100, Batch Loss:     1.881263, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-07-26 14:29:20,829 - INFO - joeynmt.training - Epoch  24, Step:    64200, Batch Loss:     2.049984, Tokens per Sec:     7485, Lr: 0.000300\n",
      "2021-07-26 14:29:49,915 - INFO - joeynmt.training - Epoch  24, Step:    64300, Batch Loss:     1.953075, Tokens per Sec:     7554, Lr: 0.000300\n",
      "2021-07-26 14:30:18,897 - INFO - joeynmt.training - Epoch  24, Step:    64400, Batch Loss:     2.006084, Tokens per Sec:     7531, Lr: 0.000300\n",
      "2021-07-26 14:30:47,718 - INFO - joeynmt.training - Epoch  24, Step:    64500, Batch Loss:     2.208160, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-07-26 14:31:16,013 - INFO - joeynmt.training - Epoch  24, Step:    64600, Batch Loss:     2.035648, Tokens per Sec:     7412, Lr: 0.000300\n",
      "2021-07-26 14:31:44,974 - INFO - joeynmt.training - Epoch  24, Step:    64700, Batch Loss:     2.021362, Tokens per Sec:     7551, Lr: 0.000300\n",
      "2021-07-26 14:32:13,951 - INFO - joeynmt.training - Epoch  24, Step:    64800, Batch Loss:     1.992961, Tokens per Sec:     7604, Lr: 0.000300\n",
      "2021-07-26 14:32:42,631 - INFO - joeynmt.training - Epoch  24, Step:    64900, Batch Loss:     2.240442, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-07-26 14:33:11,100 - INFO - joeynmt.training - Epoch  24, Step:    65000, Batch Loss:     1.975001, Tokens per Sec:     7394, Lr: 0.000300\n",
      "2021-07-26 14:33:39,939 - INFO - joeynmt.training - Epoch  24, Step:    65100, Batch Loss:     1.364522, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-07-26 14:34:08,884 - INFO - joeynmt.training - Epoch  24, Step:    65200, Batch Loss:     1.982166, Tokens per Sec:     7565, Lr: 0.000300\n",
      "2021-07-26 14:34:37,260 - INFO - joeynmt.training - Epoch  24, Step:    65300, Batch Loss:     2.026324, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-26 14:35:06,086 - INFO - joeynmt.training - Epoch  24, Step:    65400, Batch Loss:     1.919721, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-07-26 14:35:34,674 - INFO - joeynmt.training - Epoch  24, Step:    65500, Batch Loss:     1.984102, Tokens per Sec:     7387, Lr: 0.000300\n",
      "2021-07-26 14:36:03,778 - INFO - joeynmt.training - Epoch  24, Step:    65600, Batch Loss:     2.031647, Tokens per Sec:     7615, Lr: 0.000300\n",
      "2021-07-26 14:36:32,243 - INFO - joeynmt.training - Epoch  24, Step:    65700, Batch Loss:     1.851936, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-07-26 14:37:00,594 - INFO - joeynmt.training - Epoch  24, Step:    65800, Batch Loss:     1.690311, Tokens per Sec:     7554, Lr: 0.000300\n",
      "2021-07-26 14:37:28,832 - INFO - joeynmt.training - Epoch  24, Step:    65900, Batch Loss:     1.964577, Tokens per Sec:     7338, Lr: 0.000300\n",
      "2021-07-26 14:37:57,461 - INFO - joeynmt.training - Epoch  24, Step:    66000, Batch Loss:     1.820251, Tokens per Sec:     7456, Lr: 0.000300\n",
      "2021-07-26 14:38:41,407 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 14:38:41,408 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 14:38:41,408 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 14:38:41,706 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 14:38:41,706 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 14:38:42,579 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 14:38:42,581 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 14:38:42,581 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 14:38:42,581 - INFO - joeynmt.training - \tHypothesis: [ I ] hated you with good and suffer when you endure , that is what is acceptable to God . ”\n",
      "2021-07-26 14:38:42,581 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 14:38:42,582 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 14:38:42,582 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 14:38:42,583 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-26 14:38:42,583 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 14:38:42,584 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 14:38:42,584 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 14:38:42,584 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
      "2021-07-26 14:38:42,584 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 14:38:42,585 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 14:38:42,585 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 14:38:42,586 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 14:38:42,586 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step    66000: bleu:  25.34, loss: 46935.9375, ppl:   5.8585, duration: 45.1247s\n",
      "2021-07-26 14:39:11,247 - INFO - joeynmt.training - Epoch  24, Step:    66100, Batch Loss:     1.341821, Tokens per Sec:     7415, Lr: 0.000300\n",
      "2021-07-26 14:39:40,040 - INFO - joeynmt.training - Epoch  24, Step:    66200, Batch Loss:     1.990495, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 14:40:08,924 - INFO - joeynmt.training - Epoch  24, Step:    66300, Batch Loss:     1.960297, Tokens per Sec:     7485, Lr: 0.000300\n",
      "2021-07-26 14:40:37,709 - INFO - joeynmt.training - Epoch  24, Step:    66400, Batch Loss:     2.000930, Tokens per Sec:     7515, Lr: 0.000300\n",
      "2021-07-26 14:41:06,865 - INFO - joeynmt.training - Epoch  24, Step:    66500, Batch Loss:     1.832791, Tokens per Sec:     7581, Lr: 0.000300\n",
      "2021-07-26 14:41:35,541 - INFO - joeynmt.training - Epoch  24, Step:    66600, Batch Loss:     1.873839, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-07-26 14:42:03,030 - INFO - joeynmt.training - Epoch  24: total training loss 5394.66\n",
      "2021-07-26 14:42:03,031 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-07-26 14:42:04,903 - INFO - joeynmt.training - Epoch  25, Step:    66700, Batch Loss:     2.005989, Tokens per Sec:     6563, Lr: 0.000300\n",
      "2021-07-26 14:42:33,558 - INFO - joeynmt.training - Epoch  25, Step:    66800, Batch Loss:     1.690702, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-07-26 14:43:01,905 - INFO - joeynmt.training - Epoch  25, Step:    66900, Batch Loss:     1.994784, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-07-26 14:43:30,721 - INFO - joeynmt.training - Epoch  25, Step:    67000, Batch Loss:     2.128393, Tokens per Sec:     7332, Lr: 0.000300\n",
      "2021-07-26 14:43:59,446 - INFO - joeynmt.training - Epoch  25, Step:    67100, Batch Loss:     2.042164, Tokens per Sec:     7479, Lr: 0.000300\n",
      "2021-07-26 14:44:28,030 - INFO - joeynmt.training - Epoch  25, Step:    67200, Batch Loss:     2.046260, Tokens per Sec:     7555, Lr: 0.000300\n",
      "2021-07-26 14:44:56,893 - INFO - joeynmt.training - Epoch  25, Step:    67300, Batch Loss:     1.843603, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-07-26 14:45:25,562 - INFO - joeynmt.training - Epoch  25, Step:    67400, Batch Loss:     1.957986, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-07-26 14:45:54,372 - INFO - joeynmt.training - Epoch  25, Step:    67500, Batch Loss:     2.029788, Tokens per Sec:     7528, Lr: 0.000300\n",
      "2021-07-26 14:46:23,180 - INFO - joeynmt.training - Epoch  25, Step:    67600, Batch Loss:     2.067312, Tokens per Sec:     7484, Lr: 0.000300\n",
      "2021-07-26 14:46:51,814 - INFO - joeynmt.training - Epoch  25, Step:    67700, Batch Loss:     2.063201, Tokens per Sec:     7434, Lr: 0.000300\n",
      "2021-07-26 14:47:20,962 - INFO - joeynmt.training - Epoch  25, Step:    67800, Batch Loss:     2.105404, Tokens per Sec:     7550, Lr: 0.000300\n",
      "2021-07-26 14:47:49,621 - INFO - joeynmt.training - Epoch  25, Step:    67900, Batch Loss:     2.063155, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-07-26 14:48:18,006 - INFO - joeynmt.training - Epoch  25, Step:    68000, Batch Loss:     1.967350, Tokens per Sec:     7313, Lr: 0.000300\n",
      "2021-07-26 14:48:46,803 - INFO - joeynmt.training - Epoch  25, Step:    68100, Batch Loss:     1.992370, Tokens per Sec:     7416, Lr: 0.000300\n",
      "2021-07-26 14:49:15,556 - INFO - joeynmt.training - Epoch  25, Step:    68200, Batch Loss:     1.918841, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-07-26 14:49:44,517 - INFO - joeynmt.training - Epoch  25, Step:    68300, Batch Loss:     2.016020, Tokens per Sec:     7467, Lr: 0.000300\n",
      "2021-07-26 14:50:13,198 - INFO - joeynmt.training - Epoch  25, Step:    68400, Batch Loss:     2.037830, Tokens per Sec:     7402, Lr: 0.000300\n",
      "2021-07-26 14:50:41,965 - INFO - joeynmt.training - Epoch  25, Step:    68500, Batch Loss:     2.174369, Tokens per Sec:     7473, Lr: 0.000300\n",
      "2021-07-26 14:51:10,482 - INFO - joeynmt.training - Epoch  25, Step:    68600, Batch Loss:     1.640507, Tokens per Sec:     7567, Lr: 0.000300\n",
      "2021-07-26 14:51:39,310 - INFO - joeynmt.training - Epoch  25, Step:    68700, Batch Loss:     2.049034, Tokens per Sec:     7454, Lr: 0.000300\n",
      "2021-07-26 14:52:07,955 - INFO - joeynmt.training - Epoch  25, Step:    68800, Batch Loss:     2.017907, Tokens per Sec:     7363, Lr: 0.000300\n",
      "2021-07-26 14:52:36,847 - INFO - joeynmt.training - Epoch  25, Step:    68900, Batch Loss:     1.331719, Tokens per Sec:     7547, Lr: 0.000300\n",
      "2021-07-26 14:53:05,759 - INFO - joeynmt.training - Epoch  25, Step:    69000, Batch Loss:     1.829607, Tokens per Sec:     7517, Lr: 0.000300\n",
      "2021-07-26 14:53:46,954 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 14:53:46,955 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 14:53:46,955 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 14:53:47,252 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 14:53:47,252 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 14:53:48,129 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 14:53:48,131 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-26 14:53:48,131 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-26 14:53:48,131 - INFO - joeynmt.training - \tHypothesis: [ I ] hated you with good work and suffer when you endure , that is what is acceptable to God . ”\n",
      "2021-07-26 14:53:48,132 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 14:53:48,132 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-26 14:53:48,133 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-26 14:53:48,133 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son . ”\n",
      "2021-07-26 14:53:48,133 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 14:53:48,134 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-26 14:53:48,134 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-26 14:53:48,134 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
      "2021-07-26 14:53:48,135 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 14:53:48,135 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-26 14:53:48,136 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-26 14:53:48,136 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-26 14:53:48,136 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    69000: bleu:  25.02, loss: 46538.7344, ppl:   5.7715, duration: 42.3769s\n",
      "2021-07-26 14:54:17,129 - INFO - joeynmt.training - Epoch  25, Step:    69100, Batch Loss:     1.731152, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-07-26 14:54:45,851 - INFO - joeynmt.training - Epoch  25, Step:    69200, Batch Loss:     1.890201, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-07-26 14:55:14,593 - INFO - joeynmt.training - Epoch  25, Step:    69300, Batch Loss:     1.885194, Tokens per Sec:     7576, Lr: 0.000300\n",
      "2021-07-26 14:55:43,507 - INFO - joeynmt.training - Epoch  25, Step:    69400, Batch Loss:     2.154937, Tokens per Sec:     7526, Lr: 0.000300\n",
      "2021-07-26 14:56:07,193 - INFO - joeynmt.training - Epoch  25: total training loss 5384.95\n",
      "2021-07-26 14:56:07,193 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-07-26 14:56:12,285 - INFO - joeynmt.training - Epoch  26, Step:    69500, Batch Loss:     1.773978, Tokens per Sec:     6441, Lr: 0.000300\n",
      "2021-07-26 14:56:41,183 - INFO - joeynmt.training - Epoch  26, Step:    69600, Batch Loss:     1.991851, Tokens per Sec:     7438, Lr: 0.000300\n",
      "2021-07-26 14:57:10,107 - INFO - joeynmt.training - Epoch  26, Step:    69700, Batch Loss:     2.010680, Tokens per Sec:     7396, Lr: 0.000300\n",
      "2021-07-26 14:57:38,736 - INFO - joeynmt.training - Epoch  26, Step:    69800, Batch Loss:     1.564201, Tokens per Sec:     7379, Lr: 0.000300\n",
      "2021-07-26 14:58:07,489 - INFO - joeynmt.training - Epoch  26, Step:    69900, Batch Loss:     1.961138, Tokens per Sec:     7453, Lr: 0.000300\n",
      "2021-07-26 14:58:36,185 - INFO - joeynmt.training - Epoch  26, Step:    70000, Batch Loss:     1.952844, Tokens per Sec:     7530, Lr: 0.000300\n",
      "2021-07-26 14:59:04,978 - INFO - joeynmt.training - Epoch  26, Step:    70100, Batch Loss:     1.771576, Tokens per Sec:     7521, Lr: 0.000300\n",
      "2021-07-26 14:59:34,122 - INFO - joeynmt.training - Epoch  26, Step:    70200, Batch Loss:     2.011291, Tokens per Sec:     7469, Lr: 0.000300\n",
      "2021-07-26 15:00:03,119 - INFO - joeynmt.training - Epoch  26, Step:    70300, Batch Loss:     1.967120, Tokens per Sec:     7512, Lr: 0.000300\n",
      "2021-07-26 15:00:32,153 - INFO - joeynmt.training - Epoch  26, Step:    70400, Batch Loss:     2.005879, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-07-26 15:01:00,951 - INFO - joeynmt.training - Epoch  26, Step:    70500, Batch Loss:     1.884748, Tokens per Sec:     7365, Lr: 0.000300\n",
      "2021-07-26 15:01:29,980 - INFO - joeynmt.training - Epoch  26, Step:    70600, Batch Loss:     2.023139, Tokens per Sec:     7526, Lr: 0.000300\n",
      "2021-07-26 15:01:59,011 - INFO - joeynmt.training - Epoch  26, Step:    70700, Batch Loss:     1.907044, Tokens per Sec:     7486, Lr: 0.000300\n",
      "2021-07-26 15:02:27,787 - INFO - joeynmt.training - Epoch  26, Step:    70800, Batch Loss:     1.999691, Tokens per Sec:     7307, Lr: 0.000300\n",
      "2021-07-26 15:02:56,558 - INFO - joeynmt.training - Epoch  26, Step:    70900, Batch Loss:     2.063901, Tokens per Sec:     7420, Lr: 0.000300\n",
      "2021-07-26 15:03:25,617 - INFO - joeynmt.training - Epoch  26, Step:    71000, Batch Loss:     1.789774, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-07-26 15:03:54,093 - INFO - joeynmt.training - Epoch  26, Step:    71100, Batch Loss:     2.098505, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-26 15:04:22,947 - INFO - joeynmt.training - Epoch  26, Step:    71200, Batch Loss:     1.998517, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-07-26 15:04:51,654 - INFO - joeynmt.training - Epoch  26, Step:    71300, Batch Loss:     1.958778, Tokens per Sec:     7470, Lr: 0.000300\n",
      "2021-07-26 15:05:20,327 - INFO - joeynmt.training - Epoch  26, Step:    71400, Batch Loss:     1.857379, Tokens per Sec:     7553, Lr: 0.000300\n",
      "2021-07-26 15:05:48,983 - INFO - joeynmt.training - Epoch  26, Step:    71500, Batch Loss:     1.894884, Tokens per Sec:     7314, Lr: 0.000300\n",
      "2021-07-26 15:06:18,186 - INFO - joeynmt.training - Epoch  26, Step:    71600, Batch Loss:     1.896434, Tokens per Sec:     7586, Lr: 0.000300\n",
      "2021-07-26 15:06:47,218 - INFO - joeynmt.training - Epoch  26, Step:    71700, Batch Loss:     1.417143, Tokens per Sec:     7421, Lr: 0.000300\n",
      "2021-07-26 15:07:16,097 - INFO - joeynmt.training - Epoch  26, Step:    71800, Batch Loss:     2.063084, Tokens per Sec:     7418, Lr: 0.000300\n",
      "2021-07-26 15:07:45,047 - INFO - joeynmt.training - Epoch  26, Step:    71900, Batch Loss:     1.564284, Tokens per Sec:     7471, Lr: 0.000300\n",
      "2021-07-26 15:08:13,673 - INFO - joeynmt.training - Epoch  26, Step:    72000, Batch Loss:     1.800032, Tokens per Sec:     7497, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt1$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3uHyk_ECVtn",
    "outputId": "f70caa14-e51c-4319-8607-07ae401c4f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3000\tLoss: 88693.94531\tPPL: 28.24070\tbleu: 4.64948\tLR: 0.00030000\t*\n",
      "Steps: 6000\tLoss: 74349.93750\tPPL: 16.45254\tbleu: 9.56529\tLR: 0.00030000\t*\n",
      "Steps: 9000\tLoss: 67064.07031\tPPL: 12.50400\tbleu: 13.31115\tLR: 0.00030000\t*\n",
      "Steps: 12000\tLoss: 62883.14062\tPPL: 10.68210\tbleu: 16.21796\tLR: 0.00030000\t*\n",
      "Steps: 15000\tLoss: 59963.24609\tPPL: 9.56957\tbleu: 17.65447\tLR: 0.00030000\t*\n",
      "Steps: 18000\tLoss: 57535.53125\tPPL: 8.73332\tbleu: 18.88552\tLR: 0.00030000\t*\n",
      "Steps: 21000\tLoss: 55978.57422\tPPL: 8.23588\tbleu: 19.64789\tLR: 0.00030000\t*\n",
      "Steps: 24000\tLoss: 54671.21484\tPPL: 7.84014\tbleu: 20.36290\tLR: 0.00030000\t*\n",
      "Steps: 27000\tLoss: 53323.76562\tPPL: 7.45216\tbleu: 21.11758\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 52226.29688\tPPL: 7.15039\tbleu: 22.07949\tLR: 0.00030000\t*\n",
      "Steps: 33000\tLoss: 51357.41016\tPPL: 6.92016\tbleu: 22.60658\tLR: 0.00030000\t*\n",
      "Steps: 36000\tLoss: 51089.74609\tPPL: 6.85074\tbleu: 22.43566\tLR: 0.00030000\t*\n",
      "Steps: 39000\tLoss: 50230.04688\tPPL: 6.63246\tbleu: 22.83738\tLR: 0.00030000\t*\n",
      "Steps: 42000\tLoss: 49693.37891\tPPL: 6.49973\tbleu: 23.15323\tLR: 0.00030000\t*\n",
      "Steps: 45000\tLoss: 49138.16016\tPPL: 6.36521\tbleu: 23.45594\tLR: 0.00030000\t*\n",
      "Steps: 48000\tLoss: 48693.78906\tPPL: 6.25956\tbleu: 23.68504\tLR: 0.00030000\t*\n",
      "Steps: 51000\tLoss: 48376.37500\tPPL: 6.18517\tbleu: 23.91626\tLR: 0.00030000\t*\n",
      "Steps: 54000\tLoss: 48173.92188\tPPL: 6.13818\tbleu: 24.07678\tLR: 0.00030000\t*\n",
      "Steps: 57000\tLoss: 47796.12891\tPPL: 6.05145\tbleu: 24.48566\tLR: 0.00030000\t*\n",
      "Steps: 60000\tLoss: 47380.09766\tPPL: 5.95737\tbleu: 24.62420\tLR: 0.00030000\t*\n",
      "Steps: 63000\tLoss: 47135.61719\tPPL: 5.90276\tbleu: 24.86263\tLR: 0.00030000\t*\n",
      "Steps: 66000\tLoss: 46935.93750\tPPL: 5.85853\tbleu: 25.33918\tLR: 0.00030000\t*\n",
      "Steps: 69000\tLoss: 46538.73438\tPPL: 5.77153\tbleu: 25.02021\tLR: 0.00030000\t*\n",
      "Steps: 72000\tLoss: 46313.33984\tPPL: 5.72274\tbleu: 25.44434\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lgen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVchRnb3pv5x"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 72000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"joeynmt/models/lgen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lgen_reverse_transformer\"', f'model_dir: \"models/lgen_reverse_transformer2\"').replace(\n",
    "            f'epochs: 30', f'epochs: 4')\n",
    "with open(\"transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "lIxS8WNsnRQy",
    "outputId": "34fd85d3-17ca-43e8-c330-28da475e4a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lgen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lg\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer/72000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 3600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 4                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 3000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lgen_reverse_transformer2\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"transformer_lgen_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LG-p3cDfsMTq",
    "outputId": "93cadc87-df6e-4316-b2eb-998c06a46b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-27 08:55:21,463 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-27 08:55:21,504 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-27 08:55:26,948 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-27 08:55:27,661 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-27 08:55:28,719 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-27 08:55:29,812 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-27 08:55:29,812 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-27 08:55:30,032 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-27 08:55:30.206406: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-27 08:55:31,952 - INFO - joeynmt.training - Total params: 12152064\n",
      "2021-07-27 08:55:35,437 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer/72000.ckpt\n",
      "2021-07-27 08:55:46,371 - INFO - joeynmt.helpers - cfg.name                           : lgen_reverse_transformer\n",
      "2021-07-27 08:55:46,371 - INFO - joeynmt.helpers - cfg.data.src                       : lg\n",
      "2021-07-27 08:55:46,371 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-27 08:55:46,372 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\n",
      "2021-07-27 08:55:46,372 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\n",
      "2021-07-27 08:55:46,372 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\n",
      "2021-07-27 08:55:46,372 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-27 08:55:46,372 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-27 08:55:46,372 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-27 08:55:46,372 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-07-27 08:55:46,372 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-07-27 08:55:46,373 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-27 08:55:46,373 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-27 08:55:46,373 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer/72000.ckpt\n",
      "2021-07-27 08:55:46,373 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-27 08:55:46,373 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-27 08:55:46,373 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-27 08:55:46,373 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-27 08:55:46,373 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-27 08:55:46,374 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-27 08:55:46,375 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-27 08:55:46,375 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-27 08:55:46,375 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-27 08:55:46,375 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-27 08:55:46,375 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-27 08:55:46,375 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-27 08:55:46,375 - INFO - joeynmt.helpers - cfg.training.epochs                : 4\n",
      "2021-07-27 08:55:46,375 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 3000\n",
      "2021-07-27 08:55:46,376 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-27 08:55:46,376 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-27 08:55:46,376 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lgen_reverse_transformer2\n",
      "2021-07-27 08:55:46,376 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-27 08:55:46,376 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-27 08:55:46,376 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-27 08:55:46,376 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-27 08:55:46,376 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-27 08:55:46,377 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-27 08:55:46,377 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-27 08:55:46,377 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-27 08:55:46,377 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-27 08:55:46,377 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-27 08:55:46,377 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-27 08:55:46,377 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-27 08:55:46,378 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-27 08:55:46,378 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-27 08:55:46,378 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-27 08:55:46,378 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-27 08:55:46,378 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-27 08:55:46,378 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-27 08:55:46,378 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-27 08:55:46,378 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-27 08:55:46,379 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-27 08:55:46,379 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-27 08:55:46,379 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-27 08:55:46,379 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-27 08:55:46,379 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-27 08:55:46,379 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-27 08:55:46,379 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-27 08:55:46,380 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-27 08:55:46,380 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-27 08:55:46,380 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-27 08:55:46,380 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-27 08:55:46,380 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 225657,\n",
      "\tvalid 1000,\n",
      "\ttest 2692\n",
      "2021-07-27 08:55:46,380 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-07-27 08:55:46,381 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-07-27 08:55:46,381 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-07-27 08:55:46,381 - INFO - joeynmt.helpers - Number of Src words (types): 4265\n",
      "2021-07-27 08:55:46,381 - INFO - joeynmt.helpers - Number of Trg words (types): 4265\n",
      "2021-07-27 08:55:46,381 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4265),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4265))\n",
      "2021-07-27 08:55:46,392 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-27 08:55:46,393 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-27 08:56:00,429 - INFO - joeynmt.training - Epoch   1, Step:    72100, Batch Loss:     1.681960, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-07-27 08:56:13,028 - INFO - joeynmt.training - Epoch   1, Step:    72200, Batch Loss:     1.281609, Tokens per Sec:    16805, Lr: 0.000300\n",
      "2021-07-27 08:56:20,964 - INFO - joeynmt.training - Epoch   1: total training loss 503.71\n",
      "2021-07-27 08:56:20,965 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-27 08:56:26,119 - INFO - joeynmt.training - Epoch   2, Step:    72300, Batch Loss:     2.166788, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-07-27 08:56:39,069 - INFO - joeynmt.training - Epoch   2, Step:    72400, Batch Loss:     1.975562, Tokens per Sec:    16540, Lr: 0.000300\n",
      "2021-07-27 08:56:52,342 - INFO - joeynmt.training - Epoch   2, Step:    72500, Batch Loss:     2.088135, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-07-27 08:57:05,639 - INFO - joeynmt.training - Epoch   2, Step:    72600, Batch Loss:     1.749137, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-07-27 08:57:19,142 - INFO - joeynmt.training - Epoch   2, Step:    72700, Batch Loss:     1.798418, Tokens per Sec:    16254, Lr: 0.000300\n",
      "2021-07-27 08:57:32,788 - INFO - joeynmt.training - Epoch   2, Step:    72800, Batch Loss:     2.015365, Tokens per Sec:    15671, Lr: 0.000300\n",
      "2021-07-27 08:57:46,463 - INFO - joeynmt.training - Epoch   2, Step:    72900, Batch Loss:     2.021715, Tokens per Sec:    16224, Lr: 0.000300\n",
      "2021-07-27 08:57:59,885 - INFO - joeynmt.training - Epoch   2, Step:    73000, Batch Loss:     1.897434, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-07-27 08:58:13,263 - INFO - joeynmt.training - Epoch   2, Step:    73100, Batch Loss:     2.046471, Tokens per Sec:    16139, Lr: 0.000300\n",
      "2021-07-27 08:58:26,744 - INFO - joeynmt.training - Epoch   2, Step:    73200, Batch Loss:     1.978917, Tokens per Sec:    15668, Lr: 0.000300\n",
      "2021-07-27 08:58:40,218 - INFO - joeynmt.training - Epoch   2, Step:    73300, Batch Loss:     1.886358, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-07-27 08:58:53,807 - INFO - joeynmt.training - Epoch   2, Step:    73400, Batch Loss:     1.913561, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-07-27 08:59:07,292 - INFO - joeynmt.training - Epoch   2, Step:    73500, Batch Loss:     2.124382, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-07-27 08:59:20,830 - INFO - joeynmt.training - Epoch   2, Step:    73600, Batch Loss:     1.778959, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-07-27 08:59:34,457 - INFO - joeynmt.training - Epoch   2, Step:    73700, Batch Loss:     1.893860, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-07-27 08:59:47,822 - INFO - joeynmt.training - Epoch   2, Step:    73800, Batch Loss:     1.779484, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-07-27 09:00:01,192 - INFO - joeynmt.training - Epoch   2, Step:    73900, Batch Loss:     2.051799, Tokens per Sec:    15676, Lr: 0.000300\n",
      "2021-07-27 09:00:14,848 - INFO - joeynmt.training - Epoch   2, Step:    74000, Batch Loss:     1.875596, Tokens per Sec:    16527, Lr: 0.000300\n",
      "2021-07-27 09:00:28,168 - INFO - joeynmt.training - Epoch   2, Step:    74100, Batch Loss:     2.221804, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-07-27 09:00:41,660 - INFO - joeynmt.training - Epoch   2, Step:    74200, Batch Loss:     2.047357, Tokens per Sec:    15769, Lr: 0.000300\n",
      "2021-07-27 09:00:55,083 - INFO - joeynmt.training - Epoch   2, Step:    74300, Batch Loss:     1.490640, Tokens per Sec:    15577, Lr: 0.000300\n",
      "2021-07-27 09:01:08,786 - INFO - joeynmt.training - Epoch   2, Step:    74400, Batch Loss:     1.984680, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-27 09:01:22,147 - INFO - joeynmt.training - Epoch   2, Step:    74500, Batch Loss:     2.078059, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-07-27 09:01:35,560 - INFO - joeynmt.training - Epoch   2, Step:    74600, Batch Loss:     2.057964, Tokens per Sec:    15850, Lr: 0.000300\n",
      "2021-07-27 09:01:49,137 - INFO - joeynmt.training - Epoch   2, Step:    74700, Batch Loss:     1.804303, Tokens per Sec:    15848, Lr: 0.000300\n",
      "2021-07-27 09:02:02,458 - INFO - joeynmt.training - Epoch   2, Step:    74800, Batch Loss:     1.751212, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-07-27 09:02:16,145 - INFO - joeynmt.training - Epoch   2, Step:    74900, Batch Loss:     1.988706, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-07-27 09:02:29,712 - INFO - joeynmt.training - Epoch   2, Step:    75000, Batch Loss:     2.093180, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-07-27 09:02:47,915 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 09:02:47,916 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 09:02:47,916 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 09:02:48,144 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 09:02:48,144 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 09:02:48,886 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 09:02:48,886 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 09:02:48,887 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 09:02:48,887 - INFO - joeynmt.training - \tHypothesis: I have hated you when you do good and suffer when you endure , that is acceptable to God . ”\n",
      "2021-07-27 09:02:48,887 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 09:02:48,887 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 09:02:48,888 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 09:02:48,888 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 09:02:48,888 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 09:02:48,888 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 09:02:48,888 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 09:02:48,889 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
      "2021-07-27 09:02:48,889 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 09:02:48,889 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 09:02:48,889 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 09:02:48,890 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 09:02:48,890 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    75000: bleu:  25.55, loss: 46264.7070, ppl:   5.7123, duration: 19.1778s\n",
      "2021-07-27 09:02:54,424 - INFO - joeynmt.training - Epoch   2: total training loss 5314.40\n",
      "2021-07-27 09:02:54,424 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-27 09:03:02,704 - INFO - joeynmt.training - Epoch   3, Step:    75100, Batch Loss:     2.082451, Tokens per Sec:    15068, Lr: 0.000300\n",
      "2021-07-27 09:03:16,378 - INFO - joeynmt.training - Epoch   3, Step:    75200, Batch Loss:     1.985077, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-07-27 09:03:29,911 - INFO - joeynmt.training - Epoch   3, Step:    75300, Batch Loss:     1.879265, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-07-27 09:03:43,409 - INFO - joeynmt.training - Epoch   3, Step:    75400, Batch Loss:     1.958075, Tokens per Sec:    15674, Lr: 0.000300\n",
      "2021-07-27 09:03:56,777 - INFO - joeynmt.training - Epoch   3, Step:    75500, Batch Loss:     1.872803, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-07-27 09:04:10,144 - INFO - joeynmt.training - Epoch   3, Step:    75600, Batch Loss:     2.034488, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-07-27 09:04:23,680 - INFO - joeynmt.training - Epoch   3, Step:    75700, Batch Loss:     2.101898, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-07-27 09:04:37,257 - INFO - joeynmt.training - Epoch   3, Step:    75800, Batch Loss:     1.947219, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-07-27 09:04:50,624 - INFO - joeynmt.training - Epoch   3, Step:    75900, Batch Loss:     1.474759, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-07-27 09:05:04,184 - INFO - joeynmt.training - Epoch   3, Step:    76000, Batch Loss:     1.489094, Tokens per Sec:    16454, Lr: 0.000300\n",
      "2021-07-27 09:05:17,623 - INFO - joeynmt.training - Epoch   3, Step:    76100, Batch Loss:     2.017215, Tokens per Sec:    16017, Lr: 0.000300\n",
      "2021-07-27 09:05:31,255 - INFO - joeynmt.training - Epoch   3, Step:    76200, Batch Loss:     1.845311, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-07-27 09:05:44,926 - INFO - joeynmt.training - Epoch   3, Step:    76300, Batch Loss:     2.205949, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-07-27 09:05:58,409 - INFO - joeynmt.training - Epoch   3, Step:    76400, Batch Loss:     1.938946, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-07-27 09:06:11,770 - INFO - joeynmt.training - Epoch   3, Step:    76500, Batch Loss:     1.957024, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-07-27 09:06:25,121 - INFO - joeynmt.training - Epoch   3, Step:    76600, Batch Loss:     1.743345, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-07-27 09:06:38,614 - INFO - joeynmt.training - Epoch   3, Step:    76700, Batch Loss:     2.073858, Tokens per Sec:    15810, Lr: 0.000300\n",
      "2021-07-27 09:06:52,261 - INFO - joeynmt.training - Epoch   3, Step:    76800, Batch Loss:     2.025047, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-07-27 09:07:05,610 - INFO - joeynmt.training - Epoch   3, Step:    76900, Batch Loss:     1.881355, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-27 09:07:19,069 - INFO - joeynmt.training - Epoch   3, Step:    77000, Batch Loss:     1.729409, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-07-27 09:07:32,497 - INFO - joeynmt.training - Epoch   3, Step:    77100, Batch Loss:     1.894880, Tokens per Sec:    15956, Lr: 0.000300\n",
      "2021-07-27 09:07:46,123 - INFO - joeynmt.training - Epoch   3, Step:    77200, Batch Loss:     1.973187, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-07-27 09:07:59,718 - INFO - joeynmt.training - Epoch   3, Step:    77300, Batch Loss:     1.906461, Tokens per Sec:    16157, Lr: 0.000300\n",
      "2021-07-27 09:08:13,219 - INFO - joeynmt.training - Epoch   3, Step:    77400, Batch Loss:     1.895287, Tokens per Sec:    15891, Lr: 0.000300\n",
      "2021-07-27 09:08:26,611 - INFO - joeynmt.training - Epoch   3, Step:    77500, Batch Loss:     1.952480, Tokens per Sec:    15419, Lr: 0.000300\n",
      "2021-07-27 09:08:40,035 - INFO - joeynmt.training - Epoch   3, Step:    77600, Batch Loss:     1.902821, Tokens per Sec:    16027, Lr: 0.000300\n",
      "2021-07-27 09:08:53,437 - INFO - joeynmt.training - Epoch   3, Step:    77700, Batch Loss:     1.892357, Tokens per Sec:    15838, Lr: 0.000300\n",
      "2021-07-27 09:09:06,799 - INFO - joeynmt.training - Epoch   3, Step:    77800, Batch Loss:     1.945234, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-07-27 09:09:09,518 - INFO - joeynmt.training - Epoch   3: total training loss 5288.77\n",
      "2021-07-27 09:09:09,519 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-27 09:09:20,693 - INFO - joeynmt.training - Epoch   4, Step:    77900, Batch Loss:     2.002499, Tokens per Sec:    15475, Lr: 0.000300\n",
      "2021-07-27 09:09:34,345 - INFO - joeynmt.training - Epoch   4, Step:    78000, Batch Loss:     2.031312, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-07-27 09:09:52,828 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 09:09:52,829 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 09:09:52,829 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 09:09:53,060 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 09:09:53,060 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 09:09:54,192 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 09:09:54,193 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 09:09:54,193 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 09:09:54,193 - INFO - joeynmt.training - \tHypothesis: [ I ] hated what is good and suffering when you endure , that is acceptable to God . ”\n",
      "2021-07-27 09:09:54,194 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 09:09:54,194 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 09:09:54,194 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 09:09:54,194 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 09:09:54,194 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 09:09:54,195 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 09:09:54,195 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 09:09:54,195 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
      "2021-07-27 09:09:54,195 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 09:09:54,196 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 09:09:54,196 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 09:09:54,196 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 09:09:54,196 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    78000: bleu:  25.66, loss: 45841.7773, ppl:   5.6220, duration: 19.8507s\n",
      "2021-07-27 09:10:07,528 - INFO - joeynmt.training - Epoch   4, Step:    78100, Batch Loss:     1.756957, Tokens per Sec:    15467, Lr: 0.000300\n",
      "2021-07-27 09:10:21,090 - INFO - joeynmt.training - Epoch   4, Step:    78200, Batch Loss:     1.686697, Tokens per Sec:    16362, Lr: 0.000300\n",
      "2021-07-27 09:10:34,714 - INFO - joeynmt.training - Epoch   4, Step:    78300, Batch Loss:     1.938942, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-07-27 09:10:48,409 - INFO - joeynmt.training - Epoch   4, Step:    78400, Batch Loss:     2.080640, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-07-27 09:11:01,876 - INFO - joeynmt.training - Epoch   4, Step:    78500, Batch Loss:     1.911482, Tokens per Sec:    15790, Lr: 0.000300\n",
      "2021-07-27 09:11:15,479 - INFO - joeynmt.training - Epoch   4, Step:    78600, Batch Loss:     2.069764, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-07-27 09:11:28,949 - INFO - joeynmt.training - Epoch   4, Step:    78700, Batch Loss:     1.942305, Tokens per Sec:    15760, Lr: 0.000300\n",
      "2021-07-27 09:11:42,421 - INFO - joeynmt.training - Epoch   4, Step:    78800, Batch Loss:     1.926644, Tokens per Sec:    15618, Lr: 0.000300\n",
      "2021-07-27 09:11:55,942 - INFO - joeynmt.training - Epoch   4, Step:    78900, Batch Loss:     1.893911, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-07-27 09:12:09,371 - INFO - joeynmt.training - Epoch   4, Step:    79000, Batch Loss:     1.909721, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-07-27 09:12:22,765 - INFO - joeynmt.training - Epoch   4, Step:    79100, Batch Loss:     2.047141, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-07-27 09:12:36,340 - INFO - joeynmt.training - Epoch   4, Step:    79200, Batch Loss:     1.813623, Tokens per Sec:    15673, Lr: 0.000300\n",
      "2021-07-27 09:12:49,914 - INFO - joeynmt.training - Epoch   4, Step:    79300, Batch Loss:     1.904846, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-07-27 09:13:03,474 - INFO - joeynmt.training - Epoch   4, Step:    79400, Batch Loss:     2.028773, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-07-27 09:13:17,120 - INFO - joeynmt.training - Epoch   4, Step:    79500, Batch Loss:     1.864373, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-07-27 09:13:30,769 - INFO - joeynmt.training - Epoch   4, Step:    79600, Batch Loss:     1.921424, Tokens per Sec:    15937, Lr: 0.000300\n",
      "2021-07-27 09:13:44,132 - INFO - joeynmt.training - Epoch   4, Step:    79700, Batch Loss:     1.858502, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-07-27 09:13:57,504 - INFO - joeynmt.training - Epoch   4, Step:    79800, Batch Loss:     1.960669, Tokens per Sec:    16130, Lr: 0.000300\n",
      "2021-07-27 09:14:11,003 - INFO - joeynmt.training - Epoch   4, Step:    79900, Batch Loss:     1.871122, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-07-27 09:14:24,578 - INFO - joeynmt.training - Epoch   4, Step:    80000, Batch Loss:     1.813506, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-07-27 09:14:38,092 - INFO - joeynmt.training - Epoch   4, Step:    80100, Batch Loss:     2.070484, Tokens per Sec:    15809, Lr: 0.000300\n",
      "2021-07-27 09:14:51,468 - INFO - joeynmt.training - Epoch   4, Step:    80200, Batch Loss:     1.690321, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-27 09:15:04,642 - INFO - joeynmt.training - Epoch   4, Step:    80300, Batch Loss:     1.947034, Tokens per Sec:    15787, Lr: 0.000300\n",
      "2021-07-27 09:15:18,099 - INFO - joeynmt.training - Epoch   4, Step:    80400, Batch Loss:     2.071176, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-07-27 09:15:31,758 - INFO - joeynmt.training - Epoch   4, Step:    80500, Batch Loss:     1.768524, Tokens per Sec:    15929, Lr: 0.000300\n",
      "2021-07-27 09:15:45,229 - INFO - joeynmt.training - Epoch   4, Step:    80600, Batch Loss:     1.986798, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-07-27 09:15:45,512 - INFO - joeynmt.training - Epoch   4: total training loss 5272.81\n",
      "2021-07-27 09:15:45,512 - INFO - joeynmt.training - Training ended after   4 epochs.\n",
      "2021-07-27 09:15:45,513 - INFO - joeynmt.training - Best validation result (greedy) at step    78000:   5.62 ppl.\n",
      "2021-07-27 09:15:45,533 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-27 09:15:45,896 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-27 09:15:46,085 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-27 09:15:46,150 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe.en)...\n",
      "2021-07-27 09:16:06,688 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 09:16:06,689 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 09:16:06,689 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 09:16:06,912 - INFO - joeynmt.prediction -  dev bleu[13a]:  26.42 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-27 09:16:06,920 - INFO - joeynmt.prediction - Translations saved to: models/lgen_reverse_transformer2/00078000.hyps.dev\n",
      "2021-07-27 09:16:06,923 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe.en)...\n",
      "2021-07-27 09:16:46,424 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 09:16:46,425 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 09:16:46,425 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 09:16:46,992 - INFO - joeynmt.prediction - test bleu[13a]:  35.85 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-27 09:16:46,997 - INFO - joeynmt.prediction - Translations saved to: models/lgen_reverse_transformer2/00078000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "!python -m joeynmt train transformer_lgen_reload.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WcEqWfv_woS"
   },
   "source": [
    "During testing we achieve a dev set BLEU score of 26.42 and a test set BLEU score of 35.85. This is very good as the model is not overfitting during training and we see good results on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2EXlLzSlFnC"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 190000\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"joeynmt/models/lgen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/models/{name}_reverse_transformer2_continued/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lgen_reverse_transformer2\"', f'model_dir: \"models/lgen_reverse_transformer2_continued2\"').replace(\n",
    "        f'epochs: 30', f'epochs: 5')\n",
    "with open(\"joeynmt/configs/transformer_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "iD1CFfKjm9_I",
    "outputId": "e86924eb-01f0-4ffc-d028-c689396278aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lgen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lg\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/models/lgen_reverse_transformer2_continued/190000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 3600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 5                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 3000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lgen_reverse_transformer2_continued2\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_lgen_reload2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHdN00MolFjf",
    "outputId": "4ed94c40-cc68-4ec0-c4ba-1b9949f15482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-27 09:39:36,423 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-27 09:39:36,464 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-27 09:39:40,778 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-27 09:39:41,063 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-27 09:39:41,093 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-27 09:39:41,132 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-27 09:39:41,132 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-27 09:39:41,352 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-27 09:39:41.520384: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-27 09:39:42,682 - INFO - joeynmt.training - Total params: 12152064\n",
      "2021-07-27 09:39:46,130 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer2/78000.ckpt\n",
      "2021-07-27 09:39:46,574 - INFO - joeynmt.helpers - cfg.name                           : lgen_reverse_transformer\n",
      "2021-07-27 09:39:46,574 - INFO - joeynmt.helpers - cfg.data.src                       : lg\n",
      "2021-07-27 09:39:46,574 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-27 09:39:46,575 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\n",
      "2021-07-27 09:39:46,575 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\n",
      "2021-07-27 09:39:46,575 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\n",
      "2021-07-27 09:39:46,575 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-27 09:39:46,575 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-27 09:39:46,575 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-27 09:39:46,576 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-07-27 09:39:46,576 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\n",
      "2021-07-27 09:39:46,576 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-27 09:39:46,576 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-27 09:39:46,576 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/lgen_reverse_transformer2/78000.ckpt\n",
      "2021-07-27 09:39:46,576 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-27 09:39:46,577 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-27 09:39:46,577 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-27 09:39:46,577 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-27 09:39:46,577 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-27 09:39:46,577 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-27 09:39:46,577 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-27 09:39:46,578 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-27 09:39:46,578 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-27 09:39:46,578 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-27 09:39:46,578 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-27 09:39:46,578 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-27 09:39:46,578 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-27 09:39:46,578 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-27 09:39:46,579 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-27 09:39:46,579 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-27 09:39:46,579 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-27 09:39:46,579 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-27 09:39:46,579 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-27 09:39:46,579 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-27 09:39:46,580 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-27 09:39:46,580 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 3000\n",
      "2021-07-27 09:39:46,580 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-27 09:39:46,580 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-27 09:39:46,580 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lgen_reverse_transformer2_continued\n",
      "2021-07-27 09:39:46,580 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-27 09:39:46,581 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-27 09:39:46,581 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-27 09:39:46,581 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-27 09:39:46,581 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-27 09:39:46,581 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-27 09:39:46,581 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-27 09:39:46,581 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-27 09:39:46,582 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-27 09:39:46,582 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-27 09:39:46,582 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-27 09:39:46,582 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-27 09:39:46,582 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-27 09:39:46,583 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-27 09:39:46,583 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-27 09:39:46,583 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-27 09:39:46,583 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-27 09:39:46,583 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-27 09:39:46,584 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-27 09:39:46,584 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-27 09:39:46,584 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-27 09:39:46,584 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-27 09:39:46,584 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-27 09:39:46,584 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-27 09:39:46,584 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-27 09:39:46,585 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-27 09:39:46,585 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-27 09:39:46,585 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-27 09:39:46,585 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-27 09:39:46,585 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-27 09:39:46,585 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-27 09:39:46,585 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 225657,\n",
      "\tvalid 1000,\n",
      "\ttest 2692\n",
      "2021-07-27 09:39:46,586 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
      "\t[TRG] Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
      "2021-07-27 09:39:46,586 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-07-27 09:39:46,586 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
      "2021-07-27 09:39:46,586 - INFO - joeynmt.helpers - Number of Src words (types): 4265\n",
      "2021-07-27 09:39:46,586 - INFO - joeynmt.helpers - Number of Trg words (types): 4265\n",
      "2021-07-27 09:39:46,587 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4265),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4265))\n",
      "2021-07-27 09:39:46,598 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-27 09:39:46,598 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-27 09:39:59,319 - INFO - joeynmt.training - Epoch   1, Step:    78100, Batch Loss:     1.771901, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-07-27 09:40:12,080 - INFO - joeynmt.training - Epoch   1, Step:    78200, Batch Loss:     1.687671, Tokens per Sec:    17387, Lr: 0.000300\n",
      "2021-07-27 09:40:24,818 - INFO - joeynmt.training - Epoch   1, Step:    78300, Batch Loss:     1.916204, Tokens per Sec:    17048, Lr: 0.000300\n",
      "2021-07-27 09:40:37,728 - INFO - joeynmt.training - Epoch   1, Step:    78400, Batch Loss:     2.105756, Tokens per Sec:    16806, Lr: 0.000300\n",
      "2021-07-27 09:40:50,554 - INFO - joeynmt.training - Epoch   1, Step:    78500, Batch Loss:     1.919751, Tokens per Sec:    16580, Lr: 0.000300\n",
      "2021-07-27 09:41:03,732 - INFO - joeynmt.training - Epoch   1, Step:    78600, Batch Loss:     2.068482, Tokens per Sec:    16705, Lr: 0.000300\n",
      "2021-07-27 09:41:16,968 - INFO - joeynmt.training - Epoch   1, Step:    78700, Batch Loss:     1.954973, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-07-27 09:41:30,301 - INFO - joeynmt.training - Epoch   1, Step:    78800, Batch Loss:     1.895321, Tokens per Sec:    15780, Lr: 0.000300\n",
      "2021-07-27 09:41:43,867 - INFO - joeynmt.training - Epoch   1, Step:    78900, Batch Loss:     1.867186, Tokens per Sec:    15931, Lr: 0.000300\n",
      "2021-07-27 09:41:57,367 - INFO - joeynmt.training - Epoch   1, Step:    79000, Batch Loss:     1.936409, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-07-27 09:42:10,689 - INFO - joeynmt.training - Epoch   1, Step:    79100, Batch Loss:     2.052621, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-07-27 09:42:24,145 - INFO - joeynmt.training - Epoch   1, Step:    79200, Batch Loss:     1.810053, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-07-27 09:42:37,484 - INFO - joeynmt.training - Epoch   1, Step:    79300, Batch Loss:     1.929774, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-07-27 09:42:50,883 - INFO - joeynmt.training - Epoch   1, Step:    79400, Batch Loss:     1.973416, Tokens per Sec:    16152, Lr: 0.000300\n",
      "2021-07-27 09:43:04,300 - INFO - joeynmt.training - Epoch   1, Step:    79500, Batch Loss:     1.879976, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-07-27 09:43:17,831 - INFO - joeynmt.training - Epoch   1, Step:    79600, Batch Loss:     1.931682, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-07-27 09:43:31,394 - INFO - joeynmt.training - Epoch   1, Step:    79700, Batch Loss:     1.847640, Tokens per Sec:    15555, Lr: 0.000300\n",
      "2021-07-27 09:43:44,940 - INFO - joeynmt.training - Epoch   1, Step:    79800, Batch Loss:     1.959608, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-07-27 09:43:58,417 - INFO - joeynmt.training - Epoch   1, Step:    79900, Batch Loss:     1.874038, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-07-27 09:44:11,742 - INFO - joeynmt.training - Epoch   1, Step:    80000, Batch Loss:     1.823063, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-07-27 09:44:25,066 - INFO - joeynmt.training - Epoch   1, Step:    80100, Batch Loss:     2.058033, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-07-27 09:44:38,561 - INFO - joeynmt.training - Epoch   1, Step:    80200, Batch Loss:     1.706213, Tokens per Sec:    15773, Lr: 0.000300\n",
      "2021-07-27 09:44:51,935 - INFO - joeynmt.training - Epoch   1, Step:    80300, Batch Loss:     1.992903, Tokens per Sec:    15553, Lr: 0.000300\n",
      "2021-07-27 09:45:05,422 - INFO - joeynmt.training - Epoch   1, Step:    80400, Batch Loss:     2.077296, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-07-27 09:45:18,917 - INFO - joeynmt.training - Epoch   1, Step:    80500, Batch Loss:     1.736446, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-07-27 09:45:32,303 - INFO - joeynmt.training - Epoch   1, Step:    80600, Batch Loss:     1.973639, Tokens per Sec:    16194, Lr: 0.000300\n",
      "2021-07-27 09:45:32,599 - INFO - joeynmt.training - Epoch   1: total training loss 4932.54\n",
      "2021-07-27 09:45:32,600 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-27 09:45:46,212 - INFO - joeynmt.training - Epoch   2, Step:    80700, Batch Loss:     1.826377, Tokens per Sec:    15416, Lr: 0.000300\n",
      "2021-07-27 09:45:59,592 - INFO - joeynmt.training - Epoch   2, Step:    80800, Batch Loss:     2.195592, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-07-27 09:46:13,015 - INFO - joeynmt.training - Epoch   2, Step:    80900, Batch Loss:     2.022226, Tokens per Sec:    15725, Lr: 0.000300\n",
      "2021-07-27 09:46:26,638 - INFO - joeynmt.training - Epoch   2, Step:    81000, Batch Loss:     2.139182, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-07-27 09:46:43,895 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 09:46:43,895 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 09:46:43,896 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 09:46:44,117 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 09:46:44,117 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 09:46:44,872 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 09:46:44,872 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 09:46:44,872 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 09:46:44,873 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is pleased to God . ”\n",
      "2021-07-27 09:46:44,873 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 09:46:44,873 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 09:46:44,873 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 09:46:44,874 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 09:46:44,874 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 09:46:44,874 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 09:46:44,874 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 09:46:44,875 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 09:46:44,875 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 09:46:44,875 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 09:46:44,875 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 09:46:44,875 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
      "2021-07-27 09:46:44,876 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    81000: bleu:  25.60, loss: 45706.1289, ppl:   5.5933, duration: 18.2372s\n",
      "2021-07-27 09:46:58,536 - INFO - joeynmt.training - Epoch   2, Step:    81100, Batch Loss:     1.487473, Tokens per Sec:    15719, Lr: 0.000300\n",
      "2021-07-27 09:47:11,737 - INFO - joeynmt.training - Epoch   2, Step:    81200, Batch Loss:     1.703943, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-07-27 09:47:25,103 - INFO - joeynmt.training - Epoch   2, Step:    81300, Batch Loss:     1.887466, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-07-27 09:47:38,504 - INFO - joeynmt.training - Epoch   2, Step:    81400, Batch Loss:     1.922815, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-07-27 09:47:51,819 - INFO - joeynmt.training - Epoch   2, Step:    81500, Batch Loss:     2.091430, Tokens per Sec:    16449, Lr: 0.000300\n",
      "2021-07-27 09:48:05,316 - INFO - joeynmt.training - Epoch   2, Step:    81600, Batch Loss:     1.956631, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-07-27 09:48:18,712 - INFO - joeynmt.training - Epoch   2, Step:    81700, Batch Loss:     2.162307, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-07-27 09:48:32,336 - INFO - joeynmt.training - Epoch   2, Step:    81800, Batch Loss:     1.858948, Tokens per Sec:    15919, Lr: 0.000300\n",
      "2021-07-27 09:48:45,589 - INFO - joeynmt.training - Epoch   2, Step:    81900, Batch Loss:     1.782452, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-07-27 09:48:58,991 - INFO - joeynmt.training - Epoch   2, Step:    82000, Batch Loss:     1.950491, Tokens per Sec:    15865, Lr: 0.000300\n",
      "2021-07-27 09:49:12,342 - INFO - joeynmt.training - Epoch   2, Step:    82100, Batch Loss:     1.879787, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-07-27 09:49:25,873 - INFO - joeynmt.training - Epoch   2, Step:    82200, Batch Loss:     2.036339, Tokens per Sec:    16352, Lr: 0.000300\n",
      "2021-07-27 09:49:39,364 - INFO - joeynmt.training - Epoch   2, Step:    82300, Batch Loss:     1.885666, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-07-27 09:49:52,881 - INFO - joeynmt.training - Epoch   2, Step:    82400, Batch Loss:     1.782582, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-07-27 09:50:06,273 - INFO - joeynmt.training - Epoch   2, Step:    82500, Batch Loss:     1.856621, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-07-27 09:50:19,738 - INFO - joeynmt.training - Epoch   2, Step:    82600, Batch Loss:     1.833734, Tokens per Sec:    16265, Lr: 0.000300\n",
      "2021-07-27 09:50:33,237 - INFO - joeynmt.training - Epoch   2, Step:    82700, Batch Loss:     1.738319, Tokens per Sec:    15671, Lr: 0.000300\n",
      "2021-07-27 09:50:46,892 - INFO - joeynmt.training - Epoch   2, Step:    82800, Batch Loss:     2.065983, Tokens per Sec:    16124, Lr: 0.000300\n",
      "2021-07-27 09:51:00,353 - INFO - joeynmt.training - Epoch   2, Step:    82900, Batch Loss:     1.834601, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-07-27 09:51:13,719 - INFO - joeynmt.training - Epoch   2, Step:    83000, Batch Loss:     1.909832, Tokens per Sec:    15669, Lr: 0.000300\n",
      "2021-07-27 09:51:27,303 - INFO - joeynmt.training - Epoch   2, Step:    83100, Batch Loss:     1.858315, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-07-27 09:51:40,695 - INFO - joeynmt.training - Epoch   2, Step:    83200, Batch Loss:     1.938998, Tokens per Sec:    16143, Lr: 0.000300\n",
      "2021-07-27 09:51:54,117 - INFO - joeynmt.training - Epoch   2, Step:    83300, Batch Loss:     1.461923, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-07-27 09:52:05,803 - INFO - joeynmt.training - Epoch   2: total training loss 5254.42\n",
      "2021-07-27 09:52:05,804 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-27 09:52:07,837 - INFO - joeynmt.training - Epoch   3, Step:    83400, Batch Loss:     1.663681, Tokens per Sec:    13558, Lr: 0.000300\n",
      "2021-07-27 09:52:21,280 - INFO - joeynmt.training - Epoch   3, Step:    83500, Batch Loss:     1.874090, Tokens per Sec:    15535, Lr: 0.000300\n",
      "2021-07-27 09:52:34,835 - INFO - joeynmt.training - Epoch   3, Step:    83600, Batch Loss:     2.177340, Tokens per Sec:    16164, Lr: 0.000300\n",
      "2021-07-27 09:52:48,229 - INFO - joeynmt.training - Epoch   3, Step:    83700, Batch Loss:     1.967757, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-07-27 09:53:01,628 - INFO - joeynmt.training - Epoch   3, Step:    83800, Batch Loss:     2.105916, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-07-27 09:53:15,085 - INFO - joeynmt.training - Epoch   3, Step:    83900, Batch Loss:     2.153764, Tokens per Sec:    16405, Lr: 0.000300\n",
      "2021-07-27 09:53:28,606 - INFO - joeynmt.training - Epoch   3, Step:    84000, Batch Loss:     1.898441, Tokens per Sec:    15600, Lr: 0.000300\n",
      "2021-07-27 09:53:46,815 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 09:53:46,815 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 09:53:46,815 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 09:53:47,037 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 09:53:47,038 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 09:53:48,185 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 09:53:48,186 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 09:53:48,187 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 09:53:48,187 - INFO - joeynmt.training - \tHypothesis: I also hated what you do good and suffer when you endure , that is acceptable to God . ”\n",
      "2021-07-27 09:53:48,187 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 09:53:48,188 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 09:53:48,188 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 09:53:48,188 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son whom I am approved . ”\n",
      "2021-07-27 09:53:48,188 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 09:53:48,189 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 09:53:48,189 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 09:53:48,189 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 09:53:48,189 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 09:53:48,190 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 09:53:48,190 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 09:53:48,190 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 09:53:48,190 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    84000: bleu:  25.61, loss: 45610.5156, ppl:   5.5732, duration: 19.5834s\n",
      "2021-07-27 09:54:01,719 - INFO - joeynmt.training - Epoch   3, Step:    84100, Batch Loss:     1.977021, Tokens per Sec:    15995, Lr: 0.000300\n",
      "2021-07-27 09:54:15,034 - INFO - joeynmt.training - Epoch   3, Step:    84200, Batch Loss:     1.798666, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-07-27 09:54:28,650 - INFO - joeynmt.training - Epoch   3, Step:    84300, Batch Loss:     1.884733, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-07-27 09:54:42,127 - INFO - joeynmt.training - Epoch   3, Step:    84400, Batch Loss:     2.095305, Tokens per Sec:    15434, Lr: 0.000300\n",
      "2021-07-27 09:54:55,419 - INFO - joeynmt.training - Epoch   3, Step:    84500, Batch Loss:     2.024796, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-07-27 09:55:08,684 - INFO - joeynmt.training - Epoch   3, Step:    84600, Batch Loss:     2.277787, Tokens per Sec:    16463, Lr: 0.000300\n",
      "2021-07-27 09:55:22,187 - INFO - joeynmt.training - Epoch   3, Step:    84700, Batch Loss:     1.621281, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-07-27 09:55:35,728 - INFO - joeynmt.training - Epoch   3, Step:    84800, Batch Loss:     1.752738, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-07-27 09:55:49,200 - INFO - joeynmt.training - Epoch   3, Step:    84900, Batch Loss:     1.441631, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-07-27 09:56:02,567 - INFO - joeynmt.training - Epoch   3, Step:    85000, Batch Loss:     1.807938, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-07-27 09:56:15,966 - INFO - joeynmt.training - Epoch   3, Step:    85100, Batch Loss:     2.152285, Tokens per Sec:    16367, Lr: 0.000300\n",
      "2021-07-27 09:56:29,374 - INFO - joeynmt.training - Epoch   3, Step:    85200, Batch Loss:     1.987384, Tokens per Sec:    16313, Lr: 0.000300\n",
      "2021-07-27 09:56:42,736 - INFO - joeynmt.training - Epoch   3, Step:    85300, Batch Loss:     1.753036, Tokens per Sec:    15768, Lr: 0.000300\n",
      "2021-07-27 09:56:56,332 - INFO - joeynmt.training - Epoch   3, Step:    85400, Batch Loss:     1.797681, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-07-27 09:57:09,867 - INFO - joeynmt.training - Epoch   3, Step:    85500, Batch Loss:     1.904727, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-07-27 09:57:23,466 - INFO - joeynmt.training - Epoch   3, Step:    85600, Batch Loss:     1.985757, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-07-27 09:57:36,889 - INFO - joeynmt.training - Epoch   3, Step:    85700, Batch Loss:     1.749990, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-07-27 09:57:50,220 - INFO - joeynmt.training - Epoch   3, Step:    85800, Batch Loss:     1.913987, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-07-27 09:58:03,562 - INFO - joeynmt.training - Epoch   3, Step:    85900, Batch Loss:     1.946401, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-07-27 09:58:17,072 - INFO - joeynmt.training - Epoch   3, Step:    86000, Batch Loss:     1.817259, Tokens per Sec:    15925, Lr: 0.000300\n",
      "2021-07-27 09:58:30,473 - INFO - joeynmt.training - Epoch   3, Step:    86100, Batch Loss:     2.136081, Tokens per Sec:    15645, Lr: 0.000300\n",
      "2021-07-27 09:58:39,440 - INFO - joeynmt.training - Epoch   3: total training loss 5225.26\n",
      "2021-07-27 09:58:39,440 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-27 09:58:44,130 - INFO - joeynmt.training - Epoch   4, Step:    86200, Batch Loss:     1.754511, Tokens per Sec:    15393, Lr: 0.000300\n",
      "2021-07-27 09:58:57,597 - INFO - joeynmt.training - Epoch   4, Step:    86300, Batch Loss:     2.013391, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-07-27 09:59:11,013 - INFO - joeynmt.training - Epoch   4, Step:    86400, Batch Loss:     1.967292, Tokens per Sec:    16305, Lr: 0.000300\n",
      "2021-07-27 09:59:24,616 - INFO - joeynmt.training - Epoch   4, Step:    86500, Batch Loss:     1.593149, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-07-27 09:59:38,178 - INFO - joeynmt.training - Epoch   4, Step:    86600, Batch Loss:     1.808820, Tokens per Sec:    15722, Lr: 0.000300\n",
      "2021-07-27 09:59:51,588 - INFO - joeynmt.training - Epoch   4, Step:    86700, Batch Loss:     2.198513, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-07-27 10:00:04,704 - INFO - joeynmt.training - Epoch   4, Step:    86800, Batch Loss:     1.838040, Tokens per Sec:    15750, Lr: 0.000300\n",
      "2021-07-27 10:00:18,126 - INFO - joeynmt.training - Epoch   4, Step:    86900, Batch Loss:     1.915569, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-27 10:00:31,686 - INFO - joeynmt.training - Epoch   4, Step:    87000, Batch Loss:     1.909151, Tokens per Sec:    16379, Lr: 0.000300\n",
      "2021-07-27 10:00:50,183 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:00:50,183 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:00:50,183 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:00:50,404 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:00:50,404 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:00:51,193 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:00:51,194 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:00:51,194 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:00:51,195 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is acceptable to God . ”\n",
      "2021-07-27 10:00:51,195 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:00:51,195 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:00:51,195 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:00:51,196 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son , whom I am grateful . ”\n",
      "2021-07-27 10:00:51,196 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:00:51,196 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:00:51,196 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:00:51,197 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
      "2021-07-27 10:00:51,197 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:00:51,197 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:00:51,198 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:00:51,198 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 10:00:51,198 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    87000: bleu:  26.10, loss: 45465.6602, ppl:   5.5429, duration: 19.5117s\n",
      "2021-07-27 10:01:04,754 - INFO - joeynmt.training - Epoch   4, Step:    87100, Batch Loss:     1.880486, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-07-27 10:01:18,140 - INFO - joeynmt.training - Epoch   4, Step:    87200, Batch Loss:     2.022305, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-27 10:01:31,492 - INFO - joeynmt.training - Epoch   4, Step:    87300, Batch Loss:     1.546580, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-07-27 10:01:44,906 - INFO - joeynmt.training - Epoch   4, Step:    87400, Batch Loss:     1.705323, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-07-27 10:01:58,435 - INFO - joeynmt.training - Epoch   4, Step:    87500, Batch Loss:     1.916856, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-27 10:02:11,949 - INFO - joeynmt.training - Epoch   4, Step:    87600, Batch Loss:     1.774064, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-07-27 10:02:25,568 - INFO - joeynmt.training - Epoch   4, Step:    87700, Batch Loss:     1.970183, Tokens per Sec:    16109, Lr: 0.000300\n",
      "2021-07-27 10:02:38,982 - INFO - joeynmt.training - Epoch   4, Step:    87800, Batch Loss:     1.900409, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-07-27 10:02:52,342 - INFO - joeynmt.training - Epoch   4, Step:    87900, Batch Loss:     1.867343, Tokens per Sec:    16140, Lr: 0.000300\n",
      "2021-07-27 10:03:05,620 - INFO - joeynmt.training - Epoch   4, Step:    88000, Batch Loss:     1.972984, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-07-27 10:03:19,060 - INFO - joeynmt.training - Epoch   4, Step:    88100, Batch Loss:     1.942800, Tokens per Sec:    16017, Lr: 0.000300\n",
      "2021-07-27 10:03:32,367 - INFO - joeynmt.training - Epoch   4, Step:    88200, Batch Loss:     1.848028, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-07-27 10:03:45,843 - INFO - joeynmt.training - Epoch   4, Step:    88300, Batch Loss:     1.949016, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-07-27 10:03:59,319 - INFO - joeynmt.training - Epoch   4, Step:    88400, Batch Loss:     1.868750, Tokens per Sec:    16124, Lr: 0.000300\n",
      "2021-07-27 10:04:12,607 - INFO - joeynmt.training - Epoch   4, Step:    88500, Batch Loss:     2.048942, Tokens per Sec:    15906, Lr: 0.000300\n",
      "2021-07-27 10:04:26,136 - INFO - joeynmt.training - Epoch   4, Step:    88600, Batch Loss:     2.142006, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-27 10:04:39,509 - INFO - joeynmt.training - Epoch   4, Step:    88700, Batch Loss:     1.623891, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-07-27 10:04:53,002 - INFO - joeynmt.training - Epoch   4, Step:    88800, Batch Loss:     1.979871, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-07-27 10:05:06,291 - INFO - joeynmt.training - Epoch   4, Step:    88900, Batch Loss:     2.030600, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-07-27 10:05:12,150 - INFO - joeynmt.training - Epoch   4: total training loss 5201.68\n",
      "2021-07-27 10:05:12,150 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-27 10:05:20,016 - INFO - joeynmt.training - Epoch   5, Step:    89000, Batch Loss:     1.848434, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-07-27 10:05:33,660 - INFO - joeynmt.training - Epoch   5, Step:    89100, Batch Loss:     2.031881, Tokens per Sec:    16094, Lr: 0.000300\n",
      "2021-07-27 10:05:47,222 - INFO - joeynmt.training - Epoch   5, Step:    89200, Batch Loss:     1.714864, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-07-27 10:06:00,715 - INFO - joeynmt.training - Epoch   5, Step:    89300, Batch Loss:     1.874286, Tokens per Sec:    16360, Lr: 0.000300\n",
      "2021-07-27 10:06:14,137 - INFO - joeynmt.training - Epoch   5, Step:    89400, Batch Loss:     1.844287, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-07-27 10:06:27,481 - INFO - joeynmt.training - Epoch   5, Step:    89500, Batch Loss:     1.560152, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-07-27 10:06:41,220 - INFO - joeynmt.training - Epoch   5, Step:    89600, Batch Loss:     1.912444, Tokens per Sec:    16354, Lr: 0.000300\n",
      "2021-07-27 10:06:54,552 - INFO - joeynmt.training - Epoch   5, Step:    89700, Batch Loss:     1.882960, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-07-27 10:07:07,937 - INFO - joeynmt.training - Epoch   5, Step:    89800, Batch Loss:     1.719286, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-07-27 10:07:21,264 - INFO - joeynmt.training - Epoch   5, Step:    89900, Batch Loss:     2.145797, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-27 10:07:34,593 - INFO - joeynmt.training - Epoch   5, Step:    90000, Batch Loss:     1.999324, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-07-27 10:07:54,363 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:07:54,363 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:07:54,363 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:07:54,607 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:07:54,607 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:07:55,441 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:07:55,442 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:07:55,443 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:07:55,443 - INFO - joeynmt.training - \tHypothesis: If you do good and do good , you are suffering when you endure , that is acceptable to God . ”\n",
      "2021-07-27 10:07:55,443 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:07:55,443 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:07:55,444 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:07:55,444 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 10:07:55,444 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:07:55,444 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:07:55,445 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:07:55,445 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 10:07:55,445 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:07:55,445 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:07:55,446 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:07:55,446 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 10:07:55,446 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    90000: bleu:  26.00, loss: 45389.7461, ppl:   5.5271, duration: 20.8528s\n",
      "2021-07-27 10:08:09,015 - INFO - joeynmt.training - Epoch   5, Step:    90100, Batch Loss:     2.154663, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-07-27 10:08:22,433 - INFO - joeynmt.training - Epoch   5, Step:    90200, Batch Loss:     2.121293, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-07-27 10:08:35,729 - INFO - joeynmt.training - Epoch   5, Step:    90300, Batch Loss:     2.086379, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-07-27 10:08:48,946 - INFO - joeynmt.training - Epoch   5, Step:    90400, Batch Loss:     1.322294, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-07-27 10:09:02,221 - INFO - joeynmt.training - Epoch   5, Step:    90500, Batch Loss:     1.921736, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-07-27 10:09:15,476 - INFO - joeynmt.training - Epoch   5, Step:    90600, Batch Loss:     1.827785, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-07-27 10:09:28,925 - INFO - joeynmt.training - Epoch   5, Step:    90700, Batch Loss:     1.946358, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-07-27 10:09:42,335 - INFO - joeynmt.training - Epoch   5, Step:    90800, Batch Loss:     1.673532, Tokens per Sec:    16076, Lr: 0.000300\n",
      "2021-07-27 10:09:55,769 - INFO - joeynmt.training - Epoch   5, Step:    90900, Batch Loss:     2.007006, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-07-27 10:10:09,116 - INFO - joeynmt.training - Epoch   5, Step:    91000, Batch Loss:     1.921765, Tokens per Sec:    15905, Lr: 0.000300\n",
      "2021-07-27 10:10:22,540 - INFO - joeynmt.training - Epoch   5, Step:    91100, Batch Loss:     2.041848, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-07-27 10:10:36,072 - INFO - joeynmt.training - Epoch   5, Step:    91200, Batch Loss:     2.045964, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-07-27 10:10:49,264 - INFO - joeynmt.training - Epoch   5, Step:    91300, Batch Loss:     1.889196, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-07-27 10:11:02,658 - INFO - joeynmt.training - Epoch   5, Step:    91400, Batch Loss:     1.948621, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-07-27 10:11:16,121 - INFO - joeynmt.training - Epoch   5, Step:    91500, Batch Loss:     2.106603, Tokens per Sec:    16249, Lr: 0.000300\n",
      "2021-07-27 10:11:29,586 - INFO - joeynmt.training - Epoch   5, Step:    91600, Batch Loss:     1.933520, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-07-27 10:11:42,985 - INFO - joeynmt.training - Epoch   5, Step:    91700, Batch Loss:     1.948294, Tokens per Sec:    15518, Lr: 0.000300\n",
      "2021-07-27 10:11:45,694 - INFO - joeynmt.training - Epoch   5: total training loss 5182.04\n",
      "2021-07-27 10:11:45,694 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-27 10:11:56,900 - INFO - joeynmt.training - Epoch   6, Step:    91800, Batch Loss:     1.950058, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-27 10:12:10,287 - INFO - joeynmt.training - Epoch   6, Step:    91900, Batch Loss:     2.036018, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-07-27 10:12:23,719 - INFO - joeynmt.training - Epoch   6, Step:    92000, Batch Loss:     1.888739, Tokens per Sec:    16133, Lr: 0.000300\n",
      "2021-07-27 10:12:37,114 - INFO - joeynmt.training - Epoch   6, Step:    92100, Batch Loss:     1.769344, Tokens per Sec:    16139, Lr: 0.000300\n",
      "2021-07-27 10:12:50,320 - INFO - joeynmt.training - Epoch   6, Step:    92200, Batch Loss:     1.851227, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-07-27 10:13:03,785 - INFO - joeynmt.training - Epoch   6, Step:    92300, Batch Loss:     1.554227, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-07-27 10:13:17,366 - INFO - joeynmt.training - Epoch   6, Step:    92400, Batch Loss:     1.641454, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-07-27 10:13:30,762 - INFO - joeynmt.training - Epoch   6, Step:    92500, Batch Loss:     2.118319, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-27 10:13:44,212 - INFO - joeynmt.training - Epoch   6, Step:    92600, Batch Loss:     1.739755, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-07-27 10:13:57,315 - INFO - joeynmt.training - Epoch   6, Step:    92700, Batch Loss:     1.859637, Tokens per Sec:    15642, Lr: 0.000300\n",
      "2021-07-27 10:14:10,765 - INFO - joeynmt.training - Epoch   6, Step:    92800, Batch Loss:     1.899158, Tokens per Sec:    15768, Lr: 0.000300\n",
      "2021-07-27 10:14:24,327 - INFO - joeynmt.training - Epoch   6, Step:    92900, Batch Loss:     2.025175, Tokens per Sec:    15933, Lr: 0.000300\n",
      "2021-07-27 10:14:37,852 - INFO - joeynmt.training - Epoch   6, Step:    93000, Batch Loss:     1.940333, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-07-27 10:14:54,694 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:14:54,694 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:14:54,694 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:14:54,918 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:14:54,918 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:14:56,061 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:14:56,062 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:14:56,062 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:14:56,062 - INFO - joeynmt.training - \tHypothesis: If you do good , you are suffering when you endure , that is acceptable to God . ”\n",
      "2021-07-27 10:14:56,062 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:14:56,063 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:14:56,063 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:14:56,063 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 10:14:56,063 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:14:56,064 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:14:56,064 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:14:56,064 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 10:14:56,064 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:14:56,064 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:14:56,064 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:14:56,065 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 10:14:56,065 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    93000: bleu:  26.00, loss: 45101.0820, ppl:   5.4673, duration: 18.2128s\n",
      "2021-07-27 10:15:09,637 - INFO - joeynmt.training - Epoch   6, Step:    93100, Batch Loss:     1.951977, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-07-27 10:15:22,981 - INFO - joeynmt.training - Epoch   6, Step:    93200, Batch Loss:     1.865463, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-07-27 10:15:36,486 - INFO - joeynmt.training - Epoch   6, Step:    93300, Batch Loss:     1.892212, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-07-27 10:15:49,811 - INFO - joeynmt.training - Epoch   6, Step:    93400, Batch Loss:     1.605815, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-07-27 10:16:03,095 - INFO - joeynmt.training - Epoch   6, Step:    93500, Batch Loss:     1.826510, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-07-27 10:16:16,380 - INFO - joeynmt.training - Epoch   6, Step:    93600, Batch Loss:     1.841408, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-07-27 10:16:29,946 - INFO - joeynmt.training - Epoch   6, Step:    93700, Batch Loss:     1.519554, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-07-27 10:16:43,630 - INFO - joeynmt.training - Epoch   6, Step:    93800, Batch Loss:     2.030423, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-07-27 10:16:56,949 - INFO - joeynmt.training - Epoch   6, Step:    93900, Batch Loss:     1.980441, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-07-27 10:17:10,203 - INFO - joeynmt.training - Epoch   6, Step:    94000, Batch Loss:     2.022531, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-07-27 10:17:23,635 - INFO - joeynmt.training - Epoch   6, Step:    94100, Batch Loss:     1.919425, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-07-27 10:17:37,189 - INFO - joeynmt.training - Epoch   6, Step:    94200, Batch Loss:     2.117377, Tokens per Sec:    16291, Lr: 0.000300\n",
      "2021-07-27 10:17:50,609 - INFO - joeynmt.training - Epoch   6, Step:    94300, Batch Loss:     1.849079, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-07-27 10:18:04,191 - INFO - joeynmt.training - Epoch   6, Step:    94400, Batch Loss:     2.068027, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-07-27 10:18:16,874 - INFO - joeynmt.training - Epoch   6: total training loss 5158.31\n",
      "2021-07-27 10:18:16,874 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-27 10:18:17,961 - INFO - joeynmt.training - Epoch   7, Step:    94500, Batch Loss:     1.571302, Tokens per Sec:    11807, Lr: 0.000300\n",
      "2021-07-27 10:18:31,432 - INFO - joeynmt.training - Epoch   7, Step:    94600, Batch Loss:     2.100660, Tokens per Sec:    16083, Lr: 0.000300\n",
      "2021-07-27 10:18:44,876 - INFO - joeynmt.training - Epoch   7, Step:    94700, Batch Loss:     1.912166, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-07-27 10:18:58,545 - INFO - joeynmt.training - Epoch   7, Step:    94800, Batch Loss:     1.886194, Tokens per Sec:    16311, Lr: 0.000300\n",
      "2021-07-27 10:19:11,870 - INFO - joeynmt.training - Epoch   7, Step:    94900, Batch Loss:     1.656972, Tokens per Sec:    15565, Lr: 0.000300\n",
      "2021-07-27 10:19:25,378 - INFO - joeynmt.training - Epoch   7, Step:    95000, Batch Loss:     1.907513, Tokens per Sec:    15645, Lr: 0.000300\n",
      "2021-07-27 10:19:38,893 - INFO - joeynmt.training - Epoch   7, Step:    95100, Batch Loss:     1.854539, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-07-27 10:19:52,196 - INFO - joeynmt.training - Epoch   7, Step:    95200, Batch Loss:     1.869341, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-07-27 10:20:05,649 - INFO - joeynmt.training - Epoch   7, Step:    95300, Batch Loss:     1.896788, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-07-27 10:20:19,076 - INFO - joeynmt.training - Epoch   7, Step:    95400, Batch Loss:     1.794540, Tokens per Sec:    16141, Lr: 0.000300\n",
      "2021-07-27 10:20:32,864 - INFO - joeynmt.training - Epoch   7, Step:    95500, Batch Loss:     2.045545, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-07-27 10:20:46,409 - INFO - joeynmt.training - Epoch   7, Step:    95600, Batch Loss:     1.877525, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-07-27 10:20:59,653 - INFO - joeynmt.training - Epoch   7, Step:    95700, Batch Loss:     1.821247, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-07-27 10:21:12,917 - INFO - joeynmt.training - Epoch   7, Step:    95800, Batch Loss:     1.621607, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-07-27 10:21:26,453 - INFO - joeynmt.training - Epoch   7, Step:    95900, Batch Loss:     1.830629, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-07-27 10:21:39,861 - INFO - joeynmt.training - Epoch   7, Step:    96000, Batch Loss:     1.857401, Tokens per Sec:    15754, Lr: 0.000300\n",
      "2021-07-27 10:21:57,369 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:21:57,369 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:21:57,370 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:21:57,606 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:21:57,606 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:21:58,335 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:21:58,336 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:21:58,336 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:21:58,336 - INFO - joeynmt.training - \tHypothesis: “ If you do good and suffering when you endure , this is what is acceptable to God . ”\n",
      "2021-07-27 10:21:58,337 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:21:58,337 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:21:58,337 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:21:58,338 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 10:21:58,338 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:21:58,338 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:21:58,338 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:21:58,339 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
      "2021-07-27 10:21:58,339 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:21:58,339 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:21:58,339 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:21:58,339 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 10:21:58,340 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    96000: bleu:  26.52, loss: 45076.0859, ppl:   5.4622, duration: 18.4778s\n",
      "2021-07-27 10:22:11,926 - INFO - joeynmt.training - Epoch   7, Step:    96100, Batch Loss:     1.790250, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-07-27 10:22:25,465 - INFO - joeynmt.training - Epoch   7, Step:    96200, Batch Loss:     1.838810, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-07-27 10:22:39,155 - INFO - joeynmt.training - Epoch   7, Step:    96300, Batch Loss:     1.794353, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-07-27 10:22:52,477 - INFO - joeynmt.training - Epoch   7, Step:    96400, Batch Loss:     1.664846, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-07-27 10:23:06,024 - INFO - joeynmt.training - Epoch   7, Step:    96500, Batch Loss:     1.956909, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-07-27 10:23:19,328 - INFO - joeynmt.training - Epoch   7, Step:    96600, Batch Loss:     2.066128, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-07-27 10:23:32,831 - INFO - joeynmt.training - Epoch   7, Step:    96700, Batch Loss:     1.794327, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-07-27 10:23:46,252 - INFO - joeynmt.training - Epoch   7, Step:    96800, Batch Loss:     1.804979, Tokens per Sec:    15671, Lr: 0.000300\n",
      "2021-07-27 10:23:59,566 - INFO - joeynmt.training - Epoch   7, Step:    96900, Batch Loss:     1.494350, Tokens per Sec:    15510, Lr: 0.000300\n",
      "2021-07-27 10:24:12,890 - INFO - joeynmt.training - Epoch   7, Step:    97000, Batch Loss:     1.601240, Tokens per Sec:    15704, Lr: 0.000300\n",
      "2021-07-27 10:24:26,465 - INFO - joeynmt.training - Epoch   7, Step:    97100, Batch Loss:     1.462695, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-07-27 10:24:39,867 - INFO - joeynmt.training - Epoch   7, Step:    97200, Batch Loss:     1.797167, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-07-27 10:24:49,775 - INFO - joeynmt.training - Epoch   7: total training loss 5156.87\n",
      "2021-07-27 10:24:49,775 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-27 10:24:53,609 - INFO - joeynmt.training - Epoch   8, Step:    97300, Batch Loss:     1.961192, Tokens per Sec:    14502, Lr: 0.000300\n",
      "2021-07-27 10:25:06,817 - INFO - joeynmt.training - Epoch   8, Step:    97400, Batch Loss:     1.816230, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-07-27 10:25:20,308 - INFO - joeynmt.training - Epoch   8, Step:    97500, Batch Loss:     1.953337, Tokens per Sec:    16413, Lr: 0.000300\n",
      "2021-07-27 10:25:33,896 - INFO - joeynmt.training - Epoch   8, Step:    97600, Batch Loss:     1.977275, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-07-27 10:25:47,194 - INFO - joeynmt.training - Epoch   8, Step:    97700, Batch Loss:     1.874819, Tokens per Sec:    16237, Lr: 0.000300\n",
      "2021-07-27 10:26:00,444 - INFO - joeynmt.training - Epoch   8, Step:    97800, Batch Loss:     1.506946, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-07-27 10:26:13,651 - INFO - joeynmt.training - Epoch   8, Step:    97900, Batch Loss:     1.943933, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-07-27 10:26:27,234 - INFO - joeynmt.training - Epoch   8, Step:    98000, Batch Loss:     1.539904, Tokens per Sec:    15834, Lr: 0.000300\n",
      "2021-07-27 10:26:40,959 - INFO - joeynmt.training - Epoch   8, Step:    98100, Batch Loss:     1.981087, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-07-27 10:26:54,552 - INFO - joeynmt.training - Epoch   8, Step:    98200, Batch Loss:     1.895412, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-07-27 10:27:07,898 - INFO - joeynmt.training - Epoch   8, Step:    98300, Batch Loss:     1.945071, Tokens per Sec:    15723, Lr: 0.000300\n",
      "2021-07-27 10:27:21,417 - INFO - joeynmt.training - Epoch   8, Step:    98400, Batch Loss:     1.906464, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-07-27 10:27:34,884 - INFO - joeynmt.training - Epoch   8, Step:    98500, Batch Loss:     1.944851, Tokens per Sec:    15725, Lr: 0.000300\n",
      "2021-07-27 10:27:48,289 - INFO - joeynmt.training - Epoch   8, Step:    98600, Batch Loss:     1.807476, Tokens per Sec:    15548, Lr: 0.000300\n",
      "2021-07-27 10:28:01,801 - INFO - joeynmt.training - Epoch   8, Step:    98700, Batch Loss:     2.017169, Tokens per Sec:    16249, Lr: 0.000300\n",
      "2021-07-27 10:28:15,244 - INFO - joeynmt.training - Epoch   8, Step:    98800, Batch Loss:     1.674990, Tokens per Sec:    16053, Lr: 0.000300\n",
      "2021-07-27 10:28:28,618 - INFO - joeynmt.training - Epoch   8, Step:    98900, Batch Loss:     2.222675, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-07-27 10:28:42,215 - INFO - joeynmt.training - Epoch   8, Step:    99000, Batch Loss:     1.666454, Tokens per Sec:    15511, Lr: 0.000300\n",
      "2021-07-27 10:29:00,859 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:29:00,860 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:29:00,860 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:29:01,095 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:29:01,095 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:29:01,842 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:29:01,843 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:29:01,843 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:29:01,843 - INFO - joeynmt.training - \tHypothesis: I have hated you when you do good and suffer when you endure , that is what is acceptable to God . ”\n",
      "2021-07-27 10:29:01,844 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:29:01,844 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:29:01,844 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:29:01,845 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son , whom I am grateful . ”\n",
      "2021-07-27 10:29:01,845 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:29:01,845 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:29:01,845 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:29:01,846 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 10:29:01,846 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:29:01,846 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:29:01,848 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:29:01,848 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 10:29:01,848 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    99000: bleu:  26.51, loss: 44922.2969, ppl:   5.4306, duration: 19.6325s\n",
      "2021-07-27 10:29:15,478 - INFO - joeynmt.training - Epoch   8, Step:    99100, Batch Loss:     2.062360, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-07-27 10:29:28,870 - INFO - joeynmt.training - Epoch   8, Step:    99200, Batch Loss:     1.878650, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-07-27 10:29:42,167 - INFO - joeynmt.training - Epoch   8, Step:    99300, Batch Loss:     2.330217, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-07-27 10:29:55,685 - INFO - joeynmt.training - Epoch   8, Step:    99400, Batch Loss:     2.271222, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-07-27 10:30:09,093 - INFO - joeynmt.training - Epoch   8, Step:    99500, Batch Loss:     1.638400, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-07-27 10:30:22,616 - INFO - joeynmt.training - Epoch   8, Step:    99600, Batch Loss:     1.726417, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-07-27 10:30:35,968 - INFO - joeynmt.training - Epoch   8, Step:    99700, Batch Loss:     1.683869, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-07-27 10:30:49,267 - INFO - joeynmt.training - Epoch   8, Step:    99800, Batch Loss:     1.839914, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-07-27 10:31:02,595 - INFO - joeynmt.training - Epoch   8, Step:    99900, Batch Loss:     1.842617, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-27 10:31:16,033 - INFO - joeynmt.training - Epoch   8, Step:   100000, Batch Loss:     1.599191, Tokens per Sec:    15586, Lr: 0.000300\n",
      "2021-07-27 10:31:24,774 - INFO - joeynmt.training - Epoch   8: total training loss 5159.25\n",
      "2021-07-27 10:31:24,775 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-27 10:31:29,854 - INFO - joeynmt.training - Epoch   9, Step:   100100, Batch Loss:     1.862011, Tokens per Sec:    15008, Lr: 0.000300\n",
      "2021-07-27 10:31:43,337 - INFO - joeynmt.training - Epoch   9, Step:   100200, Batch Loss:     1.889470, Tokens per Sec:    15795, Lr: 0.000300\n",
      "2021-07-27 10:31:56,707 - INFO - joeynmt.training - Epoch   9, Step:   100300, Batch Loss:     1.791631, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-07-27 10:32:10,086 - INFO - joeynmt.training - Epoch   9, Step:   100400, Batch Loss:     1.830071, Tokens per Sec:    15544, Lr: 0.000300\n",
      "2021-07-27 10:32:23,633 - INFO - joeynmt.training - Epoch   9, Step:   100500, Batch Loss:     1.907436, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-07-27 10:32:37,331 - INFO - joeynmt.training - Epoch   9, Step:   100600, Batch Loss:     1.601005, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-07-27 10:32:50,695 - INFO - joeynmt.training - Epoch   9, Step:   100700, Batch Loss:     1.941778, Tokens per Sec:    15720, Lr: 0.000300\n",
      "2021-07-27 10:33:04,188 - INFO - joeynmt.training - Epoch   9, Step:   100800, Batch Loss:     1.731409, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-27 10:33:17,722 - INFO - joeynmt.training - Epoch   9, Step:   100900, Batch Loss:     1.975960, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-07-27 10:33:31,157 - INFO - joeynmt.training - Epoch   9, Step:   101000, Batch Loss:     1.911385, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-07-27 10:33:44,648 - INFO - joeynmt.training - Epoch   9, Step:   101100, Batch Loss:     1.684346, Tokens per Sec:    15436, Lr: 0.000300\n",
      "2021-07-27 10:33:57,995 - INFO - joeynmt.training - Epoch   9, Step:   101200, Batch Loss:     1.710415, Tokens per Sec:    15677, Lr: 0.000300\n",
      "2021-07-27 10:34:11,580 - INFO - joeynmt.training - Epoch   9, Step:   101300, Batch Loss:     1.687143, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-07-27 10:34:25,030 - INFO - joeynmt.training - Epoch   9, Step:   101400, Batch Loss:     1.719967, Tokens per Sec:    16274, Lr: 0.000300\n",
      "2021-07-27 10:34:38,607 - INFO - joeynmt.training - Epoch   9, Step:   101500, Batch Loss:     1.709962, Tokens per Sec:    15734, Lr: 0.000300\n",
      "2021-07-27 10:34:52,228 - INFO - joeynmt.training - Epoch   9, Step:   101600, Batch Loss:     1.737644, Tokens per Sec:    15637, Lr: 0.000300\n",
      "2021-07-27 10:35:05,638 - INFO - joeynmt.training - Epoch   9, Step:   101700, Batch Loss:     1.503876, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-07-27 10:35:19,198 - INFO - joeynmt.training - Epoch   9, Step:   101800, Batch Loss:     1.831515, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-07-27 10:35:32,782 - INFO - joeynmt.training - Epoch   9, Step:   101900, Batch Loss:     1.838188, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-07-27 10:35:46,203 - INFO - joeynmt.training - Epoch   9, Step:   102000, Batch Loss:     1.877826, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-07-27 10:36:03,491 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:36:03,492 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:36:03,492 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:36:03,722 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:36:03,723 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:36:04,793 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:36:04,794 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:36:04,794 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:36:04,794 - INFO - joeynmt.training - \tHypothesis: If you do good and do good , you are suffering when you endure , that is what is good to God . ”\n",
      "2021-07-27 10:36:04,795 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:36:04,795 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:36:04,795 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:36:04,795 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 10:36:04,795 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:36:04,796 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:36:04,796 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:36:04,796 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 10:36:04,796 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:36:04,797 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:36:04,797 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:36:04,797 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 10:36:04,797 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   102000: bleu:  26.10, loss: 44766.8203, ppl:   5.3989, duration: 18.5940s\n",
      "2021-07-27 10:36:18,467 - INFO - joeynmt.training - Epoch   9, Step:   102100, Batch Loss:     1.724547, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-07-27 10:36:32,092 - INFO - joeynmt.training - Epoch   9, Step:   102200, Batch Loss:     1.883249, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-07-27 10:36:45,499 - INFO - joeynmt.training - Epoch   9, Step:   102300, Batch Loss:     1.975677, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-07-27 10:36:58,797 - INFO - joeynmt.training - Epoch   9, Step:   102400, Batch Loss:     1.952409, Tokens per Sec:    16208, Lr: 0.000300\n",
      "2021-07-27 10:37:12,156 - INFO - joeynmt.training - Epoch   9, Step:   102500, Batch Loss:     1.988778, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-07-27 10:37:25,667 - INFO - joeynmt.training - Epoch   9, Step:   102600, Batch Loss:     1.868324, Tokens per Sec:    16179, Lr: 0.000300\n",
      "2021-07-27 10:37:39,264 - INFO - joeynmt.training - Epoch   9, Step:   102700, Batch Loss:     1.888787, Tokens per Sec:    15864, Lr: 0.000300\n",
      "2021-07-27 10:37:52,520 - INFO - joeynmt.training - Epoch   9, Step:   102800, Batch Loss:     1.909742, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-07-27 10:37:59,007 - INFO - joeynmt.training - Epoch   9: total training loss 5128.08\n",
      "2021-07-27 10:37:59,007 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-27 10:38:06,099 - INFO - joeynmt.training - Epoch  10, Step:   102900, Batch Loss:     1.902202, Tokens per Sec:    15414, Lr: 0.000300\n",
      "2021-07-27 10:38:19,491 - INFO - joeynmt.training - Epoch  10, Step:   103000, Batch Loss:     1.858475, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-07-27 10:38:32,954 - INFO - joeynmt.training - Epoch  10, Step:   103100, Batch Loss:     1.859414, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-07-27 10:38:46,618 - INFO - joeynmt.training - Epoch  10, Step:   103200, Batch Loss:     1.715920, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-07-27 10:38:59,985 - INFO - joeynmt.training - Epoch  10, Step:   103300, Batch Loss:     1.799271, Tokens per Sec:    16306, Lr: 0.000300\n",
      "2021-07-27 10:39:13,255 - INFO - joeynmt.training - Epoch  10, Step:   103400, Batch Loss:     2.065408, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-07-27 10:39:26,512 - INFO - joeynmt.training - Epoch  10, Step:   103500, Batch Loss:     1.971292, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-07-27 10:39:40,073 - INFO - joeynmt.training - Epoch  10, Step:   103600, Batch Loss:     1.842807, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-07-27 10:39:53,633 - INFO - joeynmt.training - Epoch  10, Step:   103700, Batch Loss:     1.899268, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-07-27 10:40:07,098 - INFO - joeynmt.training - Epoch  10, Step:   103800, Batch Loss:     1.952967, Tokens per Sec:    15958, Lr: 0.000300\n",
      "2021-07-27 10:40:20,627 - INFO - joeynmt.training - Epoch  10, Step:   103900, Batch Loss:     1.981954, Tokens per Sec:    15706, Lr: 0.000300\n",
      "2021-07-27 10:40:33,983 - INFO - joeynmt.training - Epoch  10, Step:   104000, Batch Loss:     1.967430, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-07-27 10:40:47,244 - INFO - joeynmt.training - Epoch  10, Step:   104100, Batch Loss:     1.787356, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-07-27 10:41:00,654 - INFO - joeynmt.training - Epoch  10, Step:   104200, Batch Loss:     1.912150, Tokens per Sec:    16333, Lr: 0.000300\n",
      "2021-07-27 10:41:14,032 - INFO - joeynmt.training - Epoch  10, Step:   104300, Batch Loss:     1.772267, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-07-27 10:41:27,615 - INFO - joeynmt.training - Epoch  10, Step:   104400, Batch Loss:     1.830122, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-07-27 10:41:40,942 - INFO - joeynmt.training - Epoch  10, Step:   104500, Batch Loss:     1.892144, Tokens per Sec:    15592, Lr: 0.000300\n",
      "2021-07-27 10:41:54,520 - INFO - joeynmt.training - Epoch  10, Step:   104600, Batch Loss:     2.124769, Tokens per Sec:    16291, Lr: 0.000300\n",
      "2021-07-27 10:42:07,867 - INFO - joeynmt.training - Epoch  10, Step:   104700, Batch Loss:     1.732307, Tokens per Sec:    15681, Lr: 0.000300\n",
      "2021-07-27 10:42:21,237 - INFO - joeynmt.training - Epoch  10, Step:   104800, Batch Loss:     1.994446, Tokens per Sec:    16132, Lr: 0.000300\n",
      "2021-07-27 10:42:34,594 - INFO - joeynmt.training - Epoch  10, Step:   104900, Batch Loss:     1.981178, Tokens per Sec:    15581, Lr: 0.000300\n",
      "2021-07-27 10:42:48,220 - INFO - joeynmt.training - Epoch  10, Step:   105000, Batch Loss:     1.887099, Tokens per Sec:    16224, Lr: 0.000300\n",
      "2021-07-27 10:43:06,422 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:43:06,422 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:43:06,423 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:43:06,645 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:43:06,645 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:43:07,351 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:43:07,352 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:43:07,352 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:43:07,352 - INFO - joeynmt.training - \tHypothesis: If you do good and are suffering when you endure , this is acceptable to God . ”\n",
      "2021-07-27 10:43:07,352 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:43:07,353 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:43:07,353 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:43:07,353 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 10:43:07,353 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:43:07,354 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:43:07,355 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:43:07,356 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 10:43:07,357 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:43:07,357 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:43:07,357 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:43:07,357 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to heal his family .\n",
      "2021-07-27 10:43:07,357 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   105000: bleu:  26.57, loss: 44760.0430, ppl:   5.3975, duration: 19.1373s\n",
      "2021-07-27 10:43:20,868 - INFO - joeynmt.training - Epoch  10, Step:   105100, Batch Loss:     1.898195, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-07-27 10:43:34,475 - INFO - joeynmt.training - Epoch  10, Step:   105200, Batch Loss:     1.887206, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-07-27 10:43:47,973 - INFO - joeynmt.training - Epoch  10, Step:   105300, Batch Loss:     1.773259, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-07-27 10:44:01,266 - INFO - joeynmt.training - Epoch  10, Step:   105400, Batch Loss:     1.800599, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-07-27 10:44:14,702 - INFO - joeynmt.training - Epoch  10, Step:   105500, Batch Loss:     1.896544, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-07-27 10:44:28,051 - INFO - joeynmt.training - Epoch  10, Step:   105600, Batch Loss:     1.865783, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-07-27 10:44:32,027 - INFO - joeynmt.training - Epoch  10: total training loss 5113.45\n",
      "2021-07-27 10:44:32,028 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-27 10:44:41,807 - INFO - joeynmt.training - Epoch  11, Step:   105700, Batch Loss:     1.945204, Tokens per Sec:    15235, Lr: 0.000300\n",
      "2021-07-27 10:44:55,242 - INFO - joeynmt.training - Epoch  11, Step:   105800, Batch Loss:     1.919515, Tokens per Sec:    15749, Lr: 0.000300\n",
      "2021-07-27 10:45:08,584 - INFO - joeynmt.training - Epoch  11, Step:   105900, Batch Loss:     1.544542, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-07-27 10:45:21,938 - INFO - joeynmt.training - Epoch  11, Step:   106000, Batch Loss:     1.501450, Tokens per Sec:    16265, Lr: 0.000300\n",
      "2021-07-27 10:45:35,188 - INFO - joeynmt.training - Epoch  11, Step:   106100, Batch Loss:     2.043373, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-07-27 10:45:48,702 - INFO - joeynmt.training - Epoch  11, Step:   106200, Batch Loss:     1.973657, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-07-27 10:46:02,152 - INFO - joeynmt.training - Epoch  11, Step:   106300, Batch Loss:     1.673043, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-07-27 10:46:15,584 - INFO - joeynmt.training - Epoch  11, Step:   106400, Batch Loss:     2.014502, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-07-27 10:46:28,996 - INFO - joeynmt.training - Epoch  11, Step:   106500, Batch Loss:     1.733449, Tokens per Sec:    15575, Lr: 0.000300\n",
      "2021-07-27 10:46:42,420 - INFO - joeynmt.training - Epoch  11, Step:   106600, Batch Loss:     1.419433, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-07-27 10:46:55,767 - INFO - joeynmt.training - Epoch  11, Step:   106700, Batch Loss:     2.014790, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-07-27 10:47:09,194 - INFO - joeynmt.training - Epoch  11, Step:   106800, Batch Loss:     1.762715, Tokens per Sec:    15986, Lr: 0.000300\n",
      "2021-07-27 10:47:22,681 - INFO - joeynmt.training - Epoch  11, Step:   106900, Batch Loss:     1.740853, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-07-27 10:47:36,323 - INFO - joeynmt.training - Epoch  11, Step:   107000, Batch Loss:     1.801408, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-07-27 10:47:49,737 - INFO - joeynmt.training - Epoch  11, Step:   107100, Batch Loss:     1.888115, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-07-27 10:48:03,007 - INFO - joeynmt.training - Epoch  11, Step:   107200, Batch Loss:     1.994316, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-07-27 10:48:16,394 - INFO - joeynmt.training - Epoch  11, Step:   107300, Batch Loss:     1.729339, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-07-27 10:48:29,998 - INFO - joeynmt.training - Epoch  11, Step:   107400, Batch Loss:     1.978121, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-07-27 10:48:43,456 - INFO - joeynmt.training - Epoch  11, Step:   107500, Batch Loss:     1.903863, Tokens per Sec:    15889, Lr: 0.000300\n",
      "2021-07-27 10:48:56,798 - INFO - joeynmt.training - Epoch  11, Step:   107600, Batch Loss:     2.243601, Tokens per Sec:    15853, Lr: 0.000300\n",
      "2021-07-27 10:49:10,128 - INFO - joeynmt.training - Epoch  11, Step:   107700, Batch Loss:     1.789538, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-27 10:49:23,700 - INFO - joeynmt.training - Epoch  11, Step:   107800, Batch Loss:     1.870592, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-07-27 10:49:37,345 - INFO - joeynmt.training - Epoch  11, Step:   107900, Batch Loss:     1.767558, Tokens per Sec:    15861, Lr: 0.000300\n",
      "2021-07-27 10:49:51,082 - INFO - joeynmt.training - Epoch  11, Step:   108000, Batch Loss:     1.744702, Tokens per Sec:    15662, Lr: 0.000300\n",
      "2021-07-27 10:50:08,781 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:50:08,782 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:50:08,782 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:50:09,013 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:50:09,014 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:50:09,756 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:50:09,757 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:50:09,757 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:50:09,757 - INFO - joeynmt.training - \tHypothesis: [ I ] hated what you do good and suffer when you endure , that is what is good to God . ”\n",
      "2021-07-27 10:50:09,758 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:50:09,758 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:50:09,758 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:50:09,758 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son , whom I have approved . ”\n",
      "2021-07-27 10:50:09,759 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:50:09,759 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:50:09,759 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:50:09,759 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 10:50:09,760 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:50:09,760 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:50:09,760 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:50:09,761 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 10:50:09,761 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   108000: bleu:  26.57, loss: 44403.1367, ppl:   5.3255, duration: 18.6783s\n",
      "2021-07-27 10:50:23,504 - INFO - joeynmt.training - Epoch  11, Step:   108100, Batch Loss:     1.893469, Tokens per Sec:    15628, Lr: 0.000300\n",
      "2021-07-27 10:50:36,978 - INFO - joeynmt.training - Epoch  11, Step:   108200, Batch Loss:     1.481914, Tokens per Sec:    15583, Lr: 0.000300\n",
      "2021-07-27 10:50:50,601 - INFO - joeynmt.training - Epoch  11, Step:   108300, Batch Loss:     2.047378, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-07-27 10:51:03,888 - INFO - joeynmt.training - Epoch  11, Step:   108400, Batch Loss:     1.951180, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-07-27 10:51:06,268 - INFO - joeynmt.training - Epoch  11: total training loss 5106.66\n",
      "2021-07-27 10:51:06,268 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-27 10:51:17,947 - INFO - joeynmt.training - Epoch  12, Step:   108500, Batch Loss:     1.926268, Tokens per Sec:    15508, Lr: 0.000300\n",
      "2021-07-27 10:51:31,368 - INFO - joeynmt.training - Epoch  12, Step:   108600, Batch Loss:     1.832433, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-07-27 10:51:44,954 - INFO - joeynmt.training - Epoch  12, Step:   108700, Batch Loss:     1.781834, Tokens per Sec:    16315, Lr: 0.000300\n",
      "2021-07-27 10:51:58,491 - INFO - joeynmt.training - Epoch  12, Step:   108800, Batch Loss:     1.848891, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-07-27 10:52:11,790 - INFO - joeynmt.training - Epoch  12, Step:   108900, Batch Loss:     1.943676, Tokens per Sec:    15713, Lr: 0.000300\n",
      "2021-07-27 10:52:25,471 - INFO - joeynmt.training - Epoch  12, Step:   109000, Batch Loss:     2.012141, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-07-27 10:52:38,822 - INFO - joeynmt.training - Epoch  12, Step:   109100, Batch Loss:     1.941175, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-07-27 10:52:52,352 - INFO - joeynmt.training - Epoch  12, Step:   109200, Batch Loss:     1.959978, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-27 10:53:05,805 - INFO - joeynmt.training - Epoch  12, Step:   109300, Batch Loss:     1.869633, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-07-27 10:53:19,345 - INFO - joeynmt.training - Epoch  12, Step:   109400, Batch Loss:     1.828173, Tokens per Sec:    15998, Lr: 0.000300\n",
      "2021-07-27 10:53:32,736 - INFO - joeynmt.training - Epoch  12, Step:   109500, Batch Loss:     1.860335, Tokens per Sec:    15765, Lr: 0.000300\n",
      "2021-07-27 10:53:46,271 - INFO - joeynmt.training - Epoch  12, Step:   109600, Batch Loss:     1.888128, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-07-27 10:53:59,681 - INFO - joeynmt.training - Epoch  12, Step:   109700, Batch Loss:     1.946231, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-07-27 10:54:13,071 - INFO - joeynmt.training - Epoch  12, Step:   109800, Batch Loss:     1.953028, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-07-27 10:54:26,585 - INFO - joeynmt.training - Epoch  12, Step:   109900, Batch Loss:     1.377746, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-07-27 10:54:40,157 - INFO - joeynmt.training - Epoch  12, Step:   110000, Batch Loss:     1.968811, Tokens per Sec:    15654, Lr: 0.000300\n",
      "2021-07-27 10:54:53,470 - INFO - joeynmt.training - Epoch  12, Step:   110100, Batch Loss:     1.688485, Tokens per Sec:    15684, Lr: 0.000300\n",
      "2021-07-27 10:55:06,796 - INFO - joeynmt.training - Epoch  12, Step:   110200, Batch Loss:     2.051217, Tokens per Sec:    15792, Lr: 0.000300\n",
      "2021-07-27 10:55:20,331 - INFO - joeynmt.training - Epoch  12, Step:   110300, Batch Loss:     1.904778, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-07-27 10:55:33,817 - INFO - joeynmt.training - Epoch  12, Step:   110400, Batch Loss:     1.813482, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-07-27 10:55:47,322 - INFO - joeynmt.training - Epoch  12, Step:   110500, Batch Loss:     1.765180, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-07-27 10:56:00,670 - INFO - joeynmt.training - Epoch  12, Step:   110600, Batch Loss:     1.830447, Tokens per Sec:    15675, Lr: 0.000300\n",
      "2021-07-27 10:56:14,171 - INFO - joeynmt.training - Epoch  12, Step:   110700, Batch Loss:     1.925503, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-27 10:56:27,588 - INFO - joeynmt.training - Epoch  12, Step:   110800, Batch Loss:     1.831316, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-07-27 10:56:41,274 - INFO - joeynmt.training - Epoch  12, Step:   110900, Batch Loss:     1.824526, Tokens per Sec:    15687, Lr: 0.000300\n",
      "2021-07-27 10:56:54,857 - INFO - joeynmt.training - Epoch  12, Step:   111000, Batch Loss:     1.967317, Tokens per Sec:    15553, Lr: 0.000300\n",
      "2021-07-27 10:57:12,309 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 10:57:12,310 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 10:57:12,310 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 10:57:12,556 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 10:57:12,557 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 10:57:13,309 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 10:57:13,312 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 10:57:13,312 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 10:57:13,312 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is acceptable to God . ”\n",
      "2021-07-27 10:57:13,312 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 10:57:13,313 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 10:57:13,313 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 10:57:13,313 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 10:57:13,313 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 10:57:13,314 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 10:57:13,314 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 10:57:13,315 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 10:57:13,315 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 10:57:13,315 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 10:57:13,316 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 10:57:13,317 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 10:57:13,317 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   111000: bleu:  26.83, loss: 44372.0156, ppl:   5.3192, duration: 18.4598s\n",
      "2021-07-27 10:57:26,938 - INFO - joeynmt.training - Epoch  12, Step:   111100, Batch Loss:     1.943334, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-07-27 10:57:39,811 - INFO - joeynmt.training - Epoch  12: total training loss 5078.03\n",
      "2021-07-27 10:57:39,811 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-27 10:57:40,782 - INFO - joeynmt.training - Epoch  13, Step:   111200, Batch Loss:     1.780708, Tokens per Sec:    10842, Lr: 0.000300\n",
      "2021-07-27 10:57:54,120 - INFO - joeynmt.training - Epoch  13, Step:   111300, Batch Loss:     1.798279, Tokens per Sec:    15937, Lr: 0.000300\n",
      "2021-07-27 10:58:07,481 - INFO - joeynmt.training - Epoch  13, Step:   111400, Batch Loss:     1.867033, Tokens per Sec:    16091, Lr: 0.000300\n",
      "2021-07-27 10:58:20,730 - INFO - joeynmt.training - Epoch  13, Step:   111500, Batch Loss:     1.588968, Tokens per Sec:    15648, Lr: 0.000300\n",
      "2021-07-27 10:58:34,300 - INFO - joeynmt.training - Epoch  13, Step:   111600, Batch Loss:     1.805986, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-07-27 10:58:47,667 - INFO - joeynmt.training - Epoch  13, Step:   111700, Batch Loss:     1.758603, Tokens per Sec:    15906, Lr: 0.000300\n",
      "2021-07-27 10:59:01,022 - INFO - joeynmt.training - Epoch  13, Step:   111800, Batch Loss:     1.848590, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-07-27 10:59:14,262 - INFO - joeynmt.training - Epoch  13, Step:   111900, Batch Loss:     1.613866, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-07-27 10:59:27,722 - INFO - joeynmt.training - Epoch  13, Step:   112000, Batch Loss:     1.850833, Tokens per Sec:    16049, Lr: 0.000300\n",
      "2021-07-27 10:59:41,201 - INFO - joeynmt.training - Epoch  13, Step:   112100, Batch Loss:     1.824349, Tokens per Sec:    15666, Lr: 0.000300\n",
      "2021-07-27 10:59:54,699 - INFO - joeynmt.training - Epoch  13, Step:   112200, Batch Loss:     1.933044, Tokens per Sec:    16300, Lr: 0.000300\n",
      "2021-07-27 11:00:07,982 - INFO - joeynmt.training - Epoch  13, Step:   112300, Batch Loss:     1.874406, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-07-27 11:00:21,329 - INFO - joeynmt.training - Epoch  13, Step:   112400, Batch Loss:     1.786674, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-07-27 11:00:34,880 - INFO - joeynmt.training - Epoch  13, Step:   112500, Batch Loss:     2.010189, Tokens per Sec:    15850, Lr: 0.000300\n",
      "2021-07-27 11:00:48,380 - INFO - joeynmt.training - Epoch  13, Step:   112600, Batch Loss:     1.752066, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-07-27 11:01:01,761 - INFO - joeynmt.training - Epoch  13, Step:   112700, Batch Loss:     1.770912, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-07-27 11:01:15,351 - INFO - joeynmt.training - Epoch  13, Step:   112800, Batch Loss:     1.931817, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-07-27 11:01:28,681 - INFO - joeynmt.training - Epoch  13, Step:   112900, Batch Loss:     1.739340, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-27 11:01:42,151 - INFO - joeynmt.training - Epoch  13, Step:   113000, Batch Loss:     2.178514, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-07-27 11:01:55,730 - INFO - joeynmt.training - Epoch  13, Step:   113100, Batch Loss:     2.028035, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-07-27 11:02:09,340 - INFO - joeynmt.training - Epoch  13, Step:   113200, Batch Loss:     1.873379, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-07-27 11:02:22,865 - INFO - joeynmt.training - Epoch  13, Step:   113300, Batch Loss:     1.815262, Tokens per Sec:    15931, Lr: 0.000300\n",
      "2021-07-27 11:02:36,265 - INFO - joeynmt.training - Epoch  13, Step:   113400, Batch Loss:     2.013551, Tokens per Sec:    16084, Lr: 0.000300\n",
      "2021-07-27 11:02:49,651 - INFO - joeynmt.training - Epoch  13, Step:   113500, Batch Loss:     1.926468, Tokens per Sec:    16165, Lr: 0.000300\n",
      "2021-07-27 11:03:02,924 - INFO - joeynmt.training - Epoch  13, Step:   113600, Batch Loss:     1.910477, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-07-27 11:03:16,519 - INFO - joeynmt.training - Epoch  13, Step:   113700, Batch Loss:     1.264121, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-07-27 11:03:29,952 - INFO - joeynmt.training - Epoch  13, Step:   113800, Batch Loss:     1.754069, Tokens per Sec:    15669, Lr: 0.000300\n",
      "2021-07-27 11:03:43,450 - INFO - joeynmt.training - Epoch  13, Step:   113900, Batch Loss:     1.802698, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-27 11:03:53,759 - INFO - joeynmt.training - Epoch  13: total training loss 5069.92\n",
      "2021-07-27 11:03:53,759 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-27 11:03:56,937 - INFO - joeynmt.training - Epoch  14, Step:   114000, Batch Loss:     1.980555, Tokens per Sec:    15010, Lr: 0.000300\n",
      "2021-07-27 11:04:14,219 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 11:04:14,220 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 11:04:14,220 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 11:04:14,466 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 11:04:14,466 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 11:04:15,933 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 11:04:15,934 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 11:04:15,934 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 11:04:15,934 - INFO - joeynmt.training - \tHypothesis: [ I ] hated you when you do good and suffer when you endure , that is acceptable to God . ”\n",
      "2021-07-27 11:04:15,934 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 11:04:15,935 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 11:04:15,935 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:04:15,935 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven saying : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 11:04:15,935 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 11:04:15,936 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 11:04:15,936 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 11:04:15,936 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 11:04:15,936 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 11:04:15,937 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 11:04:15,937 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 11:04:15,937 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 11:04:15,937 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   114000: bleu:  26.73, loss: 44362.8789, ppl:   5.3174, duration: 18.9996s\n",
      "2021-07-27 11:04:29,729 - INFO - joeynmt.training - Epoch  14, Step:   114100, Batch Loss:     1.636947, Tokens per Sec:    15398, Lr: 0.000300\n",
      "2021-07-27 11:04:43,085 - INFO - joeynmt.training - Epoch  14, Step:   114200, Batch Loss:     1.813557, Tokens per Sec:    15985, Lr: 0.000300\n",
      "2021-07-27 11:04:56,572 - INFO - joeynmt.training - Epoch  14, Step:   114300, Batch Loss:     1.802650, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-07-27 11:05:09,901 - INFO - joeynmt.training - Epoch  14, Step:   114400, Batch Loss:     1.894463, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-07-27 11:05:23,383 - INFO - joeynmt.training - Epoch  14, Step:   114500, Batch Loss:     1.850809, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-07-27 11:05:36,772 - INFO - joeynmt.training - Epoch  14, Step:   114600, Batch Loss:     1.641546, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-07-27 11:05:50,261 - INFO - joeynmt.training - Epoch  14, Step:   114700, Batch Loss:     1.793152, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-07-27 11:06:03,761 - INFO - joeynmt.training - Epoch  14, Step:   114800, Batch Loss:     1.595727, Tokens per Sec:    16073, Lr: 0.000300\n",
      "2021-07-27 11:06:17,425 - INFO - joeynmt.training - Epoch  14, Step:   114900, Batch Loss:     1.916461, Tokens per Sec:    16216, Lr: 0.000300\n",
      "2021-07-27 11:06:30,955 - INFO - joeynmt.training - Epoch  14, Step:   115000, Batch Loss:     1.791947, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-07-27 11:06:44,646 - INFO - joeynmt.training - Epoch  14, Step:   115100, Batch Loss:     1.993948, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-07-27 11:06:58,116 - INFO - joeynmt.training - Epoch  14, Step:   115200, Batch Loss:     1.824200, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-07-27 11:07:11,457 - INFO - joeynmt.training - Epoch  14, Step:   115300, Batch Loss:     1.777173, Tokens per Sec:    16252, Lr: 0.000300\n",
      "2021-07-27 11:07:24,755 - INFO - joeynmt.training - Epoch  14, Step:   115400, Batch Loss:     1.741851, Tokens per Sec:    16114, Lr: 0.000300\n",
      "2021-07-27 11:07:38,275 - INFO - joeynmt.training - Epoch  14, Step:   115500, Batch Loss:     1.955030, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-27 11:07:51,506 - INFO - joeynmt.training - Epoch  14, Step:   115600, Batch Loss:     1.284534, Tokens per Sec:    15407, Lr: 0.000300\n",
      "2021-07-27 11:08:05,068 - INFO - joeynmt.training - Epoch  14, Step:   115700, Batch Loss:     1.535331, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-07-27 11:08:18,507 - INFO - joeynmt.training - Epoch  14, Step:   115800, Batch Loss:     1.491369, Tokens per Sec:    15449, Lr: 0.000300\n",
      "2021-07-27 11:08:31,999 - INFO - joeynmt.training - Epoch  14, Step:   115900, Batch Loss:     1.306369, Tokens per Sec:    15725, Lr: 0.000300\n",
      "2021-07-27 11:08:45,377 - INFO - joeynmt.training - Epoch  14, Step:   116000, Batch Loss:     1.812070, Tokens per Sec:    16212, Lr: 0.000300\n",
      "2021-07-27 11:08:58,699 - INFO - joeynmt.training - Epoch  14, Step:   116100, Batch Loss:     1.612972, Tokens per Sec:    16212, Lr: 0.000300\n",
      "2021-07-27 11:09:12,092 - INFO - joeynmt.training - Epoch  14, Step:   116200, Batch Loss:     1.700735, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-07-27 11:09:25,450 - INFO - joeynmt.training - Epoch  14, Step:   116300, Batch Loss:     1.968701, Tokens per Sec:    15716, Lr: 0.000300\n",
      "2021-07-27 11:09:39,037 - INFO - joeynmt.training - Epoch  14, Step:   116400, Batch Loss:     1.695244, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-07-27 11:09:52,478 - INFO - joeynmt.training - Epoch  14, Step:   116500, Batch Loss:     2.008041, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-07-27 11:10:05,822 - INFO - joeynmt.training - Epoch  14, Step:   116600, Batch Loss:     1.875832, Tokens per Sec:    16182, Lr: 0.000300\n",
      "2021-07-27 11:10:19,073 - INFO - joeynmt.training - Epoch  14, Step:   116700, Batch Loss:     1.854131, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-27 11:10:27,790 - INFO - joeynmt.training - Epoch  14: total training loss 5066.73\n",
      "2021-07-27 11:10:27,791 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-27 11:10:32,994 - INFO - joeynmt.training - Epoch  15, Step:   116800, Batch Loss:     1.836533, Tokens per Sec:    15169, Lr: 0.000300\n",
      "2021-07-27 11:10:46,247 - INFO - joeynmt.training - Epoch  15, Step:   116900, Batch Loss:     1.837541, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-07-27 11:10:59,707 - INFO - joeynmt.training - Epoch  15, Step:   117000, Batch Loss:     1.715250, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-07-27 11:11:18,324 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 11:11:18,325 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 11:11:18,325 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 11:11:18,546 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 11:11:18,546 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 11:11:19,246 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 11:11:19,247 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 11:11:19,247 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 11:11:19,247 - INFO - joeynmt.training - \tHypothesis: I hated how you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
      "2021-07-27 11:11:19,247 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 11:11:19,248 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 11:11:19,248 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:11:19,248 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 11:11:19,249 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 11:11:19,249 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 11:11:19,249 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 11:11:19,249 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 11:11:19,249 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 11:11:19,250 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 11:11:19,250 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 11:11:19,250 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 11:11:19,250 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   117000: bleu:  26.84, loss: 44125.0430, ppl:   5.2700, duration: 19.5430s\n",
      "2021-07-27 11:11:32,828 - INFO - joeynmt.training - Epoch  15, Step:   117100, Batch Loss:     1.449080, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-07-27 11:11:46,515 - INFO - joeynmt.training - Epoch  15, Step:   117200, Batch Loss:     2.019717, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-07-27 11:11:59,843 - INFO - joeynmt.training - Epoch  15, Step:   117300, Batch Loss:     1.755102, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-27 11:12:13,128 - INFO - joeynmt.training - Epoch  15, Step:   117400, Batch Loss:     1.856273, Tokens per Sec:    16083, Lr: 0.000300\n",
      "2021-07-27 11:12:26,413 - INFO - joeynmt.training - Epoch  15, Step:   117500, Batch Loss:     1.975716, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-07-27 11:12:39,805 - INFO - joeynmt.training - Epoch  15, Step:   117600, Batch Loss:     1.866301, Tokens per Sec:    15544, Lr: 0.000300\n",
      "2021-07-27 11:12:53,409 - INFO - joeynmt.training - Epoch  15, Step:   117700, Batch Loss:     1.521938, Tokens per Sec:    15754, Lr: 0.000300\n",
      "2021-07-27 11:13:06,875 - INFO - joeynmt.training - Epoch  15, Step:   117800, Batch Loss:     1.883052, Tokens per Sec:    16180, Lr: 0.000300\n",
      "2021-07-27 11:13:20,481 - INFO - joeynmt.training - Epoch  15, Step:   117900, Batch Loss:     1.600144, Tokens per Sec:    15772, Lr: 0.000300\n",
      "2021-07-27 11:13:33,945 - INFO - joeynmt.training - Epoch  15, Step:   118000, Batch Loss:     1.921808, Tokens per Sec:    15760, Lr: 0.000300\n",
      "2021-07-27 11:13:47,496 - INFO - joeynmt.training - Epoch  15, Step:   118100, Batch Loss:     1.688529, Tokens per Sec:    16377, Lr: 0.000300\n",
      "2021-07-27 11:14:00,646 - INFO - joeynmt.training - Epoch  15, Step:   118200, Batch Loss:     1.062916, Tokens per Sec:    15621, Lr: 0.000300\n",
      "2021-07-27 11:14:14,197 - INFO - joeynmt.training - Epoch  15, Step:   118300, Batch Loss:     1.744373, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-07-27 11:14:27,841 - INFO - joeynmt.training - Epoch  15, Step:   118400, Batch Loss:     1.932298, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-07-27 11:14:41,163 - INFO - joeynmt.training - Epoch  15, Step:   118500, Batch Loss:     1.902504, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-07-27 11:14:54,389 - INFO - joeynmt.training - Epoch  15, Step:   118600, Batch Loss:     1.874550, Tokens per Sec:    15696, Lr: 0.000300\n",
      "2021-07-27 11:15:07,811 - INFO - joeynmt.training - Epoch  15, Step:   118700, Batch Loss:     1.813887, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-07-27 11:15:21,348 - INFO - joeynmt.training - Epoch  15, Step:   118800, Batch Loss:     1.642508, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-27 11:15:34,980 - INFO - joeynmt.training - Epoch  15, Step:   118900, Batch Loss:     1.888323, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-07-27 11:15:48,499 - INFO - joeynmt.training - Epoch  15, Step:   119000, Batch Loss:     1.678540, Tokens per Sec:    16247, Lr: 0.000300\n",
      "2021-07-27 11:16:01,927 - INFO - joeynmt.training - Epoch  15, Step:   119100, Batch Loss:     2.021519, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-07-27 11:16:15,353 - INFO - joeynmt.training - Epoch  15, Step:   119200, Batch Loss:     1.944542, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-07-27 11:16:29,040 - INFO - joeynmt.training - Epoch  15, Step:   119300, Batch Loss:     1.970878, Tokens per Sec:    16393, Lr: 0.000300\n",
      "2021-07-27 11:16:42,598 - INFO - joeynmt.training - Epoch  15, Step:   119400, Batch Loss:     1.997720, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-07-27 11:16:56,197 - INFO - joeynmt.training - Epoch  15, Step:   119500, Batch Loss:     1.490744, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-27 11:17:01,924 - INFO - joeynmt.training - Epoch  15: total training loss 5041.73\n",
      "2021-07-27 11:17:01,924 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-27 11:17:09,892 - INFO - joeynmt.training - Epoch  16, Step:   119600, Batch Loss:     1.853469, Tokens per Sec:    15260, Lr: 0.000300\n",
      "2021-07-27 11:17:23,424 - INFO - joeynmt.training - Epoch  16, Step:   119700, Batch Loss:     1.746125, Tokens per Sec:    16426, Lr: 0.000300\n",
      "2021-07-27 11:17:36,890 - INFO - joeynmt.training - Epoch  16, Step:   119800, Batch Loss:     2.009074, Tokens per Sec:    15708, Lr: 0.000300\n",
      "2021-07-27 11:17:50,473 - INFO - joeynmt.training - Epoch  16, Step:   119900, Batch Loss:     1.582069, Tokens per Sec:    15502, Lr: 0.000300\n",
      "2021-07-27 11:18:03,949 - INFO - joeynmt.training - Epoch  16, Step:   120000, Batch Loss:     1.900785, Tokens per Sec:    16270, Lr: 0.000300\n",
      "2021-07-27 11:18:21,771 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 11:18:21,771 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 11:18:21,771 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 11:18:22,694 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 11:18:22,695 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 11:18:22,695 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 11:18:22,695 - INFO - joeynmt.training - \tHypothesis: “ If you do good and do good and suffer when you endure , this is acceptable to God . ”\n",
      "2021-07-27 11:18:22,695 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 11:18:22,696 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 11:18:22,696 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:18:22,696 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:18:22,697 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 11:18:22,698 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 11:18:22,698 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 11:18:22,698 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 11:18:22,698 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 11:18:22,699 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 11:18:22,699 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 11:18:22,699 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 11:18:22,699 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   120000: bleu:  26.80, loss: 44297.3555, ppl:   5.3043, duration: 18.7501s\n",
      "2021-07-27 11:18:36,255 - INFO - joeynmt.training - Epoch  16, Step:   120100, Batch Loss:     1.787212, Tokens per Sec:    15629, Lr: 0.000300\n",
      "2021-07-27 11:18:49,783 - INFO - joeynmt.training - Epoch  16, Step:   120200, Batch Loss:     1.390701, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-07-27 11:19:03,327 - INFO - joeynmt.training - Epoch  16, Step:   120300, Batch Loss:     1.650149, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-07-27 11:19:16,763 - INFO - joeynmt.training - Epoch  16, Step:   120400, Batch Loss:     1.847957, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-07-27 11:19:30,306 - INFO - joeynmt.training - Epoch  16, Step:   120500, Batch Loss:     1.800899, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-27 11:19:43,752 - INFO - joeynmt.training - Epoch  16, Step:   120600, Batch Loss:     1.773654, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-07-27 11:19:57,288 - INFO - joeynmt.training - Epoch  16, Step:   120700, Batch Loss:     2.070245, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-07-27 11:20:10,746 - INFO - joeynmt.training - Epoch  16, Step:   120800, Batch Loss:     1.803491, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-07-27 11:20:24,371 - INFO - joeynmt.training - Epoch  16, Step:   120900, Batch Loss:     1.587273, Tokens per Sec:    16072, Lr: 0.000300\n",
      "2021-07-27 11:20:37,825 - INFO - joeynmt.training - Epoch  16, Step:   121000, Batch Loss:     1.787346, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-07-27 11:20:51,135 - INFO - joeynmt.training - Epoch  16, Step:   121100, Batch Loss:     1.788024, Tokens per Sec:    15899, Lr: 0.000300\n",
      "2021-07-27 11:21:04,575 - INFO - joeynmt.training - Epoch  16, Step:   121200, Batch Loss:     1.894739, Tokens per Sec:    15962, Lr: 0.000300\n",
      "2021-07-27 11:21:17,796 - INFO - joeynmt.training - Epoch  16, Step:   121300, Batch Loss:     1.829226, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-07-27 11:21:31,363 - INFO - joeynmt.training - Epoch  16, Step:   121400, Batch Loss:     2.252816, Tokens per Sec:    15684, Lr: 0.000300\n",
      "2021-07-27 11:21:44,799 - INFO - joeynmt.training - Epoch  16, Step:   121500, Batch Loss:     1.910116, Tokens per Sec:    15787, Lr: 0.000300\n",
      "2021-07-27 11:21:58,115 - INFO - joeynmt.training - Epoch  16, Step:   121600, Batch Loss:     1.808240, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-07-27 11:22:11,460 - INFO - joeynmt.training - Epoch  16, Step:   121700, Batch Loss:     1.887556, Tokens per Sec:    16076, Lr: 0.000300\n",
      "2021-07-27 11:22:24,655 - INFO - joeynmt.training - Epoch  16, Step:   121800, Batch Loss:     1.909616, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-07-27 11:22:38,138 - INFO - joeynmt.training - Epoch  16, Step:   121900, Batch Loss:     1.831565, Tokens per Sec:    15852, Lr: 0.000300\n",
      "2021-07-27 11:22:51,764 - INFO - joeynmt.training - Epoch  16, Step:   122000, Batch Loss:     1.795582, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-07-27 11:23:05,184 - INFO - joeynmt.training - Epoch  16, Step:   122100, Batch Loss:     1.805004, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-07-27 11:23:18,623 - INFO - joeynmt.training - Epoch  16, Step:   122200, Batch Loss:     1.760607, Tokens per Sec:    16274, Lr: 0.000300\n",
      "2021-07-27 11:23:32,151 - INFO - joeynmt.training - Epoch  16, Step:   122300, Batch Loss:     1.975296, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-07-27 11:23:35,452 - INFO - joeynmt.training - Epoch  16: total training loss 5033.49\n",
      "2021-07-27 11:23:35,453 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-27 11:23:46,153 - INFO - joeynmt.training - Epoch  17, Step:   122400, Batch Loss:     1.849933, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-07-27 11:23:59,640 - INFO - joeynmt.training - Epoch  17, Step:   122500, Batch Loss:     2.111824, Tokens per Sec:    15744, Lr: 0.000300\n",
      "2021-07-27 11:24:13,114 - INFO - joeynmt.training - Epoch  17, Step:   122600, Batch Loss:     1.853371, Tokens per Sec:    15683, Lr: 0.000300\n",
      "2021-07-27 11:24:26,510 - INFO - joeynmt.training - Epoch  17, Step:   122700, Batch Loss:     1.910019, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-27 11:24:39,600 - INFO - joeynmt.training - Epoch  17, Step:   122800, Batch Loss:     1.934539, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-07-27 11:24:52,803 - INFO - joeynmt.training - Epoch  17, Step:   122900, Batch Loss:     2.012050, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-07-27 11:25:06,251 - INFO - joeynmt.training - Epoch  17, Step:   123000, Batch Loss:     1.658723, Tokens per Sec:    16206, Lr: 0.000300\n",
      "2021-07-27 11:25:23,385 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 11:25:23,385 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 11:25:23,386 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 11:25:24,418 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 11:25:24,418 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 11:25:24,419 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 11:25:24,419 - INFO - joeynmt.training - \tHypothesis: If you do good , you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
      "2021-07-27 11:25:24,419 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 11:25:24,419 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 11:25:24,420 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:25:24,420 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son whom I have approved . ”\n",
      "2021-07-27 11:25:24,420 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 11:25:24,420 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 11:25:24,420 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 11:25:24,421 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 11:25:24,421 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 11:25:24,421 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 11:25:24,422 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 11:25:24,422 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 11:25:24,422 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   123000: bleu:  27.02, loss: 44191.5820, ppl:   5.2832, duration: 18.1704s\n",
      "2021-07-27 11:25:37,895 - INFO - joeynmt.training - Epoch  17, Step:   123100, Batch Loss:     1.810468, Tokens per Sec:    15381, Lr: 0.000300\n",
      "2021-07-27 11:25:51,304 - INFO - joeynmt.training - Epoch  17, Step:   123200, Batch Loss:     1.643294, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-07-27 11:26:04,837 - INFO - joeynmt.training - Epoch  17, Step:   123300, Batch Loss:     1.579222, Tokens per Sec:    16338, Lr: 0.000300\n",
      "2021-07-27 11:26:18,441 - INFO - joeynmt.training - Epoch  17, Step:   123400, Batch Loss:     1.705208, Tokens per Sec:    16237, Lr: 0.000300\n",
      "2021-07-27 11:26:32,025 - INFO - joeynmt.training - Epoch  17, Step:   123500, Batch Loss:     1.844436, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-07-27 11:26:45,464 - INFO - joeynmt.training - Epoch  17, Step:   123600, Batch Loss:     1.901414, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-07-27 11:26:58,972 - INFO - joeynmt.training - Epoch  17, Step:   123700, Batch Loss:     2.010334, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-07-27 11:27:12,234 - INFO - joeynmt.training - Epoch  17, Step:   123800, Batch Loss:     1.538959, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-07-27 11:27:25,657 - INFO - joeynmt.training - Epoch  17, Step:   123900, Batch Loss:     1.928604, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-07-27 11:27:39,186 - INFO - joeynmt.training - Epoch  17, Step:   124000, Batch Loss:     1.684687, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-07-27 11:27:52,882 - INFO - joeynmt.training - Epoch  17, Step:   124100, Batch Loss:     1.923838, Tokens per Sec:    16159, Lr: 0.000300\n",
      "2021-07-27 11:28:06,358 - INFO - joeynmt.training - Epoch  17, Step:   124200, Batch Loss:     1.983142, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-07-27 11:28:19,673 - INFO - joeynmt.training - Epoch  17, Step:   124300, Batch Loss:     1.980574, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-07-27 11:28:33,162 - INFO - joeynmt.training - Epoch  17, Step:   124400, Batch Loss:     1.810889, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-07-27 11:28:46,537 - INFO - joeynmt.training - Epoch  17, Step:   124500, Batch Loss:     1.481271, Tokens per Sec:    15646, Lr: 0.000300\n",
      "2021-07-27 11:29:00,031 - INFO - joeynmt.training - Epoch  17, Step:   124600, Batch Loss:     1.901384, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-07-27 11:29:13,474 - INFO - joeynmt.training - Epoch  17, Step:   124700, Batch Loss:     1.769974, Tokens per Sec:    16285, Lr: 0.000300\n",
      "2021-07-27 11:29:26,710 - INFO - joeynmt.training - Epoch  17, Step:   124800, Batch Loss:     1.610611, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-07-27 11:29:40,133 - INFO - joeynmt.training - Epoch  17, Step:   124900, Batch Loss:     1.936934, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-07-27 11:29:53,688 - INFO - joeynmt.training - Epoch  17, Step:   125000, Batch Loss:     1.809692, Tokens per Sec:    16030, Lr: 0.000300\n",
      "2021-07-27 11:30:07,223 - INFO - joeynmt.training - Epoch  17, Step:   125100, Batch Loss:     1.921849, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-07-27 11:30:08,466 - INFO - joeynmt.training - Epoch  17: total training loss 5027.81\n",
      "2021-07-27 11:30:08,466 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-27 11:30:20,916 - INFO - joeynmt.training - Epoch  18, Step:   125200, Batch Loss:     1.876512, Tokens per Sec:    15394, Lr: 0.000300\n",
      "2021-07-27 11:30:34,376 - INFO - joeynmt.training - Epoch  18, Step:   125300, Batch Loss:     1.821636, Tokens per Sec:    15749, Lr: 0.000300\n",
      "2021-07-27 11:30:47,692 - INFO - joeynmt.training - Epoch  18, Step:   125400, Batch Loss:     1.678253, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-07-27 11:31:01,255 - INFO - joeynmt.training - Epoch  18, Step:   125500, Batch Loss:     1.715869, Tokens per Sec:    16355, Lr: 0.000300\n",
      "2021-07-27 11:31:14,708 - INFO - joeynmt.training - Epoch  18, Step:   125600, Batch Loss:     1.893380, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-27 11:31:28,388 - INFO - joeynmt.training - Epoch  18, Step:   125700, Batch Loss:     1.739771, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-07-27 11:31:41,915 - INFO - joeynmt.training - Epoch  18, Step:   125800, Batch Loss:     1.749242, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-07-27 11:31:55,454 - INFO - joeynmt.training - Epoch  18, Step:   125900, Batch Loss:     1.593762, Tokens per Sec:    16415, Lr: 0.000300\n",
      "2021-07-27 11:32:08,814 - INFO - joeynmt.training - Epoch  18, Step:   126000, Batch Loss:     1.791750, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-07-27 11:32:26,821 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 11:32:26,821 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 11:32:26,821 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 11:32:27,062 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 11:32:27,062 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 11:32:27,849 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 11:32:27,857 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 11:32:27,858 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 11:32:27,858 - INFO - joeynmt.training - \tHypothesis: “ If you do good , you are suffering when you endure , that is acceptable to God . ”\n",
      "2021-07-27 11:32:27,858 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 11:32:27,859 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 11:32:27,859 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:32:27,859 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 11:32:27,859 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 11:32:27,860 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 11:32:27,860 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 11:32:27,860 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 11:32:27,860 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 11:32:27,861 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 11:32:27,861 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 11:32:27,861 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 11:32:27,861 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   126000: bleu:  27.03, loss: 43863.4844, ppl:   5.2183, duration: 19.0470s\n",
      "2021-07-27 11:32:41,760 - INFO - joeynmt.training - Epoch  18, Step:   126100, Batch Loss:     1.871272, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-07-27 11:32:54,922 - INFO - joeynmt.training - Epoch  18, Step:   126200, Batch Loss:     1.803677, Tokens per Sec:    15460, Lr: 0.000300\n",
      "2021-07-27 11:33:08,349 - INFO - joeynmt.training - Epoch  18, Step:   126300, Batch Loss:     1.731071, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-07-27 11:33:21,739 - INFO - joeynmt.training - Epoch  18, Step:   126400, Batch Loss:     1.853855, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-07-27 11:33:35,329 - INFO - joeynmt.training - Epoch  18, Step:   126500, Batch Loss:     1.806097, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-07-27 11:33:48,961 - INFO - joeynmt.training - Epoch  18, Step:   126600, Batch Loss:     1.929283, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-27 11:34:02,429 - INFO - joeynmt.training - Epoch  18, Step:   126700, Batch Loss:     1.835263, Tokens per Sec:    16141, Lr: 0.000300\n",
      "2021-07-27 11:34:15,512 - INFO - joeynmt.training - Epoch  18, Step:   126800, Batch Loss:     1.733098, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-07-27 11:34:28,914 - INFO - joeynmt.training - Epoch  18, Step:   126900, Batch Loss:     1.723578, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-07-27 11:34:42,469 - INFO - joeynmt.training - Epoch  18, Step:   127000, Batch Loss:     1.923396, Tokens per Sec:    16188, Lr: 0.000300\n",
      "2021-07-27 11:34:56,060 - INFO - joeynmt.training - Epoch  18, Step:   127100, Batch Loss:     1.814207, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-07-27 11:35:09,449 - INFO - joeynmt.training - Epoch  18, Step:   127200, Batch Loss:     1.831101, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-27 11:35:23,023 - INFO - joeynmt.training - Epoch  18, Step:   127300, Batch Loss:     1.672062, Tokens per Sec:    16082, Lr: 0.000300\n",
      "2021-07-27 11:35:36,602 - INFO - joeynmt.training - Epoch  18, Step:   127400, Batch Loss:     1.770257, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-07-27 11:35:49,942 - INFO - joeynmt.training - Epoch  18, Step:   127500, Batch Loss:     1.872157, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-07-27 11:36:03,138 - INFO - joeynmt.training - Epoch  18, Step:   127600, Batch Loss:     1.867348, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-07-27 11:36:16,618 - INFO - joeynmt.training - Epoch  18, Step:   127700, Batch Loss:     1.577699, Tokens per Sec:    16109, Lr: 0.000300\n",
      "2021-07-27 11:36:30,107 - INFO - joeynmt.training - Epoch  18, Step:   127800, Batch Loss:     1.688912, Tokens per Sec:    15669, Lr: 0.000300\n",
      "2021-07-27 11:36:41,755 - INFO - joeynmt.training - Epoch  18: total training loss 5000.72\n",
      "2021-07-27 11:36:41,755 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-27 11:36:43,742 - INFO - joeynmt.training - Epoch  19, Step:   127900, Batch Loss:     1.829767, Tokens per Sec:    12534, Lr: 0.000300\n",
      "2021-07-27 11:36:57,186 - INFO - joeynmt.training - Epoch  19, Step:   128000, Batch Loss:     1.765674, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-27 11:37:10,437 - INFO - joeynmt.training - Epoch  19, Step:   128100, Batch Loss:     1.699242, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-07-27 11:37:23,818 - INFO - joeynmt.training - Epoch  19, Step:   128200, Batch Loss:     1.758087, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-07-27 11:37:37,366 - INFO - joeynmt.training - Epoch  19, Step:   128300, Batch Loss:     1.621821, Tokens per Sec:    15566, Lr: 0.000300\n",
      "2021-07-27 11:37:50,963 - INFO - joeynmt.training - Epoch  19, Step:   128400, Batch Loss:     1.817769, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-07-27 11:38:04,310 - INFO - joeynmt.training - Epoch  19, Step:   128500, Batch Loss:     1.789911, Tokens per Sec:    15891, Lr: 0.000300\n",
      "2021-07-27 11:38:17,641 - INFO - joeynmt.training - Epoch  19, Step:   128600, Batch Loss:     1.645052, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-07-27 11:38:31,103 - INFO - joeynmt.training - Epoch  19, Step:   128700, Batch Loss:     1.803517, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-07-27 11:38:44,648 - INFO - joeynmt.training - Epoch  19, Step:   128800, Batch Loss:     1.906985, Tokens per Sec:    15704, Lr: 0.000300\n",
      "2021-07-27 11:38:57,909 - INFO - joeynmt.training - Epoch  19, Step:   128900, Batch Loss:     1.961064, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-07-27 11:39:11,253 - INFO - joeynmt.training - Epoch  19, Step:   129000, Batch Loss:     1.739488, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-07-27 11:39:30,392 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 11:39:30,392 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 11:39:30,392 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 11:39:31,365 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 11:39:31,366 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 11:39:31,366 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 11:39:31,366 - INFO - joeynmt.training - \tHypothesis: [ I ] hated what you do good and suffer when you endure , that is what is acceptable to God . ”\n",
      "2021-07-27 11:39:31,366 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 11:39:31,367 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 11:39:31,368 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:39:31,368 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
      "2021-07-27 11:39:31,368 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 11:39:31,368 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 11:39:31,369 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 11:39:31,369 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
      "2021-07-27 11:39:31,369 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 11:39:31,369 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 11:39:31,370 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 11:39:31,370 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 11:39:31,370 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   129000: bleu:  27.20, loss: 43921.2695, ppl:   5.2297, duration: 20.1167s\n",
      "2021-07-27 11:39:45,171 - INFO - joeynmt.training - Epoch  19, Step:   129100, Batch Loss:     1.922710, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-07-27 11:39:58,601 - INFO - joeynmt.training - Epoch  19, Step:   129200, Batch Loss:     1.725864, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-07-27 11:40:12,261 - INFO - joeynmt.training - Epoch  19, Step:   129300, Batch Loss:     1.712913, Tokens per Sec:    16050, Lr: 0.000300\n",
      "2021-07-27 11:40:25,819 - INFO - joeynmt.training - Epoch  19, Step:   129400, Batch Loss:     1.806891, Tokens per Sec:    15696, Lr: 0.000300\n",
      "2021-07-27 11:40:39,213 - INFO - joeynmt.training - Epoch  19, Step:   129500, Batch Loss:     1.419902, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-07-27 11:40:52,528 - INFO - joeynmt.training - Epoch  19, Step:   129600, Batch Loss:     1.826049, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-07-27 11:41:06,021 - INFO - joeynmt.training - Epoch  19, Step:   129700, Batch Loss:     1.836981, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-07-27 11:41:19,467 - INFO - joeynmt.training - Epoch  19, Step:   129800, Batch Loss:     1.974476, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-07-27 11:41:32,936 - INFO - joeynmt.training - Epoch  19, Step:   129900, Batch Loss:     1.853860, Tokens per Sec:    16035, Lr: 0.000300\n",
      "2021-07-27 11:41:46,150 - INFO - joeynmt.training - Epoch  19, Step:   130000, Batch Loss:     1.856862, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-07-27 11:41:59,626 - INFO - joeynmt.training - Epoch  19, Step:   130100, Batch Loss:     1.807332, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-07-27 11:42:12,867 - INFO - joeynmt.training - Epoch  19, Step:   130200, Batch Loss:     1.666844, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-07-27 11:42:26,395 - INFO - joeynmt.training - Epoch  19, Step:   130300, Batch Loss:     1.968922, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-07-27 11:42:39,969 - INFO - joeynmt.training - Epoch  19, Step:   130400, Batch Loss:     1.946960, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-07-27 11:42:53,384 - INFO - joeynmt.training - Epoch  19, Step:   130500, Batch Loss:     1.620018, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-07-27 11:43:06,708 - INFO - joeynmt.training - Epoch  19, Step:   130600, Batch Loss:     1.846980, Tokens per Sec:    16072, Lr: 0.000300\n",
      "2021-07-27 11:43:15,257 - INFO - joeynmt.training - Epoch  19: total training loss 4994.08\n",
      "2021-07-27 11:43:15,257 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-27 11:43:20,437 - INFO - joeynmt.training - Epoch  20, Step:   130700, Batch Loss:     1.930443, Tokens per Sec:    15104, Lr: 0.000300\n",
      "2021-07-27 11:43:34,040 - INFO - joeynmt.training - Epoch  20, Step:   130800, Batch Loss:     1.971739, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-07-27 11:43:47,792 - INFO - joeynmt.training - Epoch  20, Step:   130900, Batch Loss:     1.706580, Tokens per Sec:    16345, Lr: 0.000300\n",
      "2021-07-27 11:44:00,965 - INFO - joeynmt.training - Epoch  20, Step:   131000, Batch Loss:     1.634630, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-07-27 11:44:14,279 - INFO - joeynmt.training - Epoch  20, Step:   131100, Batch Loss:     1.803818, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-07-27 11:44:27,678 - INFO - joeynmt.training - Epoch  20, Step:   131200, Batch Loss:     0.874660, Tokens per Sec:    15628, Lr: 0.000300\n",
      "2021-07-27 11:44:41,187 - INFO - joeynmt.training - Epoch  20, Step:   131300, Batch Loss:     1.824163, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-07-27 11:44:54,582 - INFO - joeynmt.training - Epoch  20, Step:   131400, Batch Loss:     1.715451, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-07-27 11:45:07,798 - INFO - joeynmt.training - Epoch  20, Step:   131500, Batch Loss:     1.796225, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-07-27 11:45:21,112 - INFO - joeynmt.training - Epoch  20, Step:   131600, Batch Loss:     1.817445, Tokens per Sec:    16207, Lr: 0.000300\n",
      "2021-07-27 11:45:34,402 - INFO - joeynmt.training - Epoch  20, Step:   131700, Batch Loss:     1.829518, Tokens per Sec:    15501, Lr: 0.000300\n",
      "2021-07-27 11:45:48,174 - INFO - joeynmt.training - Epoch  20, Step:   131800, Batch Loss:     1.788096, Tokens per Sec:    16441, Lr: 0.000300\n",
      "2021-07-27 11:46:01,681 - INFO - joeynmt.training - Epoch  20, Step:   131900, Batch Loss:     1.614223, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-07-27 11:46:15,148 - INFO - joeynmt.training - Epoch  20, Step:   132000, Batch Loss:     1.669328, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-07-27 11:46:33,289 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 11:46:33,290 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 11:46:33,290 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 11:46:33,510 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 11:46:33,511 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 11:46:34,303 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 11:46:34,304 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 11:46:34,304 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 11:46:34,304 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is pleased to God . ”\n",
      "2021-07-27 11:46:34,304 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 11:46:34,305 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 11:46:34,305 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:46:34,305 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
      "2021-07-27 11:46:34,305 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 11:46:34,306 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 11:46:34,306 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 11:46:34,306 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
      "2021-07-27 11:46:34,306 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 11:46:34,307 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 11:46:34,307 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 11:46:34,307 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 11:46:34,307 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   132000: bleu:  27.53, loss: 43758.3359, ppl:   5.1977, duration: 19.1591s\n",
      "2021-07-27 11:46:47,881 - INFO - joeynmt.training - Epoch  20, Step:   132100, Batch Loss:     2.004049, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-07-27 11:47:01,074 - INFO - joeynmt.training - Epoch  20, Step:   132200, Batch Loss:     1.980666, Tokens per Sec:    15962, Lr: 0.000300\n",
      "2021-07-27 11:47:14,601 - INFO - joeynmt.training - Epoch  20, Step:   132300, Batch Loss:     1.992474, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-07-27 11:47:28,056 - INFO - joeynmt.training - Epoch  20, Step:   132400, Batch Loss:     1.783379, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-07-27 11:47:41,430 - INFO - joeynmt.training - Epoch  20, Step:   132500, Batch Loss:     1.959184, Tokens per Sec:    15678, Lr: 0.000300\n",
      "2021-07-27 11:47:54,832 - INFO - joeynmt.training - Epoch  20, Step:   132600, Batch Loss:     1.817080, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-07-27 11:48:08,010 - INFO - joeynmt.training - Epoch  20, Step:   132700, Batch Loss:     1.864940, Tokens per Sec:    15907, Lr: 0.000300\n",
      "2021-07-27 11:48:21,458 - INFO - joeynmt.training - Epoch  20, Step:   132800, Batch Loss:     1.999329, Tokens per Sec:    15997, Lr: 0.000300\n",
      "2021-07-27 11:48:35,043 - INFO - joeynmt.training - Epoch  20, Step:   132900, Batch Loss:     1.998983, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-07-27 11:48:48,292 - INFO - joeynmt.training - Epoch  20, Step:   133000, Batch Loss:     1.887047, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-07-27 11:49:01,675 - INFO - joeynmt.training - Epoch  20, Step:   133100, Batch Loss:     0.990088, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-07-27 11:49:15,207 - INFO - joeynmt.training - Epoch  20, Step:   133200, Batch Loss:     1.872343, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-07-27 11:49:28,551 - INFO - joeynmt.training - Epoch  20, Step:   133300, Batch Loss:     1.923820, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-07-27 11:49:42,147 - INFO - joeynmt.training - Epoch  20, Step:   133400, Batch Loss:     1.967582, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-07-27 11:49:48,619 - INFO - joeynmt.training - Epoch  20: total training loss 5002.91\n",
      "2021-07-27 11:49:48,620 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-27 11:49:55,689 - INFO - joeynmt.training - Epoch  21, Step:   133500, Batch Loss:     1.964132, Tokens per Sec:    14806, Lr: 0.000300\n",
      "2021-07-27 11:50:08,967 - INFO - joeynmt.training - Epoch  21, Step:   133600, Batch Loss:     1.690719, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-07-27 11:50:22,332 - INFO - joeynmt.training - Epoch  21, Step:   133700, Batch Loss:     1.508276, Tokens per Sec:    16164, Lr: 0.000300\n",
      "2021-07-27 11:50:35,951 - INFO - joeynmt.training - Epoch  21, Step:   133800, Batch Loss:     1.905835, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-07-27 11:50:49,465 - INFO - joeynmt.training - Epoch  21, Step:   133900, Batch Loss:     1.655691, Tokens per Sec:    16080, Lr: 0.000300\n",
      "2021-07-27 11:51:02,818 - INFO - joeynmt.training - Epoch  21, Step:   134000, Batch Loss:     1.819109, Tokens per Sec:    15649, Lr: 0.000300\n",
      "2021-07-27 11:51:16,441 - INFO - joeynmt.training - Epoch  21, Step:   134100, Batch Loss:     1.843703, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-07-27 11:51:29,809 - INFO - joeynmt.training - Epoch  21, Step:   134200, Batch Loss:     1.834723, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-07-27 11:51:43,395 - INFO - joeynmt.training - Epoch  21, Step:   134300, Batch Loss:     1.761776, Tokens per Sec:    16368, Lr: 0.000300\n",
      "2021-07-27 11:51:56,644 - INFO - joeynmt.training - Epoch  21, Step:   134400, Batch Loss:     1.952986, Tokens per Sec:    16101, Lr: 0.000300\n",
      "2021-07-27 11:52:10,070 - INFO - joeynmt.training - Epoch  21, Step:   134500, Batch Loss:     1.873049, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-07-27 11:52:23,701 - INFO - joeynmt.training - Epoch  21, Step:   134600, Batch Loss:     1.769983, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-07-27 11:52:37,059 - INFO - joeynmt.training - Epoch  21, Step:   134700, Batch Loss:     1.928235, Tokens per Sec:    15643, Lr: 0.000300\n",
      "2021-07-27 11:52:50,523 - INFO - joeynmt.training - Epoch  21, Step:   134800, Batch Loss:     1.856740, Tokens per Sec:    16627, Lr: 0.000300\n",
      "2021-07-27 11:53:03,732 - INFO - joeynmt.training - Epoch  21, Step:   134900, Batch Loss:     2.100028, Tokens per Sec:    15905, Lr: 0.000300\n",
      "2021-07-27 11:53:17,209 - INFO - joeynmt.training - Epoch  21, Step:   135000, Batch Loss:     1.921108, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-07-27 11:53:34,809 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 11:53:34,809 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 11:53:34,809 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 11:53:35,059 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 11:53:35,060 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 11:53:35,804 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 11:53:35,805 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 11:53:35,805 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 11:53:35,805 - INFO - joeynmt.training - \tHypothesis: I hated you when you do good and suffer when you endure , that is acceptable to God . ”\n",
      "2021-07-27 11:53:35,805 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 11:53:35,806 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 11:53:35,806 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 11:53:35,806 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my Son , whom I appreciate . ”\n",
      "2021-07-27 11:53:35,806 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 11:53:35,807 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 11:53:35,807 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 11:53:35,808 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 11:53:35,808 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 11:53:35,808 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 11:53:35,808 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 11:53:35,808 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 11:53:35,809 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step   135000: bleu:  27.10, loss: 43678.5195, ppl:   5.1821, duration: 18.5992s\n",
      "2021-07-27 11:53:49,485 - INFO - joeynmt.training - Epoch  21, Step:   135100, Batch Loss:     1.451900, Tokens per Sec:    16132, Lr: 0.000300\n",
      "2021-07-27 11:54:02,666 - INFO - joeynmt.training - Epoch  21, Step:   135200, Batch Loss:     1.769081, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-07-27 11:54:16,153 - INFO - joeynmt.training - Epoch  21, Step:   135300, Batch Loss:     1.943387, Tokens per Sec:    16384, Lr: 0.000300\n",
      "2021-07-27 11:54:29,422 - INFO - joeynmt.training - Epoch  21, Step:   135400, Batch Loss:     1.637788, Tokens per Sec:    15560, Lr: 0.000300\n",
      "2021-07-27 11:54:42,983 - INFO - joeynmt.training - Epoch  21, Step:   135500, Batch Loss:     1.840009, Tokens per Sec:    15882, Lr: 0.000300\n",
      "2021-07-27 11:54:56,383 - INFO - joeynmt.training - Epoch  21, Step:   135600, Batch Loss:     1.875221, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-07-27 11:55:09,828 - INFO - joeynmt.training - Epoch  21, Step:   135700, Batch Loss:     1.000233, Tokens per Sec:    16211, Lr: 0.000300\n",
      "2021-07-27 11:55:22,917 - INFO - joeynmt.training - Epoch  21, Step:   135800, Batch Loss:     1.749512, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-07-27 11:55:36,371 - INFO - joeynmt.training - Epoch  21, Step:   135900, Batch Loss:     1.794026, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-07-27 11:55:49,813 - INFO - joeynmt.training - Epoch  21, Step:   136000, Batch Loss:     1.674778, Tokens per Sec:    15683, Lr: 0.000300\n",
      "2021-07-27 11:56:03,165 - INFO - joeynmt.training - Epoch  21, Step:   136100, Batch Loss:     1.879225, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-07-27 11:56:16,651 - INFO - joeynmt.training - Epoch  21, Step:   136200, Batch Loss:     1.798862, Tokens per Sec:    16195, Lr: 0.000300\n",
      "2021-07-27 11:56:21,147 - INFO - joeynmt.training - Epoch  21: total training loss 4984.60\n",
      "2021-07-27 11:56:21,147 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-27 11:56:30,317 - INFO - joeynmt.training - Epoch  22, Step:   136300, Batch Loss:     1.780413, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-07-27 11:56:43,652 - INFO - joeynmt.training - Epoch  22, Step:   136400, Batch Loss:     1.657972, Tokens per Sec:    15482, Lr: 0.000300\n",
      "2021-07-27 11:56:57,195 - INFO - joeynmt.training - Epoch  22, Step:   136500, Batch Loss:     1.853558, Tokens per Sec:    15882, Lr: 0.000300\n",
      "2021-07-27 11:57:10,759 - INFO - joeynmt.training - Epoch  22, Step:   136600, Batch Loss:     1.804289, Tokens per Sec:    16231, Lr: 0.000300\n",
      "2021-07-27 11:57:24,385 - INFO - joeynmt.training - Epoch  22, Step:   136700, Batch Loss:     1.814852, Tokens per Sec:    16076, Lr: 0.000300\n",
      "2021-07-27 11:57:37,811 - INFO - joeynmt.training - Epoch  22, Step:   136800, Batch Loss:     1.858569, Tokens per Sec:    16030, Lr: 0.000300\n",
      "2021-07-27 11:57:51,399 - INFO - joeynmt.training - Epoch  22, Step:   136900, Batch Loss:     1.948883, Tokens per Sec:    16346, Lr: 0.000300\n",
      "2021-07-27 11:58:04,696 - INFO - joeynmt.training - Epoch  22, Step:   137000, Batch Loss:     1.803620, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-07-27 11:58:18,187 - INFO - joeynmt.training - Epoch  22, Step:   137100, Batch Loss:     1.676627, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-07-27 11:58:31,715 - INFO - joeynmt.training - Epoch  22, Step:   137200, Batch Loss:     1.857639, Tokens per Sec:    15796, Lr: 0.000300\n",
      "2021-07-27 11:58:45,061 - INFO - joeynmt.training - Epoch  22, Step:   137300, Batch Loss:     1.776720, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-27 11:58:58,455 - INFO - joeynmt.training - Epoch  22, Step:   137400, Batch Loss:     1.879087, Tokens per Sec:    16350, Lr: 0.000300\n",
      "2021-07-27 11:59:11,753 - INFO - joeynmt.training - Epoch  22, Step:   137500, Batch Loss:     1.720657, Tokens per Sec:    15809, Lr: 0.000300\n",
      "2021-07-27 11:59:25,123 - INFO - joeynmt.training - Epoch  22, Step:   137600, Batch Loss:     1.830540, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-27 11:59:38,698 - INFO - joeynmt.training - Epoch  22, Step:   137700, Batch Loss:     2.000000, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-07-27 11:59:52,171 - INFO - joeynmt.training - Epoch  22, Step:   137800, Batch Loss:     1.624452, Tokens per Sec:    16259, Lr: 0.000300\n",
      "2021-07-27 12:00:05,402 - INFO - joeynmt.training - Epoch  22, Step:   137900, Batch Loss:     1.908087, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-07-27 12:00:18,798 - INFO - joeynmt.training - Epoch  22, Step:   138000, Batch Loss:     1.782227, Tokens per Sec:    16383, Lr: 0.000300\n",
      "2021-07-27 12:00:36,176 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:00:36,177 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:00:36,177 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:00:36,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 12:00:36,427 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 12:00:37,155 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 12:00:37,156 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 12:00:37,156 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 12:00:37,156 - INFO - joeynmt.training - \tHypothesis: [ I ] hated when you do good and suffer when you endure , that is what is pleased to God . ”\n",
      "2021-07-27 12:00:37,156 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 12:00:37,157 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 12:00:37,157 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 12:00:37,157 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son . ”\n",
      "2021-07-27 12:00:37,157 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 12:00:37,157 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 12:00:37,158 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 12:00:37,158 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 12:00:37,158 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 12:00:37,158 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 12:00:37,159 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 12:00:37,159 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
      "2021-07-27 12:00:37,159 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step   138000: bleu:  27.22, loss: 43595.9219, ppl:   5.1660, duration: 18.3608s\n",
      "2021-07-27 12:00:50,548 - INFO - joeynmt.training - Epoch  22, Step:   138100, Batch Loss:     1.876870, Tokens per Sec:    15970, Lr: 0.000300\n",
      "2021-07-27 12:01:03,805 - INFO - joeynmt.training - Epoch  22, Step:   138200, Batch Loss:     1.849098, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-07-27 12:01:17,175 - INFO - joeynmt.training - Epoch  22, Step:   138300, Batch Loss:     1.401002, Tokens per Sec:    16333, Lr: 0.000300\n",
      "2021-07-27 12:01:30,542 - INFO - joeynmt.training - Epoch  22, Step:   138400, Batch Loss:     1.581442, Tokens per Sec:    15921, Lr: 0.000300\n",
      "2021-07-27 12:01:44,072 - INFO - joeynmt.training - Epoch  22, Step:   138500, Batch Loss:     1.627941, Tokens per Sec:    15782, Lr: 0.000300\n",
      "2021-07-27 12:01:57,571 - INFO - joeynmt.training - Epoch  22, Step:   138600, Batch Loss:     1.948375, Tokens per Sec:    16143, Lr: 0.000300\n",
      "2021-07-27 12:02:11,013 - INFO - joeynmt.training - Epoch  22, Step:   138700, Batch Loss:     1.968359, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-07-27 12:02:24,593 - INFO - joeynmt.training - Epoch  22, Step:   138800, Batch Loss:     1.891622, Tokens per Sec:    15819, Lr: 0.000300\n",
      "2021-07-27 12:02:38,067 - INFO - joeynmt.training - Epoch  22, Step:   138900, Batch Loss:     1.880788, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-07-27 12:02:51,347 - INFO - joeynmt.training - Epoch  22, Step:   139000, Batch Loss:     1.767694, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-07-27 12:02:52,357 - INFO - joeynmt.training - Epoch  22: total training loss 4955.68\n",
      "2021-07-27 12:02:52,357 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-27 12:03:04,752 - INFO - joeynmt.training - Epoch  23, Step:   139100, Batch Loss:     1.547022, Tokens per Sec:    15407, Lr: 0.000300\n",
      "2021-07-27 12:03:18,352 - INFO - joeynmt.training - Epoch  23, Step:   139200, Batch Loss:     1.749360, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-07-27 12:03:31,909 - INFO - joeynmt.training - Epoch  23, Step:   139300, Batch Loss:     1.799941, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-07-27 12:03:45,368 - INFO - joeynmt.training - Epoch  23, Step:   139400, Batch Loss:     1.897185, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-07-27 12:03:58,785 - INFO - joeynmt.training - Epoch  23, Step:   139500, Batch Loss:     1.343952, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-07-27 12:04:11,978 - INFO - joeynmt.training - Epoch  23, Step:   139600, Batch Loss:     1.765308, Tokens per Sec:    15872, Lr: 0.000300\n",
      "2021-07-27 12:04:25,322 - INFO - joeynmt.training - Epoch  23, Step:   139700, Batch Loss:     1.751337, Tokens per Sec:    15549, Lr: 0.000300\n",
      "2021-07-27 12:04:38,852 - INFO - joeynmt.training - Epoch  23, Step:   139800, Batch Loss:     1.751729, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-07-27 12:04:52,266 - INFO - joeynmt.training - Epoch  23, Step:   139900, Batch Loss:     1.927433, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-07-27 12:05:05,746 - INFO - joeynmt.training - Epoch  23, Step:   140000, Batch Loss:     1.563192, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-07-27 12:05:19,146 - INFO - joeynmt.training - Epoch  23, Step:   140100, Batch Loss:     1.919356, Tokens per Sec:    16295, Lr: 0.000300\n",
      "2021-07-27 12:05:32,600 - INFO - joeynmt.training - Epoch  23, Step:   140200, Batch Loss:     1.413533, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-07-27 12:05:46,131 - INFO - joeynmt.training - Epoch  23, Step:   140300, Batch Loss:     1.509911, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-27 12:05:59,587 - INFO - joeynmt.training - Epoch  23, Step:   140400, Batch Loss:     1.775887, Tokens per Sec:    16278, Lr: 0.000300\n",
      "2021-07-27 12:06:12,913 - INFO - joeynmt.training - Epoch  23, Step:   140500, Batch Loss:     1.777419, Tokens per Sec:    16213, Lr: 0.000300\n",
      "2021-07-27 12:06:26,224 - INFO - joeynmt.training - Epoch  23, Step:   140600, Batch Loss:     1.985078, Tokens per Sec:    16237, Lr: 0.000300\n",
      "2021-07-27 12:06:39,701 - INFO - joeynmt.training - Epoch  23, Step:   140700, Batch Loss:     2.003038, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-07-27 12:06:53,175 - INFO - joeynmt.training - Epoch  23, Step:   140800, Batch Loss:     1.807744, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-07-27 12:07:06,464 - INFO - joeynmt.training - Epoch  23, Step:   140900, Batch Loss:     1.932821, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-07-27 12:07:19,820 - INFO - joeynmt.training - Epoch  23, Step:   141000, Batch Loss:     1.703365, Tokens per Sec:    16249, Lr: 0.000300\n",
      "2021-07-27 12:07:38,091 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:07:38,091 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:07:38,091 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:07:38,344 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 12:07:38,344 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 12:07:39,447 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 12:07:39,448 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 12:07:39,449 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 12:07:39,449 - INFO - joeynmt.training - \tHypothesis: [ I ] hated when you do good and suffer when you endure , this is acceptable to God . ”\n",
      "2021-07-27 12:07:39,449 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 12:07:39,449 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 12:07:39,450 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 12:07:39,450 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
      "2021-07-27 12:07:39,450 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 12:07:39,450 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 12:07:39,450 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 12:07:39,451 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
      "2021-07-27 12:07:39,451 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 12:07:39,451 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 12:07:39,451 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 12:07:39,451 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 12:07:39,452 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step   141000: bleu:  27.14, loss: 43526.1484, ppl:   5.1524, duration: 19.6307s\n",
      "2021-07-27 12:07:53,112 - INFO - joeynmt.training - Epoch  23, Step:   141100, Batch Loss:     1.767141, Tokens per Sec:    15547, Lr: 0.000300\n",
      "2021-07-27 12:08:06,566 - INFO - joeynmt.training - Epoch  23, Step:   141200, Batch Loss:     1.368838, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-07-27 12:08:20,182 - INFO - joeynmt.training - Epoch  23, Step:   141300, Batch Loss:     1.784130, Tokens per Sec:    16185, Lr: 0.000300\n",
      "2021-07-27 12:08:33,675 - INFO - joeynmt.training - Epoch  23, Step:   141400, Batch Loss:     1.909007, Tokens per Sec:    16318, Lr: 0.000300\n",
      "2021-07-27 12:08:47,199 - INFO - joeynmt.training - Epoch  23, Step:   141500, Batch Loss:     1.841994, Tokens per Sec:    16326, Lr: 0.000300\n",
      "2021-07-27 12:09:00,588 - INFO - joeynmt.training - Epoch  23, Step:   141600, Batch Loss:     1.882822, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-07-27 12:09:14,057 - INFO - joeynmt.training - Epoch  23, Step:   141700, Batch Loss:     1.783882, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-07-27 12:09:25,655 - INFO - joeynmt.training - Epoch  23: total training loss 4959.49\n",
      "2021-07-27 12:09:25,655 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-27 12:09:27,803 - INFO - joeynmt.training - Epoch  24, Step:   141800, Batch Loss:     1.537800, Tokens per Sec:    14359, Lr: 0.000300\n",
      "2021-07-27 12:09:41,309 - INFO - joeynmt.training - Epoch  24, Step:   141900, Batch Loss:     1.853960, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-07-27 12:09:54,617 - INFO - joeynmt.training - Epoch  24, Step:   142000, Batch Loss:     1.768731, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-07-27 12:10:07,892 - INFO - joeynmt.training - Epoch  24, Step:   142100, Batch Loss:     1.846416, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-07-27 12:10:21,254 - INFO - joeynmt.training - Epoch  24, Step:   142200, Batch Loss:     1.886706, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-07-27 12:10:34,701 - INFO - joeynmt.training - Epoch  24, Step:   142300, Batch Loss:     1.961564, Tokens per Sec:    16100, Lr: 0.000300\n",
      "2021-07-27 12:10:48,302 - INFO - joeynmt.training - Epoch  24, Step:   142400, Batch Loss:     1.885654, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-07-27 12:11:01,812 - INFO - joeynmt.training - Epoch  24, Step:   142500, Batch Loss:     1.847580, Tokens per Sec:    16328, Lr: 0.000300\n",
      "2021-07-27 12:11:15,253 - INFO - joeynmt.training - Epoch  24, Step:   142600, Batch Loss:     1.838201, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-27 12:11:28,772 - INFO - joeynmt.training - Epoch  24, Step:   142700, Batch Loss:     1.931085, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-07-27 12:11:42,150 - INFO - joeynmt.training - Epoch  24, Step:   142800, Batch Loss:     1.969501, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-07-27 12:11:55,745 - INFO - joeynmt.training - Epoch  24, Step:   142900, Batch Loss:     1.870609, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-07-27 12:12:09,152 - INFO - joeynmt.training - Epoch  24, Step:   143000, Batch Loss:     1.802725, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-07-27 12:12:22,480 - INFO - joeynmt.training - Epoch  24, Step:   143100, Batch Loss:     1.828348, Tokens per Sec:    16184, Lr: 0.000300\n",
      "2021-07-27 12:12:35,982 - INFO - joeynmt.training - Epoch  24, Step:   143200, Batch Loss:     1.743974, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-07-27 12:12:49,400 - INFO - joeynmt.training - Epoch  24, Step:   143300, Batch Loss:     1.742545, Tokens per Sec:    15780, Lr: 0.000300\n",
      "2021-07-27 12:13:02,835 - INFO - joeynmt.training - Epoch  24, Step:   143400, Batch Loss:     1.809926, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-07-27 12:13:16,430 - INFO - joeynmt.training - Epoch  24, Step:   143500, Batch Loss:     1.795376, Tokens per Sec:    15853, Lr: 0.000300\n",
      "2021-07-27 12:13:29,938 - INFO - joeynmt.training - Epoch  24, Step:   143600, Batch Loss:     1.639065, Tokens per Sec:    15612, Lr: 0.000300\n",
      "2021-07-27 12:13:43,467 - INFO - joeynmt.training - Epoch  24, Step:   143700, Batch Loss:     1.925213, Tokens per Sec:    16251, Lr: 0.000300\n",
      "2021-07-27 12:13:56,728 - INFO - joeynmt.training - Epoch  24, Step:   143800, Batch Loss:     1.741466, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-07-27 12:14:10,054 - INFO - joeynmt.training - Epoch  24, Step:   143900, Batch Loss:     1.852107, Tokens per Sec:    16196, Lr: 0.000300\n",
      "2021-07-27 12:14:23,490 - INFO - joeynmt.training - Epoch  24, Step:   144000, Batch Loss:     1.714736, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-27 12:14:40,666 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:14:40,667 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:14:40,667 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:14:40,893 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 12:14:40,894 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 12:14:41,595 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 12:14:41,596 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 12:14:41,596 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 12:14:41,596 - INFO - joeynmt.training - \tHypothesis: I hated what you do good and suffer when you endure , this is acceptable to God . ”\n",
      "2021-07-27 12:14:41,596 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 12:14:41,597 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 12:14:41,597 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 12:14:41,597 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son , whom I am grateful . ”\n",
      "2021-07-27 12:14:41,597 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 12:14:41,598 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 12:14:41,598 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 12:14:41,598 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 12:14:41,598 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 12:14:41,599 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 12:14:41,599 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 12:14:41,599 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
      "2021-07-27 12:14:41,599 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step   144000: bleu:  27.16, loss: 43408.2070, ppl:   5.1296, duration: 18.1087s\n",
      "2021-07-27 12:14:55,220 - INFO - joeynmt.training - Epoch  24, Step:   144100, Batch Loss:     1.608286, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-07-27 12:15:08,615 - INFO - joeynmt.training - Epoch  24, Step:   144200, Batch Loss:     1.557621, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-07-27 12:15:22,083 - INFO - joeynmt.training - Epoch  24, Step:   144300, Batch Loss:     1.596980, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-07-27 12:15:35,641 - INFO - joeynmt.training - Epoch  24, Step:   144400, Batch Loss:     1.729635, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-07-27 12:15:49,043 - INFO - joeynmt.training - Epoch  24, Step:   144500, Batch Loss:     1.848219, Tokens per Sec:    16053, Lr: 0.000300\n",
      "2021-07-27 12:15:57,027 - INFO - joeynmt.training - Epoch  24: total training loss 4935.91\n",
      "2021-07-27 12:15:57,028 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-07-27 12:16:02,374 - INFO - joeynmt.training - Epoch  25, Step:   144600, Batch Loss:     1.604311, Tokens per Sec:    14739, Lr: 0.000300\n",
      "2021-07-27 12:16:15,645 - INFO - joeynmt.training - Epoch  25, Step:   144700, Batch Loss:     1.882198, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-07-27 12:16:29,117 - INFO - joeynmt.training - Epoch  25, Step:   144800, Batch Loss:     1.827471, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-07-27 12:16:42,738 - INFO - joeynmt.training - Epoch  25, Step:   144900, Batch Loss:     1.744475, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-07-27 12:16:55,962 - INFO - joeynmt.training - Epoch  25, Step:   145000, Batch Loss:     1.699755, Tokens per Sec:    15566, Lr: 0.000300\n",
      "2021-07-27 12:17:09,333 - INFO - joeynmt.training - Epoch  25, Step:   145100, Batch Loss:     1.779643, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-07-27 12:17:22,854 - INFO - joeynmt.training - Epoch  25, Step:   145200, Batch Loss:     1.825126, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-07-27 12:17:36,223 - INFO - joeynmt.training - Epoch  25, Step:   145300, Batch Loss:     1.800679, Tokens per Sec:    15620, Lr: 0.000300\n",
      "2021-07-27 12:17:49,779 - INFO - joeynmt.training - Epoch  25, Step:   145400, Batch Loss:     1.512420, Tokens per Sec:    15891, Lr: 0.000300\n",
      "2021-07-27 12:18:03,271 - INFO - joeynmt.training - Epoch  25, Step:   145500, Batch Loss:     1.649352, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-07-27 12:18:16,620 - INFO - joeynmt.training - Epoch  25, Step:   145600, Batch Loss:     1.630510, Tokens per Sec:    16234, Lr: 0.000300\n",
      "2021-07-27 12:18:30,026 - INFO - joeynmt.training - Epoch  25, Step:   145700, Batch Loss:     1.726001, Tokens per Sec:    16095, Lr: 0.000300\n",
      "2021-07-27 12:18:43,517 - INFO - joeynmt.training - Epoch  25, Step:   145800, Batch Loss:     1.904034, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-07-27 12:18:57,113 - INFO - joeynmt.training - Epoch  25, Step:   145900, Batch Loss:     1.867208, Tokens per Sec:    15958, Lr: 0.000300\n",
      "2021-07-27 12:19:10,415 - INFO - joeynmt.training - Epoch  25, Step:   146000, Batch Loss:     1.714880, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-07-27 12:19:23,983 - INFO - joeynmt.training - Epoch  25, Step:   146100, Batch Loss:     1.854789, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-07-27 12:19:37,398 - INFO - joeynmt.training - Epoch  25, Step:   146200, Batch Loss:     1.838765, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-07-27 12:19:50,671 - INFO - joeynmt.training - Epoch  25, Step:   146300, Batch Loss:     1.673118, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-07-27 12:20:04,105 - INFO - joeynmt.training - Epoch  25, Step:   146400, Batch Loss:     1.928475, Tokens per Sec:    16154, Lr: 0.000300\n",
      "2021-07-27 12:20:17,798 - INFO - joeynmt.training - Epoch  25, Step:   146500, Batch Loss:     1.841444, Tokens per Sec:    16235, Lr: 0.000300\n",
      "2021-07-27 12:20:31,127 - INFO - joeynmt.training - Epoch  25, Step:   146600, Batch Loss:     1.808690, Tokens per Sec:    15737, Lr: 0.000300\n",
      "2021-07-27 12:20:44,529 - INFO - joeynmt.training - Epoch  25, Step:   146700, Batch Loss:     1.743852, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-07-27 12:20:57,838 - INFO - joeynmt.training - Epoch  25, Step:   146800, Batch Loss:     1.473673, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-07-27 12:21:11,150 - INFO - joeynmt.training - Epoch  25, Step:   146900, Batch Loss:     1.802600, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-07-27 12:21:24,564 - INFO - joeynmt.training - Epoch  25, Step:   147000, Batch Loss:     1.865587, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-07-27 12:21:42,298 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:21:42,299 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:21:42,299 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:21:42,524 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 12:21:42,524 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 12:21:43,224 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 12:21:43,225 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 12:21:43,225 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 12:21:43,225 - INFO - joeynmt.training - \tHypothesis: I have hated what you do good and suffer when you endure , that is what is pleased to God . ”\n",
      "2021-07-27 12:21:43,225 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 12:21:43,226 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 12:21:43,226 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 12:21:43,226 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven , saying : “ This is my Son , whom I have approved . ”\n",
      "2021-07-27 12:21:43,226 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 12:21:43,227 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 12:21:43,227 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 12:21:43,227 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 12:21:43,227 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 12:21:43,228 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 12:21:43,228 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 12:21:43,228 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 12:21:43,228 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step   147000: bleu:  27.68, loss: 43324.7773, ppl:   5.1135, duration: 18.6639s\n",
      "2021-07-27 12:21:56,663 - INFO - joeynmt.training - Epoch  25, Step:   147100, Batch Loss:     1.945941, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-27 12:22:10,017 - INFO - joeynmt.training - Epoch  25, Step:   147200, Batch Loss:     1.654120, Tokens per Sec:    16139, Lr: 0.000300\n",
      "2021-07-27 12:22:23,307 - INFO - joeynmt.training - Epoch  25, Step:   147300, Batch Loss:     1.722864, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-07-27 12:22:29,135 - INFO - joeynmt.training - Epoch  25: total training loss 4942.60\n",
      "2021-07-27 12:22:29,135 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-07-27 12:22:37,154 - INFO - joeynmt.training - Epoch  26, Step:   147400, Batch Loss:     1.754255, Tokens per Sec:    15338, Lr: 0.000300\n",
      "2021-07-27 12:22:50,667 - INFO - joeynmt.training - Epoch  26, Step:   147500, Batch Loss:     1.932821, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-27 12:23:04,134 - INFO - joeynmt.training - Epoch  26, Step:   147600, Batch Loss:     1.815312, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-07-27 12:23:17,418 - INFO - joeynmt.training - Epoch  26, Step:   147700, Batch Loss:     1.728847, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-07-27 12:23:30,983 - INFO - joeynmt.training - Epoch  26, Step:   147800, Batch Loss:     1.827187, Tokens per Sec:    16337, Lr: 0.000300\n",
      "2021-07-27 12:23:44,461 - INFO - joeynmt.training - Epoch  26, Step:   147900, Batch Loss:     1.876447, Tokens per Sec:    16077, Lr: 0.000300\n",
      "2021-07-27 12:23:57,872 - INFO - joeynmt.training - Epoch  26, Step:   148000, Batch Loss:     1.340440, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-27 12:24:11,426 - INFO - joeynmt.training - Epoch  26, Step:   148100, Batch Loss:     1.730061, Tokens per Sec:    16184, Lr: 0.000300\n",
      "2021-07-27 12:24:25,019 - INFO - joeynmt.training - Epoch  26, Step:   148200, Batch Loss:     1.792717, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-07-27 12:24:38,469 - INFO - joeynmt.training - Epoch  26, Step:   148300, Batch Loss:     1.924127, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-07-27 12:24:51,759 - INFO - joeynmt.training - Epoch  26, Step:   148400, Batch Loss:     1.824385, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-07-27 12:25:04,975 - INFO - joeynmt.training - Epoch  26, Step:   148500, Batch Loss:     1.957937, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-07-27 12:25:18,532 - INFO - joeynmt.training - Epoch  26, Step:   148600, Batch Loss:     1.816990, Tokens per Sec:    16106, Lr: 0.000300\n",
      "2021-07-27 12:25:32,029 - INFO - joeynmt.training - Epoch  26, Step:   148700, Batch Loss:     1.699742, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-27 12:25:45,420 - INFO - joeynmt.training - Epoch  26, Step:   148800, Batch Loss:     1.855667, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-07-27 12:25:58,595 - INFO - joeynmt.training - Epoch  26, Step:   148900, Batch Loss:     1.775303, Tokens per Sec:    15853, Lr: 0.000300\n",
      "2021-07-27 12:26:11,862 - INFO - joeynmt.training - Epoch  26, Step:   149000, Batch Loss:     1.765141, Tokens per Sec:    16029, Lr: 0.000300\n",
      "2021-07-27 12:26:25,257 - INFO - joeynmt.training - Epoch  26, Step:   149100, Batch Loss:     1.627883, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-07-27 12:26:38,947 - INFO - joeynmt.training - Epoch  26, Step:   149200, Batch Loss:     1.189292, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-07-27 12:26:52,366 - INFO - joeynmt.training - Epoch  26, Step:   149300, Batch Loss:     2.040323, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-07-27 12:27:05,851 - INFO - joeynmt.training - Epoch  26, Step:   149400, Batch Loss:     1.727412, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-07-27 12:27:19,264 - INFO - joeynmt.training - Epoch  26, Step:   149500, Batch Loss:     1.821928, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-07-27 12:27:32,761 - INFO - joeynmt.training - Epoch  26, Step:   149600, Batch Loss:     1.675271, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-07-27 12:27:46,297 - INFO - joeynmt.training - Epoch  26, Step:   149700, Batch Loss:     1.663739, Tokens per Sec:    15558, Lr: 0.000300\n",
      "2021-07-27 12:27:59,717 - INFO - joeynmt.training - Epoch  26, Step:   149800, Batch Loss:     1.530635, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-07-27 12:28:13,220 - INFO - joeynmt.training - Epoch  26, Step:   149900, Batch Loss:     1.847425, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-07-27 12:28:26,581 - INFO - joeynmt.training - Epoch  26, Step:   150000, Batch Loss:     1.550859, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-07-27 12:28:44,543 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:28:44,543 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:28:44,543 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:28:44,808 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 12:28:44,809 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 12:28:45,567 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 12:28:45,568 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 12:28:45,568 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 12:28:45,568 - INFO - joeynmt.training - \tHypothesis: [ I ] hated what you do good and suffer when you endure , that is acceptable to God . ”\n",
      "2021-07-27 12:28:45,569 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 12:28:45,570 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 12:28:45,570 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 12:28:45,571 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my Son , whom I am grateful . ”\n",
      "2021-07-27 12:28:45,571 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 12:28:45,571 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 12:28:45,571 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 12:28:45,572 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 12:28:45,572 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 12:28:45,572 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 12:28:45,573 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 12:28:45,573 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 12:28:45,573 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step   150000: bleu:  27.42, loss: 43185.3320, ppl:   5.0867, duration: 18.9911s\n",
      "2021-07-27 12:28:59,261 - INFO - joeynmt.training - Epoch  26, Step:   150100, Batch Loss:     1.538684, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-07-27 12:29:02,387 - INFO - joeynmt.training - Epoch  26: total training loss 4928.28\n",
      "2021-07-27 12:29:02,388 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-07-27 12:29:12,989 - INFO - joeynmt.training - Epoch  27, Step:   150200, Batch Loss:     1.873450, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-27 12:29:26,373 - INFO - joeynmt.training - Epoch  27, Step:   150300, Batch Loss:     1.735148, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-07-27 12:29:39,626 - INFO - joeynmt.training - Epoch  27, Step:   150400, Batch Loss:     1.799443, Tokens per Sec:    15568, Lr: 0.000300\n",
      "2021-07-27 12:29:53,246 - INFO - joeynmt.training - Epoch  27, Step:   150500, Batch Loss:     1.829073, Tokens per Sec:    15699, Lr: 0.000300\n",
      "2021-07-27 12:30:06,862 - INFO - joeynmt.training - Epoch  27, Step:   150600, Batch Loss:     1.843861, Tokens per Sec:    16383, Lr: 0.000300\n",
      "2021-07-27 12:30:20,367 - INFO - joeynmt.training - Epoch  27, Step:   150700, Batch Loss:     1.904896, Tokens per Sec:    15662, Lr: 0.000300\n",
      "2021-07-27 12:30:33,809 - INFO - joeynmt.training - Epoch  27, Step:   150800, Batch Loss:     1.866714, Tokens per Sec:    15926, Lr: 0.000300\n",
      "2021-07-27 12:30:47,017 - INFO - joeynmt.training - Epoch  27, Step:   150900, Batch Loss:     1.767530, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-07-27 12:31:00,540 - INFO - joeynmt.training - Epoch  27, Step:   151000, Batch Loss:     1.711694, Tokens per Sec:    16117, Lr: 0.000300\n",
      "2021-07-27 12:31:13,669 - INFO - joeynmt.training - Epoch  27, Step:   151100, Batch Loss:     1.815517, Tokens per Sec:    15921, Lr: 0.000300\n",
      "2021-07-27 12:31:27,255 - INFO - joeynmt.training - Epoch  27, Step:   151200, Batch Loss:     1.833701, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-07-27 12:31:40,500 - INFO - joeynmt.training - Epoch  27, Step:   151300, Batch Loss:     1.985923, Tokens per Sec:    15605, Lr: 0.000300\n",
      "2021-07-27 12:31:53,800 - INFO - joeynmt.training - Epoch  27, Step:   151400, Batch Loss:     1.681142, Tokens per Sec:    16340, Lr: 0.000300\n",
      "2021-07-27 12:32:07,137 - INFO - joeynmt.training - Epoch  27, Step:   151500, Batch Loss:     1.972343, Tokens per Sec:    16487, Lr: 0.000300\n",
      "2021-07-27 12:32:20,410 - INFO - joeynmt.training - Epoch  27, Step:   151600, Batch Loss:     1.860083, Tokens per Sec:    15963, Lr: 0.000300\n",
      "2021-07-27 12:32:33,937 - INFO - joeynmt.training - Epoch  27, Step:   151700, Batch Loss:     2.028322, Tokens per Sec:    16427, Lr: 0.000300\n",
      "2021-07-27 12:32:47,291 - INFO - joeynmt.training - Epoch  27, Step:   151800, Batch Loss:     1.985077, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-07-27 12:33:00,634 - INFO - joeynmt.training - Epoch  27, Step:   151900, Batch Loss:     1.493164, Tokens per Sec:    16008, Lr: 0.000300\n",
      "2021-07-27 12:33:13,858 - INFO - joeynmt.training - Epoch  27, Step:   152000, Batch Loss:     2.001441, Tokens per Sec:    16043, Lr: 0.000300\n",
      "2021-07-27 12:33:27,051 - INFO - joeynmt.training - Epoch  27, Step:   152100, Batch Loss:     1.800936, Tokens per Sec:    15805, Lr: 0.000300\n",
      "2021-07-27 12:33:40,377 - INFO - joeynmt.training - Epoch  27, Step:   152200, Batch Loss:     1.650799, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-27 12:33:53,793 - INFO - joeynmt.training - Epoch  27, Step:   152300, Batch Loss:     1.890041, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-07-27 12:34:07,182 - INFO - joeynmt.training - Epoch  27, Step:   152400, Batch Loss:     1.932400, Tokens per Sec:    16320, Lr: 0.000300\n",
      "2021-07-27 12:34:20,374 - INFO - joeynmt.training - Epoch  27, Step:   152500, Batch Loss:     1.933232, Tokens per Sec:    16212, Lr: 0.000300\n",
      "2021-07-27 12:34:33,730 - INFO - joeynmt.training - Epoch  27, Step:   152600, Batch Loss:     1.930288, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-07-27 12:34:47,097 - INFO - joeynmt.training - Epoch  27, Step:   152700, Batch Loss:     1.780372, Tokens per Sec:    16058, Lr: 0.000300\n",
      "2021-07-27 12:35:00,388 - INFO - joeynmt.training - Epoch  27, Step:   152800, Batch Loss:     1.766725, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-07-27 12:35:13,942 - INFO - joeynmt.training - Epoch  27, Step:   152900, Batch Loss:     1.910838, Tokens per Sec:    16330, Lr: 0.000300\n",
      "2021-07-27 12:35:14,562 - INFO - joeynmt.training - Epoch  27: total training loss 4922.72\n",
      "2021-07-27 12:35:14,563 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-07-27 12:35:27,593 - INFO - joeynmt.training - Epoch  28, Step:   153000, Batch Loss:     1.836719, Tokens per Sec:    15588, Lr: 0.000300\n",
      "2021-07-27 12:35:45,731 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:35:45,732 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:35:45,732 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:35:45,956 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 12:35:45,956 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 12:35:46,643 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 12:35:46,644 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 12:35:46,644 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 12:35:46,644 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
      "2021-07-27 12:35:46,644 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 12:35:46,645 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 12:35:46,645 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 12:35:46,645 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard voice from heaven , saying : “ This is my beloved Son , whom I have approved . ”\n",
      "2021-07-27 12:35:46,645 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 12:35:46,646 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 12:35:46,646 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 12:35:46,646 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
      "2021-07-27 12:35:46,646 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 12:35:46,647 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 12:35:46,647 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 12:35:46,647 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 12:35:46,647 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step   153000: bleu:  27.51, loss: 43101.8008, ppl:   5.0707, duration: 19.0540s\n",
      "2021-07-27 12:36:00,142 - INFO - joeynmt.training - Epoch  28, Step:   153100, Batch Loss:     1.561432, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-07-27 12:36:13,460 - INFO - joeynmt.training - Epoch  28, Step:   153200, Batch Loss:     1.485578, Tokens per Sec:    16134, Lr: 0.000300\n",
      "2021-07-27 12:36:27,010 - INFO - joeynmt.training - Epoch  28, Step:   153300, Batch Loss:     1.776984, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-07-27 12:36:40,374 - INFO - joeynmt.training - Epoch  28, Step:   153400, Batch Loss:     2.001563, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-07-27 12:36:53,716 - INFO - joeynmt.training - Epoch  28, Step:   153500, Batch Loss:     1.151201, Tokens per Sec:    16420, Lr: 0.000300\n",
      "2021-07-27 12:37:06,998 - INFO - joeynmt.training - Epoch  28, Step:   153600, Batch Loss:     1.933736, Tokens per Sec:    16100, Lr: 0.000300\n",
      "2021-07-27 12:37:20,229 - INFO - joeynmt.training - Epoch  28, Step:   153700, Batch Loss:     1.961458, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-07-27 12:37:33,656 - INFO - joeynmt.training - Epoch  28, Step:   153800, Batch Loss:     1.893318, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-07-27 12:37:46,898 - INFO - joeynmt.training - Epoch  28, Step:   153900, Batch Loss:     1.932208, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-07-27 12:38:00,120 - INFO - joeynmt.training - Epoch  28, Step:   154000, Batch Loss:     1.684358, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-07-27 12:38:13,276 - INFO - joeynmt.training - Epoch  28, Step:   154100, Batch Loss:     1.812507, Tokens per Sec:    16017, Lr: 0.000300\n",
      "2021-07-27 12:38:26,533 - INFO - joeynmt.training - Epoch  28, Step:   154200, Batch Loss:     1.952385, Tokens per Sec:    16217, Lr: 0.000300\n",
      "2021-07-27 12:38:40,108 - INFO - joeynmt.training - Epoch  28, Step:   154300, Batch Loss:     1.988993, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-07-27 12:38:53,428 - INFO - joeynmt.training - Epoch  28, Step:   154400, Batch Loss:     1.802480, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-07-27 12:39:06,783 - INFO - joeynmt.training - Epoch  28, Step:   154500, Batch Loss:     1.694442, Tokens per Sec:    16426, Lr: 0.000300\n",
      "2021-07-27 12:39:19,947 - INFO - joeynmt.training - Epoch  28, Step:   154600, Batch Loss:     1.983843, Tokens per Sec:    16271, Lr: 0.000300\n",
      "2021-07-27 12:39:33,446 - INFO - joeynmt.training - Epoch  28, Step:   154700, Batch Loss:     1.868556, Tokens per Sec:    16652, Lr: 0.000300\n",
      "2021-07-27 12:39:46,758 - INFO - joeynmt.training - Epoch  28, Step:   154800, Batch Loss:     1.966029, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-07-27 12:39:59,995 - INFO - joeynmt.training - Epoch  28, Step:   154900, Batch Loss:     1.739713, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-07-27 12:40:13,534 - INFO - joeynmt.training - Epoch  28, Step:   155000, Batch Loss:     1.758872, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-07-27 12:40:26,947 - INFO - joeynmt.training - Epoch  28, Step:   155100, Batch Loss:     1.599154, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-07-27 12:40:40,296 - INFO - joeynmt.training - Epoch  28, Step:   155200, Batch Loss:     1.676092, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-07-27 12:40:53,550 - INFO - joeynmt.training - Epoch  28, Step:   155300, Batch Loss:     1.449722, Tokens per Sec:    16300, Lr: 0.000300\n",
      "2021-07-27 12:41:06,849 - INFO - joeynmt.training - Epoch  28, Step:   155400, Batch Loss:     1.781915, Tokens per Sec:    16185, Lr: 0.000300\n",
      "2021-07-27 12:41:20,363 - INFO - joeynmt.training - Epoch  28, Step:   155500, Batch Loss:     1.805213, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-27 12:41:33,767 - INFO - joeynmt.training - Epoch  28, Step:   155600, Batch Loss:     1.781882, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-07-27 12:41:44,193 - INFO - joeynmt.training - Epoch  28: total training loss 4897.09\n",
      "2021-07-27 12:41:44,193 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-07-27 12:41:47,383 - INFO - joeynmt.training - Epoch  29, Step:   155700, Batch Loss:     1.502186, Tokens per Sec:    14297, Lr: 0.000300\n",
      "2021-07-27 12:42:00,776 - INFO - joeynmt.training - Epoch  29, Step:   155800, Batch Loss:     1.651070, Tokens per Sec:    16509, Lr: 0.000300\n",
      "2021-07-27 12:42:14,173 - INFO - joeynmt.training - Epoch  29, Step:   155900, Batch Loss:     1.643830, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-07-27 12:42:27,641 - INFO - joeynmt.training - Epoch  29, Step:   156000, Batch Loss:     1.520146, Tokens per Sec:    16086, Lr: 0.000300\n",
      "2021-07-27 12:42:44,849 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:42:44,849 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:42:44,849 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:42:45,691 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 12:42:45,692 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 12:42:45,692 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 12:42:45,692 - INFO - joeynmt.training - \tHypothesis: I hate when you do good and suffer when you endure , this is acceptable to God . ”\n",
      "2021-07-27 12:42:45,693 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 12:42:45,693 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 12:42:45,693 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 12:42:45,693 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven , saying : “ This is my beloved Son , whom I appreciate . ”\n",
      "2021-07-27 12:42:45,693 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 12:42:45,694 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 12:42:45,694 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 12:42:45,694 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
      "2021-07-27 12:42:45,694 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 12:42:45,695 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 12:42:45,695 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 12:42:45,695 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to save his family .\n",
      "2021-07-27 12:42:45,695 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step   156000: bleu:  27.25, loss: 43160.7656, ppl:   5.0820, duration: 18.0536s\n",
      "2021-07-27 12:42:59,201 - INFO - joeynmt.training - Epoch  29, Step:   156100, Batch Loss:     1.556307, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-07-27 12:43:12,548 - INFO - joeynmt.training - Epoch  29, Step:   156200, Batch Loss:     1.868931, Tokens per Sec:    16198, Lr: 0.000300\n",
      "2021-07-27 12:43:25,724 - INFO - joeynmt.training - Epoch  29, Step:   156300, Batch Loss:     1.804966, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-07-27 12:43:39,085 - INFO - joeynmt.training - Epoch  29, Step:   156400, Batch Loss:     1.396843, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-07-27 12:43:52,250 - INFO - joeynmt.training - Epoch  29, Step:   156500, Batch Loss:     1.753021, Tokens per Sec:    16150, Lr: 0.000300\n",
      "2021-07-27 12:44:05,547 - INFO - joeynmt.training - Epoch  29, Step:   156600, Batch Loss:     1.758055, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-07-27 12:44:18,650 - INFO - joeynmt.training - Epoch  29, Step:   156700, Batch Loss:     2.013593, Tokens per Sec:    16329, Lr: 0.000300\n",
      "2021-07-27 12:44:32,172 - INFO - joeynmt.training - Epoch  29, Step:   156800, Batch Loss:     1.984867, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-07-27 12:44:45,431 - INFO - joeynmt.training - Epoch  29, Step:   156900, Batch Loss:     1.813015, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-07-27 12:44:58,719 - INFO - joeynmt.training - Epoch  29, Step:   157000, Batch Loss:     1.883034, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-07-27 12:45:11,980 - INFO - joeynmt.training - Epoch  29, Step:   157100, Batch Loss:     1.594270, Tokens per Sec:    16164, Lr: 0.000300\n",
      "2021-07-27 12:45:25,036 - INFO - joeynmt.training - Epoch  29, Step:   157200, Batch Loss:     1.810946, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-27 12:45:38,521 - INFO - joeynmt.training - Epoch  29, Step:   157300, Batch Loss:     1.615144, Tokens per Sec:    16319, Lr: 0.000300\n",
      "2021-07-27 12:45:51,976 - INFO - joeynmt.training - Epoch  29, Step:   157400, Batch Loss:     1.683998, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-07-27 12:46:05,193 - INFO - joeynmt.training - Epoch  29, Step:   157500, Batch Loss:     1.896214, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-27 12:46:18,479 - INFO - joeynmt.training - Epoch  29, Step:   157600, Batch Loss:     1.839548, Tokens per Sec:    15861, Lr: 0.000300\n",
      "2021-07-27 12:46:31,791 - INFO - joeynmt.training - Epoch  29, Step:   157700, Batch Loss:     1.705005, Tokens per Sec:    15930, Lr: 0.000300\n",
      "2021-07-27 12:46:45,200 - INFO - joeynmt.training - Epoch  29, Step:   157800, Batch Loss:     1.876423, Tokens per Sec:    16649, Lr: 0.000300\n",
      "2021-07-27 12:46:58,523 - INFO - joeynmt.training - Epoch  29, Step:   157900, Batch Loss:     1.665610, Tokens per Sec:    16456, Lr: 0.000300\n",
      "2021-07-27 12:47:11,944 - INFO - joeynmt.training - Epoch  29, Step:   158000, Batch Loss:     1.610014, Tokens per Sec:    16300, Lr: 0.000300\n",
      "2021-07-27 12:47:25,352 - INFO - joeynmt.training - Epoch  29, Step:   158100, Batch Loss:     1.757006, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-07-27 12:47:38,706 - INFO - joeynmt.training - Epoch  29, Step:   158200, Batch Loss:     1.438096, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-07-27 12:47:51,986 - INFO - joeynmt.training - Epoch  29, Step:   158300, Batch Loss:     1.790181, Tokens per Sec:    16341, Lr: 0.000300\n",
      "2021-07-27 12:48:05,281 - INFO - joeynmt.training - Epoch  29, Step:   158400, Batch Loss:     1.942025, Tokens per Sec:    16429, Lr: 0.000300\n",
      "2021-07-27 12:48:12,477 - INFO - joeynmt.training - Epoch  29: total training loss 4899.15\n",
      "2021-07-27 12:48:12,477 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-07-27 12:48:18,781 - INFO - joeynmt.training - Epoch  30, Step:   158500, Batch Loss:     1.770909, Tokens per Sec:    15287, Lr: 0.000300\n",
      "2021-07-27 12:48:32,132 - INFO - joeynmt.training - Epoch  30, Step:   158600, Batch Loss:     1.835829, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-07-27 12:48:45,486 - INFO - joeynmt.training - Epoch  30, Step:   158700, Batch Loss:     1.758502, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-07-27 12:48:58,867 - INFO - joeynmt.training - Epoch  30, Step:   158800, Batch Loss:     1.911608, Tokens per Sec:    16624, Lr: 0.000300\n",
      "2021-07-27 12:49:11,980 - INFO - joeynmt.training - Epoch  30, Step:   158900, Batch Loss:     1.502859, Tokens per Sec:    16296, Lr: 0.000300\n",
      "2021-07-27 12:49:25,016 - INFO - joeynmt.training - Epoch  30, Step:   159000, Batch Loss:     1.789077, Tokens per Sec:    15853, Lr: 0.000300\n",
      "2021-07-27 12:49:41,974 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:49:41,974 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:49:41,974 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:49:42,968 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 12:49:42,970 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
      "2021-07-27 12:49:42,970 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
      "2021-07-27 12:49:42,970 - INFO - joeynmt.training - \tHypothesis: “ If you are doing good and suffer when you endure , this is what is acceptable to God . ”\n",
      "2021-07-27 12:49:42,970 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 12:49:42,971 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
      "2021-07-27 12:49:42,971 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
      "2021-07-27 12:49:42,972 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son . ”\n",
      "2021-07-27 12:49:42,972 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 12:49:42,972 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
      "2021-07-27 12:49:42,972 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
      "2021-07-27 12:49:42,973 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayers ?\n",
      "2021-07-27 12:49:42,973 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 12:49:42,973 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
      "2021-07-27 12:49:42,973 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
      "2021-07-27 12:49:42,973 - INFO - joeynmt.training - \tHypothesis: But Abigail did something to heal his family .\n",
      "2021-07-27 12:49:42,974 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step   159000: bleu:  27.77, loss: 43259.4102, ppl:   5.1009, duration: 17.9572s\n",
      "2021-07-27 12:49:56,491 - INFO - joeynmt.training - Epoch  30, Step:   159100, Batch Loss:     1.381833, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-07-27 12:50:09,677 - INFO - joeynmt.training - Epoch  30, Step:   159200, Batch Loss:     1.872020, Tokens per Sec:    16470, Lr: 0.000300\n",
      "2021-07-27 12:50:22,974 - INFO - joeynmt.training - Epoch  30, Step:   159300, Batch Loss:     1.802560, Tokens per Sec:    16486, Lr: 0.000300\n",
      "2021-07-27 12:50:36,402 - INFO - joeynmt.training - Epoch  30, Step:   159400, Batch Loss:     1.765584, Tokens per Sec:    15925, Lr: 0.000300\n",
      "2021-07-27 12:50:49,511 - INFO - joeynmt.training - Epoch  30, Step:   159500, Batch Loss:     1.824438, Tokens per Sec:    15621, Lr: 0.000300\n",
      "2021-07-27 12:51:02,706 - INFO - joeynmt.training - Epoch  30, Step:   159600, Batch Loss:     1.913131, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-07-27 12:51:16,068 - INFO - joeynmt.training - Epoch  30, Step:   159700, Batch Loss:     1.837506, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-07-27 12:51:29,530 - INFO - joeynmt.training - Epoch  30, Step:   159800, Batch Loss:     1.982787, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-07-27 12:51:42,905 - INFO - joeynmt.training - Epoch  30, Step:   159900, Batch Loss:     1.744504, Tokens per Sec:    16375, Lr: 0.000300\n",
      "2021-07-27 12:51:55,927 - INFO - joeynmt.training - Epoch  30, Step:   160000, Batch Loss:     1.976314, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-07-27 12:52:09,312 - INFO - joeynmt.training - Epoch  30, Step:   160100, Batch Loss:     1.855988, Tokens per Sec:    16465, Lr: 0.000300\n",
      "2021-07-27 12:52:22,714 - INFO - joeynmt.training - Epoch  30, Step:   160200, Batch Loss:     1.896006, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-07-27 12:52:35,941 - INFO - joeynmt.training - Epoch  30, Step:   160300, Batch Loss:     1.780717, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-07-27 12:52:49,310 - INFO - joeynmt.training - Epoch  30, Step:   160400, Batch Loss:     1.885546, Tokens per Sec:    16539, Lr: 0.000300\n",
      "2021-07-27 12:53:02,531 - INFO - joeynmt.training - Epoch  30, Step:   160500, Batch Loss:     1.733026, Tokens per Sec:    16408, Lr: 0.000300\n",
      "2021-07-27 12:53:15,903 - INFO - joeynmt.training - Epoch  30, Step:   160600, Batch Loss:     1.666309, Tokens per Sec:    16413, Lr: 0.000300\n",
      "2021-07-27 12:53:29,380 - INFO - joeynmt.training - Epoch  30, Step:   160700, Batch Loss:     1.780516, Tokens per Sec:    16123, Lr: 0.000300\n",
      "2021-07-27 12:53:42,760 - INFO - joeynmt.training - Epoch  30, Step:   160800, Batch Loss:     1.853237, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-07-27 12:53:55,931 - INFO - joeynmt.training - Epoch  30, Step:   160900, Batch Loss:     1.686911, Tokens per Sec:    16460, Lr: 0.000300\n",
      "2021-07-27 12:54:08,975 - INFO - joeynmt.training - Epoch  30, Step:   161000, Batch Loss:     1.821860, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-07-27 12:54:22,195 - INFO - joeynmt.training - Epoch  30, Step:   161100, Batch Loss:     1.669639, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-07-27 12:54:35,811 - INFO - joeynmt.training - Epoch  30, Step:   161200, Batch Loss:     1.797698, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-07-27 12:54:40,774 - INFO - joeynmt.training - Epoch  30: total training loss 4900.57\n",
      "2021-07-27 12:54:40,775 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-07-27 12:54:40,775 - INFO - joeynmt.training - Best validation result (greedy) at step   153000:   5.07 ppl.\n",
      "2021-07-27 12:54:40,800 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-27 12:54:41,173 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-27 12:54:41,367 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-27 12:54:41,436 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe.en)...\n",
      "2021-07-27 12:55:01,947 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:55:01,947 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:55:01,948 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:55:02,165 - INFO - joeynmt.prediction -  dev bleu[13a]:  28.07 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-27 12:55:02,170 - INFO - joeynmt.prediction - Translations saved to: models/lgen_reverse_transformer2_continued/00153000.hyps.dev\n",
      "2021-07-27 12:55:02,170 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe.en)...\n",
      "2021-07-27 12:55:41,527 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 12:55:41,527 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 12:55:41,527 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 12:55:42,137 - INFO - joeynmt.prediction - test bleu[13a]:  37.28 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-27 12:55:42,143 - INFO - joeynmt.prediction - Translations saved to: models/lgen_reverse_transformer2_continued/00153000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_lgen_reload2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHeLcKzLJyvx"
   },
   "source": [
    "### Reverse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "nsWozW4Y-a7F"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (source_language, target_language1)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{source_language}{target_language1}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language1}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"joeynmt/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 3000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 200\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\", source_language=source_language, target_language1=target_language1)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0ix7KpIlAkxV",
    "outputId": "e6bc5e13-2fdf-401d-f709-cdc24e2d9f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Luganda/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
      "    main()\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Luganda/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Luganda/joeynmt/joeynmt/training.py\", line 767, in train\n",
      "    \"overwrite\", False))\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Luganda/joeynmt/joeynmt/helpers.py\", line 43, in make_model_dir\n",
      "    \"Model directory exists and overwriting is disabled.\")\n",
      "FileExistsError: Model directory exists and overwriting is disabled.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Nj5pwxs8_hin",
    "outputId": "33c73edc-bf8e-4a2f-c1fa-179027e0eed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3000\tLoss: 90329.59375\tPPL: 34.15279\tbleu: 1.50899\tLR: 0.00030000\t*\n",
      "Steps: 6000\tLoss: 78915.16406\tPPL: 21.86029\tbleu: 2.03797\tLR: 0.00030000\t*\n",
      "Steps: 9000\tLoss: 72870.83594\tPPL: 17.26029\tbleu: 3.00038\tLR: 0.00030000\t*\n",
      "Steps: 12000\tLoss: 68934.50781\tPPL: 14.79876\tbleu: 3.60862\tLR: 0.00030000\t*\n",
      "Steps: 15000\tLoss: 65144.08594\tPPL: 12.76085\tbleu: 5.62373\tLR: 0.00030000\t*\n",
      "Steps: 18000\tLoss: 62416.39453\tPPL: 11.47029\tbleu: 7.69962\tLR: 0.00030000\t*\n",
      "Steps: 21000\tLoss: 60411.26172\tPPL: 10.60561\tbleu: 9.47106\tLR: 0.00030000\t*\n",
      "Steps: 24000\tLoss: 58601.82812\tPPL: 9.88141\tbleu: 10.27910\tLR: 0.00030000\t*\n",
      "Steps: 27000\tLoss: 57266.67188\tPPL: 9.37893\tbleu: 10.88260\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 55989.72656\tPPL: 8.92228\tbleu: 11.46961\tLR: 0.00030000\t*\n",
      "Steps: 33000\tLoss: 54901.21484\tPPL: 8.55062\tbleu: 13.01796\tLR: 0.00030000\t*\n",
      "Steps: 36000\tLoss: 54212.95312\tPPL: 8.32365\tbleu: 12.76414\tLR: 0.00030000\t*\n",
      "Steps: 39000\tLoss: 53024.57422\tPPL: 7.94584\tbleu: 13.52685\tLR: 0.00030000\t*\n",
      "Steps: 42000\tLoss: 52202.44531\tPPL: 7.69456\tbleu: 14.21166\tLR: 0.00030000\t*\n",
      "Steps: 45000\tLoss: 51515.59375\tPPL: 7.49072\tbleu: 15.26987\tLR: 0.00030000\t*\n",
      "Steps: 48000\tLoss: 50852.28516\tPPL: 7.29900\tbleu: 15.42431\tLR: 0.00030000\t*\n",
      "Steps: 51000\tLoss: 50355.15234\tPPL: 7.15853\tbleu: 15.99471\tLR: 0.00030000\t*\n",
      "Steps: 54000\tLoss: 49640.96484\tPPL: 6.96146\tbleu: 16.54822\tLR: 0.00030000\t*\n",
      "Steps: 57000\tLoss: 49221.33203\tPPL: 6.84820\tbleu: 17.37156\tLR: 0.00030000\t*\n",
      "Steps: 60000\tLoss: 48706.58203\tPPL: 6.71179\tbleu: 17.12043\tLR: 0.00030000\t*\n",
      "Steps: 63000\tLoss: 48346.85547\tPPL: 6.61807\tbleu: 17.87956\tLR: 0.00030000\t*\n",
      "Steps: 66000\tLoss: 47925.41797\tPPL: 6.50994\tbleu: 17.74943\tLR: 0.00030000\t*\n",
      "Steps: 69000\tLoss: 47548.74219\tPPL: 6.41479\tbleu: 18.11610\tLR: 0.00030000\t*\n",
      "Steps: 72000\tLoss: 47285.95312\tPPL: 6.34924\tbleu: 18.61697\tLR: 0.00030000\t*\n",
      "Steps: 75000\tLoss: 46802.07812\tPPL: 6.23028\tbleu: 18.86376\tLR: 0.00030000\t*\n",
      "Steps: 78000\tLoss: 46589.00000\tPPL: 6.17860\tbleu: 19.32568\tLR: 0.00030000\t*\n",
      "Steps: 81000\tLoss: 46194.97656\tPPL: 6.08417\tbleu: 19.44003\tLR: 0.00030000\t*\n",
      "Steps: 84000\tLoss: 46007.70703\tPPL: 6.03980\tbleu: 19.65361\tLR: 0.00030000\t*\n",
      "Steps: 87000\tLoss: 45787.66406\tPPL: 5.98807\tbleu: 19.55316\tLR: 0.00030000\t*\n",
      "Steps: 90000\tLoss: 45563.75781\tPPL: 5.93589\tbleu: 19.47783\tLR: 0.00030000\t*\n",
      "Steps: 93000\tLoss: 45265.30859\tPPL: 5.86704\tbleu: 20.28252\tLR: 0.00030000\t*\n",
      "Steps: 96000\tLoss: 45113.85938\tPPL: 5.83241\tbleu: 19.97785\tLR: 0.00030000\t*\n",
      "Steps: 99000\tLoss: 44852.17188\tPPL: 5.77306\tbleu: 20.41608\tLR: 0.00030000\t*\n",
      "Steps: 102000\tLoss: 44617.91797\tPPL: 5.72044\tbleu: 20.50193\tLR: 0.00030000\t*\n",
      "Steps: 105000\tLoss: 44513.35547\tPPL: 5.69711\tbleu: 20.30506\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/enlg_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwTLsMsm_8py",
    "outputId": "c6a925a4-fa5e-4536-8789-2fddd8ffa618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-26 08:58:21,062 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-26 08:58:21,068 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-26 08:58:21,377 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-26 08:58:21,395 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-26 08:58:21,429 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-26 08:58:21,451 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-26 08:58:24,119 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-26 08:58:24,375 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-26 08:58:24,458 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/dev.bpe.lg)...\n",
      "2021-07-26 08:59:10,927 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 08:59:10,928 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 08:59:10,928 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 08:59:11,179 - INFO - joeynmt.prediction -  dev bleu[13a]:  20.37 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-26 08:59:11,179 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/test.bpe.lg)...\n",
      "2021-07-26 09:00:36,454 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 09:00:36,454 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 09:00:36,455 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 09:00:37,097 - INFO - joeynmt.prediction - test bleu[13a]:  29.84 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt test 'models/enlg_transformer/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8p3Tbx8cWEFA"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BsX6K4EQ6cyB"
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir joeynmt/models/lgen_reverse_transformer2/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7UpBGEl9T44"
   },
   "source": [
    "## Kinyarwanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQTj3_hl1gZF"
   },
   "outputs": [],
   "source": [
    "# Changing to Kinyarwanda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "YY9Rini27wHX",
    "outputId": "d1baea94-173b-4a04-c035-17b5758087cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 95 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 21.1 MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 34.3 MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.9.5-py3-none-any.whl (375 kB)\n",
      "\u001b[K     |████████████████████████████████| 375 kB 46.3 MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Collecting astroid<2.7,>=2.6.5\n",
      "  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 43.7 MB/s \n",
      "\u001b[?25hCollecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.9.2-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 38.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting mccabe<0.7,>=0.6\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting typed-ast<1.5,>=1.4.0\n",
      "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 34.4 MB/s \n",
      "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 4.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85058 sha256=4541cb1df85a573aa69aae39f7c770aa8ad540953873dd62e7fa76c523d0d318\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6wo_p9_a/wheels/59/bd/a5/113ce66d51703aba488fda9b70982ce262e9b5fd115452af28\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68443 sha256=46afbe25e4a1eea5d320ad616ce64c7d4693fcfc03f153b4d870b952a692e9f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
      "Successfully built joeynmt wrapt\n",
      "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
      "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed astroid-2.6.5 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.5 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "a7gEmbTc9bdI"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language2, source_language)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language2}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language2}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer\"\n",
    "    overwrite: False              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda\", source_language=source_language, target_language2=target_language2)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "meq21XqE9bdK",
    "outputId": "a52c2a03-6a50-41c3-d473-e7fb329fff64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 05:50:23,545 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-05-25 05:50:23,615 - INFO - joeynmt.data - Loading training data...\n",
      "2021-05-25 05:50:33,266 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-05-25 05:50:33,870 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-05-25 05:50:33,909 - INFO - joeynmt.data - Loading test data...\n",
      "2021-05-25 05:50:34,705 - INFO - joeynmt.data - Data loaded.\n",
      "2021-05-25 05:50:34,705 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-05-25 05:50:34,915 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-05-25 05:50:35.132232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-05-25 05:50:38,257 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-05-25 05:50:42,369 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-05-25 05:50:42,369 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-05-25 05:50:42,369 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-05-25 05:50:42,369 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-05-25 05:50:42,370 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-05-25 05:50:42,370 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-05-25 05:50:42,370 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-05-25 05:50:42,370 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-05-25 05:50:42,370 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-05-25 05:50:42,370 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-05-25 05:50:42,370 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-05-25 05:50:42,371 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-05-25 05:50:42,371 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-05-25 05:50:42,371 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-05-25 05:50:42,371 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-05-25 05:50:42,371 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-05-25 05:50:42,371 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-05-25 05:50:42,372 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-05-25 05:50:42,372 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-05-25 05:50:42,372 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-05-25 05:50:42,372 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-05-25 05:50:42,372 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-05-25 05:50:42,372 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-05-25 05:50:42,372 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-05-25 05:50:42,373 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-05-25 05:50:42,373 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-05-25 05:50:42,373 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-05-25 05:50:42,373 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-05-25 05:50:42,373 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-05-25 05:50:42,373 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-05-25 05:50:42,373 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-05-25 05:50:42,374 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-05-25 05:50:42,374 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-05-25 05:50:42,374 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-05-25 05:50:42,374 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-05-25 05:50:42,374 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-05-25 05:50:42,374 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-05-25 05:50:42,374 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer\n",
      "2021-05-25 05:50:42,375 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-05-25 05:50:42,375 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-05-25 05:50:42,375 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-05-25 05:50:42,375 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-05-25 05:50:42,375 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-05-25 05:50:42,375 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-05-25 05:50:42,375 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-05-25 05:50:42,376 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-05-25 05:50:42,376 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-05-25 05:50:42,376 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-05-25 05:50:42,376 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-05-25 05:50:42,376 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-05-25 05:50:42,376 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-05-25 05:50:42,376 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-05-25 05:50:42,376 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-05-25 05:50:42,377 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-05-25 05:50:42,377 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-05-25 05:50:42,377 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-05-25 05:50:42,377 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-05-25 05:50:42,377 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-05-25 05:50:42,377 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-05-25 05:50:42,378 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-05-25 05:50:42,378 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-05-25 05:50:42,378 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-05-25 05:50:42,378 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-05-25 05:50:42,378 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-05-25 05:50:42,378 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-05-25 05:50:42,379 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-05-25 05:50:42,379 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-05-25 05:50:42,379 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-05-25 05:50:42,379 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-05-25 05:50:42,379 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434471,\n",
      "\tvalid 1000,\n",
      "\ttest 2651\n",
      "2021-05-25 05:50:42,379 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-05-25 05:50:42,379 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-05-25 05:50:42,380 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-05-25 05:50:42,380 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-05-25 05:50:42,380 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-05-25 05:50:42,380 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-05-25 05:50:42,392 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-05-25 05:50:42,393 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-05-25 05:50:55,390 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.624279, Tokens per Sec:    13937, Lr: 0.000300\n",
      "2021-05-25 05:51:07,698 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.420338, Tokens per Sec:    15057, Lr: 0.000300\n",
      "2021-05-25 05:51:19,996 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.313389, Tokens per Sec:    14712, Lr: 0.000300\n",
      "2021-05-25 05:51:32,317 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.041634, Tokens per Sec:    14582, Lr: 0.000300\n",
      "2021-05-25 05:51:44,924 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.910408, Tokens per Sec:    14552, Lr: 0.000300\n",
      "2021-05-25 05:51:57,563 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.693078, Tokens per Sec:    14941, Lr: 0.000300\n",
      "2021-05-25 05:52:10,116 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.768812, Tokens per Sec:    14471, Lr: 0.000300\n",
      "2021-05-25 05:52:22,887 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.594994, Tokens per Sec:    14485, Lr: 0.000300\n",
      "2021-05-25 05:52:35,449 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.411641, Tokens per Sec:    14258, Lr: 0.000300\n",
      "2021-05-25 05:52:48,187 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.525976, Tokens per Sec:    14354, Lr: 0.000300\n",
      "2021-05-25 05:53:01,022 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     4.540503, Tokens per Sec:    14510, Lr: 0.000300\n",
      "2021-05-25 05:53:13,725 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.413385, Tokens per Sec:    13802, Lr: 0.000300\n",
      "2021-05-25 05:53:26,870 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.339561, Tokens per Sec:    14117, Lr: 0.000300\n",
      "2021-05-25 05:53:40,087 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.461972, Tokens per Sec:    13973, Lr: 0.000300\n",
      "2021-05-25 05:53:53,192 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.330723, Tokens per Sec:    13982, Lr: 0.000300\n",
      "2021-05-25 05:54:06,300 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.385557, Tokens per Sec:    13984, Lr: 0.000300\n",
      "2021-05-25 05:54:19,477 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     4.180731, Tokens per Sec:    14422, Lr: 0.000300\n",
      "2021-05-25 05:54:32,632 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.215335, Tokens per Sec:    14228, Lr: 0.000300\n",
      "2021-05-25 05:54:45,959 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     4.081343, Tokens per Sec:    13856, Lr: 0.000300\n",
      "2021-05-25 05:54:59,035 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.839901, Tokens per Sec:    14117, Lr: 0.000300\n",
      "2021-05-25 05:55:12,179 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.976755, Tokens per Sec:    14250, Lr: 0.000300\n",
      "2021-05-25 05:55:25,280 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.992982, Tokens per Sec:    13962, Lr: 0.000300\n",
      "2021-05-25 05:55:38,384 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.993232, Tokens per Sec:    14146, Lr: 0.000300\n",
      "2021-05-25 05:55:51,460 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.978636, Tokens per Sec:    13919, Lr: 0.000300\n",
      "2021-05-25 05:56:04,742 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     3.865922, Tokens per Sec:    14327, Lr: 0.000300\n",
      "2021-05-25 05:56:17,721 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.703208, Tokens per Sec:    14095, Lr: 0.000300\n",
      "2021-05-25 05:56:30,721 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.772621, Tokens per Sec:    14077, Lr: 0.000300\n",
      "2021-05-25 05:56:43,599 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.660089, Tokens per Sec:    14233, Lr: 0.000300\n",
      "2021-05-25 05:56:56,556 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.653347, Tokens per Sec:    14474, Lr: 0.000300\n",
      "2021-05-25 05:57:09,405 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.701555, Tokens per Sec:    13874, Lr: 0.000300\n",
      "2021-05-25 05:57:22,491 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.581201, Tokens per Sec:    13647, Lr: 0.000300\n",
      "2021-05-25 05:57:35,641 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.745209, Tokens per Sec:    13645, Lr: 0.000300\n",
      "2021-05-25 05:57:48,714 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     3.592508, Tokens per Sec:    14264, Lr: 0.000300\n",
      "2021-05-25 05:58:01,920 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.735855, Tokens per Sec:    13863, Lr: 0.000300\n",
      "2021-05-25 05:58:15,093 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     3.396476, Tokens per Sec:    14109, Lr: 0.000300\n",
      "2021-05-25 05:58:28,294 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.817915, Tokens per Sec:    14077, Lr: 0.000300\n",
      "2021-05-25 05:58:41,455 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     3.481749, Tokens per Sec:    13671, Lr: 0.000300\n",
      "2021-05-25 05:58:54,505 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     3.671786, Tokens per Sec:    13876, Lr: 0.000300\n",
      "2021-05-25 05:59:07,438 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     3.429517, Tokens per Sec:    14096, Lr: 0.000300\n",
      "2021-05-25 05:59:20,539 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     3.669350, Tokens per Sec:    14400, Lr: 0.000300\n",
      "2021-05-25 05:59:33,485 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     3.396822, Tokens per Sec:    14226, Lr: 0.000300\n",
      "2021-05-25 05:59:46,272 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     3.511708, Tokens per Sec:    14075, Lr: 0.000300\n",
      "2021-05-25 05:59:59,381 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     3.273206, Tokens per Sec:    14058, Lr: 0.000300\n",
      "2021-05-25 06:00:12,587 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     3.409059, Tokens per Sec:    14071, Lr: 0.000300\n",
      "2021-05-25 06:00:25,700 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     3.451251, Tokens per Sec:    14238, Lr: 0.000300\n",
      "2021-05-25 06:00:38,942 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     3.567658, Tokens per Sec:    13885, Lr: 0.000300\n",
      "2021-05-25 06:00:51,905 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     3.388330, Tokens per Sec:    13641, Lr: 0.000300\n",
      "2021-05-25 06:01:05,081 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     3.528415, Tokens per Sec:    13918, Lr: 0.000300\n",
      "2021-05-25 06:01:18,227 - INFO - joeynmt.training - Epoch   1, Step:     4900, Batch Loss:     3.602614, Tokens per Sec:    13700, Lr: 0.000300\n",
      "2021-05-25 06:01:31,409 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     3.348918, Tokens per Sec:    14203, Lr: 0.000300\n",
      "2021-05-25 06:02:09,643 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 06:02:09,644 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 06:02:09,644 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 06:02:09,928 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 06:02:09,928 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 06:02:10,689 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 06:02:10,691 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 06:02:10,691 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 06:02:10,691 - INFO - joeynmt.training - \tHypothesis: He was a good relationship with him .\n",
      "2021-05-25 06:02:10,691 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 06:02:10,692 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 06:02:10,692 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 06:02:10,693 - INFO - joeynmt.training - \tHypothesis: The Bible says that I was a member of the sign of the sign .\n",
      "2021-05-25 06:02:10,693 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 06:02:10,694 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 06:02:10,694 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:02:10,694 - INFO - joeynmt.training - \tHypothesis: Rather , we can be able to be a good relationship with God , or we should be the truth . ​ — Psalm 37 : 17 .\n",
      "2021-05-25 06:02:10,696 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 06:02:10,696 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 06:02:10,697 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 06:02:10,697 - INFO - joeynmt.training - \tHypothesis: The first time of the congregation are not the Christian congregation , and the Christian congregation are not the world of Satan’s world .\n",
      "2021-05-25 06:02:10,697 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     5000: bleu:   3.82, loss: 89031.2188, ppl:  24.6608, duration: 39.2878s\n",
      "2021-05-25 06:02:24,040 - INFO - joeynmt.training - Epoch   1, Step:     5100, Batch Loss:     3.119143, Tokens per Sec:    14184, Lr: 0.000300\n",
      "2021-05-25 06:02:36,814 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     3.475227, Tokens per Sec:    14276, Lr: 0.000300\n",
      "2021-05-25 06:02:49,597 - INFO - joeynmt.training - Epoch   1, Step:     5300, Batch Loss:     3.480042, Tokens per Sec:    14238, Lr: 0.000300\n",
      "2021-05-25 06:03:02,572 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     3.339201, Tokens per Sec:    14365, Lr: 0.000300\n",
      "2021-05-25 06:03:15,768 - INFO - joeynmt.training - Epoch   1, Step:     5500, Batch Loss:     3.351969, Tokens per Sec:    14153, Lr: 0.000300\n",
      "2021-05-25 06:03:28,691 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     3.294084, Tokens per Sec:    13934, Lr: 0.000300\n",
      "2021-05-25 06:03:42,163 - INFO - joeynmt.training - Epoch   1, Step:     5700, Batch Loss:     3.276964, Tokens per Sec:    14477, Lr: 0.000300\n",
      "2021-05-25 06:03:55,132 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     3.238982, Tokens per Sec:    13708, Lr: 0.000300\n",
      "2021-05-25 06:04:08,314 - INFO - joeynmt.training - Epoch   1, Step:     5900, Batch Loss:     3.380951, Tokens per Sec:    14099, Lr: 0.000300\n",
      "2021-05-25 06:04:21,397 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     3.463767, Tokens per Sec:    13942, Lr: 0.000300\n",
      "2021-05-25 06:04:34,569 - INFO - joeynmt.training - Epoch   1, Step:     6100, Batch Loss:     3.361301, Tokens per Sec:    14351, Lr: 0.000300\n",
      "2021-05-25 06:04:47,813 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     3.397905, Tokens per Sec:    14138, Lr: 0.000300\n",
      "2021-05-25 06:05:01,047 - INFO - joeynmt.training - Epoch   1, Step:     6300, Batch Loss:     3.237391, Tokens per Sec:    14141, Lr: 0.000300\n",
      "2021-05-25 06:05:14,250 - INFO - joeynmt.training - Epoch   1, Step:     6400, Batch Loss:     3.345575, Tokens per Sec:    14070, Lr: 0.000300\n",
      "2021-05-25 06:05:24,768 - INFO - joeynmt.training - Epoch   1: total training loss 25282.63\n",
      "2021-05-25 06:05:24,768 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-05-25 06:05:28,060 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     3.239948, Tokens per Sec:    10882, Lr: 0.000300\n",
      "2021-05-25 06:05:41,139 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     3.277789, Tokens per Sec:    14113, Lr: 0.000300\n",
      "2021-05-25 06:05:54,184 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     3.039939, Tokens per Sec:    14534, Lr: 0.000300\n",
      "2021-05-25 06:06:07,026 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     3.391088, Tokens per Sec:    14672, Lr: 0.000300\n",
      "2021-05-25 06:06:19,852 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     3.295918, Tokens per Sec:    14071, Lr: 0.000300\n",
      "2021-05-25 06:06:32,776 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     2.935422, Tokens per Sec:    14118, Lr: 0.000300\n",
      "2021-05-25 06:06:45,862 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     3.472930, Tokens per Sec:    13920, Lr: 0.000300\n",
      "2021-05-25 06:06:58,922 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     3.057116, Tokens per Sec:    14171, Lr: 0.000300\n",
      "2021-05-25 06:07:12,218 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     3.266656, Tokens per Sec:    14038, Lr: 0.000300\n",
      "2021-05-25 06:07:25,458 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     3.323978, Tokens per Sec:    13889, Lr: 0.000300\n",
      "2021-05-25 06:07:38,605 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     3.015421, Tokens per Sec:    13836, Lr: 0.000300\n",
      "2021-05-25 06:07:51,851 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     3.505934, Tokens per Sec:    13992, Lr: 0.000300\n",
      "2021-05-25 06:08:05,023 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     3.176336, Tokens per Sec:    14226, Lr: 0.000300\n",
      "2021-05-25 06:08:18,070 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     3.395770, Tokens per Sec:    13670, Lr: 0.000300\n",
      "2021-05-25 06:08:31,187 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     3.103156, Tokens per Sec:    14300, Lr: 0.000300\n",
      "2021-05-25 06:08:44,435 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     3.155415, Tokens per Sec:    13980, Lr: 0.000300\n",
      "2021-05-25 06:08:57,521 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     3.186101, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 06:09:10,788 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     2.958980, Tokens per Sec:    14247, Lr: 0.000300\n",
      "2021-05-25 06:09:23,747 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     3.129365, Tokens per Sec:    13835, Lr: 0.000300\n",
      "2021-05-25 06:09:37,106 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     3.244768, Tokens per Sec:    14470, Lr: 0.000300\n",
      "2021-05-25 06:09:49,987 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     3.198619, Tokens per Sec:    14418, Lr: 0.000300\n",
      "2021-05-25 06:10:03,060 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     3.235576, Tokens per Sec:    14105, Lr: 0.000300\n",
      "2021-05-25 06:10:16,211 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     3.109978, Tokens per Sec:    13868, Lr: 0.000300\n",
      "2021-05-25 06:10:29,561 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     3.029075, Tokens per Sec:    13826, Lr: 0.000300\n",
      "2021-05-25 06:10:42,590 - INFO - joeynmt.training - Epoch   2, Step:     8900, Batch Loss:     2.890829, Tokens per Sec:    13886, Lr: 0.000300\n",
      "2021-05-25 06:10:55,745 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     2.838319, Tokens per Sec:    13827, Lr: 0.000300\n",
      "2021-05-25 06:11:08,425 - INFO - joeynmt.training - Epoch   2, Step:     9100, Batch Loss:     3.274671, Tokens per Sec:    13810, Lr: 0.000300\n",
      "2021-05-25 06:11:21,493 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     3.280107, Tokens per Sec:    14166, Lr: 0.000300\n",
      "2021-05-25 06:11:34,307 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     3.032539, Tokens per Sec:    14070, Lr: 0.000300\n",
      "2021-05-25 06:11:47,629 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     3.227471, Tokens per Sec:    14179, Lr: 0.000300\n",
      "2021-05-25 06:12:00,699 - INFO - joeynmt.training - Epoch   2, Step:     9500, Batch Loss:     2.957325, Tokens per Sec:    14031, Lr: 0.000300\n",
      "2021-05-25 06:12:14,128 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     2.810259, Tokens per Sec:    14300, Lr: 0.000300\n",
      "2021-05-25 06:12:27,365 - INFO - joeynmt.training - Epoch   2, Step:     9700, Batch Loss:     3.096757, Tokens per Sec:    13841, Lr: 0.000300\n",
      "2021-05-25 06:12:40,471 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     3.084505, Tokens per Sec:    14021, Lr: 0.000300\n",
      "2021-05-25 06:12:53,590 - INFO - joeynmt.training - Epoch   2, Step:     9900, Batch Loss:     2.897781, Tokens per Sec:    14138, Lr: 0.000300\n",
      "2021-05-25 06:13:06,478 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     3.050336, Tokens per Sec:    13674, Lr: 0.000300\n",
      "2021-05-25 06:13:36,950 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 06:13:36,951 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 06:13:36,951 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 06:13:37,227 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 06:13:37,228 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 06:13:38,031 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 06:13:38,032 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 06:13:38,032 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 06:13:38,032 - INFO - joeynmt.training - \tHypothesis: He was a heart .\n",
      "2021-05-25 06:13:38,032 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 06:13:38,033 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 06:13:38,033 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 06:13:38,033 - INFO - joeynmt.training - \tHypothesis: The letter was used in the field of the field .\n",
      "2021-05-25 06:13:38,033 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 06:13:38,034 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 06:13:38,034 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:13:38,034 - INFO - joeynmt.training - \tHypothesis: Instead , we can be able to keep our faith or to pray for God . ​ — Romans 8 : 8 - 35 .\n",
      "2021-05-25 06:13:38,034 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 06:13:38,035 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 06:13:38,035 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 06:13:38,035 - INFO - joeynmt.training - \tHypothesis: Surely , some of the congregation in the congregation , which are some of the Christian congregation , as a result of Satan’s world .\n",
      "2021-05-25 06:13:38,035 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    10000: bleu:   8.17, loss: 76205.7344, ppl:  15.5410, duration: 31.5573s\n",
      "2021-05-25 06:13:51,492 - INFO - joeynmt.training - Epoch   2, Step:    10100, Batch Loss:     3.106979, Tokens per Sec:    14090, Lr: 0.000300\n",
      "2021-05-25 06:14:04,545 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     2.951236, Tokens per Sec:    13709, Lr: 0.000300\n",
      "2021-05-25 06:14:17,492 - INFO - joeynmt.training - Epoch   2, Step:    10300, Batch Loss:     2.989638, Tokens per Sec:    14000, Lr: 0.000300\n",
      "2021-05-25 06:14:30,322 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     2.925756, Tokens per Sec:    14158, Lr: 0.000300\n",
      "2021-05-25 06:14:43,212 - INFO - joeynmt.training - Epoch   2, Step:    10500, Batch Loss:     3.158081, Tokens per Sec:    13961, Lr: 0.000300\n",
      "2021-05-25 06:14:56,477 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     3.201451, Tokens per Sec:    14200, Lr: 0.000300\n",
      "2021-05-25 06:15:09,433 - INFO - joeynmt.training - Epoch   2, Step:    10700, Batch Loss:     3.259527, Tokens per Sec:    13800, Lr: 0.000300\n",
      "2021-05-25 06:15:22,601 - INFO - joeynmt.training - Epoch   2, Step:    10800, Batch Loss:     3.006518, Tokens per Sec:    13934, Lr: 0.000300\n",
      "2021-05-25 06:15:35,623 - INFO - joeynmt.training - Epoch   2, Step:    10900, Batch Loss:     3.014387, Tokens per Sec:    14019, Lr: 0.000300\n",
      "2021-05-25 06:15:48,760 - INFO - joeynmt.training - Epoch   2, Step:    11000, Batch Loss:     2.926566, Tokens per Sec:    14174, Lr: 0.000300\n",
      "2021-05-25 06:16:01,572 - INFO - joeynmt.training - Epoch   2, Step:    11100, Batch Loss:     3.043176, Tokens per Sec:    13887, Lr: 0.000300\n",
      "2021-05-25 06:16:14,627 - INFO - joeynmt.training - Epoch   2, Step:    11200, Batch Loss:     2.826544, Tokens per Sec:    14054, Lr: 0.000300\n",
      "2021-05-25 06:16:27,804 - INFO - joeynmt.training - Epoch   2, Step:    11300, Batch Loss:     2.651001, Tokens per Sec:    14600, Lr: 0.000300\n",
      "2021-05-25 06:16:40,616 - INFO - joeynmt.training - Epoch   2, Step:    11400, Batch Loss:     2.903244, Tokens per Sec:    14358, Lr: 0.000300\n",
      "2021-05-25 06:16:53,701 - INFO - joeynmt.training - Epoch   2, Step:    11500, Batch Loss:     3.210989, Tokens per Sec:    14433, Lr: 0.000300\n",
      "2021-05-25 06:17:06,648 - INFO - joeynmt.training - Epoch   2, Step:    11600, Batch Loss:     2.897141, Tokens per Sec:    13767, Lr: 0.000300\n",
      "2021-05-25 06:17:19,749 - INFO - joeynmt.training - Epoch   2, Step:    11700, Batch Loss:     2.874318, Tokens per Sec:    14173, Lr: 0.000300\n",
      "2021-05-25 06:17:32,941 - INFO - joeynmt.training - Epoch   2, Step:    11800, Batch Loss:     2.706044, Tokens per Sec:    14154, Lr: 0.000300\n",
      "2021-05-25 06:17:45,955 - INFO - joeynmt.training - Epoch   2, Step:    11900, Batch Loss:     2.884063, Tokens per Sec:    13566, Lr: 0.000300\n",
      "2021-05-25 06:17:59,076 - INFO - joeynmt.training - Epoch   2, Step:    12000, Batch Loss:     3.174406, Tokens per Sec:    13902, Lr: 0.000300\n",
      "2021-05-25 06:18:12,334 - INFO - joeynmt.training - Epoch   2, Step:    12100, Batch Loss:     2.846323, Tokens per Sec:    14432, Lr: 0.000300\n",
      "2021-05-25 06:18:25,330 - INFO - joeynmt.training - Epoch   2, Step:    12200, Batch Loss:     2.894021, Tokens per Sec:    14074, Lr: 0.000300\n",
      "2021-05-25 06:18:38,442 - INFO - joeynmt.training - Epoch   2, Step:    12300, Batch Loss:     3.024034, Tokens per Sec:    13991, Lr: 0.000300\n",
      "2021-05-25 06:18:51,472 - INFO - joeynmt.training - Epoch   2, Step:    12400, Batch Loss:     2.754123, Tokens per Sec:    13814, Lr: 0.000300\n",
      "2021-05-25 06:19:04,784 - INFO - joeynmt.training - Epoch   2, Step:    12500, Batch Loss:     2.917757, Tokens per Sec:    14745, Lr: 0.000300\n",
      "2021-05-25 06:19:17,902 - INFO - joeynmt.training - Epoch   2, Step:    12600, Batch Loss:     2.973294, Tokens per Sec:    13753, Lr: 0.000300\n",
      "2021-05-25 06:19:31,033 - INFO - joeynmt.training - Epoch   2, Step:    12700, Batch Loss:     2.931573, Tokens per Sec:    13731, Lr: 0.000300\n",
      "2021-05-25 06:19:44,159 - INFO - joeynmt.training - Epoch   2, Step:    12800, Batch Loss:     2.962807, Tokens per Sec:    13815, Lr: 0.000300\n",
      "2021-05-25 06:19:57,338 - INFO - joeynmt.training - Epoch   2, Step:    12900, Batch Loss:     2.773273, Tokens per Sec:    14032, Lr: 0.000300\n",
      "2021-05-25 06:20:04,743 - INFO - joeynmt.training - Epoch   2: total training loss 19822.84\n",
      "2021-05-25 06:20:04,743 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-05-25 06:20:11,093 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     2.780990, Tokens per Sec:    12028, Lr: 0.000300\n",
      "2021-05-25 06:20:24,055 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     2.758117, Tokens per Sec:    13941, Lr: 0.000300\n",
      "2021-05-25 06:20:37,180 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     2.881133, Tokens per Sec:    13961, Lr: 0.000300\n",
      "2021-05-25 06:20:50,058 - INFO - joeynmt.training - Epoch   3, Step:    13300, Batch Loss:     2.868987, Tokens per Sec:    14314, Lr: 0.000300\n",
      "2021-05-25 06:21:03,141 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     2.912816, Tokens per Sec:    14655, Lr: 0.000300\n",
      "2021-05-25 06:21:16,201 - INFO - joeynmt.training - Epoch   3, Step:    13500, Batch Loss:     2.800809, Tokens per Sec:    14068, Lr: 0.000300\n",
      "2021-05-25 06:21:29,166 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     2.884129, Tokens per Sec:    13778, Lr: 0.000300\n",
      "2021-05-25 06:21:42,426 - INFO - joeynmt.training - Epoch   3, Step:    13700, Batch Loss:     2.878604, Tokens per Sec:    14253, Lr: 0.000300\n",
      "2021-05-25 06:21:55,566 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     3.026743, Tokens per Sec:    14181, Lr: 0.000300\n",
      "2021-05-25 06:22:08,600 - INFO - joeynmt.training - Epoch   3, Step:    13900, Batch Loss:     2.885872, Tokens per Sec:    13901, Lr: 0.000300\n",
      "2021-05-25 06:22:21,677 - INFO - joeynmt.training - Epoch   3, Step:    14000, Batch Loss:     2.860191, Tokens per Sec:    14061, Lr: 0.000300\n",
      "2021-05-25 06:22:34,942 - INFO - joeynmt.training - Epoch   3, Step:    14100, Batch Loss:     2.809669, Tokens per Sec:    14222, Lr: 0.000300\n",
      "2021-05-25 06:22:47,931 - INFO - joeynmt.training - Epoch   3, Step:    14200, Batch Loss:     2.909875, Tokens per Sec:    14193, Lr: 0.000300\n",
      "2021-05-25 06:23:00,767 - INFO - joeynmt.training - Epoch   3, Step:    14300, Batch Loss:     3.027517, Tokens per Sec:    14345, Lr: 0.000300\n",
      "2021-05-25 06:23:13,503 - INFO - joeynmt.training - Epoch   3, Step:    14400, Batch Loss:     2.915075, Tokens per Sec:    14026, Lr: 0.000300\n",
      "2021-05-25 06:23:26,163 - INFO - joeynmt.training - Epoch   3, Step:    14500, Batch Loss:     2.703666, Tokens per Sec:    14064, Lr: 0.000300\n",
      "2021-05-25 06:23:39,338 - INFO - joeynmt.training - Epoch   3, Step:    14600, Batch Loss:     2.731688, Tokens per Sec:    13953, Lr: 0.000300\n",
      "2021-05-25 06:23:52,337 - INFO - joeynmt.training - Epoch   3, Step:    14700, Batch Loss:     2.880682, Tokens per Sec:    14027, Lr: 0.000300\n",
      "2021-05-25 06:24:05,470 - INFO - joeynmt.training - Epoch   3, Step:    14800, Batch Loss:     2.832430, Tokens per Sec:    14367, Lr: 0.000300\n",
      "2021-05-25 06:24:18,598 - INFO - joeynmt.training - Epoch   3, Step:    14900, Batch Loss:     2.654537, Tokens per Sec:    14059, Lr: 0.000300\n",
      "2021-05-25 06:24:31,564 - INFO - joeynmt.training - Epoch   3, Step:    15000, Batch Loss:     2.824116, Tokens per Sec:    13997, Lr: 0.000300\n",
      "2021-05-25 06:24:58,214 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 06:24:58,215 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 06:24:58,215 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 06:24:58,487 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 06:24:58,488 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 06:24:59,249 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 06:24:59,249 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 06:24:59,250 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 06:24:59,250 - INFO - joeynmt.training - \tHypothesis: He was a heart .\n",
      "2021-05-25 06:24:59,250 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 06:24:59,250 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 06:24:59,251 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 06:24:59,251 - INFO - joeynmt.training - \tHypothesis: The writer was written in the field of the first step .\n",
      "2021-05-25 06:24:59,251 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 06:24:59,252 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 06:24:59,252 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:24:59,252 - INFO - joeynmt.training - \tHypothesis: Instead of being able to see or see how we should continue to keep faith in God through his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:24:59,252 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 06:24:59,253 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 06:24:59,253 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 06:24:59,253 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation shows that they are in a similar way of Satan’s world .\n",
      "2021-05-25 06:24:59,253 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    15000: bleu:  12.18, loss: 69015.2500, ppl:  11.9965, duration: 27.6886s\n",
      "2021-05-25 06:25:12,598 - INFO - joeynmt.training - Epoch   3, Step:    15100, Batch Loss:     2.908616, Tokens per Sec:    13767, Lr: 0.000300\n",
      "2021-05-25 06:25:25,765 - INFO - joeynmt.training - Epoch   3, Step:    15200, Batch Loss:     2.602991, Tokens per Sec:    14258, Lr: 0.000300\n",
      "2021-05-25 06:25:38,818 - INFO - joeynmt.training - Epoch   3, Step:    15300, Batch Loss:     2.802623, Tokens per Sec:    13768, Lr: 0.000300\n",
      "2021-05-25 06:25:51,857 - INFO - joeynmt.training - Epoch   3, Step:    15400, Batch Loss:     2.835290, Tokens per Sec:    13750, Lr: 0.000300\n",
      "2021-05-25 06:26:04,986 - INFO - joeynmt.training - Epoch   3, Step:    15500, Batch Loss:     2.721865, Tokens per Sec:    13995, Lr: 0.000300\n",
      "2021-05-25 06:26:18,292 - INFO - joeynmt.training - Epoch   3, Step:    15600, Batch Loss:     2.916342, Tokens per Sec:    14112, Lr: 0.000300\n",
      "2021-05-25 06:26:31,085 - INFO - joeynmt.training - Epoch   3, Step:    15700, Batch Loss:     2.852424, Tokens per Sec:    14145, Lr: 0.000300\n",
      "2021-05-25 06:26:44,017 - INFO - joeynmt.training - Epoch   3, Step:    15800, Batch Loss:     2.645500, Tokens per Sec:    14354, Lr: 0.000300\n",
      "2021-05-25 06:26:56,932 - INFO - joeynmt.training - Epoch   3, Step:    15900, Batch Loss:     2.762862, Tokens per Sec:    14290, Lr: 0.000300\n",
      "2021-05-25 06:27:09,744 - INFO - joeynmt.training - Epoch   3, Step:    16000, Batch Loss:     3.055045, Tokens per Sec:    14260, Lr: 0.000300\n",
      "2021-05-25 06:27:22,562 - INFO - joeynmt.training - Epoch   3, Step:    16100, Batch Loss:     2.848687, Tokens per Sec:    14192, Lr: 0.000300\n",
      "2021-05-25 06:27:35,591 - INFO - joeynmt.training - Epoch   3, Step:    16200, Batch Loss:     2.833679, Tokens per Sec:    14350, Lr: 0.000300\n",
      "2021-05-25 06:27:48,436 - INFO - joeynmt.training - Epoch   3, Step:    16300, Batch Loss:     2.710377, Tokens per Sec:    13972, Lr: 0.000300\n",
      "2021-05-25 06:28:01,445 - INFO - joeynmt.training - Epoch   3, Step:    16400, Batch Loss:     2.530898, Tokens per Sec:    13795, Lr: 0.000300\n",
      "2021-05-25 06:28:14,486 - INFO - joeynmt.training - Epoch   3, Step:    16500, Batch Loss:     2.874605, Tokens per Sec:    14189, Lr: 0.000300\n",
      "2021-05-25 06:28:27,722 - INFO - joeynmt.training - Epoch   3, Step:    16600, Batch Loss:     2.548197, Tokens per Sec:    14103, Lr: 0.000300\n",
      "2021-05-25 06:28:40,828 - INFO - joeynmt.training - Epoch   3, Step:    16700, Batch Loss:     2.707497, Tokens per Sec:    14183, Lr: 0.000300\n",
      "2021-05-25 06:28:53,961 - INFO - joeynmt.training - Epoch   3, Step:    16800, Batch Loss:     2.682092, Tokens per Sec:    14275, Lr: 0.000300\n",
      "2021-05-25 06:29:06,927 - INFO - joeynmt.training - Epoch   3, Step:    16900, Batch Loss:     2.930260, Tokens per Sec:    14074, Lr: 0.000300\n",
      "2021-05-25 06:29:19,956 - INFO - joeynmt.training - Epoch   3, Step:    17000, Batch Loss:     2.623201, Tokens per Sec:    14198, Lr: 0.000300\n",
      "2021-05-25 06:29:33,193 - INFO - joeynmt.training - Epoch   3, Step:    17100, Batch Loss:     2.778840, Tokens per Sec:    14325, Lr: 0.000300\n",
      "2021-05-25 06:29:46,207 - INFO - joeynmt.training - Epoch   3, Step:    17200, Batch Loss:     2.684087, Tokens per Sec:    13838, Lr: 0.000300\n",
      "2021-05-25 06:29:59,304 - INFO - joeynmt.training - Epoch   3, Step:    17300, Batch Loss:     2.529740, Tokens per Sec:    13893, Lr: 0.000300\n",
      "2021-05-25 06:30:12,527 - INFO - joeynmt.training - Epoch   3, Step:    17400, Batch Loss:     2.635686, Tokens per Sec:    14432, Lr: 0.000300\n",
      "2021-05-25 06:30:25,685 - INFO - joeynmt.training - Epoch   3, Step:    17500, Batch Loss:     2.625182, Tokens per Sec:    14066, Lr: 0.000300\n",
      "2021-05-25 06:30:38,964 - INFO - joeynmt.training - Epoch   3, Step:    17600, Batch Loss:     2.623683, Tokens per Sec:    14082, Lr: 0.000300\n",
      "2021-05-25 06:30:52,114 - INFO - joeynmt.training - Epoch   3, Step:    17700, Batch Loss:     2.614069, Tokens per Sec:    14202, Lr: 0.000300\n",
      "2021-05-25 06:31:05,081 - INFO - joeynmt.training - Epoch   3, Step:    17800, Batch Loss:     2.651236, Tokens per Sec:    13862, Lr: 0.000300\n",
      "2021-05-25 06:31:18,415 - INFO - joeynmt.training - Epoch   3, Step:    17900, Batch Loss:     2.779816, Tokens per Sec:    14557, Lr: 0.000300\n",
      "2021-05-25 06:31:31,382 - INFO - joeynmt.training - Epoch   3, Step:    18000, Batch Loss:     2.724192, Tokens per Sec:    13887, Lr: 0.000300\n",
      "2021-05-25 06:31:44,524 - INFO - joeynmt.training - Epoch   3, Step:    18100, Batch Loss:     2.688755, Tokens per Sec:    14124, Lr: 0.000300\n",
      "2021-05-25 06:31:57,409 - INFO - joeynmt.training - Epoch   3, Step:    18200, Batch Loss:     2.697904, Tokens per Sec:    13691, Lr: 0.000300\n",
      "2021-05-25 06:32:10,453 - INFO - joeynmt.training - Epoch   3, Step:    18300, Batch Loss:     2.620648, Tokens per Sec:    14059, Lr: 0.000300\n",
      "2021-05-25 06:32:23,489 - INFO - joeynmt.training - Epoch   3, Step:    18400, Batch Loss:     2.552875, Tokens per Sec:    13690, Lr: 0.000300\n",
      "2021-05-25 06:32:36,782 - INFO - joeynmt.training - Epoch   3, Step:    18500, Batch Loss:     2.699680, Tokens per Sec:    14668, Lr: 0.000300\n",
      "2021-05-25 06:32:49,974 - INFO - joeynmt.training - Epoch   3, Step:    18600, Batch Loss:     2.576158, Tokens per Sec:    14213, Lr: 0.000300\n",
      "2021-05-25 06:33:02,949 - INFO - joeynmt.training - Epoch   3, Step:    18700, Batch Loss:     2.816388, Tokens per Sec:    13532, Lr: 0.000300\n",
      "2021-05-25 06:33:16,024 - INFO - joeynmt.training - Epoch   3, Step:    18800, Batch Loss:     2.545962, Tokens per Sec:    14030, Lr: 0.000300\n",
      "2021-05-25 06:33:29,125 - INFO - joeynmt.training - Epoch   3, Step:    18900, Batch Loss:     2.630157, Tokens per Sec:    14074, Lr: 0.000300\n",
      "2021-05-25 06:33:42,143 - INFO - joeynmt.training - Epoch   3, Step:    19000, Batch Loss:     2.600166, Tokens per Sec:    14176, Lr: 0.000300\n",
      "2021-05-25 06:33:55,226 - INFO - joeynmt.training - Epoch   3, Step:    19100, Batch Loss:     2.716850, Tokens per Sec:    14217, Lr: 0.000300\n",
      "2021-05-25 06:34:08,265 - INFO - joeynmt.training - Epoch   3, Step:    19200, Batch Loss:     2.689482, Tokens per Sec:    14176, Lr: 0.000300\n",
      "2021-05-25 06:34:21,095 - INFO - joeynmt.training - Epoch   3, Step:    19300, Batch Loss:     2.655170, Tokens per Sec:    13696, Lr: 0.000300\n",
      "2021-05-25 06:34:33,891 - INFO - joeynmt.training - Epoch   3, Step:    19400, Batch Loss:     2.713943, Tokens per Sec:    14201, Lr: 0.000300\n",
      "2021-05-25 06:34:39,064 - INFO - joeynmt.training - Epoch   3: total training loss 18015.67\n",
      "2021-05-25 06:34:39,064 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-05-25 06:34:47,211 - INFO - joeynmt.training - Epoch   4, Step:    19500, Batch Loss:     2.802229, Tokens per Sec:    13439, Lr: 0.000300\n",
      "2021-05-25 06:35:00,049 - INFO - joeynmt.training - Epoch   4, Step:    19600, Batch Loss:     2.503072, Tokens per Sec:    14247, Lr: 0.000300\n",
      "2021-05-25 06:35:12,999 - INFO - joeynmt.training - Epoch   4, Step:    19700, Batch Loss:     2.641339, Tokens per Sec:    14257, Lr: 0.000300\n",
      "2021-05-25 06:35:26,117 - INFO - joeynmt.training - Epoch   4, Step:    19800, Batch Loss:     2.775224, Tokens per Sec:    14462, Lr: 0.000300\n",
      "2021-05-25 06:35:39,218 - INFO - joeynmt.training - Epoch   4, Step:    19900, Batch Loss:     2.784045, Tokens per Sec:    13727, Lr: 0.000300\n",
      "2021-05-25 06:35:52,133 - INFO - joeynmt.training - Epoch   4, Step:    20000, Batch Loss:     2.643367, Tokens per Sec:    13707, Lr: 0.000300\n",
      "2021-05-25 06:36:20,833 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 06:36:20,834 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 06:36:20,834 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 06:36:21,095 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 06:36:21,095 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 06:36:21,863 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 06:36:21,863 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 06:36:21,863 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 06:36:21,864 - INFO - joeynmt.training - \tHypothesis: He was doing so .\n",
      "2021-05-25 06:36:21,864 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 06:36:21,864 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 06:36:21,864 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 06:36:21,864 - INFO - joeynmt.training - \tHypothesis: The letter was written in the first state of the soul .\n",
      "2021-05-25 06:36:21,865 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 06:36:21,865 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 06:36:21,865 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:36:21,865 - INFO - joeynmt.training - \tHypothesis: Instead of being saved or see , we should continue to maintain faith in God through his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:36:21,866 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 06:36:21,866 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 06:36:21,868 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 06:36:21,868 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show that they were not in Satan’s world .\n",
      "2021-05-25 06:36:21,868 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    20000: bleu:  14.46, loss: 64636.5195, ppl:  10.2469, duration: 29.7346s\n",
      "2021-05-25 06:36:35,393 - INFO - joeynmt.training - Epoch   4, Step:    20100, Batch Loss:     2.626103, Tokens per Sec:    13298, Lr: 0.000300\n",
      "2021-05-25 06:36:48,613 - INFO - joeynmt.training - Epoch   4, Step:    20200, Batch Loss:     2.583657, Tokens per Sec:    14549, Lr: 0.000300\n",
      "2021-05-25 06:37:01,838 - INFO - joeynmt.training - Epoch   4, Step:    20300, Batch Loss:     2.616711, Tokens per Sec:    14006, Lr: 0.000300\n",
      "2021-05-25 06:37:14,850 - INFO - joeynmt.training - Epoch   4, Step:    20400, Batch Loss:     2.791739, Tokens per Sec:    13867, Lr: 0.000300\n",
      "2021-05-25 06:37:27,934 - INFO - joeynmt.training - Epoch   4, Step:    20500, Batch Loss:     2.622384, Tokens per Sec:    14226, Lr: 0.000300\n",
      "2021-05-25 06:37:40,939 - INFO - joeynmt.training - Epoch   4, Step:    20600, Batch Loss:     2.681480, Tokens per Sec:    14117, Lr: 0.000300\n",
      "2021-05-25 06:37:54,194 - INFO - joeynmt.training - Epoch   4, Step:    20700, Batch Loss:     2.730821, Tokens per Sec:    14492, Lr: 0.000300\n",
      "2021-05-25 06:38:07,275 - INFO - joeynmt.training - Epoch   4, Step:    20800, Batch Loss:     2.683990, Tokens per Sec:    13748, Lr: 0.000300\n",
      "2021-05-25 06:38:20,233 - INFO - joeynmt.training - Epoch   4, Step:    20900, Batch Loss:     2.522216, Tokens per Sec:    13848, Lr: 0.000300\n",
      "2021-05-25 06:38:33,217 - INFO - joeynmt.training - Epoch   4, Step:    21000, Batch Loss:     2.673111, Tokens per Sec:    13975, Lr: 0.000300\n",
      "2021-05-25 06:38:46,361 - INFO - joeynmt.training - Epoch   4, Step:    21100, Batch Loss:     2.809443, Tokens per Sec:    14629, Lr: 0.000300\n",
      "2021-05-25 06:38:59,023 - INFO - joeynmt.training - Epoch   4, Step:    21200, Batch Loss:     2.696353, Tokens per Sec:    13966, Lr: 0.000300\n",
      "2021-05-25 06:39:11,878 - INFO - joeynmt.training - Epoch   4, Step:    21300, Batch Loss:     2.629949, Tokens per Sec:    14491, Lr: 0.000300\n",
      "2021-05-25 06:39:24,895 - INFO - joeynmt.training - Epoch   4, Step:    21400, Batch Loss:     2.806268, Tokens per Sec:    14652, Lr: 0.000300\n",
      "2021-05-25 06:39:37,871 - INFO - joeynmt.training - Epoch   4, Step:    21500, Batch Loss:     2.934687, Tokens per Sec:    13646, Lr: 0.000300\n",
      "2021-05-25 06:39:50,792 - INFO - joeynmt.training - Epoch   4, Step:    21600, Batch Loss:     2.649672, Tokens per Sec:    13726, Lr: 0.000300\n",
      "2021-05-25 06:40:03,938 - INFO - joeynmt.training - Epoch   4, Step:    21700, Batch Loss:     2.589139, Tokens per Sec:    14331, Lr: 0.000300\n",
      "2021-05-25 06:40:17,016 - INFO - joeynmt.training - Epoch   4, Step:    21800, Batch Loss:     2.775514, Tokens per Sec:    14319, Lr: 0.000300\n",
      "2021-05-25 06:40:29,673 - INFO - joeynmt.training - Epoch   4, Step:    21900, Batch Loss:     2.523938, Tokens per Sec:    13774, Lr: 0.000300\n",
      "2021-05-25 06:40:42,621 - INFO - joeynmt.training - Epoch   4, Step:    22000, Batch Loss:     2.643960, Tokens per Sec:    14019, Lr: 0.000300\n",
      "2021-05-25 06:40:55,683 - INFO - joeynmt.training - Epoch   4, Step:    22100, Batch Loss:     2.545749, Tokens per Sec:    14083, Lr: 0.000300\n",
      "2021-05-25 06:41:08,838 - INFO - joeynmt.training - Epoch   4, Step:    22200, Batch Loss:     2.821328, Tokens per Sec:    14062, Lr: 0.000300\n",
      "2021-05-25 06:41:21,959 - INFO - joeynmt.training - Epoch   4, Step:    22300, Batch Loss:     2.393643, Tokens per Sec:    14130, Lr: 0.000300\n",
      "2021-05-25 06:41:35,001 - INFO - joeynmt.training - Epoch   4, Step:    22400, Batch Loss:     2.593215, Tokens per Sec:    14085, Lr: 0.000300\n",
      "2021-05-25 06:41:48,284 - INFO - joeynmt.training - Epoch   4, Step:    22500, Batch Loss:     2.796636, Tokens per Sec:    14254, Lr: 0.000300\n",
      "2021-05-25 06:42:01,377 - INFO - joeynmt.training - Epoch   4, Step:    22600, Batch Loss:     2.742932, Tokens per Sec:    13819, Lr: 0.000300\n",
      "2021-05-25 06:42:14,428 - INFO - joeynmt.training - Epoch   4, Step:    22700, Batch Loss:     2.560620, Tokens per Sec:    13742, Lr: 0.000300\n",
      "2021-05-25 06:42:27,355 - INFO - joeynmt.training - Epoch   4, Step:    22800, Batch Loss:     2.580297, Tokens per Sec:    13827, Lr: 0.000300\n",
      "2021-05-25 06:42:40,470 - INFO - joeynmt.training - Epoch   4, Step:    22900, Batch Loss:     2.704105, Tokens per Sec:    13930, Lr: 0.000300\n",
      "2021-05-25 06:42:53,496 - INFO - joeynmt.training - Epoch   4, Step:    23000, Batch Loss:     2.518169, Tokens per Sec:    14210, Lr: 0.000300\n",
      "2021-05-25 06:43:06,517 - INFO - joeynmt.training - Epoch   4, Step:    23100, Batch Loss:     2.813750, Tokens per Sec:    14232, Lr: 0.000300\n",
      "2021-05-25 06:43:19,037 - INFO - joeynmt.training - Epoch   4, Step:    23200, Batch Loss:     2.456814, Tokens per Sec:    14005, Lr: 0.000300\n",
      "2021-05-25 06:43:31,955 - INFO - joeynmt.training - Epoch   4, Step:    23300, Batch Loss:     2.732790, Tokens per Sec:    14089, Lr: 0.000300\n",
      "2021-05-25 06:43:44,925 - INFO - joeynmt.training - Epoch   4, Step:    23400, Batch Loss:     2.631021, Tokens per Sec:    14365, Lr: 0.000300\n",
      "2021-05-25 06:43:57,976 - INFO - joeynmt.training - Epoch   4, Step:    23500, Batch Loss:     2.377445, Tokens per Sec:    14076, Lr: 0.000300\n",
      "2021-05-25 06:44:11,085 - INFO - joeynmt.training - Epoch   4, Step:    23600, Batch Loss:     2.628427, Tokens per Sec:    14091, Lr: 0.000300\n",
      "2021-05-25 06:44:24,154 - INFO - joeynmt.training - Epoch   4, Step:    23700, Batch Loss:     2.734653, Tokens per Sec:    14011, Lr: 0.000300\n",
      "2021-05-25 06:44:37,284 - INFO - joeynmt.training - Epoch   4, Step:    23800, Batch Loss:     2.389195, Tokens per Sec:    14376, Lr: 0.000300\n",
      "2021-05-25 06:44:50,252 - INFO - joeynmt.training - Epoch   4, Step:    23900, Batch Loss:     2.360868, Tokens per Sec:    13861, Lr: 0.000300\n",
      "2021-05-25 06:45:03,377 - INFO - joeynmt.training - Epoch   4, Step:    24000, Batch Loss:     2.542493, Tokens per Sec:    14116, Lr: 0.000300\n",
      "2021-05-25 06:45:16,594 - INFO - joeynmt.training - Epoch   4, Step:    24100, Batch Loss:     2.550232, Tokens per Sec:    14252, Lr: 0.000300\n",
      "2021-05-25 06:45:29,635 - INFO - joeynmt.training - Epoch   4, Step:    24200, Batch Loss:     2.515225, Tokens per Sec:    13927, Lr: 0.000300\n",
      "2021-05-25 06:45:42,823 - INFO - joeynmt.training - Epoch   4, Step:    24300, Batch Loss:     2.482520, Tokens per Sec:    14126, Lr: 0.000300\n",
      "2021-05-25 06:45:55,998 - INFO - joeynmt.training - Epoch   4, Step:    24400, Batch Loss:     2.589190, Tokens per Sec:    14047, Lr: 0.000300\n",
      "2021-05-25 06:46:09,148 - INFO - joeynmt.training - Epoch   4, Step:    24500, Batch Loss:     2.548169, Tokens per Sec:    14176, Lr: 0.000300\n",
      "2021-05-25 06:46:22,247 - INFO - joeynmt.training - Epoch   4, Step:    24600, Batch Loss:     2.532391, Tokens per Sec:    14252, Lr: 0.000300\n",
      "2021-05-25 06:46:35,322 - INFO - joeynmt.training - Epoch   4, Step:    24700, Batch Loss:     2.604832, Tokens per Sec:    14051, Lr: 0.000300\n",
      "2021-05-25 06:46:48,229 - INFO - joeynmt.training - Epoch   4, Step:    24800, Batch Loss:     2.488516, Tokens per Sec:    14431, Lr: 0.000300\n",
      "2021-05-25 06:47:01,256 - INFO - joeynmt.training - Epoch   4, Step:    24900, Batch Loss:     2.692568, Tokens per Sec:    14651, Lr: 0.000300\n",
      "2021-05-25 06:47:14,141 - INFO - joeynmt.training - Epoch   4, Step:    25000, Batch Loss:     2.588404, Tokens per Sec:    14219, Lr: 0.000300\n",
      "2021-05-25 06:47:41,494 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 06:47:41,494 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 06:47:41,495 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 06:47:41,742 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 06:47:41,742 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 06:47:42,475 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 06:47:42,476 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 06:47:42,477 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 06:47:42,477 - INFO - joeynmt.training - \tHypothesis: He did not have a heart .\n",
      "2021-05-25 06:47:42,477 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 06:47:42,477 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 06:47:42,478 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 06:47:42,478 - INFO - joeynmt.training - \tHypothesis: The letter was written in the first story .\n",
      "2021-05-25 06:47:42,478 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 06:47:42,478 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 06:47:42,479 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:47:42,479 - INFO - joeynmt.training - \tHypothesis: Instead of being discovered or see , we should continue faith in God through his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:47:42,479 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 06:47:42,480 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 06:47:42,480 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 06:47:42,480 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show that they were rejected in Satan’s world .\n",
      "2021-05-25 06:47:42,480 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    25000: bleu:  16.18, loss: 61545.3203, ppl:   9.1678, duration: 28.3392s\n",
      "2021-05-25 06:47:55,822 - INFO - joeynmt.training - Epoch   4, Step:    25100, Batch Loss:     2.457070, Tokens per Sec:    13760, Lr: 0.000300\n",
      "2021-05-25 06:48:08,818 - INFO - joeynmt.training - Epoch   4, Step:    25200, Batch Loss:     2.820070, Tokens per Sec:    13778, Lr: 0.000300\n",
      "2021-05-25 06:48:21,740 - INFO - joeynmt.training - Epoch   4, Step:    25300, Batch Loss:     2.534608, Tokens per Sec:    13827, Lr: 0.000300\n",
      "2021-05-25 06:48:34,797 - INFO - joeynmt.training - Epoch   4, Step:    25400, Batch Loss:     2.303944, Tokens per Sec:    14132, Lr: 0.000300\n",
      "2021-05-25 06:48:48,090 - INFO - joeynmt.training - Epoch   4, Step:    25500, Batch Loss:     2.510263, Tokens per Sec:    14187, Lr: 0.000300\n",
      "2021-05-25 06:49:01,174 - INFO - joeynmt.training - Epoch   4, Step:    25600, Batch Loss:     2.619992, Tokens per Sec:    14245, Lr: 0.000300\n",
      "2021-05-25 06:49:14,212 - INFO - joeynmt.training - Epoch   4, Step:    25700, Batch Loss:     2.551604, Tokens per Sec:    13888, Lr: 0.000300\n",
      "2021-05-25 06:49:27,213 - INFO - joeynmt.training - Epoch   4, Step:    25800, Batch Loss:     2.517269, Tokens per Sec:    13946, Lr: 0.000300\n",
      "2021-05-25 06:49:40,325 - INFO - joeynmt.training - Epoch   4, Step:    25900, Batch Loss:     2.816592, Tokens per Sec:    14187, Lr: 0.000300\n",
      "2021-05-25 06:49:43,656 - INFO - joeynmt.training - Epoch   4: total training loss 16937.68\n",
      "2021-05-25 06:49:43,656 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-05-25 06:49:54,088 - INFO - joeynmt.training - Epoch   5, Step:    26000, Batch Loss:     2.430987, Tokens per Sec:    13269, Lr: 0.000300\n",
      "2021-05-25 06:50:06,993 - INFO - joeynmt.training - Epoch   5, Step:    26100, Batch Loss:     2.586324, Tokens per Sec:    13740, Lr: 0.000300\n",
      "2021-05-25 06:50:20,117 - INFO - joeynmt.training - Epoch   5, Step:    26200, Batch Loss:     2.441875, Tokens per Sec:    14088, Lr: 0.000300\n",
      "2021-05-25 06:50:33,069 - INFO - joeynmt.training - Epoch   5, Step:    26300, Batch Loss:     2.876953, Tokens per Sec:    14058, Lr: 0.000300\n",
      "2021-05-25 06:50:46,032 - INFO - joeynmt.training - Epoch   5, Step:    26400, Batch Loss:     2.492830, Tokens per Sec:    14550, Lr: 0.000300\n",
      "2021-05-25 06:50:58,847 - INFO - joeynmt.training - Epoch   5, Step:    26500, Batch Loss:     2.493675, Tokens per Sec:    13969, Lr: 0.000300\n",
      "2021-05-25 06:51:11,652 - INFO - joeynmt.training - Epoch   5, Step:    26600, Batch Loss:     2.440997, Tokens per Sec:    14110, Lr: 0.000300\n",
      "2021-05-25 06:51:24,451 - INFO - joeynmt.training - Epoch   5, Step:    26700, Batch Loss:     2.489648, Tokens per Sec:    13918, Lr: 0.000300\n",
      "2021-05-25 06:51:37,546 - INFO - joeynmt.training - Epoch   5, Step:    26800, Batch Loss:     2.311642, Tokens per Sec:    14271, Lr: 0.000300\n",
      "2021-05-25 06:51:50,721 - INFO - joeynmt.training - Epoch   5, Step:    26900, Batch Loss:     2.608927, Tokens per Sec:    14180, Lr: 0.000300\n",
      "2021-05-25 06:52:03,895 - INFO - joeynmt.training - Epoch   5, Step:    27000, Batch Loss:     2.623385, Tokens per Sec:    14400, Lr: 0.000300\n",
      "2021-05-25 06:52:17,078 - INFO - joeynmt.training - Epoch   5, Step:    27100, Batch Loss:     2.493101, Tokens per Sec:    14093, Lr: 0.000300\n",
      "2021-05-25 06:52:30,241 - INFO - joeynmt.training - Epoch   5, Step:    27200, Batch Loss:     2.547305, Tokens per Sec:    14214, Lr: 0.000300\n",
      "2021-05-25 06:52:43,177 - INFO - joeynmt.training - Epoch   5, Step:    27300, Batch Loss:     2.514119, Tokens per Sec:    13957, Lr: 0.000300\n",
      "2021-05-25 06:52:56,287 - INFO - joeynmt.training - Epoch   5, Step:    27400, Batch Loss:     2.485493, Tokens per Sec:    14203, Lr: 0.000300\n",
      "2021-05-25 06:53:09,344 - INFO - joeynmt.training - Epoch   5, Step:    27500, Batch Loss:     2.548036, Tokens per Sec:    14354, Lr: 0.000300\n",
      "2021-05-25 06:53:22,404 - INFO - joeynmt.training - Epoch   5, Step:    27600, Batch Loss:     2.524642, Tokens per Sec:    14106, Lr: 0.000300\n",
      "2021-05-25 06:53:35,339 - INFO - joeynmt.training - Epoch   5, Step:    27700, Batch Loss:     2.505482, Tokens per Sec:    13645, Lr: 0.000300\n",
      "2021-05-25 06:53:48,234 - INFO - joeynmt.training - Epoch   5, Step:    27800, Batch Loss:     2.480822, Tokens per Sec:    14008, Lr: 0.000300\n",
      "2021-05-25 06:54:01,379 - INFO - joeynmt.training - Epoch   5, Step:    27900, Batch Loss:     2.627293, Tokens per Sec:    13725, Lr: 0.000300\n",
      "2021-05-25 06:54:14,511 - INFO - joeynmt.training - Epoch   5, Step:    28000, Batch Loss:     2.811108, Tokens per Sec:    14058, Lr: 0.000300\n",
      "2021-05-25 06:54:27,613 - INFO - joeynmt.training - Epoch   5, Step:    28100, Batch Loss:     2.432617, Tokens per Sec:    14296, Lr: 0.000300\n",
      "2021-05-25 06:54:40,573 - INFO - joeynmt.training - Epoch   5, Step:    28200, Batch Loss:     2.499468, Tokens per Sec:    14283, Lr: 0.000300\n",
      "2021-05-25 06:54:53,441 - INFO - joeynmt.training - Epoch   5, Step:    28300, Batch Loss:     2.516073, Tokens per Sec:    14346, Lr: 0.000300\n",
      "2021-05-25 06:55:06,274 - INFO - joeynmt.training - Epoch   5, Step:    28400, Batch Loss:     2.585310, Tokens per Sec:    14114, Lr: 0.000300\n",
      "2021-05-25 06:55:19,180 - INFO - joeynmt.training - Epoch   5, Step:    28500, Batch Loss:     2.483919, Tokens per Sec:    14349, Lr: 0.000300\n",
      "2021-05-25 06:55:32,000 - INFO - joeynmt.training - Epoch   5, Step:    28600, Batch Loss:     2.461748, Tokens per Sec:    14075, Lr: 0.000300\n",
      "2021-05-25 06:55:44,769 - INFO - joeynmt.training - Epoch   5, Step:    28700, Batch Loss:     2.115102, Tokens per Sec:    13925, Lr: 0.000300\n",
      "2021-05-25 06:55:58,066 - INFO - joeynmt.training - Epoch   5, Step:    28800, Batch Loss:     2.566785, Tokens per Sec:    14467, Lr: 0.000300\n",
      "2021-05-25 06:56:11,153 - INFO - joeynmt.training - Epoch   5, Step:    28900, Batch Loss:     2.286066, Tokens per Sec:    13971, Lr: 0.000300\n",
      "2021-05-25 06:56:24,071 - INFO - joeynmt.training - Epoch   5, Step:    29000, Batch Loss:     2.584388, Tokens per Sec:    14052, Lr: 0.000300\n",
      "2021-05-25 06:56:37,260 - INFO - joeynmt.training - Epoch   5, Step:    29100, Batch Loss:     2.560735, Tokens per Sec:    14331, Lr: 0.000300\n",
      "2021-05-25 06:56:50,332 - INFO - joeynmt.training - Epoch   5, Step:    29200, Batch Loss:     2.430593, Tokens per Sec:    14016, Lr: 0.000300\n",
      "2021-05-25 06:57:03,473 - INFO - joeynmt.training - Epoch   5, Step:    29300, Batch Loss:     2.528880, Tokens per Sec:    14035, Lr: 0.000300\n",
      "2021-05-25 06:57:16,273 - INFO - joeynmt.training - Epoch   5, Step:    29400, Batch Loss:     2.632318, Tokens per Sec:    13735, Lr: 0.000300\n",
      "2021-05-25 06:57:29,199 - INFO - joeynmt.training - Epoch   5, Step:    29500, Batch Loss:     2.531884, Tokens per Sec:    13700, Lr: 0.000300\n",
      "2021-05-25 06:57:42,382 - INFO - joeynmt.training - Epoch   5, Step:    29600, Batch Loss:     2.536326, Tokens per Sec:    14209, Lr: 0.000300\n",
      "2021-05-25 06:57:55,487 - INFO - joeynmt.training - Epoch   5, Step:    29700, Batch Loss:     2.287152, Tokens per Sec:    14063, Lr: 0.000300\n",
      "2021-05-25 06:58:08,509 - INFO - joeynmt.training - Epoch   5, Step:    29800, Batch Loss:     2.645001, Tokens per Sec:    14073, Lr: 0.000300\n",
      "2021-05-25 06:58:21,629 - INFO - joeynmt.training - Epoch   5, Step:    29900, Batch Loss:     2.333299, Tokens per Sec:    13989, Lr: 0.000300\n",
      "2021-05-25 06:58:34,636 - INFO - joeynmt.training - Epoch   5, Step:    30000, Batch Loss:     2.359872, Tokens per Sec:    14130, Lr: 0.000300\n",
      "2021-05-25 06:59:03,002 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 06:59:03,003 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 06:59:03,003 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 06:59:03,252 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 06:59:03,252 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 06:59:03,998 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 06:59:03,999 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 06:59:03,999 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 06:59:04,000 - INFO - joeynmt.training - \tHypothesis: I had a heart .\n",
      "2021-05-25 06:59:04,000 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 06:59:04,000 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 06:59:04,000 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 06:59:04,001 - INFO - joeynmt.training - \tHypothesis: The letter was written in the street of the scrollls .\n",
      "2021-05-25 06:59:04,001 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 06:59:04,001 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 06:59:04,002 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:59:04,002 - INFO - joeynmt.training - \tHypothesis: Instead of being saved or see , we should continue to keep our faith in God through his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 06:59:04,002 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 06:59:04,003 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 06:59:04,004 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 06:59:04,004 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a similar example in Satan’s world .\n",
      "2021-05-25 06:59:04,004 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    30000: bleu:  17.03, loss: 59109.0508, ppl:   8.3979, duration: 29.3674s\n",
      "2021-05-25 06:59:17,385 - INFO - joeynmt.training - Epoch   5, Step:    30100, Batch Loss:     2.648934, Tokens per Sec:    13837, Lr: 0.000300\n",
      "2021-05-25 06:59:30,525 - INFO - joeynmt.training - Epoch   5, Step:    30200, Batch Loss:     2.720837, Tokens per Sec:    14183, Lr: 0.000300\n",
      "2021-05-25 06:59:43,629 - INFO - joeynmt.training - Epoch   5, Step:    30300, Batch Loss:     2.485790, Tokens per Sec:    14237, Lr: 0.000300\n",
      "2021-05-25 06:59:56,629 - INFO - joeynmt.training - Epoch   5, Step:    30400, Batch Loss:     2.619704, Tokens per Sec:    14212, Lr: 0.000300\n",
      "2021-05-25 07:00:09,462 - INFO - joeynmt.training - Epoch   5, Step:    30500, Batch Loss:     2.575220, Tokens per Sec:    14473, Lr: 0.000300\n",
      "2021-05-25 07:00:22,390 - INFO - joeynmt.training - Epoch   5, Step:    30600, Batch Loss:     2.377526, Tokens per Sec:    14100, Lr: 0.000300\n",
      "2021-05-25 07:00:35,142 - INFO - joeynmt.training - Epoch   5, Step:    30700, Batch Loss:     2.432365, Tokens per Sec:    14080, Lr: 0.000300\n",
      "2021-05-25 07:00:48,002 - INFO - joeynmt.training - Epoch   5, Step:    30800, Batch Loss:     2.524423, Tokens per Sec:    14334, Lr: 0.000300\n",
      "2021-05-25 07:01:01,101 - INFO - joeynmt.training - Epoch   5, Step:    30900, Batch Loss:     2.687894, Tokens per Sec:    14107, Lr: 0.000300\n",
      "2021-05-25 07:01:14,371 - INFO - joeynmt.training - Epoch   5, Step:    31000, Batch Loss:     2.398504, Tokens per Sec:    14580, Lr: 0.000300\n",
      "2021-05-25 07:01:27,525 - INFO - joeynmt.training - Epoch   5, Step:    31100, Batch Loss:     2.829730, Tokens per Sec:    14400, Lr: 0.000300\n",
      "2021-05-25 07:01:40,542 - INFO - joeynmt.training - Epoch   5, Step:    31200, Batch Loss:     2.722150, Tokens per Sec:    13882, Lr: 0.000300\n",
      "2021-05-25 07:01:53,422 - INFO - joeynmt.training - Epoch   5, Step:    31300, Batch Loss:     2.575594, Tokens per Sec:    13890, Lr: 0.000300\n",
      "2021-05-25 07:02:06,611 - INFO - joeynmt.training - Epoch   5, Step:    31400, Batch Loss:     2.642166, Tokens per Sec:    14218, Lr: 0.000300\n",
      "2021-05-25 07:02:19,433 - INFO - joeynmt.training - Epoch   5, Step:    31500, Batch Loss:     2.670143, Tokens per Sec:    13891, Lr: 0.000300\n",
      "2021-05-25 07:02:32,609 - INFO - joeynmt.training - Epoch   5, Step:    31600, Batch Loss:     2.408669, Tokens per Sec:    14007, Lr: 0.000300\n",
      "2021-05-25 07:02:45,740 - INFO - joeynmt.training - Epoch   5, Step:    31700, Batch Loss:     2.779868, Tokens per Sec:    14228, Lr: 0.000300\n",
      "2021-05-25 07:02:58,921 - INFO - joeynmt.training - Epoch   5, Step:    31800, Batch Loss:     2.488946, Tokens per Sec:    14096, Lr: 0.000300\n",
      "2021-05-25 07:03:11,680 - INFO - joeynmt.training - Epoch   5, Step:    31900, Batch Loss:     2.393698, Tokens per Sec:    13569, Lr: 0.000300\n",
      "2021-05-25 07:03:25,079 - INFO - joeynmt.training - Epoch   5, Step:    32000, Batch Loss:     2.339305, Tokens per Sec:    14716, Lr: 0.000300\n",
      "2021-05-25 07:03:38,179 - INFO - joeynmt.training - Epoch   5, Step:    32100, Batch Loss:     2.413815, Tokens per Sec:    13935, Lr: 0.000300\n",
      "2021-05-25 07:03:51,381 - INFO - joeynmt.training - Epoch   5, Step:    32200, Batch Loss:     2.261710, Tokens per Sec:    14258, Lr: 0.000300\n",
      "2021-05-25 07:04:04,398 - INFO - joeynmt.training - Epoch   5, Step:    32300, Batch Loss:     2.348048, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 07:04:17,460 - INFO - joeynmt.training - Epoch   5, Step:    32400, Batch Loss:     2.615046, Tokens per Sec:    13694, Lr: 0.000300\n",
      "2021-05-25 07:04:18,628 - INFO - joeynmt.training - Epoch   5: total training loss 16217.60\n",
      "2021-05-25 07:04:18,628 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-05-25 07:04:31,336 - INFO - joeynmt.training - Epoch   6, Step:    32500, Batch Loss:     2.639158, Tokens per Sec:    13354, Lr: 0.000300\n",
      "2021-05-25 07:04:44,298 - INFO - joeynmt.training - Epoch   6, Step:    32600, Batch Loss:     2.536793, Tokens per Sec:    14370, Lr: 0.000300\n",
      "2021-05-25 07:04:57,035 - INFO - joeynmt.training - Epoch   6, Step:    32700, Batch Loss:     2.740018, Tokens per Sec:    13969, Lr: 0.000300\n",
      "2021-05-25 07:05:09,759 - INFO - joeynmt.training - Epoch   6, Step:    32800, Batch Loss:     2.419680, Tokens per Sec:    14060, Lr: 0.000300\n",
      "2021-05-25 07:05:22,859 - INFO - joeynmt.training - Epoch   6, Step:    32900, Batch Loss:     2.353083, Tokens per Sec:    14520, Lr: 0.000300\n",
      "2021-05-25 07:05:35,773 - INFO - joeynmt.training - Epoch   6, Step:    33000, Batch Loss:     2.448546, Tokens per Sec:    13996, Lr: 0.000300\n",
      "2021-05-25 07:05:48,829 - INFO - joeynmt.training - Epoch   6, Step:    33100, Batch Loss:     2.998217, Tokens per Sec:    13944, Lr: 0.000300\n",
      "2021-05-25 07:06:01,948 - INFO - joeynmt.training - Epoch   6, Step:    33200, Batch Loss:     2.483268, Tokens per Sec:    14216, Lr: 0.000300\n",
      "2021-05-25 07:06:15,095 - INFO - joeynmt.training - Epoch   6, Step:    33300, Batch Loss:     2.463988, Tokens per Sec:    14207, Lr: 0.000300\n",
      "2021-05-25 07:06:28,171 - INFO - joeynmt.training - Epoch   6, Step:    33400, Batch Loss:     2.456242, Tokens per Sec:    14313, Lr: 0.000300\n",
      "2021-05-25 07:06:41,157 - INFO - joeynmt.training - Epoch   6, Step:    33500, Batch Loss:     2.514159, Tokens per Sec:    13925, Lr: 0.000300\n",
      "2021-05-25 07:06:54,219 - INFO - joeynmt.training - Epoch   6, Step:    33600, Batch Loss:     2.271440, Tokens per Sec:    14092, Lr: 0.000300\n",
      "2021-05-25 07:07:07,426 - INFO - joeynmt.training - Epoch   6, Step:    33700, Batch Loss:     2.413670, Tokens per Sec:    13916, Lr: 0.000300\n",
      "2021-05-25 07:07:20,530 - INFO - joeynmt.training - Epoch   6, Step:    33800, Batch Loss:     2.386677, Tokens per Sec:    14249, Lr: 0.000300\n",
      "2021-05-25 07:07:33,734 - INFO - joeynmt.training - Epoch   6, Step:    33900, Batch Loss:     2.491597, Tokens per Sec:    13978, Lr: 0.000300\n",
      "2021-05-25 07:07:46,788 - INFO - joeynmt.training - Epoch   6, Step:    34000, Batch Loss:     2.312230, Tokens per Sec:    14100, Lr: 0.000300\n",
      "2021-05-25 07:08:00,040 - INFO - joeynmt.training - Epoch   6, Step:    34100, Batch Loss:     2.393743, Tokens per Sec:    14496, Lr: 0.000300\n",
      "2021-05-25 07:08:13,048 - INFO - joeynmt.training - Epoch   6, Step:    34200, Batch Loss:     2.454241, Tokens per Sec:    13923, Lr: 0.000300\n",
      "2021-05-25 07:08:26,126 - INFO - joeynmt.training - Epoch   6, Step:    34300, Batch Loss:     2.491171, Tokens per Sec:    14358, Lr: 0.000300\n",
      "2021-05-25 07:08:39,027 - INFO - joeynmt.training - Epoch   6, Step:    34400, Batch Loss:     2.464855, Tokens per Sec:    14202, Lr: 0.000300\n",
      "2021-05-25 07:08:52,144 - INFO - joeynmt.training - Epoch   6, Step:    34500, Batch Loss:     2.456215, Tokens per Sec:    14537, Lr: 0.000300\n",
      "2021-05-25 07:09:04,903 - INFO - joeynmt.training - Epoch   6, Step:    34600, Batch Loss:     2.601147, Tokens per Sec:    14225, Lr: 0.000300\n",
      "2021-05-25 07:09:17,524 - INFO - joeynmt.training - Epoch   6, Step:    34700, Batch Loss:     2.567005, Tokens per Sec:    14075, Lr: 0.000300\n",
      "2021-05-25 07:09:30,333 - INFO - joeynmt.training - Epoch   6, Step:    34800, Batch Loss:     2.393293, Tokens per Sec:    14515, Lr: 0.000300\n",
      "2021-05-25 07:09:43,208 - INFO - joeynmt.training - Epoch   6, Step:    34900, Batch Loss:     2.275969, Tokens per Sec:    14136, Lr: 0.000300\n",
      "2021-05-25 07:09:56,138 - INFO - joeynmt.training - Epoch   6, Step:    35000, Batch Loss:     2.906983, Tokens per Sec:    14456, Lr: 0.000300\n",
      "2021-05-25 07:10:20,391 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 07:10:20,391 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 07:10:20,391 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 07:10:20,655 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 07:10:20,655 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 07:10:21,411 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 07:10:21,412 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 07:10:21,412 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 07:10:21,412 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 07:10:21,412 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 07:10:21,413 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 07:10:21,413 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 07:10:21,414 - INFO - joeynmt.training - \tHypothesis: The letter was written in the mountains of the resurrection .\n",
      "2021-05-25 07:10:21,414 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 07:10:21,414 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 07:10:21,415 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:10:21,415 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or discipline , we should continue to remain in the faith of God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:10:21,415 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 07:10:21,415 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 07:10:21,415 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 07:10:21,416 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a certain example that they were reached in Satan’s world .\n",
      "2021-05-25 07:10:21,416 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    35000: bleu:  18.38, loss: 57419.5898, ppl:   7.9024, duration: 25.2771s\n",
      "2021-05-25 07:10:34,722 - INFO - joeynmt.training - Epoch   6, Step:    35100, Batch Loss:     2.530663, Tokens per Sec:    13477, Lr: 0.000300\n",
      "2021-05-25 07:10:47,838 - INFO - joeynmt.training - Epoch   6, Step:    35200, Batch Loss:     2.510760, Tokens per Sec:    14062, Lr: 0.000300\n",
      "2021-05-25 07:11:00,998 - INFO - joeynmt.training - Epoch   6, Step:    35300, Batch Loss:     2.350367, Tokens per Sec:    13931, Lr: 0.000300\n",
      "2021-05-25 07:11:14,131 - INFO - joeynmt.training - Epoch   6, Step:    35400, Batch Loss:     2.373609, Tokens per Sec:    14106, Lr: 0.000300\n",
      "2021-05-25 07:11:27,297 - INFO - joeynmt.training - Epoch   6, Step:    35500, Batch Loss:     2.384872, Tokens per Sec:    14185, Lr: 0.000300\n",
      "2021-05-25 07:11:40,422 - INFO - joeynmt.training - Epoch   6, Step:    35600, Batch Loss:     2.404760, Tokens per Sec:    14435, Lr: 0.000300\n",
      "2021-05-25 07:11:53,624 - INFO - joeynmt.training - Epoch   6, Step:    35700, Batch Loss:     2.339620, Tokens per Sec:    14064, Lr: 0.000300\n",
      "2021-05-25 07:12:06,615 - INFO - joeynmt.training - Epoch   6, Step:    35800, Batch Loss:     2.469649, Tokens per Sec:    13977, Lr: 0.000300\n",
      "2021-05-25 07:12:19,807 - INFO - joeynmt.training - Epoch   6, Step:    35900, Batch Loss:     2.429606, Tokens per Sec:    14195, Lr: 0.000300\n",
      "2021-05-25 07:12:32,842 - INFO - joeynmt.training - Epoch   6, Step:    36000, Batch Loss:     2.308968, Tokens per Sec:    13973, Lr: 0.000300\n",
      "2021-05-25 07:12:45,890 - INFO - joeynmt.training - Epoch   6, Step:    36100, Batch Loss:     2.566332, Tokens per Sec:    13959, Lr: 0.000300\n",
      "2021-05-25 07:12:58,970 - INFO - joeynmt.training - Epoch   6, Step:    36200, Batch Loss:     2.329622, Tokens per Sec:    14159, Lr: 0.000300\n",
      "2021-05-25 07:13:12,037 - INFO - joeynmt.training - Epoch   6, Step:    36300, Batch Loss:     2.533022, Tokens per Sec:    13853, Lr: 0.000300\n",
      "2021-05-25 07:13:25,111 - INFO - joeynmt.training - Epoch   6, Step:    36400, Batch Loss:     2.346756, Tokens per Sec:    13944, Lr: 0.000300\n",
      "2021-05-25 07:13:38,251 - INFO - joeynmt.training - Epoch   6, Step:    36500, Batch Loss:     2.308684, Tokens per Sec:    14151, Lr: 0.000300\n",
      "2021-05-25 07:13:51,097 - INFO - joeynmt.training - Epoch   6, Step:    36600, Batch Loss:     2.373098, Tokens per Sec:    13717, Lr: 0.000300\n",
      "2021-05-25 07:14:04,018 - INFO - joeynmt.training - Epoch   6, Step:    36700, Batch Loss:     2.200296, Tokens per Sec:    13572, Lr: 0.000300\n",
      "2021-05-25 07:14:17,260 - INFO - joeynmt.training - Epoch   6, Step:    36800, Batch Loss:     2.322339, Tokens per Sec:    14118, Lr: 0.000300\n",
      "2021-05-25 07:14:30,373 - INFO - joeynmt.training - Epoch   6, Step:    36900, Batch Loss:     2.379238, Tokens per Sec:    14380, Lr: 0.000300\n",
      "2021-05-25 07:14:43,300 - INFO - joeynmt.training - Epoch   6, Step:    37000, Batch Loss:     2.350853, Tokens per Sec:    13580, Lr: 0.000300\n",
      "2021-05-25 07:14:56,636 - INFO - joeynmt.training - Epoch   6, Step:    37100, Batch Loss:     2.477581, Tokens per Sec:    14368, Lr: 0.000300\n",
      "2021-05-25 07:15:09,628 - INFO - joeynmt.training - Epoch   6, Step:    37200, Batch Loss:     2.352669, Tokens per Sec:    13937, Lr: 0.000300\n",
      "2021-05-25 07:15:22,857 - INFO - joeynmt.training - Epoch   6, Step:    37300, Batch Loss:     2.195807, Tokens per Sec:    14118, Lr: 0.000300\n",
      "2021-05-25 07:15:35,854 - INFO - joeynmt.training - Epoch   6, Step:    37400, Batch Loss:     2.366921, Tokens per Sec:    14056, Lr: 0.000300\n",
      "2021-05-25 07:15:48,912 - INFO - joeynmt.training - Epoch   6, Step:    37500, Batch Loss:     2.440446, Tokens per Sec:    13917, Lr: 0.000300\n",
      "2021-05-25 07:16:01,908 - INFO - joeynmt.training - Epoch   6, Step:    37600, Batch Loss:     2.999054, Tokens per Sec:    14178, Lr: 0.000300\n",
      "2021-05-25 07:16:14,981 - INFO - joeynmt.training - Epoch   6, Step:    37700, Batch Loss:     2.532259, Tokens per Sec:    14032, Lr: 0.000300\n",
      "2021-05-25 07:16:28,084 - INFO - joeynmt.training - Epoch   6, Step:    37800, Batch Loss:     2.367490, Tokens per Sec:    14204, Lr: 0.000300\n",
      "2021-05-25 07:16:41,095 - INFO - joeynmt.training - Epoch   6, Step:    37900, Batch Loss:     2.423385, Tokens per Sec:    14230, Lr: 0.000300\n",
      "2021-05-25 07:16:54,007 - INFO - joeynmt.training - Epoch   6, Step:    38000, Batch Loss:     2.431249, Tokens per Sec:    14233, Lr: 0.000300\n",
      "2021-05-25 07:17:07,024 - INFO - joeynmt.training - Epoch   6, Step:    38100, Batch Loss:     2.142650, Tokens per Sec:    13952, Lr: 0.000300\n",
      "2021-05-25 07:17:19,865 - INFO - joeynmt.training - Epoch   6, Step:    38200, Batch Loss:     2.446030, Tokens per Sec:    14078, Lr: 0.000300\n",
      "2021-05-25 07:17:32,820 - INFO - joeynmt.training - Epoch   6, Step:    38300, Batch Loss:     2.450495, Tokens per Sec:    14847, Lr: 0.000300\n",
      "2021-05-25 07:17:45,654 - INFO - joeynmt.training - Epoch   6, Step:    38400, Batch Loss:     2.554736, Tokens per Sec:    14152, Lr: 0.000300\n",
      "2021-05-25 07:17:58,563 - INFO - joeynmt.training - Epoch   6, Step:    38500, Batch Loss:     2.538268, Tokens per Sec:    14323, Lr: 0.000300\n",
      "2021-05-25 07:18:11,720 - INFO - joeynmt.training - Epoch   6, Step:    38600, Batch Loss:     2.459707, Tokens per Sec:    14343, Lr: 0.000300\n",
      "2021-05-25 07:18:24,848 - INFO - joeynmt.training - Epoch   6, Step:    38700, Batch Loss:     2.310893, Tokens per Sec:    13927, Lr: 0.000300\n",
      "2021-05-25 07:18:37,886 - INFO - joeynmt.training - Epoch   6, Step:    38800, Batch Loss:     2.479277, Tokens per Sec:    14217, Lr: 0.000300\n",
      "2021-05-25 07:18:48,567 - INFO - joeynmt.training - Epoch   6: total training loss 15673.16\n",
      "2021-05-25 07:18:48,567 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-05-25 07:18:51,704 - INFO - joeynmt.training - Epoch   7, Step:    38900, Batch Loss:     2.357601, Tokens per Sec:    10976, Lr: 0.000300\n",
      "2021-05-25 07:19:04,681 - INFO - joeynmt.training - Epoch   7, Step:    39000, Batch Loss:     2.222605, Tokens per Sec:    14192, Lr: 0.000300\n",
      "2021-05-25 07:19:17,941 - INFO - joeynmt.training - Epoch   7, Step:    39100, Batch Loss:     2.145369, Tokens per Sec:    14209, Lr: 0.000300\n",
      "2021-05-25 07:19:31,050 - INFO - joeynmt.training - Epoch   7, Step:    39200, Batch Loss:     2.444203, Tokens per Sec:    14066, Lr: 0.000300\n",
      "2021-05-25 07:19:44,014 - INFO - joeynmt.training - Epoch   7, Step:    39300, Batch Loss:     2.376622, Tokens per Sec:    13835, Lr: 0.000300\n",
      "2021-05-25 07:19:57,085 - INFO - joeynmt.training - Epoch   7, Step:    39400, Batch Loss:     2.252409, Tokens per Sec:    14092, Lr: 0.000300\n",
      "2021-05-25 07:20:10,123 - INFO - joeynmt.training - Epoch   7, Step:    39500, Batch Loss:     2.316382, Tokens per Sec:    14114, Lr: 0.000300\n",
      "2021-05-25 07:20:23,252 - INFO - joeynmt.training - Epoch   7, Step:    39600, Batch Loss:     2.281230, Tokens per Sec:    14065, Lr: 0.000300\n",
      "2021-05-25 07:20:36,372 - INFO - joeynmt.training - Epoch   7, Step:    39700, Batch Loss:     2.450860, Tokens per Sec:    14081, Lr: 0.000300\n",
      "2021-05-25 07:20:49,558 - INFO - joeynmt.training - Epoch   7, Step:    39800, Batch Loss:     2.364983, Tokens per Sec:    14054, Lr: 0.000300\n",
      "2021-05-25 07:21:02,618 - INFO - joeynmt.training - Epoch   7, Step:    39900, Batch Loss:     2.441970, Tokens per Sec:    14116, Lr: 0.000300\n",
      "2021-05-25 07:21:15,801 - INFO - joeynmt.training - Epoch   7, Step:    40000, Batch Loss:     2.562247, Tokens per Sec:    13948, Lr: 0.000300\n",
      "2021-05-25 07:21:37,696 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 07:21:37,696 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 07:21:37,696 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 07:21:37,939 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 07:21:37,939 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 07:21:38,957 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 07:21:38,958 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 07:21:38,958 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 07:21:38,958 - INFO - joeynmt.training - \tHypothesis: I was very deeply moved .\n",
      "2021-05-25 07:21:38,958 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 07:21:38,959 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 07:21:38,959 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 07:21:38,959 - INFO - joeynmt.training - \tHypothesis: The writer wrote in the street on the side of the scroll .\n",
      "2021-05-25 07:21:38,959 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 07:21:38,959 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 07:21:38,960 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:21:38,960 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or disobedient , we should continue to remain in our faith in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:21:38,960 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 07:21:38,960 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 07:21:38,960 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 07:21:38,960 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show , in some ways , they have been made in Satan’s world .\n",
      "2021-05-25 07:21:38,961 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    40000: bleu:  19.19, loss: 55984.4531, ppl:   7.5045, duration: 23.1590s\n",
      "2021-05-25 07:21:52,030 - INFO - joeynmt.training - Epoch   7, Step:    40100, Batch Loss:     2.555536, Tokens per Sec:    14249, Lr: 0.000300\n",
      "2021-05-25 07:22:04,932 - INFO - joeynmt.training - Epoch   7, Step:    40200, Batch Loss:     2.362926, Tokens per Sec:    14371, Lr: 0.000300\n",
      "2021-05-25 07:22:18,085 - INFO - joeynmt.training - Epoch   7, Step:    40300, Batch Loss:     2.357167, Tokens per Sec:    14470, Lr: 0.000300\n",
      "2021-05-25 07:22:31,373 - INFO - joeynmt.training - Epoch   7, Step:    40400, Batch Loss:     2.284067, Tokens per Sec:    14517, Lr: 0.000300\n",
      "2021-05-25 07:22:44,406 - INFO - joeynmt.training - Epoch   7, Step:    40500, Batch Loss:     2.463778, Tokens per Sec:    13837, Lr: 0.000300\n",
      "2021-05-25 07:22:57,391 - INFO - joeynmt.training - Epoch   7, Step:    40600, Batch Loss:     2.307695, Tokens per Sec:    13743, Lr: 0.000300\n",
      "2021-05-25 07:23:10,574 - INFO - joeynmt.training - Epoch   7, Step:    40700, Batch Loss:     2.387051, Tokens per Sec:    14431, Lr: 0.000300\n",
      "2021-05-25 07:23:23,660 - INFO - joeynmt.training - Epoch   7, Step:    40800, Batch Loss:     2.227661, Tokens per Sec:    13811, Lr: 0.000300\n",
      "2021-05-25 07:23:36,815 - INFO - joeynmt.training - Epoch   7, Step:    40900, Batch Loss:     2.352859, Tokens per Sec:    14149, Lr: 0.000300\n",
      "2021-05-25 07:23:49,830 - INFO - joeynmt.training - Epoch   7, Step:    41000, Batch Loss:     2.283352, Tokens per Sec:    13988, Lr: 0.000300\n",
      "2021-05-25 07:24:03,023 - INFO - joeynmt.training - Epoch   7, Step:    41100, Batch Loss:     2.248204, Tokens per Sec:    14190, Lr: 0.000300\n",
      "2021-05-25 07:24:16,170 - INFO - joeynmt.training - Epoch   7, Step:    41200, Batch Loss:     2.419930, Tokens per Sec:    13991, Lr: 0.000300\n",
      "2021-05-25 07:24:29,142 - INFO - joeynmt.training - Epoch   7, Step:    41300, Batch Loss:     2.419779, Tokens per Sec:    14000, Lr: 0.000300\n",
      "2021-05-25 07:24:42,282 - INFO - joeynmt.training - Epoch   7, Step:    41400, Batch Loss:     2.376237, Tokens per Sec:    14052, Lr: 0.000300\n",
      "2021-05-25 07:24:55,164 - INFO - joeynmt.training - Epoch   7, Step:    41500, Batch Loss:     2.508125, Tokens per Sec:    13858, Lr: 0.000300\n",
      "2021-05-25 07:25:08,219 - INFO - joeynmt.training - Epoch   7, Step:    41600, Batch Loss:     2.269996, Tokens per Sec:    14074, Lr: 0.000300\n",
      "2021-05-25 07:25:21,336 - INFO - joeynmt.training - Epoch   7, Step:    41700, Batch Loss:     2.162754, Tokens per Sec:    14171, Lr: 0.000300\n",
      "2021-05-25 07:25:34,231 - INFO - joeynmt.training - Epoch   7, Step:    41800, Batch Loss:     2.308172, Tokens per Sec:    14139, Lr: 0.000300\n",
      "2021-05-25 07:25:47,005 - INFO - joeynmt.training - Epoch   7, Step:    41900, Batch Loss:     2.210654, Tokens per Sec:    14356, Lr: 0.000300\n",
      "2021-05-25 07:25:59,722 - INFO - joeynmt.training - Epoch   7, Step:    42000, Batch Loss:     2.387983, Tokens per Sec:    13972, Lr: 0.000300\n",
      "2021-05-25 07:26:12,512 - INFO - joeynmt.training - Epoch   7, Step:    42100, Batch Loss:     2.274573, Tokens per Sec:    14232, Lr: 0.000300\n",
      "2021-05-25 07:26:25,396 - INFO - joeynmt.training - Epoch   7, Step:    42200, Batch Loss:     2.354085, Tokens per Sec:    14189, Lr: 0.000300\n",
      "2021-05-25 07:26:38,549 - INFO - joeynmt.training - Epoch   7, Step:    42300, Batch Loss:     2.338616, Tokens per Sec:    14467, Lr: 0.000300\n",
      "2021-05-25 07:26:51,778 - INFO - joeynmt.training - Epoch   7, Step:    42400, Batch Loss:     2.419659, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 07:27:04,869 - INFO - joeynmt.training - Epoch   7, Step:    42500, Batch Loss:     2.231865, Tokens per Sec:    13948, Lr: 0.000300\n",
      "2021-05-25 07:27:18,218 - INFO - joeynmt.training - Epoch   7, Step:    42600, Batch Loss:     2.199882, Tokens per Sec:    14363, Lr: 0.000300\n",
      "2021-05-25 07:27:31,206 - INFO - joeynmt.training - Epoch   7, Step:    42700, Batch Loss:     2.414727, Tokens per Sec:    13959, Lr: 0.000300\n",
      "2021-05-25 07:27:44,271 - INFO - joeynmt.training - Epoch   7, Step:    42800, Batch Loss:     2.486791, Tokens per Sec:    13706, Lr: 0.000300\n",
      "2021-05-25 07:27:57,233 - INFO - joeynmt.training - Epoch   7, Step:    42900, Batch Loss:     2.479017, Tokens per Sec:    13760, Lr: 0.000300\n",
      "2021-05-25 07:28:10,349 - INFO - joeynmt.training - Epoch   7, Step:    43000, Batch Loss:     2.253192, Tokens per Sec:    14145, Lr: 0.000300\n",
      "2021-05-25 07:28:23,491 - INFO - joeynmt.training - Epoch   7, Step:    43100, Batch Loss:     2.364828, Tokens per Sec:    14119, Lr: 0.000300\n",
      "2021-05-25 07:28:36,482 - INFO - joeynmt.training - Epoch   7, Step:    43200, Batch Loss:     2.085559, Tokens per Sec:    13811, Lr: 0.000300\n",
      "2021-05-25 07:28:49,499 - INFO - joeynmt.training - Epoch   7, Step:    43300, Batch Loss:     2.378291, Tokens per Sec:    14173, Lr: 0.000300\n",
      "2021-05-25 07:29:02,666 - INFO - joeynmt.training - Epoch   7, Step:    43400, Batch Loss:     2.414217, Tokens per Sec:    14302, Lr: 0.000300\n",
      "2021-05-25 07:29:15,893 - INFO - joeynmt.training - Epoch   7, Step:    43500, Batch Loss:     2.339876, Tokens per Sec:    14343, Lr: 0.000300\n",
      "2021-05-25 07:29:29,116 - INFO - joeynmt.training - Epoch   7, Step:    43600, Batch Loss:     2.446075, Tokens per Sec:    14043, Lr: 0.000300\n",
      "2021-05-25 07:29:42,323 - INFO - joeynmt.training - Epoch   7, Step:    43700, Batch Loss:     2.426355, Tokens per Sec:    13767, Lr: 0.000300\n",
      "2021-05-25 07:29:55,365 - INFO - joeynmt.training - Epoch   7, Step:    43800, Batch Loss:     2.330807, Tokens per Sec:    13995, Lr: 0.000300\n",
      "2021-05-25 07:30:08,494 - INFO - joeynmt.training - Epoch   7, Step:    43900, Batch Loss:     2.280424, Tokens per Sec:    14269, Lr: 0.000300\n",
      "2021-05-25 07:30:21,392 - INFO - joeynmt.training - Epoch   7, Step:    44000, Batch Loss:     2.418210, Tokens per Sec:    14018, Lr: 0.000300\n",
      "2021-05-25 07:30:34,599 - INFO - joeynmt.training - Epoch   7, Step:    44100, Batch Loss:     2.272779, Tokens per Sec:    14732, Lr: 0.000300\n",
      "2021-05-25 07:30:47,411 - INFO - joeynmt.training - Epoch   7, Step:    44200, Batch Loss:     2.373399, Tokens per Sec:    14281, Lr: 0.000300\n",
      "2021-05-25 07:31:00,309 - INFO - joeynmt.training - Epoch   7, Step:    44300, Batch Loss:     2.340128, Tokens per Sec:    14192, Lr: 0.000300\n",
      "2021-05-25 07:31:13,312 - INFO - joeynmt.training - Epoch   7, Step:    44400, Batch Loss:     2.317554, Tokens per Sec:    14315, Lr: 0.000300\n",
      "2021-05-25 07:31:26,153 - INFO - joeynmt.training - Epoch   7, Step:    44500, Batch Loss:     2.420501, Tokens per Sec:    14368, Lr: 0.000300\n",
      "2021-05-25 07:31:39,119 - INFO - joeynmt.training - Epoch   7, Step:    44600, Batch Loss:     2.587885, Tokens per Sec:    14748, Lr: 0.000300\n",
      "2021-05-25 07:31:51,727 - INFO - joeynmt.training - Epoch   7, Step:    44700, Batch Loss:     2.184152, Tokens per Sec:    13970, Lr: 0.000300\n",
      "2021-05-25 07:32:04,555 - INFO - joeynmt.training - Epoch   7, Step:    44800, Batch Loss:     2.335339, Tokens per Sec:    13642, Lr: 0.000300\n",
      "2021-05-25 07:32:17,561 - INFO - joeynmt.training - Epoch   7, Step:    44900, Batch Loss:     2.290776, Tokens per Sec:    14564, Lr: 0.000300\n",
      "2021-05-25 07:32:30,750 - INFO - joeynmt.training - Epoch   7, Step:    45000, Batch Loss:     2.316489, Tokens per Sec:    14134, Lr: 0.000300\n",
      "2021-05-25 07:32:52,389 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 07:32:52,389 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 07:32:52,390 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 07:32:52,664 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 07:32:52,665 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 07:32:53,449 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 07:32:53,449 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 07:32:53,449 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 07:32:53,450 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 07:32:53,450 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 07:32:53,450 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 07:32:53,450 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 07:32:53,451 - INFO - joeynmt.training - \tHypothesis: The text was written in the street of the scroll .\n",
      "2021-05-25 07:32:53,451 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 07:32:53,451 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 07:32:53,451 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:32:53,452 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or see , we should continue to strengthen our faith in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:32:53,452 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 07:32:53,452 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 07:32:53,452 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 07:32:53,453 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a certain example that they have been made in Satan’s world .\n",
      "2021-05-25 07:32:53,453 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    45000: bleu:  20.20, loss: 54509.9609, ppl:   7.1165, duration: 22.7020s\n",
      "2021-05-25 07:33:06,757 - INFO - joeynmt.training - Epoch   7, Step:    45100, Batch Loss:     2.501921, Tokens per Sec:    13789, Lr: 0.000300\n",
      "2021-05-25 07:33:19,793 - INFO - joeynmt.training - Epoch   7, Step:    45200, Batch Loss:     2.473238, Tokens per Sec:    13737, Lr: 0.000300\n",
      "2021-05-25 07:33:33,075 - INFO - joeynmt.training - Epoch   7, Step:    45300, Batch Loss:     2.377391, Tokens per Sec:    14445, Lr: 0.000300\n",
      "2021-05-25 07:33:39,487 - INFO - joeynmt.training - Epoch   7: total training loss 15270.81\n",
      "2021-05-25 07:33:39,488 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-05-25 07:33:46,931 - INFO - joeynmt.training - Epoch   8, Step:    45400, Batch Loss:     2.386398, Tokens per Sec:    12919, Lr: 0.000300\n",
      "2021-05-25 07:33:59,772 - INFO - joeynmt.training - Epoch   8, Step:    45500, Batch Loss:     2.864340, Tokens per Sec:    13704, Lr: 0.000300\n",
      "2021-05-25 07:34:12,926 - INFO - joeynmt.training - Epoch   8, Step:    45600, Batch Loss:     2.184417, Tokens per Sec:    14367, Lr: 0.000300\n",
      "2021-05-25 07:34:25,814 - INFO - joeynmt.training - Epoch   8, Step:    45700, Batch Loss:     2.282769, Tokens per Sec:    13905, Lr: 0.000300\n",
      "2021-05-25 07:34:39,139 - INFO - joeynmt.training - Epoch   8, Step:    45800, Batch Loss:     2.399110, Tokens per Sec:    14151, Lr: 0.000300\n",
      "2021-05-25 07:34:52,124 - INFO - joeynmt.training - Epoch   8, Step:    45900, Batch Loss:     2.386343, Tokens per Sec:    13978, Lr: 0.000300\n",
      "2021-05-25 07:35:05,100 - INFO - joeynmt.training - Epoch   8, Step:    46000, Batch Loss:     2.194714, Tokens per Sec:    14024, Lr: 0.000300\n",
      "2021-05-25 07:35:18,107 - INFO - joeynmt.training - Epoch   8, Step:    46100, Batch Loss:     2.285826, Tokens per Sec:    13614, Lr: 0.000300\n",
      "2021-05-25 07:35:30,995 - INFO - joeynmt.training - Epoch   8, Step:    46200, Batch Loss:     2.331985, Tokens per Sec:    14321, Lr: 0.000300\n",
      "2021-05-25 07:35:43,740 - INFO - joeynmt.training - Epoch   8, Step:    46300, Batch Loss:     2.412683, Tokens per Sec:    14594, Lr: 0.000300\n",
      "2021-05-25 07:35:56,674 - INFO - joeynmt.training - Epoch   8, Step:    46400, Batch Loss:     2.565715, Tokens per Sec:    14298, Lr: 0.000300\n",
      "2021-05-25 07:36:09,591 - INFO - joeynmt.training - Epoch   8, Step:    46500, Batch Loss:     2.341027, Tokens per Sec:    14065, Lr: 0.000300\n",
      "2021-05-25 07:36:22,670 - INFO - joeynmt.training - Epoch   8, Step:    46600, Batch Loss:     2.294875, Tokens per Sec:    14177, Lr: 0.000300\n",
      "2021-05-25 07:36:35,814 - INFO - joeynmt.training - Epoch   8, Step:    46700, Batch Loss:     2.139290, Tokens per Sec:    14257, Lr: 0.000300\n",
      "2021-05-25 07:36:48,929 - INFO - joeynmt.training - Epoch   8, Step:    46800, Batch Loss:     2.411702, Tokens per Sec:    13995, Lr: 0.000300\n",
      "2021-05-25 07:37:02,003 - INFO - joeynmt.training - Epoch   8, Step:    46900, Batch Loss:     2.322384, Tokens per Sec:    14052, Lr: 0.000300\n",
      "2021-05-25 07:37:15,024 - INFO - joeynmt.training - Epoch   8, Step:    47000, Batch Loss:     2.397606, Tokens per Sec:    13842, Lr: 0.000300\n",
      "2021-05-25 07:37:28,221 - INFO - joeynmt.training - Epoch   8, Step:    47100, Batch Loss:     2.241140, Tokens per Sec:    14328, Lr: 0.000300\n",
      "2021-05-25 07:37:41,390 - INFO - joeynmt.training - Epoch   8, Step:    47200, Batch Loss:     2.259059, Tokens per Sec:    14333, Lr: 0.000300\n",
      "2021-05-25 07:37:54,549 - INFO - joeynmt.training - Epoch   8, Step:    47300, Batch Loss:     2.404147, Tokens per Sec:    14476, Lr: 0.000300\n",
      "2021-05-25 07:38:07,611 - INFO - joeynmt.training - Epoch   8, Step:    47400, Batch Loss:     2.352530, Tokens per Sec:    13679, Lr: 0.000300\n",
      "2021-05-25 07:38:20,836 - INFO - joeynmt.training - Epoch   8, Step:    47500, Batch Loss:     2.436175, Tokens per Sec:    14022, Lr: 0.000300\n",
      "2021-05-25 07:38:34,059 - INFO - joeynmt.training - Epoch   8, Step:    47600, Batch Loss:     2.351788, Tokens per Sec:    14193, Lr: 0.000300\n",
      "2021-05-25 07:38:47,174 - INFO - joeynmt.training - Epoch   8, Step:    47700, Batch Loss:     2.394756, Tokens per Sec:    13793, Lr: 0.000300\n",
      "2021-05-25 07:39:00,180 - INFO - joeynmt.training - Epoch   8, Step:    47800, Batch Loss:     2.344659, Tokens per Sec:    13914, Lr: 0.000300\n",
      "2021-05-25 07:39:13,375 - INFO - joeynmt.training - Epoch   8, Step:    47900, Batch Loss:     2.238173, Tokens per Sec:    14090, Lr: 0.000300\n",
      "2021-05-25 07:39:26,420 - INFO - joeynmt.training - Epoch   8, Step:    48000, Batch Loss:     2.372782, Tokens per Sec:    13930, Lr: 0.000300\n",
      "2021-05-25 07:39:39,533 - INFO - joeynmt.training - Epoch   8, Step:    48100, Batch Loss:     2.296290, Tokens per Sec:    14513, Lr: 0.000300\n",
      "2021-05-25 07:39:52,391 - INFO - joeynmt.training - Epoch   8, Step:    48200, Batch Loss:     2.576729, Tokens per Sec:    14408, Lr: 0.000300\n",
      "2021-05-25 07:40:05,373 - INFO - joeynmt.training - Epoch   8, Step:    48300, Batch Loss:     2.501489, Tokens per Sec:    14658, Lr: 0.000300\n",
      "2021-05-25 07:40:18,228 - INFO - joeynmt.training - Epoch   8, Step:    48400, Batch Loss:     2.349929, Tokens per Sec:    14037, Lr: 0.000300\n",
      "2021-05-25 07:40:31,271 - INFO - joeynmt.training - Epoch   8, Step:    48500, Batch Loss:     2.295290, Tokens per Sec:    13812, Lr: 0.000300\n",
      "2021-05-25 07:40:44,518 - INFO - joeynmt.training - Epoch   8, Step:    48600, Batch Loss:     2.301365, Tokens per Sec:    14347, Lr: 0.000300\n",
      "2021-05-25 07:40:57,644 - INFO - joeynmt.training - Epoch   8, Step:    48700, Batch Loss:     2.311924, Tokens per Sec:    14009, Lr: 0.000300\n",
      "2021-05-25 07:41:10,784 - INFO - joeynmt.training - Epoch   8, Step:    48800, Batch Loss:     2.446977, Tokens per Sec:    14187, Lr: 0.000300\n",
      "2021-05-25 07:41:23,810 - INFO - joeynmt.training - Epoch   8, Step:    48900, Batch Loss:     2.352448, Tokens per Sec:    14252, Lr: 0.000300\n",
      "2021-05-25 07:41:36,828 - INFO - joeynmt.training - Epoch   8, Step:    49000, Batch Loss:     2.318976, Tokens per Sec:    14195, Lr: 0.000300\n",
      "2021-05-25 07:41:49,794 - INFO - joeynmt.training - Epoch   8, Step:    49100, Batch Loss:     2.259253, Tokens per Sec:    13944, Lr: 0.000300\n",
      "2021-05-25 07:42:02,771 - INFO - joeynmt.training - Epoch   8, Step:    49200, Batch Loss:     2.378621, Tokens per Sec:    14056, Lr: 0.000300\n",
      "2021-05-25 07:42:15,952 - INFO - joeynmt.training - Epoch   8, Step:    49300, Batch Loss:     2.324324, Tokens per Sec:    13960, Lr: 0.000300\n",
      "2021-05-25 07:42:29,216 - INFO - joeynmt.training - Epoch   8, Step:    49400, Batch Loss:     2.242370, Tokens per Sec:    14455, Lr: 0.000300\n",
      "2021-05-25 07:42:41,809 - INFO - joeynmt.training - Epoch   8, Step:    49500, Batch Loss:     2.330153, Tokens per Sec:    13997, Lr: 0.000300\n",
      "2021-05-25 07:42:54,696 - INFO - joeynmt.training - Epoch   8, Step:    49600, Batch Loss:     2.341976, Tokens per Sec:    14408, Lr: 0.000300\n",
      "2021-05-25 07:43:07,413 - INFO - joeynmt.training - Epoch   8, Step:    49700, Batch Loss:     2.400590, Tokens per Sec:    14175, Lr: 0.000300\n",
      "2021-05-25 07:43:20,293 - INFO - joeynmt.training - Epoch   8, Step:    49800, Batch Loss:     2.094356, Tokens per Sec:    14364, Lr: 0.000300\n",
      "2021-05-25 07:43:33,394 - INFO - joeynmt.training - Epoch   8, Step:    49900, Batch Loss:     2.213750, Tokens per Sec:    14073, Lr: 0.000300\n",
      "2021-05-25 07:43:46,388 - INFO - joeynmt.training - Epoch   8, Step:    50000, Batch Loss:     2.395946, Tokens per Sec:    13973, Lr: 0.000300\n",
      "2021-05-25 07:44:11,133 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 07:44:11,134 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 07:44:11,134 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 07:44:11,403 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 07:44:11,404 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 07:44:12,144 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 07:44:12,145 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 07:44:12,145 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 07:44:12,146 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 07:44:12,146 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 07:44:12,146 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 07:44:12,146 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 07:44:12,147 - INFO - joeynmt.training - \tHypothesis: The letter wrote in the street of the scroll .\n",
      "2021-05-25 07:44:12,147 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 07:44:12,147 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 07:44:12,147 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:44:12,148 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or see , we should continue to strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:44:12,148 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 07:44:12,148 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 07:44:12,148 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 07:44:12,149 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show , in some ways , that they were successful in Satan’s world .\n",
      "2021-05-25 07:44:12,149 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    50000: bleu:  20.37, loss: 53734.0078, ppl:   6.9204, duration: 25.7603s\n",
      "2021-05-25 07:44:25,413 - INFO - joeynmt.training - Epoch   8, Step:    50100, Batch Loss:     2.468529, Tokens per Sec:    13740, Lr: 0.000300\n",
      "2021-05-25 07:44:38,558 - INFO - joeynmt.training - Epoch   8, Step:    50200, Batch Loss:     2.284818, Tokens per Sec:    13827, Lr: 0.000300\n",
      "2021-05-25 07:44:51,598 - INFO - joeynmt.training - Epoch   8, Step:    50300, Batch Loss:     2.293643, Tokens per Sec:    14212, Lr: 0.000300\n",
      "2021-05-25 07:45:04,620 - INFO - joeynmt.training - Epoch   8, Step:    50400, Batch Loss:     2.336528, Tokens per Sec:    13892, Lr: 0.000300\n",
      "2021-05-25 07:45:17,654 - INFO - joeynmt.training - Epoch   8, Step:    50500, Batch Loss:     2.433580, Tokens per Sec:    14107, Lr: 0.000300\n",
      "2021-05-25 07:45:30,745 - INFO - joeynmt.training - Epoch   8, Step:    50600, Batch Loss:     2.348301, Tokens per Sec:    14152, Lr: 0.000300\n",
      "2021-05-25 07:45:43,669 - INFO - joeynmt.training - Epoch   8, Step:    50700, Batch Loss:     2.410527, Tokens per Sec:    14020, Lr: 0.000300\n",
      "2021-05-25 07:45:56,645 - INFO - joeynmt.training - Epoch   8, Step:    50800, Batch Loss:     2.190223, Tokens per Sec:    14657, Lr: 0.000300\n",
      "2021-05-25 07:46:09,400 - INFO - joeynmt.training - Epoch   8, Step:    50900, Batch Loss:     2.134995, Tokens per Sec:    13872, Lr: 0.000300\n",
      "2021-05-25 07:46:22,208 - INFO - joeynmt.training - Epoch   8, Step:    51000, Batch Loss:     2.272828, Tokens per Sec:    14082, Lr: 0.000300\n",
      "2021-05-25 07:46:35,145 - INFO - joeynmt.training - Epoch   8, Step:    51100, Batch Loss:     2.489420, Tokens per Sec:    13972, Lr: 0.000300\n",
      "2021-05-25 07:46:48,222 - INFO - joeynmt.training - Epoch   8, Step:    51200, Batch Loss:     2.442858, Tokens per Sec:    13912, Lr: 0.000300\n",
      "2021-05-25 07:47:01,489 - INFO - joeynmt.training - Epoch   8, Step:    51300, Batch Loss:     2.345725, Tokens per Sec:    14169, Lr: 0.000300\n",
      "2021-05-25 07:47:14,450 - INFO - joeynmt.training - Epoch   8, Step:    51400, Batch Loss:     2.439941, Tokens per Sec:    13996, Lr: 0.000300\n",
      "2021-05-25 07:47:27,613 - INFO - joeynmt.training - Epoch   8, Step:    51500, Batch Loss:     2.274348, Tokens per Sec:    14229, Lr: 0.000300\n",
      "2021-05-25 07:47:40,720 - INFO - joeynmt.training - Epoch   8, Step:    51600, Batch Loss:     2.265383, Tokens per Sec:    13949, Lr: 0.000300\n",
      "2021-05-25 07:47:53,764 - INFO - joeynmt.training - Epoch   8, Step:    51700, Batch Loss:     2.311022, Tokens per Sec:    14170, Lr: 0.000300\n",
      "2021-05-25 07:48:06,790 - INFO - joeynmt.training - Epoch   8, Step:    51800, Batch Loss:     2.215194, Tokens per Sec:    14258, Lr: 0.000300\n",
      "2021-05-25 07:48:10,664 - INFO - joeynmt.training - Epoch   8: total training loss 14981.64\n",
      "2021-05-25 07:48:10,665 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-05-25 07:48:20,699 - INFO - joeynmt.training - Epoch   9, Step:    51900, Batch Loss:     2.394176, Tokens per Sec:    13174, Lr: 0.000300\n",
      "2021-05-25 07:48:33,799 - INFO - joeynmt.training - Epoch   9, Step:    52000, Batch Loss:     2.237330, Tokens per Sec:    14016, Lr: 0.000300\n",
      "2021-05-25 07:48:46,847 - INFO - joeynmt.training - Epoch   9, Step:    52100, Batch Loss:     2.293058, Tokens per Sec:    14135, Lr: 0.000300\n",
      "2021-05-25 07:48:59,854 - INFO - joeynmt.training - Epoch   9, Step:    52200, Batch Loss:     2.275233, Tokens per Sec:    13868, Lr: 0.000300\n",
      "2021-05-25 07:49:12,840 - INFO - joeynmt.training - Epoch   9, Step:    52300, Batch Loss:     2.217619, Tokens per Sec:    13977, Lr: 0.000300\n",
      "2021-05-25 07:49:25,783 - INFO - joeynmt.training - Epoch   9, Step:    52400, Batch Loss:     2.114902, Tokens per Sec:    14044, Lr: 0.000300\n",
      "2021-05-25 07:49:38,773 - INFO - joeynmt.training - Epoch   9, Step:    52500, Batch Loss:     2.208422, Tokens per Sec:    14545, Lr: 0.000300\n",
      "2021-05-25 07:49:51,555 - INFO - joeynmt.training - Epoch   9, Step:    52600, Batch Loss:     2.542300, Tokens per Sec:    14266, Lr: 0.000300\n",
      "2021-05-25 07:50:04,396 - INFO - joeynmt.training - Epoch   9, Step:    52700, Batch Loss:     2.307718, Tokens per Sec:    14274, Lr: 0.000300\n",
      "2021-05-25 07:50:17,387 - INFO - joeynmt.training - Epoch   9, Step:    52800, Batch Loss:     2.246719, Tokens per Sec:    14157, Lr: 0.000300\n",
      "2021-05-25 07:50:30,549 - INFO - joeynmt.training - Epoch   9, Step:    52900, Batch Loss:     2.225835, Tokens per Sec:    14224, Lr: 0.000300\n",
      "2021-05-25 07:50:43,696 - INFO - joeynmt.training - Epoch   9, Step:    53000, Batch Loss:     2.122308, Tokens per Sec:    14105, Lr: 0.000300\n",
      "2021-05-25 07:50:56,735 - INFO - joeynmt.training - Epoch   9, Step:    53100, Batch Loss:     2.203223, Tokens per Sec:    14077, Lr: 0.000300\n",
      "2021-05-25 07:51:09,701 - INFO - joeynmt.training - Epoch   9, Step:    53200, Batch Loss:     2.239248, Tokens per Sec:    14144, Lr: 0.000300\n",
      "2021-05-25 07:51:22,827 - INFO - joeynmt.training - Epoch   9, Step:    53300, Batch Loss:     2.332741, Tokens per Sec:    13927, Lr: 0.000300\n",
      "2021-05-25 07:51:36,130 - INFO - joeynmt.training - Epoch   9, Step:    53400, Batch Loss:     1.930023, Tokens per Sec:    14267, Lr: 0.000300\n",
      "2021-05-25 07:51:49,219 - INFO - joeynmt.training - Epoch   9, Step:    53500, Batch Loss:     2.345486, Tokens per Sec:    14195, Lr: 0.000300\n",
      "2021-05-25 07:52:02,396 - INFO - joeynmt.training - Epoch   9, Step:    53600, Batch Loss:     2.296025, Tokens per Sec:    14132, Lr: 0.000300\n",
      "2021-05-25 07:52:15,534 - INFO - joeynmt.training - Epoch   9, Step:    53700, Batch Loss:     2.325968, Tokens per Sec:    14480, Lr: 0.000300\n",
      "2021-05-25 07:52:28,504 - INFO - joeynmt.training - Epoch   9, Step:    53800, Batch Loss:     2.266584, Tokens per Sec:    14104, Lr: 0.000300\n",
      "2021-05-25 07:52:41,581 - INFO - joeynmt.training - Epoch   9, Step:    53900, Batch Loss:     2.218026, Tokens per Sec:    13777, Lr: 0.000300\n",
      "2021-05-25 07:52:54,745 - INFO - joeynmt.training - Epoch   9, Step:    54000, Batch Loss:     2.291609, Tokens per Sec:    14349, Lr: 0.000300\n",
      "2021-05-25 07:53:07,670 - INFO - joeynmt.training - Epoch   9, Step:    54100, Batch Loss:     2.381381, Tokens per Sec:    13600, Lr: 0.000300\n",
      "2021-05-25 07:53:20,749 - INFO - joeynmt.training - Epoch   9, Step:    54200, Batch Loss:     2.248697, Tokens per Sec:    14453, Lr: 0.000300\n",
      "2021-05-25 07:53:33,777 - INFO - joeynmt.training - Epoch   9, Step:    54300, Batch Loss:     2.178643, Tokens per Sec:    14616, Lr: 0.000300\n",
      "2021-05-25 07:53:46,766 - INFO - joeynmt.training - Epoch   9, Step:    54400, Batch Loss:     2.214072, Tokens per Sec:    13909, Lr: 0.000300\n",
      "2021-05-25 07:53:59,926 - INFO - joeynmt.training - Epoch   9, Step:    54500, Batch Loss:     2.241015, Tokens per Sec:    13666, Lr: 0.000300\n",
      "2021-05-25 07:54:13,050 - INFO - joeynmt.training - Epoch   9, Step:    54600, Batch Loss:     2.131969, Tokens per Sec:    14358, Lr: 0.000300\n",
      "2021-05-25 07:54:25,799 - INFO - joeynmt.training - Epoch   9, Step:    54700, Batch Loss:     2.139510, Tokens per Sec:    14157, Lr: 0.000300\n",
      "2021-05-25 07:54:38,707 - INFO - joeynmt.training - Epoch   9, Step:    54800, Batch Loss:     2.267113, Tokens per Sec:    14349, Lr: 0.000300\n",
      "2021-05-25 07:54:51,571 - INFO - joeynmt.training - Epoch   9, Step:    54900, Batch Loss:     2.373497, Tokens per Sec:    14046, Lr: 0.000300\n",
      "2021-05-25 07:55:04,652 - INFO - joeynmt.training - Epoch   9, Step:    55000, Batch Loss:     2.272309, Tokens per Sec:    14367, Lr: 0.000300\n",
      "2021-05-25 07:55:28,361 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 07:55:28,361 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 07:55:28,362 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 07:55:28,628 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 07:55:28,628 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 07:55:29,358 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 07:55:29,359 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 07:55:29,359 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 07:55:29,359 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 07:55:29,359 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 07:55:29,360 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 07:55:29,360 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 07:55:29,360 - INFO - joeynmt.training - \tHypothesis: The writer wrote in the street on the scroll .\n",
      "2021-05-25 07:55:29,360 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 07:55:29,361 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 07:55:29,361 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:55:29,361 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or disappointment , we should continue to maintain our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 07:55:29,361 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 07:55:29,362 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 07:55:29,362 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 07:55:29,362 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of good results in Satan’s world .\n",
      "2021-05-25 07:55:29,362 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    55000: bleu:  21.17, loss: 52948.6836, ppl:   6.7275, duration: 24.7102s\n",
      "2021-05-25 07:55:42,769 - INFO - joeynmt.training - Epoch   9, Step:    55100, Batch Loss:     2.303017, Tokens per Sec:    14115, Lr: 0.000300\n",
      "2021-05-25 07:55:55,908 - INFO - joeynmt.training - Epoch   9, Step:    55200, Batch Loss:     2.326386, Tokens per Sec:    13877, Lr: 0.000300\n",
      "2021-05-25 07:56:09,065 - INFO - joeynmt.training - Epoch   9, Step:    55300, Batch Loss:     2.163252, Tokens per Sec:    13909, Lr: 0.000300\n",
      "2021-05-25 07:56:22,083 - INFO - joeynmt.training - Epoch   9, Step:    55400, Batch Loss:     1.957351, Tokens per Sec:    13960, Lr: 0.000300\n",
      "2021-05-25 07:56:35,257 - INFO - joeynmt.training - Epoch   9, Step:    55500, Batch Loss:     2.230315, Tokens per Sec:    14296, Lr: 0.000300\n",
      "2021-05-25 07:56:48,297 - INFO - joeynmt.training - Epoch   9, Step:    55600, Batch Loss:     2.237950, Tokens per Sec:    13631, Lr: 0.000300\n",
      "2021-05-25 07:57:01,381 - INFO - joeynmt.training - Epoch   9, Step:    55700, Batch Loss:     2.366665, Tokens per Sec:    14008, Lr: 0.000300\n",
      "2021-05-25 07:57:14,151 - INFO - joeynmt.training - Epoch   9, Step:    55800, Batch Loss:     2.276576, Tokens per Sec:    13522, Lr: 0.000300\n",
      "2021-05-25 07:57:27,368 - INFO - joeynmt.training - Epoch   9, Step:    55900, Batch Loss:     2.181208, Tokens per Sec:    14335, Lr: 0.000300\n",
      "2021-05-25 07:57:40,604 - INFO - joeynmt.training - Epoch   9, Step:    56000, Batch Loss:     1.967844, Tokens per Sec:    13858, Lr: 0.000300\n",
      "2021-05-25 07:57:53,603 - INFO - joeynmt.training - Epoch   9, Step:    56100, Batch Loss:     2.145540, Tokens per Sec:    14106, Lr: 0.000300\n",
      "2021-05-25 07:58:06,609 - INFO - joeynmt.training - Epoch   9, Step:    56200, Batch Loss:     2.321972, Tokens per Sec:    14515, Lr: 0.000300\n",
      "2021-05-25 07:58:19,521 - INFO - joeynmt.training - Epoch   9, Step:    56300, Batch Loss:     2.208626, Tokens per Sec:    14434, Lr: 0.000300\n",
      "2021-05-25 07:58:32,514 - INFO - joeynmt.training - Epoch   9, Step:    56400, Batch Loss:     1.951547, Tokens per Sec:    13764, Lr: 0.000300\n",
      "2021-05-25 07:58:45,476 - INFO - joeynmt.training - Epoch   9, Step:    56500, Batch Loss:     2.272243, Tokens per Sec:    13748, Lr: 0.000300\n",
      "2021-05-25 07:58:58,727 - INFO - joeynmt.training - Epoch   9, Step:    56600, Batch Loss:     2.253191, Tokens per Sec:    14429, Lr: 0.000300\n",
      "2021-05-25 07:59:11,555 - INFO - joeynmt.training - Epoch   9, Step:    56700, Batch Loss:     2.287013, Tokens per Sec:    14014, Lr: 0.000300\n",
      "2021-05-25 07:59:24,388 - INFO - joeynmt.training - Epoch   9, Step:    56800, Batch Loss:     2.432016, Tokens per Sec:    14257, Lr: 0.000300\n",
      "2021-05-25 07:59:37,430 - INFO - joeynmt.training - Epoch   9, Step:    56900, Batch Loss:     2.204933, Tokens per Sec:    14509, Lr: 0.000300\n",
      "2021-05-25 07:59:50,425 - INFO - joeynmt.training - Epoch   9, Step:    57000, Batch Loss:     2.644284, Tokens per Sec:    14032, Lr: 0.000300\n",
      "2021-05-25 08:00:03,541 - INFO - joeynmt.training - Epoch   9, Step:    57100, Batch Loss:     2.168652, Tokens per Sec:    14038, Lr: 0.000300\n",
      "2021-05-25 08:00:16,440 - INFO - joeynmt.training - Epoch   9, Step:    57200, Batch Loss:     2.520637, Tokens per Sec:    14410, Lr: 0.000300\n",
      "2021-05-25 08:00:29,563 - INFO - joeynmt.training - Epoch   9, Step:    57300, Batch Loss:     2.200800, Tokens per Sec:    14037, Lr: 0.000300\n",
      "2021-05-25 08:00:42,525 - INFO - joeynmt.training - Epoch   9, Step:    57400, Batch Loss:     2.486681, Tokens per Sec:    13942, Lr: 0.000300\n",
      "2021-05-25 08:00:55,492 - INFO - joeynmt.training - Epoch   9, Step:    57500, Batch Loss:     2.241009, Tokens per Sec:    14479, Lr: 0.000300\n",
      "2021-05-25 08:01:08,410 - INFO - joeynmt.training - Epoch   9, Step:    57600, Batch Loss:     1.853347, Tokens per Sec:    14213, Lr: 0.000300\n",
      "2021-05-25 08:01:21,453 - INFO - joeynmt.training - Epoch   9, Step:    57700, Batch Loss:     2.216236, Tokens per Sec:    13944, Lr: 0.000300\n",
      "2021-05-25 08:01:34,519 - INFO - joeynmt.training - Epoch   9, Step:    57800, Batch Loss:     2.265136, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 08:01:47,553 - INFO - joeynmt.training - Epoch   9, Step:    57900, Batch Loss:     2.321994, Tokens per Sec:    14031, Lr: 0.000300\n",
      "2021-05-25 08:02:00,586 - INFO - joeynmt.training - Epoch   9, Step:    58000, Batch Loss:     2.254186, Tokens per Sec:    14020, Lr: 0.000300\n",
      "2021-05-25 08:02:13,773 - INFO - joeynmt.training - Epoch   9, Step:    58100, Batch Loss:     2.161161, Tokens per Sec:    14287, Lr: 0.000300\n",
      "2021-05-25 08:02:26,745 - INFO - joeynmt.training - Epoch   9, Step:    58200, Batch Loss:     2.333943, Tokens per Sec:    13920, Lr: 0.000300\n",
      "2021-05-25 08:02:39,582 - INFO - joeynmt.training - Epoch   9, Step:    58300, Batch Loss:     2.313835, Tokens per Sec:    14279, Lr: 0.000300\n",
      "2021-05-25 08:02:40,271 - INFO - joeynmt.training - Epoch   9: total training loss 14729.56\n",
      "2021-05-25 08:02:40,272 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-05-25 08:02:52,817 - INFO - joeynmt.training - Epoch  10, Step:    58400, Batch Loss:     2.171284, Tokens per Sec:    13173, Lr: 0.000300\n",
      "2021-05-25 08:03:05,633 - INFO - joeynmt.training - Epoch  10, Step:    58500, Batch Loss:     2.200385, Tokens per Sec:    14267, Lr: 0.000300\n",
      "2021-05-25 08:03:18,645 - INFO - joeynmt.training - Epoch  10, Step:    58600, Batch Loss:     2.180328, Tokens per Sec:    13859, Lr: 0.000300\n",
      "2021-05-25 08:03:31,554 - INFO - joeynmt.training - Epoch  10, Step:    58700, Batch Loss:     2.480569, Tokens per Sec:    13822, Lr: 0.000300\n",
      "2021-05-25 08:03:44,585 - INFO - joeynmt.training - Epoch  10, Step:    58800, Batch Loss:     2.127854, Tokens per Sec:    13994, Lr: 0.000300\n",
      "2021-05-25 08:03:57,593 - INFO - joeynmt.training - Epoch  10, Step:    58900, Batch Loss:     2.315024, Tokens per Sec:    13614, Lr: 0.000300\n",
      "2021-05-25 08:04:10,756 - INFO - joeynmt.training - Epoch  10, Step:    59000, Batch Loss:     2.182667, Tokens per Sec:    14078, Lr: 0.000300\n",
      "2021-05-25 08:04:23,640 - INFO - joeynmt.training - Epoch  10, Step:    59100, Batch Loss:     2.183273, Tokens per Sec:    14157, Lr: 0.000300\n",
      "2021-05-25 08:04:36,461 - INFO - joeynmt.training - Epoch  10, Step:    59200, Batch Loss:     2.245435, Tokens per Sec:    14587, Lr: 0.000300\n",
      "2021-05-25 08:04:49,529 - INFO - joeynmt.training - Epoch  10, Step:    59300, Batch Loss:     2.133982, Tokens per Sec:    14527, Lr: 0.000300\n",
      "2021-05-25 08:05:02,650 - INFO - joeynmt.training - Epoch  10, Step:    59400, Batch Loss:     2.791287, Tokens per Sec:    14112, Lr: 0.000300\n",
      "2021-05-25 08:05:15,748 - INFO - joeynmt.training - Epoch  10, Step:    59500, Batch Loss:     2.213314, Tokens per Sec:    14327, Lr: 0.000300\n",
      "2021-05-25 08:05:28,827 - INFO - joeynmt.training - Epoch  10, Step:    59600, Batch Loss:     2.515197, Tokens per Sec:    14077, Lr: 0.000300\n",
      "2021-05-25 08:05:42,007 - INFO - joeynmt.training - Epoch  10, Step:    59700, Batch Loss:     2.315057, Tokens per Sec:    14176, Lr: 0.000300\n",
      "2021-05-25 08:05:55,263 - INFO - joeynmt.training - Epoch  10, Step:    59800, Batch Loss:     2.365676, Tokens per Sec:    14365, Lr: 0.000300\n",
      "2021-05-25 08:06:08,311 - INFO - joeynmt.training - Epoch  10, Step:    59900, Batch Loss:     2.278669, Tokens per Sec:    14059, Lr: 0.000300\n",
      "2021-05-25 08:06:21,437 - INFO - joeynmt.training - Epoch  10, Step:    60000, Batch Loss:     2.074213, Tokens per Sec:    14020, Lr: 0.000300\n",
      "2021-05-25 08:06:45,189 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 08:06:45,189 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 08:06:45,189 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 08:06:45,454 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 08:06:45,455 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 08:06:46,537 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 08:06:46,538 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 08:06:46,538 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 08:06:46,538 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 08:06:46,538 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 08:06:46,539 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 08:06:46,539 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 08:06:46,539 - INFO - joeynmt.training - \tHypothesis: The letter writed in the street of the scroll .\n",
      "2021-05-25 08:06:46,539 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 08:06:46,540 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 08:06:46,540 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:06:46,540 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or discerned , we should continue to maintain our faith in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:06:46,540 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 08:06:46,541 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 08:06:46,541 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 08:06:46,541 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of being made in Satan’s world .\n",
      "2021-05-25 08:06:46,541 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    60000: bleu:  21.64, loss: 52193.8633, ppl:   6.5472, duration: 25.1039s\n",
      "2021-05-25 08:06:59,921 - INFO - joeynmt.training - Epoch  10, Step:    60100, Batch Loss:     2.203417, Tokens per Sec:    14050, Lr: 0.000300\n",
      "2021-05-25 08:07:12,946 - INFO - joeynmt.training - Epoch  10, Step:    60200, Batch Loss:     2.268035, Tokens per Sec:    13594, Lr: 0.000300\n",
      "2021-05-25 08:07:25,986 - INFO - joeynmt.training - Epoch  10, Step:    60300, Batch Loss:     2.239518, Tokens per Sec:    14126, Lr: 0.000300\n",
      "2021-05-25 08:07:38,900 - INFO - joeynmt.training - Epoch  10, Step:    60400, Batch Loss:     2.067063, Tokens per Sec:    14392, Lr: 0.000300\n",
      "2021-05-25 08:07:51,837 - INFO - joeynmt.training - Epoch  10, Step:    60500, Batch Loss:     2.172420, Tokens per Sec:    14247, Lr: 0.000300\n",
      "2021-05-25 08:08:04,773 - INFO - joeynmt.training - Epoch  10, Step:    60600, Batch Loss:     2.153242, Tokens per Sec:    14530, Lr: 0.000300\n",
      "2021-05-25 08:08:17,632 - INFO - joeynmt.training - Epoch  10, Step:    60700, Batch Loss:     2.204430, Tokens per Sec:    14236, Lr: 0.000300\n",
      "2021-05-25 08:08:30,462 - INFO - joeynmt.training - Epoch  10, Step:    60800, Batch Loss:     2.302992, Tokens per Sec:    14153, Lr: 0.000300\n",
      "2021-05-25 08:08:43,552 - INFO - joeynmt.training - Epoch  10, Step:    60900, Batch Loss:     2.181460, Tokens per Sec:    13877, Lr: 0.000300\n",
      "2021-05-25 08:08:56,569 - INFO - joeynmt.training - Epoch  10, Step:    61000, Batch Loss:     2.145666, Tokens per Sec:    14148, Lr: 0.000300\n",
      "2021-05-25 08:09:09,471 - INFO - joeynmt.training - Epoch  10, Step:    61100, Batch Loss:     2.475404, Tokens per Sec:    13934, Lr: 0.000300\n",
      "2021-05-25 08:09:22,717 - INFO - joeynmt.training - Epoch  10, Step:    61200, Batch Loss:     2.401756, Tokens per Sec:    14496, Lr: 0.000300\n",
      "2021-05-25 08:09:35,708 - INFO - joeynmt.training - Epoch  10, Step:    61300, Batch Loss:     2.243323, Tokens per Sec:    13843, Lr: 0.000300\n",
      "2021-05-25 08:09:48,710 - INFO - joeynmt.training - Epoch  10, Step:    61400, Batch Loss:     2.124568, Tokens per Sec:    13746, Lr: 0.000300\n",
      "2021-05-25 08:10:01,934 - INFO - joeynmt.training - Epoch  10, Step:    61500, Batch Loss:     2.176762, Tokens per Sec:    14373, Lr: 0.000300\n",
      "2021-05-25 08:10:15,012 - INFO - joeynmt.training - Epoch  10, Step:    61600, Batch Loss:     2.145074, Tokens per Sec:    14150, Lr: 0.000300\n",
      "2021-05-25 08:10:28,232 - INFO - joeynmt.training - Epoch  10, Step:    61700, Batch Loss:     2.084777, Tokens per Sec:    14277, Lr: 0.000300\n",
      "2021-05-25 08:10:41,056 - INFO - joeynmt.training - Epoch  10, Step:    61800, Batch Loss:     2.303487, Tokens per Sec:    13867, Lr: 0.000300\n",
      "2021-05-25 08:10:54,151 - INFO - joeynmt.training - Epoch  10, Step:    61900, Batch Loss:     2.320906, Tokens per Sec:    13975, Lr: 0.000300\n",
      "2021-05-25 08:11:07,364 - INFO - joeynmt.training - Epoch  10, Step:    62000, Batch Loss:     2.295151, Tokens per Sec:    14256, Lr: 0.000300\n",
      "2021-05-25 08:11:20,317 - INFO - joeynmt.training - Epoch  10, Step:    62100, Batch Loss:     2.298490, Tokens per Sec:    13848, Lr: 0.000300\n",
      "2021-05-25 08:11:33,549 - INFO - joeynmt.training - Epoch  10, Step:    62200, Batch Loss:     2.226178, Tokens per Sec:    14328, Lr: 0.000300\n",
      "2021-05-25 08:11:46,382 - INFO - joeynmt.training - Epoch  10, Step:    62300, Batch Loss:     2.316159, Tokens per Sec:    13667, Lr: 0.000300\n",
      "2021-05-25 08:11:59,588 - INFO - joeynmt.training - Epoch  10, Step:    62400, Batch Loss:     2.118334, Tokens per Sec:    14225, Lr: 0.000300\n",
      "2021-05-25 08:12:12,743 - INFO - joeynmt.training - Epoch  10, Step:    62500, Batch Loss:     2.324092, Tokens per Sec:    14147, Lr: 0.000300\n",
      "2021-05-25 08:12:25,764 - INFO - joeynmt.training - Epoch  10, Step:    62600, Batch Loss:     2.178341, Tokens per Sec:    13847, Lr: 0.000300\n",
      "2021-05-25 08:12:38,772 - INFO - joeynmt.training - Epoch  10, Step:    62700, Batch Loss:     2.120068, Tokens per Sec:    14096, Lr: 0.000300\n",
      "2021-05-25 08:12:51,724 - INFO - joeynmt.training - Epoch  10, Step:    62800, Batch Loss:     2.200272, Tokens per Sec:    13958, Lr: 0.000300\n",
      "2021-05-25 08:13:04,454 - INFO - joeynmt.training - Epoch  10, Step:    62900, Batch Loss:     2.163186, Tokens per Sec:    14366, Lr: 0.000300\n",
      "2021-05-25 08:13:17,443 - INFO - joeynmt.training - Epoch  10, Step:    63000, Batch Loss:     2.304407, Tokens per Sec:    14802, Lr: 0.000300\n",
      "2021-05-25 08:13:30,429 - INFO - joeynmt.training - Epoch  10, Step:    63100, Batch Loss:     2.166822, Tokens per Sec:    13959, Lr: 0.000300\n",
      "2021-05-25 08:13:43,698 - INFO - joeynmt.training - Epoch  10, Step:    63200, Batch Loss:     2.380537, Tokens per Sec:    14210, Lr: 0.000300\n",
      "2021-05-25 08:13:56,676 - INFO - joeynmt.training - Epoch  10, Step:    63300, Batch Loss:     2.141363, Tokens per Sec:    13861, Lr: 0.000300\n",
      "2021-05-25 08:14:09,695 - INFO - joeynmt.training - Epoch  10, Step:    63400, Batch Loss:     2.238413, Tokens per Sec:    13954, Lr: 0.000300\n",
      "2021-05-25 08:14:22,823 - INFO - joeynmt.training - Epoch  10, Step:    63500, Batch Loss:     2.346064, Tokens per Sec:    14310, Lr: 0.000300\n",
      "2021-05-25 08:14:35,682 - INFO - joeynmt.training - Epoch  10, Step:    63600, Batch Loss:     2.475710, Tokens per Sec:    13747, Lr: 0.000300\n",
      "2021-05-25 08:14:48,943 - INFO - joeynmt.training - Epoch  10, Step:    63700, Batch Loss:     2.123218, Tokens per Sec:    13965, Lr: 0.000300\n",
      "2021-05-25 08:15:01,671 - INFO - joeynmt.training - Epoch  10, Step:    63800, Batch Loss:     2.423980, Tokens per Sec:    14028, Lr: 0.000300\n",
      "2021-05-25 08:15:14,824 - INFO - joeynmt.training - Epoch  10, Step:    63900, Batch Loss:     2.179731, Tokens per Sec:    14723, Lr: 0.000300\n",
      "2021-05-25 08:15:27,777 - INFO - joeynmt.training - Epoch  10, Step:    64000, Batch Loss:     2.201746, Tokens per Sec:    14384, Lr: 0.000300\n",
      "2021-05-25 08:15:40,381 - INFO - joeynmt.training - Epoch  10, Step:    64100, Batch Loss:     2.145743, Tokens per Sec:    13791, Lr: 0.000300\n",
      "2021-05-25 08:15:53,208 - INFO - joeynmt.training - Epoch  10, Step:    64200, Batch Loss:     2.236696, Tokens per Sec:    14175, Lr: 0.000300\n",
      "2021-05-25 08:16:06,249 - INFO - joeynmt.training - Epoch  10, Step:    64300, Batch Loss:     2.171956, Tokens per Sec:    14053, Lr: 0.000300\n",
      "2021-05-25 08:16:19,501 - INFO - joeynmt.training - Epoch  10, Step:    64400, Batch Loss:     2.466924, Tokens per Sec:    14179, Lr: 0.000300\n",
      "2021-05-25 08:16:32,713 - INFO - joeynmt.training - Epoch  10, Step:    64500, Batch Loss:     1.898638, Tokens per Sec:    14031, Lr: 0.000300\n",
      "2021-05-25 08:16:45,785 - INFO - joeynmt.training - Epoch  10, Step:    64600, Batch Loss:     2.110740, Tokens per Sec:    14041, Lr: 0.000300\n",
      "2021-05-25 08:16:58,778 - INFO - joeynmt.training - Epoch  10, Step:    64700, Batch Loss:     2.094331, Tokens per Sec:    14088, Lr: 0.000300\n",
      "2021-05-25 08:17:10,860 - INFO - joeynmt.training - Epoch  10: total training loss 14544.27\n",
      "2021-05-25 08:17:10,860 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-05-25 08:17:12,633 - INFO - joeynmt.training - Epoch  11, Step:    64800, Batch Loss:     2.305518, Tokens per Sec:     8888, Lr: 0.000300\n",
      "2021-05-25 08:17:25,588 - INFO - joeynmt.training - Epoch  11, Step:    64900, Batch Loss:     2.262652, Tokens per Sec:    14166, Lr: 0.000300\n",
      "2021-05-25 08:17:38,768 - INFO - joeynmt.training - Epoch  11, Step:    65000, Batch Loss:     2.120523, Tokens per Sec:    14375, Lr: 0.000300\n",
      "2021-05-25 08:18:00,914 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 08:18:00,915 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 08:18:00,915 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 08:18:01,181 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 08:18:01,182 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 08:18:01,929 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 08:18:01,930 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 08:18:01,930 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 08:18:01,931 - INFO - joeynmt.training - \tHypothesis: I felt that I was deeply moved .\n",
      "2021-05-25 08:18:01,931 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 08:18:01,931 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 08:18:01,932 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 08:18:01,932 - INFO - joeynmt.training - \tHypothesis: The letter was written in the street before the scroll .\n",
      "2021-05-25 08:18:01,932 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 08:18:01,932 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 08:18:01,933 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:18:01,933 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or see , we should continue our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:18:01,933 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 08:18:01,933 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 08:18:01,934 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 08:18:01,934 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being successful in Satan’s world .\n",
      "2021-05-25 08:18:01,934 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    65000: bleu:  21.87, loss: 51547.4375, ppl:   6.3966, duration: 23.1650s\n",
      "2021-05-25 08:18:15,221 - INFO - joeynmt.training - Epoch  11, Step:    65100, Batch Loss:     2.028682, Tokens per Sec:    13940, Lr: 0.000300\n",
      "2021-05-25 08:18:28,290 - INFO - joeynmt.training - Epoch  11, Step:    65200, Batch Loss:     2.326939, Tokens per Sec:    14214, Lr: 0.000300\n",
      "2021-05-25 08:18:41,449 - INFO - joeynmt.training - Epoch  11, Step:    65300, Batch Loss:     2.277506, Tokens per Sec:    14309, Lr: 0.000300\n",
      "2021-05-25 08:18:54,075 - INFO - joeynmt.training - Epoch  11, Step:    65400, Batch Loss:     2.158983, Tokens per Sec:    13687, Lr: 0.000300\n",
      "2021-05-25 08:19:06,902 - INFO - joeynmt.training - Epoch  11, Step:    65500, Batch Loss:     2.287957, Tokens per Sec:    14324, Lr: 0.000300\n",
      "2021-05-25 08:19:19,707 - INFO - joeynmt.training - Epoch  11, Step:    65600, Batch Loss:     2.212761, Tokens per Sec:    14544, Lr: 0.000300\n",
      "2021-05-25 08:19:32,581 - INFO - joeynmt.training - Epoch  11, Step:    65700, Batch Loss:     2.315115, Tokens per Sec:    14158, Lr: 0.000300\n",
      "2021-05-25 08:19:45,661 - INFO - joeynmt.training - Epoch  11, Step:    65800, Batch Loss:     2.077095, Tokens per Sec:    13932, Lr: 0.000300\n",
      "2021-05-25 08:19:58,695 - INFO - joeynmt.training - Epoch  11, Step:    65900, Batch Loss:     2.266054, Tokens per Sec:    13995, Lr: 0.000300\n",
      "2021-05-25 08:20:11,806 - INFO - joeynmt.training - Epoch  11, Step:    66000, Batch Loss:     2.317928, Tokens per Sec:    13867, Lr: 0.000300\n",
      "2021-05-25 08:20:24,957 - INFO - joeynmt.training - Epoch  11, Step:    66100, Batch Loss:     2.098009, Tokens per Sec:    14043, Lr: 0.000300\n",
      "2021-05-25 08:20:37,926 - INFO - joeynmt.training - Epoch  11, Step:    66200, Batch Loss:     2.344684, Tokens per Sec:    13754, Lr: 0.000300\n",
      "2021-05-25 08:20:51,072 - INFO - joeynmt.training - Epoch  11, Step:    66300, Batch Loss:     2.094181, Tokens per Sec:    14251, Lr: 0.000300\n",
      "2021-05-25 08:21:04,106 - INFO - joeynmt.training - Epoch  11, Step:    66400, Batch Loss:     1.959163, Tokens per Sec:    13855, Lr: 0.000300\n",
      "2021-05-25 08:21:17,135 - INFO - joeynmt.training - Epoch  11, Step:    66500, Batch Loss:     2.060616, Tokens per Sec:    14217, Lr: 0.000300\n",
      "2021-05-25 08:21:30,309 - INFO - joeynmt.training - Epoch  11, Step:    66600, Batch Loss:     2.171742, Tokens per Sec:    13988, Lr: 0.000300\n",
      "2021-05-25 08:21:43,347 - INFO - joeynmt.training - Epoch  11, Step:    66700, Batch Loss:     2.122241, Tokens per Sec:    14250, Lr: 0.000300\n",
      "2021-05-25 08:21:56,146 - INFO - joeynmt.training - Epoch  11, Step:    66800, Batch Loss:     2.190910, Tokens per Sec:    14046, Lr: 0.000300\n",
      "2021-05-25 08:22:09,052 - INFO - joeynmt.training - Epoch  11, Step:    66900, Batch Loss:     2.060969, Tokens per Sec:    14344, Lr: 0.000300\n",
      "2021-05-25 08:22:21,995 - INFO - joeynmt.training - Epoch  11, Step:    67000, Batch Loss:     2.226983, Tokens per Sec:    14183, Lr: 0.000300\n",
      "2021-05-25 08:22:35,053 - INFO - joeynmt.training - Epoch  11, Step:    67100, Batch Loss:     2.337127, Tokens per Sec:    14153, Lr: 0.000300\n",
      "2021-05-25 08:22:48,012 - INFO - joeynmt.training - Epoch  11, Step:    67200, Batch Loss:     2.192317, Tokens per Sec:    14030, Lr: 0.000300\n",
      "2021-05-25 08:23:01,060 - INFO - joeynmt.training - Epoch  11, Step:    67300, Batch Loss:     2.120992, Tokens per Sec:    14136, Lr: 0.000300\n",
      "2021-05-25 08:23:14,089 - INFO - joeynmt.training - Epoch  11, Step:    67400, Batch Loss:     2.388156, Tokens per Sec:    13691, Lr: 0.000300\n",
      "2021-05-25 08:23:27,201 - INFO - joeynmt.training - Epoch  11, Step:    67500, Batch Loss:     2.321867, Tokens per Sec:    14526, Lr: 0.000300\n",
      "2021-05-25 08:23:39,866 - INFO - joeynmt.training - Epoch  11, Step:    67600, Batch Loss:     2.200173, Tokens per Sec:    14029, Lr: 0.000300\n",
      "2021-05-25 08:23:52,806 - INFO - joeynmt.training - Epoch  11, Step:    67700, Batch Loss:     2.178887, Tokens per Sec:    14638, Lr: 0.000300\n",
      "2021-05-25 08:24:05,683 - INFO - joeynmt.training - Epoch  11, Step:    67800, Batch Loss:     2.093568, Tokens per Sec:    14374, Lr: 0.000300\n",
      "2021-05-25 08:24:18,643 - INFO - joeynmt.training - Epoch  11, Step:    67900, Batch Loss:     2.291354, Tokens per Sec:    14392, Lr: 0.000300\n",
      "2021-05-25 08:24:31,741 - INFO - joeynmt.training - Epoch  11, Step:    68000, Batch Loss:     2.209475, Tokens per Sec:    13988, Lr: 0.000300\n",
      "2021-05-25 08:24:44,773 - INFO - joeynmt.training - Epoch  11, Step:    68100, Batch Loss:     2.007684, Tokens per Sec:    14172, Lr: 0.000300\n",
      "2021-05-25 08:24:57,868 - INFO - joeynmt.training - Epoch  11, Step:    68200, Batch Loss:     2.159490, Tokens per Sec:    14216, Lr: 0.000300\n",
      "2021-05-25 08:25:10,915 - INFO - joeynmt.training - Epoch  11, Step:    68300, Batch Loss:     2.281914, Tokens per Sec:    14151, Lr: 0.000300\n",
      "2021-05-25 08:25:24,177 - INFO - joeynmt.training - Epoch  11, Step:    68400, Batch Loss:     2.226432, Tokens per Sec:    14376, Lr: 0.000300\n",
      "2021-05-25 08:25:37,061 - INFO - joeynmt.training - Epoch  11, Step:    68500, Batch Loss:     2.112010, Tokens per Sec:    13776, Lr: 0.000300\n",
      "2021-05-25 08:25:50,198 - INFO - joeynmt.training - Epoch  11, Step:    68600, Batch Loss:     2.117107, Tokens per Sec:    14357, Lr: 0.000300\n",
      "2021-05-25 08:26:03,211 - INFO - joeynmt.training - Epoch  11, Step:    68700, Batch Loss:     2.114949, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 08:26:16,420 - INFO - joeynmt.training - Epoch  11, Step:    68800, Batch Loss:     2.090666, Tokens per Sec:    14146, Lr: 0.000300\n",
      "2021-05-25 08:26:29,457 - INFO - joeynmt.training - Epoch  11, Step:    68900, Batch Loss:     2.118685, Tokens per Sec:    14093, Lr: 0.000300\n",
      "2021-05-25 08:26:42,609 - INFO - joeynmt.training - Epoch  11, Step:    69000, Batch Loss:     2.176871, Tokens per Sec:    14144, Lr: 0.000300\n",
      "2021-05-25 08:26:55,681 - INFO - joeynmt.training - Epoch  11, Step:    69100, Batch Loss:     2.337873, Tokens per Sec:    14093, Lr: 0.000300\n",
      "2021-05-25 08:27:08,661 - INFO - joeynmt.training - Epoch  11, Step:    69200, Batch Loss:     2.144397, Tokens per Sec:    13845, Lr: 0.000300\n",
      "2021-05-25 08:27:21,717 - INFO - joeynmt.training - Epoch  11, Step:    69300, Batch Loss:     2.137796, Tokens per Sec:    14194, Lr: 0.000300\n",
      "2021-05-25 08:27:34,708 - INFO - joeynmt.training - Epoch  11, Step:    69400, Batch Loss:     2.006791, Tokens per Sec:    13895, Lr: 0.000300\n",
      "2021-05-25 08:27:47,720 - INFO - joeynmt.training - Epoch  11, Step:    69500, Batch Loss:     2.278331, Tokens per Sec:    14159, Lr: 0.000300\n",
      "2021-05-25 08:28:00,488 - INFO - joeynmt.training - Epoch  11, Step:    69600, Batch Loss:     2.348502, Tokens per Sec:    14270, Lr: 0.000300\n",
      "2021-05-25 08:28:13,216 - INFO - joeynmt.training - Epoch  11, Step:    69700, Batch Loss:     2.322254, Tokens per Sec:    14418, Lr: 0.000300\n",
      "2021-05-25 08:28:25,964 - INFO - joeynmt.training - Epoch  11, Step:    69800, Batch Loss:     2.207555, Tokens per Sec:    13970, Lr: 0.000300\n",
      "2021-05-25 08:28:38,860 - INFO - joeynmt.training - Epoch  11, Step:    69900, Batch Loss:     2.262977, Tokens per Sec:    13943, Lr: 0.000300\n",
      "2021-05-25 08:28:52,001 - INFO - joeynmt.training - Epoch  11, Step:    70000, Batch Loss:     2.125565, Tokens per Sec:    14261, Lr: 0.000300\n",
      "2021-05-25 08:29:15,463 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 08:29:15,464 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 08:29:15,464 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 08:29:15,726 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 08:29:15,726 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 08:29:16,454 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 08:29:16,457 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 08:29:16,458 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 08:29:16,458 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-05-25 08:29:16,458 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 08:29:16,458 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 08:29:16,459 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 08:29:16,459 - INFO - joeynmt.training - \tHypothesis: The writer wrote in the pages of the scroll .\n",
      "2021-05-25 08:29:16,459 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 08:29:16,459 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 08:29:16,460 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:29:16,460 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or see , we should continue our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:29:16,460 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 08:29:16,460 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 08:29:16,461 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 08:29:16,461 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-05-25 08:29:16,461 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    70000: bleu:  22.46, loss: 50884.9531, ppl:   6.2458, duration: 24.4600s\n",
      "2021-05-25 08:29:29,860 - INFO - joeynmt.training - Epoch  11, Step:    70100, Batch Loss:     2.280164, Tokens per Sec:    13780, Lr: 0.000300\n",
      "2021-05-25 08:29:42,727 - INFO - joeynmt.training - Epoch  11, Step:    70200, Batch Loss:     2.147543, Tokens per Sec:    13647, Lr: 0.000300\n",
      "2021-05-25 08:29:55,810 - INFO - joeynmt.training - Epoch  11, Step:    70300, Batch Loss:     2.250830, Tokens per Sec:    14085, Lr: 0.000300\n",
      "2021-05-25 08:30:09,084 - INFO - joeynmt.training - Epoch  11, Step:    70400, Batch Loss:     2.221099, Tokens per Sec:    14472, Lr: 0.000300\n",
      "2021-05-25 08:30:22,164 - INFO - joeynmt.training - Epoch  11, Step:    70500, Batch Loss:     1.967975, Tokens per Sec:    13992, Lr: 0.000300\n",
      "2021-05-25 08:30:35,288 - INFO - joeynmt.training - Epoch  11, Step:    70600, Batch Loss:     2.171058, Tokens per Sec:    14009, Lr: 0.000300\n",
      "2021-05-25 08:30:48,300 - INFO - joeynmt.training - Epoch  11, Step:    70700, Batch Loss:     2.285162, Tokens per Sec:    14076, Lr: 0.000300\n",
      "2021-05-25 08:31:01,249 - INFO - joeynmt.training - Epoch  11, Step:    70800, Batch Loss:     2.318356, Tokens per Sec:    13715, Lr: 0.000300\n",
      "2021-05-25 08:31:14,272 - INFO - joeynmt.training - Epoch  11, Step:    70900, Batch Loss:     2.051473, Tokens per Sec:    14890, Lr: 0.000300\n",
      "2021-05-25 08:31:26,864 - INFO - joeynmt.training - Epoch  11, Step:    71000, Batch Loss:     2.220724, Tokens per Sec:    13745, Lr: 0.000300\n",
      "2021-05-25 08:31:39,638 - INFO - joeynmt.training - Epoch  11, Step:    71100, Batch Loss:     2.188999, Tokens per Sec:    14221, Lr: 0.000300\n",
      "2021-05-25 08:31:52,766 - INFO - joeynmt.training - Epoch  11, Step:    71200, Batch Loss:     2.101268, Tokens per Sec:    14553, Lr: 0.000300\n",
      "2021-05-25 08:32:02,721 - INFO - joeynmt.training - Epoch  11: total training loss 14353.09\n",
      "2021-05-25 08:32:02,721 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-05-25 08:32:06,700 - INFO - joeynmt.training - Epoch  12, Step:    71300, Batch Loss:     2.296575, Tokens per Sec:    12269, Lr: 0.000300\n",
      "2021-05-25 08:32:19,772 - INFO - joeynmt.training - Epoch  12, Step:    71400, Batch Loss:     2.274823, Tokens per Sec:    14362, Lr: 0.000300\n",
      "2021-05-25 08:32:32,778 - INFO - joeynmt.training - Epoch  12, Step:    71500, Batch Loss:     2.161128, Tokens per Sec:    13694, Lr: 0.000300\n",
      "2021-05-25 08:32:45,954 - INFO - joeynmt.training - Epoch  12, Step:    71600, Batch Loss:     2.018402, Tokens per Sec:    14412, Lr: 0.000300\n",
      "2021-05-25 08:32:58,974 - INFO - joeynmt.training - Epoch  12, Step:    71700, Batch Loss:     2.151727, Tokens per Sec:    13666, Lr: 0.000300\n",
      "2021-05-25 08:33:12,120 - INFO - joeynmt.training - Epoch  12, Step:    71800, Batch Loss:     2.263516, Tokens per Sec:    14281, Lr: 0.000300\n",
      "2021-05-25 08:33:24,958 - INFO - joeynmt.training - Epoch  12, Step:    71900, Batch Loss:     2.051758, Tokens per Sec:    14161, Lr: 0.000300\n",
      "2021-05-25 08:33:37,932 - INFO - joeynmt.training - Epoch  12, Step:    72000, Batch Loss:     2.250279, Tokens per Sec:    13770, Lr: 0.000300\n",
      "2021-05-25 08:33:50,961 - INFO - joeynmt.training - Epoch  12, Step:    72100, Batch Loss:     2.139218, Tokens per Sec:    14018, Lr: 0.000300\n",
      "2021-05-25 08:34:04,142 - INFO - joeynmt.training - Epoch  12, Step:    72200, Batch Loss:     2.163913, Tokens per Sec:    14303, Lr: 0.000300\n",
      "2021-05-25 08:34:17,205 - INFO - joeynmt.training - Epoch  12, Step:    72300, Batch Loss:     2.151466, Tokens per Sec:    14284, Lr: 0.000300\n",
      "2021-05-25 08:34:30,227 - INFO - joeynmt.training - Epoch  12, Step:    72400, Batch Loss:     2.328256, Tokens per Sec:    13730, Lr: 0.000300\n",
      "2021-05-25 08:34:43,161 - INFO - joeynmt.training - Epoch  12, Step:    72500, Batch Loss:     2.202432, Tokens per Sec:    14207, Lr: 0.000300\n",
      "2021-05-25 08:34:55,874 - INFO - joeynmt.training - Epoch  12, Step:    72600, Batch Loss:     2.152059, Tokens per Sec:    14320, Lr: 0.000300\n",
      "2021-05-25 08:35:08,774 - INFO - joeynmt.training - Epoch  12, Step:    72700, Batch Loss:     2.221785, Tokens per Sec:    14397, Lr: 0.000300\n",
      "2021-05-25 08:35:21,609 - INFO - joeynmt.training - Epoch  12, Step:    72800, Batch Loss:     2.249026, Tokens per Sec:    14299, Lr: 0.000300\n",
      "2021-05-25 08:35:34,379 - INFO - joeynmt.training - Epoch  12, Step:    72900, Batch Loss:     2.171437, Tokens per Sec:    14227, Lr: 0.000300\n",
      "2021-05-25 08:35:47,042 - INFO - joeynmt.training - Epoch  12, Step:    73000, Batch Loss:     2.285709, Tokens per Sec:    13731, Lr: 0.000300\n",
      "2021-05-25 08:36:00,240 - INFO - joeynmt.training - Epoch  12, Step:    73100, Batch Loss:     2.078790, Tokens per Sec:    14594, Lr: 0.000300\n",
      "2021-05-25 08:36:13,330 - INFO - joeynmt.training - Epoch  12, Step:    73200, Batch Loss:     2.163559, Tokens per Sec:    13628, Lr: 0.000300\n",
      "2021-05-25 08:36:26,387 - INFO - joeynmt.training - Epoch  12, Step:    73300, Batch Loss:     2.144302, Tokens per Sec:    13912, Lr: 0.000300\n",
      "2021-05-25 08:36:39,283 - INFO - joeynmt.training - Epoch  12, Step:    73400, Batch Loss:     2.373626, Tokens per Sec:    13912, Lr: 0.000300\n",
      "2021-05-25 08:36:52,431 - INFO - joeynmt.training - Epoch  12, Step:    73500, Batch Loss:     2.271575, Tokens per Sec:    14038, Lr: 0.000300\n",
      "2021-05-25 08:37:05,442 - INFO - joeynmt.training - Epoch  12, Step:    73600, Batch Loss:     2.353080, Tokens per Sec:    14035, Lr: 0.000300\n",
      "2021-05-25 08:37:18,565 - INFO - joeynmt.training - Epoch  12, Step:    73700, Batch Loss:     2.065360, Tokens per Sec:    14444, Lr: 0.000300\n",
      "2021-05-25 08:37:31,800 - INFO - joeynmt.training - Epoch  12, Step:    73800, Batch Loss:     2.248893, Tokens per Sec:    14205, Lr: 0.000300\n",
      "2021-05-25 08:37:45,030 - INFO - joeynmt.training - Epoch  12, Step:    73900, Batch Loss:     2.237407, Tokens per Sec:    14327, Lr: 0.000300\n",
      "2021-05-25 08:37:58,170 - INFO - joeynmt.training - Epoch  12, Step:    74000, Batch Loss:     2.112823, Tokens per Sec:    13994, Lr: 0.000300\n",
      "2021-05-25 08:38:11,279 - INFO - joeynmt.training - Epoch  12, Step:    74100, Batch Loss:     2.213453, Tokens per Sec:    14046, Lr: 0.000300\n",
      "2021-05-25 08:38:24,562 - INFO - joeynmt.training - Epoch  12, Step:    74200, Batch Loss:     2.318372, Tokens per Sec:    14400, Lr: 0.000300\n",
      "2021-05-25 08:38:37,375 - INFO - joeynmt.training - Epoch  12, Step:    74300, Batch Loss:     2.245826, Tokens per Sec:    13657, Lr: 0.000300\n",
      "2021-05-25 08:38:50,335 - INFO - joeynmt.training - Epoch  12, Step:    74400, Batch Loss:     2.048278, Tokens per Sec:    14071, Lr: 0.000300\n",
      "2021-05-25 08:39:03,291 - INFO - joeynmt.training - Epoch  12, Step:    74500, Batch Loss:     2.138534, Tokens per Sec:    13689, Lr: 0.000300\n",
      "2021-05-25 08:39:16,548 - INFO - joeynmt.training - Epoch  12, Step:    74600, Batch Loss:     2.069842, Tokens per Sec:    14507, Lr: 0.000300\n",
      "2021-05-25 08:39:29,355 - INFO - joeynmt.training - Epoch  12, Step:    74700, Batch Loss:     2.229016, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 08:39:42,046 - INFO - joeynmt.training - Epoch  12, Step:    74800, Batch Loss:     2.283777, Tokens per Sec:    14427, Lr: 0.000300\n",
      "2021-05-25 08:39:54,680 - INFO - joeynmt.training - Epoch  12, Step:    74900, Batch Loss:     2.185434, Tokens per Sec:    14149, Lr: 0.000300\n",
      "2021-05-25 08:40:07,617 - INFO - joeynmt.training - Epoch  12, Step:    75000, Batch Loss:     2.217806, Tokens per Sec:    14285, Lr: 0.000300\n",
      "2021-05-25 08:40:32,866 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 08:40:32,866 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 08:40:32,867 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 08:40:33,133 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 08:40:33,134 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 08:40:33,870 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 08:40:33,871 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 08:40:33,871 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 08:40:33,871 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 08:40:33,871 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 08:40:33,872 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 08:40:33,872 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 08:40:33,872 - INFO - joeynmt.training - \tHypothesis: The text wrote in the front of the scroll .\n",
      "2021-05-25 08:40:33,872 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 08:40:33,873 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 08:40:33,873 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:40:33,873 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or see , we should continue to exercise faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:40:33,873 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 08:40:33,874 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 08:40:33,874 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 08:40:33,874 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-05-25 08:40:33,874 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    75000: bleu:  22.65, loss: 50680.6055, ppl:   6.2000, duration: 26.2573s\n",
      "2021-05-25 08:40:47,253 - INFO - joeynmt.training - Epoch  12, Step:    75100, Batch Loss:     2.381652, Tokens per Sec:    13718, Lr: 0.000300\n",
      "2021-05-25 08:41:00,075 - INFO - joeynmt.training - Epoch  12, Step:    75200, Batch Loss:     2.173192, Tokens per Sec:    13837, Lr: 0.000300\n",
      "2021-05-25 08:41:12,979 - INFO - joeynmt.training - Epoch  12, Step:    75300, Batch Loss:     2.293806, Tokens per Sec:    14048, Lr: 0.000300\n",
      "2021-05-25 08:41:26,077 - INFO - joeynmt.training - Epoch  12, Step:    75400, Batch Loss:     1.903887, Tokens per Sec:    13987, Lr: 0.000300\n",
      "2021-05-25 08:41:39,134 - INFO - joeynmt.training - Epoch  12, Step:    75500, Batch Loss:     2.297908, Tokens per Sec:    14267, Lr: 0.000300\n",
      "2021-05-25 08:41:52,245 - INFO - joeynmt.training - Epoch  12, Step:    75600, Batch Loss:     2.444265, Tokens per Sec:    14152, Lr: 0.000300\n",
      "2021-05-25 08:42:05,167 - INFO - joeynmt.training - Epoch  12, Step:    75700, Batch Loss:     2.260739, Tokens per Sec:    13903, Lr: 0.000300\n",
      "2021-05-25 08:42:18,457 - INFO - joeynmt.training - Epoch  12, Step:    75800, Batch Loss:     1.935963, Tokens per Sec:    14446, Lr: 0.000300\n",
      "2021-05-25 08:42:31,425 - INFO - joeynmt.training - Epoch  12, Step:    75900, Batch Loss:     1.962792, Tokens per Sec:    14100, Lr: 0.000300\n",
      "2021-05-25 08:42:44,443 - INFO - joeynmt.training - Epoch  12, Step:    76000, Batch Loss:     2.160359, Tokens per Sec:    14168, Lr: 0.000300\n",
      "2021-05-25 08:42:57,621 - INFO - joeynmt.training - Epoch  12, Step:    76100, Batch Loss:     2.280688, Tokens per Sec:    14448, Lr: 0.000300\n",
      "2021-05-25 08:43:10,594 - INFO - joeynmt.training - Epoch  12, Step:    76200, Batch Loss:     2.132938, Tokens per Sec:    14213, Lr: 0.000300\n",
      "2021-05-25 08:43:23,676 - INFO - joeynmt.training - Epoch  12, Step:    76300, Batch Loss:     2.185790, Tokens per Sec:    14442, Lr: 0.000300\n",
      "2021-05-25 08:43:36,706 - INFO - joeynmt.training - Epoch  12, Step:    76400, Batch Loss:     2.237736, Tokens per Sec:    14694, Lr: 0.000300\n",
      "2021-05-25 08:43:49,580 - INFO - joeynmt.training - Epoch  12, Step:    76500, Batch Loss:     2.328306, Tokens per Sec:    14511, Lr: 0.000300\n",
      "2021-05-25 08:44:02,369 - INFO - joeynmt.training - Epoch  12, Step:    76600, Batch Loss:     2.073013, Tokens per Sec:    14268, Lr: 0.000300\n",
      "2021-05-25 08:44:15,213 - INFO - joeynmt.training - Epoch  12, Step:    76700, Batch Loss:     2.089851, Tokens per Sec:    14082, Lr: 0.000300\n",
      "2021-05-25 08:44:28,356 - INFO - joeynmt.training - Epoch  12, Step:    76800, Batch Loss:     2.046794, Tokens per Sec:    14192, Lr: 0.000300\n",
      "2021-05-25 08:44:41,485 - INFO - joeynmt.training - Epoch  12, Step:    76900, Batch Loss:     2.075210, Tokens per Sec:    14197, Lr: 0.000300\n",
      "2021-05-25 08:44:54,440 - INFO - joeynmt.training - Epoch  12, Step:    77000, Batch Loss:     2.046050, Tokens per Sec:    13836, Lr: 0.000300\n",
      "2021-05-25 08:45:07,529 - INFO - joeynmt.training - Epoch  12, Step:    77100, Batch Loss:     2.138388, Tokens per Sec:    14365, Lr: 0.000300\n",
      "2021-05-25 08:45:20,753 - INFO - joeynmt.training - Epoch  12, Step:    77200, Batch Loss:     2.179498, Tokens per Sec:    14322, Lr: 0.000300\n",
      "2021-05-25 08:45:33,586 - INFO - joeynmt.training - Epoch  12, Step:    77300, Batch Loss:     2.350741, Tokens per Sec:    13572, Lr: 0.000300\n",
      "2021-05-25 08:45:46,550 - INFO - joeynmt.training - Epoch  12, Step:    77400, Batch Loss:     2.246277, Tokens per Sec:    14077, Lr: 0.000300\n",
      "2021-05-25 08:45:59,686 - INFO - joeynmt.training - Epoch  12, Step:    77500, Batch Loss:     2.086045, Tokens per Sec:    14094, Lr: 0.000300\n",
      "2021-05-25 08:46:12,653 - INFO - joeynmt.training - Epoch  12, Step:    77600, Batch Loss:     2.306834, Tokens per Sec:    14279, Lr: 0.000300\n",
      "2021-05-25 08:46:25,700 - INFO - joeynmt.training - Epoch  12, Step:    77700, Batch Loss:     2.249637, Tokens per Sec:    14005, Lr: 0.000300\n",
      "2021-05-25 08:46:32,987 - INFO - joeynmt.training - Epoch  12: total training loss 14194.64\n",
      "2021-05-25 08:46:32,987 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-05-25 08:46:39,641 - INFO - joeynmt.training - Epoch  13, Step:    77800, Batch Loss:     2.188130, Tokens per Sec:    12563, Lr: 0.000300\n",
      "2021-05-25 08:46:52,717 - INFO - joeynmt.training - Epoch  13, Step:    77900, Batch Loss:     2.042279, Tokens per Sec:    14000, Lr: 0.000300\n",
      "2021-05-25 08:47:06,123 - INFO - joeynmt.training - Epoch  13, Step:    78000, Batch Loss:     2.291775, Tokens per Sec:    14282, Lr: 0.000300\n",
      "2021-05-25 08:47:19,306 - INFO - joeynmt.training - Epoch  13, Step:    78100, Batch Loss:     2.156337, Tokens per Sec:    14549, Lr: 0.000300\n",
      "2021-05-25 08:47:32,365 - INFO - joeynmt.training - Epoch  13, Step:    78200, Batch Loss:     2.124449, Tokens per Sec:    14577, Lr: 0.000300\n",
      "2021-05-25 08:47:45,369 - INFO - joeynmt.training - Epoch  13, Step:    78300, Batch Loss:     2.117736, Tokens per Sec:    14834, Lr: 0.000300\n",
      "2021-05-25 08:47:58,515 - INFO - joeynmt.training - Epoch  13, Step:    78400, Batch Loss:     2.314633, Tokens per Sec:    14129, Lr: 0.000300\n",
      "2021-05-25 08:48:11,620 - INFO - joeynmt.training - Epoch  13, Step:    78500, Batch Loss:     2.229200, Tokens per Sec:    13943, Lr: 0.000300\n",
      "2021-05-25 08:48:24,631 - INFO - joeynmt.training - Epoch  13, Step:    78600, Batch Loss:     2.361252, Tokens per Sec:    13916, Lr: 0.000300\n",
      "2021-05-25 08:48:37,846 - INFO - joeynmt.training - Epoch  13, Step:    78700, Batch Loss:     2.038800, Tokens per Sec:    14188, Lr: 0.000300\n",
      "2021-05-25 08:48:51,176 - INFO - joeynmt.training - Epoch  13, Step:    78800, Batch Loss:     2.501879, Tokens per Sec:    14179, Lr: 0.000300\n",
      "2021-05-25 08:49:04,337 - INFO - joeynmt.training - Epoch  13, Step:    78900, Batch Loss:     2.324052, Tokens per Sec:    14018, Lr: 0.000300\n",
      "2021-05-25 08:49:17,384 - INFO - joeynmt.training - Epoch  13, Step:    79000, Batch Loss:     2.148783, Tokens per Sec:    14718, Lr: 0.000300\n",
      "2021-05-25 08:49:30,274 - INFO - joeynmt.training - Epoch  13, Step:    79100, Batch Loss:     2.006113, Tokens per Sec:    14640, Lr: 0.000300\n",
      "2021-05-25 08:49:43,258 - INFO - joeynmt.training - Epoch  13, Step:    79200, Batch Loss:     2.235013, Tokens per Sec:    14379, Lr: 0.000300\n",
      "2021-05-25 08:49:56,112 - INFO - joeynmt.training - Epoch  13, Step:    79300, Batch Loss:     2.043210, Tokens per Sec:    14382, Lr: 0.000300\n",
      "2021-05-25 08:50:09,221 - INFO - joeynmt.training - Epoch  13, Step:    79400, Batch Loss:     1.994254, Tokens per Sec:    13999, Lr: 0.000300\n",
      "2021-05-25 08:50:22,336 - INFO - joeynmt.training - Epoch  13, Step:    79500, Batch Loss:     2.268151, Tokens per Sec:    13959, Lr: 0.000300\n",
      "2021-05-25 08:50:35,548 - INFO - joeynmt.training - Epoch  13, Step:    79600, Batch Loss:     2.262379, Tokens per Sec:    14491, Lr: 0.000300\n",
      "2021-05-25 08:50:48,450 - INFO - joeynmt.training - Epoch  13, Step:    79700, Batch Loss:     2.053988, Tokens per Sec:    13546, Lr: 0.000300\n",
      "2021-05-25 08:51:01,599 - INFO - joeynmt.training - Epoch  13, Step:    79800, Batch Loss:     2.267680, Tokens per Sec:    13809, Lr: 0.000300\n",
      "2021-05-25 08:51:14,719 - INFO - joeynmt.training - Epoch  13, Step:    79900, Batch Loss:     1.982146, Tokens per Sec:    14105, Lr: 0.000300\n",
      "2021-05-25 08:51:27,768 - INFO - joeynmt.training - Epoch  13, Step:    80000, Batch Loss:     2.209328, Tokens per Sec:    13706, Lr: 0.000300\n",
      "2021-05-25 08:51:53,736 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 08:51:53,736 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 08:51:53,737 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 08:51:54,002 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 08:51:54,002 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 08:51:55,108 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 08:51:55,109 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 08:51:55,109 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 08:51:55,109 - INFO - joeynmt.training - \tHypothesis: I felt sincere .\n",
      "2021-05-25 08:51:55,109 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 08:51:55,109 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 08:51:55,110 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 08:51:55,110 - INFO - joeynmt.training - \tHypothesis: The writer wrote in the front of the scroll .\n",
      "2021-05-25 08:51:55,110 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 08:51:55,110 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 08:51:55,110 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:51:55,111 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or discernment , we should continue to exercise faith in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 08:51:55,111 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 08:51:55,111 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 08:51:55,112 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 08:51:55,112 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being successful in Satan’s world .\n",
      "2021-05-25 08:51:55,112 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    80000: bleu:  22.69, loss: 50243.0117, ppl:   6.1031, duration: 27.3433s\n",
      "2021-05-25 08:52:08,349 - INFO - joeynmt.training - Epoch  13, Step:    80100, Batch Loss:     2.013955, Tokens per Sec:    14173, Lr: 0.000300\n",
      "2021-05-25 08:52:21,311 - INFO - joeynmt.training - Epoch  13, Step:    80200, Batch Loss:     2.200795, Tokens per Sec:    13805, Lr: 0.000300\n",
      "2021-05-25 08:52:34,322 - INFO - joeynmt.training - Epoch  13, Step:    80300, Batch Loss:     1.948193, Tokens per Sec:    13951, Lr: 0.000300\n",
      "2021-05-25 08:52:47,169 - INFO - joeynmt.training - Epoch  13, Step:    80400, Batch Loss:     2.058191, Tokens per Sec:    14130, Lr: 0.000300\n",
      "2021-05-25 08:53:00,113 - INFO - joeynmt.training - Epoch  13, Step:    80500, Batch Loss:     2.254238, Tokens per Sec:    14281, Lr: 0.000300\n",
      "2021-05-25 08:53:12,971 - INFO - joeynmt.training - Epoch  13, Step:    80600, Batch Loss:     2.323251, Tokens per Sec:    14290, Lr: 0.000300\n",
      "2021-05-25 08:53:25,815 - INFO - joeynmt.training - Epoch  13, Step:    80700, Batch Loss:     2.231637, Tokens per Sec:    13983, Lr: 0.000300\n",
      "2021-05-25 08:53:38,664 - INFO - joeynmt.training - Epoch  13, Step:    80800, Batch Loss:     1.921789, Tokens per Sec:    13943, Lr: 0.000300\n",
      "2021-05-25 08:53:51,832 - INFO - joeynmt.training - Epoch  13, Step:    80900, Batch Loss:     2.303290, Tokens per Sec:    14211, Lr: 0.000300\n",
      "2021-05-25 08:54:04,772 - INFO - joeynmt.training - Epoch  13, Step:    81000, Batch Loss:     2.212160, Tokens per Sec:    14035, Lr: 0.000300\n",
      "2021-05-25 08:54:17,695 - INFO - joeynmt.training - Epoch  13, Step:    81100, Batch Loss:     2.245908, Tokens per Sec:    13820, Lr: 0.000300\n",
      "2021-05-25 08:54:30,826 - INFO - joeynmt.training - Epoch  13, Step:    81200, Batch Loss:     2.146897, Tokens per Sec:    14128, Lr: 0.000300\n",
      "2021-05-25 08:54:43,809 - INFO - joeynmt.training - Epoch  13, Step:    81300, Batch Loss:     2.070754, Tokens per Sec:    13814, Lr: 0.000300\n",
      "2021-05-25 08:54:56,785 - INFO - joeynmt.training - Epoch  13, Step:    81400, Batch Loss:     2.107790, Tokens per Sec:    13586, Lr: 0.000300\n",
      "2021-05-25 08:55:10,038 - INFO - joeynmt.training - Epoch  13, Step:    81500, Batch Loss:     2.150556, Tokens per Sec:    14342, Lr: 0.000300\n",
      "2021-05-25 08:55:23,299 - INFO - joeynmt.training - Epoch  13, Step:    81600, Batch Loss:     2.232341, Tokens per Sec:    14133, Lr: 0.000300\n",
      "2021-05-25 08:55:36,255 - INFO - joeynmt.training - Epoch  13, Step:    81700, Batch Loss:     2.173661, Tokens per Sec:    14035, Lr: 0.000300\n",
      "2021-05-25 08:55:49,375 - INFO - joeynmt.training - Epoch  13, Step:    81800, Batch Loss:     2.007525, Tokens per Sec:    13974, Lr: 0.000300\n",
      "2021-05-25 08:56:02,436 - INFO - joeynmt.training - Epoch  13, Step:    81900, Batch Loss:     2.175030, Tokens per Sec:    13992, Lr: 0.000300\n",
      "2021-05-25 08:56:15,640 - INFO - joeynmt.training - Epoch  13, Step:    82000, Batch Loss:     2.206028, Tokens per Sec:    14218, Lr: 0.000300\n",
      "2021-05-25 08:56:28,865 - INFO - joeynmt.training - Epoch  13, Step:    82100, Batch Loss:     2.255464, Tokens per Sec:    14144, Lr: 0.000300\n",
      "2021-05-25 08:56:41,593 - INFO - joeynmt.training - Epoch  13, Step:    82200, Batch Loss:     2.240255, Tokens per Sec:    13851, Lr: 0.000300\n",
      "2021-05-25 08:56:54,256 - INFO - joeynmt.training - Epoch  13, Step:    82300, Batch Loss:     2.133914, Tokens per Sec:    13992, Lr: 0.000300\n",
      "2021-05-25 08:57:07,162 - INFO - joeynmt.training - Epoch  13, Step:    82400, Batch Loss:     2.225307, Tokens per Sec:    14382, Lr: 0.000300\n",
      "2021-05-25 08:57:19,900 - INFO - joeynmt.training - Epoch  13, Step:    82500, Batch Loss:     2.180777, Tokens per Sec:    13988, Lr: 0.000300\n",
      "2021-05-25 08:57:32,961 - INFO - joeynmt.training - Epoch  13, Step:    82600, Batch Loss:     2.263383, Tokens per Sec:    14277, Lr: 0.000300\n",
      "2021-05-25 08:57:45,902 - INFO - joeynmt.training - Epoch  13, Step:    82700, Batch Loss:     2.339125, Tokens per Sec:    13754, Lr: 0.000300\n",
      "2021-05-25 08:57:58,922 - INFO - joeynmt.training - Epoch  13, Step:    82800, Batch Loss:     2.213318, Tokens per Sec:    14089, Lr: 0.000300\n",
      "2021-05-25 08:58:12,244 - INFO - joeynmt.training - Epoch  13, Step:    82900, Batch Loss:     2.164416, Tokens per Sec:    14313, Lr: 0.000300\n",
      "2021-05-25 08:58:25,285 - INFO - joeynmt.training - Epoch  13, Step:    83000, Batch Loss:     2.127480, Tokens per Sec:    13896, Lr: 0.000300\n",
      "2021-05-25 08:58:38,306 - INFO - joeynmt.training - Epoch  13, Step:    83100, Batch Loss:     2.180125, Tokens per Sec:    14210, Lr: 0.000300\n",
      "2021-05-25 08:58:51,400 - INFO - joeynmt.training - Epoch  13, Step:    83200, Batch Loss:     2.175151, Tokens per Sec:    14024, Lr: 0.000300\n",
      "2021-05-25 08:59:04,566 - INFO - joeynmt.training - Epoch  13, Step:    83300, Batch Loss:     2.183190, Tokens per Sec:    14044, Lr: 0.000300\n",
      "2021-05-25 08:59:17,485 - INFO - joeynmt.training - Epoch  13, Step:    83400, Batch Loss:     2.122234, Tokens per Sec:    13750, Lr: 0.000300\n",
      "2021-05-25 08:59:30,601 - INFO - joeynmt.training - Epoch  13, Step:    83500, Batch Loss:     2.357408, Tokens per Sec:    14051, Lr: 0.000300\n",
      "2021-05-25 08:59:44,046 - INFO - joeynmt.training - Epoch  13, Step:    83600, Batch Loss:     2.118965, Tokens per Sec:    14623, Lr: 0.000300\n",
      "2021-05-25 08:59:57,152 - INFO - joeynmt.training - Epoch  13, Step:    83700, Batch Loss:     2.000036, Tokens per Sec:    13755, Lr: 0.000300\n",
      "2021-05-25 09:00:10,376 - INFO - joeynmt.training - Epoch  13, Step:    83800, Batch Loss:     1.997335, Tokens per Sec:    14710, Lr: 0.000300\n",
      "2021-05-25 09:00:23,240 - INFO - joeynmt.training - Epoch  13, Step:    83900, Batch Loss:     1.968930, Tokens per Sec:    14025, Lr: 0.000300\n",
      "2021-05-25 09:00:36,044 - INFO - joeynmt.training - Epoch  13, Step:    84000, Batch Loss:     2.228977, Tokens per Sec:    14324, Lr: 0.000300\n",
      "2021-05-25 09:00:49,014 - INFO - joeynmt.training - Epoch  13, Step:    84100, Batch Loss:     2.097538, Tokens per Sec:    14186, Lr: 0.000300\n",
      "2021-05-25 09:01:02,004 - INFO - joeynmt.training - Epoch  13, Step:    84200, Batch Loss:     2.065789, Tokens per Sec:    14288, Lr: 0.000300\n",
      "2021-05-25 09:01:05,104 - INFO - joeynmt.training - Epoch  13: total training loss 14042.38\n",
      "2021-05-25 09:01:05,104 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-05-25 09:01:15,787 - INFO - joeynmt.training - Epoch  14, Step:    84300, Batch Loss:     2.266055, Tokens per Sec:    13199, Lr: 0.000300\n",
      "2021-05-25 09:01:28,841 - INFO - joeynmt.training - Epoch  14, Step:    84400, Batch Loss:     2.242947, Tokens per Sec:    13927, Lr: 0.000300\n",
      "2021-05-25 09:01:41,838 - INFO - joeynmt.training - Epoch  14, Step:    84500, Batch Loss:     2.285459, Tokens per Sec:    13812, Lr: 0.000300\n",
      "2021-05-25 09:01:54,753 - INFO - joeynmt.training - Epoch  14, Step:    84600, Batch Loss:     2.181477, Tokens per Sec:    13930, Lr: 0.000300\n",
      "2021-05-25 09:02:07,786 - INFO - joeynmt.training - Epoch  14, Step:    84700, Batch Loss:     2.243740, Tokens per Sec:    13965, Lr: 0.000300\n",
      "2021-05-25 09:02:20,858 - INFO - joeynmt.training - Epoch  14, Step:    84800, Batch Loss:     2.153328, Tokens per Sec:    14079, Lr: 0.000300\n",
      "2021-05-25 09:02:33,897 - INFO - joeynmt.training - Epoch  14, Step:    84900, Batch Loss:     2.234104, Tokens per Sec:    13970, Lr: 0.000300\n",
      "2021-05-25 09:02:46,729 - INFO - joeynmt.training - Epoch  14, Step:    85000, Batch Loss:     2.038325, Tokens per Sec:    13550, Lr: 0.000300\n",
      "2021-05-25 09:03:09,931 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 09:03:09,932 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 09:03:09,932 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 09:03:10,199 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 09:03:10,199 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 09:03:10,961 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 09:03:10,962 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 09:03:10,963 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 09:03:10,963 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 09:03:10,963 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 09:03:10,963 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 09:03:10,964 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 09:03:10,964 - INFO - joeynmt.training - \tHypothesis: The writer wrote in the front of the scroll .\n",
      "2021-05-25 09:03:10,964 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 09:03:10,964 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 09:03:10,965 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 09:03:10,965 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or discernment , we should continue our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 09:03:10,965 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 09:03:10,965 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 09:03:10,966 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 09:03:10,966 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being released in Satan’s world .\n",
      "2021-05-25 09:03:10,966 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    85000: bleu:  23.31, loss: 49710.8984, ppl:   5.9873, duration: 24.2366s\n",
      "2021-05-25 09:03:24,575 - INFO - joeynmt.training - Epoch  14, Step:    85100, Batch Loss:     2.069280, Tokens per Sec:    13604, Lr: 0.000300\n",
      "2021-05-25 09:03:37,630 - INFO - joeynmt.training - Epoch  14, Step:    85200, Batch Loss:     2.326301, Tokens per Sec:    14224, Lr: 0.000300\n",
      "2021-05-25 09:03:50,713 - INFO - joeynmt.training - Epoch  14, Step:    85300, Batch Loss:     2.256216, Tokens per Sec:    14125, Lr: 0.000300\n",
      "2021-05-25 09:04:03,669 - INFO - joeynmt.training - Epoch  14, Step:    85400, Batch Loss:     2.151040, Tokens per Sec:    13863, Lr: 0.000300\n",
      "2021-05-25 09:04:16,839 - INFO - joeynmt.training - Epoch  14, Step:    85500, Batch Loss:     2.230429, Tokens per Sec:    14220, Lr: 0.000300\n",
      "2021-05-25 09:04:29,734 - INFO - joeynmt.training - Epoch  14, Step:    85600, Batch Loss:     2.153679, Tokens per Sec:    14801, Lr: 0.000300\n",
      "2021-05-25 09:04:42,640 - INFO - joeynmt.training - Epoch  14, Step:    85700, Batch Loss:     2.128858, Tokens per Sec:    14228, Lr: 0.000300\n",
      "2021-05-25 09:04:55,559 - INFO - joeynmt.training - Epoch  14, Step:    85800, Batch Loss:     2.043806, Tokens per Sec:    14403, Lr: 0.000300\n",
      "2021-05-25 09:05:08,290 - INFO - joeynmt.training - Epoch  14, Step:    85900, Batch Loss:     2.357224, Tokens per Sec:    13765, Lr: 0.000300\n",
      "2021-05-25 09:05:21,442 - INFO - joeynmt.training - Epoch  14, Step:    86000, Batch Loss:     2.755588, Tokens per Sec:    14515, Lr: 0.000300\n",
      "2021-05-25 09:05:34,466 - INFO - joeynmt.training - Epoch  14, Step:    86100, Batch Loss:     2.212634, Tokens per Sec:    13968, Lr: 0.000300\n",
      "2021-05-25 09:05:47,647 - INFO - joeynmt.training - Epoch  14, Step:    86200, Batch Loss:     2.049791, Tokens per Sec:    14446, Lr: 0.000300\n",
      "2021-05-25 09:06:00,668 - INFO - joeynmt.training - Epoch  14, Step:    86300, Batch Loss:     2.303973, Tokens per Sec:    13857, Lr: 0.000300\n",
      "2021-05-25 09:06:13,759 - INFO - joeynmt.training - Epoch  14, Step:    86400, Batch Loss:     2.293804, Tokens per Sec:    14213, Lr: 0.000300\n",
      "2021-05-25 09:06:26,789 - INFO - joeynmt.training - Epoch  14, Step:    86500, Batch Loss:     2.169177, Tokens per Sec:    13730, Lr: 0.000300\n",
      "2021-05-25 09:06:39,884 - INFO - joeynmt.training - Epoch  14, Step:    86600, Batch Loss:     2.106596, Tokens per Sec:    14097, Lr: 0.000300\n",
      "2021-05-25 09:06:52,991 - INFO - joeynmt.training - Epoch  14, Step:    86700, Batch Loss:     2.095965, Tokens per Sec:    13861, Lr: 0.000300\n",
      "2021-05-25 09:07:06,068 - INFO - joeynmt.training - Epoch  14, Step:    86800, Batch Loss:     2.155883, Tokens per Sec:    14048, Lr: 0.000300\n",
      "2021-05-25 09:07:19,170 - INFO - joeynmt.training - Epoch  14, Step:    86900, Batch Loss:     2.162911, Tokens per Sec:    14121, Lr: 0.000300\n",
      "2021-05-25 09:07:32,145 - INFO - joeynmt.training - Epoch  14, Step:    87000, Batch Loss:     2.011399, Tokens per Sec:    13903, Lr: 0.000300\n",
      "2021-05-25 09:07:45,389 - INFO - joeynmt.training - Epoch  14, Step:    87100, Batch Loss:     2.030506, Tokens per Sec:    14359, Lr: 0.000300\n",
      "2021-05-25 09:07:58,425 - INFO - joeynmt.training - Epoch  14, Step:    87200, Batch Loss:     1.960316, Tokens per Sec:    14364, Lr: 0.000300\n",
      "2021-05-25 09:08:11,416 - INFO - joeynmt.training - Epoch  14, Step:    87300, Batch Loss:     2.197612, Tokens per Sec:    13743, Lr: 0.000300\n",
      "2021-05-25 09:08:24,376 - INFO - joeynmt.training - Epoch  14, Step:    87400, Batch Loss:     1.953044, Tokens per Sec:    14034, Lr: 0.000300\n",
      "2021-05-25 09:08:37,478 - INFO - joeynmt.training - Epoch  14, Step:    87500, Batch Loss:     2.199221, Tokens per Sec:    14329, Lr: 0.000300\n",
      "2021-05-25 09:08:50,541 - INFO - joeynmt.training - Epoch  14, Step:    87600, Batch Loss:     2.147537, Tokens per Sec:    14071, Lr: 0.000300\n",
      "2021-05-25 09:09:03,425 - INFO - joeynmt.training - Epoch  14, Step:    87700, Batch Loss:     2.216318, Tokens per Sec:    14134, Lr: 0.000300\n",
      "2021-05-25 09:09:16,056 - INFO - joeynmt.training - Epoch  14, Step:    87800, Batch Loss:     2.167244, Tokens per Sec:    14074, Lr: 0.000300\n",
      "2021-05-25 09:09:28,822 - INFO - joeynmt.training - Epoch  14, Step:    87900, Batch Loss:     2.015168, Tokens per Sec:    14246, Lr: 0.000300\n",
      "2021-05-25 09:09:41,869 - INFO - joeynmt.training - Epoch  14, Step:    88000, Batch Loss:     2.192530, Tokens per Sec:    14905, Lr: 0.000300\n",
      "2021-05-25 09:09:54,732 - INFO - joeynmt.training - Epoch  14, Step:    88100, Batch Loss:     2.073668, Tokens per Sec:    14311, Lr: 0.000300\n",
      "2021-05-25 09:10:07,743 - INFO - joeynmt.training - Epoch  14, Step:    88200, Batch Loss:     2.109046, Tokens per Sec:    13718, Lr: 0.000300\n",
      "2021-05-25 09:10:20,524 - INFO - joeynmt.training - Epoch  14, Step:    88300, Batch Loss:     2.083238, Tokens per Sec:    13836, Lr: 0.000300\n",
      "2021-05-25 09:10:33,735 - INFO - joeynmt.training - Epoch  14, Step:    88400, Batch Loss:     2.297340, Tokens per Sec:    14329, Lr: 0.000300\n",
      "2021-05-25 09:10:46,793 - INFO - joeynmt.training - Epoch  14, Step:    88500, Batch Loss:     2.060231, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 09:10:59,899 - INFO - joeynmt.training - Epoch  14, Step:    88600, Batch Loss:     2.160882, Tokens per Sec:    14352, Lr: 0.000300\n",
      "2021-05-25 09:11:12,882 - INFO - joeynmt.training - Epoch  14, Step:    88700, Batch Loss:     2.086257, Tokens per Sec:    13930, Lr: 0.000300\n",
      "2021-05-25 09:11:26,039 - INFO - joeynmt.training - Epoch  14, Step:    88800, Batch Loss:     2.124422, Tokens per Sec:    14135, Lr: 0.000300\n",
      "2021-05-25 09:11:39,131 - INFO - joeynmt.training - Epoch  14, Step:    88900, Batch Loss:     2.037355, Tokens per Sec:    14269, Lr: 0.000300\n",
      "2021-05-25 09:11:52,409 - INFO - joeynmt.training - Epoch  14, Step:    89000, Batch Loss:     2.020620, Tokens per Sec:    14200, Lr: 0.000300\n",
      "2021-05-25 09:12:05,530 - INFO - joeynmt.training - Epoch  14, Step:    89100, Batch Loss:     2.250646, Tokens per Sec:    13861, Lr: 0.000300\n",
      "2021-05-25 09:12:18,603 - INFO - joeynmt.training - Epoch  14, Step:    89200, Batch Loss:     2.194295, Tokens per Sec:    13976, Lr: 0.000300\n",
      "2021-05-25 09:12:31,715 - INFO - joeynmt.training - Epoch  14, Step:    89300, Batch Loss:     2.028461, Tokens per Sec:    14215, Lr: 0.000300\n",
      "2021-05-25 09:12:44,680 - INFO - joeynmt.training - Epoch  14, Step:    89400, Batch Loss:     2.272304, Tokens per Sec:    14131, Lr: 0.000300\n",
      "2021-05-25 09:12:57,534 - INFO - joeynmt.training - Epoch  14, Step:    89500, Batch Loss:     2.199157, Tokens per Sec:    14449, Lr: 0.000300\n",
      "2021-05-25 09:13:10,261 - INFO - joeynmt.training - Epoch  14, Step:    89600, Batch Loss:     2.317033, Tokens per Sec:    14250, Lr: 0.000300\n",
      "2021-05-25 09:13:23,171 - INFO - joeynmt.training - Epoch  14, Step:    89700, Batch Loss:     1.984696, Tokens per Sec:    14558, Lr: 0.000300\n",
      "2021-05-25 09:13:36,216 - INFO - joeynmt.training - Epoch  14, Step:    89800, Batch Loss:     2.269729, Tokens per Sec:    14650, Lr: 0.000300\n",
      "2021-05-25 09:13:49,226 - INFO - joeynmt.training - Epoch  14, Step:    89900, Batch Loss:     2.224760, Tokens per Sec:    13938, Lr: 0.000300\n",
      "2021-05-25 09:14:02,263 - INFO - joeynmt.training - Epoch  14, Step:    90000, Batch Loss:     1.980725, Tokens per Sec:    14017, Lr: 0.000300\n",
      "2021-05-25 09:14:26,921 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 09:14:26,922 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 09:14:26,922 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 09:14:27,182 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 09:14:27,182 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 09:14:27,927 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 09:14:27,928 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 09:14:27,928 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 09:14:27,928 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 09:14:27,928 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 09:14:27,929 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 09:14:27,929 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 09:14:27,929 - INFO - joeynmt.training - \tHypothesis: The letter wrote in the middle of the scroll .\n",
      "2021-05-25 09:14:27,929 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 09:14:27,930 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 09:14:27,930 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 09:14:27,930 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or discernment , we should continue our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 09:14:27,930 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 09:14:27,931 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 09:14:27,931 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 09:14:27,931 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of being successful in Satan’s world .\n",
      "2021-05-25 09:14:27,931 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    90000: bleu:  23.26, loss: 49284.9414, ppl:   5.8962, duration: 25.6682s\n",
      "2021-05-25 09:14:41,268 - INFO - joeynmt.training - Epoch  14, Step:    90100, Batch Loss:     2.262465, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 09:14:54,494 - INFO - joeynmt.training - Epoch  14, Step:    90200, Batch Loss:     2.035874, Tokens per Sec:    14273, Lr: 0.000300\n",
      "2021-05-25 09:15:07,517 - INFO - joeynmt.training - Epoch  14, Step:    90300, Batch Loss:     2.101702, Tokens per Sec:    14056, Lr: 0.000300\n",
      "2021-05-25 09:15:20,635 - INFO - joeynmt.training - Epoch  14, Step:    90400, Batch Loss:     2.169464, Tokens per Sec:    14211, Lr: 0.000300\n",
      "2021-05-25 09:15:33,484 - INFO - joeynmt.training - Epoch  14, Step:    90500, Batch Loss:     2.379031, Tokens per Sec:    13666, Lr: 0.000300\n",
      "2021-05-25 09:15:46,863 - INFO - joeynmt.training - Epoch  14, Step:    90600, Batch Loss:     2.005938, Tokens per Sec:    14509, Lr: 0.000300\n",
      "2021-05-25 09:15:59,899 - INFO - joeynmt.training - Epoch  14, Step:    90700, Batch Loss:     2.199856, Tokens per Sec:    13916, Lr: 0.000300\n",
      "2021-05-25 09:16:00,091 - INFO - joeynmt.training - Epoch  14: total training loss 13936.95\n",
      "2021-05-25 09:16:00,091 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-05-25 09:16:13,806 - INFO - joeynmt.training - Epoch  15, Step:    90800, Batch Loss:     2.220921, Tokens per Sec:    13574, Lr: 0.000300\n",
      "2021-05-25 09:16:26,913 - INFO - joeynmt.training - Epoch  15, Step:    90900, Batch Loss:     2.201406, Tokens per Sec:    14173, Lr: 0.000300\n",
      "2021-05-25 09:16:39,764 - INFO - joeynmt.training - Epoch  15, Step:    91000, Batch Loss:     2.255852, Tokens per Sec:    14507, Lr: 0.000300\n",
      "2021-05-25 09:16:52,405 - INFO - joeynmt.training - Epoch  15, Step:    91100, Batch Loss:     2.172692, Tokens per Sec:    14045, Lr: 0.000300\n",
      "2021-05-25 09:17:05,188 - INFO - joeynmt.training - Epoch  15, Step:    91200, Batch Loss:     2.106120, Tokens per Sec:    14664, Lr: 0.000300\n",
      "2021-05-25 09:17:18,064 - INFO - joeynmt.training - Epoch  15, Step:    91300, Batch Loss:     2.079336, Tokens per Sec:    14433, Lr: 0.000300\n",
      "2021-05-25 09:17:31,282 - INFO - joeynmt.training - Epoch  15, Step:    91400, Batch Loss:     2.144470, Tokens per Sec:    14374, Lr: 0.000300\n",
      "2021-05-25 09:17:44,201 - INFO - joeynmt.training - Epoch  15, Step:    91500, Batch Loss:     2.022316, Tokens per Sec:    13972, Lr: 0.000300\n",
      "2021-05-25 09:17:57,368 - INFO - joeynmt.training - Epoch  15, Step:    91600, Batch Loss:     2.266484, Tokens per Sec:    14150, Lr: 0.000300\n",
      "2021-05-25 09:18:10,129 - INFO - joeynmt.training - Epoch  15, Step:    91700, Batch Loss:     2.089258, Tokens per Sec:    13610, Lr: 0.000300\n",
      "2021-05-25 09:18:23,312 - INFO - joeynmt.training - Epoch  15, Step:    91800, Batch Loss:     2.296905, Tokens per Sec:    14319, Lr: 0.000300\n",
      "2021-05-25 09:18:36,341 - INFO - joeynmt.training - Epoch  15, Step:    91900, Batch Loss:     2.079051, Tokens per Sec:    14462, Lr: 0.000300\n",
      "2021-05-25 09:18:49,232 - INFO - joeynmt.training - Epoch  15, Step:    92000, Batch Loss:     2.095740, Tokens per Sec:    13842, Lr: 0.000300\n",
      "2021-05-25 09:19:02,226 - INFO - joeynmt.training - Epoch  15, Step:    92100, Batch Loss:     2.055410, Tokens per Sec:    13657, Lr: 0.000300\n",
      "2021-05-25 09:19:15,279 - INFO - joeynmt.training - Epoch  15, Step:    92200, Batch Loss:     2.017207, Tokens per Sec:    13998, Lr: 0.000300\n",
      "2021-05-25 09:19:28,380 - INFO - joeynmt.training - Epoch  15, Step:    92300, Batch Loss:     2.098133, Tokens per Sec:    13996, Lr: 0.000300\n",
      "2021-05-25 09:19:41,429 - INFO - joeynmt.training - Epoch  15, Step:    92400, Batch Loss:     2.004719, Tokens per Sec:    14351, Lr: 0.000300\n",
      "2021-05-25 09:19:54,393 - INFO - joeynmt.training - Epoch  15, Step:    92500, Batch Loss:     2.112954, Tokens per Sec:    14105, Lr: 0.000300\n",
      "2021-05-25 09:20:07,373 - INFO - joeynmt.training - Epoch  15, Step:    92600, Batch Loss:     2.021926, Tokens per Sec:    14117, Lr: 0.000300\n",
      "2021-05-25 09:20:20,325 - INFO - joeynmt.training - Epoch  15, Step:    92700, Batch Loss:     1.963660, Tokens per Sec:    14070, Lr: 0.000300\n",
      "2021-05-25 09:20:33,182 - INFO - joeynmt.training - Epoch  15, Step:    92800, Batch Loss:     2.212063, Tokens per Sec:    14230, Lr: 0.000300\n",
      "2021-05-25 09:20:45,963 - INFO - joeynmt.training - Epoch  15, Step:    92900, Batch Loss:     2.019658, Tokens per Sec:    14196, Lr: 0.000300\n",
      "2021-05-25 09:20:58,505 - INFO - joeynmt.training - Epoch  15, Step:    93000, Batch Loss:     2.121695, Tokens per Sec:    14145, Lr: 0.000300\n",
      "2021-05-25 09:21:11,479 - INFO - joeynmt.training - Epoch  15, Step:    93100, Batch Loss:     2.167032, Tokens per Sec:    14113, Lr: 0.000300\n",
      "2021-05-25 09:21:24,651 - INFO - joeynmt.training - Epoch  15, Step:    93200, Batch Loss:     2.065676, Tokens per Sec:    14397, Lr: 0.000300\n",
      "2021-05-25 09:21:37,686 - INFO - joeynmt.training - Epoch  15, Step:    93300, Batch Loss:     1.917204, Tokens per Sec:    14057, Lr: 0.000300\n",
      "2021-05-25 09:21:50,655 - INFO - joeynmt.training - Epoch  15, Step:    93400, Batch Loss:     1.985508, Tokens per Sec:    13972, Lr: 0.000300\n",
      "2021-05-25 09:22:03,654 - INFO - joeynmt.training - Epoch  15, Step:    93500, Batch Loss:     2.386403, Tokens per Sec:    13761, Lr: 0.000300\n",
      "2021-05-25 09:22:16,783 - INFO - joeynmt.training - Epoch  15, Step:    93600, Batch Loss:     2.043709, Tokens per Sec:    14316, Lr: 0.000300\n",
      "2021-05-25 09:22:29,756 - INFO - joeynmt.training - Epoch  15, Step:    93700, Batch Loss:     2.292921, Tokens per Sec:    13964, Lr: 0.000300\n",
      "2021-05-25 09:22:42,814 - INFO - joeynmt.training - Epoch  15, Step:    93800, Batch Loss:     2.253095, Tokens per Sec:    14092, Lr: 0.000300\n",
      "2021-05-25 09:22:55,823 - INFO - joeynmt.training - Epoch  15, Step:    93900, Batch Loss:     2.112665, Tokens per Sec:    14367, Lr: 0.000300\n",
      "2021-05-25 09:23:08,834 - INFO - joeynmt.training - Epoch  15, Step:    94000, Batch Loss:     1.845358, Tokens per Sec:    13897, Lr: 0.000300\n",
      "2021-05-25 09:23:21,710 - INFO - joeynmt.training - Epoch  15, Step:    94100, Batch Loss:     2.215279, Tokens per Sec:    13719, Lr: 0.000300\n",
      "2021-05-25 09:23:34,803 - INFO - joeynmt.training - Epoch  15, Step:    94200, Batch Loss:     2.083531, Tokens per Sec:    13906, Lr: 0.000300\n",
      "2021-05-25 09:23:47,891 - INFO - joeynmt.training - Epoch  15, Step:    94300, Batch Loss:     2.154257, Tokens per Sec:    13995, Lr: 0.000300\n",
      "2021-05-25 09:24:01,073 - INFO - joeynmt.training - Epoch  15, Step:    94400, Batch Loss:     2.144111, Tokens per Sec:    14115, Lr: 0.000300\n",
      "2021-05-25 09:24:14,016 - INFO - joeynmt.training - Epoch  15, Step:    94500, Batch Loss:     2.446999, Tokens per Sec:    13729, Lr: 0.000300\n",
      "2021-05-25 09:24:26,689 - INFO - joeynmt.training - Epoch  15, Step:    94600, Batch Loss:     2.077415, Tokens per Sec:    14246, Lr: 0.000300\n",
      "2021-05-25 09:24:39,584 - INFO - joeynmt.training - Epoch  15, Step:    94700, Batch Loss:     2.458275, Tokens per Sec:    14431, Lr: 0.000300\n",
      "2021-05-25 09:24:52,534 - INFO - joeynmt.training - Epoch  15, Step:    94800, Batch Loss:     2.010441, Tokens per Sec:    14576, Lr: 0.000300\n",
      "2021-05-25 09:25:05,334 - INFO - joeynmt.training - Epoch  15, Step:    94900, Batch Loss:     2.161300, Tokens per Sec:    14120, Lr: 0.000300\n",
      "2021-05-25 09:25:18,333 - INFO - joeynmt.training - Epoch  15, Step:    95000, Batch Loss:     2.171158, Tokens per Sec:    14653, Lr: 0.000300\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
      "    main()\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/training.py\", line 805, in train\n",
      "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/training.py\", line 475, in train_and_validate\n",
      "    valid_duration = self._validate(valid_data, epoch_no)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/training.py\", line 563, in _validate\n",
      "    n_gpu=self.n_gpu\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/prediction.py\", line 125, in validate_on_data\n",
      "    n_best=n_best)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/search.py\", line 444, in run_batch\n",
      "    encoder_hidden=encoder_hidden)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/search.py\", line 39, in greedy\n",
      "    src_mask, max_output_length, model, encoder_output, encoder_hidden)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/search.py\", line 146, in transformer_greedy\n",
      "    trg_mask=trg_mask\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/model.py\", line 104, in forward\n",
      "    outputs, hidden, att_probs, att_vectors = self._decode(**kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/model.py\", line 178, in _decode\n",
      "    **_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/decoders.py\", line 540, in forward\n",
      "    src_mask=src_mask, trg_mask=trg_mask)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/transformer_layers.py\", line 272, in forward\n",
      "    o = self.feed_forward(self.dropout(h2) + h1)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/transformer_layers.py\", line 116, in forward\n",
      "    return self.pwff_layer(x_norm) + x\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 119, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 94, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1753, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt2$src.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMgv7Y6_l9tU"
   },
   "source": [
    "15 epochs done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vG93x5HI-oA-",
    "outputId": "e49a1923-b505-43f9-9412-9223704369e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5000\tLoss: 89031.21875\tPPL: 24.66077\tbleu: 3.82186\tLR: 0.00030000\t*\n",
      "Steps: 10000\tLoss: 76205.73438\tPPL: 15.54103\tbleu: 8.17303\tLR: 0.00030000\t*\n",
      "Steps: 15000\tLoss: 69015.25000\tPPL: 11.99654\tbleu: 12.17688\tLR: 0.00030000\t*\n",
      "Steps: 20000\tLoss: 64636.51953\tPPL: 10.24695\tbleu: 14.46302\tLR: 0.00030000\t*\n",
      "Steps: 25000\tLoss: 61545.32031\tPPL: 9.16777\tbleu: 16.18443\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 59109.05078\tPPL: 8.39793\tbleu: 17.02692\tLR: 0.00030000\t*\n",
      "Steps: 35000\tLoss: 57419.58984\tPPL: 7.90237\tbleu: 18.38236\tLR: 0.00030000\t*\n",
      "Steps: 40000\tLoss: 55984.45312\tPPL: 7.50445\tbleu: 19.18505\tLR: 0.00030000\t*\n",
      "Steps: 45000\tLoss: 54509.96094\tPPL: 7.11648\tbleu: 20.19677\tLR: 0.00030000\t*\n",
      "Steps: 50000\tLoss: 53734.00781\tPPL: 6.92043\tbleu: 20.37179\tLR: 0.00030000\t*\n",
      "Steps: 55000\tLoss: 52948.68359\tPPL: 6.72752\tbleu: 21.17316\tLR: 0.00030000\t*\n",
      "Steps: 60000\tLoss: 52193.86328\tPPL: 6.54716\tbleu: 21.64481\tLR: 0.00030000\t*\n",
      "Steps: 65000\tLoss: 51547.43750\tPPL: 6.39656\tbleu: 21.87287\tLR: 0.00030000\t*\n",
      "Steps: 70000\tLoss: 50884.95312\tPPL: 6.24580\tbleu: 22.46251\tLR: 0.00030000\t*\n",
      "Steps: 75000\tLoss: 50680.60547\tPPL: 6.20002\tbleu: 22.64507\tLR: 0.00030000\t*\n",
      "Steps: 80000\tLoss: 50243.01172\tPPL: 6.10311\tbleu: 22.69393\tLR: 0.00030000\t*\n",
      "Steps: 85000\tLoss: 49710.89844\tPPL: 5.98731\tbleu: 23.30974\tLR: 0.00030000\t*\n",
      "Steps: 90000\tLoss: 49284.94141\tPPL: 5.89620\tbleu: 23.26082\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/rwen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "5cWYmgZR2CEi"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language2, source_language)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language2}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language2}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer/src_vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer/trg_vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer/latest.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 15                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer2\"\n",
    "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "    save_latest_ckpt: True\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda\", source_language=source_language, target_language2=target_language2)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7rxwmJOp5dsc",
    "outputId": "31e44c8d-a3d4-41dd-cdb8-48d3ebfb2679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-25 09:51:25,131 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-05-25 09:51:25,170 - INFO - joeynmt.data - Loading training data...\n",
      "2021-05-25 09:51:33,485 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-05-25 09:51:33,761 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-05-25 09:51:33,802 - INFO - joeynmt.data - Loading test data...\n",
      "2021-05-25 09:51:33,859 - INFO - joeynmt.data - Data loaded.\n",
      "2021-05-25 09:51:33,859 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-05-25 09:51:34,083 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-05-25 09:51:34.213080: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-05-25 09:51:36,791 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-05-25 09:51:40,245 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/latest.ckpt\n",
      "2021-05-25 09:51:40,668 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-05-25 09:51:40,668 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-05-25 09:51:40,668 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-05-25 09:51:40,669 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-05-25 09:51:40,669 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-05-25 09:51:40,669 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-05-25 09:51:40,669 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-05-25 09:51:40,669 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-05-25 09:51:40,669 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-05-25 09:51:40,669 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\n",
      "2021-05-25 09:51:40,670 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\n",
      "2021-05-25 09:51:40,670 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-05-25 09:51:40,670 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-05-25 09:51:40,670 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/latest.ckpt\n",
      "2021-05-25 09:51:40,670 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-05-25 09:51:40,670 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-05-25 09:51:40,670 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-05-25 09:51:40,671 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-05-25 09:51:40,671 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-05-25 09:51:40,671 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-05-25 09:51:40,671 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-05-25 09:51:40,671 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-05-25 09:51:40,671 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-05-25 09:51:40,671 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-05-25 09:51:40,671 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-05-25 09:51:40,672 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-05-25 09:51:40,672 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-05-25 09:51:40,672 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-05-25 09:51:40,672 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-05-25 09:51:40,672 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-05-25 09:51:40,672 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-05-25 09:51:40,672 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-05-25 09:51:40,672 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-05-25 09:51:40,673 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-05-25 09:51:40,673 - INFO - joeynmt.helpers - cfg.training.epochs                : 15\n",
      "2021-05-25 09:51:40,673 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-05-25 09:51:40,673 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-05-25 09:51:40,673 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-05-25 09:51:40,673 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer2\n",
      "2021-05-25 09:51:40,673 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-05-25 09:51:40,673 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-05-25 09:51:40,674 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-05-25 09:51:40,674 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-05-25 09:51:40,674 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-05-25 09:51:40,674 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-05-25 09:51:40,674 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True\n",
      "2021-05-25 09:51:40,674 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-05-25 09:51:40,674 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-05-25 09:51:40,675 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-05-25 09:51:40,675 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-05-25 09:51:40,675 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-05-25 09:51:40,675 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-05-25 09:51:40,675 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-05-25 09:51:40,675 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-05-25 09:51:40,675 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-05-25 09:51:40,675 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-05-25 09:51:40,676 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-05-25 09:51:40,676 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-05-25 09:51:40,676 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-05-25 09:51:40,676 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-05-25 09:51:40,676 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-05-25 09:51:40,676 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-05-25 09:51:40,676 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-05-25 09:51:40,677 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-05-25 09:51:40,677 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-05-25 09:51:40,677 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-05-25 09:51:40,677 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-05-25 09:51:40,677 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-05-25 09:51:40,677 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-05-25 09:51:40,677 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-05-25 09:51:40,677 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-05-25 09:51:40,678 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434471,\n",
      "\tvalid 1000,\n",
      "\ttest 2651\n",
      "2021-05-25 09:51:40,678 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-05-25 09:51:40,678 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-05-25 09:51:40,678 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-05-25 09:51:40,678 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-05-25 09:51:40,678 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-05-25 09:51:40,679 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-05-25 09:51:40,688 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-05-25 09:51:40,689 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-05-25 09:51:54,802 - INFO - joeynmt.training - Epoch   1, Step:    90100, Batch Loss:     2.220648, Tokens per Sec:    13332, Lr: 0.000300\n",
      "2021-05-25 09:52:06,997 - INFO - joeynmt.training - Epoch   1, Step:    90200, Batch Loss:     2.061533, Tokens per Sec:    15479, Lr: 0.000300\n",
      "2021-05-25 09:52:19,171 - INFO - joeynmt.training - Epoch   1, Step:    90300, Batch Loss:     2.089448, Tokens per Sec:    15036, Lr: 0.000300\n",
      "2021-05-25 09:52:31,681 - INFO - joeynmt.training - Epoch   1, Step:    90400, Batch Loss:     2.147928, Tokens per Sec:    14901, Lr: 0.000300\n",
      "2021-05-25 09:52:44,073 - INFO - joeynmt.training - Epoch   1, Step:    90500, Batch Loss:     2.361498, Tokens per Sec:    14170, Lr: 0.000300\n",
      "2021-05-25 09:52:57,080 - INFO - joeynmt.training - Epoch   1, Step:    90600, Batch Loss:     1.986000, Tokens per Sec:    14925, Lr: 0.000300\n",
      "2021-05-25 09:53:09,748 - INFO - joeynmt.training - Epoch   1, Step:    90700, Batch Loss:     2.193472, Tokens per Sec:    14321, Lr: 0.000300\n",
      "2021-05-25 09:53:09,944 - INFO - joeynmt.training - Epoch   1: total training loss 1500.91\n",
      "2021-05-25 09:53:09,945 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-05-25 09:53:23,405 - INFO - joeynmt.training - Epoch   2, Step:    90800, Batch Loss:     2.226401, Tokens per Sec:    13831, Lr: 0.000300\n",
      "2021-05-25 09:53:36,245 - INFO - joeynmt.training - Epoch   2, Step:    90900, Batch Loss:     2.205554, Tokens per Sec:    14467, Lr: 0.000300\n",
      "2021-05-25 09:53:49,132 - INFO - joeynmt.training - Epoch   2, Step:    91000, Batch Loss:     2.257162, Tokens per Sec:    14466, Lr: 0.000300\n",
      "2021-05-25 09:54:01,960 - INFO - joeynmt.training - Epoch   2, Step:    91100, Batch Loss:     2.198670, Tokens per Sec:    13841, Lr: 0.000300\n",
      "2021-05-25 09:54:15,054 - INFO - joeynmt.training - Epoch   2, Step:    91200, Batch Loss:     2.116452, Tokens per Sec:    14315, Lr: 0.000300\n",
      "2021-05-25 09:54:28,289 - INFO - joeynmt.training - Epoch   2, Step:    91300, Batch Loss:     2.057393, Tokens per Sec:    14042, Lr: 0.000300\n",
      "2021-05-25 09:54:41,707 - INFO - joeynmt.training - Epoch   2, Step:    91400, Batch Loss:     2.167253, Tokens per Sec:    14159, Lr: 0.000300\n",
      "2021-05-25 09:54:54,603 - INFO - joeynmt.training - Epoch   2, Step:    91500, Batch Loss:     2.021852, Tokens per Sec:    13996, Lr: 0.000300\n",
      "2021-05-25 09:55:07,648 - INFO - joeynmt.training - Epoch   2, Step:    91600, Batch Loss:     2.252157, Tokens per Sec:    14283, Lr: 0.000300\n",
      "2021-05-25 09:55:20,250 - INFO - joeynmt.training - Epoch   2, Step:    91700, Batch Loss:     2.114555, Tokens per Sec:    13782, Lr: 0.000300\n",
      "2021-05-25 09:55:33,319 - INFO - joeynmt.training - Epoch   2, Step:    91800, Batch Loss:     2.284547, Tokens per Sec:    14443, Lr: 0.000300\n",
      "2021-05-25 09:55:46,427 - INFO - joeynmt.training - Epoch   2, Step:    91900, Batch Loss:     2.090560, Tokens per Sec:    14375, Lr: 0.000300\n",
      "2021-05-25 09:55:59,460 - INFO - joeynmt.training - Epoch   2, Step:    92000, Batch Loss:     2.103915, Tokens per Sec:    13691, Lr: 0.000300\n",
      "2021-05-25 09:56:12,574 - INFO - joeynmt.training - Epoch   2, Step:    92100, Batch Loss:     2.043825, Tokens per Sec:    13531, Lr: 0.000300\n",
      "2021-05-25 09:56:25,748 - INFO - joeynmt.training - Epoch   2, Step:    92200, Batch Loss:     1.965782, Tokens per Sec:    13871, Lr: 0.000300\n",
      "2021-05-25 09:56:38,958 - INFO - joeynmt.training - Epoch   2, Step:    92300, Batch Loss:     2.089019, Tokens per Sec:    13879, Lr: 0.000300\n",
      "2021-05-25 09:56:52,120 - INFO - joeynmt.training - Epoch   2, Step:    92400, Batch Loss:     2.003734, Tokens per Sec:    14228, Lr: 0.000300\n",
      "2021-05-25 09:57:05,189 - INFO - joeynmt.training - Epoch   2, Step:    92500, Batch Loss:     2.106340, Tokens per Sec:    13991, Lr: 0.000300\n",
      "2021-05-25 09:57:18,308 - INFO - joeynmt.training - Epoch   2, Step:    92600, Batch Loss:     1.993472, Tokens per Sec:    13968, Lr: 0.000300\n",
      "2021-05-25 09:57:31,460 - INFO - joeynmt.training - Epoch   2, Step:    92700, Batch Loss:     1.937467, Tokens per Sec:    13857, Lr: 0.000300\n",
      "2021-05-25 09:57:44,692 - INFO - joeynmt.training - Epoch   2, Step:    92800, Batch Loss:     2.240114, Tokens per Sec:    13826, Lr: 0.000300\n",
      "2021-05-25 09:57:57,836 - INFO - joeynmt.training - Epoch   2, Step:    92900, Batch Loss:     1.951241, Tokens per Sec:    13803, Lr: 0.000300\n",
      "2021-05-25 09:58:10,655 - INFO - joeynmt.training - Epoch   2, Step:    93000, Batch Loss:     2.135939, Tokens per Sec:    13841, Lr: 0.000300\n",
      "2021-05-25 09:58:23,898 - INFO - joeynmt.training - Epoch   2, Step:    93100, Batch Loss:     2.167082, Tokens per Sec:    13826, Lr: 0.000300\n",
      "2021-05-25 09:58:37,175 - INFO - joeynmt.training - Epoch   2, Step:    93200, Batch Loss:     2.080910, Tokens per Sec:    14283, Lr: 0.000300\n",
      "2021-05-25 09:58:50,117 - INFO - joeynmt.training - Epoch   2, Step:    93300, Batch Loss:     1.957364, Tokens per Sec:    14158, Lr: 0.000300\n",
      "2021-05-25 09:59:02,917 - INFO - joeynmt.training - Epoch   2, Step:    93400, Batch Loss:     1.943483, Tokens per Sec:    14156, Lr: 0.000300\n",
      "2021-05-25 09:59:15,754 - INFO - joeynmt.training - Epoch   2, Step:    93500, Batch Loss:     2.421969, Tokens per Sec:    13935, Lr: 0.000300\n",
      "2021-05-25 09:59:28,820 - INFO - joeynmt.training - Epoch   2, Step:    93600, Batch Loss:     2.028527, Tokens per Sec:    14386, Lr: 0.000300\n",
      "2021-05-25 09:59:41,893 - INFO - joeynmt.training - Epoch   2, Step:    93700, Batch Loss:     2.309310, Tokens per Sec:    13855, Lr: 0.000300\n",
      "2021-05-25 09:59:55,016 - INFO - joeynmt.training - Epoch   2, Step:    93800, Batch Loss:     2.251626, Tokens per Sec:    14023, Lr: 0.000300\n",
      "2021-05-25 10:00:08,188 - INFO - joeynmt.training - Epoch   2, Step:    93900, Batch Loss:     2.063880, Tokens per Sec:    14189, Lr: 0.000300\n",
      "2021-05-25 10:00:21,252 - INFO - joeynmt.training - Epoch   2, Step:    94000, Batch Loss:     1.864058, Tokens per Sec:    13841, Lr: 0.000300\n",
      "2021-05-25 10:00:34,180 - INFO - joeynmt.training - Epoch   2, Step:    94100, Batch Loss:     2.200444, Tokens per Sec:    13665, Lr: 0.000300\n",
      "2021-05-25 10:00:47,218 - INFO - joeynmt.training - Epoch   2, Step:    94200, Batch Loss:     2.097692, Tokens per Sec:    13964, Lr: 0.000300\n",
      "2021-05-25 10:01:00,342 - INFO - joeynmt.training - Epoch   2, Step:    94300, Batch Loss:     2.146989, Tokens per Sec:    13956, Lr: 0.000300\n",
      "2021-05-25 10:01:13,535 - INFO - joeynmt.training - Epoch   2, Step:    94400, Batch Loss:     2.087109, Tokens per Sec:    14104, Lr: 0.000300\n",
      "2021-05-25 10:01:26,596 - INFO - joeynmt.training - Epoch   2, Step:    94500, Batch Loss:     2.417007, Tokens per Sec:    13605, Lr: 0.000300\n",
      "2021-05-25 10:01:39,574 - INFO - joeynmt.training - Epoch   2, Step:    94600, Batch Loss:     2.081503, Tokens per Sec:    13912, Lr: 0.000300\n",
      "2021-05-25 10:01:52,762 - INFO - joeynmt.training - Epoch   2, Step:    94700, Batch Loss:     2.430051, Tokens per Sec:    14109, Lr: 0.000300\n",
      "2021-05-25 10:02:05,956 - INFO - joeynmt.training - Epoch   2, Step:    94800, Batch Loss:     2.016342, Tokens per Sec:    14306, Lr: 0.000300\n",
      "2021-05-25 10:02:19,020 - INFO - joeynmt.training - Epoch   2, Step:    94900, Batch Loss:     2.183854, Tokens per Sec:    13835, Lr: 0.000300\n",
      "2021-05-25 10:02:32,285 - INFO - joeynmt.training - Epoch   2, Step:    95000, Batch Loss:     2.163294, Tokens per Sec:    14360, Lr: 0.000300\n",
      "2021-05-25 10:02:54,612 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 10:02:54,612 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 10:02:54,613 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 10:02:54,855 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 10:02:54,856 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 10:02:55,565 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 10:02:55,566 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 10:02:55,566 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 10:02:55,566 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 10:02:55,567 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 10:02:55,567 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 10:02:55,567 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 10:02:55,568 - INFO - joeynmt.training - \tHypothesis: The text that you wrote in the front of the scroll .\n",
      "2021-05-25 10:02:55,568 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 10:02:55,568 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 10:02:55,568 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:02:55,569 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or objectively , we should continue our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:02:55,569 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 10:02:55,569 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 10:02:55,569 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 10:02:55,569 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of kindness in Satan’s world .\n",
      "2021-05-25 10:02:55,570 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    95000: bleu:  23.41, loss: 49048.7969, ppl:   5.8463, duration: 23.2843s\n",
      "2021-05-25 10:03:08,663 - INFO - joeynmt.training - Epoch   2, Step:    95100, Batch Loss:     2.202874, Tokens per Sec:    14143, Lr: 0.000300\n",
      "2021-05-25 10:03:21,612 - INFO - joeynmt.training - Epoch   2, Step:    95200, Batch Loss:     2.103981, Tokens per Sec:    14150, Lr: 0.000300\n",
      "2021-05-25 10:03:34,606 - INFO - joeynmt.training - Epoch   2, Step:    95300, Batch Loss:     2.087163, Tokens per Sec:    14363, Lr: 0.000300\n",
      "2021-05-25 10:03:47,793 - INFO - joeynmt.training - Epoch   2, Step:    95400, Batch Loss:     2.047569, Tokens per Sec:    14148, Lr: 0.000300\n",
      "2021-05-25 10:04:01,195 - INFO - joeynmt.training - Epoch   2, Step:    95500, Batch Loss:     2.179697, Tokens per Sec:    14302, Lr: 0.000300\n",
      "2021-05-25 10:04:14,393 - INFO - joeynmt.training - Epoch   2, Step:    95600, Batch Loss:     2.249822, Tokens per Sec:    13825, Lr: 0.000300\n",
      "2021-05-25 10:04:27,352 - INFO - joeynmt.training - Epoch   2, Step:    95700, Batch Loss:     2.136721, Tokens per Sec:    13835, Lr: 0.000300\n",
      "2021-05-25 10:04:40,313 - INFO - joeynmt.training - Epoch   2, Step:    95800, Batch Loss:     1.934331, Tokens per Sec:    13700, Lr: 0.000300\n",
      "2021-05-25 10:04:53,452 - INFO - joeynmt.training - Epoch   2, Step:    95900, Batch Loss:     2.050752, Tokens per Sec:    14315, Lr: 0.000300\n",
      "2021-05-25 10:05:06,533 - INFO - joeynmt.training - Epoch   2, Step:    96000, Batch Loss:     2.018701, Tokens per Sec:    13786, Lr: 0.000300\n",
      "2021-05-25 10:05:19,765 - INFO - joeynmt.training - Epoch   2, Step:    96100, Batch Loss:     2.091204, Tokens per Sec:    14051, Lr: 0.000300\n",
      "2021-05-25 10:05:32,931 - INFO - joeynmt.training - Epoch   2, Step:    96200, Batch Loss:     2.017746, Tokens per Sec:    13919, Lr: 0.000300\n",
      "2021-05-25 10:05:46,168 - INFO - joeynmt.training - Epoch   2, Step:    96300, Batch Loss:     2.152800, Tokens per Sec:    14114, Lr: 0.000300\n",
      "2021-05-25 10:05:59,305 - INFO - joeynmt.training - Epoch   2, Step:    96400, Batch Loss:     2.192282, Tokens per Sec:    14174, Lr: 0.000300\n",
      "2021-05-25 10:06:12,530 - INFO - joeynmt.training - Epoch   2, Step:    96500, Batch Loss:     2.131142, Tokens per Sec:    13737, Lr: 0.000300\n",
      "2021-05-25 10:06:25,598 - INFO - joeynmt.training - Epoch   2, Step:    96600, Batch Loss:     1.931907, Tokens per Sec:    13862, Lr: 0.000300\n",
      "2021-05-25 10:06:38,821 - INFO - joeynmt.training - Epoch   2, Step:    96700, Batch Loss:     2.092613, Tokens per Sec:    14156, Lr: 0.000300\n",
      "2021-05-25 10:06:52,269 - INFO - joeynmt.training - Epoch   2, Step:    96800, Batch Loss:     2.140033, Tokens per Sec:    14565, Lr: 0.000300\n",
      "2021-05-25 10:07:05,186 - INFO - joeynmt.training - Epoch   2, Step:    96900, Batch Loss:     2.077730, Tokens per Sec:    13979, Lr: 0.000300\n",
      "2021-05-25 10:07:18,288 - INFO - joeynmt.training - Epoch   2, Step:    97000, Batch Loss:     2.029906, Tokens per Sec:    14385, Lr: 0.000300\n",
      "2021-05-25 10:07:31,472 - INFO - joeynmt.training - Epoch   2, Step:    97100, Batch Loss:     2.197507, Tokens per Sec:    13961, Lr: 0.000300\n",
      "2021-05-25 10:07:42,434 - INFO - joeynmt.training - Epoch   2: total training loss 13845.04\n",
      "2021-05-25 10:07:42,434 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-05-25 10:07:45,183 - INFO - joeynmt.training - Epoch   3, Step:    97200, Batch Loss:     1.969478, Tokens per Sec:     9555, Lr: 0.000300\n",
      "2021-05-25 10:07:58,459 - INFO - joeynmt.training - Epoch   3, Step:    97300, Batch Loss:     1.978148, Tokens per Sec:    14245, Lr: 0.000300\n",
      "2021-05-25 10:08:11,707 - INFO - joeynmt.training - Epoch   3, Step:    97400, Batch Loss:     2.210915, Tokens per Sec:    14123, Lr: 0.000300\n",
      "2021-05-25 10:08:24,782 - INFO - joeynmt.training - Epoch   3, Step:    97500, Batch Loss:     2.140087, Tokens per Sec:    13958, Lr: 0.000300\n",
      "2021-05-25 10:08:37,711 - INFO - joeynmt.training - Epoch   3, Step:    97600, Batch Loss:     2.086484, Tokens per Sec:    14048, Lr: 0.000300\n",
      "2021-05-25 10:08:50,704 - INFO - joeynmt.training - Epoch   3, Step:    97700, Batch Loss:     1.782264, Tokens per Sec:    14319, Lr: 0.000300\n",
      "2021-05-25 10:09:03,836 - INFO - joeynmt.training - Epoch   3, Step:    97800, Batch Loss:     2.182058, Tokens per Sec:    13930, Lr: 0.000300\n",
      "2021-05-25 10:09:16,974 - INFO - joeynmt.training - Epoch   3, Step:    97900, Batch Loss:     2.248175, Tokens per Sec:    13970, Lr: 0.000300\n",
      "2021-05-25 10:09:30,244 - INFO - joeynmt.training - Epoch   3, Step:    98000, Batch Loss:     1.910859, Tokens per Sec:    14029, Lr: 0.000300\n",
      "2021-05-25 10:09:43,283 - INFO - joeynmt.training - Epoch   3, Step:    98100, Batch Loss:     2.153768, Tokens per Sec:    14270, Lr: 0.000300\n",
      "2021-05-25 10:09:56,312 - INFO - joeynmt.training - Epoch   3, Step:    98200, Batch Loss:     2.107806, Tokens per Sec:    13942, Lr: 0.000300\n",
      "2021-05-25 10:10:09,162 - INFO - joeynmt.training - Epoch   3, Step:    98300, Batch Loss:     2.219792, Tokens per Sec:    14317, Lr: 0.000300\n",
      "2021-05-25 10:10:22,065 - INFO - joeynmt.training - Epoch   3, Step:    98400, Batch Loss:     2.072111, Tokens per Sec:    14391, Lr: 0.000300\n",
      "2021-05-25 10:10:34,870 - INFO - joeynmt.training - Epoch   3, Step:    98500, Batch Loss:     2.217890, Tokens per Sec:    14098, Lr: 0.000300\n",
      "2021-05-25 10:10:47,984 - INFO - joeynmt.training - Epoch   3, Step:    98600, Batch Loss:     2.087540, Tokens per Sec:    13993, Lr: 0.000300\n",
      "2021-05-25 10:11:01,109 - INFO - joeynmt.training - Epoch   3, Step:    98700, Batch Loss:     2.064798, Tokens per Sec:    14056, Lr: 0.000300\n",
      "2021-05-25 10:11:14,147 - INFO - joeynmt.training - Epoch   3, Step:    98800, Batch Loss:     2.195741, Tokens per Sec:    13843, Lr: 0.000300\n",
      "2021-05-25 10:11:26,992 - INFO - joeynmt.training - Epoch   3, Step:    98900, Batch Loss:     2.298350, Tokens per Sec:    13758, Lr: 0.000300\n",
      "2021-05-25 10:11:40,198 - INFO - joeynmt.training - Epoch   3, Step:    99000, Batch Loss:     2.083025, Tokens per Sec:    13897, Lr: 0.000300\n",
      "2021-05-25 10:11:53,318 - INFO - joeynmt.training - Epoch   3, Step:    99100, Batch Loss:     2.099221, Tokens per Sec:    13912, Lr: 0.000300\n",
      "2021-05-25 10:12:06,525 - INFO - joeynmt.training - Epoch   3, Step:    99200, Batch Loss:     2.054707, Tokens per Sec:    14252, Lr: 0.000300\n",
      "2021-05-25 10:12:19,677 - INFO - joeynmt.training - Epoch   3, Step:    99300, Batch Loss:     2.149951, Tokens per Sec:    14055, Lr: 0.000300\n",
      "2021-05-25 10:12:32,743 - INFO - joeynmt.training - Epoch   3, Step:    99400, Batch Loss:     2.090618, Tokens per Sec:    13937, Lr: 0.000300\n",
      "2021-05-25 10:12:45,983 - INFO - joeynmt.training - Epoch   3, Step:    99500, Batch Loss:     2.189968, Tokens per Sec:    14202, Lr: 0.000300\n",
      "2021-05-25 10:12:59,158 - INFO - joeynmt.training - Epoch   3, Step:    99600, Batch Loss:     2.089002, Tokens per Sec:    14010, Lr: 0.000300\n",
      "2021-05-25 10:13:12,514 - INFO - joeynmt.training - Epoch   3, Step:    99700, Batch Loss:     2.166984, Tokens per Sec:    14050, Lr: 0.000300\n",
      "2021-05-25 10:13:25,661 - INFO - joeynmt.training - Epoch   3, Step:    99800, Batch Loss:     2.196655, Tokens per Sec:    14084, Lr: 0.000300\n",
      "2021-05-25 10:13:38,605 - INFO - joeynmt.training - Epoch   3, Step:    99900, Batch Loss:     2.222854, Tokens per Sec:    13674, Lr: 0.000300\n",
      "2021-05-25 10:13:51,745 - INFO - joeynmt.training - Epoch   3, Step:   100000, Batch Loss:     2.114301, Tokens per Sec:    14767, Lr: 0.000300\n",
      "2021-05-25 10:14:17,231 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 10:14:17,231 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 10:14:17,232 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 10:14:17,470 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 10:14:17,470 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 10:14:18,172 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 10:14:18,173 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 10:14:18,173 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 10:14:18,173 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 10:14:18,174 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 10:14:18,174 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 10:14:18,174 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 10:14:18,174 - INFO - joeynmt.training - \tHypothesis: The letter recorded in the front of the scroll .\n",
      "2021-05-25 10:14:18,175 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 10:14:18,175 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 10:14:18,175 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:14:18,176 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or discerned , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:14:18,176 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 10:14:18,176 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 10:14:18,176 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 10:14:18,176 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of humility in Satan’s world .\n",
      "2021-05-25 10:14:18,177 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   100000: bleu:  24.03, loss: 48546.5820, ppl:   5.7415, duration: 26.4315s\n",
      "2021-05-25 10:14:31,345 - INFO - joeynmt.training - Epoch   3, Step:   100100, Batch Loss:     2.108838, Tokens per Sec:    14077, Lr: 0.000300\n",
      "2021-05-25 10:14:44,024 - INFO - joeynmt.training - Epoch   3, Step:   100200, Batch Loss:     2.074396, Tokens per Sec:    13973, Lr: 0.000300\n",
      "2021-05-25 10:14:57,012 - INFO - joeynmt.training - Epoch   3, Step:   100300, Batch Loss:     2.128630, Tokens per Sec:    14141, Lr: 0.000300\n",
      "2021-05-25 10:15:10,129 - INFO - joeynmt.training - Epoch   3, Step:   100400, Batch Loss:     2.199239, Tokens per Sec:    14060, Lr: 0.000300\n",
      "2021-05-25 10:15:23,357 - INFO - joeynmt.training - Epoch   3, Step:   100500, Batch Loss:     2.160379, Tokens per Sec:    14385, Lr: 0.000300\n",
      "2021-05-25 10:15:36,491 - INFO - joeynmt.training - Epoch   3, Step:   100600, Batch Loss:     1.979552, Tokens per Sec:    14079, Lr: 0.000300\n",
      "2021-05-25 10:15:49,562 - INFO - joeynmt.training - Epoch   3, Step:   100700, Batch Loss:     1.977112, Tokens per Sec:    13962, Lr: 0.000300\n",
      "2021-05-25 10:16:02,755 - INFO - joeynmt.training - Epoch   3, Step:   100800, Batch Loss:     2.349197, Tokens per Sec:    13886, Lr: 0.000300\n",
      "2021-05-25 10:16:16,214 - INFO - joeynmt.training - Epoch   3, Step:   100900, Batch Loss:     2.227817, Tokens per Sec:    14365, Lr: 0.000300\n",
      "2021-05-25 10:16:29,258 - INFO - joeynmt.training - Epoch   3, Step:   101000, Batch Loss:     2.236152, Tokens per Sec:    13973, Lr: 0.000300\n",
      "2021-05-25 10:16:42,348 - INFO - joeynmt.training - Epoch   3, Step:   101100, Batch Loss:     1.903906, Tokens per Sec:    13923, Lr: 0.000300\n",
      "2021-05-25 10:16:55,566 - INFO - joeynmt.training - Epoch   3, Step:   101200, Batch Loss:     1.987570, Tokens per Sec:    14048, Lr: 0.000300\n",
      "2021-05-25 10:17:08,513 - INFO - joeynmt.training - Epoch   3, Step:   101300, Batch Loss:     2.086123, Tokens per Sec:    13754, Lr: 0.000300\n",
      "2021-05-25 10:17:21,709 - INFO - joeynmt.training - Epoch   3, Step:   101400, Batch Loss:     1.898915, Tokens per Sec:    14159, Lr: 0.000300\n",
      "2021-05-25 10:17:34,674 - INFO - joeynmt.training - Epoch   3, Step:   101500, Batch Loss:     2.108373, Tokens per Sec:    13762, Lr: 0.000300\n",
      "2021-05-25 10:17:47,802 - INFO - joeynmt.training - Epoch   3, Step:   101600, Batch Loss:     2.403065, Tokens per Sec:    14052, Lr: 0.000300\n",
      "2021-05-25 10:18:00,610 - INFO - joeynmt.training - Epoch   3, Step:   101700, Batch Loss:     2.322637, Tokens per Sec:    13646, Lr: 0.000300\n",
      "2021-05-25 10:18:13,743 - INFO - joeynmt.training - Epoch   3, Step:   101800, Batch Loss:     2.096002, Tokens per Sec:    14094, Lr: 0.000300\n",
      "2021-05-25 10:18:26,945 - INFO - joeynmt.training - Epoch   3, Step:   101900, Batch Loss:     2.062141, Tokens per Sec:    13810, Lr: 0.000300\n",
      "2021-05-25 10:18:40,057 - INFO - joeynmt.training - Epoch   3, Step:   102000, Batch Loss:     2.135427, Tokens per Sec:    13854, Lr: 0.000300\n",
      "2021-05-25 10:18:53,159 - INFO - joeynmt.training - Epoch   3, Step:   102100, Batch Loss:     2.123185, Tokens per Sec:    14360, Lr: 0.000300\n",
      "2021-05-25 10:19:06,344 - INFO - joeynmt.training - Epoch   3, Step:   102200, Batch Loss:     2.164859, Tokens per Sec:    13679, Lr: 0.000300\n",
      "2021-05-25 10:19:19,504 - INFO - joeynmt.training - Epoch   3, Step:   102300, Batch Loss:     2.000973, Tokens per Sec:    13994, Lr: 0.000300\n",
      "2021-05-25 10:19:32,568 - INFO - joeynmt.training - Epoch   3, Step:   102400, Batch Loss:     2.281897, Tokens per Sec:    14255, Lr: 0.000300\n",
      "2021-05-25 10:19:45,761 - INFO - joeynmt.training - Epoch   3, Step:   102500, Batch Loss:     2.447670, Tokens per Sec:    14341, Lr: 0.000300\n",
      "2021-05-25 10:19:58,820 - INFO - joeynmt.training - Epoch   3, Step:   102600, Batch Loss:     2.101136, Tokens per Sec:    14168, Lr: 0.000300\n",
      "2021-05-25 10:20:11,692 - INFO - joeynmt.training - Epoch   3, Step:   102700, Batch Loss:     1.874340, Tokens per Sec:    13864, Lr: 0.000300\n",
      "2021-05-25 10:20:24,741 - INFO - joeynmt.training - Epoch   3, Step:   102800, Batch Loss:     2.129258, Tokens per Sec:    14662, Lr: 0.000300\n",
      "2021-05-25 10:20:37,526 - INFO - joeynmt.training - Epoch   3, Step:   102900, Batch Loss:     2.122752, Tokens per Sec:    14287, Lr: 0.000300\n",
      "2021-05-25 10:20:50,570 - INFO - joeynmt.training - Epoch   3, Step:   103000, Batch Loss:     2.038479, Tokens per Sec:    13985, Lr: 0.000300\n",
      "2021-05-25 10:21:03,799 - INFO - joeynmt.training - Epoch   3, Step:   103100, Batch Loss:     2.193303, Tokens per Sec:    14106, Lr: 0.000300\n",
      "2021-05-25 10:21:16,841 - INFO - joeynmt.training - Epoch   3, Step:   103200, Batch Loss:     2.259504, Tokens per Sec:    13986, Lr: 0.000300\n",
      "2021-05-25 10:21:29,863 - INFO - joeynmt.training - Epoch   3, Step:   103300, Batch Loss:     2.268914, Tokens per Sec:    14005, Lr: 0.000300\n",
      "2021-05-25 10:21:42,955 - INFO - joeynmt.training - Epoch   3, Step:   103400, Batch Loss:     2.122217, Tokens per Sec:    13944, Lr: 0.000300\n",
      "2021-05-25 10:21:56,137 - INFO - joeynmt.training - Epoch   3, Step:   103500, Batch Loss:     2.339979, Tokens per Sec:    14319, Lr: 0.000300\n",
      "2021-05-25 10:22:09,139 - INFO - joeynmt.training - Epoch   3, Step:   103600, Batch Loss:     2.182291, Tokens per Sec:    13897, Lr: 0.000300\n",
      "2021-05-25 10:22:17,189 - INFO - joeynmt.training - Epoch   3: total training loss 13727.70\n",
      "2021-05-25 10:22:17,190 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-05-25 10:22:23,091 - INFO - joeynmt.training - Epoch   4, Step:   103700, Batch Loss:     2.411990, Tokens per Sec:    12543, Lr: 0.000300\n",
      "2021-05-25 10:22:36,310 - INFO - joeynmt.training - Epoch   4, Step:   103800, Batch Loss:     1.895651, Tokens per Sec:    14130, Lr: 0.000300\n",
      "2021-05-25 10:22:49,706 - INFO - joeynmt.training - Epoch   4, Step:   103900, Batch Loss:     2.015647, Tokens per Sec:    14241, Lr: 0.000300\n",
      "2021-05-25 10:23:02,858 - INFO - joeynmt.training - Epoch   4, Step:   104000, Batch Loss:     2.077872, Tokens per Sec:    14030, Lr: 0.000300\n",
      "2021-05-25 10:23:16,112 - INFO - joeynmt.training - Epoch   4, Step:   104100, Batch Loss:     2.208707, Tokens per Sec:    14146, Lr: 0.000300\n",
      "2021-05-25 10:23:29,413 - INFO - joeynmt.training - Epoch   4, Step:   104200, Batch Loss:     2.039840, Tokens per Sec:    14100, Lr: 0.000300\n",
      "2021-05-25 10:23:42,665 - INFO - joeynmt.training - Epoch   4, Step:   104300, Batch Loss:     2.214493, Tokens per Sec:    14223, Lr: 0.000300\n",
      "2021-05-25 10:23:55,838 - INFO - joeynmt.training - Epoch   4, Step:   104400, Batch Loss:     2.028809, Tokens per Sec:    14012, Lr: 0.000300\n",
      "2021-05-25 10:24:08,745 - INFO - joeynmt.training - Epoch   4, Step:   104500, Batch Loss:     2.012629, Tokens per Sec:    13738, Lr: 0.000300\n",
      "2021-05-25 10:24:21,845 - INFO - joeynmt.training - Epoch   4, Step:   104600, Batch Loss:     2.175872, Tokens per Sec:    14112, Lr: 0.000300\n",
      "2021-05-25 10:24:35,028 - INFO - joeynmt.training - Epoch   4, Step:   104700, Batch Loss:     2.012854, Tokens per Sec:    14106, Lr: 0.000300\n",
      "2021-05-25 10:24:48,104 - INFO - joeynmt.training - Epoch   4, Step:   104800, Batch Loss:     2.188138, Tokens per Sec:    14027, Lr: 0.000300\n",
      "2021-05-25 10:25:01,003 - INFO - joeynmt.training - Epoch   4, Step:   104900, Batch Loss:     2.228179, Tokens per Sec:    14077, Lr: 0.000300\n",
      "2021-05-25 10:25:14,011 - INFO - joeynmt.training - Epoch   4, Step:   105000, Batch Loss:     2.092777, Tokens per Sec:    14388, Lr: 0.000300\n",
      "2021-05-25 10:25:38,433 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 10:25:38,433 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 10:25:38,434 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 10:25:39,413 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 10:25:39,414 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 10:25:39,414 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 10:25:39,414 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-05-25 10:25:39,415 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 10:25:39,415 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 10:25:39,415 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 10:25:39,415 - INFO - joeynmt.training - \tHypothesis: The letter was written in the front of the scroll .\n",
      "2021-05-25 10:25:39,416 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 10:25:39,416 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 10:25:39,416 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:25:39,416 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or discernment , we should continue our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:25:39,417 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 10:25:39,417 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 10:25:39,417 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 10:25:39,417 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of sadness that they are well - being in Satan’s world .\n",
      "2021-05-25 10:25:39,418 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   105000: bleu:  24.07, loss: 48555.1953, ppl:   5.7433, duration: 25.4063s\n",
      "2021-05-25 10:25:52,638 - INFO - joeynmt.training - Epoch   4, Step:   105100, Batch Loss:     2.210094, Tokens per Sec:    13754, Lr: 0.000300\n",
      "2021-05-25 10:26:05,829 - INFO - joeynmt.training - Epoch   4, Step:   105200, Batch Loss:     2.512806, Tokens per Sec:    13918, Lr: 0.000300\n",
      "2021-05-25 10:26:18,948 - INFO - joeynmt.training - Epoch   4, Step:   105300, Batch Loss:     2.029548, Tokens per Sec:    13742, Lr: 0.000300\n",
      "2021-05-25 10:26:32,117 - INFO - joeynmt.training - Epoch   4, Step:   105400, Batch Loss:     2.255557, Tokens per Sec:    13849, Lr: 0.000300\n",
      "2021-05-25 10:26:45,092 - INFO - joeynmt.training - Epoch   4, Step:   105500, Batch Loss:     2.117866, Tokens per Sec:    14373, Lr: 0.000300\n",
      "2021-05-25 10:26:58,102 - INFO - joeynmt.training - Epoch   4, Step:   105600, Batch Loss:     1.999294, Tokens per Sec:    14215, Lr: 0.000300\n",
      "2021-05-25 10:27:10,693 - INFO - joeynmt.training - Epoch   4, Step:   105700, Batch Loss:     2.299950, Tokens per Sec:    14014, Lr: 0.000300\n",
      "2021-05-25 10:27:23,773 - INFO - joeynmt.training - Epoch   4, Step:   105800, Batch Loss:     2.165414, Tokens per Sec:    14299, Lr: 0.000300\n",
      "2021-05-25 10:27:36,661 - INFO - joeynmt.training - Epoch   4, Step:   105900, Batch Loss:     2.117653, Tokens per Sec:    13857, Lr: 0.000300\n",
      "2021-05-25 10:27:49,662 - INFO - joeynmt.training - Epoch   4, Step:   106000, Batch Loss:     2.277671, Tokens per Sec:    13870, Lr: 0.000300\n",
      "2021-05-25 10:28:02,576 - INFO - joeynmt.training - Epoch   4, Step:   106100, Batch Loss:     2.180508, Tokens per Sec:    13553, Lr: 0.000300\n",
      "2021-05-25 10:28:15,888 - INFO - joeynmt.training - Epoch   4, Step:   106200, Batch Loss:     1.937550, Tokens per Sec:    14166, Lr: 0.000300\n",
      "2021-05-25 10:28:29,074 - INFO - joeynmt.training - Epoch   4, Step:   106300, Batch Loss:     2.266883, Tokens per Sec:    14040, Lr: 0.000300\n",
      "2021-05-25 10:28:42,059 - INFO - joeynmt.training - Epoch   4, Step:   106400, Batch Loss:     2.060517, Tokens per Sec:    14381, Lr: 0.000300\n",
      "2021-05-25 10:28:54,991 - INFO - joeynmt.training - Epoch   4, Step:   106500, Batch Loss:     2.340544, Tokens per Sec:    13811, Lr: 0.000300\n",
      "2021-05-25 10:29:08,150 - INFO - joeynmt.training - Epoch   4, Step:   106600, Batch Loss:     2.214614, Tokens per Sec:    13971, Lr: 0.000300\n",
      "2021-05-25 10:29:21,367 - INFO - joeynmt.training - Epoch   4, Step:   106700, Batch Loss:     2.225516, Tokens per Sec:    13983, Lr: 0.000300\n",
      "2021-05-25 10:29:34,503 - INFO - joeynmt.training - Epoch   4, Step:   106800, Batch Loss:     2.073909, Tokens per Sec:    13739, Lr: 0.000300\n",
      "2021-05-25 10:29:47,844 - INFO - joeynmt.training - Epoch   4, Step:   106900, Batch Loss:     2.171037, Tokens per Sec:    14420, Lr: 0.000300\n",
      "2021-05-25 10:30:00,942 - INFO - joeynmt.training - Epoch   4, Step:   107000, Batch Loss:     2.194306, Tokens per Sec:    13863, Lr: 0.000300\n",
      "2021-05-25 10:30:14,005 - INFO - joeynmt.training - Epoch   4, Step:   107100, Batch Loss:     2.125448, Tokens per Sec:    13973, Lr: 0.000300\n",
      "2021-05-25 10:30:27,173 - INFO - joeynmt.training - Epoch   4, Step:   107200, Batch Loss:     2.107957, Tokens per Sec:    13788, Lr: 0.000300\n",
      "2021-05-25 10:30:40,326 - INFO - joeynmt.training - Epoch   4, Step:   107300, Batch Loss:     2.018965, Tokens per Sec:    14224, Lr: 0.000300\n",
      "2021-05-25 10:30:53,331 - INFO - joeynmt.training - Epoch   4, Step:   107400, Batch Loss:     1.888374, Tokens per Sec:    13925, Lr: 0.000300\n",
      "2021-05-25 10:31:06,214 - INFO - joeynmt.training - Epoch   4, Step:   107500, Batch Loss:     2.316125, Tokens per Sec:    14356, Lr: 0.000300\n",
      "2021-05-25 10:31:19,200 - INFO - joeynmt.training - Epoch   4, Step:   107600, Batch Loss:     2.011596, Tokens per Sec:    14312, Lr: 0.000300\n",
      "2021-05-25 10:31:32,323 - INFO - joeynmt.training - Epoch   4, Step:   107700, Batch Loss:     2.161681, Tokens per Sec:    14689, Lr: 0.000300\n",
      "2021-05-25 10:31:45,231 - INFO - joeynmt.training - Epoch   4, Step:   107800, Batch Loss:     2.141932, Tokens per Sec:    14244, Lr: 0.000300\n",
      "2021-05-25 10:31:58,328 - INFO - joeynmt.training - Epoch   4, Step:   107900, Batch Loss:     1.988889, Tokens per Sec:    14518, Lr: 0.000300\n",
      "2021-05-25 10:32:11,385 - INFO - joeynmt.training - Epoch   4, Step:   108000, Batch Loss:     2.121423, Tokens per Sec:    13917, Lr: 0.000300\n",
      "2021-05-25 10:32:24,566 - INFO - joeynmt.training - Epoch   4, Step:   108100, Batch Loss:     2.089694, Tokens per Sec:    14177, Lr: 0.000300\n",
      "2021-05-25 10:32:37,741 - INFO - joeynmt.training - Epoch   4, Step:   108200, Batch Loss:     2.051393, Tokens per Sec:    13990, Lr: 0.000300\n",
      "2021-05-25 10:32:50,808 - INFO - joeynmt.training - Epoch   4, Step:   108300, Batch Loss:     2.331815, Tokens per Sec:    14045, Lr: 0.000300\n",
      "2021-05-25 10:33:03,816 - INFO - joeynmt.training - Epoch   4, Step:   108400, Batch Loss:     2.013512, Tokens per Sec:    13987, Lr: 0.000300\n",
      "2021-05-25 10:33:16,922 - INFO - joeynmt.training - Epoch   4, Step:   108500, Batch Loss:     1.752556, Tokens per Sec:    14225, Lr: 0.000300\n",
      "2021-05-25 10:33:29,707 - INFO - joeynmt.training - Epoch   4, Step:   108600, Batch Loss:     2.170857, Tokens per Sec:    13369, Lr: 0.000300\n",
      "2021-05-25 10:33:42,772 - INFO - joeynmt.training - Epoch   4, Step:   108700, Batch Loss:     2.024395, Tokens per Sec:    13958, Lr: 0.000300\n",
      "2021-05-25 10:33:55,960 - INFO - joeynmt.training - Epoch   4, Step:   108800, Batch Loss:     2.049464, Tokens per Sec:    13885, Lr: 0.000300\n",
      "2021-05-25 10:34:09,145 - INFO - joeynmt.training - Epoch   4, Step:   108900, Batch Loss:     2.263135, Tokens per Sec:    13913, Lr: 0.000300\n",
      "2021-05-25 10:34:22,422 - INFO - joeynmt.training - Epoch   4, Step:   109000, Batch Loss:     2.234307, Tokens per Sec:    14036, Lr: 0.000300\n",
      "2021-05-25 10:34:35,502 - INFO - joeynmt.training - Epoch   4, Step:   109100, Batch Loss:     2.187174, Tokens per Sec:    14095, Lr: 0.000300\n",
      "2021-05-25 10:34:48,686 - INFO - joeynmt.training - Epoch   4, Step:   109200, Batch Loss:     2.098909, Tokens per Sec:    13762, Lr: 0.000300\n",
      "2021-05-25 10:35:01,636 - INFO - joeynmt.training - Epoch   4, Step:   109300, Batch Loss:     2.176117, Tokens per Sec:    13638, Lr: 0.000300\n",
      "2021-05-25 10:35:14,789 - INFO - joeynmt.training - Epoch   4, Step:   109400, Batch Loss:     2.141335, Tokens per Sec:    14294, Lr: 0.000300\n",
      "2021-05-25 10:35:27,966 - INFO - joeynmt.training - Epoch   4, Step:   109500, Batch Loss:     2.031866, Tokens per Sec:    13965, Lr: 0.000300\n",
      "2021-05-25 10:35:40,962 - INFO - joeynmt.training - Epoch   4, Step:   109600, Batch Loss:     2.043056, Tokens per Sec:    13943, Lr: 0.000300\n",
      "2021-05-25 10:35:53,858 - INFO - joeynmt.training - Epoch   4, Step:   109700, Batch Loss:     2.096200, Tokens per Sec:    13900, Lr: 0.000300\n",
      "2021-05-25 10:36:06,825 - INFO - joeynmt.training - Epoch   4, Step:   109800, Batch Loss:     2.064785, Tokens per Sec:    14081, Lr: 0.000300\n",
      "2021-05-25 10:36:19,705 - INFO - joeynmt.training - Epoch   4, Step:   109900, Batch Loss:     1.982746, Tokens per Sec:    14462, Lr: 0.000300\n",
      "2021-05-25 10:36:32,743 - INFO - joeynmt.training - Epoch   4, Step:   110000, Batch Loss:     2.216656, Tokens per Sec:    14486, Lr: 0.000300\n",
      "2021-05-25 10:36:57,583 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 10:36:57,583 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 10:36:57,584 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 10:36:57,820 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 10:36:57,820 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 10:36:58,609 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 10:36:58,609 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 10:36:58,610 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 10:36:58,610 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-05-25 10:36:58,610 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 10:36:58,610 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 10:36:58,610 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 10:36:58,610 - INFO - joeynmt.training - \tHypothesis: The text that was written in the front of the scroll .\n",
      "2021-05-25 10:36:58,611 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 10:36:58,611 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 10:36:58,611 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:36:58,611 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or discernment , we should continue our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:36:58,612 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 10:36:58,612 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 10:36:58,612 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 10:36:58,612 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of kindness in Satan’s world .\n",
      "2021-05-25 10:36:58,613 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   110000: bleu:  24.11, loss: 48127.5312, ppl:   5.6556, duration: 25.8693s\n",
      "2021-05-25 10:37:12,120 - INFO - joeynmt.training - Epoch   4, Step:   110100, Batch Loss:     2.048663, Tokens per Sec:    14507, Lr: 0.000300\n",
      "2021-05-25 10:37:17,151 - INFO - joeynmt.training - Epoch   4: total training loss 13643.24\n",
      "2021-05-25 10:37:17,151 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-05-25 10:37:25,991 - INFO - joeynmt.training - Epoch   5, Step:   110200, Batch Loss:     1.897405, Tokens per Sec:    12991, Lr: 0.000300\n",
      "2021-05-25 10:37:39,215 - INFO - joeynmt.training - Epoch   5, Step:   110300, Batch Loss:     2.067994, Tokens per Sec:    14246, Lr: 0.000300\n",
      "2021-05-25 10:37:52,167 - INFO - joeynmt.training - Epoch   5, Step:   110400, Batch Loss:     2.167616, Tokens per Sec:    13673, Lr: 0.000300\n",
      "2021-05-25 10:38:05,403 - INFO - joeynmt.training - Epoch   5, Step:   110500, Batch Loss:     2.257698, Tokens per Sec:    14374, Lr: 0.000300\n",
      "2021-05-25 10:38:18,540 - INFO - joeynmt.training - Epoch   5, Step:   110600, Batch Loss:     2.177630, Tokens per Sec:    13734, Lr: 0.000300\n",
      "2021-05-25 10:38:31,850 - INFO - joeynmt.training - Epoch   5, Step:   110700, Batch Loss:     1.990576, Tokens per Sec:    14416, Lr: 0.000300\n",
      "2021-05-25 10:38:44,865 - INFO - joeynmt.training - Epoch   5, Step:   110800, Batch Loss:     2.209731, Tokens per Sec:    13970, Lr: 0.000300\n",
      "2021-05-25 10:38:57,904 - INFO - joeynmt.training - Epoch   5, Step:   110900, Batch Loss:     2.151918, Tokens per Sec:    13649, Lr: 0.000300\n",
      "2021-05-25 10:39:11,146 - INFO - joeynmt.training - Epoch   5, Step:   111000, Batch Loss:     1.983453, Tokens per Sec:    13868, Lr: 0.000300\n",
      "2021-05-25 10:39:24,394 - INFO - joeynmt.training - Epoch   5, Step:   111100, Batch Loss:     2.121728, Tokens per Sec:    14280, Lr: 0.000300\n",
      "2021-05-25 10:39:37,514 - INFO - joeynmt.training - Epoch   5, Step:   111200, Batch Loss:     2.257630, Tokens per Sec:    14241, Lr: 0.000300\n",
      "2021-05-25 10:39:50,747 - INFO - joeynmt.training - Epoch   5, Step:   111300, Batch Loss:     1.947584, Tokens per Sec:    14170, Lr: 0.000300\n",
      "2021-05-25 10:40:03,980 - INFO - joeynmt.training - Epoch   5, Step:   111400, Batch Loss:     1.809674, Tokens per Sec:    14250, Lr: 0.000300\n",
      "2021-05-25 10:40:17,130 - INFO - joeynmt.training - Epoch   5, Step:   111500, Batch Loss:     2.098790, Tokens per Sec:    14410, Lr: 0.000300\n",
      "2021-05-25 10:40:30,241 - INFO - joeynmt.training - Epoch   5, Step:   111600, Batch Loss:     1.969828, Tokens per Sec:    13640, Lr: 0.000300\n",
      "2021-05-25 10:40:43,481 - INFO - joeynmt.training - Epoch   5, Step:   111700, Batch Loss:     2.116486, Tokens per Sec:    14378, Lr: 0.000300\n",
      "2021-05-25 10:40:56,438 - INFO - joeynmt.training - Epoch   5, Step:   111800, Batch Loss:     2.023529, Tokens per Sec:    13755, Lr: 0.000300\n",
      "2021-05-25 10:41:09,686 - INFO - joeynmt.training - Epoch   5, Step:   111900, Batch Loss:     1.808816, Tokens per Sec:    14309, Lr: 0.000300\n",
      "2021-05-25 10:41:22,783 - INFO - joeynmt.training - Epoch   5, Step:   112000, Batch Loss:     2.077322, Tokens per Sec:    13990, Lr: 0.000300\n",
      "2021-05-25 10:41:35,945 - INFO - joeynmt.training - Epoch   5, Step:   112100, Batch Loss:     2.350651, Tokens per Sec:    14028, Lr: 0.000300\n",
      "2021-05-25 10:41:48,925 - INFO - joeynmt.training - Epoch   5, Step:   112200, Batch Loss:     2.084974, Tokens per Sec:    14011, Lr: 0.000300\n",
      "2021-05-25 10:42:01,859 - INFO - joeynmt.training - Epoch   5, Step:   112300, Batch Loss:     2.213831, Tokens per Sec:    14160, Lr: 0.000300\n",
      "2021-05-25 10:42:14,895 - INFO - joeynmt.training - Epoch   5, Step:   112400, Batch Loss:     1.837070, Tokens per Sec:    14572, Lr: 0.000300\n",
      "2021-05-25 10:42:27,796 - INFO - joeynmt.training - Epoch   5, Step:   112500, Batch Loss:     2.013157, Tokens per Sec:    14287, Lr: 0.000300\n",
      "2021-05-25 10:42:40,526 - INFO - joeynmt.training - Epoch   5, Step:   112600, Batch Loss:     2.110052, Tokens per Sec:    14320, Lr: 0.000300\n",
      "2021-05-25 10:42:53,498 - INFO - joeynmt.training - Epoch   5, Step:   112700, Batch Loss:     2.162841, Tokens per Sec:    14120, Lr: 0.000300\n",
      "2021-05-25 10:43:06,457 - INFO - joeynmt.training - Epoch   5, Step:   112800, Batch Loss:     2.022455, Tokens per Sec:    14371, Lr: 0.000300\n",
      "2021-05-25 10:43:19,344 - INFO - joeynmt.training - Epoch   5, Step:   112900, Batch Loss:     2.154014, Tokens per Sec:    13782, Lr: 0.000300\n",
      "2021-05-25 10:43:32,398 - INFO - joeynmt.training - Epoch   5, Step:   113000, Batch Loss:     2.342123, Tokens per Sec:    13921, Lr: 0.000300\n",
      "2021-05-25 10:43:45,480 - INFO - joeynmt.training - Epoch   5, Step:   113100, Batch Loss:     2.210365, Tokens per Sec:    14250, Lr: 0.000300\n",
      "2021-05-25 10:43:58,502 - INFO - joeynmt.training - Epoch   5, Step:   113200, Batch Loss:     2.050417, Tokens per Sec:    13983, Lr: 0.000300\n",
      "2021-05-25 10:44:11,754 - INFO - joeynmt.training - Epoch   5, Step:   113300, Batch Loss:     2.008081, Tokens per Sec:    13943, Lr: 0.000300\n",
      "2021-05-25 10:44:24,684 - INFO - joeynmt.training - Epoch   5, Step:   113400, Batch Loss:     2.141582, Tokens per Sec:    13857, Lr: 0.000300\n",
      "2021-05-25 10:44:37,786 - INFO - joeynmt.training - Epoch   5, Step:   113500, Batch Loss:     2.022679, Tokens per Sec:    13593, Lr: 0.000300\n",
      "2021-05-25 10:44:50,797 - INFO - joeynmt.training - Epoch   5, Step:   113600, Batch Loss:     2.001517, Tokens per Sec:    13667, Lr: 0.000300\n",
      "2021-05-25 10:45:03,860 - INFO - joeynmt.training - Epoch   5, Step:   113700, Batch Loss:     2.187144, Tokens per Sec:    14122, Lr: 0.000300\n",
      "2021-05-25 10:45:17,009 - INFO - joeynmt.training - Epoch   5, Step:   113800, Batch Loss:     2.105658, Tokens per Sec:    14088, Lr: 0.000300\n",
      "2021-05-25 10:45:30,055 - INFO - joeynmt.training - Epoch   5, Step:   113900, Batch Loss:     2.202289, Tokens per Sec:    13841, Lr: 0.000300\n",
      "2021-05-25 10:45:43,261 - INFO - joeynmt.training - Epoch   5, Step:   114000, Batch Loss:     2.235579, Tokens per Sec:    14004, Lr: 0.000300\n",
      "2021-05-25 10:45:56,159 - INFO - joeynmt.training - Epoch   5, Step:   114100, Batch Loss:     2.174918, Tokens per Sec:    13655, Lr: 0.000300\n",
      "2021-05-25 10:46:09,393 - INFO - joeynmt.training - Epoch   5, Step:   114200, Batch Loss:     1.947528, Tokens per Sec:    14054, Lr: 0.000300\n",
      "2021-05-25 10:46:22,586 - INFO - joeynmt.training - Epoch   5, Step:   114300, Batch Loss:     2.227172, Tokens per Sec:    13905, Lr: 0.000300\n",
      "2021-05-25 10:46:35,699 - INFO - joeynmt.training - Epoch   5, Step:   114400, Batch Loss:     2.169610, Tokens per Sec:    13936, Lr: 0.000300\n",
      "2021-05-25 10:46:48,831 - INFO - joeynmt.training - Epoch   5, Step:   114500, Batch Loss:     1.858141, Tokens per Sec:    13933, Lr: 0.000300\n",
      "2021-05-25 10:47:01,970 - INFO - joeynmt.training - Epoch   5, Step:   114600, Batch Loss:     2.125708, Tokens per Sec:    14164, Lr: 0.000300\n",
      "2021-05-25 10:47:15,101 - INFO - joeynmt.training - Epoch   5, Step:   114700, Batch Loss:     2.141217, Tokens per Sec:    13915, Lr: 0.000300\n",
      "2021-05-25 10:47:28,321 - INFO - joeynmt.training - Epoch   5, Step:   114800, Batch Loss:     2.267921, Tokens per Sec:    14148, Lr: 0.000300\n",
      "2021-05-25 10:47:41,438 - INFO - joeynmt.training - Epoch   5, Step:   114900, Batch Loss:     2.118634, Tokens per Sec:    13537, Lr: 0.000300\n",
      "2021-05-25 10:47:54,761 - INFO - joeynmt.training - Epoch   5, Step:   115000, Batch Loss:     1.915853, Tokens per Sec:    14141, Lr: 0.000300\n",
      "2021-05-25 10:48:19,269 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 10:48:19,269 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 10:48:19,269 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 10:48:19,533 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 10:48:19,534 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 10:48:20,289 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 10:48:20,290 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 10:48:20,290 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 10:48:20,290 - INFO - joeynmt.training - \tHypothesis: It was a heartfelt heart .\n",
      "2021-05-25 10:48:20,290 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 10:48:20,291 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 10:48:20,291 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 10:48:20,291 - INFO - joeynmt.training - \tHypothesis: The letter recorded in the front of the scroll .\n",
      "2021-05-25 10:48:20,291 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 10:48:20,292 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 10:48:20,292 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:48:20,292 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:48:20,292 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 10:48:20,294 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 10:48:20,295 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 10:48:20,295 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-05-25 10:48:20,295 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   115000: bleu:  24.41, loss: 47717.6289, ppl:   5.5727, duration: 25.5337s\n",
      "2021-05-25 10:48:33,769 - INFO - joeynmt.training - Epoch   5, Step:   115100, Batch Loss:     2.050403, Tokens per Sec:    13758, Lr: 0.000300\n",
      "2021-05-25 10:48:46,746 - INFO - joeynmt.training - Epoch   5, Step:   115200, Batch Loss:     2.026546, Tokens per Sec:    13878, Lr: 0.000300\n",
      "2021-05-25 10:48:59,886 - INFO - joeynmt.training - Epoch   5, Step:   115300, Batch Loss:     1.981805, Tokens per Sec:    14162, Lr: 0.000300\n",
      "2021-05-25 10:49:12,924 - INFO - joeynmt.training - Epoch   5, Step:   115400, Batch Loss:     2.085503, Tokens per Sec:    14101, Lr: 0.000300\n",
      "2021-05-25 10:49:25,739 - INFO - joeynmt.training - Epoch   5, Step:   115500, Batch Loss:     2.232761, Tokens per Sec:    14292, Lr: 0.000300\n",
      "2021-05-25 10:49:38,699 - INFO - joeynmt.training - Epoch   5, Step:   115600, Batch Loss:     2.099196, Tokens per Sec:    13869, Lr: 0.000300\n",
      "2021-05-25 10:49:51,604 - INFO - joeynmt.training - Epoch   5, Step:   115700, Batch Loss:     2.153681, Tokens per Sec:    14116, Lr: 0.000300\n",
      "2021-05-25 10:50:04,636 - INFO - joeynmt.training - Epoch   5, Step:   115800, Batch Loss:     2.093537, Tokens per Sec:    14039, Lr: 0.000300\n",
      "2021-05-25 10:50:17,699 - INFO - joeynmt.training - Epoch   5, Step:   115900, Batch Loss:     2.260971, Tokens per Sec:    13936, Lr: 0.000300\n",
      "2021-05-25 10:50:31,177 - INFO - joeynmt.training - Epoch   5, Step:   116000, Batch Loss:     2.178027, Tokens per Sec:    14402, Lr: 0.000300\n",
      "2021-05-25 10:50:44,260 - INFO - joeynmt.training - Epoch   5, Step:   116100, Batch Loss:     2.130658, Tokens per Sec:    13598, Lr: 0.000300\n",
      "2021-05-25 10:50:57,463 - INFO - joeynmt.training - Epoch   5, Step:   116200, Batch Loss:     1.996221, Tokens per Sec:    14130, Lr: 0.000300\n",
      "2021-05-25 10:51:10,645 - INFO - joeynmt.training - Epoch   5, Step:   116300, Batch Loss:     2.170213, Tokens per Sec:    13785, Lr: 0.000300\n",
      "2021-05-25 10:51:23,706 - INFO - joeynmt.training - Epoch   5, Step:   116400, Batch Loss:     2.045142, Tokens per Sec:    13955, Lr: 0.000300\n",
      "2021-05-25 10:51:36,753 - INFO - joeynmt.training - Epoch   5, Step:   116500, Batch Loss:     2.064869, Tokens per Sec:    13837, Lr: 0.000300\n",
      "2021-05-25 10:51:50,030 - INFO - joeynmt.training - Epoch   5, Step:   116600, Batch Loss:     2.012825, Tokens per Sec:    14214, Lr: 0.000300\n",
      "2021-05-25 10:51:53,194 - INFO - joeynmt.training - Epoch   5: total training loss 13580.73\n",
      "2021-05-25 10:51:53,195 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-05-25 10:52:03,828 - INFO - joeynmt.training - Epoch   6, Step:   116700, Batch Loss:     2.142778, Tokens per Sec:    13219, Lr: 0.000300\n",
      "2021-05-25 10:52:17,140 - INFO - joeynmt.training - Epoch   6, Step:   116800, Batch Loss:     2.012786, Tokens per Sec:    13960, Lr: 0.000300\n",
      "2021-05-25 10:52:30,214 - INFO - joeynmt.training - Epoch   6, Step:   116900, Batch Loss:     2.066249, Tokens per Sec:    14047, Lr: 0.000300\n",
      "2021-05-25 10:52:43,276 - INFO - joeynmt.training - Epoch   6, Step:   117000, Batch Loss:     1.902377, Tokens per Sec:    13693, Lr: 0.000300\n",
      "2021-05-25 10:52:56,378 - INFO - joeynmt.training - Epoch   6, Step:   117100, Batch Loss:     2.116561, Tokens per Sec:    13985, Lr: 0.000300\n",
      "2021-05-25 10:53:09,487 - INFO - joeynmt.training - Epoch   6, Step:   117200, Batch Loss:     2.074426, Tokens per Sec:    14002, Lr: 0.000300\n",
      "2021-05-25 10:53:22,516 - INFO - joeynmt.training - Epoch   6, Step:   117300, Batch Loss:     1.820481, Tokens per Sec:    14591, Lr: 0.000300\n",
      "2021-05-25 10:53:35,540 - INFO - joeynmt.training - Epoch   6, Step:   117400, Batch Loss:     1.702370, Tokens per Sec:    14351, Lr: 0.000300\n",
      "2021-05-25 10:53:48,629 - INFO - joeynmt.training - Epoch   6, Step:   117500, Batch Loss:     2.100348, Tokens per Sec:    14173, Lr: 0.000300\n",
      "2021-05-25 10:54:01,622 - INFO - joeynmt.training - Epoch   6, Step:   117600, Batch Loss:     2.172434, Tokens per Sec:    13626, Lr: 0.000300\n",
      "2021-05-25 10:54:14,844 - INFO - joeynmt.training - Epoch   6, Step:   117700, Batch Loss:     2.067745, Tokens per Sec:    14257, Lr: 0.000300\n",
      "2021-05-25 10:54:28,093 - INFO - joeynmt.training - Epoch   6, Step:   117800, Batch Loss:     2.072300, Tokens per Sec:    14052, Lr: 0.000300\n",
      "2021-05-25 10:54:41,182 - INFO - joeynmt.training - Epoch   6, Step:   117900, Batch Loss:     2.017977, Tokens per Sec:    14153, Lr: 0.000300\n",
      "2021-05-25 10:54:54,168 - INFO - joeynmt.training - Epoch   6, Step:   118000, Batch Loss:     2.120955, Tokens per Sec:    14103, Lr: 0.000300\n",
      "2021-05-25 10:55:06,986 - INFO - joeynmt.training - Epoch   6, Step:   118100, Batch Loss:     2.118000, Tokens per Sec:    14395, Lr: 0.000300\n",
      "2021-05-25 10:55:19,863 - INFO - joeynmt.training - Epoch   6, Step:   118200, Batch Loss:     2.028389, Tokens per Sec:    14113, Lr: 0.000300\n",
      "2021-05-25 10:55:33,130 - INFO - joeynmt.training - Epoch   6, Step:   118300, Batch Loss:     2.029670, Tokens per Sec:    14127, Lr: 0.000300\n",
      "2021-05-25 10:55:46,110 - INFO - joeynmt.training - Epoch   6, Step:   118400, Batch Loss:     1.841207, Tokens per Sec:    13889, Lr: 0.000300\n",
      "2021-05-25 10:55:59,422 - INFO - joeynmt.training - Epoch   6, Step:   118500, Batch Loss:     2.186272, Tokens per Sec:    14108, Lr: 0.000300\n",
      "2021-05-25 10:56:12,649 - INFO - joeynmt.training - Epoch   6, Step:   118600, Batch Loss:     2.157703, Tokens per Sec:    13786, Lr: 0.000300\n",
      "2021-05-25 10:56:25,473 - INFO - joeynmt.training - Epoch   6, Step:   118700, Batch Loss:     1.983913, Tokens per Sec:    13656, Lr: 0.000300\n",
      "2021-05-25 10:56:38,473 - INFO - joeynmt.training - Epoch   6, Step:   118800, Batch Loss:     2.046992, Tokens per Sec:    13655, Lr: 0.000300\n",
      "2021-05-25 10:56:51,708 - INFO - joeynmt.training - Epoch   6, Step:   118900, Batch Loss:     1.884365, Tokens per Sec:    14106, Lr: 0.000300\n",
      "2021-05-25 10:57:04,955 - INFO - joeynmt.training - Epoch   6, Step:   119000, Batch Loss:     1.871694, Tokens per Sec:    14271, Lr: 0.000300\n",
      "2021-05-25 10:57:18,246 - INFO - joeynmt.training - Epoch   6, Step:   119100, Batch Loss:     2.099341, Tokens per Sec:    14306, Lr: 0.000300\n",
      "2021-05-25 10:57:31,408 - INFO - joeynmt.training - Epoch   6, Step:   119200, Batch Loss:     2.155244, Tokens per Sec:    13768, Lr: 0.000300\n",
      "2021-05-25 10:57:44,521 - INFO - joeynmt.training - Epoch   6, Step:   119300, Batch Loss:     2.047160, Tokens per Sec:    13889, Lr: 0.000300\n",
      "2021-05-25 10:57:57,741 - INFO - joeynmt.training - Epoch   6, Step:   119400, Batch Loss:     2.112056, Tokens per Sec:    14066, Lr: 0.000300\n",
      "2021-05-25 10:58:10,854 - INFO - joeynmt.training - Epoch   6, Step:   119500, Batch Loss:     1.999502, Tokens per Sec:    14060, Lr: 0.000300\n",
      "2021-05-25 10:58:23,737 - INFO - joeynmt.training - Epoch   6, Step:   119600, Batch Loss:     1.987200, Tokens per Sec:    14074, Lr: 0.000300\n",
      "2021-05-25 10:58:36,526 - INFO - joeynmt.training - Epoch   6, Step:   119700, Batch Loss:     2.306289, Tokens per Sec:    14204, Lr: 0.000300\n",
      "2021-05-25 10:58:49,291 - INFO - joeynmt.training - Epoch   6, Step:   119800, Batch Loss:     2.216136, Tokens per Sec:    14265, Lr: 0.000300\n",
      "2021-05-25 10:59:02,290 - INFO - joeynmt.training - Epoch   6, Step:   119900, Batch Loss:     2.082151, Tokens per Sec:    14538, Lr: 0.000300\n",
      "2021-05-25 10:59:15,229 - INFO - joeynmt.training - Epoch   6, Step:   120000, Batch Loss:     2.099386, Tokens per Sec:    14512, Lr: 0.000300\n",
      "2021-05-25 10:59:38,671 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-05-25 10:59:38,672 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-05-25 10:59:38,672 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-05-25 10:59:38,937 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-05-25 10:59:38,937 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-05-25 10:59:39,745 - INFO - joeynmt.training - Example #0\n",
      "2021-05-25 10:59:39,746 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-05-25 10:59:39,746 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-05-25 10:59:39,746 - INFO - joeynmt.training - \tHypothesis: It was deeply moved .\n",
      "2021-05-25 10:59:39,746 - INFO - joeynmt.training - Example #1\n",
      "2021-05-25 10:59:39,747 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-05-25 10:59:39,747 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-05-25 10:59:39,747 - INFO - joeynmt.training - \tHypothesis: The text that was written in the front of the scroll .\n",
      "2021-05-25 10:59:39,747 - INFO - joeynmt.training - Example #2\n",
      "2021-05-25 10:59:39,748 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-05-25 10:59:39,748 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:59:39,748 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or considerable , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-05-25 10:59:39,748 - INFO - joeynmt.training - Example #3\n",
      "2021-05-25 10:59:39,749 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-05-25 10:59:39,749 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-05-25 10:59:39,749 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-05-25 10:59:39,749 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   120000: bleu:  24.18, loss: 47591.9180, ppl:   5.5476, duration: 24.5200s\n",
      "2021-05-25 10:59:53,199 - INFO - joeynmt.training - Epoch   6, Step:   120100, Batch Loss:     1.950532, Tokens per Sec:    14216, Lr: 0.000300\n",
      "2021-05-25 11:00:06,415 - INFO - joeynmt.training - Epoch   6, Step:   120200, Batch Loss:     2.137681, Tokens per Sec:    14083, Lr: 0.000300\n",
      "2021-05-25 11:00:19,639 - INFO - joeynmt.training - Epoch   6, Step:   120300, Batch Loss:     2.086537, Tokens per Sec:    13968, Lr: 0.000300\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
      "    main()\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/__main__.py\", line 35, in main\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/training.py\", line 805, in train\n",
      "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/training.py\", line 427, in train_and_validate\n",
      "    batch_loss += self._train_step(batch)\n",
      "  File \"/content/gdrive/Shareddrives/NMT_for_African_Language/Kinyarwanda/joeynmt/joeynmt/training.py\", line 536, in _train_step\n",
      "    norm_batch_loss.backward()\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 245, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 147, in backward\n",
      "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt2$src.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiqxC-T0mNe3"
   },
   "source": [
    "6 epochs done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75jtnPa_5qSx",
    "outputId": "a2d7a657-d691-4b6e-ae07-2ffd98833188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 95000\tLoss: 49048.79688\tPPL: 5.84628\tbleu: 23.41406\tLR: 0.00030000\t*\n",
      "Steps: 100000\tLoss: 48546.58203\tPPL: 5.74153\tbleu: 24.03219\tLR: 0.00030000\t*\n",
      "Steps: 105000\tLoss: 48555.19531\tPPL: 5.74331\tbleu: 24.07039\tLR: 0.00030000\t\n",
      "Steps: 110000\tLoss: 48127.53125\tPPL: 5.65556\tbleu: 24.11453\tLR: 0.00030000\t*\n",
      "Steps: 115000\tLoss: 47717.62891\tPPL: 5.57272\tbleu: 24.40899\tLR: 0.00030000\t*\n",
      "Steps: 120000\tLoss: 47591.91797\tPPL: 5.54755\tbleu: 24.17666\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/rwen_reverse_transformer2/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saEkHjFe8lq5"
   },
   "outputs": [],
   "source": [
    "!python3 joeynmt/scripts/plot_validations.py joeynmt/models/rwen_reverse_transformer2 --plot_values bleu PPL  --output_path joeynmt/models/rwen_reverse_transformer2/bleu-ppl.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pe-cRa9fCVrW"
   },
   "outputs": [],
   "source": [
    "!python3 joeynmt/scripts/plot_validations.py joeynmt/models/rwen_reverse_transformer --plot_values bleu PPL  --output_path joeynmt/models/rwen_reverse_transformer2/bleu-ppl1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQW58RG1Ciob"
   },
   "source": [
    "![blue](https://drive.google.com/uc?id=1-1QTxbqngZ1G1fPf1yK9BxeAJOyFdAmk) ![blue2](https://drive.google.com/uc?id=1twVqeK43f2DJyZwhAkIJsR__rSenZxc8)\n",
    "\n",
    "https://drive.google.com/file/d/1-1QTxbqngZ1G1fPf1yK9BxeAJOyFdAmk/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FY0lSW49xpr"
   },
   "source": [
    "https://drive.google.com/file/d/1twVqeK43f2DJyZwhAkIJsR__rSenZxc8/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxDnInOKB35g",
    "outputId": "aaea953b-ec90-49f9-b61f-94f70a4291e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 08:15:14,699 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-01 08:15:14,703 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-01 08:15:15,018 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-01 08:15:16,056 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-01 08:15:17,306 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-01 08:15:17,331 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-01 08:15:20,078 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-01 08:15:20,345 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-01 08:15:20,418 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe.en)...\n",
      "2021-07-01 08:16:15,003 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 08:16:15,004 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 08:16:15,004 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 08:16:15,305 - INFO - joeynmt.prediction -  dev bleu[13a]:  24.76 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-01 08:16:15,307 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe.en)...\n",
      "2021-07-01 08:17:34,475 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 08:17:34,476 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 08:17:34,476 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 08:17:35,184 - INFO - joeynmt.prediction - test bleu[13a]:  35.30 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt test 'models/rwen_reverse_transformer2/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj7qLvxQtjcJ"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 120000\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/latest.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer2/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rwen_reverse_transformer2\"', f'model_dir: \"models/rwen_reverse_transformer2_continued\"').replace(\n",
    "            f'epochs: 15', f'epochs: 9').replace(f'validation_freq: 5000', f'validation_freq: 6000')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "h_Vzbp17tjEn",
    "outputId": "42a28bba-5d16-4a2b-acf0-529b5fcc3bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rwen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2/120000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 3600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 9                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 6000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rwen_reverse_transformer2_continued\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "    save_latest_ckpt: True\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_{name}_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aAoPluHyMQV",
    "outputId": "f40947cc-ba7e-4715-ad89-962ff494cfec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-20 07:32:55,694 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-20 07:32:55,776 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-20 07:33:06,892 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-20 07:33:07,180 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-20 07:33:08,666 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-20 07:33:10,034 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-20 07:33:10,035 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-20 07:33:10,404 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-20 07:33:11.933915: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-20 07:33:13,923 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-07-20 07:33:23,796 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2/120000.ckpt\n",
      "2021-07-20 07:33:24,247 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-07-20 07:33:24,248 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-07-20 07:33:24,248 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-20 07:33:24,248 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-07-20 07:33:24,248 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-07-20 07:33:24,248 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-07-20 07:33:24,249 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-20 07:33:24,249 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-20 07:33:24,249 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-20 07:33:24,249 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\n",
      "2021-07-20 07:33:24,249 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\n",
      "2021-07-20 07:33:24,249 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-20 07:33:24,249 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-20 07:33:24,250 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2/120000.ckpt\n",
      "2021-07-20 07:33:24,250 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-20 07:33:24,250 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-20 07:33:24,250 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-20 07:33:24,250 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-20 07:33:24,250 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-20 07:33:24,251 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-20 07:33:24,251 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-20 07:33:24,251 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-20 07:33:24,251 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-20 07:33:24,251 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-20 07:33:24,251 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-20 07:33:24,251 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-20 07:33:24,252 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-20 07:33:24,252 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-20 07:33:24,252 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-20 07:33:24,252 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-20 07:33:24,252 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-20 07:33:24,252 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-20 07:33:24,253 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-20 07:33:24,253 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-20 07:33:24,253 - INFO - joeynmt.helpers - cfg.training.epochs                : 9\n",
      "2021-07-20 07:33:24,253 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 6000\n",
      "2021-07-20 07:33:24,253 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-20 07:33:24,253 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-20 07:33:24,253 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer2_continued\n",
      "2021-07-20 07:33:24,254 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-20 07:33:24,254 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-20 07:33:24,254 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-20 07:33:24,254 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-20 07:33:24,254 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-20 07:33:24,254 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-20 07:33:24,254 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True\n",
      "2021-07-20 07:33:24,255 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-20 07:33:24,255 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-20 07:33:24,255 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-20 07:33:24,255 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-20 07:33:24,255 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-20 07:33:24,255 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-20 07:33:24,255 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-20 07:33:24,256 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-20 07:33:24,256 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-20 07:33:24,256 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-20 07:33:24,256 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-20 07:33:24,256 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-20 07:33:24,256 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-20 07:33:24,256 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-20 07:33:24,257 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-20 07:33:24,257 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-20 07:33:24,257 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-20 07:33:24,257 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-20 07:33:24,257 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-20 07:33:24,257 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-20 07:33:24,258 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-20 07:33:24,258 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-20 07:33:24,258 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-20 07:33:24,258 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-20 07:33:24,258 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-20 07:33:24,258 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434519,\n",
      "\tvalid 1000,\n",
      "\ttest 2651\n",
      "2021-07-20 07:33:24,258 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-07-20 07:33:24,259 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-20 07:33:24,259 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-20 07:33:24,259 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-07-20 07:33:24,259 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-07-20 07:33:24,259 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-07-20 07:33:24,271 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-20 07:33:24,272 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-20 07:33:39,039 - INFO - joeynmt.training - Epoch   1, Step:   120100, Batch Loss:     1.709934, Tokens per Sec:    15053, Lr: 0.000300\n",
      "2021-07-20 07:33:52,067 - INFO - joeynmt.training - Epoch   1, Step:   120200, Batch Loss:     1.988415, Tokens per Sec:    16584, Lr: 0.000300\n",
      "2021-07-20 07:34:05,344 - INFO - joeynmt.training - Epoch   1, Step:   120300, Batch Loss:     2.079929, Tokens per Sec:    16992, Lr: 0.000300\n",
      "2021-07-20 07:34:18,485 - INFO - joeynmt.training - Epoch   1, Step:   120400, Batch Loss:     1.892549, Tokens per Sec:    16745, Lr: 0.000300\n",
      "2021-07-20 07:34:31,620 - INFO - joeynmt.training - Epoch   1, Step:   120500, Batch Loss:     1.598998, Tokens per Sec:    16924, Lr: 0.000300\n",
      "2021-07-20 07:34:44,990 - INFO - joeynmt.training - Epoch   1, Step:   120600, Batch Loss:     1.898095, Tokens per Sec:    16554, Lr: 0.000300\n",
      "2021-07-20 07:34:58,512 - INFO - joeynmt.training - Epoch   1, Step:   120700, Batch Loss:     1.946430, Tokens per Sec:    16329, Lr: 0.000300\n",
      "2021-07-20 07:35:11,965 - INFO - joeynmt.training - Epoch   1, Step:   120800, Batch Loss:     2.094855, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-07-20 07:35:25,538 - INFO - joeynmt.training - Epoch   1, Step:   120900, Batch Loss:     1.718764, Tokens per Sec:    16132, Lr: 0.000300\n",
      "2021-07-20 07:35:39,218 - INFO - joeynmt.training - Epoch   1, Step:   121000, Batch Loss:     1.748765, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-07-20 07:35:52,907 - INFO - joeynmt.training - Epoch   1, Step:   121100, Batch Loss:     1.803349, Tokens per Sec:    15644, Lr: 0.000300\n",
      "2021-07-20 07:36:06,844 - INFO - joeynmt.training - Epoch   1, Step:   121200, Batch Loss:     2.012140, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-07-20 07:36:20,589 - INFO - joeynmt.training - Epoch   1, Step:   121300, Batch Loss:     1.989213, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-07-20 07:36:33,949 - INFO - joeynmt.training - Epoch   1, Step:   121400, Batch Loss:     1.857244, Tokens per Sec:    16157, Lr: 0.000300\n",
      "2021-07-20 07:36:47,595 - INFO - joeynmt.training - Epoch   1, Step:   121500, Batch Loss:     1.991130, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-07-20 07:37:01,327 - INFO - joeynmt.training - Epoch   1, Step:   121600, Batch Loss:     2.089783, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-07-20 07:37:15,288 - INFO - joeynmt.training - Epoch   1, Step:   121700, Batch Loss:     1.711073, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-07-20 07:37:29,036 - INFO - joeynmt.training - Epoch   1, Step:   121800, Batch Loss:     1.837523, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-20 07:37:42,667 - INFO - joeynmt.training - Epoch   1, Step:   121900, Batch Loss:     2.026772, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-07-20 07:37:56,313 - INFO - joeynmt.training - Epoch   1, Step:   122000, Batch Loss:     2.075100, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-07-20 07:38:03,882 - INFO - joeynmt.training - Epoch   1: total training loss 3893.80\n",
      "2021-07-20 07:38:03,882 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-20 07:38:10,808 - INFO - joeynmt.training - Epoch   2, Step:   122100, Batch Loss:     1.796378, Tokens per Sec:    14933, Lr: 0.000300\n",
      "2021-07-20 07:38:24,597 - INFO - joeynmt.training - Epoch   2, Step:   122200, Batch Loss:     2.075004, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-07-20 07:38:38,465 - INFO - joeynmt.training - Epoch   2, Step:   122300, Batch Loss:     2.051381, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-07-20 07:38:52,321 - INFO - joeynmt.training - Epoch   2, Step:   122400, Batch Loss:     1.829435, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-07-20 07:39:06,000 - INFO - joeynmt.training - Epoch   2, Step:   122500, Batch Loss:     1.785605, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-07-20 07:39:19,840 - INFO - joeynmt.training - Epoch   2, Step:   122600, Batch Loss:     1.709935, Tokens per Sec:    16161, Lr: 0.000300\n",
      "2021-07-20 07:39:33,466 - INFO - joeynmt.training - Epoch   2, Step:   122700, Batch Loss:     2.104550, Tokens per Sec:    16237, Lr: 0.000300\n",
      "2021-07-20 07:39:47,194 - INFO - joeynmt.training - Epoch   2, Step:   122800, Batch Loss:     1.891810, Tokens per Sec:    15809, Lr: 0.000300\n",
      "2021-07-20 07:40:01,018 - INFO - joeynmt.training - Epoch   2, Step:   122900, Batch Loss:     1.862729, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-07-20 07:40:14,538 - INFO - joeynmt.training - Epoch   2, Step:   123000, Batch Loss:     1.766655, Tokens per Sec:    15634, Lr: 0.000300\n",
      "2021-07-20 07:40:28,373 - INFO - joeynmt.training - Epoch   2, Step:   123100, Batch Loss:     1.848147, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-07-20 07:40:41,862 - INFO - joeynmt.training - Epoch   2, Step:   123200, Batch Loss:     1.570697, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-07-20 07:40:55,891 - INFO - joeynmt.training - Epoch   2, Step:   123300, Batch Loss:     1.871269, Tokens per Sec:    16217, Lr: 0.000300\n",
      "2021-07-20 07:41:09,682 - INFO - joeynmt.training - Epoch   2, Step:   123400, Batch Loss:     1.562340, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-07-20 07:41:23,398 - INFO - joeynmt.training - Epoch   2, Step:   123500, Batch Loss:     1.659806, Tokens per Sec:    16233, Lr: 0.000300\n",
      "2021-07-20 07:41:37,179 - INFO - joeynmt.training - Epoch   2, Step:   123600, Batch Loss:     1.956958, Tokens per Sec:    16233, Lr: 0.000300\n",
      "2021-07-20 07:41:50,878 - INFO - joeynmt.training - Epoch   2, Step:   123700, Batch Loss:     1.788213, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-07-20 07:42:04,684 - INFO - joeynmt.training - Epoch   2, Step:   123800, Batch Loss:     2.016618, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-20 07:42:18,507 - INFO - joeynmt.training - Epoch   2, Step:   123900, Batch Loss:     2.094252, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-07-20 07:42:32,223 - INFO - joeynmt.training - Epoch   2, Step:   124000, Batch Loss:     1.887861, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-07-20 07:42:45,866 - INFO - joeynmt.training - Epoch   2, Step:   124100, Batch Loss:     1.676415, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-07-20 07:42:59,484 - INFO - joeynmt.training - Epoch   2, Step:   124200, Batch Loss:     2.419829, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-07-20 07:43:13,155 - INFO - joeynmt.training - Epoch   2, Step:   124300, Batch Loss:     1.845732, Tokens per Sec:    15607, Lr: 0.000300\n",
      "2021-07-20 07:43:27,134 - INFO - joeynmt.training - Epoch   2, Step:   124400, Batch Loss:     2.155990, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-07-20 07:43:40,809 - INFO - joeynmt.training - Epoch   2, Step:   124500, Batch Loss:     1.761613, Tokens per Sec:    16235, Lr: 0.000300\n",
      "2021-07-20 07:43:54,349 - INFO - joeynmt.training - Epoch   2, Step:   124600, Batch Loss:     2.010268, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-07-20 07:44:08,094 - INFO - joeynmt.training - Epoch   2, Step:   124700, Batch Loss:     1.904672, Tokens per Sec:    16345, Lr: 0.000300\n",
      "2021-07-20 07:44:21,890 - INFO - joeynmt.training - Epoch   2, Step:   124800, Batch Loss:     1.855119, Tokens per Sec:    15705, Lr: 0.000300\n",
      "2021-07-20 07:44:35,730 - INFO - joeynmt.training - Epoch   2, Step:   124900, Batch Loss:     1.955084, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-07-20 07:44:49,578 - INFO - joeynmt.training - Epoch   2, Step:   125000, Batch Loss:     2.023702, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-07-20 07:45:03,248 - INFO - joeynmt.training - Epoch   2, Step:   125100, Batch Loss:     1.772826, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-07-20 07:45:16,921 - INFO - joeynmt.training - Epoch   2, Step:   125200, Batch Loss:     1.964163, Tokens per Sec:    15864, Lr: 0.000300\n",
      "2021-07-20 07:45:30,691 - INFO - joeynmt.training - Epoch   2, Step:   125300, Batch Loss:     1.776796, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-07-20 07:45:44,385 - INFO - joeynmt.training - Epoch   2, Step:   125400, Batch Loss:     1.735885, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-07-20 07:45:58,322 - INFO - joeynmt.training - Epoch   2, Step:   125500, Batch Loss:     1.845190, Tokens per Sec:    15765, Lr: 0.000300\n",
      "2021-07-20 07:46:12,214 - INFO - joeynmt.training - Epoch   2, Step:   125600, Batch Loss:     2.268088, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-20 07:46:25,877 - INFO - joeynmt.training - Epoch   2, Step:   125700, Batch Loss:     2.097722, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-07-20 07:46:39,664 - INFO - joeynmt.training - Epoch   2, Step:   125800, Batch Loss:     1.909314, Tokens per Sec:    15988, Lr: 0.000300\n",
      "2021-07-20 07:46:53,274 - INFO - joeynmt.training - Epoch   2, Step:   125900, Batch Loss:     1.815338, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-07-20 07:47:06,996 - INFO - joeynmt.training - Epoch   2, Step:   126000, Batch Loss:     1.757914, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-07-20 07:47:31,464 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 07:47:31,464 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 07:47:31,465 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 07:47:31,723 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-20 07:47:31,723 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-20 07:47:32,890 - INFO - joeynmt.training - Example #0\n",
      "2021-07-20 07:47:32,891 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-20 07:47:32,892 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-20 07:47:32,892 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by the heart .\n",
      "2021-07-20 07:47:32,892 - INFO - joeynmt.training - Example #1\n",
      "2021-07-20 07:47:32,892 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-20 07:47:32,892 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-20 07:47:32,893 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
      "2021-07-20 07:47:32,893 - INFO - joeynmt.training - Example #2\n",
      "2021-07-20 07:47:32,893 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-20 07:47:32,893 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 07:47:32,893 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 07:47:32,894 - INFO - joeynmt.training - Example #3\n",
      "2021-07-20 07:47:32,894 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-20 07:47:32,894 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-20 07:47:32,894 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of kindness in Satan’s world .\n",
      "2021-07-20 07:47:32,894 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   126000: bleu:  24.92, loss: 46811.8125, ppl:   5.3939, duration: 25.8983s\n",
      "2021-07-20 07:47:46,580 - INFO - joeynmt.training - Epoch   2, Step:   126100, Batch Loss:     1.700886, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-07-20 07:48:00,159 - INFO - joeynmt.training - Epoch   2, Step:   126200, Batch Loss:     1.985725, Tokens per Sec:    15725, Lr: 0.000300\n",
      "2021-07-20 07:48:14,082 - INFO - joeynmt.training - Epoch   2, Step:   126300, Batch Loss:     1.788089, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-07-20 07:48:28,103 - INFO - joeynmt.training - Epoch   2, Step:   126400, Batch Loss:     1.894586, Tokens per Sec:    16268, Lr: 0.000300\n",
      "2021-07-20 07:48:41,834 - INFO - joeynmt.training - Epoch   2, Step:   126500, Batch Loss:     1.945331, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-07-20 07:48:55,504 - INFO - joeynmt.training - Epoch   2, Step:   126600, Batch Loss:     1.824615, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-07-20 07:49:09,343 - INFO - joeynmt.training - Epoch   2, Step:   126700, Batch Loss:     1.824722, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-07-20 07:49:23,145 - INFO - joeynmt.training - Epoch   2, Step:   126800, Batch Loss:     2.005078, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-07-20 07:49:36,898 - INFO - joeynmt.training - Epoch   2, Step:   126900, Batch Loss:     2.156986, Tokens per Sec:    15995, Lr: 0.000300\n",
      "2021-07-20 07:49:50,768 - INFO - joeynmt.training - Epoch   2, Step:   127000, Batch Loss:     2.042942, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-07-20 07:50:04,673 - INFO - joeynmt.training - Epoch   2, Step:   127100, Batch Loss:     1.969986, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-07-20 07:50:18,218 - INFO - joeynmt.training - Epoch   2, Step:   127200, Batch Loss:     1.771011, Tokens per Sec:    15848, Lr: 0.000300\n",
      "2021-07-20 07:50:31,842 - INFO - joeynmt.training - Epoch   2, Step:   127300, Batch Loss:     1.668186, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-07-20 07:50:45,453 - INFO - joeynmt.training - Epoch   2, Step:   127400, Batch Loss:     1.740719, Tokens per Sec:    16278, Lr: 0.000300\n",
      "2021-07-20 07:50:55,961 - INFO - joeynmt.training - Epoch   2: total training loss 10178.93\n",
      "2021-07-20 07:50:55,962 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-20 07:50:59,985 - INFO - joeynmt.training - Epoch   3, Step:   127500, Batch Loss:     1.929451, Tokens per Sec:    13893, Lr: 0.000300\n",
      "2021-07-20 07:51:13,658 - INFO - joeynmt.training - Epoch   3, Step:   127600, Batch Loss:     1.929737, Tokens per Sec:    15710, Lr: 0.000300\n",
      "2021-07-20 07:51:27,344 - INFO - joeynmt.training - Epoch   3, Step:   127700, Batch Loss:     1.926008, Tokens per Sec:    16294, Lr: 0.000300\n",
      "2021-07-20 07:51:40,946 - INFO - joeynmt.training - Epoch   3, Step:   127800, Batch Loss:     1.510528, Tokens per Sec:    16151, Lr: 0.000300\n",
      "2021-07-20 07:51:54,538 - INFO - joeynmt.training - Epoch   3, Step:   127900, Batch Loss:     1.773397, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-07-20 07:52:08,297 - INFO - joeynmt.training - Epoch   3, Step:   128000, Batch Loss:     1.929796, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-07-20 07:52:21,920 - INFO - joeynmt.training - Epoch   3, Step:   128100, Batch Loss:     1.845219, Tokens per Sec:    16172, Lr: 0.000300\n",
      "2021-07-20 07:52:35,772 - INFO - joeynmt.training - Epoch   3, Step:   128200, Batch Loss:     1.882393, Tokens per Sec:    16265, Lr: 0.000300\n",
      "2021-07-20 07:52:49,480 - INFO - joeynmt.training - Epoch   3, Step:   128300, Batch Loss:     1.832332, Tokens per Sec:    16274, Lr: 0.000300\n",
      "2021-07-20 07:53:02,992 - INFO - joeynmt.training - Epoch   3, Step:   128400, Batch Loss:     1.883749, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-07-20 07:53:16,547 - INFO - joeynmt.training - Epoch   3, Step:   128500, Batch Loss:     1.603257, Tokens per Sec:    15641, Lr: 0.000300\n",
      "2021-07-20 07:53:30,260 - INFO - joeynmt.training - Epoch   3, Step:   128600, Batch Loss:     1.677638, Tokens per Sec:    16223, Lr: 0.000300\n",
      "2021-07-20 07:53:43,993 - INFO - joeynmt.training - Epoch   3, Step:   128700, Batch Loss:     1.877751, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-07-20 07:53:57,641 - INFO - joeynmt.training - Epoch   3, Step:   128800, Batch Loss:     1.879059, Tokens per Sec:    16042, Lr: 0.000300\n",
      "2021-07-20 07:54:11,531 - INFO - joeynmt.training - Epoch   3, Step:   128900, Batch Loss:     1.795531, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-07-20 07:54:25,344 - INFO - joeynmt.training - Epoch   3, Step:   129000, Batch Loss:     1.799680, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-07-20 07:54:39,134 - INFO - joeynmt.training - Epoch   3, Step:   129100, Batch Loss:     1.973373, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-07-20 07:54:53,012 - INFO - joeynmt.training - Epoch   3, Step:   129200, Batch Loss:     1.881265, Tokens per Sec:    15800, Lr: 0.000300\n",
      "2021-07-20 07:55:06,751 - INFO - joeynmt.training - Epoch   3, Step:   129300, Batch Loss:     1.798074, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-07-20 07:55:20,286 - INFO - joeynmt.training - Epoch   3, Step:   129400, Batch Loss:     1.754781, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-07-20 07:55:34,043 - INFO - joeynmt.training - Epoch   3, Step:   129500, Batch Loss:     2.119505, Tokens per Sec:    16208, Lr: 0.000300\n",
      "2021-07-20 07:55:47,767 - INFO - joeynmt.training - Epoch   3, Step:   129600, Batch Loss:     2.021500, Tokens per Sec:    16190, Lr: 0.000300\n",
      "2021-07-20 07:56:01,557 - INFO - joeynmt.training - Epoch   3, Step:   129700, Batch Loss:     1.927607, Tokens per Sec:    15748, Lr: 0.000300\n",
      "2021-07-20 07:56:15,355 - INFO - joeynmt.training - Epoch   3, Step:   129800, Batch Loss:     1.692860, Tokens per Sec:    16168, Lr: 0.000300\n",
      "2021-07-20 07:56:29,100 - INFO - joeynmt.training - Epoch   3, Step:   129900, Batch Loss:     1.688551, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-07-20 07:56:42,559 - INFO - joeynmt.training - Epoch   3, Step:   130000, Batch Loss:     1.706852, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-07-20 07:56:56,426 - INFO - joeynmt.training - Epoch   3, Step:   130100, Batch Loss:     1.873910, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-07-20 07:57:10,317 - INFO - joeynmt.training - Epoch   3, Step:   130200, Batch Loss:     1.696288, Tokens per Sec:    16057, Lr: 0.000300\n",
      "2021-07-20 07:57:23,985 - INFO - joeynmt.training - Epoch   3, Step:   130300, Batch Loss:     1.864620, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-07-20 07:57:37,628 - INFO - joeynmt.training - Epoch   3, Step:   130400, Batch Loss:     1.809196, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-07-20 07:57:51,350 - INFO - joeynmt.training - Epoch   3, Step:   130500, Batch Loss:     1.708330, Tokens per Sec:    16149, Lr: 0.000300\n",
      "2021-07-20 07:58:05,002 - INFO - joeynmt.training - Epoch   3, Step:   130600, Batch Loss:     1.897594, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-07-20 07:58:18,690 - INFO - joeynmt.training - Epoch   3, Step:   130700, Batch Loss:     1.901513, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-07-20 07:58:32,384 - INFO - joeynmt.training - Epoch   3, Step:   130800, Batch Loss:     1.801595, Tokens per Sec:    16273, Lr: 0.000300\n",
      "2021-07-20 07:58:46,002 - INFO - joeynmt.training - Epoch   3, Step:   130900, Batch Loss:     1.806068, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-07-20 07:58:59,798 - INFO - joeynmt.training - Epoch   3, Step:   131000, Batch Loss:     1.949075, Tokens per Sec:    16236, Lr: 0.000300\n",
      "2021-07-20 07:59:13,605 - INFO - joeynmt.training - Epoch   3, Step:   131100, Batch Loss:     1.949498, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-07-20 07:59:27,306 - INFO - joeynmt.training - Epoch   3, Step:   131200, Batch Loss:     1.913342, Tokens per Sec:    16120, Lr: 0.000300\n",
      "2021-07-20 07:59:40,851 - INFO - joeynmt.training - Epoch   3, Step:   131300, Batch Loss:     1.888562, Tokens per Sec:    15990, Lr: 0.000300\n",
      "2021-07-20 07:59:54,515 - INFO - joeynmt.training - Epoch   3, Step:   131400, Batch Loss:     1.814591, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-07-20 08:00:08,093 - INFO - joeynmt.training - Epoch   3, Step:   131500, Batch Loss:     1.893653, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-07-20 08:00:21,752 - INFO - joeynmt.training - Epoch   3, Step:   131600, Batch Loss:     2.049397, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-07-20 08:00:35,515 - INFO - joeynmt.training - Epoch   3, Step:   131700, Batch Loss:     1.855026, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-07-20 08:00:49,348 - INFO - joeynmt.training - Epoch   3, Step:   131800, Batch Loss:     2.155432, Tokens per Sec:    15957, Lr: 0.000300\n",
      "2021-07-20 08:01:03,092 - INFO - joeynmt.training - Epoch   3, Step:   131900, Batch Loss:     1.655921, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-07-20 08:01:16,885 - INFO - joeynmt.training - Epoch   3, Step:   132000, Batch Loss:     1.786550, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-07-20 08:01:39,041 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 08:01:39,041 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 08:01:39,042 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 08:01:39,284 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-20 08:01:39,284 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-20 08:01:40,035 - INFO - joeynmt.training - Example #0\n",
      "2021-07-20 08:01:40,037 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-20 08:01:40,037 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-20 08:01:40,038 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-20 08:01:40,038 - INFO - joeynmt.training - Example #1\n",
      "2021-07-20 08:01:40,039 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-20 08:01:40,039 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-20 08:01:40,039 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
      "2021-07-20 08:01:40,039 - INFO - joeynmt.training - Example #2\n",
      "2021-07-20 08:01:40,040 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-20 08:01:40,040 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:01:40,040 - INFO - joeynmt.training - \tHypothesis: Rather than stress or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:01:40,040 - INFO - joeynmt.training - Example #3\n",
      "2021-07-20 08:01:40,041 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-20 08:01:40,041 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-20 08:01:40,041 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-20 08:01:40,042 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   132000: bleu:  25.39, loss: 46380.6523, ppl:   5.3108, duration: 23.1559s\n",
      "2021-07-20 08:01:54,094 - INFO - joeynmt.training - Epoch   3, Step:   132100, Batch Loss:     1.901061, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-07-20 08:02:07,755 - INFO - joeynmt.training - Epoch   3, Step:   132200, Batch Loss:     2.001971, Tokens per Sec:    15529, Lr: 0.000300\n",
      "2021-07-20 08:02:21,516 - INFO - joeynmt.training - Epoch   3, Step:   132300, Batch Loss:     1.850204, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-07-20 08:02:35,177 - INFO - joeynmt.training - Epoch   3, Step:   132400, Batch Loss:     2.067999, Tokens per Sec:    16220, Lr: 0.000300\n",
      "2021-07-20 08:02:48,824 - INFO - joeynmt.training - Epoch   3, Step:   132500, Batch Loss:     1.809605, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-07-20 08:03:02,616 - INFO - joeynmt.training - Epoch   3, Step:   132600, Batch Loss:     1.955484, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-07-20 08:03:16,369 - INFO - joeynmt.training - Epoch   3, Step:   132700, Batch Loss:     2.152063, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-07-20 08:03:30,006 - INFO - joeynmt.training - Epoch   3, Step:   132800, Batch Loss:     1.940093, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-07-20 08:03:43,442 - INFO - joeynmt.training - Epoch   3: total training loss 10094.06\n",
      "2021-07-20 08:03:43,442 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-20 08:03:44,312 - INFO - joeynmt.training - Epoch   4, Step:   132900, Batch Loss:     1.883863, Tokens per Sec:     5451, Lr: 0.000300\n",
      "2021-07-20 08:03:57,829 - INFO - joeynmt.training - Epoch   4, Step:   133000, Batch Loss:     1.934423, Tokens per Sec:    15882, Lr: 0.000300\n",
      "2021-07-20 08:04:11,669 - INFO - joeynmt.training - Epoch   4, Step:   133100, Batch Loss:     1.709852, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-07-20 08:04:25,402 - INFO - joeynmt.training - Epoch   4, Step:   133200, Batch Loss:     1.879596, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-07-20 08:04:38,912 - INFO - joeynmt.training - Epoch   4, Step:   133300, Batch Loss:     1.742530, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-07-20 08:04:52,509 - INFO - joeynmt.training - Epoch   4, Step:   133400, Batch Loss:     1.950866, Tokens per Sec:    16038, Lr: 0.000300\n",
      "2021-07-20 08:05:06,267 - INFO - joeynmt.training - Epoch   4, Step:   133500, Batch Loss:     2.004762, Tokens per Sec:    16085, Lr: 0.000300\n",
      "2021-07-20 08:05:20,006 - INFO - joeynmt.training - Epoch   4, Step:   133600, Batch Loss:     1.945859, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-07-20 08:05:33,754 - INFO - joeynmt.training - Epoch   4, Step:   133700, Batch Loss:     1.903571, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-07-20 08:05:47,710 - INFO - joeynmt.training - Epoch   4, Step:   133800, Batch Loss:     2.242898, Tokens per Sec:    16236, Lr: 0.000300\n",
      "2021-07-20 08:06:01,418 - INFO - joeynmt.training - Epoch   4, Step:   133900, Batch Loss:     1.822340, Tokens per Sec:    16070, Lr: 0.000300\n",
      "2021-07-20 08:06:15,078 - INFO - joeynmt.training - Epoch   4, Step:   134000, Batch Loss:     2.114249, Tokens per Sec:    15787, Lr: 0.000300\n",
      "2021-07-20 08:06:28,659 - INFO - joeynmt.training - Epoch   4, Step:   134100, Batch Loss:     1.717172, Tokens per Sec:    16321, Lr: 0.000300\n",
      "2021-07-20 08:06:42,342 - INFO - joeynmt.training - Epoch   4, Step:   134200, Batch Loss:     2.139710, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-07-20 08:06:56,122 - INFO - joeynmt.training - Epoch   4, Step:   134300, Batch Loss:     1.818399, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-07-20 08:07:09,928 - INFO - joeynmt.training - Epoch   4, Step:   134400, Batch Loss:     1.702976, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-07-20 08:07:23,461 - INFO - joeynmt.training - Epoch   4, Step:   134500, Batch Loss:     1.761622, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-07-20 08:07:36,942 - INFO - joeynmt.training - Epoch   4, Step:   134600, Batch Loss:     1.948210, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-07-20 08:07:50,625 - INFO - joeynmt.training - Epoch   4, Step:   134700, Batch Loss:     1.871238, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-07-20 08:08:04,494 - INFO - joeynmt.training - Epoch   4, Step:   134800, Batch Loss:     1.766674, Tokens per Sec:    15992, Lr: 0.000300\n",
      "2021-07-20 08:08:18,077 - INFO - joeynmt.training - Epoch   4, Step:   134900, Batch Loss:     1.893490, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-07-20 08:08:31,544 - INFO - joeynmt.training - Epoch   4, Step:   135000, Batch Loss:     2.025028, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-07-20 08:08:45,217 - INFO - joeynmt.training - Epoch   4, Step:   135100, Batch Loss:     1.750707, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-07-20 08:08:59,012 - INFO - joeynmt.training - Epoch   4, Step:   135200, Batch Loss:     1.711011, Tokens per Sec:    16462, Lr: 0.000300\n",
      "2021-07-20 08:09:12,794 - INFO - joeynmt.training - Epoch   4, Step:   135300, Batch Loss:     1.861680, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-07-20 08:09:26,383 - INFO - joeynmt.training - Epoch   4, Step:   135400, Batch Loss:     2.014124, Tokens per Sec:    15937, Lr: 0.000300\n",
      "2021-07-20 08:09:40,056 - INFO - joeynmt.training - Epoch   4, Step:   135500, Batch Loss:     1.925362, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-20 08:09:53,706 - INFO - joeynmt.training - Epoch   4, Step:   135600, Batch Loss:     2.002445, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-07-20 08:10:07,611 - INFO - joeynmt.training - Epoch   4, Step:   135700, Batch Loss:     1.822913, Tokens per Sec:    16177, Lr: 0.000300\n",
      "2021-07-20 08:10:21,353 - INFO - joeynmt.training - Epoch   4, Step:   135800, Batch Loss:     1.904131, Tokens per Sec:    15616, Lr: 0.000300\n",
      "2021-07-20 08:10:35,316 - INFO - joeynmt.training - Epoch   4, Step:   135900, Batch Loss:     1.693343, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-07-20 08:10:48,836 - INFO - joeynmt.training - Epoch   4, Step:   136000, Batch Loss:     1.835582, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-07-20 08:11:02,559 - INFO - joeynmt.training - Epoch   4, Step:   136100, Batch Loss:     2.047246, Tokens per Sec:    16402, Lr: 0.000300\n",
      "2021-07-20 08:11:16,432 - INFO - joeynmt.training - Epoch   4, Step:   136200, Batch Loss:     1.732536, Tokens per Sec:    16197, Lr: 0.000300\n",
      "2021-07-20 08:11:30,263 - INFO - joeynmt.training - Epoch   4, Step:   136300, Batch Loss:     1.927529, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-07-20 08:11:43,986 - INFO - joeynmt.training - Epoch   4, Step:   136400, Batch Loss:     1.791389, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-07-20 08:11:57,837 - INFO - joeynmt.training - Epoch   4, Step:   136500, Batch Loss:     1.853618, Tokens per Sec:    15886, Lr: 0.000300\n",
      "2021-07-20 08:12:11,434 - INFO - joeynmt.training - Epoch   4, Step:   136600, Batch Loss:     1.623618, Tokens per Sec:    15584, Lr: 0.000300\n",
      "2021-07-20 08:12:25,102 - INFO - joeynmt.training - Epoch   4, Step:   136700, Batch Loss:     1.799249, Tokens per Sec:    15763, Lr: 0.000300\n",
      "2021-07-20 08:12:38,749 - INFO - joeynmt.training - Epoch   4, Step:   136800, Batch Loss:     1.701378, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-07-20 08:12:52,520 - INFO - joeynmt.training - Epoch   4, Step:   136900, Batch Loss:     1.840131, Tokens per Sec:    16000, Lr: 0.000300\n",
      "2021-07-20 08:13:06,058 - INFO - joeynmt.training - Epoch   4, Step:   137000, Batch Loss:     1.677664, Tokens per Sec:    15654, Lr: 0.000300\n",
      "2021-07-20 08:13:19,871 - INFO - joeynmt.training - Epoch   4, Step:   137100, Batch Loss:     1.757084, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-20 08:13:33,474 - INFO - joeynmt.training - Epoch   4, Step:   137200, Batch Loss:     1.822682, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-07-20 08:13:47,082 - INFO - joeynmt.training - Epoch   4, Step:   137300, Batch Loss:     1.743982, Tokens per Sec:    16402, Lr: 0.000300\n",
      "2021-07-20 08:14:00,813 - INFO - joeynmt.training - Epoch   4, Step:   137400, Batch Loss:     1.747323, Tokens per Sec:    15707, Lr: 0.000300\n",
      "2021-07-20 08:14:14,743 - INFO - joeynmt.training - Epoch   4, Step:   137500, Batch Loss:     1.622015, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-07-20 08:14:28,287 - INFO - joeynmt.training - Epoch   4, Step:   137600, Batch Loss:     1.898194, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-07-20 08:14:42,143 - INFO - joeynmt.training - Epoch   4, Step:   137700, Batch Loss:     1.989244, Tokens per Sec:    16454, Lr: 0.000300\n",
      "2021-07-20 08:14:55,698 - INFO - joeynmt.training - Epoch   4, Step:   137800, Batch Loss:     1.977377, Tokens per Sec:    15962, Lr: 0.000300\n",
      "2021-07-20 08:15:09,438 - INFO - joeynmt.training - Epoch   4, Step:   137900, Batch Loss:     1.897025, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-07-20 08:15:23,434 - INFO - joeynmt.training - Epoch   4, Step:   138000, Batch Loss:     1.888445, Tokens per Sec:    16171, Lr: 0.000300\n",
      "2021-07-20 08:15:45,920 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 08:15:45,921 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 08:15:45,921 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 08:15:46,166 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-20 08:15:46,166 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-20 08:15:46,913 - INFO - joeynmt.training - Example #0\n",
      "2021-07-20 08:15:46,914 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-20 08:15:46,914 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-20 08:15:46,915 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-20 08:15:46,915 - INFO - joeynmt.training - Example #1\n",
      "2021-07-20 08:15:46,915 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-20 08:15:46,915 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-20 08:15:46,916 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
      "2021-07-20 08:15:46,916 - INFO - joeynmt.training - Example #2\n",
      "2021-07-20 08:15:46,916 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-20 08:15:46,917 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:15:46,918 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:15:46,918 - INFO - joeynmt.training - Example #3\n",
      "2021-07-20 08:15:46,918 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-20 08:15:46,919 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-20 08:15:46,919 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-20 08:15:46,919 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   138000: bleu:  25.64, loss: 45987.5547, ppl:   5.2362, duration: 23.4847s\n",
      "2021-07-20 08:16:00,814 - INFO - joeynmt.training - Epoch   4, Step:   138100, Batch Loss:     1.839525, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-07-20 08:16:14,564 - INFO - joeynmt.training - Epoch   4, Step:   138200, Batch Loss:     1.845396, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-20 08:16:28,340 - INFO - joeynmt.training - Epoch   4, Step:   138300, Batch Loss:     1.883001, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-07-20 08:16:31,468 - INFO - joeynmt.training - Epoch   4: total training loss 10029.43\n",
      "2021-07-20 08:16:31,468 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-20 08:16:42,651 - INFO - joeynmt.training - Epoch   5, Step:   138400, Batch Loss:     2.085270, Tokens per Sec:    15246, Lr: 0.000300\n",
      "2021-07-20 08:16:56,572 - INFO - joeynmt.training - Epoch   5, Step:   138500, Batch Loss:     1.729922, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-07-20 08:17:10,298 - INFO - joeynmt.training - Epoch   5, Step:   138600, Batch Loss:     1.962518, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-07-20 08:17:24,112 - INFO - joeynmt.training - Epoch   5, Step:   138700, Batch Loss:     1.952388, Tokens per Sec:    15919, Lr: 0.000300\n",
      "2021-07-20 08:17:37,761 - INFO - joeynmt.training - Epoch   5, Step:   138800, Batch Loss:     1.874715, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-07-20 08:17:51,507 - INFO - joeynmt.training - Epoch   5, Step:   138900, Batch Loss:     2.249805, Tokens per Sec:    15633, Lr: 0.000300\n",
      "2021-07-20 08:18:05,367 - INFO - joeynmt.training - Epoch   5, Step:   139000, Batch Loss:     1.883699, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-07-20 08:18:19,105 - INFO - joeynmt.training - Epoch   5, Step:   139100, Batch Loss:     1.958728, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-07-20 08:18:32,737 - INFO - joeynmt.training - Epoch   5, Step:   139200, Batch Loss:     1.873076, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-07-20 08:18:46,361 - INFO - joeynmt.training - Epoch   5, Step:   139300, Batch Loss:     1.890562, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-07-20 08:19:00,133 - INFO - joeynmt.training - Epoch   5, Step:   139400, Batch Loss:     1.775712, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-20 08:19:13,701 - INFO - joeynmt.training - Epoch   5, Step:   139500, Batch Loss:     1.761437, Tokens per Sec:    15809, Lr: 0.000300\n",
      "2021-07-20 08:19:27,321 - INFO - joeynmt.training - Epoch   5, Step:   139600, Batch Loss:     1.720361, Tokens per Sec:    16142, Lr: 0.000300\n",
      "2021-07-20 08:19:40,802 - INFO - joeynmt.training - Epoch   5, Step:   139700, Batch Loss:     1.644209, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-07-20 08:19:54,676 - INFO - joeynmt.training - Epoch   5, Step:   139800, Batch Loss:     1.726320, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-07-20 08:20:08,461 - INFO - joeynmt.training - Epoch   5, Step:   139900, Batch Loss:     1.723714, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-07-20 08:20:22,376 - INFO - joeynmt.training - Epoch   5, Step:   140000, Batch Loss:     1.978070, Tokens per Sec:    16121, Lr: 0.000300\n",
      "2021-07-20 08:20:35,956 - INFO - joeynmt.training - Epoch   5, Step:   140100, Batch Loss:     1.637487, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-07-20 08:20:49,635 - INFO - joeynmt.training - Epoch   5, Step:   140200, Batch Loss:     1.856852, Tokens per Sec:    16335, Lr: 0.000300\n",
      "2021-07-20 08:21:03,310 - INFO - joeynmt.training - Epoch   5, Step:   140300, Batch Loss:     1.759382, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-07-20 08:21:17,182 - INFO - joeynmt.training - Epoch   5, Step:   140400, Batch Loss:     2.188813, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-07-20 08:21:30,951 - INFO - joeynmt.training - Epoch   5, Step:   140500, Batch Loss:     1.844109, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-07-20 08:21:44,541 - INFO - joeynmt.training - Epoch   5, Step:   140600, Batch Loss:     1.795531, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-07-20 08:21:58,203 - INFO - joeynmt.training - Epoch   5, Step:   140700, Batch Loss:     1.753421, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-07-20 08:22:11,610 - INFO - joeynmt.training - Epoch   5, Step:   140800, Batch Loss:     1.749486, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-07-20 08:22:25,582 - INFO - joeynmt.training - Epoch   5, Step:   140900, Batch Loss:     1.820068, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-07-20 08:22:39,381 - INFO - joeynmt.training - Epoch   5, Step:   141000, Batch Loss:     2.151453, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-07-20 08:22:53,111 - INFO - joeynmt.training - Epoch   5, Step:   141100, Batch Loss:     1.951364, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-07-20 08:23:06,954 - INFO - joeynmt.training - Epoch   5, Step:   141200, Batch Loss:     1.854592, Tokens per Sec:    16188, Lr: 0.000300\n",
      "2021-07-20 08:23:20,472 - INFO - joeynmt.training - Epoch   5, Step:   141300, Batch Loss:     2.045845, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-07-20 08:23:34,338 - INFO - joeynmt.training - Epoch   5, Step:   141400, Batch Loss:     1.762859, Tokens per Sec:    16403, Lr: 0.000300\n",
      "2021-07-20 08:23:48,119 - INFO - joeynmt.training - Epoch   5, Step:   141500, Batch Loss:     2.007807, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-07-20 08:24:01,802 - INFO - joeynmt.training - Epoch   5, Step:   141600, Batch Loss:     1.779933, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-07-20 08:24:15,562 - INFO - joeynmt.training - Epoch   5, Step:   141700, Batch Loss:     1.817731, Tokens per Sec:    16312, Lr: 0.000300\n",
      "2021-07-20 08:24:29,092 - INFO - joeynmt.training - Epoch   5, Step:   141800, Batch Loss:     1.867001, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-07-20 08:24:42,653 - INFO - joeynmt.training - Epoch   5, Step:   141900, Batch Loss:     1.702745, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-07-20 08:24:56,063 - INFO - joeynmt.training - Epoch   5, Step:   142000, Batch Loss:     1.948361, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-07-20 08:25:10,058 - INFO - joeynmt.training - Epoch   5, Step:   142100, Batch Loss:     1.958218, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-07-20 08:25:23,632 - INFO - joeynmt.training - Epoch   5, Step:   142200, Batch Loss:     1.990282, Tokens per Sec:    15605, Lr: 0.000300\n",
      "2021-07-20 08:25:37,234 - INFO - joeynmt.training - Epoch   5, Step:   142300, Batch Loss:     1.917465, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-07-20 08:25:50,891 - INFO - joeynmt.training - Epoch   5, Step:   142400, Batch Loss:     1.630462, Tokens per Sec:    16242, Lr: 0.000300\n",
      "2021-07-20 08:26:04,530 - INFO - joeynmt.training - Epoch   5, Step:   142500, Batch Loss:     1.597460, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-07-20 08:26:18,167 - INFO - joeynmt.training - Epoch   5, Step:   142600, Batch Loss:     1.910890, Tokens per Sec:    15910, Lr: 0.000300\n",
      "2021-07-20 08:26:31,836 - INFO - joeynmt.training - Epoch   5, Step:   142700, Batch Loss:     1.953720, Tokens per Sec:    16122, Lr: 0.000300\n",
      "2021-07-20 08:26:45,584 - INFO - joeynmt.training - Epoch   5, Step:   142800, Batch Loss:     1.723622, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-20 08:26:59,241 - INFO - joeynmt.training - Epoch   5, Step:   142900, Batch Loss:     2.089214, Tokens per Sec:    16337, Lr: 0.000300\n",
      "2021-07-20 08:27:13,022 - INFO - joeynmt.training - Epoch   5, Step:   143000, Batch Loss:     1.643326, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-07-20 08:27:26,769 - INFO - joeynmt.training - Epoch   5, Step:   143100, Batch Loss:     1.800388, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-07-20 08:27:40,698 - INFO - joeynmt.training - Epoch   5, Step:   143200, Batch Loss:     1.927096, Tokens per Sec:    16312, Lr: 0.000300\n",
      "2021-07-20 08:27:54,481 - INFO - joeynmt.training - Epoch   5, Step:   143300, Batch Loss:     1.741035, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-07-20 08:28:08,076 - INFO - joeynmt.training - Epoch   5, Step:   143400, Batch Loss:     1.747868, Tokens per Sec:    15666, Lr: 0.000300\n",
      "2021-07-20 08:28:21,669 - INFO - joeynmt.training - Epoch   5, Step:   143500, Batch Loss:     1.712807, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-07-20 08:28:35,333 - INFO - joeynmt.training - Epoch   5, Step:   143600, Batch Loss:     1.868836, Tokens per Sec:    16265, Lr: 0.000300\n",
      "2021-07-20 08:28:49,045 - INFO - joeynmt.training - Epoch   5, Step:   143700, Batch Loss:     1.918408, Tokens per Sec:    15952, Lr: 0.000300\n",
      "2021-07-20 08:28:55,990 - INFO - joeynmt.training - Epoch   5: total training loss 9974.96\n",
      "2021-07-20 08:28:55,991 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-20 08:29:03,559 - INFO - joeynmt.training - Epoch   6, Step:   143800, Batch Loss:     1.547377, Tokens per Sec:    14543, Lr: 0.000300\n",
      "2021-07-20 08:29:17,325 - INFO - joeynmt.training - Epoch   6, Step:   143900, Batch Loss:     2.002531, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-07-20 08:29:31,036 - INFO - joeynmt.training - Epoch   6, Step:   144000, Batch Loss:     1.752307, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-07-20 08:29:54,866 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 08:29:54,867 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 08:29:54,867 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 08:29:55,137 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-20 08:29:55,137 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-20 08:29:55,900 - INFO - joeynmt.training - Example #0\n",
      "2021-07-20 08:29:55,901 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-20 08:29:55,901 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-20 08:29:55,901 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-07-20 08:29:55,901 - INFO - joeynmt.training - Example #1\n",
      "2021-07-20 08:29:55,902 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-20 08:29:55,902 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-20 08:29:55,902 - INFO - joeynmt.training - \tHypothesis: The letter was written in the pillar on the side of the scroll .\n",
      "2021-07-20 08:29:55,902 - INFO - joeynmt.training - Example #2\n",
      "2021-07-20 08:29:55,903 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-20 08:29:55,903 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:29:55,903 - INFO - joeynmt.training - \tHypothesis: Instead of stress or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:29:55,903 - INFO - joeynmt.training - Example #3\n",
      "2021-07-20 08:29:55,904 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-20 08:29:55,904 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-20 08:29:55,904 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-20 08:29:55,904 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   144000: bleu:  25.69, loss: 45888.5312, ppl:   5.2176, duration: 24.8677s\n",
      "2021-07-20 08:30:09,708 - INFO - joeynmt.training - Epoch   6, Step:   144100, Batch Loss:     1.767578, Tokens per Sec:    15543, Lr: 0.000300\n",
      "2021-07-20 08:30:23,514 - INFO - joeynmt.training - Epoch   6, Step:   144200, Batch Loss:     1.800720, Tokens per Sec:    16320, Lr: 0.000300\n",
      "2021-07-20 08:30:37,174 - INFO - joeynmt.training - Epoch   6, Step:   144300, Batch Loss:     1.788596, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-07-20 08:30:50,728 - INFO - joeynmt.training - Epoch   6, Step:   144400, Batch Loss:     1.874823, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-07-20 08:31:04,536 - INFO - joeynmt.training - Epoch   6, Step:   144500, Batch Loss:     1.560641, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-07-20 08:31:18,313 - INFO - joeynmt.training - Epoch   6, Step:   144600, Batch Loss:     1.695134, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-20 08:31:32,045 - INFO - joeynmt.training - Epoch   6, Step:   144700, Batch Loss:     2.041755, Tokens per Sec:    16100, Lr: 0.000300\n",
      "2021-07-20 08:31:45,508 - INFO - joeynmt.training - Epoch   6, Step:   144800, Batch Loss:     1.744333, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-07-20 08:31:59,229 - INFO - joeynmt.training - Epoch   6, Step:   144900, Batch Loss:     1.981297, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-07-20 08:32:13,130 - INFO - joeynmt.training - Epoch   6, Step:   145000, Batch Loss:     1.825389, Tokens per Sec:    15698, Lr: 0.000300\n",
      "2021-07-20 08:32:26,867 - INFO - joeynmt.training - Epoch   6, Step:   145100, Batch Loss:     1.562145, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-07-20 08:32:40,601 - INFO - joeynmt.training - Epoch   6, Step:   145200, Batch Loss:     1.591709, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-07-20 08:32:54,140 - INFO - joeynmt.training - Epoch   6, Step:   145300, Batch Loss:     1.871464, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-07-20 08:33:07,822 - INFO - joeynmt.training - Epoch   6, Step:   145400, Batch Loss:     1.894829, Tokens per Sec:    16253, Lr: 0.000300\n",
      "2021-07-20 08:33:21,816 - INFO - joeynmt.training - Epoch   6, Step:   145500, Batch Loss:     1.665438, Tokens per Sec:    16217, Lr: 0.000300\n",
      "2021-07-20 08:33:35,575 - INFO - joeynmt.training - Epoch   6, Step:   145600, Batch Loss:     2.171322, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-20 08:33:49,263 - INFO - joeynmt.training - Epoch   6, Step:   145700, Batch Loss:     1.828225, Tokens per Sec:    15653, Lr: 0.000300\n",
      "2021-07-20 08:34:02,903 - INFO - joeynmt.training - Epoch   6, Step:   145800, Batch Loss:     1.844808, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-07-20 08:34:16,443 - INFO - joeynmt.training - Epoch   6, Step:   145900, Batch Loss:     1.954762, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-07-20 08:34:30,242 - INFO - joeynmt.training - Epoch   6, Step:   146000, Batch Loss:     1.811034, Tokens per Sec:    16285, Lr: 0.000300\n",
      "2021-07-20 08:34:43,891 - INFO - joeynmt.training - Epoch   6, Step:   146100, Batch Loss:     1.803016, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-07-20 08:34:57,569 - INFO - joeynmt.training - Epoch   6, Step:   146200, Batch Loss:     2.018703, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-07-20 08:35:11,329 - INFO - joeynmt.training - Epoch   6, Step:   146300, Batch Loss:     1.762788, Tokens per Sec:    15875, Lr: 0.000300\n",
      "2021-07-20 08:35:25,162 - INFO - joeynmt.training - Epoch   6, Step:   146400, Batch Loss:     1.910857, Tokens per Sec:    16513, Lr: 0.000300\n",
      "2021-07-20 08:35:38,990 - INFO - joeynmt.training - Epoch   6, Step:   146500, Batch Loss:     1.711549, Tokens per Sec:    16138, Lr: 0.000300\n",
      "2021-07-20 08:35:52,618 - INFO - joeynmt.training - Epoch   6, Step:   146600, Batch Loss:     1.764127, Tokens per Sec:    16027, Lr: 0.000300\n",
      "2021-07-20 08:36:06,464 - INFO - joeynmt.training - Epoch   6, Step:   146700, Batch Loss:     1.565663, Tokens per Sec:    16128, Lr: 0.000300\n",
      "2021-07-20 08:36:20,138 - INFO - joeynmt.training - Epoch   6, Step:   146800, Batch Loss:     1.779919, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-20 08:36:33,547 - INFO - joeynmt.training - Epoch   6, Step:   146900, Batch Loss:     1.872568, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-07-20 08:36:47,043 - INFO - joeynmt.training - Epoch   6, Step:   147000, Batch Loss:     1.867357, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-07-20 08:37:00,554 - INFO - joeynmt.training - Epoch   6, Step:   147100, Batch Loss:     1.806313, Tokens per Sec:    16109, Lr: 0.000300\n",
      "2021-07-20 08:37:14,363 - INFO - joeynmt.training - Epoch   6, Step:   147200, Batch Loss:     1.940028, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-20 08:37:27,894 - INFO - joeynmt.training - Epoch   6, Step:   147300, Batch Loss:     1.911564, Tokens per Sec:    15576, Lr: 0.000300\n",
      "2021-07-20 08:37:41,550 - INFO - joeynmt.training - Epoch   6, Step:   147400, Batch Loss:     1.929312, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-07-20 08:37:55,149 - INFO - joeynmt.training - Epoch   6, Step:   147500, Batch Loss:     1.814103, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-07-20 08:38:08,733 - INFO - joeynmt.training - Epoch   6, Step:   147600, Batch Loss:     1.778149, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-07-20 08:38:22,598 - INFO - joeynmt.training - Epoch   6, Step:   147700, Batch Loss:     1.903116, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-07-20 08:38:36,476 - INFO - joeynmt.training - Epoch   6, Step:   147800, Batch Loss:     1.727956, Tokens per Sec:    16287, Lr: 0.000300\n",
      "2021-07-20 08:38:50,304 - INFO - joeynmt.training - Epoch   6, Step:   147900, Batch Loss:     1.610064, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-07-20 08:39:03,944 - INFO - joeynmt.training - Epoch   6, Step:   148000, Batch Loss:     1.752409, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-20 08:39:17,573 - INFO - joeynmt.training - Epoch   6, Step:   148100, Batch Loss:     1.782706, Tokens per Sec:    16256, Lr: 0.000300\n",
      "2021-07-20 08:39:31,360 - INFO - joeynmt.training - Epoch   6, Step:   148200, Batch Loss:     1.607028, Tokens per Sec:    16451, Lr: 0.000300\n",
      "2021-07-20 08:39:45,047 - INFO - joeynmt.training - Epoch   6, Step:   148300, Batch Loss:     1.831018, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-07-20 08:39:58,869 - INFO - joeynmt.training - Epoch   6, Step:   148400, Batch Loss:     1.823859, Tokens per Sec:    15742, Lr: 0.000300\n",
      "2021-07-20 08:40:12,707 - INFO - joeynmt.training - Epoch   6, Step:   148500, Batch Loss:     1.857394, Tokens per Sec:    15964, Lr: 0.000300\n",
      "2021-07-20 08:40:26,330 - INFO - joeynmt.training - Epoch   6, Step:   148600, Batch Loss:     1.704976, Tokens per Sec:    16144, Lr: 0.000300\n",
      "2021-07-20 08:40:40,152 - INFO - joeynmt.training - Epoch   6, Step:   148700, Batch Loss:     1.842812, Tokens per Sec:    16457, Lr: 0.000300\n",
      "2021-07-20 08:40:53,825 - INFO - joeynmt.training - Epoch   6, Step:   148800, Batch Loss:     1.883862, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-07-20 08:41:07,474 - INFO - joeynmt.training - Epoch   6, Step:   148900, Batch Loss:     1.829815, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-07-20 08:41:21,206 - INFO - joeynmt.training - Epoch   6, Step:   149000, Batch Loss:     1.951519, Tokens per Sec:    15798, Lr: 0.000300\n",
      "2021-07-20 08:41:34,849 - INFO - joeynmt.training - Epoch   6, Step:   149100, Batch Loss:     1.870717, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-07-20 08:41:45,484 - INFO - joeynmt.training - Epoch   6: total training loss 9931.81\n",
      "2021-07-20 08:41:45,485 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-20 08:41:49,055 - INFO - joeynmt.training - Epoch   7, Step:   149200, Batch Loss:     2.174036, Tokens per Sec:    13453, Lr: 0.000300\n",
      "2021-07-20 08:42:02,525 - INFO - joeynmt.training - Epoch   7, Step:   149300, Batch Loss:     1.470938, Tokens per Sec:    15605, Lr: 0.000300\n",
      "2021-07-20 08:42:16,367 - INFO - joeynmt.training - Epoch   7, Step:   149400, Batch Loss:     1.907972, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-07-20 08:42:30,091 - INFO - joeynmt.training - Epoch   7, Step:   149500, Batch Loss:     1.719332, Tokens per Sec:    16209, Lr: 0.000300\n",
      "2021-07-20 08:42:43,741 - INFO - joeynmt.training - Epoch   7, Step:   149600, Batch Loss:     1.985594, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-07-20 08:42:57,335 - INFO - joeynmt.training - Epoch   7, Step:   149700, Batch Loss:     1.887822, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-07-20 08:43:11,126 - INFO - joeynmt.training - Epoch   7, Step:   149800, Batch Loss:     1.824226, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-07-20 08:43:25,077 - INFO - joeynmt.training - Epoch   7, Step:   149900, Batch Loss:     1.785467, Tokens per Sec:    16315, Lr: 0.000300\n",
      "2021-07-20 08:43:38,731 - INFO - joeynmt.training - Epoch   7, Step:   150000, Batch Loss:     1.639045, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-07-20 08:44:00,774 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 08:44:00,775 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 08:44:00,775 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 08:44:01,020 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-20 08:44:01,020 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-20 08:44:02,039 - INFO - joeynmt.training - Example #0\n",
      "2021-07-20 08:44:02,040 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-20 08:44:02,040 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-20 08:44:02,040 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-20 08:44:02,041 - INFO - joeynmt.training - Example #1\n",
      "2021-07-20 08:44:02,041 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-20 08:44:02,041 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-20 08:44:02,041 - INFO - joeynmt.training - \tHypothesis: The writer was written in the pillar on the side of the scroll .\n",
      "2021-07-20 08:44:02,041 - INFO - joeynmt.training - Example #2\n",
      "2021-07-20 08:44:02,042 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-20 08:44:02,042 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:44:02,042 - INFO - joeynmt.training - \tHypothesis: Rather than stress or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:44:02,043 - INFO - joeynmt.training - Example #3\n",
      "2021-07-20 08:44:02,043 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-20 08:44:02,043 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-20 08:44:02,044 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-20 08:44:02,044 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   150000: bleu:  25.82, loss: 45608.1172, ppl:   5.1652, duration: 23.3121s\n",
      "2021-07-20 08:44:15,815 - INFO - joeynmt.training - Epoch   7, Step:   150100, Batch Loss:     1.760931, Tokens per Sec:    15717, Lr: 0.000300\n",
      "2021-07-20 08:44:29,621 - INFO - joeynmt.training - Epoch   7, Step:   150200, Batch Loss:     1.902190, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-07-20 08:44:43,281 - INFO - joeynmt.training - Epoch   7, Step:   150300, Batch Loss:     1.688343, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-20 08:44:57,140 - INFO - joeynmt.training - Epoch   7, Step:   150400, Batch Loss:     1.788074, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-07-20 08:45:10,882 - INFO - joeynmt.training - Epoch   7, Step:   150500, Batch Loss:     1.723037, Tokens per Sec:    16230, Lr: 0.000300\n",
      "2021-07-20 08:45:24,469 - INFO - joeynmt.training - Epoch   7, Step:   150600, Batch Loss:     2.012490, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-07-20 08:45:38,065 - INFO - joeynmt.training - Epoch   7, Step:   150700, Batch Loss:     1.756808, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-07-20 08:45:51,734 - INFO - joeynmt.training - Epoch   7, Step:   150800, Batch Loss:     2.015380, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-07-20 08:46:05,622 - INFO - joeynmt.training - Epoch   7, Step:   150900, Batch Loss:     1.826701, Tokens per Sec:    15906, Lr: 0.000300\n",
      "2021-07-20 08:46:19,272 - INFO - joeynmt.training - Epoch   7, Step:   151000, Batch Loss:     1.798449, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-07-20 08:46:32,940 - INFO - joeynmt.training - Epoch   7, Step:   151100, Batch Loss:     1.800184, Tokens per Sec:    16414, Lr: 0.000300\n",
      "2021-07-20 08:46:46,702 - INFO - joeynmt.training - Epoch   7, Step:   151200, Batch Loss:     1.878482, Tokens per Sec:    16375, Lr: 0.000300\n",
      "2021-07-20 08:47:00,309 - INFO - joeynmt.training - Epoch   7, Step:   151300, Batch Loss:     2.012907, Tokens per Sec:    15839, Lr: 0.000300\n",
      "2021-07-20 08:47:14,000 - INFO - joeynmt.training - Epoch   7, Step:   151400, Batch Loss:     1.818725, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-20 08:47:27,718 - INFO - joeynmt.training - Epoch   7, Step:   151500, Batch Loss:     1.681478, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-07-20 08:47:41,470 - INFO - joeynmt.training - Epoch   7, Step:   151600, Batch Loss:     1.979314, Tokens per Sec:    16199, Lr: 0.000300\n",
      "2021-07-20 08:47:55,052 - INFO - joeynmt.training - Epoch   7, Step:   151700, Batch Loss:     1.848321, Tokens per Sec:    16189, Lr: 0.000300\n",
      "2021-07-20 08:48:08,747 - INFO - joeynmt.training - Epoch   7, Step:   151800, Batch Loss:     1.816968, Tokens per Sec:    15817, Lr: 0.000300\n",
      "2021-07-20 08:48:22,500 - INFO - joeynmt.training - Epoch   7, Step:   151900, Batch Loss:     1.804459, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-07-20 08:48:36,190 - INFO - joeynmt.training - Epoch   7, Step:   152000, Batch Loss:     1.845695, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-07-20 08:48:49,783 - INFO - joeynmt.training - Epoch   7, Step:   152100, Batch Loss:     1.897054, Tokens per Sec:    15741, Lr: 0.000300\n",
      "2021-07-20 08:49:03,463 - INFO - joeynmt.training - Epoch   7, Step:   152200, Batch Loss:     1.949919, Tokens per Sec:    16312, Lr: 0.000300\n",
      "2021-07-20 08:49:17,428 - INFO - joeynmt.training - Epoch   7, Step:   152300, Batch Loss:     1.834867, Tokens per Sec:    16245, Lr: 0.000300\n",
      "2021-07-20 08:49:31,380 - INFO - joeynmt.training - Epoch   7, Step:   152400, Batch Loss:     1.913196, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-07-20 08:49:44,958 - INFO - joeynmt.training - Epoch   7, Step:   152500, Batch Loss:     1.835893, Tokens per Sec:    15658, Lr: 0.000300\n",
      "2021-07-20 08:49:58,649 - INFO - joeynmt.training - Epoch   7, Step:   152600, Batch Loss:     1.803620, Tokens per Sec:    15712, Lr: 0.000300\n",
      "2021-07-20 08:50:12,371 - INFO - joeynmt.training - Epoch   7, Step:   152700, Batch Loss:     1.684236, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-07-20 08:50:26,005 - INFO - joeynmt.training - Epoch   7, Step:   152800, Batch Loss:     1.986670, Tokens per Sec:    16154, Lr: 0.000300\n",
      "2021-07-20 08:50:39,538 - INFO - joeynmt.training - Epoch   7, Step:   152900, Batch Loss:     1.849629, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-07-20 08:50:53,190 - INFO - joeynmt.training - Epoch   7, Step:   153000, Batch Loss:     1.801587, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-07-20 08:51:07,142 - INFO - joeynmt.training - Epoch   7, Step:   153100, Batch Loss:     1.824816, Tokens per Sec:    15865, Lr: 0.000300\n",
      "2021-07-20 08:51:20,707 - INFO - joeynmt.training - Epoch   7, Step:   153200, Batch Loss:     1.805277, Tokens per Sec:    16281, Lr: 0.000300\n",
      "2021-07-20 08:51:34,378 - INFO - joeynmt.training - Epoch   7, Step:   153300, Batch Loss:     1.952161, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-07-20 08:51:47,890 - INFO - joeynmt.training - Epoch   7, Step:   153400, Batch Loss:     1.770311, Tokens per Sec:    16191, Lr: 0.000300\n",
      "2021-07-20 08:52:01,641 - INFO - joeynmt.training - Epoch   7, Step:   153500, Batch Loss:     1.794270, Tokens per Sec:    15866, Lr: 0.000300\n",
      "2021-07-20 08:52:15,450 - INFO - joeynmt.training - Epoch   7, Step:   153600, Batch Loss:     1.714754, Tokens per Sec:    15810, Lr: 0.000300\n",
      "2021-07-20 08:52:28,998 - INFO - joeynmt.training - Epoch   7, Step:   153700, Batch Loss:     1.707704, Tokens per Sec:    15780, Lr: 0.000300\n",
      "2021-07-20 08:52:42,837 - INFO - joeynmt.training - Epoch   7, Step:   153800, Batch Loss:     1.831639, Tokens per Sec:    16331, Lr: 0.000300\n",
      "2021-07-20 08:52:56,310 - INFO - joeynmt.training - Epoch   7, Step:   153900, Batch Loss:     1.869712, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-07-20 08:53:10,032 - INFO - joeynmt.training - Epoch   7, Step:   154000, Batch Loss:     1.842076, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-07-20 08:53:23,811 - INFO - joeynmt.training - Epoch   7, Step:   154100, Batch Loss:     1.863553, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-07-20 08:53:37,508 - INFO - joeynmt.training - Epoch   7, Step:   154200, Batch Loss:     1.863514, Tokens per Sec:    16370, Lr: 0.000300\n",
      "2021-07-20 08:53:51,216 - INFO - joeynmt.training - Epoch   7, Step:   154300, Batch Loss:     1.806786, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-07-20 08:54:04,856 - INFO - joeynmt.training - Epoch   7, Step:   154400, Batch Loss:     1.847581, Tokens per Sec:    16119, Lr: 0.000300\n",
      "2021-07-20 08:54:18,485 - INFO - joeynmt.training - Epoch   7, Step:   154500, Batch Loss:     1.905159, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-07-20 08:54:32,278 - INFO - joeynmt.training - Epoch   7, Step:   154600, Batch Loss:     1.966510, Tokens per Sec:    16239, Lr: 0.000300\n",
      "2021-07-20 08:54:33,378 - INFO - joeynmt.training - Epoch   7: total training loss 9886.23\n",
      "2021-07-20 08:54:33,378 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-20 08:54:46,625 - INFO - joeynmt.training - Epoch   8, Step:   154700, Batch Loss:     1.577974, Tokens per Sec:    15321, Lr: 0.000300\n",
      "2021-07-20 08:55:00,497 - INFO - joeynmt.training - Epoch   8, Step:   154800, Batch Loss:     1.893097, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-07-20 08:55:14,408 - INFO - joeynmt.training - Epoch   8, Step:   154900, Batch Loss:     1.938000, Tokens per Sec:    16471, Lr: 0.000300\n",
      "2021-07-20 08:55:27,989 - INFO - joeynmt.training - Epoch   8, Step:   155000, Batch Loss:     1.871760, Tokens per Sec:    16215, Lr: 0.000300\n",
      "2021-07-20 08:55:41,646 - INFO - joeynmt.training - Epoch   8, Step:   155100, Batch Loss:     1.776574, Tokens per Sec:    16081, Lr: 0.000300\n",
      "2021-07-20 08:55:55,377 - INFO - joeynmt.training - Epoch   8, Step:   155200, Batch Loss:     1.725056, Tokens per Sec:    15762, Lr: 0.000300\n",
      "2021-07-20 08:56:09,170 - INFO - joeynmt.training - Epoch   8, Step:   155300, Batch Loss:     1.758047, Tokens per Sec:    15782, Lr: 0.000300\n",
      "2021-07-20 08:56:22,597 - INFO - joeynmt.training - Epoch   8, Step:   155400, Batch Loss:     1.864629, Tokens per Sec:    15398, Lr: 0.000300\n",
      "2021-07-20 08:56:36,385 - INFO - joeynmt.training - Epoch   8, Step:   155500, Batch Loss:     1.653264, Tokens per Sec:    16265, Lr: 0.000300\n",
      "2021-07-20 08:56:50,000 - INFO - joeynmt.training - Epoch   8, Step:   155600, Batch Loss:     1.891513, Tokens per Sec:    15761, Lr: 0.000300\n",
      "2021-07-20 08:57:03,889 - INFO - joeynmt.training - Epoch   8, Step:   155700, Batch Loss:     1.697073, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-07-20 08:57:17,543 - INFO - joeynmt.training - Epoch   8, Step:   155800, Batch Loss:     2.268872, Tokens per Sec:    15896, Lr: 0.000300\n",
      "2021-07-20 08:57:31,095 - INFO - joeynmt.training - Epoch   8, Step:   155900, Batch Loss:     1.712452, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-07-20 08:57:44,676 - INFO - joeynmt.training - Epoch   8, Step:   156000, Batch Loss:     1.564123, Tokens per Sec:    15861, Lr: 0.000300\n",
      "2021-07-20 08:58:06,073 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 08:58:06,074 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 08:58:06,074 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 08:58:06,341 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-20 08:58:06,341 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-20 08:58:07,092 - INFO - joeynmt.training - Example #0\n",
      "2021-07-20 08:58:07,093 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-20 08:58:07,094 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-20 08:58:07,094 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
      "2021-07-20 08:58:07,094 - INFO - joeynmt.training - Example #1\n",
      "2021-07-20 08:58:07,095 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-20 08:58:07,095 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-20 08:58:07,095 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
      "2021-07-20 08:58:07,095 - INFO - joeynmt.training - Example #2\n",
      "2021-07-20 08:58:07,095 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-20 08:58:07,096 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:58:07,096 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 08:58:07,096 - INFO - joeynmt.training - Example #3\n",
      "2021-07-20 08:58:07,097 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-20 08:58:07,097 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-20 08:58:07,097 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-20 08:58:07,098 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   156000: bleu:  25.82, loss: 45304.3086, ppl:   5.1090, duration: 22.4210s\n",
      "2021-07-20 08:58:21,532 - INFO - joeynmt.training - Epoch   8, Step:   156100, Batch Loss:     2.226503, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-07-20 08:58:35,230 - INFO - joeynmt.training - Epoch   8, Step:   156200, Batch Loss:     1.762459, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-07-20 08:58:48,969 - INFO - joeynmt.training - Epoch   8, Step:   156300, Batch Loss:     1.965218, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-07-20 08:59:02,571 - INFO - joeynmt.training - Epoch   8, Step:   156400, Batch Loss:     1.927757, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-07-20 08:59:16,289 - INFO - joeynmt.training - Epoch   8, Step:   156500, Batch Loss:     2.016061, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-07-20 08:59:30,120 - INFO - joeynmt.training - Epoch   8, Step:   156600, Batch Loss:     1.971300, Tokens per Sec:    16278, Lr: 0.000300\n",
      "2021-07-20 08:59:43,650 - INFO - joeynmt.training - Epoch   8, Step:   156700, Batch Loss:     1.610308, Tokens per Sec:    15921, Lr: 0.000300\n",
      "2021-07-20 08:59:57,354 - INFO - joeynmt.training - Epoch   8, Step:   156800, Batch Loss:     1.883015, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-07-20 09:00:10,902 - INFO - joeynmt.training - Epoch   8, Step:   156900, Batch Loss:     1.711830, Tokens per Sec:    15739, Lr: 0.000300\n",
      "2021-07-20 09:00:24,706 - INFO - joeynmt.training - Epoch   8, Step:   157000, Batch Loss:     2.126480, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-07-20 09:00:38,459 - INFO - joeynmt.training - Epoch   8, Step:   157100, Batch Loss:     1.744423, Tokens per Sec:    16342, Lr: 0.000300\n",
      "2021-07-20 09:00:52,324 - INFO - joeynmt.training - Epoch   8, Step:   157200, Batch Loss:     1.851923, Tokens per Sec:    15748, Lr: 0.000300\n",
      "2021-07-20 09:01:06,196 - INFO - joeynmt.training - Epoch   8, Step:   157300, Batch Loss:     1.690179, Tokens per Sec:    15889, Lr: 0.000300\n",
      "2021-07-20 09:01:19,983 - INFO - joeynmt.training - Epoch   8, Step:   157400, Batch Loss:     2.022737, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-20 09:01:33,566 - INFO - joeynmt.training - Epoch   8, Step:   157500, Batch Loss:     1.907739, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-07-20 09:01:47,228 - INFO - joeynmt.training - Epoch   8, Step:   157600, Batch Loss:     1.945301, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-07-20 09:02:01,129 - INFO - joeynmt.training - Epoch   8, Step:   157700, Batch Loss:     1.966888, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-20 09:02:14,901 - INFO - joeynmt.training - Epoch   8, Step:   157800, Batch Loss:     1.723297, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-07-20 09:02:28,380 - INFO - joeynmt.training - Epoch   8, Step:   157900, Batch Loss:     1.538435, Tokens per Sec:    16076, Lr: 0.000300\n",
      "2021-07-20 09:02:42,006 - INFO - joeynmt.training - Epoch   8, Step:   158000, Batch Loss:     1.676768, Tokens per Sec:    15861, Lr: 0.000300\n",
      "2021-07-20 09:02:55,967 - INFO - joeynmt.training - Epoch   8, Step:   158100, Batch Loss:     1.866628, Tokens per Sec:    16494, Lr: 0.000300\n",
      "2021-07-20 09:03:09,602 - INFO - joeynmt.training - Epoch   8, Step:   158200, Batch Loss:     1.607916, Tokens per Sec:    15739, Lr: 0.000300\n",
      "2021-07-20 09:03:23,314 - INFO - joeynmt.training - Epoch   8, Step:   158300, Batch Loss:     1.855433, Tokens per Sec:    16174, Lr: 0.000300\n",
      "2021-07-20 09:03:36,926 - INFO - joeynmt.training - Epoch   8, Step:   158400, Batch Loss:     1.697101, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-07-20 09:03:50,608 - INFO - joeynmt.training - Epoch   8, Step:   158500, Batch Loss:     1.843897, Tokens per Sec:    16206, Lr: 0.000300\n",
      "2021-07-20 09:04:04,395 - INFO - joeynmt.training - Epoch   8, Step:   158600, Batch Loss:     1.950083, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-07-20 09:04:18,223 - INFO - joeynmt.training - Epoch   8, Step:   158700, Batch Loss:     1.973313, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-07-20 09:04:31,935 - INFO - joeynmt.training - Epoch   8, Step:   158800, Batch Loss:     1.713607, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-07-20 09:04:45,635 - INFO - joeynmt.training - Epoch   8, Step:   158900, Batch Loss:     1.654418, Tokens per Sec:    16435, Lr: 0.000300\n",
      "2021-07-20 09:04:59,242 - INFO - joeynmt.training - Epoch   8, Step:   159000, Batch Loss:     1.917370, Tokens per Sec:    16137, Lr: 0.000300\n",
      "2021-07-20 09:05:13,029 - INFO - joeynmt.training - Epoch   8, Step:   159100, Batch Loss:     1.797633, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-07-20 09:05:26,764 - INFO - joeynmt.training - Epoch   8, Step:   159200, Batch Loss:     1.759977, Tokens per Sec:    15704, Lr: 0.000300\n",
      "2021-07-20 09:05:40,392 - INFO - joeynmt.training - Epoch   8, Step:   159300, Batch Loss:     1.628358, Tokens per Sec:    16008, Lr: 0.000300\n",
      "2021-07-20 09:05:54,109 - INFO - joeynmt.training - Epoch   8, Step:   159400, Batch Loss:     1.611943, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-07-20 09:06:07,555 - INFO - joeynmt.training - Epoch   8, Step:   159500, Batch Loss:     1.700224, Tokens per Sec:    15625, Lr: 0.000300\n",
      "2021-07-20 09:06:21,466 - INFO - joeynmt.training - Epoch   8, Step:   159600, Batch Loss:     1.889824, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-20 09:06:35,107 - INFO - joeynmt.training - Epoch   8, Step:   159700, Batch Loss:     1.675558, Tokens per Sec:    15875, Lr: 0.000300\n",
      "2021-07-20 09:06:48,956 - INFO - joeynmt.training - Epoch   8, Step:   159800, Batch Loss:     1.560648, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-07-20 09:07:02,661 - INFO - joeynmt.training - Epoch   8, Step:   159900, Batch Loss:     1.769993, Tokens per Sec:    15599, Lr: 0.000300\n",
      "2021-07-20 09:07:16,464 - INFO - joeynmt.training - Epoch   8, Step:   160000, Batch Loss:     1.498448, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-07-20 09:07:21,790 - INFO - joeynmt.training - Epoch   8: total training loss 9837.14\n",
      "2021-07-20 09:07:21,790 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-20 09:07:30,719 - INFO - joeynmt.training - Epoch   9, Step:   160100, Batch Loss:     1.815166, Tokens per Sec:    15266, Lr: 0.000300\n",
      "2021-07-20 09:07:44,318 - INFO - joeynmt.training - Epoch   9, Step:   160200, Batch Loss:     2.189849, Tokens per Sec:    16175, Lr: 0.000300\n",
      "2021-07-20 09:07:58,175 - INFO - joeynmt.training - Epoch   9, Step:   160300, Batch Loss:     1.605563, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-07-20 09:08:12,200 - INFO - joeynmt.training - Epoch   9, Step:   160400, Batch Loss:     1.706159, Tokens per Sec:    16232, Lr: 0.000300\n",
      "2021-07-20 09:08:25,828 - INFO - joeynmt.training - Epoch   9, Step:   160500, Batch Loss:     1.929311, Tokens per Sec:    16022, Lr: 0.000300\n",
      "2021-07-20 09:08:39,243 - INFO - joeynmt.training - Epoch   9, Step:   160600, Batch Loss:     1.825685, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-07-20 09:08:53,085 - INFO - joeynmt.training - Epoch   9, Step:   160700, Batch Loss:     1.873626, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-20 09:09:06,777 - INFO - joeynmt.training - Epoch   9, Step:   160800, Batch Loss:     1.741006, Tokens per Sec:    16131, Lr: 0.000300\n",
      "2021-07-20 09:09:20,530 - INFO - joeynmt.training - Epoch   9, Step:   160900, Batch Loss:     1.899017, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-07-20 09:09:34,277 - INFO - joeynmt.training - Epoch   9, Step:   161000, Batch Loss:     1.684834, Tokens per Sec:    16283, Lr: 0.000300\n",
      "2021-07-20 09:09:47,913 - INFO - joeynmt.training - Epoch   9, Step:   161100, Batch Loss:     1.944116, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-07-20 09:10:01,697 - INFO - joeynmt.training - Epoch   9, Step:   161200, Batch Loss:     1.939498, Tokens per Sec:    16186, Lr: 0.000300\n",
      "2021-07-20 09:10:15,459 - INFO - joeynmt.training - Epoch   9, Step:   161300, Batch Loss:     1.694935, Tokens per Sec:    15678, Lr: 0.000300\n",
      "2021-07-20 09:10:29,275 - INFO - joeynmt.training - Epoch   9, Step:   161400, Batch Loss:     1.502300, Tokens per Sec:    16244, Lr: 0.000300\n",
      "2021-07-20 09:10:42,850 - INFO - joeynmt.training - Epoch   9, Step:   161500, Batch Loss:     1.746225, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-07-20 09:10:56,383 - INFO - joeynmt.training - Epoch   9, Step:   161600, Batch Loss:     1.635853, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-07-20 09:11:10,070 - INFO - joeynmt.training - Epoch   9, Step:   161700, Batch Loss:     1.636328, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-07-20 09:11:24,173 - INFO - joeynmt.training - Epoch   9, Step:   161800, Batch Loss:     1.907898, Tokens per Sec:    16251, Lr: 0.000300\n",
      "2021-07-20 09:11:37,841 - INFO - joeynmt.training - Epoch   9, Step:   161900, Batch Loss:     1.883214, Tokens per Sec:    16108, Lr: 0.000300\n",
      "2021-07-20 09:11:51,727 - INFO - joeynmt.training - Epoch   9, Step:   162000, Batch Loss:     1.642533, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-07-20 09:12:13,253 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 09:12:13,254 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 09:12:13,254 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 09:12:13,515 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-20 09:12:13,516 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-20 09:12:14,222 - INFO - joeynmt.training - Example #0\n",
      "2021-07-20 09:12:14,223 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-20 09:12:14,223 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-20 09:12:14,223 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-20 09:12:14,223 - INFO - joeynmt.training - Example #1\n",
      "2021-07-20 09:12:14,224 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-20 09:12:14,224 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-20 09:12:14,224 - INFO - joeynmt.training - \tHypothesis: The text was written in the pages of the scroll .\n",
      "2021-07-20 09:12:14,224 - INFO - joeynmt.training - Example #2\n",
      "2021-07-20 09:12:14,224 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-20 09:12:14,225 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 09:12:14,225 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-20 09:12:14,225 - INFO - joeynmt.training - Example #3\n",
      "2021-07-20 09:12:14,225 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-20 09:12:14,225 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-20 09:12:14,226 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-20 09:12:14,226 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   162000: bleu:  26.17, loss: 45212.9844, ppl:   5.0922, duration: 22.4985s\n",
      "2021-07-20 09:12:28,040 - INFO - joeynmt.training - Epoch   9, Step:   162100, Batch Loss:     1.729342, Tokens per Sec:    15719, Lr: 0.000300\n",
      "2021-07-20 09:12:41,767 - INFO - joeynmt.training - Epoch   9, Step:   162200, Batch Loss:     1.619652, Tokens per Sec:    16188, Lr: 0.000300\n",
      "2021-07-20 09:12:55,684 - INFO - joeynmt.training - Epoch   9, Step:   162300, Batch Loss:     1.755692, Tokens per Sec:    16232, Lr: 0.000300\n",
      "2021-07-20 09:13:09,284 - INFO - joeynmt.training - Epoch   9, Step:   162400, Batch Loss:     1.911423, Tokens per Sec:    15564, Lr: 0.000300\n",
      "2021-07-20 09:13:23,308 - INFO - joeynmt.training - Epoch   9, Step:   162500, Batch Loss:     1.748560, Tokens per Sec:    16315, Lr: 0.000300\n",
      "2021-07-20 09:13:37,005 - INFO - joeynmt.training - Epoch   9, Step:   162600, Batch Loss:     1.726313, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-07-20 09:13:50,624 - INFO - joeynmt.training - Epoch   9, Step:   162700, Batch Loss:     1.803678, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-07-20 09:14:04,371 - INFO - joeynmt.training - Epoch   9, Step:   162800, Batch Loss:     1.828753, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-07-20 09:14:18,032 - INFO - joeynmt.training - Epoch   9, Step:   162900, Batch Loss:     1.861565, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-07-20 09:14:31,742 - INFO - joeynmt.training - Epoch   9, Step:   163000, Batch Loss:     1.831346, Tokens per Sec:    16375, Lr: 0.000300\n",
      "2021-07-20 09:14:45,240 - INFO - joeynmt.training - Epoch   9, Step:   163100, Batch Loss:     1.741321, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-07-20 09:14:59,021 - INFO - joeynmt.training - Epoch   9, Step:   163200, Batch Loss:     1.717622, Tokens per Sec:    16031, Lr: 0.000300\n",
      "2021-07-20 09:15:12,878 - INFO - joeynmt.training - Epoch   9, Step:   163300, Batch Loss:     1.984806, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-07-20 09:15:26,605 - INFO - joeynmt.training - Epoch   9, Step:   163400, Batch Loss:     1.889007, Tokens per Sec:    16167, Lr: 0.000300\n",
      "2021-07-20 09:15:40,184 - INFO - joeynmt.training - Epoch   9, Step:   163500, Batch Loss:     1.832916, Tokens per Sec:    16178, Lr: 0.000300\n",
      "2021-07-20 09:15:53,773 - INFO - joeynmt.training - Epoch   9, Step:   163600, Batch Loss:     1.746564, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-07-20 09:16:07,573 - INFO - joeynmt.training - Epoch   9, Step:   163700, Batch Loss:     1.830701, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-07-20 09:16:21,354 - INFO - joeynmt.training - Epoch   9, Step:   163800, Batch Loss:     1.644508, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-07-20 09:16:35,164 - INFO - joeynmt.training - Epoch   9, Step:   163900, Batch Loss:     1.735180, Tokens per Sec:    16149, Lr: 0.000300\n",
      "2021-07-20 09:16:48,876 - INFO - joeynmt.training - Epoch   9, Step:   164000, Batch Loss:     1.417936, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-07-20 09:17:02,499 - INFO - joeynmt.training - Epoch   9, Step:   164100, Batch Loss:     1.656565, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-07-20 09:17:16,257 - INFO - joeynmt.training - Epoch   9, Step:   164200, Batch Loss:     1.996906, Tokens per Sec:    16430, Lr: 0.000300\n",
      "2021-07-20 09:17:29,852 - INFO - joeynmt.training - Epoch   9, Step:   164300, Batch Loss:     1.749221, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-07-20 09:17:43,642 - INFO - joeynmt.training - Epoch   9, Step:   164400, Batch Loss:     1.739653, Tokens per Sec:    16202, Lr: 0.000300\n",
      "2021-07-20 09:17:57,434 - INFO - joeynmt.training - Epoch   9, Step:   164500, Batch Loss:     1.874735, Tokens per Sec:    15901, Lr: 0.000300\n",
      "2021-07-20 09:18:11,370 - INFO - joeynmt.training - Epoch   9, Step:   164600, Batch Loss:     1.698992, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-07-20 09:18:24,889 - INFO - joeynmt.training - Epoch   9, Step:   164700, Batch Loss:     1.941238, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-07-20 09:18:38,670 - INFO - joeynmt.training - Epoch   9, Step:   164800, Batch Loss:     1.857657, Tokens per Sec:    16303, Lr: 0.000300\n",
      "2021-07-20 09:18:52,334 - INFO - joeynmt.training - Epoch   9, Step:   164900, Batch Loss:     1.749903, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-20 09:19:06,169 - INFO - joeynmt.training - Epoch   9, Step:   165000, Batch Loss:     1.827659, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-07-20 09:19:19,580 - INFO - joeynmt.training - Epoch   9, Step:   165100, Batch Loss:     1.641900, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-07-20 09:19:33,135 - INFO - joeynmt.training - Epoch   9, Step:   165200, Batch Loss:     1.651172, Tokens per Sec:    15918, Lr: 0.000300\n",
      "2021-07-20 09:19:46,694 - INFO - joeynmt.training - Epoch   9, Step:   165300, Batch Loss:     1.973568, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-07-20 09:20:00,450 - INFO - joeynmt.training - Epoch   9, Step:   165400, Batch Loss:     1.865227, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-20 09:20:07,752 - INFO - joeynmt.training - Epoch   9: total training loss 9768.48\n",
      "2021-07-20 09:20:07,752 - INFO - joeynmt.training - Training ended after   9 epochs.\n",
      "2021-07-20 09:20:07,752 - INFO - joeynmt.training - Best validation result (greedy) at step   162000:   5.09 ppl.\n",
      "2021-07-20 09:20:07,776 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-20 09:20:08,142 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-20 09:20:08,350 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-20 09:20:08,414 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe.en)...\n",
      "2021-07-20 09:20:33,259 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 09:20:33,260 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 09:20:33,260 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 09:20:33,502 - INFO - joeynmt.prediction -  dev bleu[13a]:  26.34 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-20 09:20:33,507 - INFO - joeynmt.prediction - Translations saved to: models/rwen_reverse_transformer2_continued/00162000.hyps.dev\n",
      "2021-07-20 09:20:33,507 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe.en)...\n",
      "2021-07-20 09:21:10,433 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-20 09:21:10,434 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-20 09:21:10,434 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-20 09:21:11,030 - INFO - joeynmt.prediction - test bleu[13a]:  37.08 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-20 09:21:11,036 - INFO - joeynmt.prediction - Translations saved to: models/rwen_reverse_transformer2_continued/00162000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_rwen_reload.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_pgYC3FmiWN"
   },
   "source": [
    "9 epochs done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UpiMPsdyM2F"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 162000\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/latest.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer2_continued/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rwen_reverse_transformer2\"', f'model_dir: \"models/rwen_reverse_transformer2_continued2\"').replace(\n",
    "            f'epochs: 15', f'epochs: 30').replace(f'validation_freq: 5000', f'validation_freq: 6000')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "W6yMelLex79k",
    "outputId": "8a84f067-f510-449e-8bf3-cd941f04be54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rwen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued/162000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 3600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 6000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rwen_reverse_transformer2_continued2\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "    save_latest_ckpt: True\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_{name}_reload2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxeCIKiKx7yz",
    "outputId": "c683963f-bab1-43a6-c782-77c7985d5b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-25 08:32:52,891 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-25 08:32:53,245 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-25 08:33:03,944 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-25 08:33:05,112 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-25 08:33:06,512 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-25 08:33:07,706 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-25 08:33:07,706 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-25 08:33:07,921 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-25 08:33:09.398157: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-25 08:33:10,562 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-07-25 08:33:14,035 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued/162000.ckpt\n",
      "2021-07-25 08:33:21,515 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-07-25 08:33:21,516 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-07-25 08:33:21,516 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-25 08:33:21,516 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-07-25 08:33:21,516 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-07-25 08:33:21,516 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-07-25 08:33:21,516 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-25 08:33:21,517 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-25 08:33:21,517 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-25 08:33:21,517 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\n",
      "2021-07-25 08:33:21,517 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\n",
      "2021-07-25 08:33:21,517 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-25 08:33:21,517 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-25 08:33:21,517 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued/162000.ckpt\n",
      "2021-07-25 08:33:21,517 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-25 08:33:21,518 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-25 08:33:21,518 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-25 08:33:21,518 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-25 08:33:21,518 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-25 08:33:21,518 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-25 08:33:21,518 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-25 08:33:21,518 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-25 08:33:21,518 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-25 08:33:21,519 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-25 08:33:21,519 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-25 08:33:21,519 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-25 08:33:21,519 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-25 08:33:21,519 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-25 08:33:21,519 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-25 08:33:21,519 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-25 08:33:21,519 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 6000\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer2_continued2\n",
      "2021-07-25 08:33:21,520 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-25 08:33:21,521 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-25 08:33:21,521 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-25 08:33:21,521 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-25 08:33:21,521 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-25 08:33:21,521 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-25 08:33:21,521 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True\n",
      "2021-07-25 08:33:21,521 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-25 08:33:21,521 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-25 08:33:21,522 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-25 08:33:21,522 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-25 08:33:21,522 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-25 08:33:21,522 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-25 08:33:21,522 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-25 08:33:21,522 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-25 08:33:21,522 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-25 08:33:21,522 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-25 08:33:21,523 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-25 08:33:21,523 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-25 08:33:21,523 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-25 08:33:21,523 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-25 08:33:21,523 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-25 08:33:21,523 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-25 08:33:21,523 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-25 08:33:21,524 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-25 08:33:21,524 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-25 08:33:21,524 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-25 08:33:21,524 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-25 08:33:21,524 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-25 08:33:21,524 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-25 08:33:21,524 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-25 08:33:21,524 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-25 08:33:21,525 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434519,\n",
      "\tvalid 1000,\n",
      "\ttest 2651\n",
      "2021-07-25 08:33:21,525 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-07-25 08:33:21,525 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-25 08:33:21,525 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-25 08:33:21,525 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-07-25 08:33:21,525 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-07-25 08:33:21,526 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-07-25 08:33:21,535 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-25 08:33:21,535 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-25 08:33:35,747 - INFO - joeynmt.training - Epoch   1, Step:   162100, Batch Loss:     1.746170, Tokens per Sec:    15281, Lr: 0.000300\n",
      "2021-07-25 08:33:48,804 - INFO - joeynmt.training - Epoch   1, Step:   162200, Batch Loss:     1.599195, Tokens per Sec:    17019, Lr: 0.000300\n",
      "2021-07-25 08:34:02,213 - INFO - joeynmt.training - Epoch   1, Step:   162300, Batch Loss:     1.729746, Tokens per Sec:    16848, Lr: 0.000300\n",
      "2021-07-25 08:34:15,480 - INFO - joeynmt.training - Epoch   1, Step:   162400, Batch Loss:     1.916645, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-07-25 08:34:29,402 - INFO - joeynmt.training - Epoch   1, Step:   162500, Batch Loss:     1.744851, Tokens per Sec:    16434, Lr: 0.000300\n",
      "2021-07-25 08:34:43,175 - INFO - joeynmt.training - Epoch   1, Step:   162600, Batch Loss:     1.723460, Tokens per Sec:    15958, Lr: 0.000300\n",
      "2021-07-25 08:34:57,060 - INFO - joeynmt.training - Epoch   1, Step:   162700, Batch Loss:     1.787032, Tokens per Sec:    15707, Lr: 0.000300\n",
      "2021-07-25 08:35:11,272 - INFO - joeynmt.training - Epoch   1, Step:   162800, Batch Loss:     1.827076, Tokens per Sec:    15228, Lr: 0.000300\n",
      "2021-07-25 08:35:25,387 - INFO - joeynmt.training - Epoch   1, Step:   162900, Batch Loss:     1.885749, Tokens per Sec:    15328, Lr: 0.000300\n",
      "2021-07-25 08:35:39,397 - INFO - joeynmt.training - Epoch   1, Step:   163000, Batch Loss:     1.856624, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-07-25 08:35:53,103 - INFO - joeynmt.training - Epoch   1, Step:   163100, Batch Loss:     1.784834, Tokens per Sec:    15668, Lr: 0.000300\n",
      "2021-07-25 08:36:07,012 - INFO - joeynmt.training - Epoch   1, Step:   163200, Batch Loss:     1.742526, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-07-25 08:36:21,046 - INFO - joeynmt.training - Epoch   1, Step:   163300, Batch Loss:     1.988631, Tokens per Sec:    15646, Lr: 0.000300\n",
      "2021-07-25 08:36:35,075 - INFO - joeynmt.training - Epoch   1, Step:   163400, Batch Loss:     1.883920, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-07-25 08:36:48,966 - INFO - joeynmt.training - Epoch   1, Step:   163500, Batch Loss:     1.825048, Tokens per Sec:    15815, Lr: 0.000300\n",
      "2021-07-25 08:37:02,852 - INFO - joeynmt.training - Epoch   1, Step:   163600, Batch Loss:     1.735236, Tokens per Sec:    15726, Lr: 0.000300\n",
      "2021-07-25 08:37:16,933 - INFO - joeynmt.training - Epoch   1, Step:   163700, Batch Loss:     1.846452, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-07-25 08:37:30,968 - INFO - joeynmt.training - Epoch   1, Step:   163800, Batch Loss:     1.641224, Tokens per Sec:    15581, Lr: 0.000300\n",
      "2021-07-25 08:37:45,097 - INFO - joeynmt.training - Epoch   1, Step:   163900, Batch Loss:     1.726373, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-07-25 08:37:59,077 - INFO - joeynmt.training - Epoch   1, Step:   164000, Batch Loss:     1.504142, Tokens per Sec:    15394, Lr: 0.000300\n",
      "2021-07-25 08:38:13,002 - INFO - joeynmt.training - Epoch   1, Step:   164100, Batch Loss:     1.692999, Tokens per Sec:    15513, Lr: 0.000300\n",
      "2021-07-25 08:38:27,045 - INFO - joeynmt.training - Epoch   1, Step:   164200, Batch Loss:     1.994402, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-07-25 08:38:40,929 - INFO - joeynmt.training - Epoch   1, Step:   164300, Batch Loss:     1.722737, Tokens per Sec:    15768, Lr: 0.000300\n",
      "2021-07-25 08:38:55,002 - INFO - joeynmt.training - Epoch   1, Step:   164400, Batch Loss:     1.751117, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-07-25 08:39:09,089 - INFO - joeynmt.training - Epoch   1, Step:   164500, Batch Loss:     1.879154, Tokens per Sec:    15567, Lr: 0.000300\n",
      "2021-07-25 08:39:23,292 - INFO - joeynmt.training - Epoch   1, Step:   164600, Batch Loss:     1.708437, Tokens per Sec:    15924, Lr: 0.000300\n",
      "2021-07-25 08:39:37,079 - INFO - joeynmt.training - Epoch   1, Step:   164700, Batch Loss:     1.921487, Tokens per Sec:    15688, Lr: 0.000300\n",
      "2021-07-25 08:39:51,143 - INFO - joeynmt.training - Epoch   1, Step:   164800, Batch Loss:     1.838798, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-07-25 08:40:05,155 - INFO - joeynmt.training - Epoch   1, Step:   164900, Batch Loss:     1.749756, Tokens per Sec:    15518, Lr: 0.000300\n",
      "2021-07-25 08:40:19,281 - INFO - joeynmt.training - Epoch   1, Step:   165000, Batch Loss:     1.814564, Tokens per Sec:    15621, Lr: 0.000300\n",
      "2021-07-25 08:40:32,930 - INFO - joeynmt.training - Epoch   1, Step:   165100, Batch Loss:     1.639656, Tokens per Sec:    15462, Lr: 0.000300\n",
      "2021-07-25 08:40:46,754 - INFO - joeynmt.training - Epoch   1, Step:   165200, Batch Loss:     1.686593, Tokens per Sec:    15608, Lr: 0.000300\n",
      "2021-07-25 08:41:00,582 - INFO - joeynmt.training - Epoch   1, Step:   165300, Batch Loss:     1.970822, Tokens per Sec:    15637, Lr: 0.000300\n",
      "2021-07-25 08:41:14,672 - INFO - joeynmt.training - Epoch   1, Step:   165400, Batch Loss:     1.877076, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-07-25 08:41:22,140 - INFO - joeynmt.training - Epoch   1: total training loss 6240.54\n",
      "2021-07-25 08:41:22,141 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-25 08:41:29,225 - INFO - joeynmt.training - Epoch   2, Step:   165500, Batch Loss:     1.820560, Tokens per Sec:    13656, Lr: 0.000300\n",
      "2021-07-25 08:41:43,208 - INFO - joeynmt.training - Epoch   2, Step:   165600, Batch Loss:     1.633701, Tokens per Sec:    15606, Lr: 0.000300\n",
      "2021-07-25 08:41:57,265 - INFO - joeynmt.training - Epoch   2, Step:   165700, Batch Loss:     1.878241, Tokens per Sec:    16056, Lr: 0.000300\n",
      "2021-07-25 08:42:11,031 - INFO - joeynmt.training - Epoch   2, Step:   165800, Batch Loss:     2.067176, Tokens per Sec:    15936, Lr: 0.000300\n",
      "2021-07-25 08:42:25,139 - INFO - joeynmt.training - Epoch   2, Step:   165900, Batch Loss:     1.716421, Tokens per Sec:    15465, Lr: 0.000300\n",
      "2021-07-25 08:42:39,214 - INFO - joeynmt.training - Epoch   2, Step:   166000, Batch Loss:     1.702318, Tokens per Sec:    15820, Lr: 0.000300\n",
      "2021-07-25 08:42:53,141 - INFO - joeynmt.training - Epoch   2, Step:   166100, Batch Loss:     1.989845, Tokens per Sec:    15497, Lr: 0.000300\n",
      "2021-07-25 08:43:07,220 - INFO - joeynmt.training - Epoch   2, Step:   166200, Batch Loss:     1.760577, Tokens per Sec:    15555, Lr: 0.000300\n",
      "2021-07-25 08:43:21,109 - INFO - joeynmt.training - Epoch   2, Step:   166300, Batch Loss:     1.805046, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-07-25 08:43:34,742 - INFO - joeynmt.training - Epoch   2, Step:   166400, Batch Loss:     1.625910, Tokens per Sec:    15536, Lr: 0.000300\n",
      "2021-07-25 08:43:48,723 - INFO - joeynmt.training - Epoch   2, Step:   166500, Batch Loss:     1.726114, Tokens per Sec:    15815, Lr: 0.000300\n",
      "2021-07-25 08:44:02,937 - INFO - joeynmt.training - Epoch   2, Step:   166600, Batch Loss:     1.767354, Tokens per Sec:    15666, Lr: 0.000300\n",
      "2021-07-25 08:44:16,782 - INFO - joeynmt.training - Epoch   2, Step:   166700, Batch Loss:     1.685036, Tokens per Sec:    15482, Lr: 0.000300\n",
      "2021-07-25 08:44:30,689 - INFO - joeynmt.training - Epoch   2, Step:   166800, Batch Loss:     2.228983, Tokens per Sec:    15660, Lr: 0.000300\n",
      "2021-07-25 08:44:44,755 - INFO - joeynmt.training - Epoch   2, Step:   166900, Batch Loss:     1.962937, Tokens per Sec:    16115, Lr: 0.000300\n",
      "2021-07-25 08:44:58,646 - INFO - joeynmt.training - Epoch   2, Step:   167000, Batch Loss:     1.881149, Tokens per Sec:    15574, Lr: 0.000300\n",
      "2021-07-25 08:45:12,862 - INFO - joeynmt.training - Epoch   2, Step:   167100, Batch Loss:     1.803359, Tokens per Sec:    15502, Lr: 0.000300\n",
      "2021-07-25 08:45:26,809 - INFO - joeynmt.training - Epoch   2, Step:   167200, Batch Loss:     1.601705, Tokens per Sec:    15573, Lr: 0.000300\n",
      "2021-07-25 08:45:40,696 - INFO - joeynmt.training - Epoch   2, Step:   167300, Batch Loss:     1.572220, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-07-25 08:45:54,655 - INFO - joeynmt.training - Epoch   2, Step:   167400, Batch Loss:     1.835822, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-07-25 08:46:08,740 - INFO - joeynmt.training - Epoch   2, Step:   167500, Batch Loss:     1.820943, Tokens per Sec:    15519, Lr: 0.000300\n",
      "2021-07-25 08:46:22,967 - INFO - joeynmt.training - Epoch   2, Step:   167600, Batch Loss:     1.737104, Tokens per Sec:    15558, Lr: 0.000300\n",
      "2021-07-25 08:46:36,830 - INFO - joeynmt.training - Epoch   2, Step:   167700, Batch Loss:     1.864625, Tokens per Sec:    15654, Lr: 0.000300\n",
      "2021-07-25 08:46:50,757 - INFO - joeynmt.training - Epoch   2, Step:   167800, Batch Loss:     1.750397, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-07-25 08:47:04,726 - INFO - joeynmt.training - Epoch   2, Step:   167900, Batch Loss:     1.836015, Tokens per Sec:    15611, Lr: 0.000300\n",
      "2021-07-25 08:47:18,843 - INFO - joeynmt.training - Epoch   2, Step:   168000, Batch Loss:     1.831789, Tokens per Sec:    15716, Lr: 0.000300\n",
      "2021-07-25 08:47:41,557 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 08:47:41,558 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 08:47:41,558 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 08:47:41,803 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 08:47:41,805 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 08:47:42,564 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 08:47:42,567 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 08:47:42,567 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 08:47:42,567 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-25 08:47:42,567 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 08:47:42,570 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 08:47:42,570 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 08:47:42,570 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
      "2021-07-25 08:47:42,570 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 08:47:42,571 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 08:47:42,571 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 08:47:42,571 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 08:47:42,571 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 08:47:42,571 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 08:47:42,572 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 08:47:42,572 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 08:47:42,572 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   168000: bleu:  26.19, loss: 44873.9609, ppl:   5.0304, duration: 23.7286s\n",
      "2021-07-25 08:47:56,756 - INFO - joeynmt.training - Epoch   2, Step:   168100, Batch Loss:     1.907476, Tokens per Sec:    15741, Lr: 0.000300\n",
      "2021-07-25 08:48:10,762 - INFO - joeynmt.training - Epoch   2, Step:   168200, Batch Loss:     1.803613, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-07-25 08:48:24,832 - INFO - joeynmt.training - Epoch   2, Step:   168300, Batch Loss:     1.744265, Tokens per Sec:    15720, Lr: 0.000300\n",
      "2021-07-25 08:48:38,704 - INFO - joeynmt.training - Epoch   2, Step:   168400, Batch Loss:     1.944679, Tokens per Sec:    15239, Lr: 0.000300\n",
      "2021-07-25 08:48:52,799 - INFO - joeynmt.training - Epoch   2, Step:   168500, Batch Loss:     1.902881, Tokens per Sec:    15863, Lr: 0.000300\n",
      "2021-07-25 08:49:07,103 - INFO - joeynmt.training - Epoch   2, Step:   168600, Batch Loss:     1.846990, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-07-25 08:49:21,175 - INFO - joeynmt.training - Epoch   2, Step:   168700, Batch Loss:     2.049528, Tokens per Sec:    15567, Lr: 0.000300\n",
      "2021-07-25 08:49:34,915 - INFO - joeynmt.training - Epoch   2, Step:   168800, Batch Loss:     1.890166, Tokens per Sec:    15452, Lr: 0.000300\n",
      "2021-07-25 08:49:48,834 - INFO - joeynmt.training - Epoch   2, Step:   168900, Batch Loss:     1.873092, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-07-25 08:50:02,839 - INFO - joeynmt.training - Epoch   2, Step:   169000, Batch Loss:     1.730938, Tokens per Sec:    15648, Lr: 0.000300\n",
      "2021-07-25 08:50:17,030 - INFO - joeynmt.training - Epoch   2, Step:   169100, Batch Loss:     1.893829, Tokens per Sec:    15340, Lr: 0.000300\n",
      "2021-07-25 08:50:31,186 - INFO - joeynmt.training - Epoch   2, Step:   169200, Batch Loss:     1.774490, Tokens per Sec:    16109, Lr: 0.000300\n",
      "2021-07-25 08:50:45,019 - INFO - joeynmt.training - Epoch   2, Step:   169300, Batch Loss:     1.719869, Tokens per Sec:    15513, Lr: 0.000300\n",
      "2021-07-25 08:50:58,884 - INFO - joeynmt.training - Epoch   2, Step:   169400, Batch Loss:     1.964318, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-25 08:51:12,971 - INFO - joeynmt.training - Epoch   2, Step:   169500, Batch Loss:     1.657903, Tokens per Sec:    15640, Lr: 0.000300\n",
      "2021-07-25 08:51:26,968 - INFO - joeynmt.training - Epoch   2, Step:   169600, Batch Loss:     1.574889, Tokens per Sec:    15666, Lr: 0.000300\n",
      "2021-07-25 08:51:40,873 - INFO - joeynmt.training - Epoch   2, Step:   169700, Batch Loss:     1.789919, Tokens per Sec:    15875, Lr: 0.000300\n",
      "2021-07-25 08:51:54,822 - INFO - joeynmt.training - Epoch   2, Step:   169800, Batch Loss:     1.876015, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-07-25 08:52:08,602 - INFO - joeynmt.training - Epoch   2, Step:   169900, Batch Loss:     1.785447, Tokens per Sec:    15577, Lr: 0.000300\n",
      "2021-07-25 08:52:22,693 - INFO - joeynmt.training - Epoch   2, Step:   170000, Batch Loss:     2.008853, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-07-25 08:52:36,704 - INFO - joeynmt.training - Epoch   2, Step:   170100, Batch Loss:     1.756254, Tokens per Sec:    15623, Lr: 0.000300\n",
      "2021-07-25 08:52:50,773 - INFO - joeynmt.training - Epoch   2, Step:   170200, Batch Loss:     1.777581, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-07-25 08:53:04,368 - INFO - joeynmt.training - Epoch   2, Step:   170300, Batch Loss:     1.899683, Tokens per Sec:    15452, Lr: 0.000300\n",
      "2021-07-25 08:53:18,470 - INFO - joeynmt.training - Epoch   2, Step:   170400, Batch Loss:     1.745083, Tokens per Sec:    15656, Lr: 0.000300\n",
      "2021-07-25 08:53:32,641 - INFO - joeynmt.training - Epoch   2, Step:   170500, Batch Loss:     1.511864, Tokens per Sec:    15836, Lr: 0.000300\n",
      "2021-07-25 08:53:46,674 - INFO - joeynmt.training - Epoch   2, Step:   170600, Batch Loss:     1.701463, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-07-25 08:54:00,504 - INFO - joeynmt.training - Epoch   2, Step:   170700, Batch Loss:     1.762187, Tokens per Sec:    15383, Lr: 0.000300\n",
      "2021-07-25 08:54:14,589 - INFO - joeynmt.training - Epoch   2, Step:   170800, Batch Loss:     1.749135, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-07-25 08:54:25,674 - INFO - joeynmt.training - Epoch   2: total training loss 9752.51\n",
      "2021-07-25 08:54:25,675 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-25 08:54:29,056 - INFO - joeynmt.training - Epoch   3, Step:   170900, Batch Loss:     1.767675, Tokens per Sec:    12965, Lr: 0.000300\n",
      "2021-07-25 08:54:42,943 - INFO - joeynmt.training - Epoch   3, Step:   171000, Batch Loss:     1.753883, Tokens per Sec:    15903, Lr: 0.000300\n",
      "2021-07-25 08:54:56,865 - INFO - joeynmt.training - Epoch   3, Step:   171100, Batch Loss:     1.949375, Tokens per Sec:    15310, Lr: 0.000300\n",
      "2021-07-25 08:55:10,897 - INFO - joeynmt.training - Epoch   3, Step:   171200, Batch Loss:     1.989621, Tokens per Sec:    15818, Lr: 0.000300\n",
      "2021-07-25 08:55:24,900 - INFO - joeynmt.training - Epoch   3, Step:   171300, Batch Loss:     1.687096, Tokens per Sec:    15814, Lr: 0.000300\n",
      "2021-07-25 08:55:38,820 - INFO - joeynmt.training - Epoch   3, Step:   171400, Batch Loss:     1.633900, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-07-25 08:55:52,594 - INFO - joeynmt.training - Epoch   3, Step:   171500, Batch Loss:     1.664364, Tokens per Sec:    15410, Lr: 0.000300\n",
      "2021-07-25 08:56:06,454 - INFO - joeynmt.training - Epoch   3, Step:   171600, Batch Loss:     1.822512, Tokens per Sec:    15292, Lr: 0.000300\n",
      "2021-07-25 08:56:20,693 - INFO - joeynmt.training - Epoch   3, Step:   171700, Batch Loss:     1.996159, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-07-25 08:56:34,591 - INFO - joeynmt.training - Epoch   3, Step:   171800, Batch Loss:     2.233856, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-07-25 08:56:48,562 - INFO - joeynmt.training - Epoch   3, Step:   171900, Batch Loss:     1.366166, Tokens per Sec:    15815, Lr: 0.000300\n",
      "2021-07-25 08:57:02,677 - INFO - joeynmt.training - Epoch   3, Step:   172000, Batch Loss:     1.754396, Tokens per Sec:    16045, Lr: 0.000300\n",
      "2021-07-25 08:57:16,549 - INFO - joeynmt.training - Epoch   3, Step:   172100, Batch Loss:     1.777268, Tokens per Sec:    15653, Lr: 0.000300\n",
      "2021-07-25 08:57:30,342 - INFO - joeynmt.training - Epoch   3, Step:   172200, Batch Loss:     1.781362, Tokens per Sec:    15300, Lr: 0.000300\n",
      "2021-07-25 08:57:44,254 - INFO - joeynmt.training - Epoch   3, Step:   172300, Batch Loss:     1.882868, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-07-25 08:57:58,121 - INFO - joeynmt.training - Epoch   3, Step:   172400, Batch Loss:     1.584108, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-07-25 08:58:12,192 - INFO - joeynmt.training - Epoch   3, Step:   172500, Batch Loss:     1.816321, Tokens per Sec:    15904, Lr: 0.000300\n",
      "2021-07-25 08:58:26,398 - INFO - joeynmt.training - Epoch   3, Step:   172600, Batch Loss:     1.988852, Tokens per Sec:    15644, Lr: 0.000300\n",
      "2021-07-25 08:58:40,300 - INFO - joeynmt.training - Epoch   3, Step:   172700, Batch Loss:     1.877045, Tokens per Sec:    15437, Lr: 0.000300\n",
      "2021-07-25 08:58:54,154 - INFO - joeynmt.training - Epoch   3, Step:   172800, Batch Loss:     1.953615, Tokens per Sec:    15605, Lr: 0.000300\n",
      "2021-07-25 08:59:08,138 - INFO - joeynmt.training - Epoch   3, Step:   172900, Batch Loss:     1.803048, Tokens per Sec:    16282, Lr: 0.000300\n",
      "2021-07-25 08:59:22,202 - INFO - joeynmt.training - Epoch   3, Step:   173000, Batch Loss:     1.701929, Tokens per Sec:    15540, Lr: 0.000300\n",
      "2021-07-25 08:59:36,291 - INFO - joeynmt.training - Epoch   3, Step:   173100, Batch Loss:     1.801825, Tokens per Sec:    15272, Lr: 0.000300\n",
      "2021-07-25 08:59:50,096 - INFO - joeynmt.training - Epoch   3, Step:   173200, Batch Loss:     1.639701, Tokens per Sec:    15590, Lr: 0.000300\n",
      "2021-07-25 09:00:04,351 - INFO - joeynmt.training - Epoch   3, Step:   173300, Batch Loss:     1.936025, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-07-25 09:00:18,575 - INFO - joeynmt.training - Epoch   3, Step:   173400, Batch Loss:     1.647515, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-07-25 09:00:32,353 - INFO - joeynmt.training - Epoch   3, Step:   173500, Batch Loss:     1.685265, Tokens per Sec:    15901, Lr: 0.000300\n",
      "2021-07-25 09:00:46,346 - INFO - joeynmt.training - Epoch   3, Step:   173600, Batch Loss:     1.771758, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-07-25 09:01:00,069 - INFO - joeynmt.training - Epoch   3, Step:   173700, Batch Loss:     1.842705, Tokens per Sec:    15587, Lr: 0.000300\n",
      "2021-07-25 09:01:14,127 - INFO - joeynmt.training - Epoch   3, Step:   173800, Batch Loss:     1.967541, Tokens per Sec:    15388, Lr: 0.000300\n",
      "2021-07-25 09:01:28,028 - INFO - joeynmt.training - Epoch   3, Step:   173900, Batch Loss:     1.933344, Tokens per Sec:    15590, Lr: 0.000300\n",
      "2021-07-25 09:01:41,870 - INFO - joeynmt.training - Epoch   3, Step:   174000, Batch Loss:     1.792050, Tokens per Sec:    15889, Lr: 0.000300\n",
      "2021-07-25 09:02:04,132 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 09:02:04,132 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 09:02:04,132 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 09:02:04,394 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 09:02:04,395 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 09:02:05,150 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 09:02:05,151 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 09:02:05,152 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 09:02:05,152 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-25 09:02:05,152 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 09:02:05,152 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 09:02:05,152 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 09:02:05,152 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
      "2021-07-25 09:02:05,153 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 09:02:05,153 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 09:02:05,153 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:02:05,153 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:02:05,154 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 09:02:05,154 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 09:02:05,154 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 09:02:05,154 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 09:02:05,154 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   174000: bleu:  26.40, loss: 44508.9258, ppl:   4.9648, duration: 23.2845s\n",
      "2021-07-25 09:02:19,384 - INFO - joeynmt.training - Epoch   3, Step:   174100, Batch Loss:     1.546023, Tokens per Sec:    15405, Lr: 0.000300\n",
      "2021-07-25 09:02:33,268 - INFO - joeynmt.training - Epoch   3, Step:   174200, Batch Loss:     1.829240, Tokens per Sec:    15525, Lr: 0.000300\n",
      "2021-07-25 09:02:47,431 - INFO - joeynmt.training - Epoch   3, Step:   174300, Batch Loss:     1.856910, Tokens per Sec:    16019, Lr: 0.000300\n",
      "2021-07-25 09:03:01,414 - INFO - joeynmt.training - Epoch   3, Step:   174400, Batch Loss:     1.814837, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-25 09:03:15,606 - INFO - joeynmt.training - Epoch   3, Step:   174500, Batch Loss:     1.920215, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-07-25 09:03:29,762 - INFO - joeynmt.training - Epoch   3, Step:   174600, Batch Loss:     1.938567, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-07-25 09:03:43,560 - INFO - joeynmt.training - Epoch   3, Step:   174700, Batch Loss:     2.183174, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-25 09:03:57,353 - INFO - joeynmt.training - Epoch   3, Step:   174800, Batch Loss:     1.696167, Tokens per Sec:    15749, Lr: 0.000300\n",
      "2021-07-25 09:04:11,275 - INFO - joeynmt.training - Epoch   3, Step:   174900, Batch Loss:     1.812176, Tokens per Sec:    15733, Lr: 0.000300\n",
      "2021-07-25 09:04:25,078 - INFO - joeynmt.training - Epoch   3, Step:   175000, Batch Loss:     1.747164, Tokens per Sec:    15131, Lr: 0.000300\n",
      "2021-07-25 09:04:39,143 - INFO - joeynmt.training - Epoch   3, Step:   175100, Batch Loss:     1.817884, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-07-25 09:04:53,306 - INFO - joeynmt.training - Epoch   3, Step:   175200, Batch Loss:     1.853231, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-07-25 09:05:07,530 - INFO - joeynmt.training - Epoch   3, Step:   175300, Batch Loss:     1.723784, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-07-25 09:05:21,311 - INFO - joeynmt.training - Epoch   3, Step:   175400, Batch Loss:     1.928765, Tokens per Sec:    15641, Lr: 0.000300\n",
      "2021-07-25 09:05:35,349 - INFO - joeynmt.training - Epoch   3, Step:   175500, Batch Loss:     1.938718, Tokens per Sec:    15845, Lr: 0.000300\n",
      "2021-07-25 09:05:49,217 - INFO - joeynmt.training - Epoch   3, Step:   175600, Batch Loss:     1.928266, Tokens per Sec:    15723, Lr: 0.000300\n",
      "2021-07-25 09:06:03,369 - INFO - joeynmt.training - Epoch   3, Step:   175700, Batch Loss:     1.873971, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-07-25 09:06:17,408 - INFO - joeynmt.training - Epoch   3, Step:   175800, Batch Loss:     1.695261, Tokens per Sec:    15753, Lr: 0.000300\n",
      "2021-07-25 09:06:31,248 - INFO - joeynmt.training - Epoch   3, Step:   175900, Batch Loss:     1.549818, Tokens per Sec:    15677, Lr: 0.000300\n",
      "2021-07-25 09:06:45,369 - INFO - joeynmt.training - Epoch   3, Step:   176000, Batch Loss:     1.739860, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-07-25 09:06:59,443 - INFO - joeynmt.training - Epoch   3, Step:   176100, Batch Loss:     1.686457, Tokens per Sec:    15730, Lr: 0.000300\n",
      "2021-07-25 09:07:13,563 - INFO - joeynmt.training - Epoch   3, Step:   176200, Batch Loss:     1.669330, Tokens per Sec:    15460, Lr: 0.000300\n",
      "2021-07-25 09:07:27,514 - INFO - joeynmt.training - Epoch   3, Step:   176300, Batch Loss:     1.806899, Tokens per Sec:    15038, Lr: 0.000300\n",
      "2021-07-25 09:07:28,631 - INFO - joeynmt.training - Epoch   3: total training loss 9717.16\n",
      "2021-07-25 09:07:28,632 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-25 09:07:42,131 - INFO - joeynmt.training - Epoch   4, Step:   176400, Batch Loss:     1.504829, Tokens per Sec:    14879, Lr: 0.000300\n",
      "2021-07-25 09:07:55,897 - INFO - joeynmt.training - Epoch   4, Step:   176500, Batch Loss:     1.977208, Tokens per Sec:    15646, Lr: 0.000300\n",
      "2021-07-25 09:08:09,868 - INFO - joeynmt.training - Epoch   4, Step:   176600, Batch Loss:     1.453938, Tokens per Sec:    15579, Lr: 0.000300\n",
      "2021-07-25 09:08:24,003 - INFO - joeynmt.training - Epoch   4, Step:   176700, Batch Loss:     1.808979, Tokens per Sec:    15640, Lr: 0.000300\n",
      "2021-07-25 09:08:38,072 - INFO - joeynmt.training - Epoch   4, Step:   176800, Batch Loss:     1.917099, Tokens per Sec:    15961, Lr: 0.000300\n",
      "2021-07-25 09:08:52,011 - INFO - joeynmt.training - Epoch   4, Step:   176900, Batch Loss:     1.834860, Tokens per Sec:    15714, Lr: 0.000300\n",
      "2021-07-25 09:09:05,994 - INFO - joeynmt.training - Epoch   4, Step:   177000, Batch Loss:     1.620870, Tokens per Sec:    15435, Lr: 0.000300\n",
      "2021-07-25 09:09:20,173 - INFO - joeynmt.training - Epoch   4, Step:   177100, Batch Loss:     1.913226, Tokens per Sec:    15676, Lr: 0.000300\n",
      "2021-07-25 09:09:34,322 - INFO - joeynmt.training - Epoch   4, Step:   177200, Batch Loss:     1.813458, Tokens per Sec:    15772, Lr: 0.000300\n",
      "2021-07-25 09:09:48,169 - INFO - joeynmt.training - Epoch   4, Step:   177300, Batch Loss:     1.798150, Tokens per Sec:    15400, Lr: 0.000300\n",
      "2021-07-25 09:10:02,143 - INFO - joeynmt.training - Epoch   4, Step:   177400, Batch Loss:     1.680640, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-07-25 09:10:15,991 - INFO - joeynmt.training - Epoch   4, Step:   177500, Batch Loss:     1.732112, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-07-25 09:10:30,146 - INFO - joeynmt.training - Epoch   4, Step:   177600, Batch Loss:     1.648378, Tokens per Sec:    15601, Lr: 0.000300\n",
      "2021-07-25 09:10:43,940 - INFO - joeynmt.training - Epoch   4, Step:   177700, Batch Loss:     1.809329, Tokens per Sec:    15272, Lr: 0.000300\n",
      "2021-07-25 09:10:57,921 - INFO - joeynmt.training - Epoch   4, Step:   177800, Batch Loss:     1.649472, Tokens per Sec:    15601, Lr: 0.000300\n",
      "2021-07-25 09:11:12,031 - INFO - joeynmt.training - Epoch   4, Step:   177900, Batch Loss:     1.926232, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-07-25 09:11:26,021 - INFO - joeynmt.training - Epoch   4, Step:   178000, Batch Loss:     1.827214, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-07-25 09:11:39,803 - INFO - joeynmt.training - Epoch   4, Step:   178100, Batch Loss:     1.965620, Tokens per Sec:    15497, Lr: 0.000300\n",
      "2021-07-25 09:11:53,622 - INFO - joeynmt.training - Epoch   4, Step:   178200, Batch Loss:     1.912974, Tokens per Sec:    15586, Lr: 0.000300\n",
      "2021-07-25 09:12:07,652 - INFO - joeynmt.training - Epoch   4, Step:   178300, Batch Loss:     1.666018, Tokens per Sec:    15493, Lr: 0.000300\n",
      "2021-07-25 09:12:21,787 - INFO - joeynmt.training - Epoch   4, Step:   178400, Batch Loss:     1.632346, Tokens per Sec:    15623, Lr: 0.000300\n",
      "2021-07-25 09:12:35,709 - INFO - joeynmt.training - Epoch   4, Step:   178500, Batch Loss:     1.718992, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-07-25 09:12:49,613 - INFO - joeynmt.training - Epoch   4, Step:   178600, Batch Loss:     1.875731, Tokens per Sec:    15753, Lr: 0.000300\n",
      "2021-07-25 09:13:03,721 - INFO - joeynmt.training - Epoch   4, Step:   178700, Batch Loss:     1.985606, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-07-25 09:13:17,655 - INFO - joeynmt.training - Epoch   4, Step:   178800, Batch Loss:     1.875145, Tokens per Sec:    15532, Lr: 0.000300\n",
      "2021-07-25 09:13:31,641 - INFO - joeynmt.training - Epoch   4, Step:   178900, Batch Loss:     1.570055, Tokens per Sec:    15768, Lr: 0.000300\n",
      "2021-07-25 09:13:45,526 - INFO - joeynmt.training - Epoch   4, Step:   179000, Batch Loss:     1.745120, Tokens per Sec:    15465, Lr: 0.000300\n",
      "2021-07-25 09:13:59,392 - INFO - joeynmt.training - Epoch   4, Step:   179100, Batch Loss:     1.914650, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-07-25 09:14:13,418 - INFO - joeynmt.training - Epoch   4, Step:   179200, Batch Loss:     1.635042, Tokens per Sec:    15966, Lr: 0.000300\n",
      "2021-07-25 09:14:27,617 - INFO - joeynmt.training - Epoch   4, Step:   179300, Batch Loss:     1.592057, Tokens per Sec:    15684, Lr: 0.000300\n",
      "2021-07-25 09:14:41,555 - INFO - joeynmt.training - Epoch   4, Step:   179400, Batch Loss:     1.692364, Tokens per Sec:    15640, Lr: 0.000300\n",
      "2021-07-25 09:14:55,744 - INFO - joeynmt.training - Epoch   4, Step:   179500, Batch Loss:     1.852544, Tokens per Sec:    16325, Lr: 0.000300\n",
      "2021-07-25 09:15:09,812 - INFO - joeynmt.training - Epoch   4, Step:   179600, Batch Loss:     1.799815, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-07-25 09:15:23,883 - INFO - joeynmt.training - Epoch   4, Step:   179700, Batch Loss:     1.637902, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-07-25 09:15:37,964 - INFO - joeynmt.training - Epoch   4, Step:   179800, Batch Loss:     1.872221, Tokens per Sec:    15329, Lr: 0.000300\n",
      "2021-07-25 09:15:51,854 - INFO - joeynmt.training - Epoch   4, Step:   179900, Batch Loss:     1.755294, Tokens per Sec:    15512, Lr: 0.000300\n",
      "2021-07-25 09:16:05,834 - INFO - joeynmt.training - Epoch   4, Step:   180000, Batch Loss:     1.603520, Tokens per Sec:    15327, Lr: 0.000300\n",
      "2021-07-25 09:16:28,296 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 09:16:28,297 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 09:16:28,297 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 09:16:28,532 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 09:16:28,532 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 09:16:29,310 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 09:16:29,311 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 09:16:29,311 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 09:16:29,311 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-25 09:16:29,311 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 09:16:29,312 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 09:16:29,312 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 09:16:29,312 - INFO - joeynmt.training - \tHypothesis: The letter written in the pillar on the side of the scroll .\n",
      "2021-07-25 09:16:29,312 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 09:16:29,313 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 09:16:29,313 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:16:29,313 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:16:29,313 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 09:16:29,313 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 09:16:29,314 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 09:16:29,314 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 09:16:29,314 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   180000: bleu:  26.64, loss: 44444.0820, ppl:   4.9532, duration: 23.4798s\n",
      "2021-07-25 09:16:43,423 - INFO - joeynmt.training - Epoch   4, Step:   180100, Batch Loss:     1.745355, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-07-25 09:16:57,393 - INFO - joeynmt.training - Epoch   4, Step:   180200, Batch Loss:     1.758160, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-07-25 09:17:11,362 - INFO - joeynmt.training - Epoch   4, Step:   180300, Batch Loss:     1.615987, Tokens per Sec:    15560, Lr: 0.000300\n",
      "2021-07-25 09:17:25,289 - INFO - joeynmt.training - Epoch   4, Step:   180400, Batch Loss:     1.845690, Tokens per Sec:    15412, Lr: 0.000300\n",
      "2021-07-25 09:17:38,918 - INFO - joeynmt.training - Epoch   4, Step:   180500, Batch Loss:     1.942972, Tokens per Sec:    15612, Lr: 0.000300\n",
      "2021-07-25 09:17:52,971 - INFO - joeynmt.training - Epoch   4, Step:   180600, Batch Loss:     1.847599, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-07-25 09:18:07,149 - INFO - joeynmt.training - Epoch   4, Step:   180700, Batch Loss:     1.607116, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-07-25 09:18:21,239 - INFO - joeynmt.training - Epoch   4, Step:   180800, Batch Loss:     1.995635, Tokens per Sec:    15525, Lr: 0.000300\n",
      "2021-07-25 09:18:35,327 - INFO - joeynmt.training - Epoch   4, Step:   180900, Batch Loss:     1.457186, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-25 09:18:49,284 - INFO - joeynmt.training - Epoch   4, Step:   181000, Batch Loss:     1.862424, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-07-25 09:19:03,132 - INFO - joeynmt.training - Epoch   4, Step:   181100, Batch Loss:     1.792105, Tokens per Sec:    15744, Lr: 0.000300\n",
      "2021-07-25 09:19:17,153 - INFO - joeynmt.training - Epoch   4, Step:   181200, Batch Loss:     1.805229, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-07-25 09:19:31,193 - INFO - joeynmt.training - Epoch   4, Step:   181300, Batch Loss:     1.858027, Tokens per Sec:    15545, Lr: 0.000300\n",
      "2021-07-25 09:19:45,178 - INFO - joeynmt.training - Epoch   4, Step:   181400, Batch Loss:     1.925745, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-07-25 09:19:59,082 - INFO - joeynmt.training - Epoch   4, Step:   181500, Batch Loss:     1.616018, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-07-25 09:20:13,090 - INFO - joeynmt.training - Epoch   4, Step:   181600, Batch Loss:     2.177916, Tokens per Sec:    15683, Lr: 0.000300\n",
      "2021-07-25 09:20:27,214 - INFO - joeynmt.training - Epoch   4, Step:   181700, Batch Loss:     1.746121, Tokens per Sec:    15591, Lr: 0.000300\n",
      "2021-07-25 09:20:32,241 - INFO - joeynmt.training - Epoch   4: total training loss 9687.78\n",
      "2021-07-25 09:20:32,242 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-25 09:20:41,602 - INFO - joeynmt.training - Epoch   5, Step:   181800, Batch Loss:     1.805366, Tokens per Sec:    13977, Lr: 0.000300\n",
      "2021-07-25 09:20:55,517 - INFO - joeynmt.training - Epoch   5, Step:   181900, Batch Loss:     1.637879, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-07-25 09:21:09,481 - INFO - joeynmt.training - Epoch   5, Step:   182000, Batch Loss:     1.747391, Tokens per Sec:    15994, Lr: 0.000300\n",
      "2021-07-25 09:21:23,540 - INFO - joeynmt.training - Epoch   5, Step:   182100, Batch Loss:     1.567592, Tokens per Sec:    15602, Lr: 0.000300\n",
      "2021-07-25 09:21:37,807 - INFO - joeynmt.training - Epoch   5, Step:   182200, Batch Loss:     1.603593, Tokens per Sec:    15756, Lr: 0.000300\n",
      "2021-07-25 09:21:51,841 - INFO - joeynmt.training - Epoch   5, Step:   182300, Batch Loss:     1.829517, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-07-25 09:22:05,878 - INFO - joeynmt.training - Epoch   5, Step:   182400, Batch Loss:     1.979270, Tokens per Sec:    15640, Lr: 0.000300\n",
      "2021-07-25 09:22:19,956 - INFO - joeynmt.training - Epoch   5, Step:   182500, Batch Loss:     1.612741, Tokens per Sec:    15784, Lr: 0.000300\n",
      "2021-07-25 09:22:33,874 - INFO - joeynmt.training - Epoch   5, Step:   182600, Batch Loss:     1.685181, Tokens per Sec:    15554, Lr: 0.000300\n",
      "2021-07-25 09:22:47,795 - INFO - joeynmt.training - Epoch   5, Step:   182700, Batch Loss:     1.765025, Tokens per Sec:    15710, Lr: 0.000300\n",
      "2021-07-25 09:23:01,937 - INFO - joeynmt.training - Epoch   5, Step:   182800, Batch Loss:     1.703477, Tokens per Sec:    15675, Lr: 0.000300\n",
      "2021-07-25 09:23:15,971 - INFO - joeynmt.training - Epoch   5, Step:   182900, Batch Loss:     1.901485, Tokens per Sec:    15579, Lr: 0.000300\n",
      "2021-07-25 09:23:30,043 - INFO - joeynmt.training - Epoch   5, Step:   183000, Batch Loss:     1.595962, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-07-25 09:23:43,936 - INFO - joeynmt.training - Epoch   5, Step:   183100, Batch Loss:     1.741238, Tokens per Sec:    15675, Lr: 0.000300\n",
      "2021-07-25 09:23:57,715 - INFO - joeynmt.training - Epoch   5, Step:   183200, Batch Loss:     1.839251, Tokens per Sec:    15643, Lr: 0.000300\n",
      "2021-07-25 09:24:11,746 - INFO - joeynmt.training - Epoch   5, Step:   183300, Batch Loss:     1.823380, Tokens per Sec:    15184, Lr: 0.000300\n",
      "2021-07-25 09:24:25,707 - INFO - joeynmt.training - Epoch   5, Step:   183400, Batch Loss:     1.909504, Tokens per Sec:    15443, Lr: 0.000300\n",
      "2021-07-25 09:24:39,587 - INFO - joeynmt.training - Epoch   5, Step:   183500, Batch Loss:     1.791207, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-07-25 09:24:53,493 - INFO - joeynmt.training - Epoch   5, Step:   183600, Batch Loss:     1.748530, Tokens per Sec:    15653, Lr: 0.000300\n",
      "2021-07-25 09:25:07,384 - INFO - joeynmt.training - Epoch   5, Step:   183700, Batch Loss:     1.919786, Tokens per Sec:    15496, Lr: 0.000300\n",
      "2021-07-25 09:25:21,577 - INFO - joeynmt.training - Epoch   5, Step:   183800, Batch Loss:     1.677960, Tokens per Sec:    15978, Lr: 0.000300\n",
      "2021-07-25 09:25:35,515 - INFO - joeynmt.training - Epoch   5, Step:   183900, Batch Loss:     1.712289, Tokens per Sec:    15636, Lr: 0.000300\n",
      "2021-07-25 09:25:49,430 - INFO - joeynmt.training - Epoch   5, Step:   184000, Batch Loss:     1.912669, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-07-25 09:26:03,407 - INFO - joeynmt.training - Epoch   5, Step:   184100, Batch Loss:     1.815715, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-25 09:26:17,423 - INFO - joeynmt.training - Epoch   5, Step:   184200, Batch Loss:     1.980285, Tokens per Sec:    15715, Lr: 0.000300\n",
      "2021-07-25 09:26:31,503 - INFO - joeynmt.training - Epoch   5, Step:   184300, Batch Loss:     1.832760, Tokens per Sec:    15544, Lr: 0.000300\n",
      "2021-07-25 09:26:45,461 - INFO - joeynmt.training - Epoch   5, Step:   184400, Batch Loss:     1.713629, Tokens per Sec:    15753, Lr: 0.000300\n",
      "2021-07-25 09:26:59,480 - INFO - joeynmt.training - Epoch   5, Step:   184500, Batch Loss:     1.883369, Tokens per Sec:    15498, Lr: 0.000300\n",
      "2021-07-25 09:27:13,724 - INFO - joeynmt.training - Epoch   5, Step:   184600, Batch Loss:     1.782309, Tokens per Sec:    15943, Lr: 0.000300\n",
      "2021-07-25 09:27:27,698 - INFO - joeynmt.training - Epoch   5, Step:   184700, Batch Loss:     1.489810, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-07-25 09:27:41,585 - INFO - joeynmt.training - Epoch   5, Step:   184800, Batch Loss:     1.897444, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-07-25 09:27:55,509 - INFO - joeynmt.training - Epoch   5, Step:   184900, Batch Loss:     1.704106, Tokens per Sec:    15814, Lr: 0.000300\n",
      "2021-07-25 09:28:09,814 - INFO - joeynmt.training - Epoch   5, Step:   185000, Batch Loss:     1.747708, Tokens per Sec:    16064, Lr: 0.000300\n",
      "2021-07-25 09:28:23,759 - INFO - joeynmt.training - Epoch   5, Step:   185100, Batch Loss:     1.499543, Tokens per Sec:    15534, Lr: 0.000300\n",
      "2021-07-25 09:28:37,716 - INFO - joeynmt.training - Epoch   5, Step:   185200, Batch Loss:     1.716885, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-07-25 09:28:51,535 - INFO - joeynmt.training - Epoch   5, Step:   185300, Batch Loss:     1.694649, Tokens per Sec:    15687, Lr: 0.000300\n",
      "2021-07-25 09:29:05,603 - INFO - joeynmt.training - Epoch   5, Step:   185400, Batch Loss:     2.114470, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-07-25 09:29:19,752 - INFO - joeynmt.training - Epoch   5, Step:   185500, Batch Loss:     1.720738, Tokens per Sec:    15433, Lr: 0.000300\n",
      "2021-07-25 09:29:33,707 - INFO - joeynmt.training - Epoch   5, Step:   185600, Batch Loss:     1.683473, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-07-25 09:29:47,687 - INFO - joeynmt.training - Epoch   5, Step:   185700, Batch Loss:     1.509866, Tokens per Sec:    15667, Lr: 0.000300\n",
      "2021-07-25 09:30:01,555 - INFO - joeynmt.training - Epoch   5, Step:   185800, Batch Loss:     1.711444, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-07-25 09:30:15,656 - INFO - joeynmt.training - Epoch   5, Step:   185900, Batch Loss:     1.973233, Tokens per Sec:    15682, Lr: 0.000300\n",
      "2021-07-25 09:30:29,895 - INFO - joeynmt.training - Epoch   5, Step:   186000, Batch Loss:     1.822388, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-07-25 09:30:52,844 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 09:30:52,845 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 09:30:52,845 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 09:30:53,083 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 09:30:53,083 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 09:30:53,876 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 09:30:53,877 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 09:30:53,877 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 09:30:53,877 - INFO - joeynmt.training - \tHypothesis: It impressed me .\n",
      "2021-07-25 09:30:53,877 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 09:30:53,878 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 09:30:53,878 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 09:30:53,878 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
      "2021-07-25 09:30:53,878 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 09:30:53,878 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 09:30:53,879 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:30:53,879 - INFO - joeynmt.training - \tHypothesis: Rather than worry or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:30:53,879 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 09:30:53,879 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 09:30:53,879 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 09:30:53,880 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 09:30:53,880 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   186000: bleu:  26.50, loss: 44303.2344, ppl:   4.9281, duration: 23.9843s\n",
      "2021-07-25 09:31:07,967 - INFO - joeynmt.training - Epoch   5, Step:   186100, Batch Loss:     1.900903, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-07-25 09:31:21,972 - INFO - joeynmt.training - Epoch   5, Step:   186200, Batch Loss:     2.052591, Tokens per Sec:    15563, Lr: 0.000300\n",
      "2021-07-25 09:31:35,949 - INFO - joeynmt.training - Epoch   5, Step:   186300, Batch Loss:     1.668983, Tokens per Sec:    15484, Lr: 0.000300\n",
      "2021-07-25 09:31:49,812 - INFO - joeynmt.training - Epoch   5, Step:   186400, Batch Loss:     1.844043, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-07-25 09:32:03,744 - INFO - joeynmt.training - Epoch   5, Step:   186500, Batch Loss:     1.794894, Tokens per Sec:    15556, Lr: 0.000300\n",
      "2021-07-25 09:32:17,651 - INFO - joeynmt.training - Epoch   5, Step:   186600, Batch Loss:     1.759239, Tokens per Sec:    15607, Lr: 0.000300\n",
      "2021-07-25 09:32:31,799 - INFO - joeynmt.training - Epoch   5, Step:   186700, Batch Loss:     1.772406, Tokens per Sec:    15607, Lr: 0.000300\n",
      "2021-07-25 09:32:45,569 - INFO - joeynmt.training - Epoch   5, Step:   186800, Batch Loss:     1.630437, Tokens per Sec:    15174, Lr: 0.000300\n",
      "2021-07-25 09:32:59,645 - INFO - joeynmt.training - Epoch   5, Step:   186900, Batch Loss:     1.862606, Tokens per Sec:    15630, Lr: 0.000300\n",
      "2021-07-25 09:33:13,727 - INFO - joeynmt.training - Epoch   5, Step:   187000, Batch Loss:     1.741958, Tokens per Sec:    15339, Lr: 0.000300\n",
      "2021-07-25 09:33:27,894 - INFO - joeynmt.training - Epoch   5, Step:   187100, Batch Loss:     1.813145, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 09:33:36,461 - INFO - joeynmt.training - Epoch   5: total training loss 9645.16\n",
      "2021-07-25 09:33:36,461 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-25 09:33:42,466 - INFO - joeynmt.training - Epoch   6, Step:   187200, Batch Loss:     1.638693, Tokens per Sec:    14210, Lr: 0.000300\n",
      "2021-07-25 09:33:56,504 - INFO - joeynmt.training - Epoch   6, Step:   187300, Batch Loss:     1.840837, Tokens per Sec:    15729, Lr: 0.000300\n",
      "2021-07-25 09:34:10,572 - INFO - joeynmt.training - Epoch   6, Step:   187400, Batch Loss:     1.594897, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-07-25 09:34:24,546 - INFO - joeynmt.training - Epoch   6, Step:   187500, Batch Loss:     1.779080, Tokens per Sec:    15659, Lr: 0.000300\n",
      "2021-07-25 09:34:38,570 - INFO - joeynmt.training - Epoch   6, Step:   187600, Batch Loss:     1.802456, Tokens per Sec:    15511, Lr: 0.000300\n",
      "2021-07-25 09:34:52,546 - INFO - joeynmt.training - Epoch   6, Step:   187700, Batch Loss:     1.883785, Tokens per Sec:    15809, Lr: 0.000300\n",
      "2021-07-25 09:35:06,587 - INFO - joeynmt.training - Epoch   6, Step:   187800, Batch Loss:     1.591904, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-07-25 09:35:20,669 - INFO - joeynmt.training - Epoch   6, Step:   187900, Batch Loss:     1.814978, Tokens per Sec:    15494, Lr: 0.000300\n",
      "2021-07-25 09:35:34,546 - INFO - joeynmt.training - Epoch   6, Step:   188000, Batch Loss:     1.850666, Tokens per Sec:    15569, Lr: 0.000300\n",
      "2021-07-25 09:35:48,475 - INFO - joeynmt.training - Epoch   6, Step:   188100, Batch Loss:     1.911639, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-07-25 09:36:02,267 - INFO - joeynmt.training - Epoch   6, Step:   188200, Batch Loss:     1.686274, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-07-25 09:36:16,282 - INFO - joeynmt.training - Epoch   6, Step:   188300, Batch Loss:     1.835796, Tokens per Sec:    15524, Lr: 0.000300\n",
      "2021-07-25 09:36:30,588 - INFO - joeynmt.training - Epoch   6, Step:   188400, Batch Loss:     1.858344, Tokens per Sec:    15787, Lr: 0.000300\n",
      "2021-07-25 09:36:44,499 - INFO - joeynmt.training - Epoch   6, Step:   188500, Batch Loss:     1.807813, Tokens per Sec:    15769, Lr: 0.000300\n",
      "2021-07-25 09:36:58,656 - INFO - joeynmt.training - Epoch   6, Step:   188600, Batch Loss:     1.722006, Tokens per Sec:    16192, Lr: 0.000300\n",
      "2021-07-25 09:37:12,602 - INFO - joeynmt.training - Epoch   6, Step:   188700, Batch Loss:     2.203542, Tokens per Sec:    15819, Lr: 0.000300\n",
      "2021-07-25 09:37:26,774 - INFO - joeynmt.training - Epoch   6, Step:   188800, Batch Loss:     1.628264, Tokens per Sec:    15621, Lr: 0.000300\n",
      "2021-07-25 09:37:40,737 - INFO - joeynmt.training - Epoch   6, Step:   188900, Batch Loss:     1.634777, Tokens per Sec:    15534, Lr: 0.000300\n",
      "2021-07-25 09:37:54,677 - INFO - joeynmt.training - Epoch   6, Step:   189000, Batch Loss:     1.772963, Tokens per Sec:    15522, Lr: 0.000300\n",
      "2021-07-25 09:38:09,052 - INFO - joeynmt.training - Epoch   6, Step:   189100, Batch Loss:     1.656617, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-07-25 09:38:23,004 - INFO - joeynmt.training - Epoch   6, Step:   189200, Batch Loss:     1.600684, Tokens per Sec:    15723, Lr: 0.000300\n",
      "2021-07-25 09:38:36,902 - INFO - joeynmt.training - Epoch   6, Step:   189300, Batch Loss:     1.625154, Tokens per Sec:    15545, Lr: 0.000300\n",
      "2021-07-25 09:38:50,986 - INFO - joeynmt.training - Epoch   6, Step:   189400, Batch Loss:     1.917773, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-07-25 09:39:04,993 - INFO - joeynmt.training - Epoch   6, Step:   189500, Batch Loss:     1.584591, Tokens per Sec:    15599, Lr: 0.000300\n",
      "2021-07-25 09:39:19,189 - INFO - joeynmt.training - Epoch   6, Step:   189600, Batch Loss:     1.762158, Tokens per Sec:    15699, Lr: 0.000300\n",
      "2021-07-25 09:39:33,185 - INFO - joeynmt.training - Epoch   6, Step:   189700, Batch Loss:     1.821487, Tokens per Sec:    15704, Lr: 0.000300\n",
      "2021-07-25 09:39:46,876 - INFO - joeynmt.training - Epoch   6, Step:   189800, Batch Loss:     1.714808, Tokens per Sec:    15480, Lr: 0.000300\n",
      "2021-07-25 09:40:00,799 - INFO - joeynmt.training - Epoch   6, Step:   189900, Batch Loss:     1.917449, Tokens per Sec:    15741, Lr: 0.000300\n",
      "2021-07-25 09:40:14,913 - INFO - joeynmt.training - Epoch   6, Step:   190000, Batch Loss:     1.661444, Tokens per Sec:    15573, Lr: 0.000300\n",
      "2021-07-25 09:40:28,928 - INFO - joeynmt.training - Epoch   6, Step:   190100, Batch Loss:     1.715053, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-25 09:40:42,706 - INFO - joeynmt.training - Epoch   6, Step:   190200, Batch Loss:     1.863637, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-07-25 09:40:56,612 - INFO - joeynmt.training - Epoch   6, Step:   190300, Batch Loss:     2.136410, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-07-25 09:41:10,443 - INFO - joeynmt.training - Epoch   6, Step:   190400, Batch Loss:     1.824557, Tokens per Sec:    15505, Lr: 0.000300\n",
      "2021-07-25 09:41:24,187 - INFO - joeynmt.training - Epoch   6, Step:   190500, Batch Loss:     1.618873, Tokens per Sec:    15591, Lr: 0.000300\n",
      "2021-07-25 09:41:38,044 - INFO - joeynmt.training - Epoch   6, Step:   190600, Batch Loss:     1.900491, Tokens per Sec:    15534, Lr: 0.000300\n",
      "2021-07-25 09:41:51,847 - INFO - joeynmt.training - Epoch   6, Step:   190700, Batch Loss:     1.552724, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-07-25 09:42:05,458 - INFO - joeynmt.training - Epoch   6, Step:   190800, Batch Loss:     1.899881, Tokens per Sec:    15581, Lr: 0.000300\n",
      "2021-07-25 09:42:19,615 - INFO - joeynmt.training - Epoch   6, Step:   190900, Batch Loss:     1.810545, Tokens per Sec:    16235, Lr: 0.000300\n",
      "2021-07-25 09:42:33,467 - INFO - joeynmt.training - Epoch   6, Step:   191000, Batch Loss:     1.788485, Tokens per Sec:    15619, Lr: 0.000300\n",
      "2021-07-25 09:42:47,278 - INFO - joeynmt.training - Epoch   6, Step:   191100, Batch Loss:     1.634665, Tokens per Sec:    15589, Lr: 0.000300\n",
      "2021-07-25 09:43:01,139 - INFO - joeynmt.training - Epoch   6, Step:   191200, Batch Loss:     1.763582, Tokens per Sec:    15248, Lr: 0.000300\n",
      "2021-07-25 09:43:14,979 - INFO - joeynmt.training - Epoch   6, Step:   191300, Batch Loss:     1.603049, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-07-25 09:43:28,824 - INFO - joeynmt.training - Epoch   6, Step:   191400, Batch Loss:     1.847513, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-07-25 09:43:42,698 - INFO - joeynmt.training - Epoch   6, Step:   191500, Batch Loss:     1.841402, Tokens per Sec:    16122, Lr: 0.000300\n",
      "2021-07-25 09:43:56,760 - INFO - joeynmt.training - Epoch   6, Step:   191600, Batch Loss:     1.799406, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-07-25 09:44:10,819 - INFO - joeynmt.training - Epoch   6, Step:   191700, Batch Loss:     1.860133, Tokens per Sec:    15779, Lr: 0.000300\n",
      "2021-07-25 09:44:24,518 - INFO - joeynmt.training - Epoch   6, Step:   191800, Batch Loss:     1.686919, Tokens per Sec:    15617, Lr: 0.000300\n",
      "2021-07-25 09:44:38,431 - INFO - joeynmt.training - Epoch   6, Step:   191900, Batch Loss:     1.682785, Tokens per Sec:    15934, Lr: 0.000300\n",
      "2021-07-25 09:44:52,256 - INFO - joeynmt.training - Epoch   6, Step:   192000, Batch Loss:     1.581731, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-07-25 09:45:14,707 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 09:45:14,707 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 09:45:14,708 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 09:45:14,976 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 09:45:14,977 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 09:45:15,695 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 09:45:15,696 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 09:45:15,696 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 09:45:15,696 - INFO - joeynmt.training - \tHypothesis: I touched my heart .\n",
      "2021-07-25 09:45:15,696 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 09:45:15,696 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 09:45:15,697 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 09:45:15,697 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar before the scroll .\n",
      "2021-07-25 09:45:15,697 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 09:45:15,697 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 09:45:15,697 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:45:15,697 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:45:15,698 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 09:45:15,698 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 09:45:15,698 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 09:45:15,699 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 09:45:15,699 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   192000: bleu:  26.54, loss: 44059.2852, ppl:   4.8850, duration: 23.4428s\n",
      "2021-07-25 09:45:29,756 - INFO - joeynmt.training - Epoch   6, Step:   192100, Batch Loss:     1.578234, Tokens per Sec:    15739, Lr: 0.000300\n",
      "2021-07-25 09:45:43,716 - INFO - joeynmt.training - Epoch   6, Step:   192200, Batch Loss:     1.726389, Tokens per Sec:    16170, Lr: 0.000300\n",
      "2021-07-25 09:45:57,550 - INFO - joeynmt.training - Epoch   6, Step:   192300, Batch Loss:     1.855641, Tokens per Sec:    15841, Lr: 0.000300\n",
      "2021-07-25 09:46:11,591 - INFO - joeynmt.training - Epoch   6, Step:   192400, Batch Loss:     1.910122, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-07-25 09:46:25,524 - INFO - joeynmt.training - Epoch   6, Step:   192500, Batch Loss:     1.823287, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-25 09:46:37,265 - INFO - joeynmt.training - Epoch   6: total training loss 9605.52\n",
      "2021-07-25 09:46:37,266 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-25 09:46:39,939 - INFO - joeynmt.training - Epoch   7, Step:   192600, Batch Loss:     1.698788, Tokens per Sec:    12080, Lr: 0.000300\n",
      "2021-07-25 09:46:53,727 - INFO - joeynmt.training - Epoch   7, Step:   192700, Batch Loss:     1.826750, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-07-25 09:47:07,612 - INFO - joeynmt.training - Epoch   7, Step:   192800, Batch Loss:     1.731636, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-07-25 09:47:21,608 - INFO - joeynmt.training - Epoch   7, Step:   192900, Batch Loss:     1.860320, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-07-25 09:47:35,715 - INFO - joeynmt.training - Epoch   7, Step:   193000, Batch Loss:     1.730102, Tokens per Sec:    16056, Lr: 0.000300\n",
      "2021-07-25 09:47:49,376 - INFO - joeynmt.training - Epoch   7, Step:   193100, Batch Loss:     1.682319, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-07-25 09:48:03,203 - INFO - joeynmt.training - Epoch   7, Step:   193200, Batch Loss:     1.542782, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-07-25 09:48:17,093 - INFO - joeynmt.training - Epoch   7, Step:   193300, Batch Loss:     1.853469, Tokens per Sec:    15536, Lr: 0.000300\n",
      "2021-07-25 09:48:31,008 - INFO - joeynmt.training - Epoch   7, Step:   193400, Batch Loss:     1.624527, Tokens per Sec:    15629, Lr: 0.000300\n",
      "2021-07-25 09:48:45,034 - INFO - joeynmt.training - Epoch   7, Step:   193500, Batch Loss:     1.732593, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-07-25 09:48:58,913 - INFO - joeynmt.training - Epoch   7, Step:   193600, Batch Loss:     1.763046, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-07-25 09:49:12,940 - INFO - joeynmt.training - Epoch   7, Step:   193700, Batch Loss:     1.973184, Tokens per Sec:    15618, Lr: 0.000300\n",
      "2021-07-25 09:49:26,976 - INFO - joeynmt.training - Epoch   7, Step:   193800, Batch Loss:     2.026683, Tokens per Sec:    16176, Lr: 0.000300\n",
      "2021-07-25 09:49:40,795 - INFO - joeynmt.training - Epoch   7, Step:   193900, Batch Loss:     1.823529, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-07-25 09:49:54,650 - INFO - joeynmt.training - Epoch   7, Step:   194000, Batch Loss:     1.711650, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-07-25 09:50:08,712 - INFO - joeynmt.training - Epoch   7, Step:   194100, Batch Loss:     1.784412, Tokens per Sec:    15976, Lr: 0.000300\n",
      "2021-07-25 09:50:22,588 - INFO - joeynmt.training - Epoch   7, Step:   194200, Batch Loss:     1.495209, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-07-25 09:50:36,197 - INFO - joeynmt.training - Epoch   7, Step:   194300, Batch Loss:     1.649124, Tokens per Sec:    15589, Lr: 0.000300\n",
      "2021-07-25 09:50:49,997 - INFO - joeynmt.training - Epoch   7, Step:   194400, Batch Loss:     1.893753, Tokens per Sec:    16025, Lr: 0.000300\n",
      "2021-07-25 09:51:04,042 - INFO - joeynmt.training - Epoch   7, Step:   194500, Batch Loss:     1.536350, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-07-25 09:51:17,979 - INFO - joeynmt.training - Epoch   7, Step:   194600, Batch Loss:     1.787523, Tokens per Sec:    15571, Lr: 0.000300\n",
      "2021-07-25 09:51:31,880 - INFO - joeynmt.training - Epoch   7, Step:   194700, Batch Loss:     1.720537, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-25 09:51:45,735 - INFO - joeynmt.training - Epoch   7, Step:   194800, Batch Loss:     1.765226, Tokens per Sec:    15745, Lr: 0.000300\n",
      "2021-07-25 09:51:59,517 - INFO - joeynmt.training - Epoch   7, Step:   194900, Batch Loss:     1.845511, Tokens per Sec:    16193, Lr: 0.000300\n",
      "2021-07-25 09:52:13,515 - INFO - joeynmt.training - Epoch   7, Step:   195000, Batch Loss:     1.720779, Tokens per Sec:    15588, Lr: 0.000300\n",
      "2021-07-25 09:52:27,334 - INFO - joeynmt.training - Epoch   7, Step:   195100, Batch Loss:     1.808221, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-07-25 09:52:41,163 - INFO - joeynmt.training - Epoch   7, Step:   195200, Batch Loss:     1.631183, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-07-25 09:52:54,998 - INFO - joeynmt.training - Epoch   7, Step:   195300, Batch Loss:     1.985181, Tokens per Sec:    15857, Lr: 0.000300\n",
      "2021-07-25 09:53:08,830 - INFO - joeynmt.training - Epoch   7, Step:   195400, Batch Loss:     1.732633, Tokens per Sec:    16089, Lr: 0.000300\n",
      "2021-07-25 09:53:22,852 - INFO - joeynmt.training - Epoch   7, Step:   195500, Batch Loss:     1.669241, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-07-25 09:53:36,751 - INFO - joeynmt.training - Epoch   7, Step:   195600, Batch Loss:     1.738672, Tokens per Sec:    15752, Lr: 0.000300\n",
      "2021-07-25 09:53:50,972 - INFO - joeynmt.training - Epoch   7, Step:   195700, Batch Loss:     1.796779, Tokens per Sec:    15974, Lr: 0.000300\n",
      "2021-07-25 09:54:05,066 - INFO - joeynmt.training - Epoch   7, Step:   195800, Batch Loss:     1.746608, Tokens per Sec:    15959, Lr: 0.000300\n",
      "2021-07-25 09:54:18,950 - INFO - joeynmt.training - Epoch   7, Step:   195900, Batch Loss:     1.652055, Tokens per Sec:    15805, Lr: 0.000300\n",
      "2021-07-25 09:54:32,587 - INFO - joeynmt.training - Epoch   7, Step:   196000, Batch Loss:     1.807046, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-07-25 09:54:46,368 - INFO - joeynmt.training - Epoch   7, Step:   196100, Batch Loss:     1.738801, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-07-25 09:55:00,438 - INFO - joeynmt.training - Epoch   7, Step:   196200, Batch Loss:     1.799931, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-07-25 09:55:14,552 - INFO - joeynmt.training - Epoch   7, Step:   196300, Batch Loss:     1.884209, Tokens per Sec:    15698, Lr: 0.000300\n",
      "2021-07-25 09:55:28,328 - INFO - joeynmt.training - Epoch   7, Step:   196400, Batch Loss:     1.544490, Tokens per Sec:    15466, Lr: 0.000300\n",
      "2021-07-25 09:55:41,949 - INFO - joeynmt.training - Epoch   7, Step:   196500, Batch Loss:     1.511706, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-07-25 09:55:55,621 - INFO - joeynmt.training - Epoch   7, Step:   196600, Batch Loss:     1.973376, Tokens per Sec:    15701, Lr: 0.000300\n",
      "2021-07-25 09:56:09,628 - INFO - joeynmt.training - Epoch   7, Step:   196700, Batch Loss:     1.868852, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-07-25 09:56:23,666 - INFO - joeynmt.training - Epoch   7, Step:   196800, Batch Loss:     1.782896, Tokens per Sec:    15760, Lr: 0.000300\n",
      "2021-07-25 09:56:37,629 - INFO - joeynmt.training - Epoch   7, Step:   196900, Batch Loss:     1.888100, Tokens per Sec:    15933, Lr: 0.000300\n",
      "2021-07-25 09:56:51,475 - INFO - joeynmt.training - Epoch   7, Step:   197000, Batch Loss:     1.709624, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-07-25 09:57:05,421 - INFO - joeynmt.training - Epoch   7, Step:   197100, Batch Loss:     1.828215, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-07-25 09:57:19,549 - INFO - joeynmt.training - Epoch   7, Step:   197200, Batch Loss:     1.858503, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-07-25 09:57:33,393 - INFO - joeynmt.training - Epoch   7, Step:   197300, Batch Loss:     1.673547, Tokens per Sec:    15801, Lr: 0.000300\n",
      "2021-07-25 09:57:47,229 - INFO - joeynmt.training - Epoch   7, Step:   197400, Batch Loss:     1.485354, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-07-25 09:58:01,233 - INFO - joeynmt.training - Epoch   7, Step:   197500, Batch Loss:     1.814059, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-07-25 09:58:14,909 - INFO - joeynmt.training - Epoch   7, Step:   197600, Batch Loss:     1.873340, Tokens per Sec:    15365, Lr: 0.000300\n",
      "2021-07-25 09:58:28,884 - INFO - joeynmt.training - Epoch   7, Step:   197700, Batch Loss:     1.780973, Tokens per Sec:    15664, Lr: 0.000300\n",
      "2021-07-25 09:58:42,810 - INFO - joeynmt.training - Epoch   7, Step:   197800, Batch Loss:     1.671686, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-07-25 09:58:56,455 - INFO - joeynmt.training - Epoch   7, Step:   197900, Batch Loss:     1.578769, Tokens per Sec:    15485, Lr: 0.000300\n",
      "2021-07-25 09:59:10,292 - INFO - joeynmt.training - Epoch   7, Step:   198000, Batch Loss:     1.710905, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-07-25 09:59:32,728 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 09:59:32,728 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 09:59:32,729 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 09:59:32,998 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 09:59:32,999 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 09:59:33,744 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 09:59:33,745 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 09:59:33,745 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 09:59:33,745 - INFO - joeynmt.training - \tHypothesis: I touched my heart .\n",
      "2021-07-25 09:59:33,745 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 09:59:33,746 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 09:59:33,746 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 09:59:33,746 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
      "2021-07-25 09:59:33,746 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 09:59:33,746 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 09:59:33,747 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:59:33,747 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 09:59:33,747 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 09:59:33,747 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 09:59:33,747 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 09:59:33,748 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 09:59:33,748 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   198000: bleu:  26.73, loss: 43921.4883, ppl:   4.8609, duration: 23.4552s\n",
      "2021-07-25 09:59:34,089 - INFO - joeynmt.training - Epoch   7: total training loss 9570.29\n",
      "2021-07-25 09:59:34,089 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-25 09:59:48,499 - INFO - joeynmt.training - Epoch   8, Step:   198100, Batch Loss:     1.692088, Tokens per Sec:    15073, Lr: 0.000300\n",
      "2021-07-25 10:00:02,696 - INFO - joeynmt.training - Epoch   8, Step:   198200, Batch Loss:     1.650673, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-07-25 10:00:16,573 - INFO - joeynmt.training - Epoch   8, Step:   198300, Batch Loss:     1.692827, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-07-25 10:00:30,621 - INFO - joeynmt.training - Epoch   8, Step:   198400, Batch Loss:     1.557124, Tokens per Sec:    16283, Lr: 0.000300\n",
      "2021-07-25 10:00:44,497 - INFO - joeynmt.training - Epoch   8, Step:   198500, Batch Loss:     1.690615, Tokens per Sec:    15968, Lr: 0.000300\n",
      "2021-07-25 10:00:58,379 - INFO - joeynmt.training - Epoch   8, Step:   198600, Batch Loss:     1.857504, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-07-25 10:01:12,341 - INFO - joeynmt.training - Epoch   8, Step:   198700, Batch Loss:     2.034301, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-07-25 10:01:26,231 - INFO - joeynmt.training - Epoch   8, Step:   198800, Batch Loss:     1.759168, Tokens per Sec:    15798, Lr: 0.000300\n",
      "2021-07-25 10:01:40,165 - INFO - joeynmt.training - Epoch   8, Step:   198900, Batch Loss:     1.845267, Tokens per Sec:    16195, Lr: 0.000300\n",
      "2021-07-25 10:01:53,931 - INFO - joeynmt.training - Epoch   8, Step:   199000, Batch Loss:     1.957143, Tokens per Sec:    15945, Lr: 0.000300\n",
      "2021-07-25 10:02:07,666 - INFO - joeynmt.training - Epoch   8, Step:   199100, Batch Loss:     1.784184, Tokens per Sec:    15632, Lr: 0.000300\n",
      "2021-07-25 10:02:21,571 - INFO - joeynmt.training - Epoch   8, Step:   199200, Batch Loss:     1.509196, Tokens per Sec:    15353, Lr: 0.000300\n",
      "2021-07-25 10:02:35,379 - INFO - joeynmt.training - Epoch   8, Step:   199300, Batch Loss:     1.512095, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-07-25 10:02:49,182 - INFO - joeynmt.training - Epoch   8, Step:   199400, Batch Loss:     1.783960, Tokens per Sec:    15837, Lr: 0.000300\n",
      "2021-07-25 10:03:03,043 - INFO - joeynmt.training - Epoch   8, Step:   199500, Batch Loss:     1.795020, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-07-25 10:03:17,245 - INFO - joeynmt.training - Epoch   8, Step:   199600, Batch Loss:     1.704723, Tokens per Sec:    15672, Lr: 0.000300\n",
      "2021-07-25 10:03:31,308 - INFO - joeynmt.training - Epoch   8, Step:   199700, Batch Loss:     1.833339, Tokens per Sec:    15607, Lr: 0.000300\n",
      "2021-07-25 10:03:45,276 - INFO - joeynmt.training - Epoch   8, Step:   199800, Batch Loss:     1.726381, Tokens per Sec:    15944, Lr: 0.000300\n",
      "2021-07-25 10:03:59,425 - INFO - joeynmt.training - Epoch   8, Step:   199900, Batch Loss:     1.576743, Tokens per Sec:    16063, Lr: 0.000300\n",
      "2021-07-25 10:04:13,351 - INFO - joeynmt.training - Epoch   8, Step:   200000, Batch Loss:     1.713982, Tokens per Sec:    15561, Lr: 0.000300\n",
      "2021-07-25 10:04:27,400 - INFO - joeynmt.training - Epoch   8, Step:   200100, Batch Loss:     1.719389, Tokens per Sec:    15351, Lr: 0.000300\n",
      "2021-07-25 10:04:41,476 - INFO - joeynmt.training - Epoch   8, Step:   200200, Batch Loss:     1.743155, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-07-25 10:04:55,454 - INFO - joeynmt.training - Epoch   8, Step:   200300, Batch Loss:     1.657994, Tokens per Sec:    15551, Lr: 0.000300\n",
      "2021-07-25 10:05:09,437 - INFO - joeynmt.training - Epoch   8, Step:   200400, Batch Loss:     1.854875, Tokens per Sec:    15723, Lr: 0.000300\n",
      "2021-07-25 10:05:23,180 - INFO - joeynmt.training - Epoch   8, Step:   200500, Batch Loss:     1.722975, Tokens per Sec:    15469, Lr: 0.000300\n",
      "2021-07-25 10:05:37,258 - INFO - joeynmt.training - Epoch   8, Step:   200600, Batch Loss:     1.733610, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-07-25 10:05:51,215 - INFO - joeynmt.training - Epoch   8, Step:   200700, Batch Loss:     1.693422, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-07-25 10:06:05,126 - INFO - joeynmt.training - Epoch   8, Step:   200800, Batch Loss:     1.888655, Tokens per Sec:    15214, Lr: 0.000300\n",
      "2021-07-25 10:06:19,162 - INFO - joeynmt.training - Epoch   8, Step:   200900, Batch Loss:     1.829551, Tokens per Sec:    15546, Lr: 0.000300\n",
      "2021-07-25 10:06:33,125 - INFO - joeynmt.training - Epoch   8, Step:   201000, Batch Loss:     1.664953, Tokens per Sec:    15765, Lr: 0.000300\n",
      "2021-07-25 10:06:46,978 - INFO - joeynmt.training - Epoch   8, Step:   201100, Batch Loss:     1.838477, Tokens per Sec:    15732, Lr: 0.000300\n",
      "2021-07-25 10:07:01,005 - INFO - joeynmt.training - Epoch   8, Step:   201200, Batch Loss:     1.724595, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-07-25 10:07:15,064 - INFO - joeynmt.training - Epoch   8, Step:   201300, Batch Loss:     1.530374, Tokens per Sec:    15824, Lr: 0.000300\n",
      "2021-07-25 10:07:29,072 - INFO - joeynmt.training - Epoch   8, Step:   201400, Batch Loss:     1.879671, Tokens per Sec:    15643, Lr: 0.000300\n",
      "2021-07-25 10:07:42,982 - INFO - joeynmt.training - Epoch   8, Step:   201500, Batch Loss:     1.883850, Tokens per Sec:    15840, Lr: 0.000300\n",
      "2021-07-25 10:07:56,912 - INFO - joeynmt.training - Epoch   8, Step:   201600, Batch Loss:     1.809390, Tokens per Sec:    15631, Lr: 0.000300\n",
      "2021-07-25 10:08:10,911 - INFO - joeynmt.training - Epoch   8, Step:   201700, Batch Loss:     1.701567, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-25 10:08:25,240 - INFO - joeynmt.training - Epoch   8, Step:   201800, Batch Loss:     1.781415, Tokens per Sec:    15931, Lr: 0.000300\n",
      "2021-07-25 10:08:39,129 - INFO - joeynmt.training - Epoch   8, Step:   201900, Batch Loss:     1.955247, Tokens per Sec:    15716, Lr: 0.000300\n",
      "2021-07-25 10:08:53,030 - INFO - joeynmt.training - Epoch   8, Step:   202000, Batch Loss:     1.471231, Tokens per Sec:    15603, Lr: 0.000300\n",
      "2021-07-25 10:09:07,207 - INFO - joeynmt.training - Epoch   8, Step:   202100, Batch Loss:     1.802223, Tokens per Sec:    16242, Lr: 0.000300\n",
      "2021-07-25 10:09:21,014 - INFO - joeynmt.training - Epoch   8, Step:   202200, Batch Loss:     1.629360, Tokens per Sec:    15460, Lr: 0.000300\n",
      "2021-07-25 10:09:35,188 - INFO - joeynmt.training - Epoch   8, Step:   202300, Batch Loss:     1.880822, Tokens per Sec:    16079, Lr: 0.000300\n",
      "2021-07-25 10:09:49,133 - INFO - joeynmt.training - Epoch   8, Step:   202400, Batch Loss:     1.776503, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-07-25 10:10:03,024 - INFO - joeynmt.training - Epoch   8, Step:   202500, Batch Loss:     1.775212, Tokens per Sec:    15527, Lr: 0.000300\n",
      "2021-07-25 10:10:16,861 - INFO - joeynmt.training - Epoch   8, Step:   202600, Batch Loss:     1.710326, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-07-25 10:10:30,868 - INFO - joeynmt.training - Epoch   8, Step:   202700, Batch Loss:     1.811401, Tokens per Sec:    15568, Lr: 0.000300\n",
      "2021-07-25 10:10:44,853 - INFO - joeynmt.training - Epoch   8, Step:   202800, Batch Loss:     1.691929, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-07-25 10:10:58,843 - INFO - joeynmt.training - Epoch   8, Step:   202900, Batch Loss:     1.744085, Tokens per Sec:    15721, Lr: 0.000300\n",
      "2021-07-25 10:11:12,953 - INFO - joeynmt.training - Epoch   8, Step:   203000, Batch Loss:     1.605650, Tokens per Sec:    15676, Lr: 0.000300\n",
      "2021-07-25 10:11:26,895 - INFO - joeynmt.training - Epoch   8, Step:   203100, Batch Loss:     1.883583, Tokens per Sec:    15925, Lr: 0.000300\n",
      "2021-07-25 10:11:40,618 - INFO - joeynmt.training - Epoch   8, Step:   203200, Batch Loss:     1.765979, Tokens per Sec:    15432, Lr: 0.000300\n",
      "2021-07-25 10:11:54,449 - INFO - joeynmt.training - Epoch   8, Step:   203300, Batch Loss:     1.517596, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-07-25 10:12:08,522 - INFO - joeynmt.training - Epoch   8, Step:   203400, Batch Loss:     2.003021, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-07-25 10:12:11,569 - INFO - joeynmt.training - Epoch   8: total training loss 9544.07\n",
      "2021-07-25 10:12:11,570 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-25 10:12:23,204 - INFO - joeynmt.training - Epoch   9, Step:   203500, Batch Loss:     1.639491, Tokens per Sec:    14853, Lr: 0.000300\n",
      "2021-07-25 10:12:36,995 - INFO - joeynmt.training - Epoch   9, Step:   203600, Batch Loss:     1.719329, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-07-25 10:12:50,782 - INFO - joeynmt.training - Epoch   9, Step:   203700, Batch Loss:     1.866497, Tokens per Sec:    15566, Lr: 0.000300\n",
      "2021-07-25 10:13:04,784 - INFO - joeynmt.training - Epoch   9, Step:   203800, Batch Loss:     1.624593, Tokens per Sec:    15687, Lr: 0.000300\n",
      "2021-07-25 10:13:18,772 - INFO - joeynmt.training - Epoch   9, Step:   203900, Batch Loss:     1.624597, Tokens per Sec:    15668, Lr: 0.000300\n",
      "2021-07-25 10:13:32,648 - INFO - joeynmt.training - Epoch   9, Step:   204000, Batch Loss:     1.897074, Tokens per Sec:    15590, Lr: 0.000300\n",
      "2021-07-25 10:13:53,460 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 10:13:53,460 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 10:13:53,460 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 10:13:53,700 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 10:13:53,700 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 10:13:54,412 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 10:13:54,412 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 10:13:54,413 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 10:13:54,413 - INFO - joeynmt.training - \tHypothesis: I was deeply impressed .\n",
      "2021-07-25 10:13:54,413 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 10:13:54,413 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 10:13:54,413 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 10:13:54,414 - INFO - joeynmt.training - \tHypothesis: The writer was written on the front of the scroll .\n",
      "2021-07-25 10:13:54,414 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 10:13:54,414 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 10:13:54,414 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 10:13:54,414 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 10:13:54,415 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 10:13:54,415 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 10:13:54,415 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 10:13:54,415 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 10:13:54,415 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   204000: bleu:  26.83, loss: 43806.0547, ppl:   4.8407, duration: 21.7674s\n",
      "2021-07-25 10:14:08,766 - INFO - joeynmt.training - Epoch   9, Step:   204100, Batch Loss:     1.613999, Tokens per Sec:    15171, Lr: 0.000300\n",
      "2021-07-25 10:14:22,897 - INFO - joeynmt.training - Epoch   9, Step:   204200, Batch Loss:     1.709345, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-07-25 10:14:36,814 - INFO - joeynmt.training - Epoch   9, Step:   204300, Batch Loss:     1.823036, Tokens per Sec:    15833, Lr: 0.000300\n",
      "2021-07-25 10:14:50,596 - INFO - joeynmt.training - Epoch   9, Step:   204400, Batch Loss:     1.722121, Tokens per Sec:    15575, Lr: 0.000300\n",
      "2021-07-25 10:15:04,404 - INFO - joeynmt.training - Epoch   9, Step:   204500, Batch Loss:     1.870339, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-25 10:15:18,429 - INFO - joeynmt.training - Epoch   9, Step:   204600, Batch Loss:     1.792675, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-07-25 10:15:32,315 - INFO - joeynmt.training - Epoch   9, Step:   204700, Batch Loss:     1.766452, Tokens per Sec:    15607, Lr: 0.000300\n",
      "2021-07-25 10:15:46,132 - INFO - joeynmt.training - Epoch   9, Step:   204800, Batch Loss:     1.844956, Tokens per Sec:    15677, Lr: 0.000300\n",
      "2021-07-25 10:16:00,194 - INFO - joeynmt.training - Epoch   9, Step:   204900, Batch Loss:     1.723874, Tokens per Sec:    15626, Lr: 0.000300\n",
      "2021-07-25 10:16:13,902 - INFO - joeynmt.training - Epoch   9, Step:   205000, Batch Loss:     1.811840, Tokens per Sec:    15584, Lr: 0.000300\n",
      "2021-07-25 10:16:27,865 - INFO - joeynmt.training - Epoch   9, Step:   205100, Batch Loss:     1.513995, Tokens per Sec:    16238, Lr: 0.000300\n",
      "2021-07-25 10:16:41,771 - INFO - joeynmt.training - Epoch   9, Step:   205200, Batch Loss:     1.763641, Tokens per Sec:    15752, Lr: 0.000300\n",
      "2021-07-25 10:16:55,732 - INFO - joeynmt.training - Epoch   9, Step:   205300, Batch Loss:     1.545097, Tokens per Sec:    15881, Lr: 0.000300\n",
      "2021-07-25 10:17:09,622 - INFO - joeynmt.training - Epoch   9, Step:   205400, Batch Loss:     1.886666, Tokens per Sec:    15363, Lr: 0.000300\n",
      "2021-07-25 10:17:23,406 - INFO - joeynmt.training - Epoch   9, Step:   205500, Batch Loss:     1.822038, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-07-25 10:17:37,223 - INFO - joeynmt.training - Epoch   9, Step:   205600, Batch Loss:     1.686058, Tokens per Sec:    15712, Lr: 0.000300\n",
      "2021-07-25 10:17:50,939 - INFO - joeynmt.training - Epoch   9, Step:   205700, Batch Loss:     1.685604, Tokens per Sec:    16078, Lr: 0.000300\n",
      "2021-07-25 10:18:04,932 - INFO - joeynmt.training - Epoch   9, Step:   205800, Batch Loss:     1.759194, Tokens per Sec:    15899, Lr: 0.000300\n",
      "2021-07-25 10:18:19,111 - INFO - joeynmt.training - Epoch   9, Step:   205900, Batch Loss:     1.803183, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-07-25 10:18:33,021 - INFO - joeynmt.training - Epoch   9, Step:   206000, Batch Loss:     2.032290, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-07-25 10:18:46,971 - INFO - joeynmt.training - Epoch   9, Step:   206100, Batch Loss:     1.753722, Tokens per Sec:    16107, Lr: 0.000300\n",
      "2021-07-25 10:19:00,777 - INFO - joeynmt.training - Epoch   9, Step:   206200, Batch Loss:     2.185452, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-07-25 10:19:14,772 - INFO - joeynmt.training - Epoch   9, Step:   206300, Batch Loss:     2.025002, Tokens per Sec:    15606, Lr: 0.000300\n",
      "2021-07-25 10:19:28,735 - INFO - joeynmt.training - Epoch   9, Step:   206400, Batch Loss:     1.698441, Tokens per Sec:    15312, Lr: 0.000300\n",
      "2021-07-25 10:19:42,541 - INFO - joeynmt.training - Epoch   9, Step:   206500, Batch Loss:     1.814286, Tokens per Sec:    15763, Lr: 0.000300\n",
      "2021-07-25 10:19:56,490 - INFO - joeynmt.training - Epoch   9, Step:   206600, Batch Loss:     1.807487, Tokens per Sec:    15756, Lr: 0.000300\n",
      "2021-07-25 10:20:10,146 - INFO - joeynmt.training - Epoch   9, Step:   206700, Batch Loss:     1.874189, Tokens per Sec:    15620, Lr: 0.000300\n",
      "2021-07-25 10:20:24,176 - INFO - joeynmt.training - Epoch   9, Step:   206800, Batch Loss:     1.678931, Tokens per Sec:    15731, Lr: 0.000300\n",
      "2021-07-25 10:20:38,114 - INFO - joeynmt.training - Epoch   9, Step:   206900, Batch Loss:     1.687489, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-25 10:20:51,939 - INFO - joeynmt.training - Epoch   9, Step:   207000, Batch Loss:     1.740173, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-07-25 10:21:05,861 - INFO - joeynmt.training - Epoch   9, Step:   207100, Batch Loss:     1.768665, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-07-25 10:21:19,815 - INFO - joeynmt.training - Epoch   9, Step:   207200, Batch Loss:     1.686479, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-07-25 10:21:33,879 - INFO - joeynmt.training - Epoch   9, Step:   207300, Batch Loss:     1.806603, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-07-25 10:21:47,664 - INFO - joeynmt.training - Epoch   9, Step:   207400, Batch Loss:     1.803082, Tokens per Sec:    15716, Lr: 0.000300\n",
      "2021-07-25 10:22:01,621 - INFO - joeynmt.training - Epoch   9, Step:   207500, Batch Loss:     1.616928, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-07-25 10:22:15,506 - INFO - joeynmt.training - Epoch   9, Step:   207600, Batch Loss:     1.647580, Tokens per Sec:    15450, Lr: 0.000300\n",
      "2021-07-25 10:22:29,504 - INFO - joeynmt.training - Epoch   9, Step:   207700, Batch Loss:     1.887701, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-07-25 10:22:43,374 - INFO - joeynmt.training - Epoch   9, Step:   207800, Batch Loss:     1.696671, Tokens per Sec:    15894, Lr: 0.000300\n",
      "2021-07-25 10:22:57,227 - INFO - joeynmt.training - Epoch   9, Step:   207900, Batch Loss:     1.666909, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-07-25 10:23:11,329 - INFO - joeynmt.training - Epoch   9, Step:   208000, Batch Loss:     1.721333, Tokens per Sec:    15833, Lr: 0.000300\n",
      "2021-07-25 10:23:25,214 - INFO - joeynmt.training - Epoch   9, Step:   208100, Batch Loss:     1.558628, Tokens per Sec:    16183, Lr: 0.000300\n",
      "2021-07-25 10:23:39,072 - INFO - joeynmt.training - Epoch   9, Step:   208200, Batch Loss:     1.790760, Tokens per Sec:    15749, Lr: 0.000300\n",
      "2021-07-25 10:23:52,967 - INFO - joeynmt.training - Epoch   9, Step:   208300, Batch Loss:     1.740179, Tokens per Sec:    16208, Lr: 0.000300\n",
      "2021-07-25 10:24:06,781 - INFO - joeynmt.training - Epoch   9, Step:   208400, Batch Loss:     1.810639, Tokens per Sec:    15470, Lr: 0.000300\n",
      "2021-07-25 10:24:20,737 - INFO - joeynmt.training - Epoch   9, Step:   208500, Batch Loss:     2.029376, Tokens per Sec:    15929, Lr: 0.000300\n",
      "2021-07-25 10:24:34,641 - INFO - joeynmt.training - Epoch   9, Step:   208600, Batch Loss:     1.713019, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-07-25 10:24:48,526 - INFO - joeynmt.training - Epoch   9, Step:   208700, Batch Loss:     1.923593, Tokens per Sec:    15973, Lr: 0.000300\n",
      "2021-07-25 10:25:02,280 - INFO - joeynmt.training - Epoch   9, Step:   208800, Batch Loss:     1.881552, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-25 10:25:08,844 - INFO - joeynmt.training - Epoch   9: total training loss 9531.07\n",
      "2021-07-25 10:25:08,844 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-25 10:25:16,849 - INFO - joeynmt.training - Epoch  10, Step:   208900, Batch Loss:     1.812853, Tokens per Sec:    14722, Lr: 0.000300\n",
      "2021-07-25 10:25:30,772 - INFO - joeynmt.training - Epoch  10, Step:   209000, Batch Loss:     1.579169, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-07-25 10:25:44,484 - INFO - joeynmt.training - Epoch  10, Step:   209100, Batch Loss:     1.835078, Tokens per Sec:    15834, Lr: 0.000300\n",
      "2021-07-25 10:25:58,176 - INFO - joeynmt.training - Epoch  10, Step:   209200, Batch Loss:     1.740914, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-07-25 10:26:11,936 - INFO - joeynmt.training - Epoch  10, Step:   209300, Batch Loss:     2.198062, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-07-25 10:26:25,862 - INFO - joeynmt.training - Epoch  10, Step:   209400, Batch Loss:     1.816197, Tokens per Sec:    15681, Lr: 0.000300\n",
      "2021-07-25 10:26:39,642 - INFO - joeynmt.training - Epoch  10, Step:   209500, Batch Loss:     2.075777, Tokens per Sec:    15732, Lr: 0.000300\n",
      "2021-07-25 10:26:53,447 - INFO - joeynmt.training - Epoch  10, Step:   209600, Batch Loss:     1.674303, Tokens per Sec:    15465, Lr: 0.000300\n",
      "2021-07-25 10:27:07,380 - INFO - joeynmt.training - Epoch  10, Step:   209700, Batch Loss:     1.868402, Tokens per Sec:    15710, Lr: 0.000300\n",
      "2021-07-25 10:27:21,510 - INFO - joeynmt.training - Epoch  10, Step:   209800, Batch Loss:     1.774006, Tokens per Sec:    16040, Lr: 0.000300\n",
      "2021-07-25 10:27:35,442 - INFO - joeynmt.training - Epoch  10, Step:   209900, Batch Loss:     1.940799, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-25 10:27:49,394 - INFO - joeynmt.training - Epoch  10, Step:   210000, Batch Loss:     1.674992, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-07-25 10:28:11,337 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 10:28:11,338 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 10:28:11,338 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 10:28:11,600 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 10:28:11,601 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 10:28:12,694 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 10:28:12,696 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 10:28:12,696 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 10:28:12,696 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-25 10:28:12,696 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 10:28:12,697 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 10:28:12,697 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 10:28:12,697 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
      "2021-07-25 10:28:12,697 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 10:28:12,698 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 10:28:12,698 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 10:28:12,698 - INFO - joeynmt.training - \tHypothesis: Rather than worry or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 10:28:12,698 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 10:28:12,699 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 10:28:12,699 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 10:28:12,699 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in a sense , that they have been well - fashioned in Satan’s world .\n",
      "2021-07-25 10:28:12,699 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   210000: bleu:  27.03, loss: 43627.5430, ppl:   4.8097, duration: 23.3045s\n",
      "2021-07-25 10:28:26,629 - INFO - joeynmt.training - Epoch  10, Step:   210100, Batch Loss:     1.639949, Tokens per Sec:    15792, Lr: 0.000300\n",
      "2021-07-25 10:28:40,612 - INFO - joeynmt.training - Epoch  10, Step:   210200, Batch Loss:     1.751115, Tokens per Sec:    16005, Lr: 0.000300\n",
      "2021-07-25 10:28:54,517 - INFO - joeynmt.training - Epoch  10, Step:   210300, Batch Loss:     1.760235, Tokens per Sec:    16205, Lr: 0.000300\n",
      "2021-07-25 10:29:08,342 - INFO - joeynmt.training - Epoch  10, Step:   210400, Batch Loss:     1.861404, Tokens per Sec:    15869, Lr: 0.000300\n",
      "2021-07-25 10:29:22,101 - INFO - joeynmt.training - Epoch  10, Step:   210500, Batch Loss:     1.421697, Tokens per Sec:    15554, Lr: 0.000300\n",
      "2021-07-25 10:29:35,936 - INFO - joeynmt.training - Epoch  10, Step:   210600, Batch Loss:     1.807793, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-25 10:29:49,687 - INFO - joeynmt.training - Epoch  10, Step:   210700, Batch Loss:     1.645861, Tokens per Sec:    15796, Lr: 0.000300\n",
      "2021-07-25 10:30:03,571 - INFO - joeynmt.training - Epoch  10, Step:   210800, Batch Loss:     1.684588, Tokens per Sec:    15812, Lr: 0.000300\n",
      "2021-07-25 10:30:17,500 - INFO - joeynmt.training - Epoch  10, Step:   210900, Batch Loss:     1.521252, Tokens per Sec:    15648, Lr: 0.000300\n",
      "2021-07-25 10:30:31,329 - INFO - joeynmt.training - Epoch  10, Step:   211000, Batch Loss:     1.774963, Tokens per Sec:    15789, Lr: 0.000300\n",
      "2021-07-25 10:30:45,076 - INFO - joeynmt.training - Epoch  10, Step:   211100, Batch Loss:     1.572035, Tokens per Sec:    15812, Lr: 0.000300\n",
      "2021-07-25 10:30:58,983 - INFO - joeynmt.training - Epoch  10, Step:   211200, Batch Loss:     1.657853, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-07-25 10:31:13,049 - INFO - joeynmt.training - Epoch  10, Step:   211300, Batch Loss:     1.817016, Tokens per Sec:    16210, Lr: 0.000300\n",
      "2021-07-25 10:31:26,930 - INFO - joeynmt.training - Epoch  10, Step:   211400, Batch Loss:     1.859661, Tokens per Sec:    15846, Lr: 0.000300\n",
      "2021-07-25 10:31:40,914 - INFO - joeynmt.training - Epoch  10, Step:   211500, Batch Loss:     1.721844, Tokens per Sec:    15798, Lr: 0.000300\n",
      "2021-07-25 10:31:54,859 - INFO - joeynmt.training - Epoch  10, Step:   211600, Batch Loss:     1.850916, Tokens per Sec:    16092, Lr: 0.000300\n",
      "2021-07-25 10:32:08,675 - INFO - joeynmt.training - Epoch  10, Step:   211700, Batch Loss:     1.740757, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-07-25 10:32:22,424 - INFO - joeynmt.training - Epoch  10, Step:   211800, Batch Loss:     1.755371, Tokens per Sec:    15608, Lr: 0.000300\n",
      "2021-07-25 10:32:36,650 - INFO - joeynmt.training - Epoch  10, Step:   211900, Batch Loss:     1.561782, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-07-25 10:32:50,668 - INFO - joeynmt.training - Epoch  10, Step:   212000, Batch Loss:     1.980894, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-07-25 10:33:04,808 - INFO - joeynmt.training - Epoch  10, Step:   212100, Batch Loss:     1.709882, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-07-25 10:33:18,598 - INFO - joeynmt.training - Epoch  10, Step:   212200, Batch Loss:     1.827500, Tokens per Sec:    15536, Lr: 0.000300\n",
      "2021-07-25 10:33:32,334 - INFO - joeynmt.training - Epoch  10, Step:   212300, Batch Loss:     1.686370, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-07-25 10:33:46,169 - INFO - joeynmt.training - Epoch  10, Step:   212400, Batch Loss:     1.798362, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-07-25 10:33:59,962 - INFO - joeynmt.training - Epoch  10, Step:   212500, Batch Loss:     1.697997, Tokens per Sec:    15421, Lr: 0.000300\n",
      "2021-07-25 10:34:13,863 - INFO - joeynmt.training - Epoch  10, Step:   212600, Batch Loss:     1.756030, Tokens per Sec:    15505, Lr: 0.000300\n",
      "2021-07-25 10:34:27,795 - INFO - joeynmt.training - Epoch  10, Step:   212700, Batch Loss:     1.820310, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-07-25 10:34:41,623 - INFO - joeynmt.training - Epoch  10, Step:   212800, Batch Loss:     1.692124, Tokens per Sec:    15844, Lr: 0.000300\n",
      "2021-07-25 10:34:55,634 - INFO - joeynmt.training - Epoch  10, Step:   212900, Batch Loss:     1.906910, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-07-25 10:35:09,633 - INFO - joeynmt.training - Epoch  10, Step:   213000, Batch Loss:     1.700112, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-07-25 10:35:23,635 - INFO - joeynmt.training - Epoch  10, Step:   213100, Batch Loss:     1.493492, Tokens per Sec:    15557, Lr: 0.000300\n",
      "2021-07-25 10:35:37,534 - INFO - joeynmt.training - Epoch  10, Step:   213200, Batch Loss:     1.759085, Tokens per Sec:    15730, Lr: 0.000300\n",
      "2021-07-25 10:35:51,594 - INFO - joeynmt.training - Epoch  10, Step:   213300, Batch Loss:     1.786754, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-07-25 10:36:05,416 - INFO - joeynmt.training - Epoch  10, Step:   213400, Batch Loss:     1.733774, Tokens per Sec:    15566, Lr: 0.000300\n",
      "2021-07-25 10:36:19,530 - INFO - joeynmt.training - Epoch  10, Step:   213500, Batch Loss:     1.669119, Tokens per Sec:    15491, Lr: 0.000300\n",
      "2021-07-25 10:36:33,641 - INFO - joeynmt.training - Epoch  10, Step:   213600, Batch Loss:     1.976681, Tokens per Sec:    15999, Lr: 0.000300\n",
      "2021-07-25 10:36:47,458 - INFO - joeynmt.training - Epoch  10, Step:   213700, Batch Loss:     1.735767, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-07-25 10:37:01,456 - INFO - joeynmt.training - Epoch  10, Step:   213800, Batch Loss:     1.698349, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-07-25 10:37:15,306 - INFO - joeynmt.training - Epoch  10, Step:   213900, Batch Loss:     1.597355, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-07-25 10:37:29,229 - INFO - joeynmt.training - Epoch  10, Step:   214000, Batch Loss:     2.202692, Tokens per Sec:    15580, Lr: 0.000300\n",
      "2021-07-25 10:37:43,082 - INFO - joeynmt.training - Epoch  10, Step:   214100, Batch Loss:     2.001299, Tokens per Sec:    15533, Lr: 0.000300\n",
      "2021-07-25 10:37:57,179 - INFO - joeynmt.training - Epoch  10, Step:   214200, Batch Loss:     1.945973, Tokens per Sec:    15740, Lr: 0.000300\n",
      "2021-07-25 10:38:06,984 - INFO - joeynmt.training - Epoch  10: total training loss 9501.79\n",
      "2021-07-25 10:38:06,985 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-25 10:38:11,720 - INFO - joeynmt.training - Epoch  11, Step:   214300, Batch Loss:     1.741032, Tokens per Sec:    13891, Lr: 0.000300\n",
      "2021-07-25 10:38:25,684 - INFO - joeynmt.training - Epoch  11, Step:   214400, Batch Loss:     1.648864, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-07-25 10:38:39,662 - INFO - joeynmt.training - Epoch  11, Step:   214500, Batch Loss:     1.917991, Tokens per Sec:    15798, Lr: 0.000300\n",
      "2021-07-25 10:38:53,833 - INFO - joeynmt.training - Epoch  11, Step:   214600, Batch Loss:     1.546453, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-25 10:39:07,770 - INFO - joeynmt.training - Epoch  11, Step:   214700, Batch Loss:     1.548885, Tokens per Sec:    15730, Lr: 0.000300\n",
      "2021-07-25 10:39:21,856 - INFO - joeynmt.training - Epoch  11, Step:   214800, Batch Loss:     1.651141, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-07-25 10:39:35,861 - INFO - joeynmt.training - Epoch  11, Step:   214900, Batch Loss:     1.635707, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 10:39:49,930 - INFO - joeynmt.training - Epoch  11, Step:   215000, Batch Loss:     1.800907, Tokens per Sec:    16108, Lr: 0.000300\n",
      "2021-07-25 10:40:03,895 - INFO - joeynmt.training - Epoch  11, Step:   215100, Batch Loss:     1.732167, Tokens per Sec:    15729, Lr: 0.000300\n",
      "2021-07-25 10:40:17,793 - INFO - joeynmt.training - Epoch  11, Step:   215200, Batch Loss:     1.780921, Tokens per Sec:    15540, Lr: 0.000300\n",
      "2021-07-25 10:40:31,843 - INFO - joeynmt.training - Epoch  11, Step:   215300, Batch Loss:     1.779212, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-07-25 10:40:45,799 - INFO - joeynmt.training - Epoch  11, Step:   215400, Batch Loss:     1.636933, Tokens per Sec:    15890, Lr: 0.000300\n",
      "2021-07-25 10:40:59,662 - INFO - joeynmt.training - Epoch  11, Step:   215500, Batch Loss:     1.747900, Tokens per Sec:    15569, Lr: 0.000300\n",
      "2021-07-25 10:41:13,696 - INFO - joeynmt.training - Epoch  11, Step:   215600, Batch Loss:     1.714884, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-07-25 10:41:27,591 - INFO - joeynmt.training - Epoch  11, Step:   215700, Batch Loss:     1.809645, Tokens per Sec:    15484, Lr: 0.000300\n",
      "2021-07-25 10:41:41,658 - INFO - joeynmt.training - Epoch  11, Step:   215800, Batch Loss:     1.801881, Tokens per Sec:    15783, Lr: 0.000300\n",
      "2021-07-25 10:41:55,291 - INFO - joeynmt.training - Epoch  11, Step:   215900, Batch Loss:     1.725680, Tokens per Sec:    15731, Lr: 0.000300\n",
      "2021-07-25 10:42:09,442 - INFO - joeynmt.training - Epoch  11, Step:   216000, Batch Loss:     1.758423, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-07-25 10:42:31,351 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 10:42:31,351 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 10:42:31,352 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 10:42:31,613 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 10:42:31,614 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 10:42:32,348 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 10:42:32,349 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 10:42:32,349 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 10:42:32,349 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
      "2021-07-25 10:42:32,349 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 10:42:32,350 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 10:42:32,350 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 10:42:32,350 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
      "2021-07-25 10:42:32,350 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 10:42:32,351 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 10:42:32,351 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 10:42:32,351 - INFO - joeynmt.training - \tHypothesis: Rather than anxiety or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 10:42:32,351 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 10:42:32,352 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 10:42:32,352 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 10:42:32,353 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 10:42:32,353 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   216000: bleu:  27.36, loss: 43613.4453, ppl:   4.8073, duration: 22.9107s\n",
      "2021-07-25 10:42:46,589 - INFO - joeynmt.training - Epoch  11, Step:   216100, Batch Loss:     1.742248, Tokens per Sec:    15745, Lr: 0.000300\n",
      "2021-07-25 10:43:00,505 - INFO - joeynmt.training - Epoch  11, Step:   216200, Batch Loss:     1.471336, Tokens per Sec:    15379, Lr: 0.000300\n",
      "2021-07-25 10:43:14,518 - INFO - joeynmt.training - Epoch  11, Step:   216300, Batch Loss:     1.760760, Tokens per Sec:    15748, Lr: 0.000300\n",
      "2021-07-25 10:43:28,546 - INFO - joeynmt.training - Epoch  11, Step:   216400, Batch Loss:     1.758751, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-07-25 10:43:42,537 - INFO - joeynmt.training - Epoch  11, Step:   216500, Batch Loss:     1.625530, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-07-25 10:43:56,507 - INFO - joeynmt.training - Epoch  11, Step:   216600, Batch Loss:     1.886526, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-07-25 10:44:10,607 - INFO - joeynmt.training - Epoch  11, Step:   216700, Batch Loss:     1.703750, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-07-25 10:44:24,373 - INFO - joeynmt.training - Epoch  11, Step:   216800, Batch Loss:     1.711884, Tokens per Sec:    15321, Lr: 0.000300\n",
      "2021-07-25 10:44:38,250 - INFO - joeynmt.training - Epoch  11, Step:   216900, Batch Loss:     1.648447, Tokens per Sec:    15798, Lr: 0.000300\n",
      "2021-07-25 10:44:52,004 - INFO - joeynmt.training - Epoch  11, Step:   217000, Batch Loss:     1.719716, Tokens per Sec:    15650, Lr: 0.000300\n",
      "2021-07-25 10:45:06,029 - INFO - joeynmt.training - Epoch  11, Step:   217100, Batch Loss:     1.687908, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-07-25 10:45:19,896 - INFO - joeynmt.training - Epoch  11, Step:   217200, Batch Loss:     1.636518, Tokens per Sec:    15311, Lr: 0.000300\n",
      "2021-07-25 10:45:33,825 - INFO - joeynmt.training - Epoch  11, Step:   217300, Batch Loss:     1.634984, Tokens per Sec:    15459, Lr: 0.000300\n",
      "2021-07-25 10:45:47,728 - INFO - joeynmt.training - Epoch  11, Step:   217400, Batch Loss:     1.737780, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-07-25 10:46:01,564 - INFO - joeynmt.training - Epoch  11, Step:   217500, Batch Loss:     1.603911, Tokens per Sec:    15642, Lr: 0.000300\n",
      "2021-07-25 10:46:15,730 - INFO - joeynmt.training - Epoch  11, Step:   217600, Batch Loss:     1.580273, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-07-25 10:46:29,713 - INFO - joeynmt.training - Epoch  11, Step:   217700, Batch Loss:     1.492514, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-07-25 10:46:43,602 - INFO - joeynmt.training - Epoch  11, Step:   217800, Batch Loss:     1.802480, Tokens per Sec:    15760, Lr: 0.000300\n",
      "2021-07-25 10:46:57,652 - INFO - joeynmt.training - Epoch  11, Step:   217900, Batch Loss:     1.752246, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-25 10:47:11,455 - INFO - joeynmt.training - Epoch  11, Step:   218000, Batch Loss:     1.824351, Tokens per Sec:    15587, Lr: 0.000300\n",
      "2021-07-25 10:47:25,510 - INFO - joeynmt.training - Epoch  11, Step:   218100, Batch Loss:     1.427754, Tokens per Sec:    15696, Lr: 0.000300\n",
      "2021-07-25 10:47:39,413 - INFO - joeynmt.training - Epoch  11, Step:   218200, Batch Loss:     1.732202, Tokens per Sec:    15619, Lr: 0.000300\n",
      "2021-07-25 10:47:53,315 - INFO - joeynmt.training - Epoch  11, Step:   218300, Batch Loss:     1.716413, Tokens per Sec:    15668, Lr: 0.000300\n",
      "2021-07-25 10:48:07,465 - INFO - joeynmt.training - Epoch  11, Step:   218400, Batch Loss:     1.639614, Tokens per Sec:    15997, Lr: 0.000300\n",
      "2021-07-25 10:48:21,474 - INFO - joeynmt.training - Epoch  11, Step:   218500, Batch Loss:     1.776000, Tokens per Sec:    15754, Lr: 0.000300\n",
      "2021-07-25 10:48:35,623 - INFO - joeynmt.training - Epoch  11, Step:   218600, Batch Loss:     1.832780, Tokens per Sec:    15624, Lr: 0.000300\n",
      "2021-07-25 10:48:49,685 - INFO - joeynmt.training - Epoch  11, Step:   218700, Batch Loss:     1.898000, Tokens per Sec:    15750, Lr: 0.000300\n",
      "2021-07-25 10:49:03,689 - INFO - joeynmt.training - Epoch  11, Step:   218800, Batch Loss:     1.751234, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-07-25 10:49:17,654 - INFO - joeynmt.training - Epoch  11, Step:   218900, Batch Loss:     1.770172, Tokens per Sec:    15499, Lr: 0.000300\n",
      "2021-07-25 10:49:31,749 - INFO - joeynmt.training - Epoch  11, Step:   219000, Batch Loss:     1.744098, Tokens per Sec:    16104, Lr: 0.000300\n",
      "2021-07-25 10:49:45,643 - INFO - joeynmt.training - Epoch  11, Step:   219100, Batch Loss:     1.635202, Tokens per Sec:    15624, Lr: 0.000300\n",
      "2021-07-25 10:49:59,758 - INFO - joeynmt.training - Epoch  11, Step:   219200, Batch Loss:     1.622490, Tokens per Sec:    15668, Lr: 0.000300\n",
      "2021-07-25 10:50:13,740 - INFO - joeynmt.training - Epoch  11, Step:   219300, Batch Loss:     2.021788, Tokens per Sec:    15412, Lr: 0.000300\n",
      "2021-07-25 10:50:27,546 - INFO - joeynmt.training - Epoch  11, Step:   219400, Batch Loss:     1.831953, Tokens per Sec:    15274, Lr: 0.000300\n",
      "2021-07-25 10:50:41,315 - INFO - joeynmt.training - Epoch  11, Step:   219500, Batch Loss:     1.650187, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-07-25 10:50:55,182 - INFO - joeynmt.training - Epoch  11, Step:   219600, Batch Loss:     1.624386, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-07-25 10:51:08,862 - INFO - joeynmt.training - Epoch  11: total training loss 9480.66\n",
      "2021-07-25 10:51:08,862 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-25 10:51:09,783 - INFO - joeynmt.training - Epoch  12, Step:   219700, Batch Loss:     1.643808, Tokens per Sec:     4729, Lr: 0.000300\n",
      "2021-07-25 10:51:23,775 - INFO - joeynmt.training - Epoch  12, Step:   219800, Batch Loss:     1.547921, Tokens per Sec:    15570, Lr: 0.000300\n",
      "2021-07-25 10:51:37,597 - INFO - joeynmt.training - Epoch  12, Step:   219900, Batch Loss:     1.687951, Tokens per Sec:    15706, Lr: 0.000300\n",
      "2021-07-25 10:51:51,455 - INFO - joeynmt.training - Epoch  12, Step:   220000, Batch Loss:     1.688891, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-25 10:52:05,373 - INFO - joeynmt.training - Epoch  12, Step:   220100, Batch Loss:     1.785778, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-25 10:52:19,528 - INFO - joeynmt.training - Epoch  12, Step:   220200, Batch Loss:     1.678682, Tokens per Sec:    15859, Lr: 0.000300\n",
      "2021-07-25 10:52:33,504 - INFO - joeynmt.training - Epoch  12, Step:   220300, Batch Loss:     1.675713, Tokens per Sec:    15734, Lr: 0.000300\n",
      "2021-07-25 10:52:47,349 - INFO - joeynmt.training - Epoch  12, Step:   220400, Batch Loss:     1.902889, Tokens per Sec:    15897, Lr: 0.000300\n",
      "2021-07-25 10:53:01,284 - INFO - joeynmt.training - Epoch  12, Step:   220500, Batch Loss:     1.646367, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-07-25 10:53:15,074 - INFO - joeynmt.training - Epoch  12, Step:   220600, Batch Loss:     1.735044, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-07-25 10:53:28,787 - INFO - joeynmt.training - Epoch  12, Step:   220700, Batch Loss:     1.806085, Tokens per Sec:    15484, Lr: 0.000300\n",
      "2021-07-25 10:53:42,746 - INFO - joeynmt.training - Epoch  12, Step:   220800, Batch Loss:     1.677759, Tokens per Sec:    15941, Lr: 0.000300\n",
      "2021-07-25 10:53:56,726 - INFO - joeynmt.training - Epoch  12, Step:   220900, Batch Loss:     1.742327, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-07-25 10:54:10,730 - INFO - joeynmt.training - Epoch  12, Step:   221000, Batch Loss:     1.575825, Tokens per Sec:    15926, Lr: 0.000300\n",
      "2021-07-25 10:54:24,574 - INFO - joeynmt.training - Epoch  12, Step:   221100, Batch Loss:     1.617317, Tokens per Sec:    15703, Lr: 0.000300\n",
      "2021-07-25 10:54:38,788 - INFO - joeynmt.training - Epoch  12, Step:   221200, Batch Loss:     1.609333, Tokens per Sec:    16127, Lr: 0.000300\n",
      "2021-07-25 10:54:52,597 - INFO - joeynmt.training - Epoch  12, Step:   221300, Batch Loss:     1.834088, Tokens per Sec:    15526, Lr: 0.000300\n",
      "2021-07-25 10:55:06,817 - INFO - joeynmt.training - Epoch  12, Step:   221400, Batch Loss:     1.822307, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-07-25 10:55:20,759 - INFO - joeynmt.training - Epoch  12, Step:   221500, Batch Loss:     1.708508, Tokens per Sec:    15649, Lr: 0.000300\n",
      "2021-07-25 10:55:34,227 - INFO - joeynmt.training - Epoch  12, Step:   221600, Batch Loss:     1.823935, Tokens per Sec:    15427, Lr: 0.000300\n",
      "2021-07-25 10:55:47,938 - INFO - joeynmt.training - Epoch  12, Step:   221700, Batch Loss:     1.650158, Tokens per Sec:    15634, Lr: 0.000300\n",
      "2021-07-25 10:56:01,941 - INFO - joeynmt.training - Epoch  12, Step:   221800, Batch Loss:     1.697039, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-07-25 10:56:15,913 - INFO - joeynmt.training - Epoch  12, Step:   221900, Batch Loss:     1.890890, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 10:56:29,794 - INFO - joeynmt.training - Epoch  12, Step:   222000, Batch Loss:     1.706617, Tokens per Sec:    15697, Lr: 0.000300\n",
      "2021-07-25 10:56:53,162 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 10:56:53,162 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 10:56:53,162 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 10:56:53,400 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 10:56:53,400 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 10:56:54,119 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 10:56:54,123 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 10:56:54,123 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 10:56:54,124 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-25 10:56:54,124 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 10:56:54,124 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 10:56:54,124 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 10:56:54,124 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
      "2021-07-25 10:56:54,125 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 10:56:54,125 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 10:56:54,125 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 10:56:54,125 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 10:56:54,126 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 10:56:54,126 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 10:56:54,126 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 10:56:54,126 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 10:56:54,127 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   222000: bleu:  27.21, loss: 43249.8242, ppl:   4.7447, duration: 24.3319s\n",
      "2021-07-25 10:57:08,312 - INFO - joeynmt.training - Epoch  12, Step:   222100, Batch Loss:     1.337499, Tokens per Sec:    15648, Lr: 0.000300\n",
      "2021-07-25 10:57:22,335 - INFO - joeynmt.training - Epoch  12, Step:   222200, Batch Loss:     1.763732, Tokens per Sec:    15633, Lr: 0.000300\n",
      "2021-07-25 10:57:36,271 - INFO - joeynmt.training - Epoch  12, Step:   222300, Batch Loss:     1.647220, Tokens per Sec:    15772, Lr: 0.000300\n",
      "2021-07-25 10:57:49,814 - INFO - joeynmt.training - Epoch  12, Step:   222400, Batch Loss:     1.630670, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-07-25 10:58:03,799 - INFO - joeynmt.training - Epoch  12, Step:   222500, Batch Loss:     1.801740, Tokens per Sec:    16209, Lr: 0.000300\n",
      "2021-07-25 10:58:17,702 - INFO - joeynmt.training - Epoch  12, Step:   222600, Batch Loss:     1.687567, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-07-25 10:58:31,852 - INFO - joeynmt.training - Epoch  12, Step:   222700, Batch Loss:     1.796721, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-07-25 10:58:45,655 - INFO - joeynmt.training - Epoch  12, Step:   222800, Batch Loss:     1.940249, Tokens per Sec:    15702, Lr: 0.000300\n",
      "2021-07-25 10:58:59,531 - INFO - joeynmt.training - Epoch  12, Step:   222900, Batch Loss:     1.763176, Tokens per Sec:    16044, Lr: 0.000300\n",
      "2021-07-25 10:59:13,246 - INFO - joeynmt.training - Epoch  12, Step:   223000, Batch Loss:     1.637189, Tokens per Sec:    15723, Lr: 0.000300\n",
      "2021-07-25 10:59:27,184 - INFO - joeynmt.training - Epoch  12, Step:   223100, Batch Loss:     1.632046, Tokens per Sec:    15816, Lr: 0.000300\n",
      "2021-07-25 10:59:40,994 - INFO - joeynmt.training - Epoch  12, Step:   223200, Batch Loss:     1.916506, Tokens per Sec:    15373, Lr: 0.000300\n",
      "2021-07-25 10:59:55,001 - INFO - joeynmt.training - Epoch  12, Step:   223300, Batch Loss:     1.948947, Tokens per Sec:    15700, Lr: 0.000300\n",
      "2021-07-25 11:00:09,216 - INFO - joeynmt.training - Epoch  12, Step:   223400, Batch Loss:     1.704097, Tokens per Sec:    16001, Lr: 0.000300\n",
      "2021-07-25 11:00:22,958 - INFO - joeynmt.training - Epoch  12, Step:   223500, Batch Loss:     1.871965, Tokens per Sec:    15716, Lr: 0.000300\n",
      "2021-07-25 11:00:36,533 - INFO - joeynmt.training - Epoch  12, Step:   223600, Batch Loss:     1.587828, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-07-25 11:00:50,478 - INFO - joeynmt.training - Epoch  12, Step:   223700, Batch Loss:     1.835313, Tokens per Sec:    15983, Lr: 0.000300\n",
      "2021-07-25 11:01:04,573 - INFO - joeynmt.training - Epoch  12, Step:   223800, Batch Loss:     1.911392, Tokens per Sec:    15561, Lr: 0.000300\n",
      "2021-07-25 11:01:18,650 - INFO - joeynmt.training - Epoch  12, Step:   223900, Batch Loss:     1.732386, Tokens per Sec:    15795, Lr: 0.000300\n",
      "2021-07-25 11:01:32,412 - INFO - joeynmt.training - Epoch  12, Step:   224000, Batch Loss:     1.774729, Tokens per Sec:    15762, Lr: 0.000300\n",
      "2021-07-25 11:01:46,058 - INFO - joeynmt.training - Epoch  12, Step:   224100, Batch Loss:     1.828258, Tokens per Sec:    15782, Lr: 0.000300\n",
      "2021-07-25 11:01:59,824 - INFO - joeynmt.training - Epoch  12, Step:   224200, Batch Loss:     1.625698, Tokens per Sec:    16004, Lr: 0.000300\n",
      "2021-07-25 11:02:13,739 - INFO - joeynmt.training - Epoch  12, Step:   224300, Batch Loss:     1.851497, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-07-25 11:02:27,931 - INFO - joeynmt.training - Epoch  12, Step:   224400, Batch Loss:     1.894462, Tokens per Sec:    15982, Lr: 0.000300\n",
      "2021-07-25 11:02:41,926 - INFO - joeynmt.training - Epoch  12, Step:   224500, Batch Loss:     1.817468, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-07-25 11:02:55,851 - INFO - joeynmt.training - Epoch  12, Step:   224600, Batch Loss:     1.640604, Tokens per Sec:    16061, Lr: 0.000300\n",
      "2021-07-25 11:03:09,569 - INFO - joeynmt.training - Epoch  12, Step:   224700, Batch Loss:     1.521316, Tokens per Sec:    15643, Lr: 0.000300\n",
      "2021-07-25 11:03:23,569 - INFO - joeynmt.training - Epoch  12, Step:   224800, Batch Loss:     1.750481, Tokens per Sec:    15683, Lr: 0.000300\n",
      "2021-07-25 11:03:37,428 - INFO - joeynmt.training - Epoch  12, Step:   224900, Batch Loss:     1.716415, Tokens per Sec:    15756, Lr: 0.000300\n",
      "2021-07-25 11:03:51,247 - INFO - joeynmt.training - Epoch  12, Step:   225000, Batch Loss:     1.826063, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-07-25 11:04:04,977 - INFO - joeynmt.training - Epoch  12, Step:   225100, Batch Loss:     1.603431, Tokens per Sec:    16006, Lr: 0.000300\n",
      "2021-07-25 11:04:08,505 - INFO - joeynmt.training - Epoch  12: total training loss 9460.42\n",
      "2021-07-25 11:04:08,506 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-25 11:04:19,477 - INFO - joeynmt.training - Epoch  13, Step:   225200, Batch Loss:     1.571465, Tokens per Sec:    14851, Lr: 0.000300\n",
      "2021-07-25 11:04:33,513 - INFO - joeynmt.training - Epoch  13, Step:   225300, Batch Loss:     1.816578, Tokens per Sec:    15492, Lr: 0.000300\n",
      "2021-07-25 11:04:47,459 - INFO - joeynmt.training - Epoch  13, Step:   225400, Batch Loss:     1.696265, Tokens per Sec:    16012, Lr: 0.000300\n",
      "2021-07-25 11:05:01,562 - INFO - joeynmt.training - Epoch  13, Step:   225500, Batch Loss:     1.735543, Tokens per Sec:    15744, Lr: 0.000300\n",
      "2021-07-25 11:05:15,663 - INFO - joeynmt.training - Epoch  13, Step:   225600, Batch Loss:     2.070437, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-25 11:05:29,513 - INFO - joeynmt.training - Epoch  13, Step:   225700, Batch Loss:     1.806206, Tokens per Sec:    15778, Lr: 0.000300\n",
      "2021-07-25 11:05:43,221 - INFO - joeynmt.training - Epoch  13, Step:   225800, Batch Loss:     1.723382, Tokens per Sec:    15859, Lr: 0.000300\n",
      "2021-07-25 11:05:57,227 - INFO - joeynmt.training - Epoch  13, Step:   225900, Batch Loss:     1.662416, Tokens per Sec:    16024, Lr: 0.000300\n",
      "2021-07-25 11:06:11,162 - INFO - joeynmt.training - Epoch  13, Step:   226000, Batch Loss:     1.576947, Tokens per Sec:    15646, Lr: 0.000300\n",
      "2021-07-25 11:06:25,074 - INFO - joeynmt.training - Epoch  13, Step:   226100, Batch Loss:     1.608131, Tokens per Sec:    15546, Lr: 0.000300\n",
      "2021-07-25 11:06:38,816 - INFO - joeynmt.training - Epoch  13, Step:   226200, Batch Loss:     1.759234, Tokens per Sec:    15664, Lr: 0.000300\n",
      "2021-07-25 11:06:52,889 - INFO - joeynmt.training - Epoch  13, Step:   226300, Batch Loss:     1.626556, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-07-25 11:07:07,015 - INFO - joeynmt.training - Epoch  13, Step:   226400, Batch Loss:     1.757180, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-25 11:07:21,116 - INFO - joeynmt.training - Epoch  13, Step:   226500, Batch Loss:     1.792109, Tokens per Sec:    15807, Lr: 0.000300\n",
      "2021-07-25 11:07:34,947 - INFO - joeynmt.training - Epoch  13, Step:   226600, Batch Loss:     1.699889, Tokens per Sec:    15638, Lr: 0.000300\n",
      "2021-07-25 11:07:48,742 - INFO - joeynmt.training - Epoch  13, Step:   226700, Batch Loss:     1.860791, Tokens per Sec:    15612, Lr: 0.000300\n",
      "2021-07-25 11:08:02,714 - INFO - joeynmt.training - Epoch  13, Step:   226800, Batch Loss:     1.612339, Tokens per Sec:    16287, Lr: 0.000300\n",
      "2021-07-25 11:08:16,839 - INFO - joeynmt.training - Epoch  13, Step:   226900, Batch Loss:     1.551504, Tokens per Sec:    15879, Lr: 0.000300\n",
      "2021-07-25 11:08:30,688 - INFO - joeynmt.training - Epoch  13, Step:   227000, Batch Loss:     1.789659, Tokens per Sec:    15606, Lr: 0.000300\n",
      "2021-07-25 11:08:44,517 - INFO - joeynmt.training - Epoch  13, Step:   227100, Batch Loss:     1.805887, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-07-25 11:08:58,139 - INFO - joeynmt.training - Epoch  13, Step:   227200, Batch Loss:     1.607788, Tokens per Sec:    15435, Lr: 0.000300\n",
      "2021-07-25 11:09:11,926 - INFO - joeynmt.training - Epoch  13, Step:   227300, Batch Loss:     1.628317, Tokens per Sec:    15873, Lr: 0.000300\n",
      "2021-07-25 11:09:25,964 - INFO - joeynmt.training - Epoch  13, Step:   227400, Batch Loss:     1.706357, Tokens per Sec:    15726, Lr: 0.000300\n",
      "2021-07-25 11:09:39,846 - INFO - joeynmt.training - Epoch  13, Step:   227500, Batch Loss:     1.607045, Tokens per Sec:    15569, Lr: 0.000300\n",
      "2021-07-25 11:09:53,776 - INFO - joeynmt.training - Epoch  13, Step:   227600, Batch Loss:     1.675723, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-07-25 11:10:07,619 - INFO - joeynmt.training - Epoch  13, Step:   227700, Batch Loss:     1.750725, Tokens per Sec:    15756, Lr: 0.000300\n",
      "2021-07-25 11:10:21,510 - INFO - joeynmt.training - Epoch  13, Step:   227800, Batch Loss:     1.864185, Tokens per Sec:    15522, Lr: 0.000300\n",
      "2021-07-25 11:10:35,362 - INFO - joeynmt.training - Epoch  13, Step:   227900, Batch Loss:     1.683660, Tokens per Sec:    15525, Lr: 0.000300\n",
      "2021-07-25 11:10:49,468 - INFO - joeynmt.training - Epoch  13, Step:   228000, Batch Loss:     1.776441, Tokens per Sec:    15614, Lr: 0.000300\n",
      "2021-07-25 11:11:12,228 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 11:11:12,229 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 11:11:12,229 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 11:11:12,500 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 11:11:12,500 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 11:11:13,289 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 11:11:13,289 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 11:11:13,290 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 11:11:13,290 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
      "2021-07-25 11:11:13,290 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 11:11:13,290 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 11:11:13,291 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 11:11:13,291 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
      "2021-07-25 11:11:13,291 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 11:11:13,291 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 11:11:13,292 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 11:11:13,292 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 11:11:13,292 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 11:11:13,292 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 11:11:13,292 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 11:11:13,293 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in some sense , that they have been well - being in Satan’s world .\n",
      "2021-07-25 11:11:13,293 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   228000: bleu:  27.25, loss: 43233.4961, ppl:   4.7420, duration: 23.8248s\n",
      "2021-07-25 11:11:27,203 - INFO - joeynmt.training - Epoch  13, Step:   228100, Batch Loss:     1.678207, Tokens per Sec:    15440, Lr: 0.000300\n",
      "2021-07-25 11:11:41,211 - INFO - joeynmt.training - Epoch  13, Step:   228200, Batch Loss:     1.566636, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-07-25 11:11:54,989 - INFO - joeynmt.training - Epoch  13, Step:   228300, Batch Loss:     1.794432, Tokens per Sec:    15648, Lr: 0.000300\n",
      "2021-07-25 11:12:09,074 - INFO - joeynmt.training - Epoch  13, Step:   228400, Batch Loss:     1.659491, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-07-25 11:12:22,973 - INFO - joeynmt.training - Epoch  13, Step:   228500, Batch Loss:     1.693853, Tokens per Sec:    15496, Lr: 0.000300\n",
      "2021-07-25 11:12:36,881 - INFO - joeynmt.training - Epoch  13, Step:   228600, Batch Loss:     1.834677, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-07-25 11:12:50,821 - INFO - joeynmt.training - Epoch  13, Step:   228700, Batch Loss:     1.812821, Tokens per Sec:    16018, Lr: 0.000300\n",
      "2021-07-25 11:13:04,733 - INFO - joeynmt.training - Epoch  13, Step:   228800, Batch Loss:     1.837521, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-07-25 11:13:18,624 - INFO - joeynmt.training - Epoch  13, Step:   228900, Batch Loss:     1.727428, Tokens per Sec:    15626, Lr: 0.000300\n",
      "2021-07-25 11:13:32,574 - INFO - joeynmt.training - Epoch  13, Step:   229000, Batch Loss:     1.661439, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-25 11:13:46,562 - INFO - joeynmt.training - Epoch  13, Step:   229100, Batch Loss:     1.639113, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-07-25 11:14:00,270 - INFO - joeynmt.training - Epoch  13, Step:   229200, Batch Loss:     1.677499, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-07-25 11:14:14,179 - INFO - joeynmt.training - Epoch  13, Step:   229300, Batch Loss:     1.548554, Tokens per Sec:    15721, Lr: 0.000300\n",
      "2021-07-25 11:14:28,066 - INFO - joeynmt.training - Epoch  13, Step:   229400, Batch Loss:     1.877426, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-07-25 11:14:42,044 - INFO - joeynmt.training - Epoch  13, Step:   229500, Batch Loss:     1.610671, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-07-25 11:14:55,924 - INFO - joeynmt.training - Epoch  13, Step:   229600, Batch Loss:     1.719263, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-07-25 11:15:09,890 - INFO - joeynmt.training - Epoch  13, Step:   229700, Batch Loss:     1.808694, Tokens per Sec:    15772, Lr: 0.000300\n",
      "2021-07-25 11:15:23,812 - INFO - joeynmt.training - Epoch  13, Step:   229800, Batch Loss:     1.796661, Tokens per Sec:    15578, Lr: 0.000300\n",
      "2021-07-25 11:15:37,688 - INFO - joeynmt.training - Epoch  13, Step:   229900, Batch Loss:     1.852351, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-25 11:15:51,856 - INFO - joeynmt.training - Epoch  13, Step:   230000, Batch Loss:     1.790991, Tokens per Sec:    16065, Lr: 0.000300\n",
      "2021-07-25 11:16:05,979 - INFO - joeynmt.training - Epoch  13, Step:   230100, Batch Loss:     1.748978, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-07-25 11:16:19,761 - INFO - joeynmt.training - Epoch  13, Step:   230200, Batch Loss:     1.787943, Tokens per Sec:    15695, Lr: 0.000300\n",
      "2021-07-25 11:16:33,511 - INFO - joeynmt.training - Epoch  13, Step:   230300, Batch Loss:     1.722283, Tokens per Sec:    15863, Lr: 0.000300\n",
      "2021-07-25 11:16:47,388 - INFO - joeynmt.training - Epoch  13, Step:   230400, Batch Loss:     1.844004, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-07-25 11:17:01,473 - INFO - joeynmt.training - Epoch  13, Step:   230500, Batch Loss:     1.575546, Tokens per Sec:    15915, Lr: 0.000300\n",
      "2021-07-25 11:17:08,232 - INFO - joeynmt.training - Epoch  13: total training loss 9431.71\n",
      "2021-07-25 11:17:08,233 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-25 11:17:16,015 - INFO - joeynmt.training - Epoch  14, Step:   230600, Batch Loss:     1.483826, Tokens per Sec:    14407, Lr: 0.000300\n",
      "2021-07-25 11:17:29,853 - INFO - joeynmt.training - Epoch  14, Step:   230700, Batch Loss:     1.828963, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-07-25 11:17:43,779 - INFO - joeynmt.training - Epoch  14, Step:   230800, Batch Loss:     1.794341, Tokens per Sec:    15648, Lr: 0.000300\n",
      "2021-07-25 11:17:57,528 - INFO - joeynmt.training - Epoch  14, Step:   230900, Batch Loss:     1.760329, Tokens per Sec:    15979, Lr: 0.000300\n",
      "2021-07-25 11:18:11,457 - INFO - joeynmt.training - Epoch  14, Step:   231000, Batch Loss:     1.805713, Tokens per Sec:    15373, Lr: 0.000300\n",
      "2021-07-25 11:18:25,418 - INFO - joeynmt.training - Epoch  14, Step:   231100, Batch Loss:     1.546211, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-07-25 11:18:39,192 - INFO - joeynmt.training - Epoch  14, Step:   231200, Batch Loss:     2.045795, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-07-25 11:18:53,062 - INFO - joeynmt.training - Epoch  14, Step:   231300, Batch Loss:     1.772649, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-07-25 11:19:06,953 - INFO - joeynmt.training - Epoch  14, Step:   231400, Batch Loss:     1.760228, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-07-25 11:19:21,005 - INFO - joeynmt.training - Epoch  14, Step:   231500, Batch Loss:     1.567764, Tokens per Sec:    15601, Lr: 0.000300\n",
      "2021-07-25 11:19:34,967 - INFO - joeynmt.training - Epoch  14, Step:   231600, Batch Loss:     1.580785, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-25 11:19:48,925 - INFO - joeynmt.training - Epoch  14, Step:   231700, Batch Loss:     1.686480, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-07-25 11:20:02,847 - INFO - joeynmt.training - Epoch  14, Step:   231800, Batch Loss:     1.855316, Tokens per Sec:    15977, Lr: 0.000300\n",
      "2021-07-25 11:20:16,729 - INFO - joeynmt.training - Epoch  14, Step:   231900, Batch Loss:     1.924572, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-07-25 11:20:30,735 - INFO - joeynmt.training - Epoch  14, Step:   232000, Batch Loss:     1.767592, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-07-25 11:20:44,743 - INFO - joeynmt.training - Epoch  14, Step:   232100, Batch Loss:     1.972438, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-07-25 11:20:58,577 - INFO - joeynmt.training - Epoch  14, Step:   232200, Batch Loss:     1.724879, Tokens per Sec:    15858, Lr: 0.000300\n",
      "2021-07-25 11:21:12,398 - INFO - joeynmt.training - Epoch  14, Step:   232300, Batch Loss:     1.707043, Tokens per Sec:    16055, Lr: 0.000300\n",
      "2021-07-25 11:21:26,396 - INFO - joeynmt.training - Epoch  14, Step:   232400, Batch Loss:     1.813694, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-25 11:21:40,432 - INFO - joeynmt.training - Epoch  14, Step:   232500, Batch Loss:     1.668191, Tokens per Sec:    15674, Lr: 0.000300\n",
      "2021-07-25 11:21:54,455 - INFO - joeynmt.training - Epoch  14, Step:   232600, Batch Loss:     1.702494, Tokens per Sec:    16112, Lr: 0.000300\n",
      "2021-07-25 11:22:08,308 - INFO - joeynmt.training - Epoch  14, Step:   232700, Batch Loss:     1.594023, Tokens per Sec:    15561, Lr: 0.000300\n",
      "2021-07-25 11:22:22,334 - INFO - joeynmt.training - Epoch  14, Step:   232800, Batch Loss:     1.689666, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-07-25 11:22:36,167 - INFO - joeynmt.training - Epoch  14, Step:   232900, Batch Loss:     1.755796, Tokens per Sec:    15672, Lr: 0.000300\n",
      "2021-07-25 11:22:49,907 - INFO - joeynmt.training - Epoch  14, Step:   233000, Batch Loss:     1.547063, Tokens per Sec:    15692, Lr: 0.000300\n",
      "2021-07-25 11:23:03,992 - INFO - joeynmt.training - Epoch  14, Step:   233100, Batch Loss:     1.885663, Tokens per Sec:    15907, Lr: 0.000300\n",
      "2021-07-25 11:23:17,906 - INFO - joeynmt.training - Epoch  14, Step:   233200, Batch Loss:     1.836026, Tokens per Sec:    15614, Lr: 0.000300\n",
      "2021-07-25 11:23:31,970 - INFO - joeynmt.training - Epoch  14, Step:   233300, Batch Loss:     1.793207, Tokens per Sec:    15886, Lr: 0.000300\n",
      "2021-07-25 11:23:45,906 - INFO - joeynmt.training - Epoch  14, Step:   233400, Batch Loss:     1.787145, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-07-25 11:23:59,790 - INFO - joeynmt.training - Epoch  14, Step:   233500, Batch Loss:     1.779117, Tokens per Sec:    15581, Lr: 0.000300\n",
      "2021-07-25 11:24:13,721 - INFO - joeynmt.training - Epoch  14, Step:   233600, Batch Loss:     1.689843, Tokens per Sec:    15687, Lr: 0.000300\n",
      "2021-07-25 11:24:27,727 - INFO - joeynmt.training - Epoch  14, Step:   233700, Batch Loss:     1.697465, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-07-25 11:24:41,575 - INFO - joeynmt.training - Epoch  14, Step:   233800, Batch Loss:     1.702368, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-25 11:24:55,048 - INFO - joeynmt.training - Epoch  14, Step:   233900, Batch Loss:     1.817125, Tokens per Sec:    15473, Lr: 0.000300\n",
      "2021-07-25 11:25:09,077 - INFO - joeynmt.training - Epoch  14, Step:   234000, Batch Loss:     1.630700, Tokens per Sec:    15981, Lr: 0.000300\n",
      "2021-07-25 11:25:32,408 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 11:25:32,408 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 11:25:32,408 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 11:25:33,304 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 11:25:33,305 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 11:25:33,305 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 11:25:33,305 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
      "2021-07-25 11:25:33,306 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 11:25:33,306 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 11:25:33,306 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 11:25:33,306 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
      "2021-07-25 11:25:33,307 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 11:25:33,307 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 11:25:33,307 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 11:25:33,307 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 11:25:33,308 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 11:25:33,308 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 11:25:33,308 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 11:25:33,308 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 11:25:33,308 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   234000: bleu:  27.25, loss: 43349.4141, ppl:   4.7618, duration: 24.2315s\n",
      "2021-07-25 11:25:47,484 - INFO - joeynmt.training - Epoch  14, Step:   234100, Batch Loss:     1.689292, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-07-25 11:26:01,177 - INFO - joeynmt.training - Epoch  14, Step:   234200, Batch Loss:     1.752407, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-07-25 11:26:14,907 - INFO - joeynmt.training - Epoch  14, Step:   234300, Batch Loss:     1.865249, Tokens per Sec:    15829, Lr: 0.000300\n",
      "2021-07-25 11:26:28,693 - INFO - joeynmt.training - Epoch  14, Step:   234400, Batch Loss:     1.861815, Tokens per Sec:    15491, Lr: 0.000300\n",
      "2021-07-25 11:26:42,535 - INFO - joeynmt.training - Epoch  14, Step:   234500, Batch Loss:     1.715426, Tokens per Sec:    15533, Lr: 0.000300\n",
      "2021-07-25 11:26:56,496 - INFO - joeynmt.training - Epoch  14, Step:   234600, Batch Loss:     1.790383, Tokens per Sec:    15684, Lr: 0.000300\n",
      "2021-07-25 11:27:10,423 - INFO - joeynmt.training - Epoch  14, Step:   234700, Batch Loss:     1.766588, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-25 11:27:24,501 - INFO - joeynmt.training - Epoch  14, Step:   234800, Batch Loss:     1.834467, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-07-25 11:27:38,258 - INFO - joeynmt.training - Epoch  14, Step:   234900, Batch Loss:     1.651057, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-07-25 11:27:52,256 - INFO - joeynmt.training - Epoch  14, Step:   235000, Batch Loss:     1.838253, Tokens per Sec:    16099, Lr: 0.000300\n",
      "2021-07-25 11:28:06,337 - INFO - joeynmt.training - Epoch  14, Step:   235100, Batch Loss:     2.076868, Tokens per Sec:    15786, Lr: 0.000300\n",
      "2021-07-25 11:28:20,345 - INFO - joeynmt.training - Epoch  14, Step:   235200, Batch Loss:     1.783922, Tokens per Sec:    15806, Lr: 0.000300\n",
      "2021-07-25 11:28:34,079 - INFO - joeynmt.training - Epoch  14, Step:   235300, Batch Loss:     1.727574, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-25 11:28:48,019 - INFO - joeynmt.training - Epoch  14, Step:   235400, Batch Loss:     1.435529, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-07-25 11:29:01,872 - INFO - joeynmt.training - Epoch  14, Step:   235500, Batch Loss:     1.713789, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-07-25 11:29:15,670 - INFO - joeynmt.training - Epoch  14, Step:   235600, Batch Loss:     1.851465, Tokens per Sec:    15475, Lr: 0.000300\n",
      "2021-07-25 11:29:29,625 - INFO - joeynmt.training - Epoch  14, Step:   235700, Batch Loss:     1.815161, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-07-25 11:29:43,429 - INFO - joeynmt.training - Epoch  14, Step:   235800, Batch Loss:     1.780998, Tokens per Sec:    15950, Lr: 0.000300\n",
      "2021-07-25 11:29:57,244 - INFO - joeynmt.training - Epoch  14, Step:   235900, Batch Loss:     1.678990, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-07-25 11:30:06,959 - INFO - joeynmt.training - Epoch  14: total training loss 9411.82\n",
      "2021-07-25 11:30:06,960 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-25 11:30:11,474 - INFO - joeynmt.training - Epoch  15, Step:   236000, Batch Loss:     1.606088, Tokens per Sec:    13343, Lr: 0.000300\n",
      "2021-07-25 11:30:25,509 - INFO - joeynmt.training - Epoch  15, Step:   236100, Batch Loss:     1.698449, Tokens per Sec:    16090, Lr: 0.000300\n",
      "2021-07-25 11:30:39,357 - INFO - joeynmt.training - Epoch  15, Step:   236200, Batch Loss:     1.777565, Tokens per Sec:    15616, Lr: 0.000300\n",
      "2021-07-25 11:30:53,218 - INFO - joeynmt.training - Epoch  15, Step:   236300, Batch Loss:     1.627051, Tokens per Sec:    15814, Lr: 0.000300\n",
      "2021-07-25 11:31:07,187 - INFO - joeynmt.training - Epoch  15, Step:   236400, Batch Loss:     1.904664, Tokens per Sec:    15876, Lr: 0.000300\n",
      "2021-07-25 11:31:20,999 - INFO - joeynmt.training - Epoch  15, Step:   236500, Batch Loss:     1.731720, Tokens per Sec:    15469, Lr: 0.000300\n",
      "2021-07-25 11:31:35,029 - INFO - joeynmt.training - Epoch  15, Step:   236600, Batch Loss:     1.817431, Tokens per Sec:    15673, Lr: 0.000300\n",
      "2021-07-25 11:31:48,944 - INFO - joeynmt.training - Epoch  15, Step:   236700, Batch Loss:     1.761782, Tokens per Sec:    16062, Lr: 0.000300\n",
      "2021-07-25 11:32:02,791 - INFO - joeynmt.training - Epoch  15, Step:   236800, Batch Loss:     1.697207, Tokens per Sec:    15937, Lr: 0.000300\n",
      "2021-07-25 11:32:16,647 - INFO - joeynmt.training - Epoch  15, Step:   236900, Batch Loss:     1.813757, Tokens per Sec:    15693, Lr: 0.000300\n",
      "2021-07-25 11:32:30,623 - INFO - joeynmt.training - Epoch  15, Step:   237000, Batch Loss:     1.778668, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-07-25 11:32:44,673 - INFO - joeynmt.training - Epoch  15, Step:   237100, Batch Loss:     1.508271, Tokens per Sec:    15686, Lr: 0.000300\n",
      "2021-07-25 11:32:58,660 - INFO - joeynmt.training - Epoch  15, Step:   237200, Batch Loss:     1.791680, Tokens per Sec:    15677, Lr: 0.000300\n",
      "2021-07-25 11:33:12,678 - INFO - joeynmt.training - Epoch  15, Step:   237300, Batch Loss:     1.770134, Tokens per Sec:    15875, Lr: 0.000300\n",
      "2021-07-25 11:33:26,422 - INFO - joeynmt.training - Epoch  15, Step:   237400, Batch Loss:     1.725472, Tokens per Sec:    15697, Lr: 0.000300\n",
      "2021-07-25 11:33:40,221 - INFO - joeynmt.training - Epoch  15, Step:   237500, Batch Loss:     1.893115, Tokens per Sec:    15850, Lr: 0.000300\n",
      "2021-07-25 11:33:53,993 - INFO - joeynmt.training - Epoch  15, Step:   237600, Batch Loss:     1.714284, Tokens per Sec:    15718, Lr: 0.000300\n",
      "2021-07-25 11:34:07,916 - INFO - joeynmt.training - Epoch  15, Step:   237700, Batch Loss:     1.518702, Tokens per Sec:    15665, Lr: 0.000300\n",
      "2021-07-25 11:34:21,862 - INFO - joeynmt.training - Epoch  15, Step:   237800, Batch Loss:     1.799522, Tokens per Sec:    15810, Lr: 0.000300\n",
      "2021-07-25 11:34:36,057 - INFO - joeynmt.training - Epoch  15, Step:   237900, Batch Loss:     1.875018, Tokens per Sec:    16264, Lr: 0.000300\n",
      "2021-07-25 11:34:49,964 - INFO - joeynmt.training - Epoch  15, Step:   238000, Batch Loss:     1.654258, Tokens per Sec:    15690, Lr: 0.000300\n",
      "2021-07-25 11:35:03,632 - INFO - joeynmt.training - Epoch  15, Step:   238100, Batch Loss:     1.744291, Tokens per Sec:    15614, Lr: 0.000300\n",
      "2021-07-25 11:35:17,645 - INFO - joeynmt.training - Epoch  15, Step:   238200, Batch Loss:     1.840984, Tokens per Sec:    15798, Lr: 0.000300\n",
      "2021-07-25 11:35:31,657 - INFO - joeynmt.training - Epoch  15, Step:   238300, Batch Loss:     1.785264, Tokens per Sec:    15723, Lr: 0.000300\n",
      "2021-07-25 11:35:45,534 - INFO - joeynmt.training - Epoch  15, Step:   238400, Batch Loss:     1.717230, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-07-25 11:35:59,144 - INFO - joeynmt.training - Epoch  15, Step:   238500, Batch Loss:     1.761671, Tokens per Sec:    15880, Lr: 0.000300\n",
      "2021-07-25 11:36:12,817 - INFO - joeynmt.training - Epoch  15, Step:   238600, Batch Loss:     1.861202, Tokens per Sec:    15282, Lr: 0.000300\n",
      "2021-07-25 11:36:26,732 - INFO - joeynmt.training - Epoch  15, Step:   238700, Batch Loss:     1.903704, Tokens per Sec:    15481, Lr: 0.000300\n",
      "2021-07-25 11:36:40,778 - INFO - joeynmt.training - Epoch  15, Step:   238800, Batch Loss:     1.722992, Tokens per Sec:    15937, Lr: 0.000300\n",
      "2021-07-25 11:36:54,597 - INFO - joeynmt.training - Epoch  15, Step:   238900, Batch Loss:     1.808538, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-25 11:37:08,586 - INFO - joeynmt.training - Epoch  15, Step:   239000, Batch Loss:     1.837897, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-07-25 11:37:22,676 - INFO - joeynmt.training - Epoch  15, Step:   239100, Batch Loss:     1.622531, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-07-25 11:37:36,954 - INFO - joeynmt.training - Epoch  15, Step:   239200, Batch Loss:     1.873767, Tokens per Sec:    16033, Lr: 0.000300\n",
      "2021-07-25 11:37:50,746 - INFO - joeynmt.training - Epoch  15, Step:   239300, Batch Loss:     1.670591, Tokens per Sec:    15602, Lr: 0.000300\n",
      "2021-07-25 11:38:04,730 - INFO - joeynmt.training - Epoch  15, Step:   239400, Batch Loss:     1.789274, Tokens per Sec:    15766, Lr: 0.000300\n",
      "2021-07-25 11:38:18,646 - INFO - joeynmt.training - Epoch  15, Step:   239500, Batch Loss:     1.677759, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-07-25 11:38:32,393 - INFO - joeynmt.training - Epoch  15, Step:   239600, Batch Loss:     1.714982, Tokens per Sec:    15746, Lr: 0.000300\n",
      "2021-07-25 11:38:46,124 - INFO - joeynmt.training - Epoch  15, Step:   239700, Batch Loss:     1.746326, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-07-25 11:38:59,960 - INFO - joeynmt.training - Epoch  15, Step:   239800, Batch Loss:     1.578771, Tokens per Sec:    15580, Lr: 0.000300\n",
      "2021-07-25 11:39:14,073 - INFO - joeynmt.training - Epoch  15, Step:   239900, Batch Loss:     1.744539, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-07-25 11:39:28,016 - INFO - joeynmt.training - Epoch  15, Step:   240000, Batch Loss:     1.785325, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-07-25 11:39:50,872 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 11:39:50,872 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 11:39:50,873 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 11:39:51,108 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 11:39:51,108 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 11:39:51,916 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 11:39:51,916 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 11:39:51,917 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 11:39:51,917 - INFO - joeynmt.training - \tHypothesis: I touched my heart .\n",
      "2021-07-25 11:39:51,917 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 11:39:51,917 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 11:39:51,918 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 11:39:51,918 - INFO - joeynmt.training - \tHypothesis: The text writes on the outside of the scroll .\n",
      "2021-07-25 11:39:51,918 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 11:39:51,919 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 11:39:51,919 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 11:39:51,919 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 11:39:51,919 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 11:39:51,920 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 11:39:51,920 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 11:39:51,920 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of kindness in Satan’s world .\n",
      "2021-07-25 11:39:51,920 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   240000: bleu:  27.20, loss: 42929.1055, ppl:   4.6903, duration: 23.9037s\n",
      "2021-07-25 11:40:06,130 - INFO - joeynmt.training - Epoch  15, Step:   240100, Batch Loss:     1.525178, Tokens per Sec:    15730, Lr: 0.000300\n",
      "2021-07-25 11:40:19,946 - INFO - joeynmt.training - Epoch  15, Step:   240200, Batch Loss:     1.866370, Tokens per Sec:    15526, Lr: 0.000300\n",
      "2021-07-25 11:40:33,925 - INFO - joeynmt.training - Epoch  15, Step:   240300, Batch Loss:     1.651742, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-25 11:40:47,850 - INFO - joeynmt.training - Epoch  15, Step:   240400, Batch Loss:     1.723983, Tokens per Sec:    16153, Lr: 0.000300\n",
      "2021-07-25 11:41:01,717 - INFO - joeynmt.training - Epoch  15, Step:   240500, Batch Loss:     1.658042, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-07-25 11:41:15,475 - INFO - joeynmt.training - Epoch  15, Step:   240600, Batch Loss:     1.892487, Tokens per Sec:    15762, Lr: 0.000300\n",
      "2021-07-25 11:41:29,466 - INFO - joeynmt.training - Epoch  15, Step:   240700, Batch Loss:     1.605461, Tokens per Sec:    15451, Lr: 0.000300\n",
      "2021-07-25 11:41:43,414 - INFO - joeynmt.training - Epoch  15, Step:   240800, Batch Loss:     1.671973, Tokens per Sec:    16100, Lr: 0.000300\n",
      "2021-07-25 11:41:56,959 - INFO - joeynmt.training - Epoch  15, Step:   240900, Batch Loss:     1.632410, Tokens per Sec:    15731, Lr: 0.000300\n",
      "2021-07-25 11:42:10,874 - INFO - joeynmt.training - Epoch  15, Step:   241000, Batch Loss:     1.893769, Tokens per Sec:    15874, Lr: 0.000300\n",
      "2021-07-25 11:42:24,738 - INFO - joeynmt.training - Epoch  15, Step:   241100, Batch Loss:     1.643158, Tokens per Sec:    15633, Lr: 0.000300\n",
      "2021-07-25 11:42:38,652 - INFO - joeynmt.training - Epoch  15, Step:   241200, Batch Loss:     1.472224, Tokens per Sec:    15733, Lr: 0.000300\n",
      "2021-07-25 11:42:52,622 - INFO - joeynmt.training - Epoch  15, Step:   241300, Batch Loss:     1.649236, Tokens per Sec:    16039, Lr: 0.000300\n",
      "2021-07-25 11:43:05,979 - INFO - joeynmt.training - Epoch  15: total training loss 9383.95\n",
      "2021-07-25 11:43:05,979 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-25 11:43:07,276 - INFO - joeynmt.training - Epoch  16, Step:   241400, Batch Loss:     1.784234, Tokens per Sec:     7617, Lr: 0.000300\n",
      "2021-07-25 11:43:21,079 - INFO - joeynmt.training - Epoch  16, Step:   241500, Batch Loss:     1.791020, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-07-25 11:43:35,051 - INFO - joeynmt.training - Epoch  16, Step:   241600, Batch Loss:     1.639761, Tokens per Sec:    16097, Lr: 0.000300\n",
      "2021-07-25 11:43:48,823 - INFO - joeynmt.training - Epoch  16, Step:   241700, Batch Loss:     1.794815, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-07-25 11:44:02,678 - INFO - joeynmt.training - Epoch  16, Step:   241800, Batch Loss:     1.578130, Tokens per Sec:    15490, Lr: 0.000300\n",
      "2021-07-25 11:44:16,461 - INFO - joeynmt.training - Epoch  16, Step:   241900, Batch Loss:     1.596286, Tokens per Sec:    15662, Lr: 0.000300\n",
      "2021-07-25 11:44:30,255 - INFO - joeynmt.training - Epoch  16, Step:   242000, Batch Loss:     1.513083, Tokens per Sec:    15748, Lr: 0.000300\n",
      "2021-07-25 11:44:44,222 - INFO - joeynmt.training - Epoch  16, Step:   242100, Batch Loss:     1.785061, Tokens per Sec:    16271, Lr: 0.000300\n",
      "2021-07-25 11:44:57,953 - INFO - joeynmt.training - Epoch  16, Step:   242200, Batch Loss:     1.853241, Tokens per Sec:    15757, Lr: 0.000300\n",
      "2021-07-25 11:45:12,097 - INFO - joeynmt.training - Epoch  16, Step:   242300, Batch Loss:     2.025474, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-07-25 11:45:26,244 - INFO - joeynmt.training - Epoch  16, Step:   242400, Batch Loss:     1.772097, Tokens per Sec:    15965, Lr: 0.000300\n",
      "2021-07-25 11:45:40,006 - INFO - joeynmt.training - Epoch  16, Step:   242500, Batch Loss:     1.774036, Tokens per Sec:    15753, Lr: 0.000300\n",
      "2021-07-25 11:45:53,774 - INFO - joeynmt.training - Epoch  16, Step:   242600, Batch Loss:     1.773140, Tokens per Sec:    15938, Lr: 0.000300\n",
      "2021-07-25 11:46:07,565 - INFO - joeynmt.training - Epoch  16, Step:   242700, Batch Loss:     1.704638, Tokens per Sec:    15843, Lr: 0.000300\n",
      "2021-07-25 11:46:21,704 - INFO - joeynmt.training - Epoch  16, Step:   242800, Batch Loss:     1.689451, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-07-25 11:46:35,348 - INFO - joeynmt.training - Epoch  16, Step:   242900, Batch Loss:     2.003846, Tokens per Sec:    15614, Lr: 0.000300\n",
      "2021-07-25 11:46:49,054 - INFO - joeynmt.training - Epoch  16, Step:   243000, Batch Loss:     1.793556, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-07-25 11:47:03,020 - INFO - joeynmt.training - Epoch  16, Step:   243100, Batch Loss:     1.607311, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-07-25 11:47:17,009 - INFO - joeynmt.training - Epoch  16, Step:   243200, Batch Loss:     1.695941, Tokens per Sec:    15886, Lr: 0.000300\n",
      "2021-07-25 11:47:31,058 - INFO - joeynmt.training - Epoch  16, Step:   243300, Batch Loss:     1.993862, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-07-25 11:47:44,818 - INFO - joeynmt.training - Epoch  16, Step:   243400, Batch Loss:     1.837944, Tokens per Sec:    15787, Lr: 0.000300\n",
      "2021-07-25 11:47:58,819 - INFO - joeynmt.training - Epoch  16, Step:   243500, Batch Loss:     2.222364, Tokens per Sec:    16248, Lr: 0.000300\n",
      "2021-07-25 11:48:12,546 - INFO - joeynmt.training - Epoch  16, Step:   243600, Batch Loss:     1.658290, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-25 11:48:26,561 - INFO - joeynmt.training - Epoch  16, Step:   243700, Batch Loss:     1.671014, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-07-25 11:48:40,617 - INFO - joeynmt.training - Epoch  16, Step:   243800, Batch Loss:     1.639136, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 11:48:54,688 - INFO - joeynmt.training - Epoch  16, Step:   243900, Batch Loss:     1.703537, Tokens per Sec:    16114, Lr: 0.000300\n",
      "2021-07-25 11:49:08,558 - INFO - joeynmt.training - Epoch  16, Step:   244000, Batch Loss:     1.826203, Tokens per Sec:    15630, Lr: 0.000300\n",
      "2021-07-25 11:49:22,471 - INFO - joeynmt.training - Epoch  16, Step:   244100, Batch Loss:     1.696900, Tokens per Sec:    15882, Lr: 0.000300\n",
      "2021-07-25 11:49:36,418 - INFO - joeynmt.training - Epoch  16, Step:   244200, Batch Loss:     1.598904, Tokens per Sec:    16069, Lr: 0.000300\n",
      "2021-07-25 11:49:50,186 - INFO - joeynmt.training - Epoch  16, Step:   244300, Batch Loss:     1.549760, Tokens per Sec:    16010, Lr: 0.000300\n",
      "2021-07-25 11:50:04,086 - INFO - joeynmt.training - Epoch  16, Step:   244400, Batch Loss:     1.430214, Tokens per Sec:    15615, Lr: 0.000300\n",
      "2021-07-25 11:50:17,891 - INFO - joeynmt.training - Epoch  16, Step:   244500, Batch Loss:     1.862669, Tokens per Sec:    15553, Lr: 0.000300\n",
      "2021-07-25 11:50:31,704 - INFO - joeynmt.training - Epoch  16, Step:   244600, Batch Loss:     1.714754, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-07-25 11:50:45,591 - INFO - joeynmt.training - Epoch  16, Step:   244700, Batch Loss:     1.479223, Tokens per Sec:    16114, Lr: 0.000300\n",
      "2021-07-25 11:50:59,456 - INFO - joeynmt.training - Epoch  16, Step:   244800, Batch Loss:     1.457472, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-07-25 11:51:13,498 - INFO - joeynmt.training - Epoch  16, Step:   244900, Batch Loss:     1.636864, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-07-25 11:51:27,435 - INFO - joeynmt.training - Epoch  16, Step:   245000, Batch Loss:     1.881003, Tokens per Sec:    15706, Lr: 0.000300\n",
      "2021-07-25 11:51:41,310 - INFO - joeynmt.training - Epoch  16, Step:   245100, Batch Loss:     1.711390, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-25 11:51:54,806 - INFO - joeynmt.training - Epoch  16, Step:   245200, Batch Loss:     1.556044, Tokens per Sec:    15269, Lr: 0.000300\n",
      "2021-07-25 11:52:08,587 - INFO - joeynmt.training - Epoch  16, Step:   245300, Batch Loss:     1.963168, Tokens per Sec:    15664, Lr: 0.000300\n",
      "2021-07-25 11:52:22,488 - INFO - joeynmt.training - Epoch  16, Step:   245400, Batch Loss:     1.889966, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-07-25 11:52:36,345 - INFO - joeynmt.training - Epoch  16, Step:   245500, Batch Loss:     2.178308, Tokens per Sec:    15721, Lr: 0.000300\n",
      "2021-07-25 11:52:50,220 - INFO - joeynmt.training - Epoch  16, Step:   245600, Batch Loss:     1.737370, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-07-25 11:53:04,106 - INFO - joeynmt.training - Epoch  16, Step:   245700, Batch Loss:     1.708828, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-07-25 11:53:17,965 - INFO - joeynmt.training - Epoch  16, Step:   245800, Batch Loss:     1.710328, Tokens per Sec:    15370, Lr: 0.000300\n",
      "2021-07-25 11:53:31,900 - INFO - joeynmt.training - Epoch  16, Step:   245900, Batch Loss:     1.768973, Tokens per Sec:    15795, Lr: 0.000300\n",
      "2021-07-25 11:53:45,824 - INFO - joeynmt.training - Epoch  16, Step:   246000, Batch Loss:     1.749851, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-07-25 11:54:07,031 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 11:54:07,031 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 11:54:07,031 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 11:54:07,301 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 11:54:07,301 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 11:54:08,038 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 11:54:08,038 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 11:54:08,039 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 11:54:08,039 - INFO - joeynmt.training - \tHypothesis: I was moved by my heart .\n",
      "2021-07-25 11:54:08,039 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 11:54:08,039 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 11:54:08,040 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 11:54:08,040 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
      "2021-07-25 11:54:08,040 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 11:54:08,040 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 11:54:08,041 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 11:54:08,041 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 11:54:08,041 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 11:54:08,042 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 11:54:08,042 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 11:54:08,043 - INFO - joeynmt.training - \tHypothesis: Sadly , some in the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 11:54:08,043 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   246000: bleu:  27.29, loss: 42911.6094, ppl:   4.6873, duration: 22.2185s\n",
      "2021-07-25 11:54:22,272 - INFO - joeynmt.training - Epoch  16, Step:   246100, Batch Loss:     1.723752, Tokens per Sec:    15227, Lr: 0.000300\n",
      "2021-07-25 11:54:36,318 - INFO - joeynmt.training - Epoch  16, Step:   246200, Batch Loss:     1.801201, Tokens per Sec:    16239, Lr: 0.000300\n",
      "2021-07-25 11:54:50,144 - INFO - joeynmt.training - Epoch  16, Step:   246300, Batch Loss:     1.841336, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-07-25 11:55:04,117 - INFO - joeynmt.training - Epoch  16, Step:   246400, Batch Loss:     1.720452, Tokens per Sec:    15563, Lr: 0.000300\n",
      "2021-07-25 11:55:17,944 - INFO - joeynmt.training - Epoch  16, Step:   246500, Batch Loss:     1.742649, Tokens per Sec:    15647, Lr: 0.000300\n",
      "2021-07-25 11:55:32,022 - INFO - joeynmt.training - Epoch  16, Step:   246600, Batch Loss:     1.599729, Tokens per Sec:    16146, Lr: 0.000300\n",
      "2021-07-25 11:55:45,681 - INFO - joeynmt.training - Epoch  16, Step:   246700, Batch Loss:     1.893882, Tokens per Sec:    15691, Lr: 0.000300\n",
      "2021-07-25 11:55:59,267 - INFO - joeynmt.training - Epoch  16, Step:   246800, Batch Loss:     1.687339, Tokens per Sec:    15743, Lr: 0.000300\n",
      "2021-07-25 11:56:02,239 - INFO - joeynmt.training - Epoch  16: total training loss 9376.76\n",
      "2021-07-25 11:56:02,239 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-25 11:56:13,982 - INFO - joeynmt.training - Epoch  17, Step:   246900, Batch Loss:     1.594709, Tokens per Sec:    15097, Lr: 0.000300\n",
      "2021-07-25 11:56:27,828 - INFO - joeynmt.training - Epoch  17, Step:   247000, Batch Loss:     1.751060, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-07-25 11:56:41,529 - INFO - joeynmt.training - Epoch  17, Step:   247100, Batch Loss:     1.760588, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-07-25 11:56:55,407 - INFO - joeynmt.training - Epoch  17, Step:   247200, Batch Loss:     1.937939, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-25 11:57:09,166 - INFO - joeynmt.training - Epoch  17, Step:   247300, Batch Loss:     1.405204, Tokens per Sec:    15502, Lr: 0.000300\n",
      "2021-07-25 11:57:23,234 - INFO - joeynmt.training - Epoch  17, Step:   247400, Batch Loss:     1.811359, Tokens per Sec:    16002, Lr: 0.000300\n",
      "2021-07-25 11:57:37,168 - INFO - joeynmt.training - Epoch  17, Step:   247500, Batch Loss:     1.715008, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-07-25 11:57:51,007 - INFO - joeynmt.training - Epoch  17, Step:   247600, Batch Loss:     1.569947, Tokens per Sec:    15955, Lr: 0.000300\n",
      "2021-07-25 11:58:04,820 - INFO - joeynmt.training - Epoch  17, Step:   247700, Batch Loss:     1.767505, Tokens per Sec:    15851, Lr: 0.000300\n",
      "2021-07-25 11:58:18,906 - INFO - joeynmt.training - Epoch  17, Step:   247800, Batch Loss:     1.713242, Tokens per Sec:    16156, Lr: 0.000300\n",
      "2021-07-25 11:58:32,828 - INFO - joeynmt.training - Epoch  17, Step:   247900, Batch Loss:     1.619899, Tokens per Sec:    15527, Lr: 0.000300\n",
      "2021-07-25 11:58:46,636 - INFO - joeynmt.training - Epoch  17, Step:   248000, Batch Loss:     2.064603, Tokens per Sec:    15951, Lr: 0.000300\n",
      "2021-07-25 11:59:00,530 - INFO - joeynmt.training - Epoch  17, Step:   248100, Batch Loss:     1.778510, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-07-25 11:59:14,456 - INFO - joeynmt.training - Epoch  17, Step:   248200, Batch Loss:     1.531720, Tokens per Sec:    16009, Lr: 0.000300\n",
      "2021-07-25 11:59:28,448 - INFO - joeynmt.training - Epoch  17, Step:   248300, Batch Loss:     1.736480, Tokens per Sec:    15688, Lr: 0.000300\n",
      "2021-07-25 11:59:42,389 - INFO - joeynmt.training - Epoch  17, Step:   248400, Batch Loss:     1.610155, Tokens per Sec:    15673, Lr: 0.000300\n",
      "2021-07-25 11:59:56,201 - INFO - joeynmt.training - Epoch  17, Step:   248500, Batch Loss:     1.742893, Tokens per Sec:    15594, Lr: 0.000300\n",
      "2021-07-25 12:00:10,131 - INFO - joeynmt.training - Epoch  17, Step:   248600, Batch Loss:     1.811231, Tokens per Sec:    15768, Lr: 0.000300\n",
      "2021-07-25 12:00:23,948 - INFO - joeynmt.training - Epoch  17, Step:   248700, Batch Loss:     1.727024, Tokens per Sec:    15651, Lr: 0.000300\n",
      "2021-07-25 12:00:37,620 - INFO - joeynmt.training - Epoch  17, Step:   248800, Batch Loss:     1.662388, Tokens per Sec:    15798, Lr: 0.000300\n",
      "2021-07-25 12:00:51,562 - INFO - joeynmt.training - Epoch  17, Step:   248900, Batch Loss:     1.689009, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-25 12:01:05,614 - INFO - joeynmt.training - Epoch  17, Step:   249000, Batch Loss:     1.710710, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-07-25 12:01:19,503 - INFO - joeynmt.training - Epoch  17, Step:   249100, Batch Loss:     1.614215, Tokens per Sec:    15895, Lr: 0.000300\n",
      "2021-07-25 12:01:33,217 - INFO - joeynmt.training - Epoch  17, Step:   249200, Batch Loss:     2.058411, Tokens per Sec:    16087, Lr: 0.000300\n",
      "2021-07-25 12:01:47,036 - INFO - joeynmt.training - Epoch  17, Step:   249300, Batch Loss:     1.558304, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 12:02:00,895 - INFO - joeynmt.training - Epoch  17, Step:   249400, Batch Loss:     1.786041, Tokens per Sec:    15570, Lr: 0.000300\n",
      "2021-07-25 12:02:14,869 - INFO - joeynmt.training - Epoch  17, Step:   249500, Batch Loss:     1.829684, Tokens per Sec:    16102, Lr: 0.000300\n",
      "2021-07-25 12:02:28,705 - INFO - joeynmt.training - Epoch  17, Step:   249600, Batch Loss:     1.575364, Tokens per Sec:    15429, Lr: 0.000300\n",
      "2021-07-25 12:02:42,638 - INFO - joeynmt.training - Epoch  17, Step:   249700, Batch Loss:     1.852088, Tokens per Sec:    16353, Lr: 0.000300\n",
      "2021-07-25 12:02:56,532 - INFO - joeynmt.training - Epoch  17, Step:   249800, Batch Loss:     1.650549, Tokens per Sec:    16067, Lr: 0.000300\n",
      "2021-07-25 12:03:10,413 - INFO - joeynmt.training - Epoch  17, Step:   249900, Batch Loss:     1.797794, Tokens per Sec:    15850, Lr: 0.000300\n",
      "2021-07-25 12:03:24,384 - INFO - joeynmt.training - Epoch  17, Step:   250000, Batch Loss:     1.639513, Tokens per Sec:    15760, Lr: 0.000300\n",
      "2021-07-25 12:03:38,432 - INFO - joeynmt.training - Epoch  17, Step:   250100, Batch Loss:     1.638764, Tokens per Sec:    15811, Lr: 0.000300\n",
      "2021-07-25 12:03:52,124 - INFO - joeynmt.training - Epoch  17, Step:   250200, Batch Loss:     1.665105, Tokens per Sec:    15860, Lr: 0.000300\n",
      "2021-07-25 12:04:05,790 - INFO - joeynmt.training - Epoch  17, Step:   250300, Batch Loss:     1.838141, Tokens per Sec:    15665, Lr: 0.000300\n",
      "2021-07-25 12:04:19,861 - INFO - joeynmt.training - Epoch  17, Step:   250400, Batch Loss:     1.777671, Tokens per Sec:    15749, Lr: 0.000300\n",
      "2021-07-25 12:04:33,761 - INFO - joeynmt.training - Epoch  17, Step:   250500, Batch Loss:     1.658908, Tokens per Sec:    15650, Lr: 0.000300\n",
      "2021-07-25 12:04:47,699 - INFO - joeynmt.training - Epoch  17, Step:   250600, Batch Loss:     1.848180, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-07-25 12:05:01,577 - INFO - joeynmt.training - Epoch  17, Step:   250700, Batch Loss:     1.570135, Tokens per Sec:    15578, Lr: 0.000300\n",
      "2021-07-25 12:05:15,415 - INFO - joeynmt.training - Epoch  17, Step:   250800, Batch Loss:     1.721349, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-07-25 12:05:29,415 - INFO - joeynmt.training - Epoch  17, Step:   250900, Batch Loss:     1.630355, Tokens per Sec:    15855, Lr: 0.000300\n",
      "2021-07-25 12:05:43,169 - INFO - joeynmt.training - Epoch  17, Step:   251000, Batch Loss:     1.571131, Tokens per Sec:    15971, Lr: 0.000300\n",
      "2021-07-25 12:05:57,174 - INFO - joeynmt.training - Epoch  17, Step:   251100, Batch Loss:     1.563615, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-07-25 12:06:11,221 - INFO - joeynmt.training - Epoch  17, Step:   251200, Batch Loss:     1.906342, Tokens per Sec:    15658, Lr: 0.000300\n",
      "2021-07-25 12:06:25,070 - INFO - joeynmt.training - Epoch  17, Step:   251300, Batch Loss:     1.689691, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-07-25 12:06:38,975 - INFO - joeynmt.training - Epoch  17, Step:   251400, Batch Loss:     1.858586, Tokens per Sec:    15927, Lr: 0.000300\n",
      "2021-07-25 12:06:52,622 - INFO - joeynmt.training - Epoch  17, Step:   251500, Batch Loss:     1.647932, Tokens per Sec:    15713, Lr: 0.000300\n",
      "2021-07-25 12:07:06,609 - INFO - joeynmt.training - Epoch  17, Step:   251600, Batch Loss:     1.854799, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-07-25 12:07:20,605 - INFO - joeynmt.training - Epoch  17, Step:   251700, Batch Loss:     1.675896, Tokens per Sec:    15782, Lr: 0.000300\n",
      "2021-07-25 12:07:34,567 - INFO - joeynmt.training - Epoch  17, Step:   251800, Batch Loss:     1.694235, Tokens per Sec:    16051, Lr: 0.000300\n",
      "2021-07-25 12:07:48,500 - INFO - joeynmt.training - Epoch  17, Step:   251900, Batch Loss:     1.799133, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-25 12:08:02,447 - INFO - joeynmt.training - Epoch  17, Step:   252000, Batch Loss:     1.803973, Tokens per Sec:    15908, Lr: 0.000300\n",
      "2021-07-25 12:08:24,470 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 12:08:24,470 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 12:08:24,470 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 12:08:24,732 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 12:08:24,733 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 12:08:25,802 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 12:08:25,802 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 12:08:25,803 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 12:08:25,803 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
      "2021-07-25 12:08:25,803 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 12:08:25,804 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 12:08:25,804 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 12:08:25,804 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the outside of the scroll .\n",
      "2021-07-25 12:08:25,804 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 12:08:25,804 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 12:08:25,805 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 12:08:25,805 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 12:08:25,805 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 12:08:25,805 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 12:08:25,805 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 12:08:25,806 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in a sense , that they have been successful in Satan’s world .\n",
      "2021-07-25 12:08:25,806 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   252000: bleu:  27.24, loss: 42891.4102, ppl:   4.6839, duration: 23.3579s\n",
      "2021-07-25 12:08:39,430 - INFO - joeynmt.training - Epoch  17, Step:   252100, Batch Loss:     1.638988, Tokens per Sec:    15385, Lr: 0.000300\n",
      "2021-07-25 12:08:53,398 - INFO - joeynmt.training - Epoch  17, Step:   252200, Batch Loss:     1.840891, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-07-25 12:08:59,003 - INFO - joeynmt.training - Epoch  17: total training loss 9344.39\n",
      "2021-07-25 12:08:59,003 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-25 12:09:07,791 - INFO - joeynmt.training - Epoch  18, Step:   252300, Batch Loss:     1.686004, Tokens per Sec:    14928, Lr: 0.000300\n",
      "2021-07-25 12:09:21,832 - INFO - joeynmt.training - Epoch  18, Step:   252400, Batch Loss:     2.782602, Tokens per Sec:    15867, Lr: 0.000300\n",
      "2021-07-25 12:09:35,742 - INFO - joeynmt.training - Epoch  18, Step:   252500, Batch Loss:     1.823956, Tokens per Sec:    15627, Lr: 0.000300\n",
      "2021-07-25 12:09:49,724 - INFO - joeynmt.training - Epoch  18, Step:   252600, Batch Loss:     1.757584, Tokens per Sec:    16218, Lr: 0.000300\n",
      "2021-07-25 12:10:03,390 - INFO - joeynmt.training - Epoch  18, Step:   252700, Batch Loss:     1.593725, Tokens per Sec:    15849, Lr: 0.000300\n",
      "2021-07-25 12:10:17,226 - INFO - joeynmt.training - Epoch  18, Step:   252800, Batch Loss:     1.831733, Tokens per Sec:    16036, Lr: 0.000300\n",
      "2021-07-25 12:10:31,490 - INFO - joeynmt.training - Epoch  18, Step:   252900, Batch Loss:     1.828588, Tokens per Sec:    16262, Lr: 0.000300\n",
      "2021-07-25 12:10:45,526 - INFO - joeynmt.training - Epoch  18, Step:   253000, Batch Loss:     1.792078, Tokens per Sec:    15940, Lr: 0.000300\n",
      "2021-07-25 12:10:59,452 - INFO - joeynmt.training - Epoch  18, Step:   253100, Batch Loss:     1.763997, Tokens per Sec:    16058, Lr: 0.000300\n",
      "2021-07-25 12:11:13,446 - INFO - joeynmt.training - Epoch  18, Step:   253200, Batch Loss:     1.714326, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-07-25 12:11:27,328 - INFO - joeynmt.training - Epoch  18, Step:   253300, Batch Loss:     1.774715, Tokens per Sec:    15760, Lr: 0.000300\n",
      "2021-07-25 12:11:41,006 - INFO - joeynmt.training - Epoch  18, Step:   253400, Batch Loss:     1.905676, Tokens per Sec:    15598, Lr: 0.000300\n",
      "2021-07-25 12:11:54,791 - INFO - joeynmt.training - Epoch  18, Step:   253500, Batch Loss:     1.798702, Tokens per Sec:    16026, Lr: 0.000300\n",
      "2021-07-25 12:12:08,811 - INFO - joeynmt.training - Epoch  18, Step:   253600, Batch Loss:     1.561567, Tokens per Sec:    15884, Lr: 0.000300\n",
      "2021-07-25 12:12:22,583 - INFO - joeynmt.training - Epoch  18, Step:   253700, Batch Loss:     1.859207, Tokens per Sec:    15642, Lr: 0.000300\n",
      "2021-07-25 12:12:36,341 - INFO - joeynmt.training - Epoch  18, Step:   253800, Batch Loss:     1.643074, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 12:12:50,265 - INFO - joeynmt.training - Epoch  18, Step:   253900, Batch Loss:     1.539260, Tokens per Sec:    16028, Lr: 0.000300\n",
      "2021-07-25 12:13:04,191 - INFO - joeynmt.training - Epoch  18, Step:   254000, Batch Loss:     1.730334, Tokens per Sec:    16041, Lr: 0.000300\n",
      "2021-07-25 12:13:18,352 - INFO - joeynmt.training - Epoch  18, Step:   254100, Batch Loss:     1.817436, Tokens per Sec:    15608, Lr: 0.000300\n",
      "2021-07-25 12:13:32,304 - INFO - joeynmt.training - Epoch  18, Step:   254200, Batch Loss:     1.662810, Tokens per Sec:    15947, Lr: 0.000300\n",
      "2021-07-25 12:13:45,983 - INFO - joeynmt.training - Epoch  18, Step:   254300, Batch Loss:     1.810812, Tokens per Sec:    15381, Lr: 0.000300\n",
      "2021-07-25 12:13:59,958 - INFO - joeynmt.training - Epoch  18, Step:   254400, Batch Loss:     1.523634, Tokens per Sec:    16210, Lr: 0.000300\n",
      "2021-07-25 12:14:14,176 - INFO - joeynmt.training - Epoch  18, Step:   254500, Batch Loss:     1.696490, Tokens per Sec:    15991, Lr: 0.000300\n",
      "2021-07-25 12:14:28,014 - INFO - joeynmt.training - Epoch  18, Step:   254600, Batch Loss:     1.583651, Tokens per Sec:    15388, Lr: 0.000300\n",
      "2021-07-25 12:14:41,875 - INFO - joeynmt.training - Epoch  18, Step:   254700, Batch Loss:     1.638461, Tokens per Sec:    15790, Lr: 0.000300\n",
      "2021-07-25 12:14:55,630 - INFO - joeynmt.training - Epoch  18, Step:   254800, Batch Loss:     1.708743, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-07-25 12:15:09,497 - INFO - joeynmt.training - Epoch  18, Step:   254900, Batch Loss:     1.858869, Tokens per Sec:    16117, Lr: 0.000300\n",
      "2021-07-25 12:15:23,362 - INFO - joeynmt.training - Epoch  18, Step:   255000, Batch Loss:     1.560432, Tokens per Sec:    15569, Lr: 0.000300\n",
      "2021-07-25 12:15:37,247 - INFO - joeynmt.training - Epoch  18, Step:   255100, Batch Loss:     1.819845, Tokens per Sec:    15413, Lr: 0.000300\n",
      "2021-07-25 12:15:51,028 - INFO - joeynmt.training - Epoch  18, Step:   255200, Batch Loss:     1.601457, Tokens per Sec:    15969, Lr: 0.000300\n",
      "2021-07-25 12:16:04,974 - INFO - joeynmt.training - Epoch  18, Step:   255300, Batch Loss:     1.805735, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-07-25 12:16:18,974 - INFO - joeynmt.training - Epoch  18, Step:   255400, Batch Loss:     1.557384, Tokens per Sec:    16098, Lr: 0.000300\n",
      "2021-07-25 12:16:32,812 - INFO - joeynmt.training - Epoch  18, Step:   255500, Batch Loss:     1.719170, Tokens per Sec:    15907, Lr: 0.000300\n",
      "2021-07-25 12:16:46,523 - INFO - joeynmt.training - Epoch  18, Step:   255600, Batch Loss:     1.833044, Tokens per Sec:    15834, Lr: 0.000300\n",
      "2021-07-25 12:17:00,411 - INFO - joeynmt.training - Epoch  18, Step:   255700, Batch Loss:     1.746378, Tokens per Sec:    15838, Lr: 0.000300\n",
      "2021-07-25 12:17:14,522 - INFO - joeynmt.training - Epoch  18, Step:   255800, Batch Loss:     1.837217, Tokens per Sec:    16003, Lr: 0.000300\n",
      "2021-07-25 12:17:28,310 - INFO - joeynmt.training - Epoch  18, Step:   255900, Batch Loss:     1.823065, Tokens per Sec:    15789, Lr: 0.000300\n",
      "2021-07-25 12:17:42,110 - INFO - joeynmt.training - Epoch  18, Step:   256000, Batch Loss:     1.733839, Tokens per Sec:    16135, Lr: 0.000300\n",
      "2021-07-25 12:17:55,895 - INFO - joeynmt.training - Epoch  18, Step:   256100, Batch Loss:     1.786881, Tokens per Sec:    16116, Lr: 0.000300\n",
      "2021-07-25 12:18:09,609 - INFO - joeynmt.training - Epoch  18, Step:   256200, Batch Loss:     1.830931, Tokens per Sec:    15465, Lr: 0.000300\n",
      "2021-07-25 12:18:23,664 - INFO - joeynmt.training - Epoch  18, Step:   256300, Batch Loss:     1.653339, Tokens per Sec:    15654, Lr: 0.000300\n",
      "2021-07-25 12:18:37,530 - INFO - joeynmt.training - Epoch  18, Step:   256400, Batch Loss:     1.870998, Tokens per Sec:    15635, Lr: 0.000300\n",
      "2021-07-25 12:18:51,288 - INFO - joeynmt.training - Epoch  18, Step:   256500, Batch Loss:     1.512580, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-07-25 12:19:05,230 - INFO - joeynmt.training - Epoch  18, Step:   256600, Batch Loss:     1.663687, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-07-25 12:19:19,178 - INFO - joeynmt.training - Epoch  18, Step:   256700, Batch Loss:     1.700586, Tokens per Sec:    15557, Lr: 0.000300\n",
      "2021-07-25 12:19:33,007 - INFO - joeynmt.training - Epoch  18, Step:   256800, Batch Loss:     1.644132, Tokens per Sec:    15813, Lr: 0.000300\n",
      "2021-07-25 12:19:46,790 - INFO - joeynmt.training - Epoch  18, Step:   256900, Batch Loss:     1.716634, Tokens per Sec:    15736, Lr: 0.000300\n",
      "2021-07-25 12:20:00,348 - INFO - joeynmt.training - Epoch  18, Step:   257000, Batch Loss:     1.915756, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-07-25 12:20:14,108 - INFO - joeynmt.training - Epoch  18, Step:   257100, Batch Loss:     1.734856, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-07-25 12:20:28,139 - INFO - joeynmt.training - Epoch  18, Step:   257200, Batch Loss:     1.762236, Tokens per Sec:    15577, Lr: 0.000300\n",
      "2021-07-25 12:20:41,881 - INFO - joeynmt.training - Epoch  18, Step:   257300, Batch Loss:     1.769112, Tokens per Sec:    15735, Lr: 0.000300\n",
      "2021-07-25 12:20:55,522 - INFO - joeynmt.training - Epoch  18, Step:   257400, Batch Loss:     1.794563, Tokens per Sec:    15709, Lr: 0.000300\n",
      "2021-07-25 12:21:09,490 - INFO - joeynmt.training - Epoch  18, Step:   257500, Batch Loss:     1.844890, Tokens per Sec:    15690, Lr: 0.000300\n",
      "2021-07-25 12:21:23,325 - INFO - joeynmt.training - Epoch  18, Step:   257600, Batch Loss:     1.541551, Tokens per Sec:    15717, Lr: 0.000300\n",
      "2021-07-25 12:21:32,763 - INFO - joeynmt.training - Epoch  18: total training loss 9343.99\n",
      "2021-07-25 12:21:32,764 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-25 12:21:38,228 - INFO - joeynmt.training - Epoch  19, Step:   257700, Batch Loss:     1.628879, Tokens per Sec:    14188, Lr: 0.000300\n",
      "2021-07-25 12:21:51,821 - INFO - joeynmt.training - Epoch  19, Step:   257800, Batch Loss:     1.708936, Tokens per Sec:    15579, Lr: 0.000300\n",
      "2021-07-25 12:22:05,909 - INFO - joeynmt.training - Epoch  19, Step:   257900, Batch Loss:     1.680048, Tokens per Sec:    15842, Lr: 0.000300\n",
      "2021-07-25 12:22:19,697 - INFO - joeynmt.training - Epoch  19, Step:   258000, Batch Loss:     1.720900, Tokens per Sec:    15752, Lr: 0.000300\n",
      "2021-07-25 12:22:41,823 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 12:22:41,823 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 12:22:41,823 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 12:22:42,730 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 12:22:42,732 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 12:22:42,732 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 12:22:42,732 - INFO - joeynmt.training - \tHypothesis: I was deeply moved by my heart .\n",
      "2021-07-25 12:22:42,732 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 12:22:42,733 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 12:22:42,733 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 12:22:42,733 - INFO - joeynmt.training - \tHypothesis: The text was written on the outside of the scroll .\n",
      "2021-07-25 12:22:42,733 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 12:22:42,734 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 12:22:42,734 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 12:22:42,734 - INFO - joeynmt.training - \tHypothesis: Rather than worry or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 12:22:42,734 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 12:22:42,735 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 12:22:42,735 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 12:22:42,735 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 12:22:42,735 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   258000: bleu:  27.25, loss: 42920.4336, ppl:   4.6888, duration: 23.0376s\n",
      "2021-07-25 12:22:56,868 - INFO - joeynmt.training - Epoch  19, Step:   258100, Batch Loss:     1.437819, Tokens per Sec:    16046, Lr: 0.000300\n",
      "2021-07-25 12:23:10,743 - INFO - joeynmt.training - Epoch  19, Step:   258200, Batch Loss:     1.671075, Tokens per Sec:    15505, Lr: 0.000300\n",
      "2021-07-25 12:23:24,430 - INFO - joeynmt.training - Epoch  19, Step:   258300, Batch Loss:     1.575103, Tokens per Sec:    15423, Lr: 0.000300\n",
      "2021-07-25 12:23:38,450 - INFO - joeynmt.training - Epoch  19, Step:   258400, Batch Loss:     1.813108, Tokens per Sec:    16011, Lr: 0.000300\n",
      "2021-07-25 12:23:52,244 - INFO - joeynmt.training - Epoch  19, Step:   258500, Batch Loss:     1.324170, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-07-25 12:24:05,936 - INFO - joeynmt.training - Epoch  19, Step:   258600, Batch Loss:     1.751211, Tokens per Sec:    15781, Lr: 0.000300\n",
      "2021-07-25 12:24:19,761 - INFO - joeynmt.training - Epoch  19, Step:   258700, Batch Loss:     1.599241, Tokens per Sec:    15607, Lr: 0.000300\n",
      "2021-07-25 12:24:33,849 - INFO - joeynmt.training - Epoch  19, Step:   258800, Batch Loss:     1.746910, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-07-25 12:24:47,631 - INFO - joeynmt.training - Epoch  19, Step:   258900, Batch Loss:     1.862888, Tokens per Sec:    15928, Lr: 0.000300\n",
      "2021-07-25 12:25:01,624 - INFO - joeynmt.training - Epoch  19, Step:   259000, Batch Loss:     1.888631, Tokens per Sec:    16096, Lr: 0.000300\n",
      "2021-07-25 12:25:15,453 - INFO - joeynmt.training - Epoch  19, Step:   259100, Batch Loss:     1.826041, Tokens per Sec:    15769, Lr: 0.000300\n",
      "2021-07-25 12:25:29,574 - INFO - joeynmt.training - Epoch  19, Step:   259200, Batch Loss:     1.673221, Tokens per Sec:    15674, Lr: 0.000300\n",
      "2021-07-25 12:25:43,489 - INFO - joeynmt.training - Epoch  19, Step:   259300, Batch Loss:     1.607252, Tokens per Sec:    15932, Lr: 0.000300\n",
      "2021-07-25 12:25:57,381 - INFO - joeynmt.training - Epoch  19, Step:   259400, Batch Loss:     1.778296, Tokens per Sec:    15801, Lr: 0.000300\n",
      "2021-07-25 12:26:11,165 - INFO - joeynmt.training - Epoch  19, Step:   259500, Batch Loss:     1.866970, Tokens per Sec:    15989, Lr: 0.000300\n",
      "2021-07-25 12:26:24,994 - INFO - joeynmt.training - Epoch  19, Step:   259600, Batch Loss:     1.764767, Tokens per Sec:    15747, Lr: 0.000300\n",
      "2021-07-25 12:26:38,859 - INFO - joeynmt.training - Epoch  19, Step:   259700, Batch Loss:     1.609210, Tokens per Sec:    15333, Lr: 0.000300\n",
      "2021-07-25 12:26:52,582 - INFO - joeynmt.training - Epoch  19, Step:   259800, Batch Loss:     1.916963, Tokens per Sec:    15739, Lr: 0.000300\n",
      "2021-07-25 12:27:06,609 - INFO - joeynmt.training - Epoch  19, Step:   259900, Batch Loss:     1.770967, Tokens per Sec:    15803, Lr: 0.000300\n",
      "2021-07-25 12:27:20,433 - INFO - joeynmt.training - Epoch  19, Step:   260000, Batch Loss:     1.691326, Tokens per Sec:    15684, Lr: 0.000300\n",
      "2021-07-25 12:27:34,218 - INFO - joeynmt.training - Epoch  19, Step:   260100, Batch Loss:     1.604526, Tokens per Sec:    15715, Lr: 0.000300\n",
      "2021-07-25 12:27:47,691 - INFO - joeynmt.training - Epoch  19, Step:   260200, Batch Loss:     1.654444, Tokens per Sec:    15689, Lr: 0.000300\n",
      "2021-07-25 12:28:01,687 - INFO - joeynmt.training - Epoch  19, Step:   260300, Batch Loss:     1.710737, Tokens per Sec:    15676, Lr: 0.000300\n",
      "2021-07-25 12:28:15,635 - INFO - joeynmt.training - Epoch  19, Step:   260400, Batch Loss:     1.664024, Tokens per Sec:    15825, Lr: 0.000300\n",
      "2021-07-25 12:28:29,618 - INFO - joeynmt.training - Epoch  19, Step:   260500, Batch Loss:     1.602518, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-07-25 12:28:43,239 - INFO - joeynmt.training - Epoch  19, Step:   260600, Batch Loss:     1.870894, Tokens per Sec:    15678, Lr: 0.000300\n",
      "2021-07-25 12:28:57,192 - INFO - joeynmt.training - Epoch  19, Step:   260700, Batch Loss:     1.688105, Tokens per Sec:    16071, Lr: 0.000300\n",
      "2021-07-25 12:29:11,035 - INFO - joeynmt.training - Epoch  19, Step:   260800, Batch Loss:     1.653855, Tokens per Sec:    15569, Lr: 0.000300\n",
      "2021-07-25 12:29:24,961 - INFO - joeynmt.training - Epoch  19, Step:   260900, Batch Loss:     1.595943, Tokens per Sec:    15425, Lr: 0.000300\n",
      "2021-07-25 12:29:38,652 - INFO - joeynmt.training - Epoch  19, Step:   261000, Batch Loss:     1.695098, Tokens per Sec:    15619, Lr: 0.000300\n",
      "2021-07-25 12:29:52,527 - INFO - joeynmt.training - Epoch  19, Step:   261100, Batch Loss:     1.652219, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-07-25 12:30:06,384 - INFO - joeynmt.training - Epoch  19, Step:   261200, Batch Loss:     1.802292, Tokens per Sec:    15847, Lr: 0.000300\n",
      "2021-07-25 12:30:20,213 - INFO - joeynmt.training - Epoch  19, Step:   261300, Batch Loss:     1.663929, Tokens per Sec:    15656, Lr: 0.000300\n",
      "2021-07-25 12:30:34,052 - INFO - joeynmt.training - Epoch  19, Step:   261400, Batch Loss:     1.675616, Tokens per Sec:    15883, Lr: 0.000300\n",
      "2021-07-25 12:30:47,874 - INFO - joeynmt.training - Epoch  19, Step:   261500, Batch Loss:     1.879253, Tokens per Sec:    15877, Lr: 0.000300\n",
      "2021-07-25 12:31:01,830 - INFO - joeynmt.training - Epoch  19, Step:   261600, Batch Loss:     1.721120, Tokens per Sec:    16052, Lr: 0.000300\n",
      "2021-07-25 12:31:15,655 - INFO - joeynmt.training - Epoch  19, Step:   261700, Batch Loss:     1.715438, Tokens per Sec:    15656, Lr: 0.000300\n",
      "2021-07-25 12:31:29,686 - INFO - joeynmt.training - Epoch  19, Step:   261800, Batch Loss:     1.806331, Tokens per Sec:    15568, Lr: 0.000300\n",
      "2021-07-25 12:31:43,598 - INFO - joeynmt.training - Epoch  19, Step:   261900, Batch Loss:     1.742460, Tokens per Sec:    15801, Lr: 0.000300\n",
      "2021-07-25 12:31:57,304 - INFO - joeynmt.training - Epoch  19, Step:   262000, Batch Loss:     1.802964, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-07-25 12:32:11,046 - INFO - joeynmt.training - Epoch  19, Step:   262100, Batch Loss:     1.708216, Tokens per Sec:    15975, Lr: 0.000300\n",
      "2021-07-25 12:32:25,093 - INFO - joeynmt.training - Epoch  19, Step:   262200, Batch Loss:     1.765277, Tokens per Sec:    15827, Lr: 0.000300\n",
      "2021-07-25 12:32:39,106 - INFO - joeynmt.training - Epoch  19, Step:   262300, Batch Loss:     1.782285, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-07-25 12:32:52,970 - INFO - joeynmt.training - Epoch  19, Step:   262400, Batch Loss:     1.911418, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-07-25 12:33:07,070 - INFO - joeynmt.training - Epoch  19, Step:   262500, Batch Loss:     1.819448, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-25 12:33:20,888 - INFO - joeynmt.training - Epoch  19, Step:   262600, Batch Loss:     1.757738, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-07-25 12:33:34,922 - INFO - joeynmt.training - Epoch  19, Step:   262700, Batch Loss:     1.895226, Tokens per Sec:    15984, Lr: 0.000300\n",
      "2021-07-25 12:33:48,837 - INFO - joeynmt.training - Epoch  19, Step:   262800, Batch Loss:     1.873565, Tokens per Sec:    16048, Lr: 0.000300\n",
      "2021-07-25 12:34:02,727 - INFO - joeynmt.training - Epoch  19, Step:   262900, Batch Loss:     1.650112, Tokens per Sec:    15871, Lr: 0.000300\n",
      "2021-07-25 12:34:16,699 - INFO - joeynmt.training - Epoch  19, Step:   263000, Batch Loss:     1.880541, Tokens per Sec:    15641, Lr: 0.000300\n",
      "2021-07-25 12:34:30,662 - INFO - joeynmt.training - Epoch  19, Step:   263100, Batch Loss:     1.626325, Tokens per Sec:    16014, Lr: 0.000300\n",
      "2021-07-25 12:34:31,302 - INFO - joeynmt.training - Epoch  19: total training loss 9344.97\n",
      "2021-07-25 12:34:31,302 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-25 12:34:45,287 - INFO - joeynmt.training - Epoch  20, Step:   263200, Batch Loss:     1.611766, Tokens per Sec:    15584, Lr: 0.000300\n",
      "2021-07-25 12:34:59,026 - INFO - joeynmt.training - Epoch  20, Step:   263300, Batch Loss:     1.627647, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-07-25 12:35:13,146 - INFO - joeynmt.training - Epoch  20, Step:   263400, Batch Loss:     1.700712, Tokens per Sec:    15870, Lr: 0.000300\n",
      "2021-07-25 12:35:27,006 - INFO - joeynmt.training - Epoch  20, Step:   263500, Batch Loss:     1.685345, Tokens per Sec:    15822, Lr: 0.000300\n",
      "2021-07-25 12:35:40,750 - INFO - joeynmt.training - Epoch  20, Step:   263600, Batch Loss:     1.831955, Tokens per Sec:    15466, Lr: 0.000300\n",
      "2021-07-25 12:35:54,703 - INFO - joeynmt.training - Epoch  20, Step:   263700, Batch Loss:     1.858432, Tokens per Sec:    16047, Lr: 0.000300\n",
      "2021-07-25 12:36:08,548 - INFO - joeynmt.training - Epoch  20, Step:   263800, Batch Loss:     1.763520, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-07-25 12:36:22,526 - INFO - joeynmt.training - Epoch  20, Step:   263900, Batch Loss:     1.969135, Tokens per Sec:    15505, Lr: 0.000300\n",
      "2021-07-25 12:36:36,660 - INFO - joeynmt.training - Epoch  20, Step:   264000, Batch Loss:     1.530005, Tokens per Sec:    15863, Lr: 0.000300\n",
      "2021-07-25 12:36:57,412 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 12:36:57,413 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 12:36:57,413 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 12:36:57,664 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 12:36:57,665 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 12:36:58,373 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 12:36:58,373 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 12:36:58,373 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 12:36:58,374 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
      "2021-07-25 12:36:58,374 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 12:36:58,374 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 12:36:58,374 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 12:36:58,374 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the outside of the scroll .\n",
      "2021-07-25 12:36:58,375 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 12:36:58,375 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 12:36:58,375 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 12:36:58,375 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 12:36:58,376 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 12:36:58,376 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 12:36:58,376 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 12:36:58,376 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 12:36:58,376 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   264000: bleu:  27.42, loss: 42726.0156, ppl:   4.6561, duration: 21.7159s\n",
      "2021-07-25 12:37:12,764 - INFO - joeynmt.training - Epoch  20, Step:   264100, Batch Loss:     1.988780, Tokens per Sec:    15724, Lr: 0.000300\n",
      "2021-07-25 12:37:26,561 - INFO - joeynmt.training - Epoch  20, Step:   264200, Batch Loss:     1.832028, Tokens per Sec:    15367, Lr: 0.000300\n",
      "2021-07-25 12:37:40,670 - INFO - joeynmt.training - Epoch  20, Step:   264300, Batch Loss:     1.578312, Tokens per Sec:    15503, Lr: 0.000300\n",
      "2021-07-25 12:37:54,709 - INFO - joeynmt.training - Epoch  20, Step:   264400, Batch Loss:     1.699681, Tokens per Sec:    15901, Lr: 0.000300\n",
      "2021-07-25 12:38:08,797 - INFO - joeynmt.training - Epoch  20, Step:   264500, Batch Loss:     1.758127, Tokens per Sec:    15630, Lr: 0.000300\n",
      "2021-07-25 12:38:22,803 - INFO - joeynmt.training - Epoch  20, Step:   264600, Batch Loss:     1.840907, Tokens per Sec:    16162, Lr: 0.000300\n",
      "2021-07-25 12:38:36,513 - INFO - joeynmt.training - Epoch  20, Step:   264700, Batch Loss:     1.684819, Tokens per Sec:    15226, Lr: 0.000300\n",
      "2021-07-25 12:38:50,541 - INFO - joeynmt.training - Epoch  20, Step:   264800, Batch Loss:     1.751073, Tokens per Sec:    16075, Lr: 0.000300\n",
      "2021-07-25 12:39:04,687 - INFO - joeynmt.training - Epoch  20, Step:   264900, Batch Loss:     1.786898, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-07-25 12:39:18,697 - INFO - joeynmt.training - Epoch  20, Step:   265000, Batch Loss:     1.692353, Tokens per Sec:    15533, Lr: 0.000300\n",
      "2021-07-25 12:39:32,603 - INFO - joeynmt.training - Epoch  20, Step:   265100, Batch Loss:     1.803463, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-07-25 12:39:46,349 - INFO - joeynmt.training - Epoch  20, Step:   265200, Batch Loss:     1.869504, Tokens per Sec:    15738, Lr: 0.000300\n",
      "2021-07-25 12:40:00,155 - INFO - joeynmt.training - Epoch  20, Step:   265300, Batch Loss:     1.786195, Tokens per Sec:    15526, Lr: 0.000300\n",
      "2021-07-25 12:40:14,151 - INFO - joeynmt.training - Epoch  20, Step:   265400, Batch Loss:     1.689922, Tokens per Sec:    15632, Lr: 0.000300\n",
      "2021-07-25 12:40:28,017 - INFO - joeynmt.training - Epoch  20, Step:   265500, Batch Loss:     1.658684, Tokens per Sec:    15697, Lr: 0.000300\n",
      "2021-07-25 12:40:41,828 - INFO - joeynmt.training - Epoch  20, Step:   265600, Batch Loss:     1.847581, Tokens per Sec:    15808, Lr: 0.000300\n",
      "2021-07-25 12:40:55,702 - INFO - joeynmt.training - Epoch  20, Step:   265700, Batch Loss:     1.739553, Tokens per Sec:    16066, Lr: 0.000300\n",
      "2021-07-25 12:41:09,754 - INFO - joeynmt.training - Epoch  20, Step:   265800, Batch Loss:     1.763349, Tokens per Sec:    15893, Lr: 0.000300\n",
      "2021-07-25 12:41:23,836 - INFO - joeynmt.training - Epoch  20, Step:   265900, Batch Loss:     1.640063, Tokens per Sec:    15948, Lr: 0.000300\n",
      "2021-07-25 12:41:37,890 - INFO - joeynmt.training - Epoch  20, Step:   266000, Batch Loss:     1.466411, Tokens per Sec:    16113, Lr: 0.000300\n",
      "2021-07-25 12:41:51,858 - INFO - joeynmt.training - Epoch  20, Step:   266100, Batch Loss:     1.499433, Tokens per Sec:    15997, Lr: 0.000300\n",
      "2021-07-25 12:42:05,688 - INFO - joeynmt.training - Epoch  20, Step:   266200, Batch Loss:     1.767312, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-07-25 12:42:19,558 - INFO - joeynmt.training - Epoch  20, Step:   266300, Batch Loss:     1.799377, Tokens per Sec:    15605, Lr: 0.000300\n",
      "2021-07-25 12:42:33,631 - INFO - joeynmt.training - Epoch  20, Step:   266400, Batch Loss:     1.711113, Tokens per Sec:    15865, Lr: 0.000300\n",
      "2021-07-25 12:42:47,332 - INFO - joeynmt.training - Epoch  20, Step:   266500, Batch Loss:     1.900493, Tokens per Sec:    15516, Lr: 0.000300\n",
      "2021-07-25 12:43:01,167 - INFO - joeynmt.training - Epoch  20, Step:   266600, Batch Loss:     1.640384, Tokens per Sec:    15453, Lr: 0.000300\n",
      "2021-07-25 12:43:15,165 - INFO - joeynmt.training - Epoch  20, Step:   266700, Batch Loss:     1.680575, Tokens per Sec:    15854, Lr: 0.000300\n",
      "2021-07-25 12:43:28,985 - INFO - joeynmt.training - Epoch  20, Step:   266800, Batch Loss:     1.762523, Tokens per Sec:    15732, Lr: 0.000300\n",
      "2021-07-25 12:43:42,718 - INFO - joeynmt.training - Epoch  20, Step:   266900, Batch Loss:     1.717834, Tokens per Sec:    15643, Lr: 0.000300\n",
      "2021-07-25 12:43:56,624 - INFO - joeynmt.training - Epoch  20, Step:   267000, Batch Loss:     1.757688, Tokens per Sec:    15535, Lr: 0.000300\n",
      "2021-07-25 12:44:10,668 - INFO - joeynmt.training - Epoch  20, Step:   267100, Batch Loss:     1.693321, Tokens per Sec:    15493, Lr: 0.000300\n",
      "2021-07-25 12:44:24,697 - INFO - joeynmt.training - Epoch  20, Step:   267200, Batch Loss:     1.589341, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-07-25 12:44:38,562 - INFO - joeynmt.training - Epoch  20, Step:   267300, Batch Loss:     1.535069, Tokens per Sec:    16054, Lr: 0.000300\n",
      "2021-07-25 12:44:52,526 - INFO - joeynmt.training - Epoch  20, Step:   267400, Batch Loss:     1.880050, Tokens per Sec:    16163, Lr: 0.000300\n",
      "2021-07-25 12:45:06,625 - INFO - joeynmt.training - Epoch  20, Step:   267500, Batch Loss:     1.652792, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-07-25 12:45:20,498 - INFO - joeynmt.training - Epoch  20, Step:   267600, Batch Loss:     1.718282, Tokens per Sec:    15521, Lr: 0.000300\n",
      "2021-07-25 12:45:34,301 - INFO - joeynmt.training - Epoch  20, Step:   267700, Batch Loss:     1.755252, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-07-25 12:45:48,375 - INFO - joeynmt.training - Epoch  20, Step:   267800, Batch Loss:     1.496648, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-07-25 12:46:02,337 - INFO - joeynmt.training - Epoch  20, Step:   267900, Batch Loss:     1.729747, Tokens per Sec:    16187, Lr: 0.000300\n",
      "2021-07-25 12:46:16,204 - INFO - joeynmt.training - Epoch  20, Step:   268000, Batch Loss:     1.874786, Tokens per Sec:    15528, Lr: 0.000300\n",
      "2021-07-25 12:46:30,102 - INFO - joeynmt.training - Epoch  20, Step:   268100, Batch Loss:     1.765264, Tokens per Sec:    15674, Lr: 0.000300\n",
      "2021-07-25 12:46:43,764 - INFO - joeynmt.training - Epoch  20, Step:   268200, Batch Loss:     1.925464, Tokens per Sec:    15537, Lr: 0.000300\n",
      "2021-07-25 12:46:57,547 - INFO - joeynmt.training - Epoch  20, Step:   268300, Batch Loss:     1.735036, Tokens per Sec:    15708, Lr: 0.000300\n",
      "2021-07-25 12:47:11,583 - INFO - joeynmt.training - Epoch  20, Step:   268400, Batch Loss:     1.724451, Tokens per Sec:    15887, Lr: 0.000300\n",
      "2021-07-25 12:47:25,472 - INFO - joeynmt.training - Epoch  20, Step:   268500, Batch Loss:     1.578579, Tokens per Sec:    15322, Lr: 0.000300\n",
      "2021-07-25 12:47:28,955 - INFO - joeynmt.training - Epoch  20: total training loss 9295.52\n",
      "2021-07-25 12:47:28,955 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-25 12:47:40,357 - INFO - joeynmt.training - Epoch  21, Step:   268600, Batch Loss:     1.567418, Tokens per Sec:    14997, Lr: 0.000300\n",
      "2021-07-25 12:47:54,411 - INFO - joeynmt.training - Epoch  21, Step:   268700, Batch Loss:     1.541840, Tokens per Sec:    15862, Lr: 0.000300\n",
      "2021-07-25 12:48:08,385 - INFO - joeynmt.training - Epoch  21, Step:   268800, Batch Loss:     1.536608, Tokens per Sec:    15935, Lr: 0.000300\n",
      "2021-07-25 12:48:22,343 - INFO - joeynmt.training - Epoch  21, Step:   268900, Batch Loss:     1.574328, Tokens per Sec:    15547, Lr: 0.000300\n",
      "2021-07-25 12:48:36,252 - INFO - joeynmt.training - Epoch  21, Step:   269000, Batch Loss:     1.521598, Tokens per Sec:    15385, Lr: 0.000300\n",
      "2021-07-25 12:48:50,336 - INFO - joeynmt.training - Epoch  21, Step:   269100, Batch Loss:     1.713073, Tokens per Sec:    16160, Lr: 0.000300\n",
      "2021-07-25 12:49:04,350 - INFO - joeynmt.training - Epoch  21, Step:   269200, Batch Loss:     1.555485, Tokens per Sec:    15545, Lr: 0.000300\n",
      "2021-07-25 12:49:18,175 - INFO - joeynmt.training - Epoch  21, Step:   269300, Batch Loss:     1.765651, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-07-25 12:49:32,119 - INFO - joeynmt.training - Epoch  21, Step:   269400, Batch Loss:     1.780406, Tokens per Sec:    15788, Lr: 0.000300\n",
      "2021-07-25 12:49:45,790 - INFO - joeynmt.training - Epoch  21, Step:   269500, Batch Loss:     1.675663, Tokens per Sec:    15830, Lr: 0.000300\n",
      "2021-07-25 12:49:59,856 - INFO - joeynmt.training - Epoch  21, Step:   269600, Batch Loss:     1.705009, Tokens per Sec:    15885, Lr: 0.000300\n",
      "2021-07-25 12:50:13,656 - INFO - joeynmt.training - Epoch  21, Step:   269700, Batch Loss:     1.763612, Tokens per Sec:    15467, Lr: 0.000300\n",
      "2021-07-25 12:50:27,650 - INFO - joeynmt.training - Epoch  21, Step:   269800, Batch Loss:     1.894374, Tokens per Sec:    15638, Lr: 0.000300\n",
      "2021-07-25 12:50:41,505 - INFO - joeynmt.training - Epoch  21, Step:   269900, Batch Loss:     1.734229, Tokens per Sec:    15852, Lr: 0.000300\n",
      "2021-07-25 12:50:55,266 - INFO - joeynmt.training - Epoch  21, Step:   270000, Batch Loss:     1.808502, Tokens per Sec:    15650, Lr: 0.000300\n",
      "2021-07-25 12:51:17,584 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 12:51:17,585 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 12:51:17,585 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 12:51:17,843 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 12:51:17,843 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 12:51:18,586 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 12:51:18,587 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 12:51:18,587 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 12:51:18,587 - INFO - joeynmt.training - \tHypothesis: I was touched by my heart .\n",
      "2021-07-25 12:51:18,587 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 12:51:18,588 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 12:51:18,588 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 12:51:18,588 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
      "2021-07-25 12:51:18,588 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 12:51:18,589 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 12:51:18,589 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 12:51:18,590 - INFO - joeynmt.training - \tHypothesis: Rather than worry or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 12:51:18,590 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 12:51:18,590 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 12:51:18,590 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 12:51:18,591 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 12:51:18,591 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step   270000: bleu:  27.76, loss: 42709.4766, ppl:   4.6533, duration: 23.3249s\n",
      "2021-07-25 12:51:33,072 - INFO - joeynmt.training - Epoch  21, Step:   270100, Batch Loss:     1.829017, Tokens per Sec:    15317, Lr: 0.000300\n",
      "2021-07-25 12:51:46,912 - INFO - joeynmt.training - Epoch  21, Step:   270200, Batch Loss:     1.671697, Tokens per Sec:    15835, Lr: 0.000300\n",
      "2021-07-25 12:52:00,958 - INFO - joeynmt.training - Epoch  21, Step:   270300, Batch Loss:     1.729357, Tokens per Sec:    16023, Lr: 0.000300\n",
      "2021-07-25 12:52:14,804 - INFO - joeynmt.training - Epoch  21, Step:   270400, Batch Loss:     2.048892, Tokens per Sec:    15562, Lr: 0.000300\n",
      "2021-07-25 12:52:28,884 - INFO - joeynmt.training - Epoch  21, Step:   270500, Batch Loss:     1.681422, Tokens per Sec:    15711, Lr: 0.000300\n",
      "2021-07-25 12:52:42,558 - INFO - joeynmt.training - Epoch  21, Step:   270600, Batch Loss:     1.783459, Tokens per Sec:    15794, Lr: 0.000300\n",
      "2021-07-25 12:52:56,402 - INFO - joeynmt.training - Epoch  21, Step:   270700, Batch Loss:     1.463062, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-07-25 12:53:10,022 - INFO - joeynmt.training - Epoch  21, Step:   270800, Batch Loss:     1.597029, Tokens per Sec:    15741, Lr: 0.000300\n",
      "2021-07-25 12:53:23,939 - INFO - joeynmt.training - Epoch  21, Step:   270900, Batch Loss:     1.637199, Tokens per Sec:    15667, Lr: 0.000300\n",
      "2021-07-25 12:53:37,907 - INFO - joeynmt.training - Epoch  21, Step:   271000, Batch Loss:     1.863378, Tokens per Sec:    15669, Lr: 0.000300\n",
      "2021-07-25 12:53:52,099 - INFO - joeynmt.training - Epoch  21, Step:   271100, Batch Loss:     1.897590, Tokens per Sec:    15889, Lr: 0.000300\n",
      "2021-07-25 12:54:06,170 - INFO - joeynmt.training - Epoch  21, Step:   271200, Batch Loss:     1.803977, Tokens per Sec:    15929, Lr: 0.000300\n",
      "2021-07-25 12:54:19,973 - INFO - joeynmt.training - Epoch  21, Step:   271300, Batch Loss:     1.672601, Tokens per Sec:    15552, Lr: 0.000300\n",
      "2021-07-25 12:54:33,972 - INFO - joeynmt.training - Epoch  21, Step:   271400, Batch Loss:     1.782042, Tokens per Sec:    16032, Lr: 0.000300\n",
      "2021-07-25 12:54:47,841 - INFO - joeynmt.training - Epoch  21, Step:   271500, Batch Loss:     1.527728, Tokens per Sec:    15967, Lr: 0.000300\n",
      "2021-07-25 12:55:01,895 - INFO - joeynmt.training - Epoch  21, Step:   271600, Batch Loss:     1.699857, Tokens per Sec:    16156, Lr: 0.000300\n",
      "2021-07-25 12:55:15,918 - INFO - joeynmt.training - Epoch  21, Step:   271700, Batch Loss:     1.374783, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-07-25 12:55:29,792 - INFO - joeynmt.training - Epoch  21, Step:   271800, Batch Loss:     1.722620, Tokens per Sec:    15722, Lr: 0.000300\n",
      "2021-07-25 12:55:43,586 - INFO - joeynmt.training - Epoch  21, Step:   271900, Batch Loss:     1.683126, Tokens per Sec:    15923, Lr: 0.000300\n",
      "2021-07-25 12:55:57,568 - INFO - joeynmt.training - Epoch  21, Step:   272000, Batch Loss:     1.707461, Tokens per Sec:    16021, Lr: 0.000300\n",
      "2021-07-25 12:56:11,655 - INFO - joeynmt.training - Epoch  21, Step:   272100, Batch Loss:     1.793024, Tokens per Sec:    15621, Lr: 0.000300\n",
      "2021-07-25 12:56:25,613 - INFO - joeynmt.training - Epoch  21, Step:   272200, Batch Loss:     1.771641, Tokens per Sec:    15717, Lr: 0.000300\n",
      "2021-07-25 12:56:39,317 - INFO - joeynmt.training - Epoch  21, Step:   272300, Batch Loss:     1.782273, Tokens per Sec:    15911, Lr: 0.000300\n",
      "2021-07-25 12:56:53,129 - INFO - joeynmt.training - Epoch  21, Step:   272400, Batch Loss:     1.679712, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-07-25 12:57:06,996 - INFO - joeynmt.training - Epoch  21, Step:   272500, Batch Loss:     1.580507, Tokens per Sec:    15777, Lr: 0.000300\n",
      "2021-07-25 12:57:20,903 - INFO - joeynmt.training - Epoch  21, Step:   272600, Batch Loss:     1.661924, Tokens per Sec:    15524, Lr: 0.000300\n",
      "2021-07-25 12:57:34,805 - INFO - joeynmt.training - Epoch  21, Step:   272700, Batch Loss:     1.581635, Tokens per Sec:    15715, Lr: 0.000300\n",
      "2021-07-25 12:57:48,749 - INFO - joeynmt.training - Epoch  21, Step:   272800, Batch Loss:     1.711455, Tokens per Sec:    16034, Lr: 0.000300\n",
      "2021-07-25 12:58:02,713 - INFO - joeynmt.training - Epoch  21, Step:   272900, Batch Loss:     1.716888, Tokens per Sec:    15871, Lr: 0.000300\n",
      "2021-07-25 12:58:16,702 - INFO - joeynmt.training - Epoch  21, Step:   273000, Batch Loss:     1.673323, Tokens per Sec:    15665, Lr: 0.000300\n",
      "2021-07-25 12:58:30,739 - INFO - joeynmt.training - Epoch  21, Step:   273100, Batch Loss:     1.546100, Tokens per Sec:    15551, Lr: 0.000300\n",
      "2021-07-25 12:58:44,565 - INFO - joeynmt.training - Epoch  21, Step:   273200, Batch Loss:     1.718695, Tokens per Sec:    15773, Lr: 0.000300\n",
      "2021-07-25 12:58:58,402 - INFO - joeynmt.training - Epoch  21, Step:   273300, Batch Loss:     1.772086, Tokens per Sec:    15741, Lr: 0.000300\n",
      "2021-07-25 12:59:12,186 - INFO - joeynmt.training - Epoch  21, Step:   273400, Batch Loss:     1.696770, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 12:59:26,204 - INFO - joeynmt.training - Epoch  21, Step:   273500, Batch Loss:     1.773706, Tokens per Sec:    15593, Lr: 0.000300\n",
      "2021-07-25 12:59:40,456 - INFO - joeynmt.training - Epoch  21, Step:   273600, Batch Loss:     1.641110, Tokens per Sec:    15787, Lr: 0.000300\n",
      "2021-07-25 12:59:54,497 - INFO - joeynmt.training - Epoch  21, Step:   273700, Batch Loss:     1.665987, Tokens per Sec:    15628, Lr: 0.000300\n",
      "2021-07-25 13:00:08,488 - INFO - joeynmt.training - Epoch  21, Step:   273800, Batch Loss:     1.838428, Tokens per Sec:    15532, Lr: 0.000300\n",
      "2021-07-25 13:00:22,376 - INFO - joeynmt.training - Epoch  21, Step:   273900, Batch Loss:     1.612781, Tokens per Sec:    15710, Lr: 0.000300\n",
      "2021-07-25 13:00:29,198 - INFO - joeynmt.training - Epoch  21: total training loss 9289.55\n",
      "2021-07-25 13:00:29,198 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-25 13:00:37,083 - INFO - joeynmt.training - Epoch  22, Step:   274000, Batch Loss:     1.576330, Tokens per Sec:    14695, Lr: 0.000300\n",
      "2021-07-25 13:00:50,937 - INFO - joeynmt.training - Epoch  22, Step:   274100, Batch Loss:     1.562809, Tokens per Sec:    15745, Lr: 0.000300\n",
      "2021-07-25 13:01:04,926 - INFO - joeynmt.training - Epoch  22, Step:   274200, Batch Loss:     1.711983, Tokens per Sec:    15668, Lr: 0.000300\n",
      "2021-07-25 13:01:18,990 - INFO - joeynmt.training - Epoch  22, Step:   274300, Batch Loss:     1.787942, Tokens per Sec:    15626, Lr: 0.000300\n",
      "2021-07-25 13:01:32,911 - INFO - joeynmt.training - Epoch  22, Step:   274400, Batch Loss:     1.747542, Tokens per Sec:    15823, Lr: 0.000300\n",
      "2021-07-25 13:01:46,871 - INFO - joeynmt.training - Epoch  22, Step:   274500, Batch Loss:     1.803470, Tokens per Sec:    16145, Lr: 0.000300\n",
      "2021-07-25 13:02:00,903 - INFO - joeynmt.training - Epoch  22, Step:   274600, Batch Loss:     1.803265, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-07-25 13:02:14,944 - INFO - joeynmt.training - Epoch  22, Step:   274700, Batch Loss:     1.696388, Tokens per Sec:    15785, Lr: 0.000300\n",
      "2021-07-25 13:02:28,935 - INFO - joeynmt.training - Epoch  22, Step:   274800, Batch Loss:     1.832289, Tokens per Sec:    16013, Lr: 0.000300\n",
      "2021-07-25 13:02:42,750 - INFO - joeynmt.training - Epoch  22, Step:   274900, Batch Loss:     1.788269, Tokens per Sec:    16074, Lr: 0.000300\n",
      "2021-07-25 13:02:56,441 - INFO - joeynmt.training - Epoch  22, Step:   275000, Batch Loss:     1.899842, Tokens per Sec:    15588, Lr: 0.000300\n",
      "2021-07-25 13:03:10,130 - INFO - joeynmt.training - Epoch  22, Step:   275100, Batch Loss:     1.653846, Tokens per Sec:    15614, Lr: 0.000300\n",
      "2021-07-25 13:03:24,068 - INFO - joeynmt.training - Epoch  22, Step:   275200, Batch Loss:     1.752077, Tokens per Sec:    15954, Lr: 0.000300\n",
      "2021-07-25 13:03:38,053 - INFO - joeynmt.training - Epoch  22, Step:   275300, Batch Loss:     1.623583, Tokens per Sec:    15819, Lr: 0.000300\n",
      "2021-07-25 13:03:51,883 - INFO - joeynmt.training - Epoch  22, Step:   275400, Batch Loss:     1.602155, Tokens per Sec:    15929, Lr: 0.000300\n",
      "2021-07-25 13:04:05,759 - INFO - joeynmt.training - Epoch  22, Step:   275500, Batch Loss:     1.741281, Tokens per Sec:    15914, Lr: 0.000300\n",
      "2021-07-25 13:04:19,659 - INFO - joeynmt.training - Epoch  22, Step:   275600, Batch Loss:     1.929518, Tokens per Sec:    15468, Lr: 0.000300\n",
      "2021-07-25 13:04:33,720 - INFO - joeynmt.training - Epoch  22, Step:   275700, Batch Loss:     1.761610, Tokens per Sec:    15749, Lr: 0.000300\n",
      "2021-07-25 13:04:47,658 - INFO - joeynmt.training - Epoch  22, Step:   275800, Batch Loss:     1.830422, Tokens per Sec:    15491, Lr: 0.000300\n",
      "2021-07-25 13:05:01,718 - INFO - joeynmt.training - Epoch  22, Step:   275900, Batch Loss:     1.511791, Tokens per Sec:    15628, Lr: 0.000300\n",
      "2021-07-25 13:05:15,506 - INFO - joeynmt.training - Epoch  22, Step:   276000, Batch Loss:     1.417502, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-07-25 13:05:37,410 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 13:05:37,410 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 13:05:37,410 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 13:05:37,658 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 13:05:37,658 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 13:05:38,666 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 13:05:38,667 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 13:05:38,667 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 13:05:38,668 - INFO - joeynmt.training - \tHypothesis: I was touched by my heart .\n",
      "2021-07-25 13:05:38,668 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 13:05:38,668 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 13:05:38,668 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 13:05:38,668 - INFO - joeynmt.training - \tHypothesis: The text was written on the pillar on the front of the scroll .\n",
      "2021-07-25 13:05:38,668 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 13:05:38,669 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 13:05:38,669 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 13:05:38,669 - INFO - joeynmt.training - \tHypothesis: Instead of worrying or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 13:05:38,669 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 13:05:38,670 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 13:05:38,670 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 13:05:38,670 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in a sense , that they have been well - fallen in Satan’s world .\n",
      "2021-07-25 13:05:38,670 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step   276000: bleu:  27.32, loss: 42663.1133, ppl:   4.6456, duration: 23.1635s\n",
      "2021-07-25 13:05:52,686 - INFO - joeynmt.training - Epoch  22, Step:   276100, Batch Loss:     1.743614, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-07-25 13:06:06,628 - INFO - joeynmt.training - Epoch  22, Step:   276200, Batch Loss:     1.761933, Tokens per Sec:    15530, Lr: 0.000300\n",
      "2021-07-25 13:06:20,711 - INFO - joeynmt.training - Epoch  22, Step:   276300, Batch Loss:     1.763721, Tokens per Sec:    15700, Lr: 0.000300\n",
      "2021-07-25 13:06:34,580 - INFO - joeynmt.training - Epoch  22, Step:   276400, Batch Loss:     1.652887, Tokens per Sec:    15821, Lr: 0.000300\n",
      "2021-07-25 13:06:48,519 - INFO - joeynmt.training - Epoch  22, Step:   276500, Batch Loss:     1.739247, Tokens per Sec:    15996, Lr: 0.000300\n",
      "2021-07-25 13:07:02,527 - INFO - joeynmt.training - Epoch  22, Step:   276600, Batch Loss:     1.547887, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-07-25 13:07:16,562 - INFO - joeynmt.training - Epoch  22, Step:   276700, Batch Loss:     1.868765, Tokens per Sec:    15692, Lr: 0.000300\n",
      "2021-07-25 13:07:30,539 - INFO - joeynmt.training - Epoch  22, Step:   276800, Batch Loss:     1.562831, Tokens per Sec:    15582, Lr: 0.000300\n",
      "2021-07-25 13:07:44,381 - INFO - joeynmt.training - Epoch  22, Step:   276900, Batch Loss:     1.637332, Tokens per Sec:    15953, Lr: 0.000300\n",
      "2021-07-25 13:07:58,298 - INFO - joeynmt.training - Epoch  22, Step:   277000, Batch Loss:     1.739352, Tokens per Sec:    15645, Lr: 0.000300\n",
      "2021-07-25 13:08:12,305 - INFO - joeynmt.training - Epoch  22, Step:   277100, Batch Loss:     1.683091, Tokens per Sec:    15700, Lr: 0.000300\n",
      "2021-07-25 13:08:26,323 - INFO - joeynmt.training - Epoch  22, Step:   277200, Batch Loss:     1.778968, Tokens per Sec:    15685, Lr: 0.000300\n",
      "2021-07-25 13:08:40,200 - INFO - joeynmt.training - Epoch  22, Step:   277300, Batch Loss:     1.873611, Tokens per Sec:    15828, Lr: 0.000300\n",
      "2021-07-25 13:08:54,189 - INFO - joeynmt.training - Epoch  22, Step:   277400, Batch Loss:     1.435189, Tokens per Sec:    15900, Lr: 0.000300\n",
      "2021-07-25 13:09:08,106 - INFO - joeynmt.training - Epoch  22, Step:   277500, Batch Loss:     1.530632, Tokens per Sec:    15794, Lr: 0.000300\n",
      "2021-07-25 13:09:22,080 - INFO - joeynmt.training - Epoch  22, Step:   277600, Batch Loss:     1.732794, Tokens per Sec:    15802, Lr: 0.000300\n",
      "2021-07-25 13:09:36,112 - INFO - joeynmt.training - Epoch  22, Step:   277700, Batch Loss:     1.651618, Tokens per Sec:    15379, Lr: 0.000300\n",
      "2021-07-25 13:09:50,057 - INFO - joeynmt.training - Epoch  22, Step:   277800, Batch Loss:     1.702371, Tokens per Sec:    15682, Lr: 0.000300\n",
      "2021-07-25 13:10:03,734 - INFO - joeynmt.training - Epoch  22, Step:   277900, Batch Loss:     1.755937, Tokens per Sec:    15764, Lr: 0.000300\n",
      "2021-07-25 13:10:17,796 - INFO - joeynmt.training - Epoch  22, Step:   278000, Batch Loss:     1.843499, Tokens per Sec:    15909, Lr: 0.000300\n",
      "2021-07-25 13:10:31,936 - INFO - joeynmt.training - Epoch  22, Step:   278100, Batch Loss:     1.599855, Tokens per Sec:    15553, Lr: 0.000300\n",
      "2021-07-25 13:10:45,709 - INFO - joeynmt.training - Epoch  22, Step:   278200, Batch Loss:     1.729841, Tokens per Sec:    15624, Lr: 0.000300\n",
      "2021-07-25 13:10:59,703 - INFO - joeynmt.training - Epoch  22, Step:   278300, Batch Loss:     1.678692, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 13:11:13,734 - INFO - joeynmt.training - Epoch  22, Step:   278400, Batch Loss:     1.945241, Tokens per Sec:    15493, Lr: 0.000300\n",
      "2021-07-25 13:11:27,596 - INFO - joeynmt.training - Epoch  22, Step:   278500, Batch Loss:     1.715996, Tokens per Sec:    15642, Lr: 0.000300\n",
      "2021-07-25 13:11:41,749 - INFO - joeynmt.training - Epoch  22, Step:   278600, Batch Loss:     1.641662, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-25 13:11:55,810 - INFO - joeynmt.training - Epoch  22, Step:   278700, Batch Loss:     1.729285, Tokens per Sec:    15706, Lr: 0.000300\n",
      "2021-07-25 13:12:09,787 - INFO - joeynmt.training - Epoch  22, Step:   278800, Batch Loss:     1.729576, Tokens per Sec:    15716, Lr: 0.000300\n",
      "2021-07-25 13:12:23,575 - INFO - joeynmt.training - Epoch  22, Step:   278900, Batch Loss:     1.880356, Tokens per Sec:    15500, Lr: 0.000300\n",
      "2021-07-25 13:12:37,489 - INFO - joeynmt.training - Epoch  22, Step:   279000, Batch Loss:     1.679279, Tokens per Sec:    15418, Lr: 0.000300\n",
      "2021-07-25 13:12:51,618 - INFO - joeynmt.training - Epoch  22, Step:   279100, Batch Loss:     1.614544, Tokens per Sec:    15987, Lr: 0.000300\n",
      "2021-07-25 13:13:05,619 - INFO - joeynmt.training - Epoch  22, Step:   279200, Batch Loss:     1.730262, Tokens per Sec:    15912, Lr: 0.000300\n",
      "2021-07-25 13:13:19,751 - INFO - joeynmt.training - Epoch  22, Step:   279300, Batch Loss:     1.811895, Tokens per Sec:    15704, Lr: 0.000300\n",
      "2021-07-25 13:13:29,557 - INFO - joeynmt.training - Epoch  22: total training loss 9264.35\n",
      "2021-07-25 13:13:29,557 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-25 13:13:34,438 - INFO - joeynmt.training - Epoch  23, Step:   279400, Batch Loss:     1.520295, Tokens per Sec:    13398, Lr: 0.000300\n",
      "2021-07-25 13:13:48,280 - INFO - joeynmt.training - Epoch  23, Step:   279500, Batch Loss:     1.616137, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-07-25 13:14:02,286 - INFO - joeynmt.training - Epoch  23, Step:   279600, Batch Loss:     1.783369, Tokens per Sec:    15765, Lr: 0.000300\n",
      "2021-07-25 13:14:16,132 - INFO - joeynmt.training - Epoch  23, Step:   279700, Batch Loss:     1.751382, Tokens per Sec:    15794, Lr: 0.000300\n",
      "2021-07-25 13:14:30,149 - INFO - joeynmt.training - Epoch  23, Step:   279800, Batch Loss:     1.630260, Tokens per Sec:    15569, Lr: 0.000300\n",
      "2021-07-25 13:14:44,140 - INFO - joeynmt.training - Epoch  23, Step:   279900, Batch Loss:     1.849047, Tokens per Sec:    15755, Lr: 0.000300\n",
      "2021-07-25 13:14:58,091 - INFO - joeynmt.training - Epoch  23, Step:   280000, Batch Loss:     1.771150, Tokens per Sec:    15680, Lr: 0.000300\n",
      "2021-07-25 13:15:11,860 - INFO - joeynmt.training - Epoch  23, Step:   280100, Batch Loss:     1.797549, Tokens per Sec:    15771, Lr: 0.000300\n",
      "2021-07-25 13:15:25,883 - INFO - joeynmt.training - Epoch  23, Step:   280200, Batch Loss:     1.593319, Tokens per Sec:    15589, Lr: 0.000300\n",
      "2021-07-25 13:15:39,818 - INFO - joeynmt.training - Epoch  23, Step:   280300, Batch Loss:     1.669547, Tokens per Sec:    15386, Lr: 0.000300\n",
      "2021-07-25 13:15:53,742 - INFO - joeynmt.training - Epoch  23, Step:   280400, Batch Loss:     1.702694, Tokens per Sec:    15331, Lr: 0.000300\n",
      "2021-07-25 13:16:07,929 - INFO - joeynmt.training - Epoch  23, Step:   280500, Batch Loss:     1.780560, Tokens per Sec:    15868, Lr: 0.000300\n",
      "2021-07-25 13:16:21,745 - INFO - joeynmt.training - Epoch  23, Step:   280600, Batch Loss:     1.572592, Tokens per Sec:    15640, Lr: 0.000300\n",
      "2021-07-25 13:16:35,689 - INFO - joeynmt.training - Epoch  23, Step:   280700, Batch Loss:     2.000798, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-25 13:16:49,598 - INFO - joeynmt.training - Epoch  23, Step:   280800, Batch Loss:     1.733453, Tokens per Sec:    15776, Lr: 0.000300\n",
      "2021-07-25 13:17:03,719 - INFO - joeynmt.training - Epoch  23, Step:   280900, Batch Loss:     1.893749, Tokens per Sec:    15682, Lr: 0.000300\n",
      "2021-07-25 13:17:17,816 - INFO - joeynmt.training - Epoch  23, Step:   281000, Batch Loss:     1.806179, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-07-25 13:17:31,596 - INFO - joeynmt.training - Epoch  23, Step:   281100, Batch Loss:     1.839945, Tokens per Sec:    15797, Lr: 0.000300\n",
      "2021-07-25 13:17:45,583 - INFO - joeynmt.training - Epoch  23, Step:   281200, Batch Loss:     1.763945, Tokens per Sec:    15710, Lr: 0.000300\n",
      "2021-07-25 13:17:59,658 - INFO - joeynmt.training - Epoch  23, Step:   281300, Batch Loss:     1.705151, Tokens per Sec:    15856, Lr: 0.000300\n",
      "2021-07-25 13:18:13,889 - INFO - joeynmt.training - Epoch  23, Step:   281400, Batch Loss:     1.411845, Tokens per Sec:    15758, Lr: 0.000300\n",
      "2021-07-25 13:18:27,946 - INFO - joeynmt.training - Epoch  23, Step:   281500, Batch Loss:     1.698335, Tokens per Sec:    15574, Lr: 0.000300\n",
      "2021-07-25 13:18:41,710 - INFO - joeynmt.training - Epoch  23, Step:   281600, Batch Loss:     1.841950, Tokens per Sec:    15766, Lr: 0.000300\n",
      "2021-07-25 13:18:55,789 - INFO - joeynmt.training - Epoch  23, Step:   281700, Batch Loss:     1.638969, Tokens per Sec:    16037, Lr: 0.000300\n",
      "2021-07-25 13:19:09,743 - INFO - joeynmt.training - Epoch  23, Step:   281800, Batch Loss:     1.708788, Tokens per Sec:    15628, Lr: 0.000300\n",
      "2021-07-25 13:19:23,773 - INFO - joeynmt.training - Epoch  23, Step:   281900, Batch Loss:     1.692285, Tokens per Sec:    15742, Lr: 0.000300\n",
      "2021-07-25 13:19:37,689 - INFO - joeynmt.training - Epoch  23, Step:   282000, Batch Loss:     1.695250, Tokens per Sec:    15770, Lr: 0.000300\n",
      "2021-07-25 13:20:00,441 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 13:20:00,442 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 13:20:00,442 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 13:20:00,689 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-25 13:20:00,690 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-25 13:20:01,432 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 13:20:01,433 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 13:20:01,433 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 13:20:01,433 - INFO - joeynmt.training - \tHypothesis: I was touched by the heart .\n",
      "2021-07-25 13:20:01,433 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 13:20:01,434 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 13:20:01,434 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 13:20:01,434 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
      "2021-07-25 13:20:01,434 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 13:20:01,435 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 13:20:01,435 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 13:20:01,435 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 13:20:01,435 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 13:20:01,435 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 13:20:01,436 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 13:20:01,436 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in a sense , that they have been released in Satan’s world .\n",
      "2021-07-25 13:20:01,436 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step   282000: bleu:  27.77, loss: 42283.9453, ppl:   4.5826, duration: 23.7462s\n",
      "2021-07-25 13:20:15,419 - INFO - joeynmt.training - Epoch  23, Step:   282100, Batch Loss:     1.595821, Tokens per Sec:    15411, Lr: 0.000300\n",
      "2021-07-25 13:20:29,451 - INFO - joeynmt.training - Epoch  23, Step:   282200, Batch Loss:     1.902335, Tokens per Sec:    15699, Lr: 0.000300\n",
      "2021-07-25 13:20:43,342 - INFO - joeynmt.training - Epoch  23, Step:   282300, Batch Loss:     1.396972, Tokens per Sec:    15670, Lr: 0.000300\n",
      "2021-07-25 13:20:57,113 - INFO - joeynmt.training - Epoch  23, Step:   282400, Batch Loss:     1.739355, Tokens per Sec:    15728, Lr: 0.000300\n",
      "2021-07-25 13:21:10,991 - INFO - joeynmt.training - Epoch  23, Step:   282500, Batch Loss:     1.771499, Tokens per Sec:    15949, Lr: 0.000300\n",
      "2021-07-25 13:21:25,004 - INFO - joeynmt.training - Epoch  23, Step:   282600, Batch Loss:     1.612609, Tokens per Sec:    15370, Lr: 0.000300\n",
      "2021-07-25 13:21:39,032 - INFO - joeynmt.training - Epoch  23, Step:   282700, Batch Loss:     1.644775, Tokens per Sec:    15917, Lr: 0.000300\n",
      "2021-07-25 13:21:53,208 - INFO - joeynmt.training - Epoch  23, Step:   282800, Batch Loss:     1.728406, Tokens per Sec:    15960, Lr: 0.000300\n",
      "2021-07-25 13:22:07,209 - INFO - joeynmt.training - Epoch  23, Step:   282900, Batch Loss:     1.850902, Tokens per Sec:    15713, Lr: 0.000300\n",
      "2021-07-25 13:22:21,075 - INFO - joeynmt.training - Epoch  23, Step:   283000, Batch Loss:     1.579457, Tokens per Sec:    15791, Lr: 0.000300\n",
      "2021-07-25 13:22:35,070 - INFO - joeynmt.training - Epoch  23, Step:   283100, Batch Loss:     1.777075, Tokens per Sec:    15993, Lr: 0.000300\n",
      "2021-07-25 13:22:49,039 - INFO - joeynmt.training - Epoch  23, Step:   283200, Batch Loss:     1.875829, Tokens per Sec:    15751, Lr: 0.000300\n",
      "2021-07-25 13:23:02,840 - INFO - joeynmt.training - Epoch  23, Step:   283300, Batch Loss:     1.759615, Tokens per Sec:    15608, Lr: 0.000300\n",
      "2021-07-25 13:23:16,919 - INFO - joeynmt.training - Epoch  23, Step:   283400, Batch Loss:     1.752700, Tokens per Sec:    15826, Lr: 0.000300\n",
      "2021-07-25 13:23:30,903 - INFO - joeynmt.training - Epoch  23, Step:   283500, Batch Loss:     1.771384, Tokens per Sec:    15694, Lr: 0.000300\n",
      "2021-07-25 13:23:44,552 - INFO - joeynmt.training - Epoch  23, Step:   283600, Batch Loss:     1.819760, Tokens per Sec:    15502, Lr: 0.000300\n",
      "2021-07-25 13:23:58,490 - INFO - joeynmt.training - Epoch  23, Step:   283700, Batch Loss:     1.593122, Tokens per Sec:    16068, Lr: 0.000300\n",
      "2021-07-25 13:24:12,506 - INFO - joeynmt.training - Epoch  23, Step:   283800, Batch Loss:     1.666236, Tokens per Sec:    15655, Lr: 0.000300\n",
      "2021-07-25 13:24:26,749 - INFO - joeynmt.training - Epoch  23, Step:   283900, Batch Loss:     1.755651, Tokens per Sec:    15958, Lr: 0.000300\n",
      "2021-07-25 13:24:40,726 - INFO - joeynmt.training - Epoch  23, Step:   284000, Batch Loss:     1.815709, Tokens per Sec:    15796, Lr: 0.000300\n",
      "2021-07-25 13:24:54,481 - INFO - joeynmt.training - Epoch  23, Step:   284100, Batch Loss:     1.687733, Tokens per Sec:    15668, Lr: 0.000300\n",
      "2021-07-25 13:25:08,538 - INFO - joeynmt.training - Epoch  23, Step:   284200, Batch Loss:     1.809706, Tokens per Sec:    15864, Lr: 0.000300\n",
      "2021-07-25 13:25:22,494 - INFO - joeynmt.training - Epoch  23, Step:   284300, Batch Loss:     1.824960, Tokens per Sec:    15632, Lr: 0.000300\n",
      "2021-07-25 13:25:36,485 - INFO - joeynmt.training - Epoch  23, Step:   284400, Batch Loss:     1.800178, Tokens per Sec:    15690, Lr: 0.000300\n",
      "2021-07-25 13:25:50,509 - INFO - joeynmt.training - Epoch  23, Step:   284500, Batch Loss:     1.670022, Tokens per Sec:    16083, Lr: 0.000300\n",
      "2021-07-25 13:26:04,318 - INFO - joeynmt.training - Epoch  23, Step:   284600, Batch Loss:     1.739171, Tokens per Sec:    15642, Lr: 0.000300\n",
      "2021-07-25 13:26:18,373 - INFO - joeynmt.training - Epoch  23, Step:   284700, Batch Loss:     1.723534, Tokens per Sec:    15608, Lr: 0.000300\n",
      "2021-07-25 13:26:31,085 - INFO - joeynmt.training - Epoch  23: total training loss 9246.77\n",
      "2021-07-25 13:26:31,086 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-25 13:26:33,154 - INFO - joeynmt.training - Epoch  24, Step:   284800, Batch Loss:     1.890087, Tokens per Sec:    10127, Lr: 0.000300\n",
      "2021-07-25 13:26:47,227 - INFO - joeynmt.training - Epoch  24, Step:   284900, Batch Loss:     1.778930, Tokens per Sec:    15741, Lr: 0.000300\n",
      "2021-07-25 13:27:01,297 - INFO - joeynmt.training - Epoch  24, Step:   285000, Batch Loss:     1.698900, Tokens per Sec:    15767, Lr: 0.000300\n",
      "2021-07-25 13:27:15,226 - INFO - joeynmt.training - Epoch  24, Step:   285100, Batch Loss:     1.685389, Tokens per Sec:    15621, Lr: 0.000300\n",
      "2021-07-25 13:27:29,028 - INFO - joeynmt.training - Epoch  24, Step:   285200, Batch Loss:     1.853878, Tokens per Sec:    15832, Lr: 0.000300\n",
      "2021-07-25 13:27:43,039 - INFO - joeynmt.training - Epoch  24, Step:   285300, Batch Loss:     1.610136, Tokens per Sec:    15846, Lr: 0.000300\n",
      "2021-07-25 13:27:56,939 - INFO - joeynmt.training - Epoch  24, Step:   285400, Batch Loss:     1.547241, Tokens per Sec:    15754, Lr: 0.000300\n",
      "2021-07-25 13:28:10,965 - INFO - joeynmt.training - Epoch  24, Step:   285500, Batch Loss:     1.524151, Tokens per Sec:    15657, Lr: 0.000300\n",
      "2021-07-25 13:28:25,072 - INFO - joeynmt.training - Epoch  24, Step:   285600, Batch Loss:     1.635153, Tokens per Sec:    15793, Lr: 0.000300\n",
      "2021-07-25 13:28:38,749 - INFO - joeynmt.training - Epoch  24, Step:   285700, Batch Loss:     1.489302, Tokens per Sec:    15666, Lr: 0.000300\n",
      "2021-07-25 13:28:52,596 - INFO - joeynmt.training - Epoch  24, Step:   285800, Batch Loss:     1.646483, Tokens per Sec:    16147, Lr: 0.000300\n",
      "2021-07-25 13:29:06,437 - INFO - joeynmt.training - Epoch  24, Step:   285900, Batch Loss:     1.759218, Tokens per Sec:    15593, Lr: 0.000300\n",
      "2021-07-25 13:29:20,452 - INFO - joeynmt.training - Epoch  24, Step:   286000, Batch Loss:     1.654085, Tokens per Sec:    15428, Lr: 0.000300\n",
      "2021-07-25 13:29:34,621 - INFO - joeynmt.training - Epoch  24, Step:   286100, Batch Loss:     1.612506, Tokens per Sec:    16110, Lr: 0.000300\n",
      "2021-07-25 13:29:48,668 - INFO - joeynmt.training - Epoch  24, Step:   286200, Batch Loss:     1.718881, Tokens per Sec:    15902, Lr: 0.000300\n",
      "2021-07-25 13:30:02,650 - INFO - joeynmt.training - Epoch  24, Step:   286300, Batch Loss:     1.689499, Tokens per Sec:    15913, Lr: 0.000300\n",
      "2021-07-25 13:30:16,617 - INFO - joeynmt.training - Epoch  24, Step:   286400, Batch Loss:     1.736107, Tokens per Sec:    15592, Lr: 0.000300\n",
      "2021-07-25 13:30:30,747 - INFO - joeynmt.training - Epoch  24, Step:   286500, Batch Loss:     1.750651, Tokens per Sec:    15831, Lr: 0.000300\n",
      "2021-07-25 13:30:44,680 - INFO - joeynmt.training - Epoch  24, Step:   286600, Batch Loss:     1.744780, Tokens per Sec:    16007, Lr: 0.000300\n",
      "2021-07-25 13:30:58,395 - INFO - joeynmt.training - Epoch  24, Step:   286700, Batch Loss:     1.887794, Tokens per Sec:    15599, Lr: 0.000300\n",
      "2021-07-25 13:31:12,393 - INFO - joeynmt.training - Epoch  24, Step:   286800, Batch Loss:     1.658097, Tokens per Sec:    15922, Lr: 0.000300\n",
      "2021-07-25 13:31:26,473 - INFO - joeynmt.training - Epoch  24, Step:   286900, Batch Loss:     1.811222, Tokens per Sec:    15920, Lr: 0.000300\n",
      "2021-07-25 13:31:40,404 - INFO - joeynmt.training - Epoch  24, Step:   287000, Batch Loss:     1.841125, Tokens per Sec:    15640, Lr: 0.000300\n",
      "2021-07-25 13:31:54,311 - INFO - joeynmt.training - Epoch  24, Step:   287100, Batch Loss:     1.507286, Tokens per Sec:    15888, Lr: 0.000300\n",
      "2021-07-25 13:32:08,058 - INFO - joeynmt.training - Epoch  24, Step:   287200, Batch Loss:     1.862272, Tokens per Sec:    15705, Lr: 0.000300\n",
      "2021-07-25 13:32:21,854 - INFO - joeynmt.training - Epoch  24, Step:   287300, Batch Loss:     1.705433, Tokens per Sec:    15759, Lr: 0.000300\n",
      "2021-07-25 13:32:35,954 - INFO - joeynmt.training - Epoch  24, Step:   287400, Batch Loss:     1.688119, Tokens per Sec:    15910, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_rwen_reload2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNm-DzIqm4Is"
   },
   "source": [
    "24 epochs done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd1YLR0d_ZAs"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 288000\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/latest.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer2_continued2/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rwen_reverse_transformer2\"', f'model_dir: \"models/rwen_reverse_transformer2_continued3\"').replace(\n",
    "            f'epochs: 15', f'epochs: 7').replace(f'validation_freq: 5000', f'validation_freq: 6000')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "S5vZXi6i_Y7Z",
    "outputId": "b1b6bf7a-6c3a-435c-db01-01810d22a3d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rwen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued2/288000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 3600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 7                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 6000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rwen_reverse_transformer2_continued3\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "    save_latest_ckpt: True\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_{name}_reload3.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rtr8k3akAqvP",
    "outputId": "0d1e1c83-18db-42f2-919a-0378e569eea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-25 14:15:19,185 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-25 14:15:19,262 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-25 14:15:29,907 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-25 14:15:30,227 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-25 14:15:31,274 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-25 14:15:32,322 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-25 14:15:32,322 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-25 14:15:32,743 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-25 14:15:34.376676: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-25 14:15:36,502 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-07-25 14:15:45,191 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued2/288000.ckpt\n",
      "2021-07-25 14:15:45,752 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-07-25 14:15:45,752 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-07-25 14:15:45,752 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-25 14:15:45,753 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-07-25 14:15:45,753 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-07-25 14:15:45,753 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-07-25 14:15:45,753 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-25 14:15:45,754 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-25 14:15:45,754 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-25 14:15:45,754 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\n",
      "2021-07-25 14:15:45,754 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\n",
      "2021-07-25 14:15:45,755 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-25 14:15:45,755 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-25 14:15:45,755 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued2/288000.ckpt\n",
      "2021-07-25 14:15:45,755 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-25 14:15:45,756 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-25 14:15:45,756 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-25 14:15:45,756 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-25 14:15:45,756 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-25 14:15:45,757 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-25 14:15:45,757 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-25 14:15:45,757 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-25 14:15:45,757 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-25 14:15:45,758 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-25 14:15:45,758 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-25 14:15:45,758 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-25 14:15:45,758 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-25 14:15:45,759 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-25 14:15:45,759 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-25 14:15:45,759 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-25 14:15:45,759 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-25 14:15:45,760 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-25 14:15:45,760 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-25 14:15:45,760 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-25 14:15:45,760 - INFO - joeynmt.helpers - cfg.training.epochs                : 7\n",
      "2021-07-25 14:15:45,761 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 6000\n",
      "2021-07-25 14:15:45,761 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-25 14:15:45,761 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-25 14:15:45,761 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer2_continued3\n",
      "2021-07-25 14:15:45,762 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-25 14:15:45,762 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-25 14:15:45,762 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-25 14:15:45,762 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-25 14:15:45,763 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-25 14:15:45,763 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-25 14:15:45,763 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True\n",
      "2021-07-25 14:15:45,763 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-25 14:15:45,764 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-25 14:15:45,764 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-25 14:15:45,765 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-25 14:15:45,765 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-25 14:15:45,765 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-25 14:15:45,765 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-25 14:15:45,766 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-25 14:15:45,766 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-25 14:15:45,766 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-25 14:15:45,766 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-25 14:15:45,767 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-25 14:15:45,767 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-25 14:15:45,767 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-25 14:15:45,767 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-25 14:15:45,768 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-25 14:15:45,768 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-25 14:15:45,768 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-25 14:15:45,768 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-25 14:15:45,769 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-25 14:15:45,769 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-25 14:15:45,769 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-25 14:15:45,769 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-25 14:15:45,770 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-25 14:15:45,770 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-25 14:15:45,770 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434519,\n",
      "\tvalid 1000,\n",
      "\ttest 2651\n",
      "2021-07-25 14:15:45,770 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-07-25 14:15:45,771 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-25 14:15:45,771 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-25 14:15:45,771 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-07-25 14:15:45,771 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-07-25 14:15:45,772 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-07-25 14:15:45,790 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-25 14:15:45,790 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-25 14:16:17,818 - INFO - joeynmt.training - Epoch   1, Step:   288100, Batch Loss:     1.779779, Tokens per Sec:     6815, Lr: 0.000300\n",
      "2021-07-25 14:16:48,050 - INFO - joeynmt.training - Epoch   1, Step:   288200, Batch Loss:     1.468801, Tokens per Sec:     7197, Lr: 0.000300\n",
      "2021-07-25 14:17:18,205 - INFO - joeynmt.training - Epoch   1, Step:   288300, Batch Loss:     1.642437, Tokens per Sec:     7124, Lr: 0.000300\n",
      "2021-07-25 14:17:48,109 - INFO - joeynmt.training - Epoch   1, Step:   288400, Batch Loss:     1.690280, Tokens per Sec:     7123, Lr: 0.000300\n",
      "2021-07-25 14:18:18,441 - INFO - joeynmt.training - Epoch   1, Step:   288500, Batch Loss:     1.936443, Tokens per Sec:     7300, Lr: 0.000300\n",
      "2021-07-25 14:18:48,379 - INFO - joeynmt.training - Epoch   1, Step:   288600, Batch Loss:     1.652585, Tokens per Sec:     7250, Lr: 0.000300\n",
      "2021-07-25 14:19:18,314 - INFO - joeynmt.training - Epoch   1, Step:   288700, Batch Loss:     1.586011, Tokens per Sec:     7195, Lr: 0.000300\n",
      "2021-07-25 14:19:48,680 - INFO - joeynmt.training - Epoch   1, Step:   288800, Batch Loss:     1.734314, Tokens per Sec:     7248, Lr: 0.000300\n",
      "2021-07-25 14:20:19,065 - INFO - joeynmt.training - Epoch   1, Step:   288900, Batch Loss:     1.713174, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-07-25 14:20:48,856 - INFO - joeynmt.training - Epoch   1, Step:   289000, Batch Loss:     1.958907, Tokens per Sec:     7184, Lr: 0.000300\n",
      "2021-07-25 14:21:19,363 - INFO - joeynmt.training - Epoch   1, Step:   289100, Batch Loss:     1.740958, Tokens per Sec:     7408, Lr: 0.000300\n",
      "2021-07-25 14:21:49,376 - INFO - joeynmt.training - Epoch   1, Step:   289200, Batch Loss:     2.135643, Tokens per Sec:     7251, Lr: 0.000300\n",
      "2021-07-25 14:22:19,257 - INFO - joeynmt.training - Epoch   1, Step:   289300, Batch Loss:     1.733684, Tokens per Sec:     7243, Lr: 0.000300\n",
      "2021-07-25 14:22:49,775 - INFO - joeynmt.training - Epoch   1, Step:   289400, Batch Loss:     1.499946, Tokens per Sec:     7372, Lr: 0.000300\n",
      "2021-07-25 14:23:20,170 - INFO - joeynmt.training - Epoch   1, Step:   289500, Batch Loss:     1.727717, Tokens per Sec:     7350, Lr: 0.000300\n",
      "2021-07-25 14:23:50,262 - INFO - joeynmt.training - Epoch   1, Step:   289600, Batch Loss:     1.487138, Tokens per Sec:     7270, Lr: 0.000300\n",
      "2021-07-25 14:24:20,522 - INFO - joeynmt.training - Epoch   1, Step:   289700, Batch Loss:     1.924889, Tokens per Sec:     7285, Lr: 0.000300\n",
      "2021-07-25 14:24:50,781 - INFO - joeynmt.training - Epoch   1, Step:   289800, Batch Loss:     1.931805, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-25 14:25:21,034 - INFO - joeynmt.training - Epoch   1, Step:   289900, Batch Loss:     1.611783, Tokens per Sec:     7233, Lr: 0.000300\n",
      "2021-07-25 14:25:51,402 - INFO - joeynmt.training - Epoch   1, Step:   290000, Batch Loss:     1.629703, Tokens per Sec:     7429, Lr: 0.000300\n",
      "2021-07-25 14:26:21,644 - INFO - joeynmt.training - Epoch   1, Step:   290100, Batch Loss:     1.677235, Tokens per Sec:     7276, Lr: 0.000300\n",
      "2021-07-25 14:26:51,805 - INFO - joeynmt.training - Epoch   1, Step:   290200, Batch Loss:     1.810799, Tokens per Sec:     7268, Lr: 0.000300\n",
      "2021-07-25 14:26:55,609 - INFO - joeynmt.training - Epoch   1: total training loss 3777.37\n",
      "2021-07-25 14:26:55,610 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-25 14:27:22,536 - INFO - joeynmt.training - Epoch   2, Step:   290300, Batch Loss:     1.669135, Tokens per Sec:     7088, Lr: 0.000300\n",
      "2021-07-25 14:27:52,515 - INFO - joeynmt.training - Epoch   2, Step:   290400, Batch Loss:     1.613355, Tokens per Sec:     7208, Lr: 0.000300\n",
      "2021-07-25 14:28:22,647 - INFO - joeynmt.training - Epoch   2, Step:   290500, Batch Loss:     1.688722, Tokens per Sec:     7356, Lr: 0.000300\n",
      "2021-07-25 14:28:52,426 - INFO - joeynmt.training - Epoch   2, Step:   290600, Batch Loss:     1.820927, Tokens per Sec:     7147, Lr: 0.000300\n",
      "2021-07-25 14:29:22,702 - INFO - joeynmt.training - Epoch   2, Step:   290700, Batch Loss:     1.749670, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-07-25 14:29:52,873 - INFO - joeynmt.training - Epoch   2, Step:   290800, Batch Loss:     2.003525, Tokens per Sec:     7290, Lr: 0.000300\n",
      "2021-07-25 14:30:22,587 - INFO - joeynmt.training - Epoch   2, Step:   290900, Batch Loss:     1.661702, Tokens per Sec:     7197, Lr: 0.000300\n",
      "2021-07-25 14:30:53,251 - INFO - joeynmt.training - Epoch   2, Step:   291000, Batch Loss:     1.603918, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-07-25 14:31:23,466 - INFO - joeynmt.training - Epoch   2, Step:   291100, Batch Loss:     1.633205, Tokens per Sec:     7293, Lr: 0.000300\n",
      "2021-07-25 14:31:53,505 - INFO - joeynmt.training - Epoch   2, Step:   291200, Batch Loss:     1.735116, Tokens per Sec:     7304, Lr: 0.000300\n",
      "2021-07-25 14:32:23,662 - INFO - joeynmt.training - Epoch   2, Step:   291300, Batch Loss:     1.669569, Tokens per Sec:     7241, Lr: 0.000300\n",
      "2021-07-25 14:32:53,877 - INFO - joeynmt.training - Epoch   2, Step:   291400, Batch Loss:     1.557832, Tokens per Sec:     7437, Lr: 0.000300\n",
      "2021-07-25 14:33:24,320 - INFO - joeynmt.training - Epoch   2, Step:   291500, Batch Loss:     1.725431, Tokens per Sec:     7312, Lr: 0.000300\n",
      "2021-07-25 14:33:54,721 - INFO - joeynmt.training - Epoch   2, Step:   291600, Batch Loss:     1.755058, Tokens per Sec:     7333, Lr: 0.000300\n",
      "2021-07-25 14:34:24,901 - INFO - joeynmt.training - Epoch   2, Step:   291700, Batch Loss:     1.604918, Tokens per Sec:     7192, Lr: 0.000300\n",
      "2021-07-25 14:34:55,014 - INFO - joeynmt.training - Epoch   2, Step:   291800, Batch Loss:     1.573966, Tokens per Sec:     7284, Lr: 0.000300\n",
      "2021-07-25 14:35:25,282 - INFO - joeynmt.training - Epoch   2, Step:   291900, Batch Loss:     1.622115, Tokens per Sec:     7158, Lr: 0.000300\n",
      "2021-07-25 14:35:55,579 - INFO - joeynmt.training - Epoch   2, Step:   292000, Batch Loss:     1.601744, Tokens per Sec:     7262, Lr: 0.000300\n",
      "2021-07-25 14:36:25,719 - INFO - joeynmt.training - Epoch   2, Step:   292100, Batch Loss:     1.674308, Tokens per Sec:     7240, Lr: 0.000300\n",
      "2021-07-25 14:36:55,833 - INFO - joeynmt.training - Epoch   2, Step:   292200, Batch Loss:     1.704835, Tokens per Sec:     7186, Lr: 0.000300\n",
      "2021-07-25 14:37:26,314 - INFO - joeynmt.training - Epoch   2, Step:   292300, Batch Loss:     1.737394, Tokens per Sec:     7360, Lr: 0.000300\n",
      "2021-07-25 14:37:56,559 - INFO - joeynmt.training - Epoch   2, Step:   292400, Batch Loss:     1.818126, Tokens per Sec:     7321, Lr: 0.000300\n",
      "2021-07-25 14:38:26,427 - INFO - joeynmt.training - Epoch   2, Step:   292500, Batch Loss:     1.581395, Tokens per Sec:     7285, Lr: 0.000300\n",
      "2021-07-25 14:38:56,348 - INFO - joeynmt.training - Epoch   2, Step:   292600, Batch Loss:     1.747820, Tokens per Sec:     7199, Lr: 0.000300\n",
      "2021-07-25 14:39:26,660 - INFO - joeynmt.training - Epoch   2, Step:   292700, Batch Loss:     1.569197, Tokens per Sec:     7227, Lr: 0.000300\n",
      "2021-07-25 14:39:56,892 - INFO - joeynmt.training - Epoch   2, Step:   292800, Batch Loss:     1.720807, Tokens per Sec:     7308, Lr: 0.000300\n",
      "2021-07-25 14:40:27,398 - INFO - joeynmt.training - Epoch   2, Step:   292900, Batch Loss:     1.636293, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-07-25 14:40:57,415 - INFO - joeynmt.training - Epoch   2, Step:   293000, Batch Loss:     1.787035, Tokens per Sec:     7267, Lr: 0.000300\n",
      "2021-07-25 14:41:27,261 - INFO - joeynmt.training - Epoch   2, Step:   293100, Batch Loss:     1.662591, Tokens per Sec:     7189, Lr: 0.000300\n",
      "2021-07-25 14:41:57,595 - INFO - joeynmt.training - Epoch   2, Step:   293200, Batch Loss:     1.745528, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-07-25 14:42:27,955 - INFO - joeynmt.training - Epoch   2, Step:   293300, Batch Loss:     1.754854, Tokens per Sec:     7382, Lr: 0.000300\n",
      "2021-07-25 14:42:58,310 - INFO - joeynmt.training - Epoch   2, Step:   293400, Batch Loss:     1.730870, Tokens per Sec:     7335, Lr: 0.000300\n",
      "2021-07-25 14:43:28,523 - INFO - joeynmt.training - Epoch   2, Step:   293500, Batch Loss:     1.486779, Tokens per Sec:     7266, Lr: 0.000300\n",
      "2021-07-25 14:43:58,502 - INFO - joeynmt.training - Epoch   2, Step:   293600, Batch Loss:     1.675139, Tokens per Sec:     7368, Lr: 0.000300\n",
      "2021-07-25 14:44:28,673 - INFO - joeynmt.training - Epoch   2, Step:   293700, Batch Loss:     1.598013, Tokens per Sec:     7144, Lr: 0.000300\n",
      "2021-07-25 14:44:58,868 - INFO - joeynmt.training - Epoch   2, Step:   293800, Batch Loss:     1.821012, Tokens per Sec:     7307, Lr: 0.000300\n",
      "2021-07-25 14:45:28,465 - INFO - joeynmt.training - Epoch   2, Step:   293900, Batch Loss:     1.740096, Tokens per Sec:     7176, Lr: 0.000300\n",
      "2021-07-25 14:45:59,170 - INFO - joeynmt.training - Epoch   2, Step:   294000, Batch Loss:     1.605192, Tokens per Sec:     7484, Lr: 0.000300\n",
      "2021-07-25 14:46:51,910 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 14:46:51,910 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 14:46:51,911 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 14:46:53,129 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 14:46:53,130 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 14:46:53,130 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 14:46:53,130 - INFO - joeynmt.training - \tHypothesis: I touched my heart .\n",
      "2021-07-25 14:46:53,131 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 14:46:53,131 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 14:46:53,132 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 14:46:53,132 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
      "2021-07-25 14:46:53,132 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 14:46:53,133 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 14:46:53,133 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 14:46:53,133 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 14:46:53,133 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 14:46:53,134 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 14:46:53,134 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 14:46:53,135 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 14:46:53,135 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   294000: bleu:  27.77, loss: 42380.7266, ppl:   4.5986, duration: 53.9638s\n",
      "2021-07-25 14:47:23,329 - INFO - joeynmt.training - Epoch   2, Step:   294100, Batch Loss:     1.779071, Tokens per Sec:     7268, Lr: 0.000300\n",
      "2021-07-25 14:47:53,504 - INFO - joeynmt.training - Epoch   2, Step:   294200, Batch Loss:     1.660570, Tokens per Sec:     7282, Lr: 0.000300\n",
      "2021-07-25 14:48:23,454 - INFO - joeynmt.training - Epoch   2, Step:   294300, Batch Loss:     1.686194, Tokens per Sec:     7279, Lr: 0.000300\n",
      "2021-07-25 14:48:53,724 - INFO - joeynmt.training - Epoch   2, Step:   294400, Batch Loss:     1.509862, Tokens per Sec:     7336, Lr: 0.000300\n",
      "2021-07-25 14:49:23,770 - INFO - joeynmt.training - Epoch   2, Step:   294500, Batch Loss:     1.705928, Tokens per Sec:     7290, Lr: 0.000300\n",
      "2021-07-25 14:49:53,415 - INFO - joeynmt.training - Epoch   2, Step:   294600, Batch Loss:     1.578245, Tokens per Sec:     7224, Lr: 0.000300\n",
      "2021-07-25 14:50:23,388 - INFO - joeynmt.training - Epoch   2, Step:   294700, Batch Loss:     1.490476, Tokens per Sec:     7291, Lr: 0.000300\n",
      "2021-07-25 14:50:53,512 - INFO - joeynmt.training - Epoch   2, Step:   294800, Batch Loss:     1.592018, Tokens per Sec:     7305, Lr: 0.000300\n",
      "2021-07-25 14:51:23,269 - INFO - joeynmt.training - Epoch   2, Step:   294900, Batch Loss:     1.925014, Tokens per Sec:     7135, Lr: 0.000300\n",
      "2021-07-25 14:51:53,546 - INFO - joeynmt.training - Epoch   2, Step:   295000, Batch Loss:     1.621861, Tokens per Sec:     7279, Lr: 0.000300\n",
      "2021-07-25 14:52:24,106 - INFO - joeynmt.training - Epoch   2, Step:   295100, Batch Loss:     1.716892, Tokens per Sec:     7256, Lr: 0.000300\n",
      "2021-07-25 14:52:54,182 - INFO - joeynmt.training - Epoch   2, Step:   295200, Batch Loss:     1.572359, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-07-25 14:53:24,221 - INFO - joeynmt.training - Epoch   2, Step:   295300, Batch Loss:     1.830982, Tokens per Sec:     7247, Lr: 0.000300\n",
      "2021-07-25 14:53:54,295 - INFO - joeynmt.training - Epoch   2, Step:   295400, Batch Loss:     1.623207, Tokens per Sec:     7225, Lr: 0.000300\n",
      "2021-07-25 14:54:24,626 - INFO - joeynmt.training - Epoch   2, Step:   295500, Batch Loss:     1.784102, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-07-25 14:54:54,756 - INFO - joeynmt.training - Epoch   2, Step:   295600, Batch Loss:     1.539758, Tokens per Sec:     7185, Lr: 0.000300\n",
      "2021-07-25 14:55:06,996 - INFO - joeynmt.training - Epoch   2: total training loss 9227.10\n",
      "2021-07-25 14:55:06,996 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-25 14:55:25,726 - INFO - joeynmt.training - Epoch   3, Step:   295700, Batch Loss:     1.463448, Tokens per Sec:     6980, Lr: 0.000300\n",
      "2021-07-25 14:55:56,237 - INFO - joeynmt.training - Epoch   3, Step:   295800, Batch Loss:     1.675053, Tokens per Sec:     7314, Lr: 0.000300\n",
      "2021-07-25 14:56:26,450 - INFO - joeynmt.training - Epoch   3, Step:   295900, Batch Loss:     1.640132, Tokens per Sec:     7190, Lr: 0.000300\n",
      "2021-07-25 14:56:56,657 - INFO - joeynmt.training - Epoch   3, Step:   296000, Batch Loss:     1.859711, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-07-25 14:57:26,464 - INFO - joeynmt.training - Epoch   3, Step:   296100, Batch Loss:     1.711207, Tokens per Sec:     7320, Lr: 0.000300\n",
      "2021-07-25 14:57:56,477 - INFO - joeynmt.training - Epoch   3, Step:   296200, Batch Loss:     1.677918, Tokens per Sec:     7226, Lr: 0.000300\n",
      "2021-07-25 14:58:26,551 - INFO - joeynmt.training - Epoch   3, Step:   296300, Batch Loss:     1.746874, Tokens per Sec:     7233, Lr: 0.000300\n",
      "2021-07-25 14:58:56,703 - INFO - joeynmt.training - Epoch   3, Step:   296400, Batch Loss:     1.815493, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-07-25 14:59:27,153 - INFO - joeynmt.training - Epoch   3, Step:   296500, Batch Loss:     1.784169, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-07-25 14:59:57,606 - INFO - joeynmt.training - Epoch   3, Step:   296600, Batch Loss:     1.737246, Tokens per Sec:     7367, Lr: 0.000300\n",
      "2021-07-25 15:00:28,134 - INFO - joeynmt.training - Epoch   3, Step:   296700, Batch Loss:     1.663550, Tokens per Sec:     7299, Lr: 0.000300\n",
      "2021-07-25 15:00:58,024 - INFO - joeynmt.training - Epoch   3, Step:   296800, Batch Loss:     1.659364, Tokens per Sec:     7336, Lr: 0.000300\n",
      "2021-07-25 15:01:27,883 - INFO - joeynmt.training - Epoch   3, Step:   296900, Batch Loss:     1.618723, Tokens per Sec:     7139, Lr: 0.000300\n",
      "2021-07-25 15:01:58,153 - INFO - joeynmt.training - Epoch   3, Step:   297000, Batch Loss:     1.803325, Tokens per Sec:     7217, Lr: 0.000300\n",
      "2021-07-25 15:02:28,526 - INFO - joeynmt.training - Epoch   3, Step:   297100, Batch Loss:     1.635978, Tokens per Sec:     7312, Lr: 0.000300\n",
      "2021-07-25 15:02:58,324 - INFO - joeynmt.training - Epoch   3, Step:   297200, Batch Loss:     1.510497, Tokens per Sec:     7105, Lr: 0.000300\n",
      "2021-07-25 15:03:28,333 - INFO - joeynmt.training - Epoch   3, Step:   297300, Batch Loss:     1.888488, Tokens per Sec:     7179, Lr: 0.000300\n",
      "2021-07-25 15:03:58,936 - INFO - joeynmt.training - Epoch   3, Step:   297400, Batch Loss:     1.713470, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-07-25 15:04:28,667 - INFO - joeynmt.training - Epoch   3, Step:   297500, Batch Loss:     1.580598, Tokens per Sec:     6975, Lr: 0.000300\n",
      "2021-07-25 15:04:58,734 - INFO - joeynmt.training - Epoch   3, Step:   297600, Batch Loss:     1.722961, Tokens per Sec:     7230, Lr: 0.000300\n",
      "2021-07-25 15:05:29,074 - INFO - joeynmt.training - Epoch   3, Step:   297700, Batch Loss:     1.787791, Tokens per Sec:     7289, Lr: 0.000300\n",
      "2021-07-25 15:05:59,311 - INFO - joeynmt.training - Epoch   3, Step:   297800, Batch Loss:     1.790835, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-07-25 15:06:29,684 - INFO - joeynmt.training - Epoch   3, Step:   297900, Batch Loss:     1.459644, Tokens per Sec:     7295, Lr: 0.000300\n",
      "2021-07-25 15:06:59,963 - INFO - joeynmt.training - Epoch   3, Step:   298000, Batch Loss:     1.554182, Tokens per Sec:     7210, Lr: 0.000300\n",
      "2021-07-25 15:07:29,879 - INFO - joeynmt.training - Epoch   3, Step:   298100, Batch Loss:     1.923125, Tokens per Sec:     7186, Lr: 0.000300\n",
      "2021-07-25 15:08:00,180 - INFO - joeynmt.training - Epoch   3, Step:   298200, Batch Loss:     1.591124, Tokens per Sec:     7284, Lr: 0.000300\n",
      "2021-07-25 15:08:30,438 - INFO - joeynmt.training - Epoch   3, Step:   298300, Batch Loss:     1.687475, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-07-25 15:09:00,749 - INFO - joeynmt.training - Epoch   3, Step:   298400, Batch Loss:     1.619014, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-07-25 15:09:30,665 - INFO - joeynmt.training - Epoch   3, Step:   298500, Batch Loss:     1.408402, Tokens per Sec:     7152, Lr: 0.000300\n",
      "2021-07-25 15:10:01,172 - INFO - joeynmt.training - Epoch   3, Step:   298600, Batch Loss:     1.475784, Tokens per Sec:     7439, Lr: 0.000300\n",
      "2021-07-25 15:10:31,480 - INFO - joeynmt.training - Epoch   3, Step:   298700, Batch Loss:     1.792160, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-07-25 15:11:01,177 - INFO - joeynmt.training - Epoch   3, Step:   298800, Batch Loss:     1.792803, Tokens per Sec:     7169, Lr: 0.000300\n",
      "2021-07-25 15:11:31,293 - INFO - joeynmt.training - Epoch   3, Step:   298900, Batch Loss:     1.874668, Tokens per Sec:     7215, Lr: 0.000300\n",
      "2021-07-25 15:12:01,468 - INFO - joeynmt.training - Epoch   3, Step:   299000, Batch Loss:     1.779640, Tokens per Sec:     7251, Lr: 0.000300\n",
      "2021-07-25 15:12:31,452 - INFO - joeynmt.training - Epoch   3, Step:   299100, Batch Loss:     1.529868, Tokens per Sec:     7274, Lr: 0.000300\n",
      "2021-07-25 15:13:01,766 - INFO - joeynmt.training - Epoch   3, Step:   299200, Batch Loss:     1.817686, Tokens per Sec:     7410, Lr: 0.000300\n",
      "2021-07-25 15:13:32,144 - INFO - joeynmt.training - Epoch   3, Step:   299300, Batch Loss:     1.694720, Tokens per Sec:     7222, Lr: 0.000300\n",
      "2021-07-25 15:14:02,341 - INFO - joeynmt.training - Epoch   3, Step:   299400, Batch Loss:     1.780838, Tokens per Sec:     7136, Lr: 0.000300\n",
      "2021-07-25 15:14:32,477 - INFO - joeynmt.training - Epoch   3, Step:   299500, Batch Loss:     1.722329, Tokens per Sec:     7293, Lr: 0.000300\n",
      "2021-07-25 15:15:02,580 - INFO - joeynmt.training - Epoch   3, Step:   299600, Batch Loss:     1.660631, Tokens per Sec:     7167, Lr: 0.000300\n",
      "2021-07-25 15:15:32,796 - INFO - joeynmt.training - Epoch   3, Step:   299700, Batch Loss:     1.417837, Tokens per Sec:     7235, Lr: 0.000300\n",
      "2021-07-25 15:16:02,742 - INFO - joeynmt.training - Epoch   3, Step:   299800, Batch Loss:     1.541079, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-07-25 15:16:32,563 - INFO - joeynmt.training - Epoch   3, Step:   299900, Batch Loss:     1.708393, Tokens per Sec:     7272, Lr: 0.000300\n",
      "2021-07-25 15:17:02,719 - INFO - joeynmt.training - Epoch   3, Step:   300000, Batch Loss:     1.568515, Tokens per Sec:     7330, Lr: 0.000300\n",
      "2021-07-25 15:17:56,238 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-25 15:17:56,238 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-25 15:17:56,239 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-25 15:17:57,479 - INFO - joeynmt.training - Example #0\n",
      "2021-07-25 15:17:57,479 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-25 15:17:57,480 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-25 15:17:57,480 - INFO - joeynmt.training - \tHypothesis: I was touched by my heart .\n",
      "2021-07-25 15:17:57,480 - INFO - joeynmt.training - Example #1\n",
      "2021-07-25 15:17:57,481 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-25 15:17:57,481 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-25 15:17:57,481 - INFO - joeynmt.training - \tHypothesis: The text was written on the pillar on the front of the scroll .\n",
      "2021-07-25 15:17:57,482 - INFO - joeynmt.training - Example #2\n",
      "2021-07-25 15:17:57,482 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-25 15:17:57,483 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 15:17:57,483 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-25 15:17:57,483 - INFO - joeynmt.training - Example #3\n",
      "2021-07-25 15:17:57,484 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-25 15:17:57,484 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-25 15:17:57,484 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-25 15:17:57,485 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   300000: bleu:  27.98, loss: 42546.2461, ppl:   4.6261, duration: 54.7651s\n",
      "2021-07-25 15:18:27,895 - INFO - joeynmt.training - Epoch   3, Step:   300100, Batch Loss:     1.744706, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-25 15:18:58,057 - INFO - joeynmt.training - Epoch   3, Step:   300200, Batch Loss:     1.808793, Tokens per Sec:     7288, Lr: 0.000300\n",
      "2021-07-25 15:19:28,652 - INFO - joeynmt.training - Epoch   3, Step:   300300, Batch Loss:     1.809644, Tokens per Sec:     7378, Lr: 0.000300\n",
      "2021-07-25 15:19:58,756 - INFO - joeynmt.training - Epoch   3, Step:   300400, Batch Loss:     1.826049, Tokens per Sec:     7269, Lr: 0.000300\n",
      "2021-07-25 15:20:28,873 - INFO - joeynmt.training - Epoch   3, Step:   300500, Batch Loss:     1.723103, Tokens per Sec:     7282, Lr: 0.000300\n",
      "2021-07-25 15:20:58,548 - INFO - joeynmt.training - Epoch   3, Step:   300600, Batch Loss:     1.577094, Tokens per Sec:     7340, Lr: 0.000300\n",
      "2021-07-25 15:21:28,862 - INFO - joeynmt.training - Epoch   3, Step:   300700, Batch Loss:     1.747277, Tokens per Sec:     7328, Lr: 0.000300\n",
      "2021-07-25 15:21:58,527 - INFO - joeynmt.training - Epoch   3, Step:   300800, Batch Loss:     1.623210, Tokens per Sec:     7192, Lr: 0.000300\n",
      "2021-07-25 15:22:28,843 - INFO - joeynmt.training - Epoch   3, Step:   300900, Batch Loss:     1.759088, Tokens per Sec:     7329, Lr: 0.000300\n",
      "2021-07-25 15:22:59,324 - INFO - joeynmt.training - Epoch   3, Step:   301000, Batch Loss:     1.719743, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-07-25 15:23:21,179 - INFO - joeynmt.training - Epoch   3: total training loss 9217.57\n",
      "2021-07-25 15:23:21,179 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-25 15:23:30,300 - INFO - joeynmt.training - Epoch   4, Step:   301100, Batch Loss:     1.673786, Tokens per Sec:     6810, Lr: 0.000300\n",
      "2021-07-25 15:24:00,556 - INFO - joeynmt.training - Epoch   4, Step:   301200, Batch Loss:     1.865636, Tokens per Sec:     7142, Lr: 0.000300\n",
      "2021-07-25 15:24:30,804 - INFO - joeynmt.training - Epoch   4, Step:   301300, Batch Loss:     1.370748, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-25 15:25:00,928 - INFO - joeynmt.training - Epoch   4, Step:   301400, Batch Loss:     1.761687, Tokens per Sec:     7205, Lr: 0.000300\n",
      "2021-07-25 15:25:30,769 - INFO - joeynmt.training - Epoch   4, Step:   301500, Batch Loss:     1.775184, Tokens per Sec:     7227, Lr: 0.000300\n",
      "2021-07-25 15:26:01,013 - INFO - joeynmt.training - Epoch   4, Step:   301600, Batch Loss:     1.681121, Tokens per Sec:     7294, Lr: 0.000300\n",
      "2021-07-25 15:26:31,251 - INFO - joeynmt.training - Epoch   4, Step:   301700, Batch Loss:     1.600039, Tokens per Sec:     7127, Lr: 0.000300\n",
      "2021-07-25 15:27:01,319 - INFO - joeynmt.training - Epoch   4, Step:   301800, Batch Loss:     1.813944, Tokens per Sec:     7292, Lr: 0.000300\n",
      "2021-07-25 15:27:31,702 - INFO - joeynmt.training - Epoch   4, Step:   301900, Batch Loss:     1.826647, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-07-25 15:28:01,724 - INFO - joeynmt.training - Epoch   4, Step:   302000, Batch Loss:     1.665958, Tokens per Sec:     7212, Lr: 0.000300\n",
      "2021-07-25 15:28:32,196 - INFO - joeynmt.training - Epoch   4, Step:   302100, Batch Loss:     1.553803, Tokens per Sec:     7260, Lr: 0.000300\n",
      "2021-07-25 15:29:02,760 - INFO - joeynmt.training - Epoch   4, Step:   302200, Batch Loss:     1.811871, Tokens per Sec:     7491, Lr: 0.000300\n",
      "2021-07-25 15:29:33,364 - INFO - joeynmt.training - Epoch   4, Step:   302300, Batch Loss:     1.562144, Tokens per Sec:     7289, Lr: 0.000300\n",
      "2021-07-25 15:30:03,896 - INFO - joeynmt.training - Epoch   4, Step:   302400, Batch Loss:     1.929100, Tokens per Sec:     7304, Lr: 0.000300\n",
      "2021-07-25 15:30:34,556 - INFO - joeynmt.training - Epoch   4, Step:   302500, Batch Loss:     1.691547, Tokens per Sec:     7326, Lr: 0.000300\n",
      "2021-07-25 15:31:04,765 - INFO - joeynmt.training - Epoch   4, Step:   302600, Batch Loss:     1.725891, Tokens per Sec:     7202, Lr: 0.000300\n",
      "2021-07-25 15:31:34,746 - INFO - joeynmt.training - Epoch   4, Step:   302700, Batch Loss:     1.586991, Tokens per Sec:     7311, Lr: 0.000300\n",
      "2021-07-25 15:32:04,468 - INFO - joeynmt.training - Epoch   4, Step:   302800, Batch Loss:     1.655605, Tokens per Sec:     7149, Lr: 0.000300\n",
      "2021-07-25 15:32:34,699 - INFO - joeynmt.training - Epoch   4, Step:   302900, Batch Loss:     1.700770, Tokens per Sec:     7242, Lr: 0.000300\n",
      "2021-07-25 15:33:05,183 - INFO - joeynmt.training - Epoch   4, Step:   303000, Batch Loss:     1.729592, Tokens per Sec:     7315, Lr: 0.000300\n",
      "2021-07-25 15:33:35,534 - INFO - joeynmt.training - Epoch   4, Step:   303100, Batch Loss:     1.718299, Tokens per Sec:     7232, Lr: 0.000300\n",
      "2021-07-25 15:34:05,795 - INFO - joeynmt.training - Epoch   4, Step:   303200, Batch Loss:     1.737569, Tokens per Sec:     7286, Lr: 0.000300\n",
      "2021-07-25 15:34:35,767 - INFO - joeynmt.training - Epoch   4, Step:   303300, Batch Loss:     1.642801, Tokens per Sec:     7134, Lr: 0.000300\n",
      "2021-07-25 15:35:05,985 - INFO - joeynmt.training - Epoch   4, Step:   303400, Batch Loss:     1.583416, Tokens per Sec:     7328, Lr: 0.000300\n",
      "2021-07-25 15:35:36,240 - INFO - joeynmt.training - Epoch   4, Step:   303500, Batch Loss:     1.758156, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-07-25 15:36:06,578 - INFO - joeynmt.training - Epoch   4, Step:   303600, Batch Loss:     1.805480, Tokens per Sec:     7354, Lr: 0.000300\n",
      "2021-07-25 15:36:37,174 - INFO - joeynmt.training - Epoch   4, Step:   303700, Batch Loss:     1.703351, Tokens per Sec:     7291, Lr: 0.000300\n",
      "2021-07-25 15:37:07,491 - INFO - joeynmt.training - Epoch   4, Step:   303800, Batch Loss:     1.755727, Tokens per Sec:     7315, Lr: 0.000300\n",
      "2021-07-25 15:37:37,421 - INFO - joeynmt.training - Epoch   4, Step:   303900, Batch Loss:     1.647134, Tokens per Sec:     7229, Lr: 0.000300\n",
      "2021-07-25 15:38:07,638 - INFO - joeynmt.training - Epoch   4, Step:   304000, Batch Loss:     1.641240, Tokens per Sec:     7307, Lr: 0.000300\n",
      "2021-07-25 15:38:37,997 - INFO - joeynmt.training - Epoch   4, Step:   304100, Batch Loss:     1.693425, Tokens per Sec:     7384, Lr: 0.000300\n",
      "2021-07-25 15:39:08,157 - INFO - joeynmt.training - Epoch   4, Step:   304200, Batch Loss:     1.659592, Tokens per Sec:     7112, Lr: 0.000300\n",
      "2021-07-25 15:39:38,316 - INFO - joeynmt.training - Epoch   4, Step:   304300, Batch Loss:     1.730411, Tokens per Sec:     7237, Lr: 0.000300\n",
      "2021-07-25 15:40:08,405 - INFO - joeynmt.training - Epoch   4, Step:   304400, Batch Loss:     1.646561, Tokens per Sec:     7185, Lr: 0.000300\n",
      "2021-07-25 15:40:38,744 - INFO - joeynmt.training - Epoch   4, Step:   304500, Batch Loss:     1.639422, Tokens per Sec:     7319, Lr: 0.000300\n",
      "2021-07-25 15:41:08,595 - INFO - joeynmt.training - Epoch   4, Step:   304600, Batch Loss:     1.516790, Tokens per Sec:     7157, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_rwen_reload3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6XM5Y0fnAfT"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 300000\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/latest.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/{name}_reverse_transformer2_continued3/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/rwen_reverse_transformer2\"', f'model_dir: \"models/rwen_reverse_transformer2_continued4\"').replace(\n",
    "            f'epochs: 15', f'epochs: 4').replace(f'validation_freq: 5000', f'validation_freq: 6000')\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}_reload4.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6sdSkjdzAqsJ",
    "outputId": "5dc68ec9-aa66-44b8-c686-7d9fe1f77e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"rwen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"rw\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued3/300000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 3600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 4                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 6000         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/rwen_reverse_transformer2_continued4\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "    save_latest_ckpt: True\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_reverse_{name}_reload4.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBgahgYtnXP4",
    "outputId": "026c62f8-d54d-4b2d-e3fc-86dec39ccb02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-26 07:16:13,457 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-26 07:16:13,536 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-26 07:16:25,881 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-26 07:16:26,215 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-26 07:16:27,613 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-26 07:16:28,897 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-26 07:16:28,898 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-26 07:16:29,335 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-26 07:16:31.106715: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-26 07:16:33,444 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-07-26 07:16:42,179 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued3/300000.ckpt\n",
      "2021-07-26 07:16:42,762 - INFO - joeynmt.helpers - cfg.name                           : rwen_reverse_transformer\n",
      "2021-07-26 07:16:42,763 - INFO - joeynmt.helpers - cfg.data.src                       : rw\n",
      "2021-07-26 07:16:42,763 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-26 07:16:42,763 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-07-26 07:16:42,763 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-07-26 07:16:42,764 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-07-26 07:16:42,764 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-26 07:16:42,764 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-26 07:16:42,764 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-26 07:16:42,765 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/src_vocab.txt\n",
      "2021-07-26 07:16:42,765 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer/trg_vocab.txt\n",
      "2021-07-26 07:16:42,765 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-26 07:16:42,765 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-26 07:16:42,766 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/joeynmt/models/rwen_reverse_transformer2_continued3/300000.ckpt\n",
      "2021-07-26 07:16:42,766 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-26 07:16:42,766 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-26 07:16:42,766 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-26 07:16:42,767 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-26 07:16:42,767 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-26 07:16:42,767 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-26 07:16:42,767 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-26 07:16:42,768 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-26 07:16:42,768 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-26 07:16:42,768 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-26 07:16:42,768 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-26 07:16:42,769 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-26 07:16:42,769 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-26 07:16:42,769 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-26 07:16:42,769 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-26 07:16:42,770 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-26 07:16:42,770 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-26 07:16:42,770 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-26 07:16:42,770 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-26 07:16:42,771 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-26 07:16:42,771 - INFO - joeynmt.helpers - cfg.training.epochs                : 4\n",
      "2021-07-26 07:16:42,771 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 6000\n",
      "2021-07-26 07:16:42,771 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-26 07:16:42,771 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-26 07:16:42,772 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rwen_reverse_transformer2_continued4\n",
      "2021-07-26 07:16:42,772 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-26 07:16:42,772 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-26 07:16:42,772 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-26 07:16:42,773 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-26 07:16:42,773 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-26 07:16:42,773 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-26 07:16:42,774 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True\n",
      "2021-07-26 07:16:42,774 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-26 07:16:42,774 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-26 07:16:42,775 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-26 07:16:42,775 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-26 07:16:42,775 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-26 07:16:42,775 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-26 07:16:42,775 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-26 07:16:42,776 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-26 07:16:42,776 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-26 07:16:42,776 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-26 07:16:42,776 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-26 07:16:42,777 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-26 07:16:42,777 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-26 07:16:42,777 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-26 07:16:42,777 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-26 07:16:42,778 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-26 07:16:42,778 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-26 07:16:42,778 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-26 07:16:42,778 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-26 07:16:42,779 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-26 07:16:42,779 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-26 07:16:42,779 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-26 07:16:42,779 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-26 07:16:42,780 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-26 07:16:42,780 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-26 07:16:42,780 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434519,\n",
      "\tvalid 1000,\n",
      "\ttest 2651\n",
      "2021-07-26 07:16:42,781 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "2021-07-26 07:16:42,781 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-26 07:16:42,781 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-26 07:16:42,781 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-07-26 07:16:42,782 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-07-26 07:16:42,782 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-07-26 07:16:42,804 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-26 07:16:42,804 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-26 07:17:15,894 - INFO - joeynmt.training - Epoch   1, Step:   300100, Batch Loss:     1.762680, Tokens per Sec:     6779, Lr: 0.000300\n",
      "2021-07-26 07:17:46,002 - INFO - joeynmt.training - Epoch   1, Step:   300200, Batch Loss:     1.782966, Tokens per Sec:     7301, Lr: 0.000300\n",
      "2021-07-26 07:18:16,524 - INFO - joeynmt.training - Epoch   1, Step:   300300, Batch Loss:     1.825512, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-07-26 07:18:46,602 - INFO - joeynmt.training - Epoch   1, Step:   300400, Batch Loss:     1.807079, Tokens per Sec:     7275, Lr: 0.000300\n",
      "2021-07-26 07:19:16,527 - INFO - joeynmt.training - Epoch   1, Step:   300500, Batch Loss:     1.731646, Tokens per Sec:     7328, Lr: 0.000300\n",
      "2021-07-26 07:19:45,982 - INFO - joeynmt.training - Epoch   1, Step:   300600, Batch Loss:     1.596618, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-07-26 07:20:15,899 - INFO - joeynmt.training - Epoch   1, Step:   300700, Batch Loss:     1.754760, Tokens per Sec:     7426, Lr: 0.000300\n",
      "2021-07-26 07:20:45,246 - INFO - joeynmt.training - Epoch   1, Step:   300800, Batch Loss:     1.604448, Tokens per Sec:     7270, Lr: 0.000300\n",
      "2021-07-26 07:21:15,265 - INFO - joeynmt.training - Epoch   1, Step:   300900, Batch Loss:     1.750840, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-07-26 07:21:45,378 - INFO - joeynmt.training - Epoch   1, Step:   301000, Batch Loss:     1.730520, Tokens per Sec:     7448, Lr: 0.000300\n",
      "2021-07-26 07:22:06,969 - INFO - joeynmt.training - Epoch   1: total training loss 1826.87\n",
      "2021-07-26 07:22:06,969 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-26 07:22:15,781 - INFO - joeynmt.training - Epoch   2, Step:   301100, Batch Loss:     1.649563, Tokens per Sec:     7049, Lr: 0.000300\n",
      "2021-07-26 07:22:45,390 - INFO - joeynmt.training - Epoch   2, Step:   301200, Batch Loss:     1.858969, Tokens per Sec:     7298, Lr: 0.000300\n",
      "2021-07-26 07:23:15,284 - INFO - joeynmt.training - Epoch   2, Step:   301300, Batch Loss:     1.386081, Tokens per Sec:     7449, Lr: 0.000300\n",
      "2021-07-26 07:23:44,983 - INFO - joeynmt.training - Epoch   2, Step:   301400, Batch Loss:     1.757329, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-07-26 07:24:14,293 - INFO - joeynmt.training - Epoch   2, Step:   301500, Batch Loss:     1.758003, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-07-26 07:24:44,108 - INFO - joeynmt.training - Epoch   2, Step:   301600, Batch Loss:     1.666254, Tokens per Sec:     7399, Lr: 0.000300\n",
      "2021-07-26 07:25:13,798 - INFO - joeynmt.training - Epoch   2, Step:   301700, Batch Loss:     1.612140, Tokens per Sec:     7259, Lr: 0.000300\n",
      "2021-07-26 07:25:43,328 - INFO - joeynmt.training - Epoch   2, Step:   301800, Batch Loss:     1.782794, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-07-26 07:26:13,150 - INFO - joeynmt.training - Epoch   2, Step:   301900, Batch Loss:     1.819131, Tokens per Sec:     7496, Lr: 0.000300\n",
      "2021-07-26 07:26:42,701 - INFO - joeynmt.training - Epoch   2, Step:   302000, Batch Loss:     1.652653, Tokens per Sec:     7327, Lr: 0.000300\n",
      "2021-07-26 07:27:12,663 - INFO - joeynmt.training - Epoch   2, Step:   302100, Batch Loss:     1.575970, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-07-26 07:27:42,852 - INFO - joeynmt.training - Epoch   2, Step:   302200, Batch Loss:     1.808856, Tokens per Sec:     7584, Lr: 0.000300\n",
      "2021-07-26 07:28:12,998 - INFO - joeynmt.training - Epoch   2, Step:   302300, Batch Loss:     1.562054, Tokens per Sec:     7400, Lr: 0.000300\n",
      "2021-07-26 07:28:43,142 - INFO - joeynmt.training - Epoch   2, Step:   302400, Batch Loss:     1.948846, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-07-26 07:29:13,382 - INFO - joeynmt.training - Epoch   2, Step:   302500, Batch Loss:     1.704615, Tokens per Sec:     7428, Lr: 0.000300\n",
      "2021-07-26 07:29:43,095 - INFO - joeynmt.training - Epoch   2, Step:   302600, Batch Loss:     1.797207, Tokens per Sec:     7322, Lr: 0.000300\n",
      "2021-07-26 07:30:12,626 - INFO - joeynmt.training - Epoch   2, Step:   302700, Batch Loss:     1.591653, Tokens per Sec:     7422, Lr: 0.000300\n",
      "2021-07-26 07:30:41,907 - INFO - joeynmt.training - Epoch   2, Step:   302800, Batch Loss:     1.691081, Tokens per Sec:     7256, Lr: 0.000300\n",
      "2021-07-26 07:31:11,542 - INFO - joeynmt.training - Epoch   2, Step:   302900, Batch Loss:     1.702079, Tokens per Sec:     7387, Lr: 0.000300\n",
      "2021-07-26 07:31:41,578 - INFO - joeynmt.training - Epoch   2, Step:   303000, Batch Loss:     1.751407, Tokens per Sec:     7424, Lr: 0.000300\n",
      "2021-07-26 07:32:11,332 - INFO - joeynmt.training - Epoch   2, Step:   303100, Batch Loss:     1.683739, Tokens per Sec:     7378, Lr: 0.000300\n",
      "2021-07-26 07:32:41,160 - INFO - joeynmt.training - Epoch   2, Step:   303200, Batch Loss:     1.693270, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-07-26 07:33:10,626 - INFO - joeynmt.training - Epoch   2, Step:   303300, Batch Loss:     1.624012, Tokens per Sec:     7256, Lr: 0.000300\n",
      "2021-07-26 07:33:40,309 - INFO - joeynmt.training - Epoch   2, Step:   303400, Batch Loss:     1.580126, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-07-26 07:34:10,018 - INFO - joeynmt.training - Epoch   2, Step:   303500, Batch Loss:     1.733216, Tokens per Sec:     7443, Lr: 0.000300\n",
      "2021-07-26 07:34:39,831 - INFO - joeynmt.training - Epoch   2, Step:   303600, Batch Loss:     1.819871, Tokens per Sec:     7484, Lr: 0.000300\n",
      "2021-07-26 07:35:09,855 - INFO - joeynmt.training - Epoch   2, Step:   303700, Batch Loss:     1.721021, Tokens per Sec:     7430, Lr: 0.000300\n",
      "2021-07-26 07:35:39,593 - INFO - joeynmt.training - Epoch   2, Step:   303800, Batch Loss:     1.777040, Tokens per Sec:     7457, Lr: 0.000300\n",
      "2021-07-26 07:36:09,000 - INFO - joeynmt.training - Epoch   2, Step:   303900, Batch Loss:     1.663677, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-07-26 07:36:38,712 - INFO - joeynmt.training - Epoch   2, Step:   304000, Batch Loss:     1.641054, Tokens per Sec:     7432, Lr: 0.000300\n",
      "2021-07-26 07:37:08,527 - INFO - joeynmt.training - Epoch   2, Step:   304100, Batch Loss:     1.695517, Tokens per Sec:     7519, Lr: 0.000300\n",
      "2021-07-26 07:37:38,139 - INFO - joeynmt.training - Epoch   2, Step:   304200, Batch Loss:     1.675094, Tokens per Sec:     7244, Lr: 0.000300\n",
      "2021-07-26 07:38:07,772 - INFO - joeynmt.training - Epoch   2, Step:   304300, Batch Loss:     1.741866, Tokens per Sec:     7365, Lr: 0.000300\n",
      "2021-07-26 07:38:37,237 - INFO - joeynmt.training - Epoch   2, Step:   304400, Batch Loss:     1.602314, Tokens per Sec:     7338, Lr: 0.000300\n",
      "2021-07-26 07:39:07,067 - INFO - joeynmt.training - Epoch   2, Step:   304500, Batch Loss:     1.636112, Tokens per Sec:     7444, Lr: 0.000300\n",
      "2021-07-26 07:39:36,366 - INFO - joeynmt.training - Epoch   2, Step:   304600, Batch Loss:     1.528713, Tokens per Sec:     7291, Lr: 0.000300\n",
      "2021-07-26 07:40:06,047 - INFO - joeynmt.training - Epoch   2, Step:   304700, Batch Loss:     1.601778, Tokens per Sec:     7309, Lr: 0.000300\n",
      "2021-07-26 07:40:36,008 - INFO - joeynmt.training - Epoch   2, Step:   304800, Batch Loss:     1.595889, Tokens per Sec:     7599, Lr: 0.000300\n",
      "2021-07-26 07:41:05,972 - INFO - joeynmt.training - Epoch   2, Step:   304900, Batch Loss:     1.832334, Tokens per Sec:     7532, Lr: 0.000300\n",
      "2021-07-26 07:41:35,838 - INFO - joeynmt.training - Epoch   2, Step:   305000, Batch Loss:     1.487674, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-07-26 07:42:05,656 - INFO - joeynmt.training - Epoch   2, Step:   305100, Batch Loss:     1.837953, Tokens per Sec:     7445, Lr: 0.000300\n",
      "2021-07-26 07:42:35,358 - INFO - joeynmt.training - Epoch   2, Step:   305200, Batch Loss:     1.656664, Tokens per Sec:     7339, Lr: 0.000300\n",
      "2021-07-26 07:43:05,228 - INFO - joeynmt.training - Epoch   2, Step:   305300, Batch Loss:     1.424605, Tokens per Sec:     7312, Lr: 0.000300\n",
      "2021-07-26 07:43:35,151 - INFO - joeynmt.training - Epoch   2, Step:   305400, Batch Loss:     1.545862, Tokens per Sec:     7480, Lr: 0.000300\n",
      "2021-07-26 07:44:05,078 - INFO - joeynmt.training - Epoch   2, Step:   305500, Batch Loss:     1.823391, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-07-26 07:44:34,788 - INFO - joeynmt.training - Epoch   2, Step:   305600, Batch Loss:     1.733820, Tokens per Sec:     7352, Lr: 0.000300\n",
      "2021-07-26 07:45:04,587 - INFO - joeynmt.training - Epoch   2, Step:   305700, Batch Loss:     1.603337, Tokens per Sec:     7373, Lr: 0.000300\n",
      "2021-07-26 07:45:34,146 - INFO - joeynmt.training - Epoch   2, Step:   305800, Batch Loss:     1.828095, Tokens per Sec:     7381, Lr: 0.000300\n",
      "2021-07-26 07:46:03,914 - INFO - joeynmt.training - Epoch   2, Step:   305900, Batch Loss:     1.740474, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-07-26 07:46:33,582 - INFO - joeynmt.training - Epoch   2, Step:   306000, Batch Loss:     1.365141, Tokens per Sec:     7375, Lr: 0.000300\n",
      "2021-07-26 07:47:26,843 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 07:47:26,843 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 07:47:26,844 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 07:47:27,161 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 07:47:27,161 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 07:47:28,274 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 07:47:28,275 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-26 07:47:28,275 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-26 07:47:28,275 - INFO - joeynmt.training - \tHypothesis: I was touched by the heart .\n",
      "2021-07-26 07:47:28,276 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 07:47:28,276 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-26 07:47:28,277 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-26 07:47:28,277 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
      "2021-07-26 07:47:28,277 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 07:47:28,278 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-26 07:47:28,278 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-26 07:47:28,279 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-26 07:47:28,279 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 07:47:28,280 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-26 07:47:28,280 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-26 07:47:28,280 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in a sense , that they have been successful in Satan’s world .\n",
      "2021-07-26 07:47:28,281 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   306000: bleu:  27.96, loss: 42211.3672, ppl:   4.5706, duration: 54.6977s\n",
      "2021-07-26 07:47:58,141 - INFO - joeynmt.training - Epoch   2, Step:   306100, Batch Loss:     1.607031, Tokens per Sec:     7304, Lr: 0.000300\n",
      "2021-07-26 07:48:28,064 - INFO - joeynmt.training - Epoch   2, Step:   306200, Batch Loss:     1.708295, Tokens per Sec:     7498, Lr: 0.000300\n",
      "2021-07-26 07:48:57,669 - INFO - joeynmt.training - Epoch   2, Step:   306300, Batch Loss:     1.550991, Tokens per Sec:     7225, Lr: 0.000300\n",
      "2021-07-26 07:49:27,905 - INFO - joeynmt.training - Epoch   2, Step:   306400, Batch Loss:     1.667080, Tokens per Sec:     7562, Lr: 0.000300\n",
      "2021-07-26 07:49:54,347 - INFO - joeynmt.training - Epoch   2: total training loss 9175.91\n",
      "2021-07-26 07:49:54,347 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-26 07:49:58,464 - INFO - joeynmt.training - Epoch   3, Step:   306500, Batch Loss:     1.699297, Tokens per Sec:     6110, Lr: 0.000300\n",
      "2021-07-26 07:50:27,911 - INFO - joeynmt.training - Epoch   3, Step:   306600, Batch Loss:     1.800744, Tokens per Sec:     7365, Lr: 0.000300\n",
      "2021-07-26 07:50:57,208 - INFO - joeynmt.training - Epoch   3, Step:   306700, Batch Loss:     1.734287, Tokens per Sec:     7147, Lr: 0.000300\n",
      "2021-07-26 07:51:26,897 - INFO - joeynmt.training - Epoch   3, Step:   306800, Batch Loss:     1.789735, Tokens per Sec:     7337, Lr: 0.000300\n",
      "2021-07-26 07:51:56,432 - INFO - joeynmt.training - Epoch   3, Step:   306900, Batch Loss:     1.677775, Tokens per Sec:     7503, Lr: 0.000300\n",
      "2021-07-26 07:52:26,599 - INFO - joeynmt.training - Epoch   3, Step:   307000, Batch Loss:     1.775665, Tokens per Sec:     7501, Lr: 0.000300\n",
      "2021-07-26 07:52:56,506 - INFO - joeynmt.training - Epoch   3, Step:   307100, Batch Loss:     1.861832, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-07-26 07:53:26,387 - INFO - joeynmt.training - Epoch   3, Step:   307200, Batch Loss:     1.673083, Tokens per Sec:     7397, Lr: 0.000300\n",
      "2021-07-26 07:53:56,025 - INFO - joeynmt.training - Epoch   3, Step:   307300, Batch Loss:     1.631109, Tokens per Sec:     7388, Lr: 0.000300\n",
      "2021-07-26 07:54:25,530 - INFO - joeynmt.training - Epoch   3, Step:   307400, Batch Loss:     1.302646, Tokens per Sec:     7436, Lr: 0.000300\n",
      "2021-07-26 07:54:55,721 - INFO - joeynmt.training - Epoch   3, Step:   307500, Batch Loss:     1.669989, Tokens per Sec:     7551, Lr: 0.000300\n",
      "2021-07-26 07:55:25,833 - INFO - joeynmt.training - Epoch   3, Step:   307600, Batch Loss:     1.733389, Tokens per Sec:     7624, Lr: 0.000300\n",
      "2021-07-26 07:55:55,442 - INFO - joeynmt.training - Epoch   3, Step:   307700, Batch Loss:     1.880432, Tokens per Sec:     7313, Lr: 0.000300\n",
      "2021-07-26 07:56:25,545 - INFO - joeynmt.training - Epoch   3, Step:   307800, Batch Loss:     1.896851, Tokens per Sec:     7517, Lr: 0.000300\n",
      "2021-07-26 07:56:55,209 - INFO - joeynmt.training - Epoch   3, Step:   307900, Batch Loss:     1.611569, Tokens per Sec:     7245, Lr: 0.000300\n",
      "2021-07-26 07:57:24,915 - INFO - joeynmt.training - Epoch   3, Step:   308000, Batch Loss:     1.752735, Tokens per Sec:     7407, Lr: 0.000300\n",
      "2021-07-26 07:57:54,383 - INFO - joeynmt.training - Epoch   3, Step:   308100, Batch Loss:     1.784217, Tokens per Sec:     7303, Lr: 0.000300\n",
      "2021-07-26 07:58:23,973 - INFO - joeynmt.training - Epoch   3, Step:   308200, Batch Loss:     1.695018, Tokens per Sec:     7310, Lr: 0.000300\n",
      "2021-07-26 07:58:53,693 - INFO - joeynmt.training - Epoch   3, Step:   308300, Batch Loss:     1.668958, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-07-26 07:59:23,858 - INFO - joeynmt.training - Epoch   3, Step:   308400, Batch Loss:     1.540121, Tokens per Sec:     7586, Lr: 0.000300\n",
      "2021-07-26 07:59:53,341 - INFO - joeynmt.training - Epoch   3, Step:   308500, Batch Loss:     1.818904, Tokens per Sec:     7406, Lr: 0.000300\n",
      "2021-07-26 08:00:22,851 - INFO - joeynmt.training - Epoch   3, Step:   308600, Batch Loss:     1.709837, Tokens per Sec:     7325, Lr: 0.000300\n",
      "2021-07-26 08:00:52,330 - INFO - joeynmt.training - Epoch   3, Step:   308700, Batch Loss:     1.777331, Tokens per Sec:     7395, Lr: 0.000300\n",
      "2021-07-26 08:01:21,893 - INFO - joeynmt.training - Epoch   3, Step:   308800, Batch Loss:     1.717323, Tokens per Sec:     7387, Lr: 0.000300\n",
      "2021-07-26 08:01:51,430 - INFO - joeynmt.training - Epoch   3, Step:   308900, Batch Loss:     1.586553, Tokens per Sec:     7326, Lr: 0.000300\n",
      "2021-07-26 08:02:20,850 - INFO - joeynmt.training - Epoch   3, Step:   309000, Batch Loss:     1.635236, Tokens per Sec:     7333, Lr: 0.000300\n",
      "2021-07-26 08:02:50,655 - INFO - joeynmt.training - Epoch   3, Step:   309100, Batch Loss:     1.688640, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-07-26 08:03:20,075 - INFO - joeynmt.training - Epoch   3, Step:   309200, Batch Loss:     1.530353, Tokens per Sec:     7178, Lr: 0.000300\n",
      "2021-07-26 08:03:49,795 - INFO - joeynmt.training - Epoch   3, Step:   309300, Batch Loss:     1.603710, Tokens per Sec:     7455, Lr: 0.000300\n",
      "2021-07-26 08:04:19,593 - INFO - joeynmt.training - Epoch   3, Step:   309400, Batch Loss:     1.721853, Tokens per Sec:     7414, Lr: 0.000300\n",
      "2021-07-26 08:04:49,066 - INFO - joeynmt.training - Epoch   3, Step:   309500, Batch Loss:     1.602027, Tokens per Sec:     7280, Lr: 0.000300\n",
      "2021-07-26 08:05:18,730 - INFO - joeynmt.training - Epoch   3, Step:   309600, Batch Loss:     1.602942, Tokens per Sec:     7380, Lr: 0.000300\n",
      "2021-07-26 08:05:48,814 - INFO - joeynmt.training - Epoch   3, Step:   309700, Batch Loss:     1.741500, Tokens per Sec:     7512, Lr: 0.000300\n",
      "2021-07-26 08:06:18,545 - INFO - joeynmt.training - Epoch   3, Step:   309800, Batch Loss:     1.923959, Tokens per Sec:     7427, Lr: 0.000300\n",
      "2021-07-26 08:06:48,307 - INFO - joeynmt.training - Epoch   3, Step:   309900, Batch Loss:     1.685455, Tokens per Sec:     7375, Lr: 0.000300\n",
      "2021-07-26 08:07:18,040 - INFO - joeynmt.training - Epoch   3, Step:   310000, Batch Loss:     1.804192, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-07-26 08:07:47,899 - INFO - joeynmt.training - Epoch   3, Step:   310100, Batch Loss:     1.929384, Tokens per Sec:     7605, Lr: 0.000300\n",
      "2021-07-26 08:08:17,502 - INFO - joeynmt.training - Epoch   3, Step:   310200, Batch Loss:     1.730896, Tokens per Sec:     7372, Lr: 0.000300\n",
      "2021-07-26 08:08:47,325 - INFO - joeynmt.training - Epoch   3, Step:   310300, Batch Loss:     1.754505, Tokens per Sec:     7495, Lr: 0.000300\n",
      "2021-07-26 08:09:17,105 - INFO - joeynmt.training - Epoch   3, Step:   310400, Batch Loss:     1.518632, Tokens per Sec:     7486, Lr: 0.000300\n",
      "2021-07-26 08:09:46,859 - INFO - joeynmt.training - Epoch   3, Step:   310500, Batch Loss:     1.714587, Tokens per Sec:     7419, Lr: 0.000300\n",
      "2021-07-26 08:10:16,398 - INFO - joeynmt.training - Epoch   3, Step:   310600, Batch Loss:     1.708874, Tokens per Sec:     7383, Lr: 0.000300\n",
      "2021-07-26 08:10:46,281 - INFO - joeynmt.training - Epoch   3, Step:   310700, Batch Loss:     1.592178, Tokens per Sec:     7375, Lr: 0.000300\n",
      "2021-07-26 08:11:16,192 - INFO - joeynmt.training - Epoch   3, Step:   310800, Batch Loss:     1.830396, Tokens per Sec:     7537, Lr: 0.000300\n",
      "2021-07-26 08:11:46,025 - INFO - joeynmt.training - Epoch   3, Step:   310900, Batch Loss:     1.600025, Tokens per Sec:     7509, Lr: 0.000300\n",
      "2021-07-26 08:12:15,575 - INFO - joeynmt.training - Epoch   3, Step:   311000, Batch Loss:     1.509235, Tokens per Sec:     7364, Lr: 0.000300\n",
      "2021-07-26 08:12:45,468 - INFO - joeynmt.training - Epoch   3, Step:   311100, Batch Loss:     1.801940, Tokens per Sec:     7505, Lr: 0.000300\n",
      "2021-07-26 08:13:14,862 - INFO - joeynmt.training - Epoch   3, Step:   311200, Batch Loss:     1.844700, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-26 08:13:44,497 - INFO - joeynmt.training - Epoch   3, Step:   311300, Batch Loss:     1.496220, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-07-26 08:14:14,239 - INFO - joeynmt.training - Epoch   3, Step:   311400, Batch Loss:     1.796019, Tokens per Sec:     7404, Lr: 0.000300\n",
      "2021-07-26 08:14:43,918 - INFO - joeynmt.training - Epoch   3, Step:   311500, Batch Loss:     1.584321, Tokens per Sec:     7461, Lr: 0.000300\n",
      "2021-07-26 08:15:13,501 - INFO - joeynmt.training - Epoch   3, Step:   311600, Batch Loss:     1.669821, Tokens per Sec:     7499, Lr: 0.000300\n",
      "2021-07-26 08:15:43,015 - INFO - joeynmt.training - Epoch   3, Step:   311700, Batch Loss:     1.797851, Tokens per Sec:     7270, Lr: 0.000300\n",
      "2021-07-26 08:16:12,430 - INFO - joeynmt.training - Epoch   3, Step:   311800, Batch Loss:     1.484578, Tokens per Sec:     7317, Lr: 0.000300\n",
      "2021-07-26 08:16:41,982 - INFO - joeynmt.training - Epoch   3, Step:   311900, Batch Loss:     1.567757, Tokens per Sec:     7358, Lr: 0.000300\n",
      "2021-07-26 08:16:44,818 - INFO - joeynmt.training - Epoch   3: total training loss 9174.78\n",
      "2021-07-26 08:16:44,818 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-26 08:17:12,522 - INFO - joeynmt.training - Epoch   4, Step:   312000, Batch Loss:     1.830282, Tokens per Sec:     7159, Lr: 0.000300\n",
      "2021-07-26 08:18:02,123 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 08:18:02,124 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 08:18:02,124 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 08:18:02,431 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-26 08:18:02,432 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-26 08:18:03,373 - INFO - joeynmt.training - Example #0\n",
      "2021-07-26 08:18:03,375 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
      "2021-07-26 08:18:03,375 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
      "2021-07-26 08:18:03,376 - INFO - joeynmt.training - \tHypothesis: I was touched by the heart .\n",
      "2021-07-26 08:18:03,376 - INFO - joeynmt.training - Example #1\n",
      "2021-07-26 08:18:03,377 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-26 08:18:03,377 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
      "2021-07-26 08:18:03,377 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
      "2021-07-26 08:18:03,378 - INFO - joeynmt.training - Example #2\n",
      "2021-07-26 08:18:03,378 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-26 08:18:03,378 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-26 08:18:03,379 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-26 08:18:03,379 - INFO - joeynmt.training - Example #3\n",
      "2021-07-26 08:18:03,380 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-26 08:18:03,380 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-26 08:18:03,380 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
      "2021-07-26 08:18:03,380 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   312000: bleu:  27.90, loss: 42106.5195, ppl:   4.5534, duration: 50.8579s\n",
      "2021-07-26 08:18:33,206 - INFO - joeynmt.training - Epoch   4, Step:   312100, Batch Loss:     1.641380, Tokens per Sec:     7425, Lr: 0.000300\n",
      "2021-07-26 08:19:02,682 - INFO - joeynmt.training - Epoch   4, Step:   312200, Batch Loss:     1.691062, Tokens per Sec:     7244, Lr: 0.000300\n",
      "2021-07-26 08:19:32,725 - INFO - joeynmt.training - Epoch   4, Step:   312300, Batch Loss:     1.644979, Tokens per Sec:     7589, Lr: 0.000300\n",
      "2021-07-26 08:20:02,338 - INFO - joeynmt.training - Epoch   4, Step:   312400, Batch Loss:     1.756030, Tokens per Sec:     7371, Lr: 0.000300\n",
      "2021-07-26 08:20:32,209 - INFO - joeynmt.training - Epoch   4, Step:   312500, Batch Loss:     1.626598, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-07-26 08:21:02,265 - INFO - joeynmt.training - Epoch   4, Step:   312600, Batch Loss:     1.841257, Tokens per Sec:     7347, Lr: 0.000300\n",
      "2021-07-26 08:21:31,912 - INFO - joeynmt.training - Epoch   4, Step:   312700, Batch Loss:     1.714514, Tokens per Sec:     7239, Lr: 0.000300\n",
      "2021-07-26 08:22:01,861 - INFO - joeynmt.training - Epoch   4, Step:   312800, Batch Loss:     1.867511, Tokens per Sec:     7464, Lr: 0.000300\n",
      "2021-07-26 08:22:31,615 - INFO - joeynmt.training - Epoch   4, Step:   312900, Batch Loss:     1.758625, Tokens per Sec:     7389, Lr: 0.000300\n",
      "2021-07-26 08:23:01,345 - INFO - joeynmt.training - Epoch   4, Step:   313000, Batch Loss:     1.724524, Tokens per Sec:     7377, Lr: 0.000300\n",
      "2021-07-26 08:23:30,971 - INFO - joeynmt.training - Epoch   4, Step:   313100, Batch Loss:     1.492444, Tokens per Sec:     7363, Lr: 0.000300\n",
      "2021-07-26 08:24:00,513 - INFO - joeynmt.training - Epoch   4, Step:   313200, Batch Loss:     1.783417, Tokens per Sec:     7285, Lr: 0.000300\n",
      "2021-07-26 08:24:30,146 - INFO - joeynmt.training - Epoch   4, Step:   313300, Batch Loss:     1.723889, Tokens per Sec:     7468, Lr: 0.000300\n",
      "2021-07-26 08:25:00,004 - INFO - joeynmt.training - Epoch   4, Step:   313400, Batch Loss:     1.731125, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-07-26 08:25:30,281 - INFO - joeynmt.training - Epoch   4, Step:   313500, Batch Loss:     1.515565, Tokens per Sec:     7507, Lr: 0.000300\n",
      "2021-07-26 08:25:59,910 - INFO - joeynmt.training - Epoch   4, Step:   313600, Batch Loss:     1.629999, Tokens per Sec:     7475, Lr: 0.000300\n",
      "2021-07-26 08:26:29,518 - INFO - joeynmt.training - Epoch   4, Step:   313700, Batch Loss:     1.649065, Tokens per Sec:     7341, Lr: 0.000300\n",
      "2021-07-26 08:26:58,687 - INFO - joeynmt.training - Epoch   4, Step:   313800, Batch Loss:     1.618419, Tokens per Sec:     7238, Lr: 0.000300\n",
      "2021-07-26 08:27:28,493 - INFO - joeynmt.training - Epoch   4, Step:   313900, Batch Loss:     1.633883, Tokens per Sec:     7405, Lr: 0.000300\n",
      "2021-07-26 08:27:58,558 - INFO - joeynmt.training - Epoch   4, Step:   314000, Batch Loss:     1.465588, Tokens per Sec:     7568, Lr: 0.000300\n",
      "2021-07-26 08:28:28,310 - INFO - joeynmt.training - Epoch   4, Step:   314100, Batch Loss:     1.667790, Tokens per Sec:     7413, Lr: 0.000300\n",
      "2021-07-26 08:28:58,293 - INFO - joeynmt.training - Epoch   4, Step:   314200, Batch Loss:     1.789886, Tokens per Sec:     7460, Lr: 0.000300\n",
      "2021-07-26 08:29:27,825 - INFO - joeynmt.training - Epoch   4, Step:   314300, Batch Loss:     1.835671, Tokens per Sec:     7465, Lr: 0.000300\n",
      "2021-07-26 08:29:57,026 - INFO - joeynmt.training - Epoch   4, Step:   314400, Batch Loss:     1.629474, Tokens per Sec:     7262, Lr: 0.000300\n",
      "2021-07-26 08:30:26,892 - INFO - joeynmt.training - Epoch   4, Step:   314500, Batch Loss:     1.651522, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-07-26 08:30:56,681 - INFO - joeynmt.training - Epoch   4, Step:   314600, Batch Loss:     1.635454, Tokens per Sec:     7313, Lr: 0.000300\n",
      "2021-07-26 08:31:26,178 - INFO - joeynmt.training - Epoch   4, Step:   314700, Batch Loss:     1.599794, Tokens per Sec:     7401, Lr: 0.000300\n",
      "2021-07-26 08:31:55,987 - INFO - joeynmt.training - Epoch   4, Step:   314800, Batch Loss:     1.679219, Tokens per Sec:     7335, Lr: 0.000300\n",
      "2021-07-26 08:32:25,868 - INFO - joeynmt.training - Epoch   4, Step:   314900, Batch Loss:     1.741655, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-26 08:32:55,385 - INFO - joeynmt.training - Epoch   4, Step:   315000, Batch Loss:     1.582758, Tokens per Sec:     7349, Lr: 0.000300\n",
      "2021-07-26 08:33:25,207 - INFO - joeynmt.training - Epoch   4, Step:   315100, Batch Loss:     1.496099, Tokens per Sec:     7452, Lr: 0.000300\n",
      "2021-07-26 08:33:55,046 - INFO - joeynmt.training - Epoch   4, Step:   315200, Batch Loss:     1.601386, Tokens per Sec:     7514, Lr: 0.000300\n",
      "2021-07-26 08:34:24,571 - INFO - joeynmt.training - Epoch   4, Step:   315300, Batch Loss:     1.974743, Tokens per Sec:     7451, Lr: 0.000300\n",
      "2021-07-26 08:34:54,155 - INFO - joeynmt.training - Epoch   4, Step:   315400, Batch Loss:     1.760847, Tokens per Sec:     7272, Lr: 0.000300\n",
      "2021-07-26 08:35:24,021 - INFO - joeynmt.training - Epoch   4, Step:   315500, Batch Loss:     1.662398, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-26 08:35:54,011 - INFO - joeynmt.training - Epoch   4, Step:   315600, Batch Loss:     1.776238, Tokens per Sec:     7398, Lr: 0.000300\n",
      "2021-07-26 08:36:23,583 - INFO - joeynmt.training - Epoch   4, Step:   315700, Batch Loss:     1.811450, Tokens per Sec:     7302, Lr: 0.000300\n",
      "2021-07-26 08:36:53,209 - INFO - joeynmt.training - Epoch   4, Step:   315800, Batch Loss:     1.652150, Tokens per Sec:     7483, Lr: 0.000300\n",
      "2021-07-26 08:37:22,807 - INFO - joeynmt.training - Epoch   4, Step:   315900, Batch Loss:     1.878351, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-07-26 08:37:52,730 - INFO - joeynmt.training - Epoch   4, Step:   316000, Batch Loss:     1.724467, Tokens per Sec:     7290, Lr: 0.000300\n",
      "2021-07-26 08:38:22,568 - INFO - joeynmt.training - Epoch   4, Step:   316100, Batch Loss:     1.615064, Tokens per Sec:     7573, Lr: 0.000300\n",
      "2021-07-26 08:38:52,351 - INFO - joeynmt.training - Epoch   4, Step:   316200, Batch Loss:     1.699055, Tokens per Sec:     7331, Lr: 0.000300\n",
      "2021-07-26 08:39:21,605 - INFO - joeynmt.training - Epoch   4, Step:   316300, Batch Loss:     1.529319, Tokens per Sec:     7435, Lr: 0.000300\n",
      "2021-07-26 08:39:51,361 - INFO - joeynmt.training - Epoch   4, Step:   316400, Batch Loss:     1.609145, Tokens per Sec:     7391, Lr: 0.000300\n",
      "2021-07-26 08:40:21,122 - INFO - joeynmt.training - Epoch   4, Step:   316500, Batch Loss:     1.603756, Tokens per Sec:     7346, Lr: 0.000300\n",
      "2021-07-26 08:40:51,159 - INFO - joeynmt.training - Epoch   4, Step:   316600, Batch Loss:     1.995122, Tokens per Sec:     7480, Lr: 0.000300\n",
      "2021-07-26 08:41:20,761 - INFO - joeynmt.training - Epoch   4, Step:   316700, Batch Loss:     1.540491, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-26 08:41:50,370 - INFO - joeynmt.training - Epoch   4, Step:   316800, Batch Loss:     1.767607, Tokens per Sec:     7362, Lr: 0.000300\n",
      "2021-07-26 08:42:20,286 - INFO - joeynmt.training - Epoch   4, Step:   316900, Batch Loss:     1.949062, Tokens per Sec:     7374, Lr: 0.000300\n",
      "2021-07-26 08:42:49,832 - INFO - joeynmt.training - Epoch   4, Step:   317000, Batch Loss:     1.545716, Tokens per Sec:     7393, Lr: 0.000300\n",
      "2021-07-26 08:43:19,689 - INFO - joeynmt.training - Epoch   4, Step:   317100, Batch Loss:     1.858334, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-07-26 08:43:49,192 - INFO - joeynmt.training - Epoch   4, Step:   317200, Batch Loss:     2.020175, Tokens per Sec:     7359, Lr: 0.000300\n",
      "2021-07-26 08:44:18,832 - INFO - joeynmt.training - Epoch   4, Step:   317300, Batch Loss:     1.618204, Tokens per Sec:     7386, Lr: 0.000300\n",
      "2021-07-26 08:44:28,516 - INFO - joeynmt.training - Epoch   4: total training loss 9170.11\n",
      "2021-07-26 08:44:28,516 - INFO - joeynmt.training - Training ended after   4 epochs.\n",
      "2021-07-26 08:44:28,516 - INFO - joeynmt.training - Best validation result (greedy) at step   312000:   4.55 ppl.\n",
      "2021-07-26 08:44:28,545 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-26 08:44:29,005 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-26 08:44:29,248 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-26 08:44:29,315 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe.en)...\n",
      "2021-07-26 08:45:26,291 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 08:45:26,292 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 08:45:26,292 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 08:45:26,594 - INFO - joeynmt.prediction -  dev bleu[13a]:  28.50 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-26 08:45:26,600 - INFO - joeynmt.prediction - Translations saved to: models/rwen_reverse_transformer2_continued4/00312000.hyps.dev\n",
      "2021-07-26 08:45:26,600 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe.en)...\n",
      "2021-07-26 08:46:49,991 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-26 08:46:49,992 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-26 08:46:49,992 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-26 08:46:50,695 - INFO - joeynmt.prediction - test bleu[13a]:  38.67 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-26 08:46:50,702 - INFO - joeynmt.prediction - Translations saved to: models/rwen_reverse_transformer2_continued4/00312000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_rwen_reload4.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtL9KT-vxx6m"
   },
   "source": [
    "### Reverse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4ErmIHnAvcd"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (source_language, target_language2)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{source_language}{target_language2}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language2}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"joeynmt/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 3000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 200\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda\", source_language=source_language, target_language2=target_language2)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0AkVpfZ6BfX5",
    "outputId": "7a7c57d3-9b77-4635-e89c-0f9816a20626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-09 06:48:41,357 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-09 06:48:41,614 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-09 06:48:52,577 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-09 06:48:53,179 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-09 06:48:53,893 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-09 06:48:54,816 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-09 06:48:54,816 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-09 06:48:55,021 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-09 06:48:55.266319: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-09 06:48:56,880 - INFO - joeynmt.training - Total params: 12177664\n",
      "2021-07-09 06:49:00,418 - INFO - joeynmt.helpers - cfg.name                           : enrw_transformer\n",
      "2021-07-09 06:49:00,418 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
      "2021-07-09 06:49:00,418 - INFO - joeynmt.helpers - cfg.data.trg                       : rw\n",
      "2021-07-09 06:49:00,418 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/train.bpe\n",
      "2021-07-09 06:49:00,419 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe\n",
      "2021-07-09 06:49:00,419 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe\n",
      "2021-07-09 06:49:00,419 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-09 06:49:00,419 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-09 06:49:00,419 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-09 06:49:00,419 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-07-09 06:49:00,419 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/vocab.txt\n",
      "2021-07-09 06:49:00,419 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-09 06:49:00,420 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-09 06:49:00,420 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-09 06:49:00,420 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-09 06:49:00,420 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-09 06:49:00,420 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-09 06:49:00,420 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-09 06:49:00,420 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-09 06:49:00,420 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-09 06:49:00,421 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-09 06:49:00,421 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-09 06:49:00,421 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-09 06:49:00,421 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-09 06:49:00,421 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-09 06:49:00,421 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-09 06:49:00,421 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-09 06:49:00,421 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-09 06:49:00,422 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-09 06:49:00,422 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-09 06:49:00,422 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-09 06:49:00,422 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-09 06:49:00,422 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-09 06:49:00,422 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-09 06:49:00,422 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 3000\n",
      "2021-07-09 06:49:00,422 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
      "2021-07-09 06:49:00,423 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-09 06:49:00,423 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/enrw_transformer\n",
      "2021-07-09 06:49:00,423 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
      "2021-07-09 06:49:00,423 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-09 06:49:00,423 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-09 06:49:00,423 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-09 06:49:00,423 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-09 06:49:00,423 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-09 06:49:00,424 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-09 06:49:00,424 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-09 06:49:00,424 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-09 06:49:00,424 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-09 06:49:00,424 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-09 06:49:00,424 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-09 06:49:00,424 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-09 06:49:00,424 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-09 06:49:00,425 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-09 06:49:00,425 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-09 06:49:00,425 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-09 06:49:00,425 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-09 06:49:00,426 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-09 06:49:00,426 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-09 06:49:00,426 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-09 06:49:00,426 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-09 06:49:00,426 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-09 06:49:00,426 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-09 06:49:00,427 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-09 06:49:00,427 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-09 06:49:00,427 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-09 06:49:00,427 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-09 06:49:00,427 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-09 06:49:00,427 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-09 06:49:00,427 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-09 06:49:00,428 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 434471,\n",
      "\tvalid 1000,\n",
      "\ttest 2651\n",
      "2021-07-09 06:49:00,428 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
      "\t[TRG] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
      "2021-07-09 06:49:00,428 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-09 06:49:00,428 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
      "2021-07-09 06:49:00,428 - INFO - joeynmt.helpers - Number of Src words (types): 4365\n",
      "2021-07-09 06:49:00,429 - INFO - joeynmt.helpers - Number of Trg words (types): 4365\n",
      "2021-07-09 06:49:00,429 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4365),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4365))\n",
      "2021-07-09 06:49:00,441 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-09 06:49:00,441 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-09 06:49:25,477 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.679941, Tokens per Sec:    14593, Lr: 0.000300\n",
      "2021-07-09 06:49:50,206 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.465016, Tokens per Sec:    14836, Lr: 0.000300\n",
      "2021-07-09 06:50:14,929 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     5.143500, Tokens per Sec:    14769, Lr: 0.000300\n",
      "2021-07-09 06:50:39,806 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.810545, Tokens per Sec:    14734, Lr: 0.000300\n",
      "2021-07-09 06:51:05,114 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.573690, Tokens per Sec:    14598, Lr: 0.000300\n",
      "2021-07-09 06:51:31,081 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.510453, Tokens per Sec:    14273, Lr: 0.000300\n",
      "2021-07-09 06:51:56,751 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.579916, Tokens per Sec:    14137, Lr: 0.000300\n",
      "2021-07-09 06:52:22,439 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.381755, Tokens per Sec:    14577, Lr: 0.000300\n",
      "2021-07-09 06:52:48,045 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.000751, Tokens per Sec:    14231, Lr: 0.000300\n",
      "2021-07-09 06:53:13,607 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.076670, Tokens per Sec:    14340, Lr: 0.000300\n",
      "2021-07-09 06:53:39,185 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.823152, Tokens per Sec:    14359, Lr: 0.000300\n",
      "2021-07-09 06:54:04,864 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.764109, Tokens per Sec:    14546, Lr: 0.000300\n",
      "2021-07-09 06:54:30,374 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     4.008996, Tokens per Sec:    14398, Lr: 0.000300\n",
      "2021-07-09 06:54:56,033 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.648392, Tokens per Sec:    14293, Lr: 0.000300\n",
      "2021-07-09 06:55:21,632 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.927820, Tokens per Sec:    14350, Lr: 0.000300\n",
      "2021-07-09 06:56:02,147 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 06:56:02,148 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 06:56:02,148 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 06:56:02,385 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 06:56:02,386 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 06:56:03,172 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 06:56:03,173 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 06:56:03,173 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 06:56:03,173 - INFO - joeynmt.training - \tHypothesis: Nari nanjye nanjye nanjye .\n",
      "2021-07-09 06:56:03,174 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 06:56:03,174 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 06:56:03,174 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 06:56:03,174 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , hari igihe cyose cyagombaga kuba umuntu w’ingenzi .\n",
      "2021-07-09 06:56:03,175 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 06:56:03,175 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 06:56:03,175 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 06:56:03,175 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , hari icyo gihe twagombye kuba twifuza ko twifuza ko Imana ari we . — Zaburi ya 5 : 3 .\n",
      "2021-07-09 06:56:03,176 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 06:56:03,176 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 06:56:03,176 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 06:56:03,176 - INFO - joeynmt.training - \tHypothesis: Mu gihe , Abakristo b’itorero , bakomeje kuba abantu benshi batyo mu isi , kandi bakabigiranye umwuka wera .\n",
      "2021-07-09 06:56:03,176 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     3000: bleu:   1.70, loss: 100428.3750, ppl:  34.9478, duration: 41.5441s\n",
      "2021-07-09 06:56:29,097 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     4.092618, Tokens per Sec:    14456, Lr: 0.000300\n",
      "2021-07-09 06:56:54,732 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.472031, Tokens per Sec:    14469, Lr: 0.000300\n",
      "2021-07-09 06:57:20,436 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.835132, Tokens per Sec:    14430, Lr: 0.000300\n",
      "2021-07-09 06:57:46,164 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     3.494193, Tokens per Sec:    14357, Lr: 0.000300\n",
      "2021-07-09 06:58:11,540 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     3.683992, Tokens per Sec:    14325, Lr: 0.000300\n",
      "2021-07-09 06:58:37,174 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     3.421767, Tokens per Sec:    14420, Lr: 0.000300\n",
      "2021-07-09 06:59:02,650 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     3.229305, Tokens per Sec:    14352, Lr: 0.000300\n",
      "2021-07-09 06:59:28,289 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     3.432029, Tokens per Sec:    14545, Lr: 0.000300\n",
      "2021-07-09 06:59:53,976 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     3.300050, Tokens per Sec:    14471, Lr: 0.000300\n",
      "2021-07-09 07:00:19,405 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     3.412633, Tokens per Sec:    14508, Lr: 0.000300\n",
      "2021-07-09 07:00:44,845 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     3.492190, Tokens per Sec:    14318, Lr: 0.000300\n",
      "2021-07-09 07:01:10,439 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     3.360960, Tokens per Sec:    14292, Lr: 0.000300\n",
      "2021-07-09 07:01:36,159 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     3.036915, Tokens per Sec:    14547, Lr: 0.000300\n",
      "2021-07-09 07:02:01,862 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     3.340405, Tokens per Sec:    14504, Lr: 0.000300\n",
      "2021-07-09 07:02:27,829 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     3.351320, Tokens per Sec:    14530, Lr: 0.000300\n",
      "2021-07-09 07:02:58,894 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:02:58,895 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:02:58,895 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:02:59,113 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:02:59,113 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:02:59,842 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:02:59,843 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:02:59,843 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:02:59,843 - INFO - joeynmt.training - \tHypothesis: Nari nanjye nanjye nanjye .\n",
      "2021-07-09 07:02:59,843 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:02:59,844 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:02:59,844 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:02:59,844 - INFO - joeynmt.training - \tHypothesis: Icyakora , ibyo byabaye mu buryo bwihariye , ni ukuvuga mu buryo bwihariye .\n",
      "2021-07-09 07:02:59,844 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:02:59,845 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:02:59,845 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:02:59,845 - INFO - joeynmt.training - \tHypothesis: Ahubwo , dushobora kuba maso mu buryo bw’umwuka , cyangwa tukaba twumva twumva twumva twishimiye Ijambo ry’Imana . — Abaroma 6 : 19 - 20 .\n",
      "2021-07-09 07:02:59,845 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:02:59,846 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:02:59,846 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:02:59,846 - INFO - joeynmt.training - \tHypothesis: Ku bw’ibyo , hari igihe Abakristo b’itorero bo mu kinyejana cya mbere , bashobora kuba baratekerezaga ko bari bafite ibyiringiro byo kuzabaho iteka .\n",
      "2021-07-09 07:02:59,846 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     6000: bleu:   4.09, loss: 85745.3906, ppl:  20.7858, duration: 32.0171s\n",
      "2021-07-09 07:03:25,824 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     3.393516, Tokens per Sec:    14286, Lr: 0.000300\n",
      "2021-07-09 07:03:51,738 - INFO - joeynmt.training - Epoch   1, Step:     6400, Batch Loss:     3.412397, Tokens per Sec:    14680, Lr: 0.000300\n",
      "2021-07-09 07:04:11,132 - INFO - joeynmt.training - Epoch   1: total training loss 25803.34\n",
      "2021-07-09 07:04:11,132 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-09 07:04:18,041 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     3.323434, Tokens per Sec:    12918, Lr: 0.000300\n",
      "2021-07-09 07:04:43,555 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     3.287035, Tokens per Sec:    14257, Lr: 0.000300\n",
      "2021-07-09 07:05:09,224 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     3.275147, Tokens per Sec:    14385, Lr: 0.000300\n",
      "2021-07-09 07:05:34,706 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     3.146268, Tokens per Sec:    14332, Lr: 0.000300\n",
      "2021-07-09 07:06:00,691 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     3.389159, Tokens per Sec:    14786, Lr: 0.000300\n",
      "2021-07-09 07:06:26,505 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     3.252632, Tokens per Sec:    14425, Lr: 0.000300\n",
      "2021-07-09 07:06:52,288 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     3.125576, Tokens per Sec:    14234, Lr: 0.000300\n",
      "2021-07-09 07:07:18,023 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     3.239196, Tokens per Sec:    14178, Lr: 0.000300\n",
      "2021-07-09 07:07:43,556 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     3.050169, Tokens per Sec:    14315, Lr: 0.000300\n",
      "2021-07-09 07:08:09,233 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     3.112921, Tokens per Sec:    14304, Lr: 0.000300\n",
      "2021-07-09 07:08:34,981 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     2.913934, Tokens per Sec:    14472, Lr: 0.000300\n",
      "2021-07-09 07:09:00,546 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     2.957756, Tokens per Sec:    14243, Lr: 0.000300\n",
      "2021-07-09 07:09:26,167 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     3.185453, Tokens per Sec:    14452, Lr: 0.000300\n",
      "2021-07-09 07:09:58,220 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:09:58,221 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:09:58,221 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:09:58,441 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:09:58,441 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:09:59,549 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:09:59,551 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:09:59,551 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:09:59,551 - INFO - joeynmt.training - \tHypothesis: Nari mfite umutima wanjye .\n",
      "2021-07-09 07:09:59,551 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:09:59,552 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:09:59,552 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:09:59,552 - INFO - joeynmt.training - \tHypothesis: Icyakora , ibyo byabaye ngombwa ko gusoma Bibiliya .\n",
      "2021-07-09 07:09:59,552 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:09:59,552 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:09:59,553 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:09:59,553 - INFO - joeynmt.training - \tHypothesis: Ahubwo , dushobora guhitamo umuntu cyangwa ngo twumve ko twagombye kwigana Ijambo ry’Imana . — Abaroma 8 : 35 - 35 .\n",
      "2021-07-09 07:09:59,553 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:09:59,553 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:09:59,553 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:09:59,554 - INFO - joeynmt.training - \tHypothesis: Ku bw’ibyo , hari bamwe mu itorero rya gikristo , bagaragaje ko ari bo bonyine bonyine bumvaga ko ari bo Satani .\n",
      "2021-07-09 07:09:59,554 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     9000: bleu:   6.56, loss: 78221.2500, ppl:  15.9269, duration: 33.3860s\n",
      "2021-07-09 07:10:25,663 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     3.206000, Tokens per Sec:    14527, Lr: 0.000300\n",
      "2021-07-09 07:10:51,102 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     2.889015, Tokens per Sec:    14359, Lr: 0.000300\n",
      "2021-07-09 07:11:16,762 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     3.219207, Tokens per Sec:    14278, Lr: 0.000300\n",
      "2021-07-09 07:11:42,365 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     2.806900, Tokens per Sec:    14548, Lr: 0.000300\n",
      "2021-07-09 07:12:07,922 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     3.038746, Tokens per Sec:    14550, Lr: 0.000300\n",
      "2021-07-09 07:12:33,627 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     2.876923, Tokens per Sec:    14639, Lr: 0.000300\n",
      "2021-07-09 07:12:59,489 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     3.005629, Tokens per Sec:    14481, Lr: 0.000300\n",
      "2021-07-09 07:13:25,174 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     2.938742, Tokens per Sec:    14284, Lr: 0.000300\n",
      "2021-07-09 07:13:50,701 - INFO - joeynmt.training - Epoch   2, Step:    10800, Batch Loss:     2.956887, Tokens per Sec:    14271, Lr: 0.000300\n",
      "2021-07-09 07:14:16,434 - INFO - joeynmt.training - Epoch   2, Step:    11000, Batch Loss:     2.901983, Tokens per Sec:    14290, Lr: 0.000300\n",
      "2021-07-09 07:14:42,268 - INFO - joeynmt.training - Epoch   2, Step:    11200, Batch Loss:     2.624494, Tokens per Sec:    14495, Lr: 0.000300\n",
      "2021-07-09 07:15:07,850 - INFO - joeynmt.training - Epoch   2, Step:    11400, Batch Loss:     3.000985, Tokens per Sec:    14369, Lr: 0.000300\n",
      "2021-07-09 07:15:33,409 - INFO - joeynmt.training - Epoch   2, Step:    11600, Batch Loss:     3.151151, Tokens per Sec:    14279, Lr: 0.000300\n",
      "2021-07-09 07:15:58,872 - INFO - joeynmt.training - Epoch   2, Step:    11800, Batch Loss:     2.703696, Tokens per Sec:    14379, Lr: 0.000300\n",
      "2021-07-09 07:16:24,643 - INFO - joeynmt.training - Epoch   2, Step:    12000, Batch Loss:     2.834210, Tokens per Sec:    14562, Lr: 0.000300\n",
      "2021-07-09 07:16:55,985 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:16:55,985 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:16:55,986 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:16:56,222 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:16:56,223 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:16:57,020 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:16:57,021 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:16:57,021 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:16:57,021 - INFO - joeynmt.training - \tHypothesis: Nari mfite umutima wanjye .\n",
      "2021-07-09 07:16:57,021 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:16:57,022 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:16:57,022 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:16:57,022 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume ibyo byabaye mu isoma .\n",
      "2021-07-09 07:16:57,022 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:16:57,023 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:16:57,023 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:16:57,023 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twirinde kurwanya cyangwa tukaba twiringirwa , twagombye kwiringira Imana mu Ijambo rye . — Abaroma 8 : 35 - 35 .\n",
      "2021-07-09 07:16:57,023 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:16:57,024 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:16:57,024 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:16:57,024 - INFO - joeynmt.training - \tHypothesis: Mu by’ukuri , hari itorero rya gikristo rigaragaza ko mu itorero rya gikristo rigaragaza ko ari ryo rizwi , rizwi cyane cyane ku isi ya Satani .\n",
      "2021-07-09 07:16:57,024 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    12000: bleu:   9.16, loss: 72449.9688, ppl:  12.9849, duration: 32.3808s\n",
      "2021-07-09 07:17:22,783 - INFO - joeynmt.training - Epoch   2, Step:    12200, Batch Loss:     2.814002, Tokens per Sec:    14431, Lr: 0.000300\n",
      "2021-07-09 07:17:48,544 - INFO - joeynmt.training - Epoch   2, Step:    12400, Batch Loss:     2.994031, Tokens per Sec:    14641, Lr: 0.000300\n",
      "2021-07-09 07:18:14,149 - INFO - joeynmt.training - Epoch   2, Step:    12600, Batch Loss:     2.928656, Tokens per Sec:    14352, Lr: 0.000300\n",
      "2021-07-09 07:18:39,798 - INFO - joeynmt.training - Epoch   2, Step:    12800, Batch Loss:     2.861776, Tokens per Sec:    14386, Lr: 0.000300\n",
      "2021-07-09 07:19:05,563 - INFO - joeynmt.training - Epoch   2, Step:    13000, Batch Loss:     2.880752, Tokens per Sec:    14348, Lr: 0.000300\n",
      "2021-07-09 07:19:17,657 - INFO - joeynmt.training - Epoch   2: total training loss 19809.38\n",
      "2021-07-09 07:19:17,658 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-09 07:19:31,846 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     2.927285, Tokens per Sec:    13939, Lr: 0.000300\n",
      "2021-07-09 07:19:57,527 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     3.212340, Tokens per Sec:    14391, Lr: 0.000300\n",
      "2021-07-09 07:20:23,198 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     2.851571, Tokens per Sec:    14424, Lr: 0.000300\n",
      "2021-07-09 07:20:48,986 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     3.003441, Tokens per Sec:    14620, Lr: 0.000300\n",
      "2021-07-09 07:21:14,795 - INFO - joeynmt.training - Epoch   3, Step:    14000, Batch Loss:     2.780506, Tokens per Sec:    14473, Lr: 0.000300\n",
      "2021-07-09 07:21:40,518 - INFO - joeynmt.training - Epoch   3, Step:    14200, Batch Loss:     3.009260, Tokens per Sec:    14173, Lr: 0.000300\n",
      "2021-07-09 07:22:06,088 - INFO - joeynmt.training - Epoch   3, Step:    14400, Batch Loss:     2.936255, Tokens per Sec:    14407, Lr: 0.000300\n",
      "2021-07-09 07:22:31,564 - INFO - joeynmt.training - Epoch   3, Step:    14600, Batch Loss:     2.921762, Tokens per Sec:    14285, Lr: 0.000300\n",
      "2021-07-09 07:22:56,946 - INFO - joeynmt.training - Epoch   3, Step:    14800, Batch Loss:     2.926442, Tokens per Sec:    14371, Lr: 0.000300\n",
      "2021-07-09 07:23:22,601 - INFO - joeynmt.training - Epoch   3, Step:    15000, Batch Loss:     2.661934, Tokens per Sec:    14529, Lr: 0.000300\n",
      "2021-07-09 07:23:51,454 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:23:51,455 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:23:51,455 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:23:51,687 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:23:51,687 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:23:52,416 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:23:52,417 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:23:52,417 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:23:52,417 - INFO - joeynmt.training - \tHypothesis: Nari narakaye .\n",
      "2021-07-09 07:23:52,417 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:23:52,418 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:23:52,418 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:23:52,418 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo gikorwa cyagombaga gusoma .\n",
      "2021-07-09 07:23:52,418 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:23:52,419 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:23:52,419 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:23:52,419 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twemere ko twiringira Imana mu Ijambo rye . — Abaroma 8 : 35 - 35 .\n",
      "2021-07-09 07:23:52,419 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:23:52,420 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:23:52,420 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:23:52,420 - INFO - joeynmt.training - \tHypothesis: Mu by’ukuri , hari bamwe mu itorero rya gikristo rigaragaza ko hari ibintu bitandukanye , kandi ntibumva ko ari bo Satani .\n",
      "2021-07-09 07:23:52,420 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    15000: bleu:  10.38, loss: 68738.1328, ppl:  11.3866, duration: 29.8188s\n",
      "2021-07-09 07:24:18,556 - INFO - joeynmt.training - Epoch   3, Step:    15200, Batch Loss:     2.702178, Tokens per Sec:    14418, Lr: 0.000300\n",
      "2021-07-09 07:24:44,187 - INFO - joeynmt.training - Epoch   3, Step:    15400, Batch Loss:     2.712556, Tokens per Sec:    14351, Lr: 0.000300\n",
      "2021-07-09 07:25:09,542 - INFO - joeynmt.training - Epoch   3, Step:    15600, Batch Loss:     2.411409, Tokens per Sec:    14409, Lr: 0.000300\n",
      "2021-07-09 07:25:35,225 - INFO - joeynmt.training - Epoch   3, Step:    15800, Batch Loss:     2.688996, Tokens per Sec:    14233, Lr: 0.000300\n",
      "2021-07-09 07:26:00,697 - INFO - joeynmt.training - Epoch   3, Step:    16000, Batch Loss:     2.758132, Tokens per Sec:    14294, Lr: 0.000300\n",
      "2021-07-09 07:26:26,251 - INFO - joeynmt.training - Epoch   3, Step:    16200, Batch Loss:     2.604086, Tokens per Sec:    14316, Lr: 0.000300\n",
      "2021-07-09 07:26:51,924 - INFO - joeynmt.training - Epoch   3, Step:    16400, Batch Loss:     2.830926, Tokens per Sec:    14389, Lr: 0.000300\n",
      "2021-07-09 07:27:17,590 - INFO - joeynmt.training - Epoch   3, Step:    16600, Batch Loss:     2.768815, Tokens per Sec:    14372, Lr: 0.000300\n",
      "2021-07-09 07:27:43,323 - INFO - joeynmt.training - Epoch   3, Step:    16800, Batch Loss:     2.907627, Tokens per Sec:    14779, Lr: 0.000300\n",
      "2021-07-09 07:28:08,809 - INFO - joeynmt.training - Epoch   3, Step:    17000, Batch Loss:     2.729386, Tokens per Sec:    14524, Lr: 0.000300\n",
      "2021-07-09 07:28:34,638 - INFO - joeynmt.training - Epoch   3, Step:    17200, Batch Loss:     2.570837, Tokens per Sec:    14392, Lr: 0.000300\n",
      "2021-07-09 07:29:00,162 - INFO - joeynmt.training - Epoch   3, Step:    17400, Batch Loss:     2.820940, Tokens per Sec:    14445, Lr: 0.000300\n",
      "2021-07-09 07:29:25,861 - INFO - joeynmt.training - Epoch   3, Step:    17600, Batch Loss:     2.903069, Tokens per Sec:    14534, Lr: 0.000300\n",
      "2021-07-09 07:29:51,279 - INFO - joeynmt.training - Epoch   3, Step:    17800, Batch Loss:     2.761414, Tokens per Sec:    14155, Lr: 0.000300\n",
      "2021-07-09 07:30:16,919 - INFO - joeynmt.training - Epoch   3, Step:    18000, Batch Loss:     2.685546, Tokens per Sec:    14411, Lr: 0.000300\n",
      "2021-07-09 07:30:47,203 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:30:47,204 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:30:47,204 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:30:47,431 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:30:47,432 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:30:48,276 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:30:48,277 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:30:48,277 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:30:48,277 - INFO - joeynmt.training - \tHypothesis: Mama yari yararararakaye .\n",
      "2021-07-09 07:30:48,277 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:30:48,278 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:30:48,278 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:30:48,278 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo gikorwa cyagombaga gusoma mu isiganwa ry’umuzika .\n",
      "2021-07-09 07:30:48,278 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:30:48,279 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:30:48,279 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:30:48,279 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twirinde kurwanya cyangwa tukareka , twagombye kwiringira Imana binyuriye mu gusoma Ijambo rye . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:30:48,279 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:30:48,280 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:30:48,280 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:30:48,280 - INFO - joeynmt.training - \tHypothesis: Mu by’ukuri , bamwe mu bagize itorero rya Gikristo basobanuye ko hari ibintu bitandukanye , bumvaga ko ari byo Satani .\n",
      "2021-07-09 07:30:48,280 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    18000: bleu:  11.61, loss: 65747.7422, ppl:  10.2432, duration: 31.3603s\n",
      "2021-07-09 07:31:14,282 - INFO - joeynmt.training - Epoch   3, Step:    18200, Batch Loss:     2.799028, Tokens per Sec:    14413, Lr: 0.000300\n",
      "2021-07-09 07:31:40,169 - INFO - joeynmt.training - Epoch   3, Step:    18400, Batch Loss:     2.729722, Tokens per Sec:    14626, Lr: 0.000300\n",
      "2021-07-09 07:32:05,955 - INFO - joeynmt.training - Epoch   3, Step:    18600, Batch Loss:     2.613880, Tokens per Sec:    14516, Lr: 0.000300\n",
      "2021-07-09 07:32:31,546 - INFO - joeynmt.training - Epoch   3, Step:    18800, Batch Loss:     2.541983, Tokens per Sec:    14139, Lr: 0.000300\n",
      "2021-07-09 07:32:57,067 - INFO - joeynmt.training - Epoch   3, Step:    19000, Batch Loss:     2.852990, Tokens per Sec:    14309, Lr: 0.000300\n",
      "2021-07-09 07:33:22,576 - INFO - joeynmt.training - Epoch   3, Step:    19200, Batch Loss:     2.740634, Tokens per Sec:    14485, Lr: 0.000300\n",
      "2021-07-09 07:33:48,336 - INFO - joeynmt.training - Epoch   3, Step:    19400, Batch Loss:     2.723529, Tokens per Sec:    14299, Lr: 0.000300\n",
      "2021-07-09 07:34:14,154 - INFO - joeynmt.training - Epoch   3, Step:    19600, Batch Loss:     2.645028, Tokens per Sec:    14443, Lr: 0.000300\n",
      "2021-07-09 07:34:19,366 - INFO - joeynmt.training - Epoch   3: total training loss 17941.89\n",
      "2021-07-09 07:34:19,366 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-09 07:34:40,575 - INFO - joeynmt.training - Epoch   4, Step:    19800, Batch Loss:     2.259511, Tokens per Sec:    14257, Lr: 0.000300\n",
      "2021-07-09 07:35:06,113 - INFO - joeynmt.training - Epoch   4, Step:    20000, Batch Loss:     2.837795, Tokens per Sec:    14316, Lr: 0.000300\n",
      "2021-07-09 07:35:31,742 - INFO - joeynmt.training - Epoch   4, Step:    20200, Batch Loss:     3.054194, Tokens per Sec:    14272, Lr: 0.000300\n",
      "2021-07-09 07:35:57,423 - INFO - joeynmt.training - Epoch   4, Step:    20400, Batch Loss:     2.689902, Tokens per Sec:    14583, Lr: 0.000300\n",
      "2021-07-09 07:36:22,850 - INFO - joeynmt.training - Epoch   4, Step:    20600, Batch Loss:     2.219066, Tokens per Sec:    14366, Lr: 0.000300\n",
      "2021-07-09 07:36:48,601 - INFO - joeynmt.training - Epoch   4, Step:    20800, Batch Loss:     2.280733, Tokens per Sec:    14458, Lr: 0.000300\n",
      "2021-07-09 07:37:14,277 - INFO - joeynmt.training - Epoch   4, Step:    21000, Batch Loss:     2.618734, Tokens per Sec:    14489, Lr: 0.000300\n",
      "2021-07-09 07:37:39,111 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:37:39,111 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:37:39,111 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:37:39,347 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:37:39,347 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:37:40,359 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:37:40,360 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:37:40,360 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:37:40,360 - INFO - joeynmt.training - \tHypothesis: Naramfashije .\n",
      "2021-07-09 07:37:40,360 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:37:40,361 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:37:40,361 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:37:40,361 - INFO - joeynmt.training - \tHypothesis: Icyakora , reka dusuzume icyo kibazo cyari gikubiyemo n’isomo .\n",
      "2021-07-09 07:37:40,361 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:37:40,362 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:37:40,362 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:37:40,362 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twumve ko twiringira Imana binyuriye mu gusoma Ijambo rye . — Abaroma 8 : 39 - 39 .\n",
      "2021-07-09 07:37:40,362 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:37:40,362 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:37:40,363 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:37:40,363 - INFO - joeynmt.training - \tHypothesis: Mu by’ukuri , hari bamwe mu itorero rya gikristo bahabwa igihamya cy’uko hazabaho , bumva ko nta cyo bitwaye , bumva ko nta cyo ari cyo Satani yumvise .\n",
      "2021-07-09 07:37:40,363 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    21000: bleu:  12.74, loss: 63069.1562, ppl:   9.3169, duration: 26.0853s\n",
      "2021-07-09 07:38:05,851 - INFO - joeynmt.training - Epoch   4, Step:    21200, Batch Loss:     2.558821, Tokens per Sec:    14021, Lr: 0.000300\n",
      "2021-07-09 07:38:31,632 - INFO - joeynmt.training - Epoch   4, Step:    21400, Batch Loss:     2.586639, Tokens per Sec:    14593, Lr: 0.000300\n",
      "2021-07-09 07:38:57,221 - INFO - joeynmt.training - Epoch   4, Step:    21600, Batch Loss:     2.622803, Tokens per Sec:    14341, Lr: 0.000300\n",
      "2021-07-09 07:39:23,019 - INFO - joeynmt.training - Epoch   4, Step:    21800, Batch Loss:     2.554500, Tokens per Sec:    14596, Lr: 0.000300\n",
      "2021-07-09 07:39:48,517 - INFO - joeynmt.training - Epoch   4, Step:    22000, Batch Loss:     2.598893, Tokens per Sec:    14293, Lr: 0.000300\n",
      "2021-07-09 07:40:14,246 - INFO - joeynmt.training - Epoch   4, Step:    22200, Batch Loss:     2.667173, Tokens per Sec:    14448, Lr: 0.000300\n",
      "2021-07-09 07:40:39,770 - INFO - joeynmt.training - Epoch   4, Step:    22400, Batch Loss:     2.837911, Tokens per Sec:    14390, Lr: 0.000300\n",
      "2021-07-09 07:41:05,348 - INFO - joeynmt.training - Epoch   4, Step:    22600, Batch Loss:     2.493027, Tokens per Sec:    14252, Lr: 0.000300\n",
      "2021-07-09 07:41:30,908 - INFO - joeynmt.training - Epoch   4, Step:    22800, Batch Loss:     2.526176, Tokens per Sec:    14281, Lr: 0.000300\n",
      "2021-07-09 07:41:56,701 - INFO - joeynmt.training - Epoch   4, Step:    23000, Batch Loss:     2.701049, Tokens per Sec:    14565, Lr: 0.000300\n",
      "2021-07-09 07:42:22,198 - INFO - joeynmt.training - Epoch   4, Step:    23200, Batch Loss:     2.552794, Tokens per Sec:    14551, Lr: 0.000300\n",
      "2021-07-09 07:42:47,741 - INFO - joeynmt.training - Epoch   4, Step:    23400, Batch Loss:     2.632570, Tokens per Sec:    14531, Lr: 0.000300\n",
      "2021-07-09 07:43:13,480 - INFO - joeynmt.training - Epoch   4, Step:    23600, Batch Loss:     2.484929, Tokens per Sec:    14768, Lr: 0.000300\n",
      "2021-07-09 07:43:38,885 - INFO - joeynmt.training - Epoch   4, Step:    23800, Batch Loss:     2.701479, Tokens per Sec:    14397, Lr: 0.000300\n",
      "2021-07-09 07:44:04,165 - INFO - joeynmt.training - Epoch   4, Step:    24000, Batch Loss:     2.668171, Tokens per Sec:    14154, Lr: 0.000300\n",
      "2021-07-09 07:44:26,350 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:44:26,351 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:44:26,351 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:44:26,580 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:44:26,580 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:44:27,315 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:44:27,316 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:44:27,316 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:44:27,316 - INFO - joeynmt.training - \tHypothesis: Narakundaga cyane .\n",
      "2021-07-09 07:44:27,316 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:44:27,317 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:44:27,317 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:44:27,318 - INFO - joeynmt.training - \tHypothesis: Icyakora , reka dusuzume icyo gusoma mu isanzure ry’ikirere .\n",
      "2021-07-09 07:44:27,318 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:44:27,318 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:44:27,318 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:44:27,319 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa tukarokoka , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:44:27,319 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:44:27,319 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:44:27,319 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:44:27,320 - INFO - joeynmt.training - \tHypothesis: Mu by’ukuri , mu itorero rya Gikristo hari ibintu bihamya ko mu itorero rya Gikristo , bumvise bijyanye n’ibintu bya Satani .\n",
      "2021-07-09 07:44:27,320 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    24000: bleu:  13.75, loss: 61083.1484, ppl:   8.6846, duration: 23.1542s\n",
      "2021-07-09 07:44:53,357 - INFO - joeynmt.training - Epoch   4, Step:    24200, Batch Loss:     2.485649, Tokens per Sec:    13956, Lr: 0.000300\n",
      "2021-07-09 07:45:19,165 - INFO - joeynmt.training - Epoch   4, Step:    24400, Batch Loss:     2.579873, Tokens per Sec:    14510, Lr: 0.000300\n",
      "2021-07-09 07:45:44,742 - INFO - joeynmt.training - Epoch   4, Step:    24600, Batch Loss:     2.670563, Tokens per Sec:    14267, Lr: 0.000300\n",
      "2021-07-09 07:46:10,408 - INFO - joeynmt.training - Epoch   4, Step:    24800, Batch Loss:     2.375509, Tokens per Sec:    14490, Lr: 0.000300\n",
      "2021-07-09 07:46:36,048 - INFO - joeynmt.training - Epoch   4, Step:    25000, Batch Loss:     2.606800, Tokens per Sec:    14453, Lr: 0.000300\n",
      "2021-07-09 07:47:01,664 - INFO - joeynmt.training - Epoch   4, Step:    25200, Batch Loss:     2.459246, Tokens per Sec:    14402, Lr: 0.000300\n",
      "2021-07-09 07:47:27,314 - INFO - joeynmt.training - Epoch   4, Step:    25400, Batch Loss:     2.422908, Tokens per Sec:    14375, Lr: 0.000300\n",
      "2021-07-09 07:47:52,944 - INFO - joeynmt.training - Epoch   4, Step:    25600, Batch Loss:     2.556416, Tokens per Sec:    14286, Lr: 0.000300\n",
      "2021-07-09 07:48:18,608 - INFO - joeynmt.training - Epoch   4, Step:    25800, Batch Loss:     2.299605, Tokens per Sec:    14370, Lr: 0.000300\n",
      "2021-07-09 07:48:44,373 - INFO - joeynmt.training - Epoch   4, Step:    26000, Batch Loss:     2.407938, Tokens per Sec:    14917, Lr: 0.000300\n",
      "2021-07-09 07:49:08,683 - INFO - joeynmt.training - Epoch   4: total training loss 16883.67\n",
      "2021-07-09 07:49:08,683 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-09 07:49:10,336 - INFO - joeynmt.training - Epoch   5, Step:    26200, Batch Loss:     2.645115, Tokens per Sec:     8875, Lr: 0.000300\n",
      "2021-07-09 07:49:35,695 - INFO - joeynmt.training - Epoch   5, Step:    26400, Batch Loss:     2.444614, Tokens per Sec:    14363, Lr: 0.000300\n",
      "2021-07-09 07:50:00,982 - INFO - joeynmt.training - Epoch   5, Step:    26600, Batch Loss:     2.667504, Tokens per Sec:    14252, Lr: 0.000300\n",
      "2021-07-09 07:50:26,314 - INFO - joeynmt.training - Epoch   5, Step:    26800, Batch Loss:     2.860805, Tokens per Sec:    14276, Lr: 0.000300\n",
      "2021-07-09 07:50:51,967 - INFO - joeynmt.training - Epoch   5, Step:    27000, Batch Loss:     2.496688, Tokens per Sec:    14455, Lr: 0.000300\n",
      "2021-07-09 07:51:13,214 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:51:13,215 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:51:13,215 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:51:13,444 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:51:13,445 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:51:14,166 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:51:14,166 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:51:14,166 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:51:14,166 - INFO - joeynmt.training - \tHypothesis: Mu mutima wanjye yarambajije .\n",
      "2021-07-09 07:51:14,167 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:51:14,167 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:51:14,167 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:51:14,167 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo gusoma mu mva .\n",
      "2021-07-09 07:51:14,167 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:51:14,168 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:51:14,168 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:51:14,168 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twiringire ko twiringira Imana binyuriye mu gusoma Ijambo rye . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:51:14,168 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:51:14,169 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:51:14,169 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:51:14,169 - INFO - joeynmt.training - \tHypothesis: Mu by’ukuri , hari bamwe mu bagize itorero rya Gikristo bahamya ko mu itorero rya Gikristo , bagasura , bumva ko ari uko bimeze ku isi ya Satani .\n",
      "2021-07-09 07:51:14,169 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    27000: bleu:  14.55, loss: 59487.3516, ppl:   8.2077, duration: 22.2017s\n",
      "2021-07-09 07:51:39,662 - INFO - joeynmt.training - Epoch   5, Step:    27200, Batch Loss:     2.506338, Tokens per Sec:    14304, Lr: 0.000300\n",
      "2021-07-09 07:52:05,343 - INFO - joeynmt.training - Epoch   5, Step:    27400, Batch Loss:     2.512779, Tokens per Sec:    14600, Lr: 0.000300\n",
      "2021-07-09 07:52:30,851 - INFO - joeynmt.training - Epoch   5, Step:    27600, Batch Loss:     2.397459, Tokens per Sec:    14430, Lr: 0.000300\n",
      "2021-07-09 07:52:56,257 - INFO - joeynmt.training - Epoch   5, Step:    27800, Batch Loss:     2.472411, Tokens per Sec:    14473, Lr: 0.000300\n",
      "2021-07-09 07:53:21,699 - INFO - joeynmt.training - Epoch   5, Step:    28000, Batch Loss:     2.612406, Tokens per Sec:    14615, Lr: 0.000300\n",
      "2021-07-09 07:53:47,258 - INFO - joeynmt.training - Epoch   5, Step:    28200, Batch Loss:     2.625436, Tokens per Sec:    14742, Lr: 0.000300\n",
      "2021-07-09 07:54:12,928 - INFO - joeynmt.training - Epoch   5, Step:    28400, Batch Loss:     2.488217, Tokens per Sec:    14479, Lr: 0.000300\n",
      "2021-07-09 07:54:38,602 - INFO - joeynmt.training - Epoch   5, Step:    28600, Batch Loss:     2.340156, Tokens per Sec:    14639, Lr: 0.000300\n",
      "2021-07-09 07:55:04,192 - INFO - joeynmt.training - Epoch   5, Step:    28800, Batch Loss:     2.476047, Tokens per Sec:    14083, Lr: 0.000300\n",
      "2021-07-09 07:55:29,968 - INFO - joeynmt.training - Epoch   5, Step:    29000, Batch Loss:     2.449146, Tokens per Sec:    14526, Lr: 0.000300\n",
      "2021-07-09 07:55:55,471 - INFO - joeynmt.training - Epoch   5, Step:    29200, Batch Loss:     2.538588, Tokens per Sec:    14310, Lr: 0.000300\n",
      "2021-07-09 07:56:21,098 - INFO - joeynmt.training - Epoch   5, Step:    29400, Batch Loss:     2.107145, Tokens per Sec:    14483, Lr: 0.000300\n",
      "2021-07-09 07:56:46,695 - INFO - joeynmt.training - Epoch   5, Step:    29600, Batch Loss:     2.413419, Tokens per Sec:    14523, Lr: 0.000300\n",
      "2021-07-09 07:57:12,183 - INFO - joeynmt.training - Epoch   5, Step:    29800, Batch Loss:     2.517746, Tokens per Sec:    14663, Lr: 0.000300\n",
      "2021-07-09 07:57:37,707 - INFO - joeynmt.training - Epoch   5, Step:    30000, Batch Loss:     2.472821, Tokens per Sec:    14656, Lr: 0.000300\n",
      "2021-07-09 07:58:04,381 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 07:58:04,381 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 07:58:04,381 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 07:58:04,614 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 07:58:04,615 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 07:58:05,318 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 07:58:05,319 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 07:58:05,319 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 07:58:05,319 - INFO - joeynmt.training - \tHypothesis: Nari nararakaye .\n",
      "2021-07-09 07:58:05,319 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 07:58:05,319 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 07:58:05,320 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 07:58:05,320 - INFO - joeynmt.training - \tHypothesis: Icyakora , reka dusuzume ibyo byabaye mu isarura ry’umuzingo .\n",
      "2021-07-09 07:58:05,320 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 07:58:05,321 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 07:58:05,321 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:58:05,321 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twiringire ko twiringira Imana binyuriye mu gusoma Ijambo rye . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 07:58:05,321 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 07:58:05,322 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 07:58:05,322 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 07:58:05,322 - INFO - joeynmt.training - \tHypothesis: Mu by’ukuri , bamwe mu bagize itorero rya gikristo bahabwa igihamya cy’uko mu gihe cy’ibinyabuzima , bumva ko ari mu isi ya Satani .\n",
      "2021-07-09 07:58:05,322 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    30000: bleu:  15.20, loss: 58098.7383, ppl:   7.8142, duration: 27.6149s\n",
      "2021-07-09 07:58:31,114 - INFO - joeynmt.training - Epoch   5, Step:    30200, Batch Loss:     2.300550, Tokens per Sec:    14127, Lr: 0.000300\n",
      "2021-07-09 07:58:56,543 - INFO - joeynmt.training - Epoch   5, Step:    30400, Batch Loss:     2.303429, Tokens per Sec:    14599, Lr: 0.000300\n",
      "2021-07-09 07:59:22,013 - INFO - joeynmt.training - Epoch   5, Step:    30600, Batch Loss:     2.408859, Tokens per Sec:    14566, Lr: 0.000300\n",
      "2021-07-09 07:59:47,413 - INFO - joeynmt.training - Epoch   5, Step:    30800, Batch Loss:     2.394906, Tokens per Sec:    14239, Lr: 0.000300\n",
      "2021-07-09 08:00:12,953 - INFO - joeynmt.training - Epoch   5, Step:    31000, Batch Loss:     2.480023, Tokens per Sec:    14494, Lr: 0.000300\n",
      "2021-07-09 08:00:38,816 - INFO - joeynmt.training - Epoch   5, Step:    31200, Batch Loss:     2.596714, Tokens per Sec:    14618, Lr: 0.000300\n",
      "2021-07-09 08:01:04,390 - INFO - joeynmt.training - Epoch   5, Step:    31400, Batch Loss:     2.245815, Tokens per Sec:    14473, Lr: 0.000300\n",
      "2021-07-09 08:01:30,042 - INFO - joeynmt.training - Epoch   5, Step:    31600, Batch Loss:     2.531583, Tokens per Sec:    14540, Lr: 0.000300\n",
      "2021-07-09 08:01:55,424 - INFO - joeynmt.training - Epoch   5, Step:    31800, Batch Loss:     2.499016, Tokens per Sec:    14085, Lr: 0.000300\n",
      "2021-07-09 08:02:20,845 - INFO - joeynmt.training - Epoch   5, Step:    32000, Batch Loss:     2.450066, Tokens per Sec:    14263, Lr: 0.000300\n",
      "2021-07-09 08:02:46,637 - INFO - joeynmt.training - Epoch   5, Step:    32200, Batch Loss:     2.343368, Tokens per Sec:    14604, Lr: 0.000300\n",
      "2021-07-09 08:03:12,514 - INFO - joeynmt.training - Epoch   5, Step:    32400, Batch Loss:     2.343026, Tokens per Sec:    14718, Lr: 0.000300\n",
      "2021-07-09 08:03:38,016 - INFO - joeynmt.training - Epoch   5, Step:    32600, Batch Loss:     2.591726, Tokens per Sec:    14464, Lr: 0.000300\n",
      "2021-07-09 08:03:55,545 - INFO - joeynmt.training - Epoch   5: total training loss 16144.77\n",
      "2021-07-09 08:03:55,545 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-09 08:04:04,459 - INFO - joeynmt.training - Epoch   6, Step:    32800, Batch Loss:     2.213998, Tokens per Sec:    13166, Lr: 0.000300\n",
      "2021-07-09 08:04:29,879 - INFO - joeynmt.training - Epoch   6, Step:    33000, Batch Loss:     2.357696, Tokens per Sec:    14442, Lr: 0.000300\n",
      "2021-07-09 08:04:56,806 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:04:56,806 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:04:56,806 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:04:57,033 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:04:57,033 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:04:57,774 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:04:57,775 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:04:57,775 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:04:57,775 - INFO - joeynmt.training - \tHypothesis: Nararakaye .\n",
      "2021-07-09 08:04:57,775 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:04:57,776 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:04:57,776 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:04:57,777 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo byagize uruhare mu gusoma umuzingo .\n",
      "2021-07-09 08:04:57,777 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:04:57,777 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:04:57,777 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:04:57,778 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twumve ko twiringira Imana binyuriye mu gusoma Ijambo rye . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:04:57,778 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:04:57,778 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:04:57,778 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:04:57,779 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bababarira , bagahungira mu mutego wo kwakira , bumva ko ari ibintu byuzuye mu isi ya Satani .\n",
      "2021-07-09 08:04:57,779 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    33000: bleu:  15.63, loss: 57091.4727, ppl:   7.5405, duration: 27.8990s\n",
      "2021-07-09 08:05:23,657 - INFO - joeynmt.training - Epoch   6, Step:    33200, Batch Loss:     2.396231, Tokens per Sec:    14345, Lr: 0.000300\n",
      "2021-07-09 08:05:49,204 - INFO - joeynmt.training - Epoch   6, Step:    33400, Batch Loss:     2.434979, Tokens per Sec:    14194, Lr: 0.000300\n",
      "2021-07-09 08:06:14,765 - INFO - joeynmt.training - Epoch   6, Step:    33600, Batch Loss:     2.293648, Tokens per Sec:    14602, Lr: 0.000300\n",
      "2021-07-09 08:06:40,348 - INFO - joeynmt.training - Epoch   6, Step:    33800, Batch Loss:     2.265874, Tokens per Sec:    14726, Lr: 0.000300\n",
      "2021-07-09 08:07:05,899 - INFO - joeynmt.training - Epoch   6, Step:    34000, Batch Loss:     2.378252, Tokens per Sec:    14453, Lr: 0.000300\n",
      "2021-07-09 08:07:31,614 - INFO - joeynmt.training - Epoch   6, Step:    34200, Batch Loss:     2.289596, Tokens per Sec:    14503, Lr: 0.000300\n",
      "2021-07-09 08:07:57,220 - INFO - joeynmt.training - Epoch   6, Step:    34400, Batch Loss:     2.405790, Tokens per Sec:    14340, Lr: 0.000300\n",
      "2021-07-09 08:08:22,887 - INFO - joeynmt.training - Epoch   6, Step:    34600, Batch Loss:     1.991048, Tokens per Sec:    14387, Lr: 0.000300\n",
      "2021-07-09 08:08:48,670 - INFO - joeynmt.training - Epoch   6, Step:    34800, Batch Loss:     2.459593, Tokens per Sec:    14164, Lr: 0.000300\n",
      "2021-07-09 08:09:14,219 - INFO - joeynmt.training - Epoch   6, Step:    35000, Batch Loss:     2.443828, Tokens per Sec:    14327, Lr: 0.000300\n",
      "2021-07-09 08:09:39,848 - INFO - joeynmt.training - Epoch   6, Step:    35200, Batch Loss:     2.287667, Tokens per Sec:    14351, Lr: 0.000300\n",
      "2021-07-09 08:10:05,649 - INFO - joeynmt.training - Epoch   6, Step:    35400, Batch Loss:     2.373338, Tokens per Sec:    14707, Lr: 0.000300\n",
      "2021-07-09 08:10:31,259 - INFO - joeynmt.training - Epoch   6, Step:    35600, Batch Loss:     2.375925, Tokens per Sec:    14434, Lr: 0.000300\n",
      "2021-07-09 08:10:56,851 - INFO - joeynmt.training - Epoch   6, Step:    35800, Batch Loss:     2.108629, Tokens per Sec:    14664, Lr: 0.000300\n",
      "2021-07-09 08:11:22,118 - INFO - joeynmt.training - Epoch   6, Step:    36000, Batch Loss:     2.244219, Tokens per Sec:    14102, Lr: 0.000300\n",
      "2021-07-09 08:11:49,860 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:11:49,861 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:11:49,861 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:11:50,103 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:11:50,103 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:11:50,833 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:11:50,835 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:11:50,835 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:11:50,835 - INFO - joeynmt.training - \tHypothesis: Narabyishimiye .\n",
      "2021-07-09 08:11:50,835 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:11:50,836 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:11:50,836 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:11:50,836 - INFO - joeynmt.training - \tHypothesis: Icyakora , reka turebe icyo gusoma imvura .\n",
      "2021-07-09 08:11:50,836 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:11:50,837 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:11:50,837 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:11:50,837 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa tukareba , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:11:50,837 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:11:50,838 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:11:50,838 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:11:50,838 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahumuriza , bagasanga mu mutego w’ibanze , bumva bafite imitima itaryarya mu isi ya Satani .\n",
      "2021-07-09 08:11:50,838 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    36000: bleu:  16.45, loss: 56009.3008, ppl:   7.2572, duration: 28.7192s\n",
      "2021-07-09 08:12:16,501 - INFO - joeynmt.training - Epoch   6, Step:    36200, Batch Loss:     2.449384, Tokens per Sec:    14297, Lr: 0.000300\n",
      "2021-07-09 08:12:41,826 - INFO - joeynmt.training - Epoch   6, Step:    36400, Batch Loss:     2.507075, Tokens per Sec:    14357, Lr: 0.000300\n",
      "2021-07-09 08:13:07,459 - INFO - joeynmt.training - Epoch   6, Step:    36600, Batch Loss:     2.511630, Tokens per Sec:    14512, Lr: 0.000300\n",
      "2021-07-09 08:13:33,038 - INFO - joeynmt.training - Epoch   6, Step:    36800, Batch Loss:     2.440967, Tokens per Sec:    14575, Lr: 0.000300\n",
      "2021-07-09 08:13:58,739 - INFO - joeynmt.training - Epoch   6, Step:    37000, Batch Loss:     2.472954, Tokens per Sec:    14730, Lr: 0.000300\n",
      "2021-07-09 08:14:24,154 - INFO - joeynmt.training - Epoch   6, Step:    37200, Batch Loss:     2.656886, Tokens per Sec:    14792, Lr: 0.000300\n",
      "2021-07-09 08:14:49,749 - INFO - joeynmt.training - Epoch   6, Step:    37400, Batch Loss:     2.547359, Tokens per Sec:    14520, Lr: 0.000300\n",
      "2021-07-09 08:15:15,300 - INFO - joeynmt.training - Epoch   6, Step:    37600, Batch Loss:     2.393202, Tokens per Sec:    14403, Lr: 0.000300\n",
      "2021-07-09 08:15:40,921 - INFO - joeynmt.training - Epoch   6, Step:    37800, Batch Loss:     2.440405, Tokens per Sec:    14360, Lr: 0.000300\n",
      "2021-07-09 08:16:06,620 - INFO - joeynmt.training - Epoch   6, Step:    38000, Batch Loss:     2.306353, Tokens per Sec:    14771, Lr: 0.000300\n",
      "2021-07-09 08:16:31,905 - INFO - joeynmt.training - Epoch   6, Step:    38200, Batch Loss:     2.520242, Tokens per Sec:    14189, Lr: 0.000300\n",
      "2021-07-09 08:16:57,213 - INFO - joeynmt.training - Epoch   6, Step:    38400, Batch Loss:     2.505956, Tokens per Sec:    14505, Lr: 0.000300\n",
      "2021-07-09 08:17:22,373 - INFO - joeynmt.training - Epoch   6, Step:    38600, Batch Loss:     2.593721, Tokens per Sec:    14357, Lr: 0.000300\n",
      "2021-07-09 08:17:47,672 - INFO - joeynmt.training - Epoch   6, Step:    38800, Batch Loss:     2.476445, Tokens per Sec:    14417, Lr: 0.000300\n",
      "2021-07-09 08:18:13,376 - INFO - joeynmt.training - Epoch   6, Step:    39000, Batch Loss:     2.290950, Tokens per Sec:    14846, Lr: 0.000300\n",
      "2021-07-09 08:18:36,589 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:18:36,589 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:18:36,589 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:18:36,823 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:18:36,823 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:18:37,863 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:18:37,864 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:18:37,864 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:18:37,865 - INFO - joeynmt.training - \tHypothesis: Narakomeje .\n",
      "2021-07-09 08:18:37,865 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:18:37,865 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:18:37,865 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:18:37,866 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byabaga bikubiyemo mu isomo ry’umuzingo .\n",
      "2021-07-09 08:18:37,866 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:18:37,866 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:18:37,866 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:18:37,867 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa tukareba , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:18:37,867 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:18:37,867 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:18:37,867 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:18:37,868 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko mu itorero rya gikristo , bagasanga mu buryo butandukanye , bumva ko ari ibintu byimbitse mu isi ya Satani .\n",
      "2021-07-09 08:18:37,868 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    39000: bleu:  16.98, loss: 55082.5664, ppl:   7.0231, duration: 24.4916s\n",
      "2021-07-09 08:19:03,537 - INFO - joeynmt.training - Epoch   6, Step:    39200, Batch Loss:     2.438573, Tokens per Sec:    14343, Lr: 0.000300\n",
      "2021-07-09 08:19:13,851 - INFO - joeynmt.training - Epoch   6: total training loss 15626.07\n",
      "2021-07-09 08:19:13,852 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-09 08:19:29,967 - INFO - joeynmt.training - Epoch   7, Step:    39400, Batch Loss:     2.345485, Tokens per Sec:    13783, Lr: 0.000300\n",
      "2021-07-09 08:19:55,574 - INFO - joeynmt.training - Epoch   7, Step:    39600, Batch Loss:     2.344150, Tokens per Sec:    14757, Lr: 0.000300\n",
      "2021-07-09 08:20:21,129 - INFO - joeynmt.training - Epoch   7, Step:    39800, Batch Loss:     2.441956, Tokens per Sec:    14289, Lr: 0.000300\n",
      "2021-07-09 08:20:46,832 - INFO - joeynmt.training - Epoch   7, Step:    40000, Batch Loss:     2.291755, Tokens per Sec:    14731, Lr: 0.000300\n",
      "2021-07-09 08:21:12,271 - INFO - joeynmt.training - Epoch   7, Step:    40200, Batch Loss:     2.369619, Tokens per Sec:    14503, Lr: 0.000300\n",
      "2021-07-09 08:21:37,564 - INFO - joeynmt.training - Epoch   7, Step:    40400, Batch Loss:     2.482849, Tokens per Sec:    14519, Lr: 0.000300\n",
      "2021-07-09 08:22:03,166 - INFO - joeynmt.training - Epoch   7, Step:    40600, Batch Loss:     2.233906, Tokens per Sec:    14368, Lr: 0.000300\n",
      "2021-07-09 08:22:28,751 - INFO - joeynmt.training - Epoch   7, Step:    40800, Batch Loss:     2.398271, Tokens per Sec:    14427, Lr: 0.000300\n",
      "2021-07-09 08:22:54,226 - INFO - joeynmt.training - Epoch   7, Step:    41000, Batch Loss:     2.293740, Tokens per Sec:    14355, Lr: 0.000300\n",
      "2021-07-09 08:23:19,804 - INFO - joeynmt.training - Epoch   7, Step:    41200, Batch Loss:     2.327956, Tokens per Sec:    14634, Lr: 0.000300\n",
      "2021-07-09 08:23:45,287 - INFO - joeynmt.training - Epoch   7, Step:    41400, Batch Loss:     2.301351, Tokens per Sec:    14486, Lr: 0.000300\n",
      "2021-07-09 08:24:10,908 - INFO - joeynmt.training - Epoch   7, Step:    41600, Batch Loss:     2.288408, Tokens per Sec:    14477, Lr: 0.000300\n",
      "2021-07-09 08:24:36,362 - INFO - joeynmt.training - Epoch   7, Step:    41800, Batch Loss:     2.249432, Tokens per Sec:    14540, Lr: 0.000300\n",
      "2021-07-09 08:25:01,871 - INFO - joeynmt.training - Epoch   7, Step:    42000, Batch Loss:     2.537254, Tokens per Sec:    14649, Lr: 0.000300\n",
      "2021-07-09 08:25:25,156 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:25:25,156 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:25:25,156 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:25:25,386 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:25:25,386 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:25:26,415 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:25:26,417 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:25:26,417 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:25:26,417 - INFO - joeynmt.training - \tHypothesis: Narakoraga .\n",
      "2021-07-09 08:25:26,417 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:25:26,418 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:25:26,418 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:25:26,418 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo gusoma imizingo .\n",
      "2021-07-09 08:25:26,418 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:25:26,418 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:25:26,419 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:25:26,419 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twiringire Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:25:26,420 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:25:26,421 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:25:26,421 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:25:26,421 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bahamya ko mu itorero rya gikristo , nibura mu buryo bw’ikigereranyo , bumva ko ari ibintu byihutirwa mu isi ya Satani .\n",
      "2021-07-09 08:25:26,421 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    42000: bleu:  17.15, loss: 54270.2344, ppl:   6.8241, duration: 24.5497s\n",
      "2021-07-09 08:25:52,370 - INFO - joeynmt.training - Epoch   7, Step:    42200, Batch Loss:     2.436847, Tokens per Sec:    14349, Lr: 0.000300\n",
      "2021-07-09 08:26:17,806 - INFO - joeynmt.training - Epoch   7, Step:    42400, Batch Loss:     2.481016, Tokens per Sec:    14279, Lr: 0.000300\n",
      "2021-07-09 08:26:43,111 - INFO - joeynmt.training - Epoch   7, Step:    42600, Batch Loss:     2.233800, Tokens per Sec:    14543, Lr: 0.000300\n",
      "2021-07-09 08:27:08,580 - INFO - joeynmt.training - Epoch   7, Step:    42800, Batch Loss:     2.437875, Tokens per Sec:    14342, Lr: 0.000300\n",
      "2021-07-09 08:27:34,388 - INFO - joeynmt.training - Epoch   7, Step:    43000, Batch Loss:     2.232581, Tokens per Sec:    14682, Lr: 0.000300\n",
      "2021-07-09 08:27:59,986 - INFO - joeynmt.training - Epoch   7, Step:    43200, Batch Loss:     2.334667, Tokens per Sec:    14713, Lr: 0.000300\n",
      "2021-07-09 08:28:25,362 - INFO - joeynmt.training - Epoch   7, Step:    43400, Batch Loss:     2.284982, Tokens per Sec:    14450, Lr: 0.000300\n",
      "2021-07-09 08:28:50,947 - INFO - joeynmt.training - Epoch   7, Step:    43600, Batch Loss:     2.341472, Tokens per Sec:    14388, Lr: 0.000300\n",
      "2021-07-09 08:29:16,362 - INFO - joeynmt.training - Epoch   7, Step:    43800, Batch Loss:     2.209591, Tokens per Sec:    14532, Lr: 0.000300\n",
      "2021-07-09 08:29:41,853 - INFO - joeynmt.training - Epoch   7, Step:    44000, Batch Loss:     2.252507, Tokens per Sec:    14303, Lr: 0.000300\n",
      "2021-07-09 08:30:07,523 - INFO - joeynmt.training - Epoch   7, Step:    44200, Batch Loss:     2.487005, Tokens per Sec:    14623, Lr: 0.000300\n",
      "2021-07-09 08:30:32,949 - INFO - joeynmt.training - Epoch   7, Step:    44400, Batch Loss:     2.167272, Tokens per Sec:    14452, Lr: 0.000300\n",
      "2021-07-09 08:30:58,263 - INFO - joeynmt.training - Epoch   7, Step:    44600, Batch Loss:     2.839339, Tokens per Sec:    14477, Lr: 0.000300\n",
      "2021-07-09 08:31:23,783 - INFO - joeynmt.training - Epoch   7, Step:    44800, Batch Loss:     2.242273, Tokens per Sec:    14268, Lr: 0.000300\n",
      "2021-07-09 08:31:49,409 - INFO - joeynmt.training - Epoch   7, Step:    45000, Batch Loss:     2.489680, Tokens per Sec:    14658, Lr: 0.000300\n",
      "2021-07-09 08:32:11,715 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:32:11,715 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:32:11,715 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:32:11,946 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:32:11,946 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:32:12,656 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:32:12,657 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:32:12,658 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:32:12,658 - INFO - joeynmt.training - \tHypothesis: Narushijeho kwishimisha .\n",
      "2021-07-09 08:32:12,658 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:32:12,658 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:32:12,659 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:32:12,659 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka turebe icyo ibyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 08:32:12,659 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:32:12,659 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:32:12,659 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:32:12,660 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa tukiheba , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:32:12,660 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:32:12,660 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:32:12,660 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:32:12,661 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko mu gihe cy’umwanya , bumvaga ko ari umutekano mu isi ya Satani .\n",
      "2021-07-09 08:32:12,661 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    45000: bleu:  17.95, loss: 53609.9805, ppl:   6.6665, duration: 23.2509s\n",
      "2021-07-09 08:32:38,209 - INFO - joeynmt.training - Epoch   7, Step:    45200, Batch Loss:     2.389427, Tokens per Sec:    14547, Lr: 0.000300\n",
      "2021-07-09 08:33:03,455 - INFO - joeynmt.training - Epoch   7, Step:    45400, Batch Loss:     2.129729, Tokens per Sec:    14182, Lr: 0.000300\n",
      "2021-07-09 08:33:28,928 - INFO - joeynmt.training - Epoch   7, Step:    45600, Batch Loss:     2.175188, Tokens per Sec:    14350, Lr: 0.000300\n",
      "2021-07-09 08:33:54,390 - INFO - joeynmt.training - Epoch   7, Step:    45800, Batch Loss:     2.376856, Tokens per Sec:    14114, Lr: 0.000300\n",
      "2021-07-09 08:33:58,427 - INFO - joeynmt.training - Epoch   7: total training loss 15246.79\n",
      "2021-07-09 08:33:58,427 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-09 08:34:20,433 - INFO - joeynmt.training - Epoch   8, Step:    46000, Batch Loss:     2.367654, Tokens per Sec:    14134, Lr: 0.000300\n",
      "2021-07-09 08:34:45,646 - INFO - joeynmt.training - Epoch   8, Step:    46200, Batch Loss:     2.236104, Tokens per Sec:    14308, Lr: 0.000300\n",
      "2021-07-09 08:35:10,919 - INFO - joeynmt.training - Epoch   8, Step:    46400, Batch Loss:     2.276708, Tokens per Sec:    14476, Lr: 0.000300\n",
      "2021-07-09 08:35:36,486 - INFO - joeynmt.training - Epoch   8, Step:    46600, Batch Loss:     2.135468, Tokens per Sec:    14478, Lr: 0.000300\n",
      "2021-07-09 08:36:01,887 - INFO - joeynmt.training - Epoch   8, Step:    46800, Batch Loss:     2.252616, Tokens per Sec:    14260, Lr: 0.000300\n",
      "2021-07-09 08:36:27,404 - INFO - joeynmt.training - Epoch   8, Step:    47000, Batch Loss:     2.652970, Tokens per Sec:    14462, Lr: 0.000300\n",
      "2021-07-09 08:36:52,964 - INFO - joeynmt.training - Epoch   8, Step:    47200, Batch Loss:     2.219832, Tokens per Sec:    14516, Lr: 0.000300\n",
      "2021-07-09 08:37:18,432 - INFO - joeynmt.training - Epoch   8, Step:    47400, Batch Loss:     2.397177, Tokens per Sec:    14489, Lr: 0.000300\n",
      "2021-07-09 08:37:43,859 - INFO - joeynmt.training - Epoch   8, Step:    47600, Batch Loss:     2.313928, Tokens per Sec:    14432, Lr: 0.000300\n",
      "2021-07-09 08:38:09,365 - INFO - joeynmt.training - Epoch   8, Step:    47800, Batch Loss:     2.231023, Tokens per Sec:    14269, Lr: 0.000300\n",
      "2021-07-09 08:38:35,067 - INFO - joeynmt.training - Epoch   8, Step:    48000, Batch Loss:     2.120733, Tokens per Sec:    14513, Lr: 0.000300\n",
      "2021-07-09 08:38:57,795 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:38:57,795 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:38:57,796 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:38:58,026 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:38:58,028 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:38:58,793 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:38:58,794 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:38:58,794 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:38:58,794 - INFO - joeynmt.training - \tHypothesis: Narakomeje .\n",
      "2021-07-09 08:38:58,794 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:38:58,795 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:38:58,795 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:38:58,795 - INFO - joeynmt.training - \tHypothesis: Ariko reka turebe icyo ibyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 08:38:58,795 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:38:58,796 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:38:58,796 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:38:58,796 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twihebye cyangwa tukareba , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:38:58,796 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:38:58,796 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:38:58,797 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:38:58,797 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko mu gihe cy’umwanya , bumva ko ari meza mu isi ya Satani .\n",
      "2021-07-09 08:38:58,797 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    48000: bleu:  18.24, loss: 53163.1016, ppl:   6.5619, duration: 23.7292s\n",
      "2021-07-09 08:39:24,424 - INFO - joeynmt.training - Epoch   8, Step:    48200, Batch Loss:     2.221258, Tokens per Sec:    14426, Lr: 0.000300\n",
      "2021-07-09 08:39:49,734 - INFO - joeynmt.training - Epoch   8, Step:    48400, Batch Loss:     2.173133, Tokens per Sec:    14105, Lr: 0.000300\n",
      "2021-07-09 08:40:15,256 - INFO - joeynmt.training - Epoch   8, Step:    48600, Batch Loss:     2.134390, Tokens per Sec:    14600, Lr: 0.000300\n",
      "2021-07-09 08:40:40,829 - INFO - joeynmt.training - Epoch   8, Step:    48800, Batch Loss:     2.321986, Tokens per Sec:    14731, Lr: 0.000300\n",
      "2021-07-09 08:41:06,303 - INFO - joeynmt.training - Epoch   8, Step:    49000, Batch Loss:     2.469491, Tokens per Sec:    14442, Lr: 0.000300\n",
      "2021-07-09 08:41:31,835 - INFO - joeynmt.training - Epoch   8, Step:    49200, Batch Loss:     2.556795, Tokens per Sec:    14575, Lr: 0.000300\n",
      "2021-07-09 08:41:57,459 - INFO - joeynmt.training - Epoch   8, Step:    49400, Batch Loss:     2.567019, Tokens per Sec:    14610, Lr: 0.000300\n",
      "2021-07-09 08:42:23,015 - INFO - joeynmt.training - Epoch   8, Step:    49600, Batch Loss:     2.189251, Tokens per Sec:    14494, Lr: 0.000300\n",
      "2021-07-09 08:42:48,384 - INFO - joeynmt.training - Epoch   8, Step:    49800, Batch Loss:     1.817431, Tokens per Sec:    14454, Lr: 0.000300\n",
      "2021-07-09 08:43:13,847 - INFO - joeynmt.training - Epoch   8, Step:    50000, Batch Loss:     2.343015, Tokens per Sec:    14503, Lr: 0.000300\n",
      "2021-07-09 08:43:39,501 - INFO - joeynmt.training - Epoch   8, Step:    50200, Batch Loss:     2.266056, Tokens per Sec:    14573, Lr: 0.000300\n",
      "2021-07-09 08:44:05,129 - INFO - joeynmt.training - Epoch   8, Step:    50400, Batch Loss:     2.164191, Tokens per Sec:    14475, Lr: 0.000300\n",
      "2021-07-09 08:44:30,693 - INFO - joeynmt.training - Epoch   8, Step:    50600, Batch Loss:     2.287955, Tokens per Sec:    14646, Lr: 0.000300\n",
      "2021-07-09 08:44:56,114 - INFO - joeynmt.training - Epoch   8, Step:    50800, Batch Loss:     2.205779, Tokens per Sec:    14416, Lr: 0.000300\n",
      "2021-07-09 08:45:21,633 - INFO - joeynmt.training - Epoch   8, Step:    51000, Batch Loss:     2.086081, Tokens per Sec:    14720, Lr: 0.000300\n",
      "2021-07-09 08:45:41,053 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:45:41,054 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:45:41,054 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:45:41,279 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:45:41,279 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:45:41,999 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:45:41,999 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:45:42,000 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:45:42,000 - INFO - joeynmt.training - \tHypothesis: Umutima wanjye wari warakoze .\n",
      "2021-07-09 08:45:42,000 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:45:42,000 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:45:42,001 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:45:42,002 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka turebe icyo ibyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 08:45:42,002 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:45:42,002 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:45:42,003 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:45:42,003 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twumve ko twiringiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:45:42,003 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:45:42,003 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:45:42,003 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:45:42,004 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bahamya ko mu gihe cy’ubutegetsi , bumva ko ari byiza mu isi ya Satani .\n",
      "2021-07-09 08:45:42,004 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    51000: bleu:  18.22, loss: 52971.1211, ppl:   6.5175, duration: 20.3703s\n",
      "2021-07-09 08:46:07,597 - INFO - joeynmt.training - Epoch   8, Step:    51200, Batch Loss:     2.255913, Tokens per Sec:    14555, Lr: 0.000300\n",
      "2021-07-09 08:46:33,113 - INFO - joeynmt.training - Epoch   8, Step:    51400, Batch Loss:     2.379339, Tokens per Sec:    14374, Lr: 0.000300\n",
      "2021-07-09 08:46:58,615 - INFO - joeynmt.training - Epoch   8, Step:    51600, Batch Loss:     2.244882, Tokens per Sec:    14405, Lr: 0.000300\n",
      "2021-07-09 08:47:24,180 - INFO - joeynmt.training - Epoch   8, Step:    51800, Batch Loss:     2.096465, Tokens per Sec:    14370, Lr: 0.000300\n",
      "2021-07-09 08:47:49,717 - INFO - joeynmt.training - Epoch   8, Step:    52000, Batch Loss:     2.439407, Tokens per Sec:    14466, Lr: 0.000300\n",
      "2021-07-09 08:48:15,527 - INFO - joeynmt.training - Epoch   8, Step:    52200, Batch Loss:     2.250556, Tokens per Sec:    14781, Lr: 0.000300\n",
      "2021-07-09 08:48:38,422 - INFO - joeynmt.training - Epoch   8: total training loss 14929.62\n",
      "2021-07-09 08:48:38,422 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-09 08:48:41,584 - INFO - joeynmt.training - Epoch   9, Step:    52400, Batch Loss:     2.455301, Tokens per Sec:    11845, Lr: 0.000300\n",
      "2021-07-09 08:49:07,162 - INFO - joeynmt.training - Epoch   9, Step:    52600, Batch Loss:     2.132486, Tokens per Sec:    14768, Lr: 0.000300\n",
      "2021-07-09 08:49:32,179 - INFO - joeynmt.training - Epoch   9, Step:    52800, Batch Loss:     2.465905, Tokens per Sec:    14259, Lr: 0.000300\n",
      "2021-07-09 08:49:57,624 - INFO - joeynmt.training - Epoch   9, Step:    53000, Batch Loss:     2.384553, Tokens per Sec:    14586, Lr: 0.000300\n",
      "2021-07-09 08:50:23,249 - INFO - joeynmt.training - Epoch   9, Step:    53200, Batch Loss:     2.269924, Tokens per Sec:    14622, Lr: 0.000300\n",
      "2021-07-09 08:50:48,729 - INFO - joeynmt.training - Epoch   9, Step:    53400, Batch Loss:     2.238026, Tokens per Sec:    14566, Lr: 0.000300\n",
      "2021-07-09 08:51:14,134 - INFO - joeynmt.training - Epoch   9, Step:    53600, Batch Loss:     2.261854, Tokens per Sec:    14373, Lr: 0.000300\n",
      "2021-07-09 08:51:39,607 - INFO - joeynmt.training - Epoch   9, Step:    53800, Batch Loss:     2.042960, Tokens per Sec:    14295, Lr: 0.000300\n",
      "2021-07-09 08:52:05,283 - INFO - joeynmt.training - Epoch   9, Step:    54000, Batch Loss:     2.317174, Tokens per Sec:    14691, Lr: 0.000300\n",
      "2021-07-09 08:52:26,065 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:52:26,066 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:52:26,066 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:52:26,295 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:52:26,296 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:52:27,055 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:52:27,057 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:52:27,057 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:52:27,057 - INFO - joeynmt.training - \tHypothesis: Narabyishimiye .\n",
      "2021-07-09 08:52:27,057 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:52:27,058 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:52:27,058 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:52:27,058 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byagize uruhare mu gusoma umuzingo .\n",
      "2021-07-09 08:52:27,058 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:52:27,060 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:52:27,060 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:52:27,060 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twihebye cyangwa tukihebye , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:52:27,060 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:52:27,061 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:52:27,061 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:52:27,061 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko nibura mu giti cy’umwelayo , bumva bihumuriza mu isi ya Satani .\n",
      "2021-07-09 08:52:27,061 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    54000: bleu:  18.48, loss: 52267.3359, ppl:   6.3572, duration: 21.7779s\n",
      "2021-07-09 08:52:52,873 - INFO - joeynmt.training - Epoch   9, Step:    54200, Batch Loss:     2.186759, Tokens per Sec:    14461, Lr: 0.000300\n",
      "2021-07-09 08:53:18,357 - INFO - joeynmt.training - Epoch   9, Step:    54400, Batch Loss:     2.459149, Tokens per Sec:    14309, Lr: 0.000300\n",
      "2021-07-09 08:53:43,654 - INFO - joeynmt.training - Epoch   9, Step:    54600, Batch Loss:     2.325958, Tokens per Sec:    14411, Lr: 0.000300\n",
      "2021-07-09 08:54:09,085 - INFO - joeynmt.training - Epoch   9, Step:    54800, Batch Loss:     2.304896, Tokens per Sec:    14594, Lr: 0.000300\n",
      "2021-07-09 08:54:34,714 - INFO - joeynmt.training - Epoch   9, Step:    55000, Batch Loss:     2.122601, Tokens per Sec:    14536, Lr: 0.000300\n",
      "2021-07-09 08:55:00,318 - INFO - joeynmt.training - Epoch   9, Step:    55200, Batch Loss:     2.057992, Tokens per Sec:    14788, Lr: 0.000300\n",
      "2021-07-09 08:55:25,714 - INFO - joeynmt.training - Epoch   9, Step:    55400, Batch Loss:     2.163621, Tokens per Sec:    14466, Lr: 0.000300\n",
      "2021-07-09 08:55:51,087 - INFO - joeynmt.training - Epoch   9, Step:    55600, Batch Loss:     2.216389, Tokens per Sec:    14485, Lr: 0.000300\n",
      "2021-07-09 08:56:16,362 - INFO - joeynmt.training - Epoch   9, Step:    55800, Batch Loss:     2.153164, Tokens per Sec:    14707, Lr: 0.000300\n",
      "2021-07-09 08:56:41,542 - INFO - joeynmt.training - Epoch   9, Step:    56000, Batch Loss:     2.393912, Tokens per Sec:    14618, Lr: 0.000300\n",
      "2021-07-09 08:57:06,843 - INFO - joeynmt.training - Epoch   9, Step:    56200, Batch Loss:     2.190706, Tokens per Sec:    14560, Lr: 0.000300\n",
      "2021-07-09 08:57:32,204 - INFO - joeynmt.training - Epoch   9, Step:    56400, Batch Loss:     2.029211, Tokens per Sec:    14568, Lr: 0.000300\n",
      "2021-07-09 08:57:57,487 - INFO - joeynmt.training - Epoch   9, Step:    56600, Batch Loss:     2.166894, Tokens per Sec:    14551, Lr: 0.000300\n",
      "2021-07-09 08:58:23,023 - INFO - joeynmt.training - Epoch   9, Step:    56800, Batch Loss:     2.287327, Tokens per Sec:    14618, Lr: 0.000300\n",
      "2021-07-09 08:58:48,276 - INFO - joeynmt.training - Epoch   9, Step:    57000, Batch Loss:     2.215001, Tokens per Sec:    14814, Lr: 0.000300\n",
      "2021-07-09 08:59:10,750 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 08:59:10,750 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 08:59:10,750 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 08:59:10,974 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 08:59:10,975 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 08:59:11,684 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 08:59:11,685 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 08:59:11,685 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 08:59:11,685 - INFO - joeynmt.training - \tHypothesis: Narakomeje cyane .\n",
      "2021-07-09 08:59:11,685 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 08:59:11,686 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 08:59:11,686 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 08:59:11,686 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 08:59:11,686 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 08:59:11,687 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 08:59:11,688 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:59:11,688 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twirinde kurwanya cyangwa guhura n’ikibazo , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 08:59:11,688 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 08:59:11,688 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 08:59:11,688 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 08:59:11,689 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bahamya ko mu gihe cy’ubutegetsi , bumva bafite ihumure mu isi ya Satani .\n",
      "2021-07-09 08:59:11,689 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    57000: bleu:  19.25, loss: 51397.9258, ppl:   6.1646, duration: 23.4127s\n",
      "2021-07-09 08:59:37,239 - INFO - joeynmt.training - Epoch   9, Step:    57200, Batch Loss:     1.971125, Tokens per Sec:    14451, Lr: 0.000300\n",
      "2021-07-09 09:00:02,398 - INFO - joeynmt.training - Epoch   9, Step:    57400, Batch Loss:     2.177154, Tokens per Sec:    14396, Lr: 0.000300\n",
      "2021-07-09 09:00:27,760 - INFO - joeynmt.training - Epoch   9, Step:    57600, Batch Loss:     2.109509, Tokens per Sec:    14617, Lr: 0.000300\n",
      "2021-07-09 09:00:53,122 - INFO - joeynmt.training - Epoch   9, Step:    57800, Batch Loss:     1.970387, Tokens per Sec:    14549, Lr: 0.000300\n",
      "2021-07-09 09:01:18,413 - INFO - joeynmt.training - Epoch   9, Step:    58000, Batch Loss:     2.328943, Tokens per Sec:    14522, Lr: 0.000300\n",
      "2021-07-09 09:01:43,778 - INFO - joeynmt.training - Epoch   9, Step:    58200, Batch Loss:     2.275949, Tokens per Sec:    14406, Lr: 0.000300\n",
      "2021-07-09 09:02:09,321 - INFO - joeynmt.training - Epoch   9, Step:    58400, Batch Loss:     2.300200, Tokens per Sec:    14690, Lr: 0.000300\n",
      "2021-07-09 09:02:34,619 - INFO - joeynmt.training - Epoch   9, Step:    58600, Batch Loss:     2.078183, Tokens per Sec:    14353, Lr: 0.000300\n",
      "2021-07-09 09:02:59,868 - INFO - joeynmt.training - Epoch   9, Step:    58800, Batch Loss:     2.157119, Tokens per Sec:    14635, Lr: 0.000300\n",
      "2021-07-09 09:03:16,489 - INFO - joeynmt.training - Epoch   9: total training loss 14677.65\n",
      "2021-07-09 09:03:16,490 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-09 09:03:25,932 - INFO - joeynmt.training - Epoch  10, Step:    59000, Batch Loss:     2.237999, Tokens per Sec:    13644, Lr: 0.000300\n",
      "2021-07-09 09:03:51,280 - INFO - joeynmt.training - Epoch  10, Step:    59200, Batch Loss:     2.268501, Tokens per Sec:    14549, Lr: 0.000300\n",
      "2021-07-09 09:04:16,663 - INFO - joeynmt.training - Epoch  10, Step:    59400, Batch Loss:     2.222792, Tokens per Sec:    14455, Lr: 0.000300\n",
      "2021-07-09 09:04:42,209 - INFO - joeynmt.training - Epoch  10, Step:    59600, Batch Loss:     2.312429, Tokens per Sec:    14700, Lr: 0.000300\n",
      "2021-07-09 09:05:07,485 - INFO - joeynmt.training - Epoch  10, Step:    59800, Batch Loss:     1.923849, Tokens per Sec:    14274, Lr: 0.000300\n",
      "2021-07-09 09:05:33,064 - INFO - joeynmt.training - Epoch  10, Step:    60000, Batch Loss:     2.242729, Tokens per Sec:    14718, Lr: 0.000300\n",
      "2021-07-09 09:05:54,046 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:05:54,046 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:05:54,047 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:05:54,269 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:05:54,269 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:05:54,974 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:05:54,975 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:05:54,975 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:05:54,975 - INFO - joeynmt.training - \tHypothesis: Narakoraga .\n",
      "2021-07-09 09:05:54,975 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:05:54,976 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:05:54,976 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:05:54,976 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka turebe icyo ibyo byakoreshwaga mu gusoma umuzingo .\n",
      "2021-07-09 09:05:54,976 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:05:54,977 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:05:54,977 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:05:54,977 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twihebye cyangwa tukiheba , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:05:54,977 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:05:54,978 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:05:54,978 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:05:54,978 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ariko , bamwe mu bagize itorero rya Gikristo bahamya ko nibura mu rugero runaka , bumva ko ari byiza mu isi ya Satani .\n",
      "2021-07-09 09:05:54,978 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    60000: bleu:  19.29, loss: 51015.2656, ppl:   6.0816, duration: 21.9137s\n",
      "2021-07-09 09:06:21,159 - INFO - joeynmt.training - Epoch  10, Step:    60200, Batch Loss:     2.197103, Tokens per Sec:    14575, Lr: 0.000300\n",
      "2021-07-09 09:06:46,552 - INFO - joeynmt.training - Epoch  10, Step:    60400, Batch Loss:     2.121588, Tokens per Sec:    14402, Lr: 0.000300\n",
      "2021-07-09 09:07:11,920 - INFO - joeynmt.training - Epoch  10, Step:    60600, Batch Loss:     2.245156, Tokens per Sec:    14875, Lr: 0.000300\n",
      "2021-07-09 09:07:37,405 - INFO - joeynmt.training - Epoch  10, Step:    60800, Batch Loss:     2.768448, Tokens per Sec:    14700, Lr: 0.000300\n",
      "2021-07-09 09:08:02,787 - INFO - joeynmt.training - Epoch  10, Step:    61000, Batch Loss:     2.292240, Tokens per Sec:    14590, Lr: 0.000300\n",
      "2021-07-09 09:08:28,122 - INFO - joeynmt.training - Epoch  10, Step:    61200, Batch Loss:     2.337671, Tokens per Sec:    14809, Lr: 0.000300\n",
      "2021-07-09 09:08:53,500 - INFO - joeynmt.training - Epoch  10, Step:    61400, Batch Loss:     2.394583, Tokens per Sec:    14698, Lr: 0.000300\n",
      "2021-07-09 09:09:19,028 - INFO - joeynmt.training - Epoch  10, Step:    61600, Batch Loss:     2.065835, Tokens per Sec:    14880, Lr: 0.000300\n",
      "2021-07-09 09:09:44,097 - INFO - joeynmt.training - Epoch  10, Step:    61800, Batch Loss:     1.728097, Tokens per Sec:    14486, Lr: 0.000300\n",
      "2021-07-09 09:10:09,426 - INFO - joeynmt.training - Epoch  10, Step:    62000, Batch Loss:     2.519044, Tokens per Sec:    14570, Lr: 0.000300\n",
      "2021-07-09 09:10:34,614 - INFO - joeynmt.training - Epoch  10, Step:    62200, Batch Loss:     2.296033, Tokens per Sec:    14406, Lr: 0.000300\n",
      "2021-07-09 09:10:59,842 - INFO - joeynmt.training - Epoch  10, Step:    62400, Batch Loss:     2.588439, Tokens per Sec:    14731, Lr: 0.000300\n",
      "2021-07-09 09:11:25,102 - INFO - joeynmt.training - Epoch  10, Step:    62600, Batch Loss:     2.197614, Tokens per Sec:    14539, Lr: 0.000300\n",
      "2021-07-09 09:11:50,606 - INFO - joeynmt.training - Epoch  10, Step:    62800, Batch Loss:     2.463087, Tokens per Sec:    14662, Lr: 0.000300\n",
      "2021-07-09 09:12:15,753 - INFO - joeynmt.training - Epoch  10, Step:    63000, Batch Loss:     2.102380, Tokens per Sec:    14390, Lr: 0.000300\n",
      "2021-07-09 09:12:35,983 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:12:35,983 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:12:35,983 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:12:36,210 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:12:36,210 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:12:36,911 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:12:36,912 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:12:36,912 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:12:36,912 - INFO - joeynmt.training - \tHypothesis: Narakoraga .\n",
      "2021-07-09 09:12:36,912 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:12:36,913 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:12:36,913 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:12:36,913 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 09:12:36,913 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:12:36,915 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:12:36,915 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:12:36,915 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa duhura n’ibibazo , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:12:36,915 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:12:36,916 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:12:36,916 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:12:36,916 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 09:12:36,917 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    63000: bleu:  19.88, loss: 50691.2227, ppl:   6.0123, duration: 21.1628s\n",
      "2021-07-09 09:13:02,123 - INFO - joeynmt.training - Epoch  10, Step:    63200, Batch Loss:     2.296990, Tokens per Sec:    14249, Lr: 0.000300\n",
      "2021-07-09 09:13:27,531 - INFO - joeynmt.training - Epoch  10, Step:    63400, Batch Loss:     2.722573, Tokens per Sec:    14609, Lr: 0.000300\n",
      "2021-07-09 09:13:52,719 - INFO - joeynmt.training - Epoch  10, Step:    63600, Batch Loss:     2.222153, Tokens per Sec:    14582, Lr: 0.000300\n",
      "2021-07-09 09:14:18,121 - INFO - joeynmt.training - Epoch  10, Step:    63800, Batch Loss:     2.065536, Tokens per Sec:    14541, Lr: 0.000300\n",
      "2021-07-09 09:14:43,382 - INFO - joeynmt.training - Epoch  10, Step:    64000, Batch Loss:     2.422596, Tokens per Sec:    14424, Lr: 0.000300\n",
      "2021-07-09 09:15:08,638 - INFO - joeynmt.training - Epoch  10, Step:    64200, Batch Loss:     2.203751, Tokens per Sec:    14571, Lr: 0.000300\n",
      "2021-07-09 09:15:33,950 - INFO - joeynmt.training - Epoch  10, Step:    64400, Batch Loss:     2.506034, Tokens per Sec:    14806, Lr: 0.000300\n",
      "2021-07-09 09:15:59,514 - INFO - joeynmt.training - Epoch  10, Step:    64600, Batch Loss:     2.199951, Tokens per Sec:    14835, Lr: 0.000300\n",
      "2021-07-09 09:16:24,695 - INFO - joeynmt.training - Epoch  10, Step:    64800, Batch Loss:     2.262887, Tokens per Sec:    14439, Lr: 0.000300\n",
      "2021-07-09 09:16:50,184 - INFO - joeynmt.training - Epoch  10, Step:    65000, Batch Loss:     2.172030, Tokens per Sec:    14573, Lr: 0.000300\n",
      "2021-07-09 09:17:15,478 - INFO - joeynmt.training - Epoch  10, Step:    65200, Batch Loss:     2.211372, Tokens per Sec:    14639, Lr: 0.000300\n",
      "2021-07-09 09:17:40,647 - INFO - joeynmt.training - Epoch  10, Step:    65400, Batch Loss:     2.191041, Tokens per Sec:    14592, Lr: 0.000300\n",
      "2021-07-09 09:17:49,301 - INFO - joeynmt.training - Epoch  10: total training loss 14424.64\n",
      "2021-07-09 09:17:49,301 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-09 09:18:06,531 - INFO - joeynmt.training - Epoch  11, Step:    65600, Batch Loss:     2.112751, Tokens per Sec:    14209, Lr: 0.000300\n",
      "2021-07-09 09:18:31,819 - INFO - joeynmt.training - Epoch  11, Step:    65800, Batch Loss:     2.144800, Tokens per Sec:    14623, Lr: 0.000300\n",
      "2021-07-09 09:18:57,233 - INFO - joeynmt.training - Epoch  11, Step:    66000, Batch Loss:     2.050485, Tokens per Sec:    14509, Lr: 0.000300\n",
      "2021-07-09 09:19:19,030 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:19:19,031 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:19:19,031 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:19:19,254 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:19:19,254 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:19:19,961 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:19:19,962 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:19:19,962 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:19:19,962 - INFO - joeynmt.training - \tHypothesis: Narakaye cyane .\n",
      "2021-07-09 09:19:19,962 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:19:19,963 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:19:19,963 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:19:19,963 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 09:19:19,963 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:19:19,964 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:19:19,964 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:19:19,964 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa duhura n’ingorane , twagombye kwizeza Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:19:19,965 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:19:19,965 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:19:19,965 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:19:19,965 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bahamya ko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 09:19:19,966 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    66000: bleu:  19.69, loss: 50130.7227, ppl:   5.8942, duration: 22.7317s\n",
      "2021-07-09 09:19:45,442 - INFO - joeynmt.training - Epoch  11, Step:    66200, Batch Loss:     2.100514, Tokens per Sec:    14307, Lr: 0.000300\n",
      "2021-07-09 09:20:10,891 - INFO - joeynmt.training - Epoch  11, Step:    66400, Batch Loss:     2.220546, Tokens per Sec:    14471, Lr: 0.000300\n",
      "2021-07-09 09:20:36,048 - INFO - joeynmt.training - Epoch  11, Step:    66600, Batch Loss:     2.223305, Tokens per Sec:    14414, Lr: 0.000300\n",
      "2021-07-09 09:21:01,244 - INFO - joeynmt.training - Epoch  11, Step:    66800, Batch Loss:     2.095352, Tokens per Sec:    14619, Lr: 0.000300\n",
      "2021-07-09 09:21:26,617 - INFO - joeynmt.training - Epoch  11, Step:    67000, Batch Loss:     2.114927, Tokens per Sec:    14617, Lr: 0.000300\n",
      "2021-07-09 09:21:51,684 - INFO - joeynmt.training - Epoch  11, Step:    67200, Batch Loss:     2.277342, Tokens per Sec:    14356, Lr: 0.000300\n",
      "2021-07-09 09:22:16,866 - INFO - joeynmt.training - Epoch  11, Step:    67400, Batch Loss:     2.408523, Tokens per Sec:    14425, Lr: 0.000300\n",
      "2021-07-09 09:22:42,101 - INFO - joeynmt.training - Epoch  11, Step:    67600, Batch Loss:     2.185335, Tokens per Sec:    14427, Lr: 0.000300\n",
      "2021-07-09 09:23:07,448 - INFO - joeynmt.training - Epoch  11, Step:    67800, Batch Loss:     2.106744, Tokens per Sec:    14534, Lr: 0.000300\n",
      "2021-07-09 09:23:32,863 - INFO - joeynmt.training - Epoch  11, Step:    68000, Batch Loss:     2.260210, Tokens per Sec:    14380, Lr: 0.000300\n",
      "2021-07-09 09:23:58,579 - INFO - joeynmt.training - Epoch  11, Step:    68200, Batch Loss:     2.084583, Tokens per Sec:    14392, Lr: 0.000300\n",
      "2021-07-09 09:24:23,962 - INFO - joeynmt.training - Epoch  11, Step:    68400, Batch Loss:     2.216771, Tokens per Sec:    14577, Lr: 0.000300\n",
      "2021-07-09 09:24:49,402 - INFO - joeynmt.training - Epoch  11, Step:    68600, Batch Loss:     2.141245, Tokens per Sec:    14833, Lr: 0.000300\n",
      "2021-07-09 09:25:14,720 - INFO - joeynmt.training - Epoch  11, Step:    68800, Batch Loss:     2.078765, Tokens per Sec:    14583, Lr: 0.000300\n",
      "2021-07-09 09:25:40,135 - INFO - joeynmt.training - Epoch  11, Step:    69000, Batch Loss:     2.290383, Tokens per Sec:    14470, Lr: 0.000300\n",
      "2021-07-09 09:26:03,524 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:26:03,524 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:26:03,524 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:26:03,748 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:26:03,748 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:26:04,796 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:26:04,798 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:26:04,798 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:26:04,798 - INFO - joeynmt.training - \tHypothesis: Narakoraga .\n",
      "2021-07-09 09:26:04,798 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:26:04,798 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:26:04,799 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:26:04,799 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka turebe icyo ibyo byari bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 09:26:04,799 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:26:04,799 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:26:04,799 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:26:04,800 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:26:04,800 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:26:04,800 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:26:04,800 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:26:04,800 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko nibura mu rugero runaka , bumva ko bahumurizwa mu isi ya Satani .\n",
      "2021-07-09 09:26:04,801 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    69000: bleu:  19.60, loss: 49922.0586, ppl:   5.8509, duration: 24.6653s\n",
      "2021-07-09 09:26:30,446 - INFO - joeynmt.training - Epoch  11, Step:    69200, Batch Loss:     2.266917, Tokens per Sec:    14714, Lr: 0.000300\n",
      "2021-07-09 09:26:55,749 - INFO - joeynmt.training - Epoch  11, Step:    69400, Batch Loss:     2.285810, Tokens per Sec:    14479, Lr: 0.000300\n",
      "2021-07-09 09:27:21,139 - INFO - joeynmt.training - Epoch  11, Step:    69600, Batch Loss:     2.290062, Tokens per Sec:    14776, Lr: 0.000300\n",
      "2021-07-09 09:27:46,312 - INFO - joeynmt.training - Epoch  11, Step:    69800, Batch Loss:     2.190183, Tokens per Sec:    14388, Lr: 0.000300\n",
      "2021-07-09 09:28:11,844 - INFO - joeynmt.training - Epoch  11, Step:    70000, Batch Loss:     2.186873, Tokens per Sec:    14633, Lr: 0.000300\n",
      "2021-07-09 09:28:36,874 - INFO - joeynmt.training - Epoch  11, Step:    70200, Batch Loss:     2.063226, Tokens per Sec:    14579, Lr: 0.000300\n",
      "2021-07-09 09:29:02,191 - INFO - joeynmt.training - Epoch  11, Step:    70400, Batch Loss:     2.206376, Tokens per Sec:    14598, Lr: 0.000300\n",
      "2021-07-09 09:29:27,781 - INFO - joeynmt.training - Epoch  11, Step:    70600, Batch Loss:     2.027444, Tokens per Sec:    14689, Lr: 0.000300\n",
      "2021-07-09 09:29:53,160 - INFO - joeynmt.training - Epoch  11, Step:    70800, Batch Loss:     2.219426, Tokens per Sec:    14660, Lr: 0.000300\n",
      "2021-07-09 09:30:18,696 - INFO - joeynmt.training - Epoch  11, Step:    71000, Batch Loss:     1.945290, Tokens per Sec:    14787, Lr: 0.000300\n",
      "2021-07-09 09:30:43,933 - INFO - joeynmt.training - Epoch  11, Step:    71200, Batch Loss:     2.459375, Tokens per Sec:    14544, Lr: 0.000300\n",
      "2021-07-09 09:31:09,128 - INFO - joeynmt.training - Epoch  11, Step:    71400, Batch Loss:     2.157884, Tokens per Sec:    14502, Lr: 0.000300\n",
      "2021-07-09 09:31:34,565 - INFO - joeynmt.training - Epoch  11, Step:    71600, Batch Loss:     2.170439, Tokens per Sec:    14747, Lr: 0.000300\n",
      "2021-07-09 09:31:59,677 - INFO - joeynmt.training - Epoch  11, Step:    71800, Batch Loss:     2.115216, Tokens per Sec:    14516, Lr: 0.000300\n",
      "2021-07-09 09:32:25,013 - INFO - joeynmt.training - Epoch  11, Step:    72000, Batch Loss:     2.279695, Tokens per Sec:    14504, Lr: 0.000300\n",
      "2021-07-09 09:32:48,445 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:32:48,445 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:32:48,445 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:32:48,678 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:32:48,679 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:32:49,380 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:32:49,381 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:32:49,381 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:32:49,381 - INFO - joeynmt.training - \tHypothesis: Narakomeje cyane .\n",
      "2021-07-09 09:32:49,381 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:32:49,382 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:32:49,382 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:32:49,382 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo cyabaye mu gusoma umuzingo .\n",
      "2021-07-09 09:32:49,382 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:32:49,383 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:32:49,383 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:32:49,383 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa duhura n’ikibazo , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:32:49,383 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:32:49,384 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:32:49,384 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:32:49,384 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ariko , bamwe mu bagize itorero rya gikristo bagaragaza ko nibura mu rugero runaka , bumva bahumurijwe n’isi ya Satani .\n",
      "2021-07-09 09:32:49,384 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    72000: bleu:  20.21, loss: 49734.0977, ppl:   5.8121, duration: 24.3705s\n",
      "2021-07-09 09:32:52,817 - INFO - joeynmt.training - Epoch  11: total training loss 14295.21\n",
      "2021-07-09 09:32:52,818 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-09 09:33:15,592 - INFO - joeynmt.training - Epoch  12, Step:    72200, Batch Loss:     2.169600, Tokens per Sec:    14259, Lr: 0.000300\n",
      "2021-07-09 09:33:40,998 - INFO - joeynmt.training - Epoch  12, Step:    72400, Batch Loss:     2.048607, Tokens per Sec:    14790, Lr: 0.000300\n",
      "2021-07-09 09:34:06,578 - INFO - joeynmt.training - Epoch  12, Step:    72600, Batch Loss:     2.021297, Tokens per Sec:    14735, Lr: 0.000300\n",
      "2021-07-09 09:34:32,086 - INFO - joeynmt.training - Epoch  12, Step:    72800, Batch Loss:     1.997266, Tokens per Sec:    14638, Lr: 0.000300\n",
      "2021-07-09 09:34:57,394 - INFO - joeynmt.training - Epoch  12, Step:    73000, Batch Loss:     2.336276, Tokens per Sec:    14682, Lr: 0.000300\n",
      "2021-07-09 09:35:22,586 - INFO - joeynmt.training - Epoch  12, Step:    73200, Batch Loss:     2.461785, Tokens per Sec:    14406, Lr: 0.000300\n",
      "2021-07-09 09:35:47,716 - INFO - joeynmt.training - Epoch  12, Step:    73400, Batch Loss:     1.976326, Tokens per Sec:    14511, Lr: 0.000300\n",
      "2021-07-09 09:36:13,101 - INFO - joeynmt.training - Epoch  12, Step:    73600, Batch Loss:     2.321559, Tokens per Sec:    14555, Lr: 0.000300\n",
      "2021-07-09 09:36:38,296 - INFO - joeynmt.training - Epoch  12, Step:    73800, Batch Loss:     2.207790, Tokens per Sec:    14357, Lr: 0.000300\n",
      "2021-07-09 09:37:03,824 - INFO - joeynmt.training - Epoch  12, Step:    74000, Batch Loss:     2.021898, Tokens per Sec:    14543, Lr: 0.000300\n",
      "2021-07-09 09:37:29,061 - INFO - joeynmt.training - Epoch  12, Step:    74200, Batch Loss:     2.280801, Tokens per Sec:    14584, Lr: 0.000300\n",
      "2021-07-09 09:37:54,418 - INFO - joeynmt.training - Epoch  12, Step:    74400, Batch Loss:     2.039276, Tokens per Sec:    14485, Lr: 0.000300\n",
      "2021-07-09 09:38:19,731 - INFO - joeynmt.training - Epoch  12, Step:    74600, Batch Loss:     2.139936, Tokens per Sec:    14628, Lr: 0.000300\n",
      "2021-07-09 09:38:45,010 - INFO - joeynmt.training - Epoch  12, Step:    74800, Batch Loss:     1.975214, Tokens per Sec:    14557, Lr: 0.000300\n",
      "2021-07-09 09:39:10,544 - INFO - joeynmt.training - Epoch  12, Step:    75000, Batch Loss:     2.243429, Tokens per Sec:    14725, Lr: 0.000300\n",
      "2021-07-09 09:39:30,351 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:39:30,351 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:39:30,351 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:39:30,581 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:39:30,582 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:39:31,438 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:39:31,439 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:39:31,439 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:39:31,439 - INFO - joeynmt.training - \tHypothesis: Narakoraga ku mutima .\n",
      "2021-07-09 09:39:31,439 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:39:31,440 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:39:31,440 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:39:31,440 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo cyakoreshwaga mu gusoma umuzingo .\n",
      "2021-07-09 09:39:31,440 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:39:31,441 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:39:31,441 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:39:31,441 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho mu gihe duhanganye n’ikibazo cyangwa duhebye , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:39:31,441 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:39:31,442 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:39:31,442 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:39:31,442 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bahamya ko nibura mu rugero runaka , bumva bahumurijwe n’isi ya Satani .\n",
      "2021-07-09 09:39:31,443 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    75000: bleu:  20.46, loss: 49350.6094, ppl:   5.7337, duration: 20.8979s\n",
      "2021-07-09 09:39:57,087 - INFO - joeynmt.training - Epoch  12, Step:    75200, Batch Loss:     2.456525, Tokens per Sec:    14553, Lr: 0.000300\n",
      "2021-07-09 09:40:22,357 - INFO - joeynmt.training - Epoch  12, Step:    75400, Batch Loss:     2.056331, Tokens per Sec:    14506, Lr: 0.000300\n",
      "2021-07-09 09:40:47,771 - INFO - joeynmt.training - Epoch  12, Step:    75600, Batch Loss:     2.076644, Tokens per Sec:    14507, Lr: 0.000300\n",
      "2021-07-09 09:41:13,026 - INFO - joeynmt.training - Epoch  12, Step:    75800, Batch Loss:     1.954167, Tokens per Sec:    14547, Lr: 0.000300\n",
      "2021-07-09 09:41:38,377 - INFO - joeynmt.training - Epoch  12, Step:    76000, Batch Loss:     2.297369, Tokens per Sec:    14347, Lr: 0.000300\n",
      "2021-07-09 09:42:03,603 - INFO - joeynmt.training - Epoch  12, Step:    76200, Batch Loss:     2.145142, Tokens per Sec:    14532, Lr: 0.000300\n",
      "2021-07-09 09:42:28,902 - INFO - joeynmt.training - Epoch  12, Step:    76400, Batch Loss:     2.057451, Tokens per Sec:    14570, Lr: 0.000300\n",
      "2021-07-09 09:42:54,062 - INFO - joeynmt.training - Epoch  12, Step:    76600, Batch Loss:     1.959774, Tokens per Sec:    14439, Lr: 0.000300\n",
      "2021-07-09 09:43:19,467 - INFO - joeynmt.training - Epoch  12, Step:    76800, Batch Loss:     2.213653, Tokens per Sec:    14780, Lr: 0.000300\n",
      "2021-07-09 09:43:45,024 - INFO - joeynmt.training - Epoch  12, Step:    77000, Batch Loss:     2.019885, Tokens per Sec:    14929, Lr: 0.000300\n",
      "2021-07-09 09:44:10,309 - INFO - joeynmt.training - Epoch  12, Step:    77200, Batch Loss:     2.313408, Tokens per Sec:    14522, Lr: 0.000300\n",
      "2021-07-09 09:44:35,483 - INFO - joeynmt.training - Epoch  12, Step:    77400, Batch Loss:     2.141797, Tokens per Sec:    14631, Lr: 0.000300\n",
      "2021-07-09 09:45:00,740 - INFO - joeynmt.training - Epoch  12, Step:    77600, Batch Loss:     2.040860, Tokens per Sec:    14631, Lr: 0.000300\n",
      "2021-07-09 09:45:26,147 - INFO - joeynmt.training - Epoch  12, Step:    77800, Batch Loss:     1.948671, Tokens per Sec:    14866, Lr: 0.000300\n",
      "2021-07-09 09:45:51,432 - INFO - joeynmt.training - Epoch  12, Step:    78000, Batch Loss:     2.282106, Tokens per Sec:    14450, Lr: 0.000300\n",
      "2021-07-09 09:46:13,188 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:46:13,189 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:46:13,189 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:46:13,417 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:46:13,417 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:46:14,458 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:46:14,459 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:46:14,460 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:46:14,460 - INFO - joeynmt.training - \tHypothesis: Narakomeje .\n",
      "2021-07-09 09:46:14,460 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:46:14,460 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:46:14,460 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:46:14,461 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 09:46:14,461 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:46:14,461 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:46:14,461 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:46:14,461 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twihebye cyangwa duhangayitse , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:46:14,462 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:46:14,462 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:46:14,462 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:46:14,462 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bahamya ko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 09:46:14,463 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    78000: bleu:  20.53, loss: 49010.2344, ppl:   5.6651, duration: 23.0299s\n",
      "2021-07-09 09:46:39,686 - INFO - joeynmt.training - Epoch  12, Step:    78200, Batch Loss:     2.064814, Tokens per Sec:    14126, Lr: 0.000300\n",
      "2021-07-09 09:47:04,969 - INFO - joeynmt.training - Epoch  12, Step:    78400, Batch Loss:     1.924152, Tokens per Sec:    14755, Lr: 0.000300\n",
      "2021-07-09 09:47:27,043 - INFO - joeynmt.training - Epoch  12: total training loss 14110.37\n",
      "2021-07-09 09:47:27,043 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-09 09:47:30,850 - INFO - joeynmt.training - Epoch  13, Step:    78600, Batch Loss:     2.046351, Tokens per Sec:    12746, Lr: 0.000300\n",
      "2021-07-09 09:47:56,009 - INFO - joeynmt.training - Epoch  13, Step:    78800, Batch Loss:     2.186030, Tokens per Sec:    14481, Lr: 0.000300\n",
      "2021-07-09 09:48:21,205 - INFO - joeynmt.training - Epoch  13, Step:    79000, Batch Loss:     2.232721, Tokens per Sec:    14812, Lr: 0.000300\n",
      "2021-07-09 09:48:46,537 - INFO - joeynmt.training - Epoch  13, Step:    79200, Batch Loss:     2.209497, Tokens per Sec:    14629, Lr: 0.000300\n",
      "2021-07-09 09:49:11,953 - INFO - joeynmt.training - Epoch  13, Step:    79400, Batch Loss:     2.139766, Tokens per Sec:    14904, Lr: 0.000300\n",
      "2021-07-09 09:49:37,323 - INFO - joeynmt.training - Epoch  13, Step:    79600, Batch Loss:     2.183012, Tokens per Sec:    14620, Lr: 0.000300\n",
      "2021-07-09 09:50:02,716 - INFO - joeynmt.training - Epoch  13, Step:    79800, Batch Loss:     1.974982, Tokens per Sec:    14545, Lr: 0.000300\n",
      "2021-07-09 09:50:28,046 - INFO - joeynmt.training - Epoch  13, Step:    80000, Batch Loss:     2.221556, Tokens per Sec:    14558, Lr: 0.000300\n",
      "2021-07-09 09:50:53,109 - INFO - joeynmt.training - Epoch  13, Step:    80200, Batch Loss:     1.810031, Tokens per Sec:    14279, Lr: 0.000300\n",
      "2021-07-09 09:51:18,465 - INFO - joeynmt.training - Epoch  13, Step:    80400, Batch Loss:     2.044144, Tokens per Sec:    14810, Lr: 0.000300\n",
      "2021-07-09 09:51:43,885 - INFO - joeynmt.training - Epoch  13, Step:    80600, Batch Loss:     1.790353, Tokens per Sec:    14557, Lr: 0.000300\n",
      "2021-07-09 09:52:09,631 - INFO - joeynmt.training - Epoch  13, Step:    80800, Batch Loss:     2.050366, Tokens per Sec:    14694, Lr: 0.000300\n",
      "2021-07-09 09:52:35,593 - INFO - joeynmt.training - Epoch  13, Step:    81000, Batch Loss:     1.995265, Tokens per Sec:    14305, Lr: 0.000300\n",
      "2021-07-09 09:52:55,544 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:52:55,544 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:52:55,545 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:52:55,791 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:52:55,791 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:52:56,539 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:52:56,539 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:52:56,540 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:52:56,540 - INFO - joeynmt.training - \tHypothesis: Narakoze ku mutima .\n",
      "2021-07-09 09:52:56,540 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:52:56,540 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:52:56,540 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:52:56,541 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo cyakoreshejwe mu gusoma umuzingo .\n",
      "2021-07-09 09:52:56,541 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:52:56,541 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:52:56,541 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:52:56,542 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:52:56,542 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:52:56,542 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:52:56,542 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:52:56,542 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bagaragaza ko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 09:52:56,543 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    81000: bleu:  20.65, loss: 48625.6484, ppl:   5.5885, duration: 20.9489s\n",
      "2021-07-09 09:53:22,095 - INFO - joeynmt.training - Epoch  13, Step:    81200, Batch Loss:     2.266392, Tokens per Sec:    14275, Lr: 0.000300\n",
      "2021-07-09 09:53:47,568 - INFO - joeynmt.training - Epoch  13, Step:    81400, Batch Loss:     2.102039, Tokens per Sec:    14390, Lr: 0.000300\n",
      "2021-07-09 09:54:13,056 - INFO - joeynmt.training - Epoch  13, Step:    81600, Batch Loss:     1.957575, Tokens per Sec:    14482, Lr: 0.000300\n",
      "2021-07-09 09:54:37,946 - INFO - joeynmt.training - Epoch  13, Step:    81800, Batch Loss:     2.169411, Tokens per Sec:    14228, Lr: 0.000300\n",
      "2021-07-09 09:55:03,491 - INFO - joeynmt.training - Epoch  13, Step:    82000, Batch Loss:     2.124160, Tokens per Sec:    14525, Lr: 0.000300\n",
      "2021-07-09 09:55:28,869 - INFO - joeynmt.training - Epoch  13, Step:    82200, Batch Loss:     2.001750, Tokens per Sec:    14652, Lr: 0.000300\n",
      "2021-07-09 09:55:54,246 - INFO - joeynmt.training - Epoch  13, Step:    82400, Batch Loss:     2.015304, Tokens per Sec:    14643, Lr: 0.000300\n",
      "2021-07-09 09:56:19,772 - INFO - joeynmt.training - Epoch  13, Step:    82600, Batch Loss:     2.181021, Tokens per Sec:    14683, Lr: 0.000300\n",
      "2021-07-09 09:56:45,213 - INFO - joeynmt.training - Epoch  13, Step:    82800, Batch Loss:     1.954103, Tokens per Sec:    14381, Lr: 0.000300\n",
      "2021-07-09 09:57:10,640 - INFO - joeynmt.training - Epoch  13, Step:    83000, Batch Loss:     1.957413, Tokens per Sec:    14570, Lr: 0.000300\n",
      "2021-07-09 09:57:35,972 - INFO - joeynmt.training - Epoch  13, Step:    83200, Batch Loss:     2.375913, Tokens per Sec:    14754, Lr: 0.000300\n",
      "2021-07-09 09:58:01,399 - INFO - joeynmt.training - Epoch  13, Step:    83400, Batch Loss:     1.987361, Tokens per Sec:    14640, Lr: 0.000300\n",
      "2021-07-09 09:58:26,744 - INFO - joeynmt.training - Epoch  13, Step:    83600, Batch Loss:     2.313658, Tokens per Sec:    14549, Lr: 0.000300\n",
      "2021-07-09 09:58:51,977 - INFO - joeynmt.training - Epoch  13, Step:    83800, Batch Loss:     1.995951, Tokens per Sec:    14552, Lr: 0.000300\n",
      "2021-07-09 09:59:17,353 - INFO - joeynmt.training - Epoch  13, Step:    84000, Batch Loss:     2.154936, Tokens per Sec:    14303, Lr: 0.000300\n",
      "2021-07-09 09:59:37,737 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 09:59:37,737 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 09:59:37,738 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 09:59:37,962 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 09:59:37,962 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 09:59:38,705 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 09:59:38,705 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 09:59:38,705 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 09:59:38,706 - INFO - joeynmt.training - \tHypothesis: Narushijeho gukora .\n",
      "2021-07-09 09:59:38,706 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 09:59:38,706 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 09:59:38,707 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 09:59:38,707 - INFO - joeynmt.training - \tHypothesis: Icyakora , reka dusuzume icyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 09:59:38,707 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 09:59:38,707 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 09:59:38,707 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:59:38,708 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twihebye cyangwa duhangayitse , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 09:59:38,708 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 09:59:38,708 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 09:59:38,708 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 09:59:38,709 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 09:59:38,709 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    84000: bleu:  20.71, loss: 48549.7930, ppl:   5.5735, duration: 21.3553s\n",
      "2021-07-09 10:00:04,292 - INFO - joeynmt.training - Epoch  13, Step:    84200, Batch Loss:     2.339294, Tokens per Sec:    14369, Lr: 0.000300\n",
      "2021-07-09 10:00:29,957 - INFO - joeynmt.training - Epoch  13, Step:    84400, Batch Loss:     2.307824, Tokens per Sec:    14721, Lr: 0.000300\n",
      "2021-07-09 10:00:55,581 - INFO - joeynmt.training - Epoch  13, Step:    84600, Batch Loss:     2.052108, Tokens per Sec:    14452, Lr: 0.000300\n",
      "2021-07-09 10:01:21,020 - INFO - joeynmt.training - Epoch  13, Step:    84800, Batch Loss:     2.386646, Tokens per Sec:    14397, Lr: 0.000300\n",
      "2021-07-09 10:01:46,281 - INFO - joeynmt.training - Epoch  13, Step:    85000, Batch Loss:     2.158581, Tokens per Sec:    14497, Lr: 0.000300\n",
      "2021-07-09 10:02:01,655 - INFO - joeynmt.training - Epoch  13: total training loss 13973.35\n",
      "2021-07-09 10:02:01,655 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-09 10:02:12,437 - INFO - joeynmt.training - Epoch  14, Step:    85200, Batch Loss:     2.050222, Tokens per Sec:    13714, Lr: 0.000300\n",
      "2021-07-09 10:02:38,208 - INFO - joeynmt.training - Epoch  14, Step:    85400, Batch Loss:     2.203573, Tokens per Sec:    14821, Lr: 0.000300\n",
      "2021-07-09 10:03:03,407 - INFO - joeynmt.training - Epoch  14, Step:    85600, Batch Loss:     2.030457, Tokens per Sec:    14371, Lr: 0.000300\n",
      "2021-07-09 10:03:28,950 - INFO - joeynmt.training - Epoch  14, Step:    85800, Batch Loss:     2.126596, Tokens per Sec:    14621, Lr: 0.000300\n",
      "2021-07-09 10:03:54,507 - INFO - joeynmt.training - Epoch  14, Step:    86000, Batch Loss:     1.994361, Tokens per Sec:    14879, Lr: 0.000300\n",
      "2021-07-09 10:04:19,775 - INFO - joeynmt.training - Epoch  14, Step:    86200, Batch Loss:     2.121475, Tokens per Sec:    14373, Lr: 0.000300\n",
      "2021-07-09 10:04:45,510 - INFO - joeynmt.training - Epoch  14, Step:    86400, Batch Loss:     2.230054, Tokens per Sec:    14890, Lr: 0.000300\n",
      "2021-07-09 10:05:10,851 - INFO - joeynmt.training - Epoch  14, Step:    86600, Batch Loss:     2.098209, Tokens per Sec:    14370, Lr: 0.000300\n",
      "2021-07-09 10:05:36,133 - INFO - joeynmt.training - Epoch  14, Step:    86800, Batch Loss:     1.977168, Tokens per Sec:    14484, Lr: 0.000300\n",
      "2021-07-09 10:06:01,320 - INFO - joeynmt.training - Epoch  14, Step:    87000, Batch Loss:     2.167733, Tokens per Sec:    14307, Lr: 0.000300\n",
      "2021-07-09 10:06:21,476 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:06:21,476 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:06:21,476 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:06:21,714 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 10:06:21,715 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 10:06:22,750 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:06:22,762 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:06:22,762 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:06:22,762 - INFO - joeynmt.training - \tHypothesis: Narakoraga .\n",
      "2021-07-09 10:06:22,762 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:06:22,763 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:06:22,763 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:06:22,763 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byari bikubiyemo mu gusoma umuzingo .\n",
      "2021-07-09 10:06:22,763 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:06:22,764 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:06:22,764 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:06:22,764 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:06:22,764 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:06:22,765 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:06:22,765 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:06:22,765 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bagaragaza ko nibura mu rugero runaka , bumva ko ari byiza mu isi ya Satani .\n",
      "2021-07-09 10:06:22,765 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    87000: bleu:  21.14, loss: 48366.3125, ppl:   5.5375, duration: 21.4442s\n",
      "2021-07-09 10:06:48,297 - INFO - joeynmt.training - Epoch  14, Step:    87200, Batch Loss:     2.208174, Tokens per Sec:    14443, Lr: 0.000300\n",
      "2021-07-09 10:07:13,898 - INFO - joeynmt.training - Epoch  14, Step:    87400, Batch Loss:     2.160736, Tokens per Sec:    14721, Lr: 0.000300\n",
      "2021-07-09 10:07:39,291 - INFO - joeynmt.training - Epoch  14, Step:    87600, Batch Loss:     1.964913, Tokens per Sec:    14659, Lr: 0.000300\n",
      "2021-07-09 10:08:04,597 - INFO - joeynmt.training - Epoch  14, Step:    87800, Batch Loss:     2.418147, Tokens per Sec:    14628, Lr: 0.000300\n",
      "2021-07-09 10:08:29,924 - INFO - joeynmt.training - Epoch  14, Step:    88000, Batch Loss:     2.025343, Tokens per Sec:    14452, Lr: 0.000300\n",
      "2021-07-09 10:08:55,294 - INFO - joeynmt.training - Epoch  14, Step:    88200, Batch Loss:     2.217903, Tokens per Sec:    14471, Lr: 0.000300\n",
      "2021-07-09 10:09:20,658 - INFO - joeynmt.training - Epoch  14, Step:    88400, Batch Loss:     2.045422, Tokens per Sec:    14357, Lr: 0.000300\n",
      "2021-07-09 10:09:46,107 - INFO - joeynmt.training - Epoch  14, Step:    88600, Batch Loss:     2.000870, Tokens per Sec:    14780, Lr: 0.000300\n",
      "2021-07-09 10:10:11,244 - INFO - joeynmt.training - Epoch  14, Step:    88800, Batch Loss:     2.001502, Tokens per Sec:    14186, Lr: 0.000300\n",
      "2021-07-09 10:10:36,643 - INFO - joeynmt.training - Epoch  14, Step:    89000, Batch Loss:     2.041796, Tokens per Sec:    14510, Lr: 0.000300\n",
      "2021-07-09 10:11:02,124 - INFO - joeynmt.training - Epoch  14, Step:    89200, Batch Loss:     2.179416, Tokens per Sec:    14727, Lr: 0.000300\n",
      "2021-07-09 10:11:27,388 - INFO - joeynmt.training - Epoch  14, Step:    89400, Batch Loss:     2.289979, Tokens per Sec:    14402, Lr: 0.000300\n",
      "2021-07-09 10:11:52,844 - INFO - joeynmt.training - Epoch  14, Step:    89600, Batch Loss:     2.211132, Tokens per Sec:    14576, Lr: 0.000300\n",
      "2021-07-09 10:12:18,133 - INFO - joeynmt.training - Epoch  14, Step:    89800, Batch Loss:     2.022182, Tokens per Sec:    14593, Lr: 0.000300\n",
      "2021-07-09 10:12:43,629 - INFO - joeynmt.training - Epoch  14, Step:    90000, Batch Loss:     2.058844, Tokens per Sec:    14619, Lr: 0.000300\n",
      "2021-07-09 10:13:05,087 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:13:05,087 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:13:05,087 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:13:05,311 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 10:13:05,311 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 10:13:06,058 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:13:06,059 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:13:06,059 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:13:06,059 - INFO - joeynmt.training - \tHypothesis: Narakomeje .\n",
      "2021-07-09 10:13:06,059 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:13:06,060 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:13:06,060 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:13:06,060 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byari bikubiyemo mu gusoma umuzingo .\n",
      "2021-07-09 10:13:06,060 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:13:06,061 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:13:06,061 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:13:06,061 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo twihebye cyangwa duhangayitse , twagombye gukomeza kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:13:06,061 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:13:06,062 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:13:06,062 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:13:06,062 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko nibura mu rugero runaka , bumva bahumurizwa mu isi ya Satani .\n",
      "2021-07-09 10:13:06,062 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    90000: bleu:  21.38, loss: 48077.7656, ppl:   5.4812, duration: 22.4328s\n",
      "2021-07-09 10:13:31,519 - INFO - joeynmt.training - Epoch  14, Step:    90200, Batch Loss:     1.933383, Tokens per Sec:    14537, Lr: 0.000300\n",
      "2021-07-09 10:13:57,013 - INFO - joeynmt.training - Epoch  14, Step:    90400, Batch Loss:     2.061198, Tokens per Sec:    14859, Lr: 0.000300\n",
      "2021-07-09 10:14:22,452 - INFO - joeynmt.training - Epoch  14, Step:    90600, Batch Loss:     2.079807, Tokens per Sec:    14557, Lr: 0.000300\n",
      "2021-07-09 10:14:47,679 - INFO - joeynmt.training - Epoch  14, Step:    90800, Batch Loss:     2.222575, Tokens per Sec:    14397, Lr: 0.000300\n",
      "2021-07-09 10:15:13,215 - INFO - joeynmt.training - Epoch  14, Step:    91000, Batch Loss:     2.207075, Tokens per Sec:    14823, Lr: 0.000300\n",
      "2021-07-09 10:15:38,519 - INFO - joeynmt.training - Epoch  14, Step:    91200, Batch Loss:     2.219748, Tokens per Sec:    14509, Lr: 0.000300\n",
      "2021-07-09 10:16:03,790 - INFO - joeynmt.training - Epoch  14, Step:    91400, Batch Loss:     2.034149, Tokens per Sec:    14707, Lr: 0.000300\n",
      "2021-07-09 10:16:29,044 - INFO - joeynmt.training - Epoch  14, Step:    91600, Batch Loss:     1.725457, Tokens per Sec:    14579, Lr: 0.000300\n",
      "2021-07-09 10:16:36,441 - INFO - joeynmt.training - Epoch  14: total training loss 13831.24\n",
      "2021-07-09 10:16:36,441 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-09 10:16:55,069 - INFO - joeynmt.training - Epoch  15, Step:    91800, Batch Loss:     2.068752, Tokens per Sec:    13777, Lr: 0.000300\n",
      "2021-07-09 10:17:20,149 - INFO - joeynmt.training - Epoch  15, Step:    92000, Batch Loss:     2.497156, Tokens per Sec:    14247, Lr: 0.000300\n",
      "2021-07-09 10:17:45,368 - INFO - joeynmt.training - Epoch  15, Step:    92200, Batch Loss:     1.848297, Tokens per Sec:    14755, Lr: 0.000300\n",
      "2021-07-09 10:18:10,748 - INFO - joeynmt.training - Epoch  15, Step:    92400, Batch Loss:     2.020755, Tokens per Sec:    14637, Lr: 0.000300\n",
      "2021-07-09 10:18:36,141 - INFO - joeynmt.training - Epoch  15, Step:    92600, Batch Loss:     2.118567, Tokens per Sec:    14756, Lr: 0.000300\n",
      "2021-07-09 10:19:01,398 - INFO - joeynmt.training - Epoch  15, Step:    92800, Batch Loss:     2.269874, Tokens per Sec:    14495, Lr: 0.000300\n",
      "2021-07-09 10:19:26,715 - INFO - joeynmt.training - Epoch  15, Step:    93000, Batch Loss:     2.090452, Tokens per Sec:    14576, Lr: 0.000300\n",
      "2021-07-09 10:19:45,997 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:19:45,997 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:19:45,998 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:19:46,232 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 10:19:46,232 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 10:19:46,928 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:19:46,929 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:19:46,929 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:19:46,929 - INFO - joeynmt.training - \tHypothesis: Umutima wanjye wari warakoze .\n",
      "2021-07-09 10:19:46,929 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:19:46,929 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:19:46,930 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:19:46,930 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byari bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 10:19:46,930 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:19:46,931 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:19:46,931 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:19:46,931 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho , twagombye kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:19:46,931 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:19:46,932 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:19:46,932 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:19:46,932 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko nibura mu rugero runaka , bumva ko ari byiza mu isi ya Satani .\n",
      "2021-07-09 10:19:46,932 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    93000: bleu:  21.37, loss: 47829.8125, ppl:   5.4333, duration: 20.2173s\n",
      "2021-07-09 10:20:12,521 - INFO - joeynmt.training - Epoch  15, Step:    93200, Batch Loss:     2.229497, Tokens per Sec:    14667, Lr: 0.000300\n",
      "2021-07-09 10:20:37,862 - INFO - joeynmt.training - Epoch  15, Step:    93400, Batch Loss:     2.205066, Tokens per Sec:    14538, Lr: 0.000300\n",
      "2021-07-09 10:21:03,104 - INFO - joeynmt.training - Epoch  15, Step:    93600, Batch Loss:     2.115194, Tokens per Sec:    14722, Lr: 0.000300\n",
      "2021-07-09 10:21:28,488 - INFO - joeynmt.training - Epoch  15, Step:    93800, Batch Loss:     2.238764, Tokens per Sec:    14826, Lr: 0.000300\n",
      "2021-07-09 10:21:53,940 - INFO - joeynmt.training - Epoch  15, Step:    94000, Batch Loss:     2.123738, Tokens per Sec:    14823, Lr: 0.000300\n",
      "2021-07-09 10:22:19,245 - INFO - joeynmt.training - Epoch  15, Step:    94200, Batch Loss:     2.047404, Tokens per Sec:    14758, Lr: 0.000300\n",
      "2021-07-09 10:22:44,693 - INFO - joeynmt.training - Epoch  15, Step:    94400, Batch Loss:     1.986378, Tokens per Sec:    14753, Lr: 0.000300\n",
      "2021-07-09 10:23:10,044 - INFO - joeynmt.training - Epoch  15, Step:    94600, Batch Loss:     1.926723, Tokens per Sec:    14381, Lr: 0.000300\n",
      "2021-07-09 10:23:35,180 - INFO - joeynmt.training - Epoch  15, Step:    94800, Batch Loss:     1.784206, Tokens per Sec:    14459, Lr: 0.000300\n",
      "2021-07-09 10:24:00,583 - INFO - joeynmt.training - Epoch  15, Step:    95000, Batch Loss:     2.113409, Tokens per Sec:    14615, Lr: 0.000300\n",
      "2021-07-09 10:24:25,842 - INFO - joeynmt.training - Epoch  15, Step:    95200, Batch Loss:     2.016710, Tokens per Sec:    14673, Lr: 0.000300\n",
      "2021-07-09 10:24:51,132 - INFO - joeynmt.training - Epoch  15, Step:    95400, Batch Loss:     2.092618, Tokens per Sec:    14419, Lr: 0.000300\n",
      "2021-07-09 10:25:16,309 - INFO - joeynmt.training - Epoch  15, Step:    95600, Batch Loss:     2.092929, Tokens per Sec:    14646, Lr: 0.000300\n",
      "2021-07-09 10:25:41,746 - INFO - joeynmt.training - Epoch  15, Step:    95800, Batch Loss:     2.264613, Tokens per Sec:    14628, Lr: 0.000300\n",
      "2021-07-09 10:26:07,225 - INFO - joeynmt.training - Epoch  15, Step:    96000, Batch Loss:     2.135293, Tokens per Sec:    14663, Lr: 0.000300\n",
      "2021-07-09 10:26:27,061 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:26:27,061 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:26:27,062 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:26:27,290 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 10:26:27,290 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 10:26:27,989 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:26:27,989 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:26:27,990 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:26:27,990 - INFO - joeynmt.training - \tHypothesis: Narushijeho gukora .\n",
      "2021-07-09 10:26:27,990 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:26:27,990 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:26:27,991 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:26:27,991 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo ibyo byari bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 10:26:27,991 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:26:27,991 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:26:27,992 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:26:27,992 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho mu buryo bw’umubiri cyangwa ihebye , twagombye gukomeza kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:26:27,993 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:26:27,993 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:26:27,993 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:26:27,994 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bagaragaza ko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 10:26:27,994 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    96000: bleu:  21.64, loss: 47648.8477, ppl:   5.3986, duration: 20.7680s\n",
      "2021-07-09 10:26:53,559 - INFO - joeynmt.training - Epoch  15, Step:    96200, Batch Loss:     2.055967, Tokens per Sec:    14652, Lr: 0.000300\n",
      "2021-07-09 10:27:18,830 - INFO - joeynmt.training - Epoch  15, Step:    96400, Batch Loss:     2.073421, Tokens per Sec:    14502, Lr: 0.000300\n",
      "2021-07-09 10:27:44,267 - INFO - joeynmt.training - Epoch  15, Step:    96600, Batch Loss:     1.949085, Tokens per Sec:    14730, Lr: 0.000300\n",
      "2021-07-09 10:28:09,501 - INFO - joeynmt.training - Epoch  15, Step:    96800, Batch Loss:     1.905080, Tokens per Sec:    14578, Lr: 0.000300\n",
      "2021-07-09 10:28:34,808 - INFO - joeynmt.training - Epoch  15, Step:    97000, Batch Loss:     2.099379, Tokens per Sec:    14581, Lr: 0.000300\n",
      "2021-07-09 10:29:00,256 - INFO - joeynmt.training - Epoch  15, Step:    97200, Batch Loss:     1.897950, Tokens per Sec:    14778, Lr: 0.000300\n",
      "2021-07-09 10:29:25,510 - INFO - joeynmt.training - Epoch  15, Step:    97400, Batch Loss:     2.001729, Tokens per Sec:    14698, Lr: 0.000300\n",
      "2021-07-09 10:29:50,682 - INFO - joeynmt.training - Epoch  15, Step:    97600, Batch Loss:     2.043238, Tokens per Sec:    14268, Lr: 0.000300\n",
      "2021-07-09 10:30:16,142 - INFO - joeynmt.training - Epoch  15, Step:    97800, Batch Loss:     2.001007, Tokens per Sec:    14630, Lr: 0.000300\n",
      "2021-07-09 10:30:41,207 - INFO - joeynmt.training - Epoch  15, Step:    98000, Batch Loss:     2.170448, Tokens per Sec:    14423, Lr: 0.000300\n",
      "2021-07-09 10:31:06,477 - INFO - joeynmt.training - Epoch  15, Step:    98200, Batch Loss:     2.252099, Tokens per Sec:    14221, Lr: 0.000300\n",
      "2021-07-09 10:31:07,270 - INFO - joeynmt.training - Epoch  15: total training loss 13737.94\n",
      "2021-07-09 10:31:07,271 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-09 10:31:32,471 - INFO - joeynmt.training - Epoch  16, Step:    98400, Batch Loss:     2.334378, Tokens per Sec:    14384, Lr: 0.000300\n",
      "2021-07-09 10:31:57,661 - INFO - joeynmt.training - Epoch  16, Step:    98600, Batch Loss:     2.072889, Tokens per Sec:    14608, Lr: 0.000300\n",
      "2021-07-09 10:32:23,127 - INFO - joeynmt.training - Epoch  16, Step:    98800, Batch Loss:     2.032166, Tokens per Sec:    14436, Lr: 0.000300\n",
      "2021-07-09 10:32:48,635 - INFO - joeynmt.training - Epoch  16, Step:    99000, Batch Loss:     2.257809, Tokens per Sec:    14501, Lr: 0.000300\n",
      "2021-07-09 10:33:07,921 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:33:07,922 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:33:07,922 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:33:08,153 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 10:33:08,153 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 10:33:08,855 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:33:08,856 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:33:08,856 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:33:08,856 - INFO - joeynmt.training - \tHypothesis: Narakoze ku mutima .\n",
      "2021-07-09 10:33:08,856 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:33:08,856 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:33:08,857 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:33:08,857 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo byari bikubiyemo mu gusoma umuzingo .\n",
      "2021-07-09 10:33:08,857 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:33:08,857 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:33:08,858 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:33:08,858 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho , twagombye kwizeza Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:33:08,858 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:33:08,858 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:33:08,858 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:33:08,859 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo batanga igihamya cy’uko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 10:33:08,859 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    99000: bleu:  21.83, loss: 47533.6016, ppl:   5.3767, duration: 20.2234s\n",
      "2021-07-09 10:33:34,328 - INFO - joeynmt.training - Epoch  16, Step:    99200, Batch Loss:     2.112487, Tokens per Sec:    14502, Lr: 0.000300\n",
      "2021-07-09 10:33:59,537 - INFO - joeynmt.training - Epoch  16, Step:    99400, Batch Loss:     1.992151, Tokens per Sec:    14636, Lr: 0.000300\n",
      "2021-07-09 10:34:25,063 - INFO - joeynmt.training - Epoch  16, Step:    99600, Batch Loss:     2.079430, Tokens per Sec:    14704, Lr: 0.000300\n",
      "2021-07-09 10:34:50,339 - INFO - joeynmt.training - Epoch  16, Step:    99800, Batch Loss:     2.021106, Tokens per Sec:    14576, Lr: 0.000300\n",
      "2021-07-09 10:35:15,697 - INFO - joeynmt.training - Epoch  16, Step:   100000, Batch Loss:     2.024557, Tokens per Sec:    14609, Lr: 0.000300\n",
      "2021-07-09 10:35:40,878 - INFO - joeynmt.training - Epoch  16, Step:   100200, Batch Loss:     1.933508, Tokens per Sec:    14402, Lr: 0.000300\n",
      "2021-07-09 10:36:06,220 - INFO - joeynmt.training - Epoch  16, Step:   100400, Batch Loss:     2.255172, Tokens per Sec:    14453, Lr: 0.000300\n",
      "2021-07-09 10:36:31,590 - INFO - joeynmt.training - Epoch  16, Step:   100600, Batch Loss:     1.954015, Tokens per Sec:    14498, Lr: 0.000300\n",
      "2021-07-09 10:36:56,843 - INFO - joeynmt.training - Epoch  16, Step:   100800, Batch Loss:     1.870078, Tokens per Sec:    14415, Lr: 0.000300\n",
      "2021-07-09 10:37:22,269 - INFO - joeynmt.training - Epoch  16, Step:   101000, Batch Loss:     2.038967, Tokens per Sec:    14813, Lr: 0.000300\n",
      "2021-07-09 10:37:47,546 - INFO - joeynmt.training - Epoch  16, Step:   101200, Batch Loss:     2.182709, Tokens per Sec:    14676, Lr: 0.000300\n",
      "2021-07-09 10:38:12,680 - INFO - joeynmt.training - Epoch  16, Step:   101400, Batch Loss:     2.091326, Tokens per Sec:    14418, Lr: 0.000300\n",
      "2021-07-09 10:38:38,122 - INFO - joeynmt.training - Epoch  16, Step:   101600, Batch Loss:     2.002154, Tokens per Sec:    14649, Lr: 0.000300\n",
      "2021-07-09 10:39:03,743 - INFO - joeynmt.training - Epoch  16, Step:   101800, Batch Loss:     2.027874, Tokens per Sec:    14923, Lr: 0.000300\n",
      "2021-07-09 10:39:28,910 - INFO - joeynmt.training - Epoch  16, Step:   102000, Batch Loss:     1.934542, Tokens per Sec:    14495, Lr: 0.000300\n",
      "2021-07-09 10:39:50,139 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:39:50,140 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:39:50,140 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:39:50,371 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 10:39:50,371 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 10:39:51,394 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:39:51,395 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:39:51,395 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:39:51,395 - INFO - joeynmt.training - \tHypothesis: Nakoraga ku mutima .\n",
      "2021-07-09 10:39:51,396 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:39:51,396 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:39:51,396 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:39:51,396 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka turebe icyo ibyo byari bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 10:39:51,396 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:39:51,397 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:39:51,397 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:39:51,397 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho , twagombye kwishingikiriza ku Mana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:39:51,397 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:39:51,398 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:39:51,398 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:39:51,398 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo batanga igihamya cy’uko nibura mu rugero runaka , bumva ko ari byiza mu isi ya Satani .\n",
      "2021-07-09 10:39:51,398 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   102000: bleu:  22.13, loss: 47282.3320, ppl:   5.3291, duration: 22.4882s\n",
      "2021-07-09 10:40:17,000 - INFO - joeynmt.training - Epoch  16, Step:   102200, Batch Loss:     2.142359, Tokens per Sec:    14536, Lr: 0.000300\n",
      "2021-07-09 10:40:42,462 - INFO - joeynmt.training - Epoch  16, Step:   102400, Batch Loss:     2.180547, Tokens per Sec:    14683, Lr: 0.000300\n",
      "2021-07-09 10:41:07,741 - INFO - joeynmt.training - Epoch  16, Step:   102600, Batch Loss:     2.170520, Tokens per Sec:    14617, Lr: 0.000300\n",
      "2021-07-09 10:41:32,949 - INFO - joeynmt.training - Epoch  16, Step:   102800, Batch Loss:     1.989777, Tokens per Sec:    14694, Lr: 0.000300\n",
      "2021-07-09 10:41:58,307 - INFO - joeynmt.training - Epoch  16, Step:   103000, Batch Loss:     2.003812, Tokens per Sec:    14503, Lr: 0.000300\n",
      "2021-07-09 10:42:23,643 - INFO - joeynmt.training - Epoch  16, Step:   103200, Batch Loss:     2.178823, Tokens per Sec:    14211, Lr: 0.000300\n",
      "2021-07-09 10:42:49,061 - INFO - joeynmt.training - Epoch  16, Step:   103400, Batch Loss:     1.869119, Tokens per Sec:    14499, Lr: 0.000300\n",
      "2021-07-09 10:43:14,480 - INFO - joeynmt.training - Epoch  16, Step:   103600, Batch Loss:     1.913349, Tokens per Sec:    14509, Lr: 0.000300\n",
      "2021-07-09 10:43:40,207 - INFO - joeynmt.training - Epoch  16, Step:   103800, Batch Loss:     2.114999, Tokens per Sec:    14797, Lr: 0.000300\n",
      "2021-07-09 10:44:05,674 - INFO - joeynmt.training - Epoch  16, Step:   104000, Batch Loss:     1.962376, Tokens per Sec:    14929, Lr: 0.000300\n",
      "2021-07-09 10:44:30,923 - INFO - joeynmt.training - Epoch  16, Step:   104200, Batch Loss:     2.112510, Tokens per Sec:    14329, Lr: 0.000300\n",
      "2021-07-09 10:44:56,241 - INFO - joeynmt.training - Epoch  16, Step:   104400, Batch Loss:     1.991247, Tokens per Sec:    14685, Lr: 0.000300\n",
      "2021-07-09 10:45:21,699 - INFO - joeynmt.training - Epoch  16, Step:   104600, Batch Loss:     1.951297, Tokens per Sec:    14765, Lr: 0.000300\n",
      "2021-07-09 10:45:39,854 - INFO - joeynmt.training - Epoch  16: total training loss 13617.05\n",
      "2021-07-09 10:45:39,854 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-09 10:45:47,603 - INFO - joeynmt.training - Epoch  17, Step:   104800, Batch Loss:     2.390805, Tokens per Sec:    13801, Lr: 0.000300\n",
      "2021-07-09 10:46:12,669 - INFO - joeynmt.training - Epoch  17, Step:   105000, Batch Loss:     2.163923, Tokens per Sec:    14548, Lr: 0.000300\n",
      "2021-07-09 10:46:32,739 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:46:32,739 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:46:32,739 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:46:32,961 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 10:46:32,961 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 10:46:33,706 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:46:33,707 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:46:33,707 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:46:33,707 - INFO - joeynmt.training - \tHypothesis: Nakoraga ku mutima .\n",
      "2021-07-09 10:46:33,707 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:46:33,708 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:46:33,708 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:46:33,708 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo gusoma umuzingo byari bikubiyemo .\n",
      "2021-07-09 10:46:33,708 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:46:33,709 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:46:33,709 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:46:33,709 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa twihebye , twagombye kwizerana Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:46:33,709 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:46:33,710 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:46:33,711 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:46:33,711 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bahamya ko nibura mu rugero runaka , bumva ko bahumurijwe mu isi ya Satani .\n",
      "2021-07-09 10:46:33,711 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   105000: bleu:  21.48, loss: 47096.2383, ppl:   5.2941, duration: 21.0415s\n",
      "2021-07-09 10:46:59,506 - INFO - joeynmt.training - Epoch  17, Step:   105200, Batch Loss:     2.369016, Tokens per Sec:    14670, Lr: 0.000300\n",
      "2021-07-09 10:47:24,806 - INFO - joeynmt.training - Epoch  17, Step:   105400, Batch Loss:     2.078423, Tokens per Sec:    14657, Lr: 0.000300\n",
      "2021-07-09 10:47:50,423 - INFO - joeynmt.training - Epoch  17, Step:   105600, Batch Loss:     2.099073, Tokens per Sec:    14764, Lr: 0.000300\n",
      "2021-07-09 10:48:15,840 - INFO - joeynmt.training - Epoch  17, Step:   105800, Batch Loss:     1.982772, Tokens per Sec:    14651, Lr: 0.000300\n",
      "2021-07-09 10:48:41,166 - INFO - joeynmt.training - Epoch  17, Step:   106000, Batch Loss:     2.142259, Tokens per Sec:    14429, Lr: 0.000300\n",
      "2021-07-09 10:49:06,510 - INFO - joeynmt.training - Epoch  17, Step:   106200, Batch Loss:     2.119847, Tokens per Sec:    14604, Lr: 0.000300\n",
      "2021-07-09 10:49:31,709 - INFO - joeynmt.training - Epoch  17, Step:   106400, Batch Loss:     1.798205, Tokens per Sec:    14473, Lr: 0.000300\n",
      "2021-07-09 10:49:57,256 - INFO - joeynmt.training - Epoch  17, Step:   106600, Batch Loss:     1.981556, Tokens per Sec:    14626, Lr: 0.000300\n",
      "2021-07-09 10:50:22,633 - INFO - joeynmt.training - Epoch  17, Step:   106800, Batch Loss:     2.118226, Tokens per Sec:    14485, Lr: 0.000300\n",
      "2021-07-09 10:50:47,678 - INFO - joeynmt.training - Epoch  17, Step:   107000, Batch Loss:     2.170062, Tokens per Sec:    14393, Lr: 0.000300\n",
      "2021-07-09 10:51:12,865 - INFO - joeynmt.training - Epoch  17, Step:   107200, Batch Loss:     2.025692, Tokens per Sec:    14772, Lr: 0.000300\n",
      "2021-07-09 10:51:38,219 - INFO - joeynmt.training - Epoch  17, Step:   107400, Batch Loss:     2.187206, Tokens per Sec:    14526, Lr: 0.000300\n",
      "2021-07-09 10:52:03,119 - INFO - joeynmt.training - Epoch  17, Step:   107600, Batch Loss:     2.105566, Tokens per Sec:    14196, Lr: 0.000300\n",
      "2021-07-09 10:52:28,260 - INFO - joeynmt.training - Epoch  17, Step:   107800, Batch Loss:     2.107494, Tokens per Sec:    14380, Lr: 0.000300\n",
      "2021-07-09 10:52:53,620 - INFO - joeynmt.training - Epoch  17, Step:   108000, Batch Loss:     2.016347, Tokens per Sec:    14793, Lr: 0.000300\n",
      "2021-07-09 10:53:14,019 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:53:14,020 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:53:14,020 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:53:14,246 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 10:53:14,246 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 10:53:14,945 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:53:14,945 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:53:14,946 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:53:14,946 - INFO - joeynmt.training - \tHypothesis: Nakoraga ku mutima .\n",
      "2021-07-09 10:53:14,946 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:53:14,947 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:53:14,947 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:53:14,947 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka turebe icyo byabaga bikubiyemo mu gusoma umuzingo .\n",
      "2021-07-09 10:53:14,947 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:53:14,948 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:53:14,948 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:53:14,948 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho , twagombye kwishingikiriza ku Mana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:53:14,948 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:53:14,949 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:53:14,949 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:53:14,949 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bagaragaza ko nibura mu rugero runaka , bumva bahumurijwe mu isi ya Satani .\n",
      "2021-07-09 10:53:14,949 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   108000: bleu:  21.76, loss: 47049.2188, ppl:   5.2853, duration: 21.3292s\n",
      "2021-07-09 10:53:40,552 - INFO - joeynmt.training - Epoch  17, Step:   108200, Batch Loss:     2.043908, Tokens per Sec:    14497, Lr: 0.000300\n",
      "2021-07-09 10:54:06,030 - INFO - joeynmt.training - Epoch  17, Step:   108400, Batch Loss:     1.992501, Tokens per Sec:    14843, Lr: 0.000300\n",
      "2021-07-09 10:54:31,467 - INFO - joeynmt.training - Epoch  17, Step:   108600, Batch Loss:     2.189002, Tokens per Sec:    14604, Lr: 0.000300\n",
      "2021-07-09 10:54:56,718 - INFO - joeynmt.training - Epoch  17, Step:   108800, Batch Loss:     2.022680, Tokens per Sec:    14461, Lr: 0.000300\n",
      "2021-07-09 10:55:22,203 - INFO - joeynmt.training - Epoch  17, Step:   109000, Batch Loss:     2.033035, Tokens per Sec:    14717, Lr: 0.000300\n",
      "2021-07-09 10:55:47,709 - INFO - joeynmt.training - Epoch  17, Step:   109200, Batch Loss:     2.154953, Tokens per Sec:    14637, Lr: 0.000300\n",
      "2021-07-09 10:56:13,128 - INFO - joeynmt.training - Epoch  17, Step:   109400, Batch Loss:     2.331497, Tokens per Sec:    14597, Lr: 0.000300\n",
      "2021-07-09 10:56:38,468 - INFO - joeynmt.training - Epoch  17, Step:   109600, Batch Loss:     2.252588, Tokens per Sec:    14628, Lr: 0.000300\n",
      "2021-07-09 10:57:03,848 - INFO - joeynmt.training - Epoch  17, Step:   109800, Batch Loss:     2.395046, Tokens per Sec:    14377, Lr: 0.000300\n",
      "2021-07-09 10:57:29,238 - INFO - joeynmt.training - Epoch  17, Step:   110000, Batch Loss:     2.078266, Tokens per Sec:    14741, Lr: 0.000300\n",
      "2021-07-09 10:57:54,690 - INFO - joeynmt.training - Epoch  17, Step:   110200, Batch Loss:     2.075892, Tokens per Sec:    14646, Lr: 0.000300\n",
      "2021-07-09 10:58:20,017 - INFO - joeynmt.training - Epoch  17, Step:   110400, Batch Loss:     2.146945, Tokens per Sec:    14626, Lr: 0.000300\n",
      "2021-07-09 10:58:45,205 - INFO - joeynmt.training - Epoch  17, Step:   110600, Batch Loss:     2.100533, Tokens per Sec:    14507, Lr: 0.000300\n",
      "2021-07-09 10:59:10,480 - INFO - joeynmt.training - Epoch  17, Step:   110800, Batch Loss:     2.211151, Tokens per Sec:    14531, Lr: 0.000300\n",
      "2021-07-09 10:59:35,591 - INFO - joeynmt.training - Epoch  17, Step:   111000, Batch Loss:     1.997112, Tokens per Sec:    14488, Lr: 0.000300\n",
      "2021-07-09 10:59:56,180 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 10:59:56,180 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 10:59:56,180 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 10:59:57,070 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 10:59:57,070 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 10:59:57,072 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 10:59:57,072 - INFO - joeynmt.training - \tHypothesis: Narakoze ku mutima .\n",
      "2021-07-09 10:59:57,072 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 10:59:57,073 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 10:59:57,076 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 10:59:57,077 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka turebe icyo cyakoreshwaga mu gusoma umuzingo .\n",
      "2021-07-09 10:59:57,077 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 10:59:57,078 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 10:59:57,078 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:59:57,078 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa twihebye , twagombye gukomeza kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 10:59:57,078 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 10:59:57,079 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 10:59:57,079 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 10:59:57,081 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo batanga igihamya kigaragaza ko nibura mu rugero runaka , bumva bahumurijwe mu isi ya Satani .\n",
      "2021-07-09 10:59:57,081 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   111000: bleu:  22.00, loss: 47055.6602, ppl:   5.2865, duration: 21.4899s\n",
      "2021-07-09 11:00:22,517 - INFO - joeynmt.training - Epoch  17, Step:   111200, Batch Loss:     2.197623, Tokens per Sec:    14468, Lr: 0.000300\n",
      "2021-07-09 11:00:34,459 - INFO - joeynmt.training - Epoch  17: total training loss 13556.10\n",
      "2021-07-09 11:00:34,459 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-09 11:00:48,378 - INFO - joeynmt.training - Epoch  18, Step:   111400, Batch Loss:     2.017402, Tokens per Sec:    13706, Lr: 0.000300\n",
      "2021-07-09 11:01:14,004 - INFO - joeynmt.training - Epoch  18, Step:   111600, Batch Loss:     1.991549, Tokens per Sec:    14387, Lr: 0.000300\n",
      "2021-07-09 11:01:39,329 - INFO - joeynmt.training - Epoch  18, Step:   111800, Batch Loss:     2.082044, Tokens per Sec:    14533, Lr: 0.000300\n",
      "2021-07-09 11:02:04,822 - INFO - joeynmt.training - Epoch  18, Step:   112000, Batch Loss:     2.053621, Tokens per Sec:    14751, Lr: 0.000300\n",
      "2021-07-09 11:02:30,386 - INFO - joeynmt.training - Epoch  18, Step:   112200, Batch Loss:     1.940080, Tokens per Sec:    14480, Lr: 0.000300\n",
      "2021-07-09 11:02:55,626 - INFO - joeynmt.training - Epoch  18, Step:   112400, Batch Loss:     1.869544, Tokens per Sec:    14866, Lr: 0.000300\n",
      "2021-07-09 11:03:21,167 - INFO - joeynmt.training - Epoch  18, Step:   112600, Batch Loss:     1.896741, Tokens per Sec:    14871, Lr: 0.000300\n",
      "2021-07-09 11:03:46,543 - INFO - joeynmt.training - Epoch  18, Step:   112800, Batch Loss:     2.013592, Tokens per Sec:    14687, Lr: 0.000300\n",
      "2021-07-09 11:04:11,886 - INFO - joeynmt.training - Epoch  18, Step:   113000, Batch Loss:     2.081454, Tokens per Sec:    14477, Lr: 0.000300\n",
      "2021-07-09 11:04:37,135 - INFO - joeynmt.training - Epoch  18, Step:   113200, Batch Loss:     2.012277, Tokens per Sec:    14373, Lr: 0.000300\n",
      "2021-07-09 11:05:02,292 - INFO - joeynmt.training - Epoch  18, Step:   113400, Batch Loss:     2.096258, Tokens per Sec:    14641, Lr: 0.000300\n",
      "2021-07-09 11:05:27,679 - INFO - joeynmt.training - Epoch  18, Step:   113600, Batch Loss:     2.092951, Tokens per Sec:    14618, Lr: 0.000300\n",
      "2021-07-09 11:05:53,018 - INFO - joeynmt.training - Epoch  18, Step:   113800, Batch Loss:     2.161701, Tokens per Sec:    14589, Lr: 0.000300\n",
      "2021-07-09 11:06:18,457 - INFO - joeynmt.training - Epoch  18, Step:   114000, Batch Loss:     2.108807, Tokens per Sec:    14517, Lr: 0.000300\n",
      "2021-07-09 11:06:38,900 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 11:06:38,900 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 11:06:38,900 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 11:06:39,133 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 11:06:39,133 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 11:06:39,854 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 11:06:39,855 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 11:06:39,855 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 11:06:39,855 - INFO - joeynmt.training - \tHypothesis: Narakoze ku mutima .\n",
      "2021-07-09 11:06:39,855 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 11:06:39,856 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 11:06:39,856 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 11:06:39,856 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo bagombaga gusoma umuzingo .\n",
      "2021-07-09 11:06:39,856 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 11:06:39,857 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 11:06:39,857 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 11:06:39,857 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho kugira ngo twihebye cyangwa twihebye , twagombye gukomeza kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 11:06:39,857 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 11:06:39,858 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 11:06:39,858 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 11:06:39,858 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo batanga igihamya cy’uko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 11:06:39,858 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   114000: bleu:  22.02, loss: 46725.8867, ppl:   5.2252, duration: 21.4009s\n",
      "2021-07-09 11:07:05,508 - INFO - joeynmt.training - Epoch  18, Step:   114200, Batch Loss:     2.161787, Tokens per Sec:    14651, Lr: 0.000300\n",
      "2021-07-09 11:07:30,605 - INFO - joeynmt.training - Epoch  18, Step:   114400, Batch Loss:     2.177218, Tokens per Sec:    14414, Lr: 0.000300\n",
      "2021-07-09 11:07:55,938 - INFO - joeynmt.training - Epoch  18, Step:   114600, Batch Loss:     2.016459, Tokens per Sec:    14258, Lr: 0.000300\n",
      "2021-07-09 11:08:21,387 - INFO - joeynmt.training - Epoch  18, Step:   114800, Batch Loss:     1.971335, Tokens per Sec:    14448, Lr: 0.000300\n",
      "2021-07-09 11:08:46,865 - INFO - joeynmt.training - Epoch  18, Step:   115000, Batch Loss:     1.951772, Tokens per Sec:    14656, Lr: 0.000300\n",
      "2021-07-09 11:09:12,207 - INFO - joeynmt.training - Epoch  18, Step:   115200, Batch Loss:     1.917850, Tokens per Sec:    14496, Lr: 0.000300\n",
      "2021-07-09 11:09:37,798 - INFO - joeynmt.training - Epoch  18, Step:   115400, Batch Loss:     2.156228, Tokens per Sec:    14665, Lr: 0.000300\n",
      "2021-07-09 11:10:03,033 - INFO - joeynmt.training - Epoch  18, Step:   115600, Batch Loss:     2.094123, Tokens per Sec:    14322, Lr: 0.000300\n",
      "2021-07-09 11:10:28,710 - INFO - joeynmt.training - Epoch  18, Step:   115800, Batch Loss:     2.068903, Tokens per Sec:    14893, Lr: 0.000300\n",
      "2021-07-09 11:10:53,924 - INFO - joeynmt.training - Epoch  18, Step:   116000, Batch Loss:     2.041445, Tokens per Sec:    14491, Lr: 0.000300\n",
      "2021-07-09 11:11:19,280 - INFO - joeynmt.training - Epoch  18, Step:   116200, Batch Loss:     1.967161, Tokens per Sec:    14626, Lr: 0.000300\n",
      "2021-07-09 11:11:44,692 - INFO - joeynmt.training - Epoch  18, Step:   116400, Batch Loss:     2.129862, Tokens per Sec:    14330, Lr: 0.000300\n",
      "2021-07-09 11:12:10,152 - INFO - joeynmt.training - Epoch  18, Step:   116600, Batch Loss:     1.848674, Tokens per Sec:    14423, Lr: 0.000300\n",
      "2021-07-09 11:12:35,342 - INFO - joeynmt.training - Epoch  18, Step:   116800, Batch Loss:     2.287225, Tokens per Sec:    14536, Lr: 0.000300\n",
      "2021-07-09 11:13:00,705 - INFO - joeynmt.training - Epoch  18, Step:   117000, Batch Loss:     2.160595, Tokens per Sec:    14458, Lr: 0.000300\n",
      "2021-07-09 11:13:24,569 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 11:13:24,569 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 11:13:24,569 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 11:13:24,805 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-09 11:13:24,805 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-09 11:13:25,880 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 11:13:25,881 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 11:13:25,881 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 11:13:25,881 - INFO - joeynmt.training - \tHypothesis: Nakoraga ku mutima .\n",
      "2021-07-09 11:13:25,881 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 11:13:25,881 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 11:13:25,882 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 11:13:25,882 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo gusoma umuzingo byari bikubiyemo .\n",
      "2021-07-09 11:13:25,882 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 11:13:25,882 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 11:13:25,882 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 11:13:25,883 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho cyangwa twihebye , twagombye gukomeza kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 11:13:25,883 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 11:13:25,883 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 11:13:25,883 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 11:13:25,883 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya gikristo bahamya ko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 11:13:25,884 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   117000: bleu:  22.32, loss: 46503.8594, ppl:   5.1843, duration: 25.1778s\n",
      "2021-07-09 11:13:51,579 - INFO - joeynmt.training - Epoch  18, Step:   117200, Batch Loss:     2.099033, Tokens per Sec:    14438, Lr: 0.000300\n",
      "2021-07-09 11:14:16,966 - INFO - joeynmt.training - Epoch  18, Step:   117400, Batch Loss:     2.241018, Tokens per Sec:    14387, Lr: 0.000300\n",
      "2021-07-09 11:14:42,219 - INFO - joeynmt.training - Epoch  18, Step:   117600, Batch Loss:     2.164613, Tokens per Sec:    14568, Lr: 0.000300\n",
      "2021-07-09 11:15:07,653 - INFO - joeynmt.training - Epoch  18, Step:   117800, Batch Loss:     1.968946, Tokens per Sec:    14639, Lr: 0.000300\n",
      "2021-07-09 11:15:13,344 - INFO - joeynmt.training - Epoch  18: total training loss 13464.97\n",
      "2021-07-09 11:15:13,345 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-09 11:15:33,832 - INFO - joeynmt.training - Epoch  19, Step:   118000, Batch Loss:     1.926312, Tokens per Sec:    14089, Lr: 0.000300\n",
      "2021-07-09 11:15:59,126 - INFO - joeynmt.training - Epoch  19, Step:   118200, Batch Loss:     2.038680, Tokens per Sec:    14399, Lr: 0.000300\n",
      "2021-07-09 11:16:24,315 - INFO - joeynmt.training - Epoch  19, Step:   118400, Batch Loss:     2.266692, Tokens per Sec:    14539, Lr: 0.000300\n",
      "2021-07-09 11:16:49,708 - INFO - joeynmt.training - Epoch  19, Step:   118600, Batch Loss:     1.913449, Tokens per Sec:    14494, Lr: 0.000300\n",
      "2021-07-09 11:17:15,112 - INFO - joeynmt.training - Epoch  19, Step:   118800, Batch Loss:     2.029361, Tokens per Sec:    14566, Lr: 0.000300\n",
      "2021-07-09 11:17:40,308 - INFO - joeynmt.training - Epoch  19, Step:   119000, Batch Loss:     1.862911, Tokens per Sec:    14580, Lr: 0.000300\n",
      "2021-07-09 11:18:05,644 - INFO - joeynmt.training - Epoch  19, Step:   119200, Batch Loss:     2.076457, Tokens per Sec:    14518, Lr: 0.000300\n",
      "2021-07-09 11:18:31,016 - INFO - joeynmt.training - Epoch  19, Step:   119400, Batch Loss:     2.236531, Tokens per Sec:    14419, Lr: 0.000300\n",
      "2021-07-09 11:18:56,374 - INFO - joeynmt.training - Epoch  19, Step:   119600, Batch Loss:     2.021213, Tokens per Sec:    14715, Lr: 0.000300\n",
      "2021-07-09 11:19:21,372 - INFO - joeynmt.training - Epoch  19, Step:   119800, Batch Loss:     2.171004, Tokens per Sec:    14339, Lr: 0.000300\n",
      "2021-07-09 11:19:46,665 - INFO - joeynmt.training - Epoch  19, Step:   120000, Batch Loss:     2.176523, Tokens per Sec:    14618, Lr: 0.000300\n",
      "2021-07-09 11:20:08,673 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-09 11:20:08,673 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-09 11:20:08,673 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-09 11:20:09,605 - INFO - joeynmt.training - Example #0\n",
      "2021-07-09 11:20:09,606 - INFO - joeynmt.training - \tSource:     My heart was touched .\n",
      "2021-07-09 11:20:09,606 - INFO - joeynmt.training - \tReference:  Byankoze ku mutima .\n",
      "2021-07-09 11:20:09,606 - INFO - joeynmt.training - \tHypothesis: Nakoze ku mutima .\n",
      "2021-07-09 11:20:09,606 - INFO - joeynmt.training - Example #1\n",
      "2021-07-09 11:20:09,607 - INFO - joeynmt.training - \tSource:     Consider , however , what was involved in reading a scroll .\n",
      "2021-07-09 11:20:09,607 - INFO - joeynmt.training - \tReference:  Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
      "2021-07-09 11:20:09,607 - INFO - joeynmt.training - \tHypothesis: Ariko kandi , reka dusuzume icyo byabaga bikubiyemo gusoma umuzingo .\n",
      "2021-07-09 11:20:09,607 - INFO - joeynmt.training - Example #2\n",
      "2021-07-09 11:20:09,608 - INFO - joeynmt.training - \tSource:     Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
      "2021-07-09 11:20:09,608 - INFO - joeynmt.training - \tReference:  Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 11:20:09,608 - INFO - joeynmt.training - \tHypothesis: Aho kugira ngo tugire icyo tugeraho , twagombye gukomeza kwiringira Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
      "2021-07-09 11:20:09,608 - INFO - joeynmt.training - Example #3\n",
      "2021-07-09 11:20:09,609 - INFO - joeynmt.training - \tSource:     Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
      "2021-07-09 11:20:09,609 - INFO - joeynmt.training - \tReference:  Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
      "2021-07-09 11:20:09,609 - INFO - joeynmt.training - \tHypothesis: Ikibabaje ni uko bamwe mu bagize itorero rya Gikristo bahamya ko nibura mu rugero runaka , bumva bihumurije mu isi ya Satani .\n",
      "2021-07-09 11:20:09,609 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   120000: bleu:  22.23, loss: 46539.3359, ppl:   5.1908, duration: 22.9436s\n",
      "2021-07-09 11:20:35,062 - INFO - joeynmt.training - Epoch  19, Step:   120200, Batch Loss:     2.047122, Tokens per Sec:    14438, Lr: 0.000300\n",
      "2021-07-09 11:21:00,431 - INFO - joeynmt.training - Epoch  19, Step:   120400, Batch Loss:     2.079525, Tokens per Sec:    14568, Lr: 0.000300\n",
      "2021-07-09 11:21:25,870 - INFO - joeynmt.training - Epoch  19, Step:   120600, Batch Loss:     2.314065, Tokens per Sec:    14640, Lr: 0.000300\n",
      "2021-07-09 11:21:51,270 - INFO - joeynmt.training - Epoch  19, Step:   120800, Batch Loss:     2.018618, Tokens per Sec:    14701, Lr: 0.000300\n",
      "2021-07-09 11:22:16,819 - INFO - joeynmt.training - Epoch  19, Step:   121000, Batch Loss:     1.987122, Tokens per Sec:    14850, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PFoD_xpxZEv",
    "outputId": "1cf4cdc6-2760-4e6c-b046-a467ba400bc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3000\tLoss: 100428.37500\tPPL: 34.94775\tbleu: 1.69885\tLR: 0.00030000\t*\n",
      "Steps: 6000\tLoss: 85745.39062\tPPL: 20.78576\tbleu: 4.09416\tLR: 0.00030000\t*\n",
      "Steps: 9000\tLoss: 78221.25000\tPPL: 15.92694\tbleu: 6.55603\tLR: 0.00030000\t*\n",
      "Steps: 12000\tLoss: 72449.96875\tPPL: 12.98485\tbleu: 9.16044\tLR: 0.00030000\t*\n",
      "Steps: 15000\tLoss: 68738.13281\tPPL: 11.38655\tbleu: 10.37678\tLR: 0.00030000\t*\n",
      "Steps: 18000\tLoss: 65747.74219\tPPL: 10.24318\tbleu: 11.60779\tLR: 0.00030000\t*\n",
      "Steps: 21000\tLoss: 63069.15625\tPPL: 9.31686\tbleu: 12.74133\tLR: 0.00030000\t*\n",
      "Steps: 24000\tLoss: 61083.14844\tPPL: 8.68456\tbleu: 13.74739\tLR: 0.00030000\t*\n",
      "Steps: 27000\tLoss: 59487.35156\tPPL: 8.20773\tbleu: 14.55375\tLR: 0.00030000\t*\n",
      "Steps: 30000\tLoss: 58098.73828\tPPL: 7.81416\tbleu: 15.19644\tLR: 0.00030000\t*\n",
      "Steps: 33000\tLoss: 57091.47266\tPPL: 7.54054\tbleu: 15.63053\tLR: 0.00030000\t*\n",
      "Steps: 36000\tLoss: 56009.30078\tPPL: 7.25723\tbleu: 16.45190\tLR: 0.00030000\t*\n",
      "Steps: 39000\tLoss: 55082.56641\tPPL: 7.02310\tbleu: 16.97624\tLR: 0.00030000\t*\n",
      "Steps: 42000\tLoss: 54270.23438\tPPL: 6.82409\tbleu: 17.15271\tLR: 0.00030000\t*\n",
      "Steps: 45000\tLoss: 53609.98047\tPPL: 6.66649\tbleu: 17.94789\tLR: 0.00030000\t*\n",
      "Steps: 48000\tLoss: 53163.10156\tPPL: 6.56190\tbleu: 18.24000\tLR: 0.00030000\t*\n",
      "Steps: 51000\tLoss: 52971.12109\tPPL: 6.51747\tbleu: 18.21536\tLR: 0.00030000\t*\n",
      "Steps: 54000\tLoss: 52267.33594\tPPL: 6.35716\tbleu: 18.47918\tLR: 0.00030000\t*\n",
      "Steps: 57000\tLoss: 51397.92578\tPPL: 6.16456\tbleu: 19.25130\tLR: 0.00030000\t*\n",
      "Steps: 60000\tLoss: 51015.26562\tPPL: 6.08164\tbleu: 19.28816\tLR: 0.00030000\t*\n",
      "Steps: 63000\tLoss: 50691.22266\tPPL: 6.01230\tbleu: 19.88424\tLR: 0.00030000\t*\n",
      "Steps: 66000\tLoss: 50130.72266\tPPL: 5.89423\tbleu: 19.68610\tLR: 0.00030000\t*\n",
      "Steps: 69000\tLoss: 49922.05859\tPPL: 5.85087\tbleu: 19.59682\tLR: 0.00030000\t*\n",
      "Steps: 72000\tLoss: 49734.09766\tPPL: 5.81208\tbleu: 20.20671\tLR: 0.00030000\t*\n",
      "Steps: 75000\tLoss: 49350.60938\tPPL: 5.73374\tbleu: 20.46012\tLR: 0.00030000\t*\n",
      "Steps: 78000\tLoss: 49010.23438\tPPL: 5.66509\tbleu: 20.52823\tLR: 0.00030000\t*\n",
      "Steps: 81000\tLoss: 48625.64844\tPPL: 5.58851\tbleu: 20.65158\tLR: 0.00030000\t*\n",
      "Steps: 84000\tLoss: 48549.79297\tPPL: 5.57353\tbleu: 20.71393\tLR: 0.00030000\t*\n",
      "Steps: 87000\tLoss: 48366.31250\tPPL: 5.53746\tbleu: 21.13971\tLR: 0.00030000\t*\n",
      "Steps: 90000\tLoss: 48077.76562\tPPL: 5.48121\tbleu: 21.37548\tLR: 0.00030000\t*\n",
      "Steps: 93000\tLoss: 47829.81250\tPPL: 5.43333\tbleu: 21.37054\tLR: 0.00030000\t*\n",
      "Steps: 96000\tLoss: 47648.84766\tPPL: 5.39864\tbleu: 21.64330\tLR: 0.00030000\t*\n",
      "Steps: 99000\tLoss: 47533.60156\tPPL: 5.37667\tbleu: 21.83323\tLR: 0.00030000\t*\n",
      "Steps: 102000\tLoss: 47282.33203\tPPL: 5.32907\tbleu: 22.13085\tLR: 0.00030000\t*\n",
      "Steps: 105000\tLoss: 47096.23828\tPPL: 5.29410\tbleu: 21.48257\tLR: 0.00030000\t*\n",
      "Steps: 108000\tLoss: 47049.21875\tPPL: 5.28530\tbleu: 21.76387\tLR: 0.00030000\t*\n",
      "Steps: 111000\tLoss: 47055.66016\tPPL: 5.28650\tbleu: 21.99662\tLR: 0.00030000\t\n",
      "Steps: 114000\tLoss: 46725.88672\tPPL: 5.22517\tbleu: 22.02179\tLR: 0.00030000\t*\n",
      "Steps: 117000\tLoss: 46503.85938\tPPL: 5.18427\tbleu: 22.31707\tLR: 0.00030000\t*\n",
      "Steps: 120000\tLoss: 46539.33594\tPPL: 5.19079\tbleu: 22.22513\tLR: 0.00030000\t\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/enrw_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdsv-nkLxhUs",
    "outputId": "b7d0a449-f792-4734-893d-1d93805e1851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-10 09:37:50,168 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-10 09:37:50,173 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-10 09:37:50,998 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-10 09:37:52,280 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-10 09:37:53,768 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-10 09:37:53,830 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-10 09:38:00,004 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 09:38:00,357 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 09:38:00,428 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/dev.bpe.rw)...\n",
      "2021-07-10 09:38:23,528 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 09:38:23,528 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 09:38:23,528 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 09:38:23,760 - INFO - joeynmt.prediction -  dev bleu[13a]:  23.34 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 09:38:23,761 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Kinyarwanda/test.bpe.rw)...\n",
      "2021-07-10 09:39:03,020 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 09:39:03,020 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 09:39:03,021 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 09:39:03,528 - INFO - joeynmt.prediction - test bleu[13a]:  34.25 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt test 'models/enrw_transformer/config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbCNBp8BdCpd"
   },
   "source": [
    "## Luhyia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJddfbVID9rw"
   },
   "outputs": [],
   "source": [
    "# Changing to Luhyia directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kE77vmoBEJXK",
    "outputId": "c5541d6b-c8f9-41fc-e92f-7f4efc80cf22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Luhyia/joeynmt\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
      "Collecting numpy==1.20.1\n",
      "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 101 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 23.4 MB/s \n",
      "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s \n",
      "\u001b[?25hCollecting subword-nmt\n",
      "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 45.3 MB/s \n",
      "\u001b[?25hCollecting pylint\n",
      "  Downloading pylint-2.9.5-py3-none-any.whl (375 kB)\n",
      "\u001b[K     |████████████████████████████████| 375 kB 69.1 MB/s \n",
      "\u001b[?25hCollecting six==1.12\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
      "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
      "Collecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.9.2-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 63.9 MB/s \n",
      "\u001b[?25hCollecting astroid<2.7,>=2.6.5\n",
      "  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 72.6 MB/s \n",
      "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 4.6 MB/s \n",
      "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
      "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
      "\u001b[K     |████████████████████████████████| 743 kB 44.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
      "Building wheels for collected packages: joeynmt, wrapt\n",
      "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85058 sha256=0a760e581cd30870226b414b6c78f31835ec9cf9bf173b03840345786206cfd9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y__d5zz_/wheels/4c/c1/26/6ad139117beb3e51d701309aa531dfa79005312872e6135b43\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68429 sha256=3a2dfb2434d7dc9eff0fa2345e27ac073f01a8345fd0c26190afdae772e75793\n",
      "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
      "Successfully built joeynmt wrapt\n",
      "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.12.1\n",
      "    Uninstalling wrapt-1.12.1:\n",
      "      Successfully uninstalled wrapt-1.12.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.10.0\n",
      "    Uninstalling torchtext-0.10.0:\n",
      "      Successfully uninstalled torchtext-0.10.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
      "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
      "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed astroid-2.6.5 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.5 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/joeynmt/joeynmt.git\n",
    "! cd joeynmt; pip3 install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "ZAHPSbe89GrT"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language3, source_language)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language3}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language3}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 1096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 1600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 200         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_reverse_transformer\"\n",
    "    overwrite: False              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\", source_language=source_language, target_language3=target_language3)\n",
    "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESL-qhTaelt0",
    "outputId": "332ff85b-e1e4-46bb-f1cc-107c1de30112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 09:16:23,658 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-01 09:16:23,688 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-01 09:16:23,785 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-01 09:16:24,049 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-01 09:16:24,070 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-01 09:16:24,086 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-01 09:16:24,087 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-01 09:16:24,361 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-01 09:16:24.555521: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-01 09:16:26,408 - INFO - joeynmt.training - Total params: 12097024\n",
      "2021-07-01 09:16:28,708 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-01 09:16:28,708 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-01 09:16:28,709 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-01 09:16:28,709 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\n",
      "2021-07-01 09:16:28,709 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\n",
      "2021-07-01 09:16:28,709 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\n",
      "2021-07-01 09:16:28,710 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-01 09:16:28,710 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-01 09:16:28,710 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-01 09:16:28,710 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-01 09:16:28,710 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-01 09:16:28,711 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-01 09:16:28,711 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-01 09:16:28,711 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-01 09:16:28,711 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-01 09:16:28,712 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-01 09:16:28,712 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-01 09:16:28,712 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-01 09:16:28,712 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-01 09:16:28,713 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-01 09:16:28,713 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-01 09:16:28,713 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-01 09:16:28,713 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-01 09:16:28,714 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-01 09:16:28,714 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-01 09:16:28,714 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-01 09:16:28,714 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-01 09:16:28,715 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1096\n",
      "2021-07-01 09:16:28,715 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-01 09:16:28,715 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-01 09:16:28,715 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-01 09:16:28,716 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-01 09:16:28,716 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-01 09:16:28,716 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-01 09:16:28,716 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 200\n",
      "2021-07-01 09:16:28,716 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-01 09:16:28,717 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-01 09:16:28,717 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lhen_reverse_transformer\n",
      "2021-07-01 09:16:28,717 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-01 09:16:28,717 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-01 09:16:28,718 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-01 09:16:28,718 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-01 09:16:28,718 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-01 09:16:28,718 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-01 09:16:28,719 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-01 09:16:28,719 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-01 09:16:28,719 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-01 09:16:28,719 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-01 09:16:28,720 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-01 09:16:28,720 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-01 09:16:28,720 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-01 09:16:28,720 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-01 09:16:28,720 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-01 09:16:28,721 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-01 09:16:28,721 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-01 09:16:28,721 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-01 09:16:28,721 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-01 09:16:28,722 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-01 09:16:28,722 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-01 09:16:28,722 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-01 09:16:28,722 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-01 09:16:28,723 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-01 09:16:28,723 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-01 09:16:28,723 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-01 09:16:28,723 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-01 09:16:28,724 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-01 09:16:28,724 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-01 09:16:28,724 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-01 09:16:28,724 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-01 09:16:28,724 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 5904,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-01 09:16:28,725 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-01 09:16:28,725 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-01 09:16:28,725 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-01 09:16:28,726 - INFO - joeynmt.helpers - Number of Src words (types): 4050\n",
      "2021-07-01 09:16:28,726 - INFO - joeynmt.helpers - Number of Trg words (types): 4050\n",
      "2021-07-01 09:16:28,726 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4050),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4050))\n",
      "2021-07-01 09:16:28,741 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 1096\n",
      "\ttotal batch size (w. parallel & accumulation): 1096\n",
      "2021-07-01 09:16:28,741 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-01 09:16:42,363 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.371404, Tokens per Sec:     5285, Lr: 0.000300\n",
      "2021-07-01 09:16:55,845 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.094975, Tokens per Sec:     5344, Lr: 0.000300\n",
      "2021-07-01 09:18:47,449 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:18:47,449 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:18:48,328 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:18:48,329 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:18:48,329 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:18:48,329 - INFO - joeynmt.training - \tHypothesis: And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And the the the the the the the the the the the the the the the the the And And And And And And And And And And And And And And And And And And And And And And And And And And And And the the the the the the\n",
      "2021-07-01 09:18:48,330 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:18:48,330 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:18:48,331 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:18:48,331 - INFO - joeynmt.training - \tHypothesis: And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And the the the the the the the the the the the the the the the the And And And And And And And And And And And And And And And And And And And And And And And And And And And And And the the the the the the\n",
      "2021-07-01 09:18:48,331 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:18:48,332 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:18:48,332 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:18:48,332 - INFO - joeynmt.training - \tHypothesis: And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And\n",
      "2021-07-01 09:18:48,332 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:18:48,333 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:18:48,333 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:18:48,333 - INFO - joeynmt.training - \tHypothesis: And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And And the the the the the the the the the the the the the the the the the And And And And And And And And And And And And And And And And And And And And And And And And And And And the the the the the the the\n",
      "2021-07-01 09:18:48,334 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      200: bleu:   0.01, loss: 165704.7031, ppl: 161.2265, duration: 112.4884s\n",
      "2021-07-01 09:18:57,461 - INFO - joeynmt.training - Epoch   1: total training loss 1416.51\n",
      "2021-07-01 09:18:57,462 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-01 09:19:02,221 - INFO - joeynmt.training - Epoch   2, Step:      300, Batch Loss:     4.849167, Tokens per Sec:     4934, Lr: 0.000300\n",
      "2021-07-01 09:19:15,639 - INFO - joeynmt.training - Epoch   2, Step:      400, Batch Loss:     4.788450, Tokens per Sec:     5314, Lr: 0.000300\n",
      "2021-07-01 09:21:07,062 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:21:07,063 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:21:07,964 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:21:07,965 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:21:07,965 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:21:07,966 - INFO - joeynmt.training - \tHypothesis: And they the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2021-07-01 09:21:07,966 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:21:07,967 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:21:07,967 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:21:07,967 - INFO - joeynmt.training - \tHypothesis: And they the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2021-07-01 09:21:07,968 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:21:07,968 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:21:07,969 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:21:07,969 - INFO - joeynmt.training - \tHypothesis: And they the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2021-07-01 09:21:07,969 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:21:07,970 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:21:07,970 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:21:07,970 - INFO - joeynmt.training - \tHypothesis: And they the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "2021-07-01 09:21:07,970 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      400: bleu:   0.01, loss: 154830.3438, ppl: 115.4978, duration: 112.3308s\n",
      "2021-07-01 09:21:21,882 - INFO - joeynmt.training - Epoch   2, Step:      500, Batch Loss:     4.596107, Tokens per Sec:     5110, Lr: 0.000300\n",
      "2021-07-01 09:21:26,940 - INFO - joeynmt.training - Epoch   2: total training loss 1280.92\n",
      "2021-07-01 09:21:26,941 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-01 09:21:35,238 - INFO - joeynmt.training - Epoch   3, Step:      600, Batch Loss:     4.409629, Tokens per Sec:     5285, Lr: 0.000300\n",
      "2021-07-01 09:23:26,642 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:23:26,643 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:23:27,556 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:23:27,558 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:23:27,558 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:23:27,558 - INFO - joeynmt.training - \tHypothesis: And they said , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-01 09:23:27,558 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:23:27,559 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:23:27,559 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:23:27,560 - INFO - joeynmt.training - \tHypothesis: And He said , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-01 09:23:27,560 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:23:27,560 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:23:27,561 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:23:27,561 - INFO - joeynmt.training - \tHypothesis: And they said to the Lord , and said , and said , and said to the Lord ,\n",
      "2021-07-01 09:23:27,561 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:23:27,562 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:23:27,562 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:23:27,562 - INFO - joeynmt.training - \tHypothesis: And they said , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-01 09:23:27,563 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step      600: bleu:   0.14, loss: 142989.3125, ppl:  80.3218, duration: 112.3239s\n",
      "2021-07-01 09:23:41,855 - INFO - joeynmt.training - Epoch   3, Step:      700, Batch Loss:     4.433087, Tokens per Sec:     5016, Lr: 0.000300\n",
      "2021-07-01 09:23:55,383 - INFO - joeynmt.training - Epoch   3, Step:      800, Batch Loss:     4.282960, Tokens per Sec:     5393, Lr: 0.000300\n",
      "2021-07-01 09:25:46,246 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:25:46,247 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:25:46,247 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:25:46,748 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:25:46,748 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:25:47,683 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:25:47,685 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:25:47,685 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:25:47,686 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-01 09:25:47,686 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:25:47,686 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:25:47,687 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:25:47,687 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-01 09:25:47,687 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:25:47,688 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:25:47,688 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:25:47,688 - INFO - joeynmt.training - \tHypothesis: And they said to them , “ And the Lord , and the Lord , and the Lord , and the Lord .\n",
      "2021-07-01 09:25:47,689 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:25:47,689 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:25:47,690 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:25:47,690 - INFO - joeynmt.training - \tHypothesis: And the Lord was the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord of the Lord of the Lord , and the Lord .\n",
      "2021-07-01 09:25:47,690 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step      800: bleu:   0.70, loss: 137030.9531, ppl:  66.9051, duration: 112.3068s\n",
      "2021-07-01 09:25:48,472 - INFO - joeynmt.training - Epoch   3: total training loss 1164.52\n",
      "2021-07-01 09:25:48,472 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-01 09:26:01,573 - INFO - joeynmt.training - Epoch   4, Step:      900, Batch Loss:     4.128816, Tokens per Sec:     5132, Lr: 0.000300\n",
      "2021-07-01 09:26:14,986 - INFO - joeynmt.training - Epoch   4, Step:     1000, Batch Loss:     3.892668, Tokens per Sec:     5328, Lr: 0.000300\n",
      "2021-07-01 09:28:05,288 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:28:05,289 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:28:05,289 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:28:05,632 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:28:05,632 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:28:06,485 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:28:06,486 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:28:06,487 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:28:06,487 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ What do not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not\n",
      "2021-07-01 09:28:06,487 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:28:06,488 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:28:06,488 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:28:06,488 - INFO - joeynmt.training - \tHypothesis: And He said to them , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ What do not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not\n",
      "2021-07-01 09:28:06,488 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:28:06,489 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:28:06,489 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:28:06,490 - INFO - joeynmt.training - \tHypothesis: And they had been been been a own own , and they had been been been been been been been been been been been been been been been been been been been been given to the Lord .\n",
      "2021-07-01 09:28:06,490 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:28:06,490 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:28:06,491 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:28:06,491 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord ,\n",
      "2021-07-01 09:28:06,491 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     1000: bleu:   0.93, loss: 133380.6250, ppl:  59.8179, duration: 111.5043s\n",
      "2021-07-01 09:28:16,934 - INFO - joeynmt.training - Epoch   4: total training loss 1116.92\n",
      "2021-07-01 09:28:16,934 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-01 09:28:20,195 - INFO - joeynmt.training - Epoch   5, Step:     1100, Batch Loss:     4.114851, Tokens per Sec:     5263, Lr: 0.000300\n",
      "2021-07-01 09:28:33,540 - INFO - joeynmt.training - Epoch   5, Step:     1200, Batch Loss:     4.100003, Tokens per Sec:     5371, Lr: 0.000300\n",
      "2021-07-01 09:29:43,622 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:29:43,622 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:29:43,623 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:29:43,997 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:29:43,998 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:29:44,883 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:29:44,884 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:29:44,884 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:29:44,884 - INFO - joeynmt.training - \tHypothesis: And they had come to them , and said , “ I am not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not . ”\n",
      "2021-07-01 09:29:44,885 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:29:44,885 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:29:44,886 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:29:44,886 - INFO - joeynmt.training - \tHypothesis: And they said to them , “ You have come to you , and the Lord , and the Lord , and the Lord , and the Lord , and said , “ You have come to you , “ You have be be be be be come . ”\n",
      "2021-07-01 09:29:44,886 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:29:44,887 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:29:44,887 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:29:44,887 - INFO - joeynmt.training - \tHypothesis: And the Lord was a man , and the Lord , and the Lord ,\n",
      "2021-07-01 09:29:44,887 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:29:44,888 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:29:44,888 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:29:44,889 - INFO - joeynmt.training - \tHypothesis: And the Lord was a man , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord , and the Lord ,\n",
      "2021-07-01 09:29:44,889 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     1200: bleu:   1.88, loss: 129902.8750, ppl:  53.7653, duration: 71.3476s\n",
      "2021-07-01 09:29:58,763 - INFO - joeynmt.training - Epoch   5, Step:     1300, Batch Loss:     3.952628, Tokens per Sec:     5245, Lr: 0.000300\n",
      "2021-07-01 09:30:04,670 - INFO - joeynmt.training - Epoch   5: total training loss 1068.93\n",
      "2021-07-01 09:30:04,671 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-01 09:30:12,208 - INFO - joeynmt.training - Epoch   6, Step:     1400, Batch Loss:     3.987895, Tokens per Sec:     5318, Lr: 0.000300\n",
      "2021-07-01 09:31:59,450 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:31:59,451 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:31:59,451 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:31:59,855 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:31:59,856 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:32:00,682 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:32:00,683 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:32:00,683 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:32:00,684 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , they had come to them , and they had come to them , and they had come to them , “ The Lord , and they had come to them , and they will be be the Lord . ”\n",
      "2021-07-01 09:32:00,684 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:32:00,684 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:32:00,685 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:32:00,685 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , they had come to them , and said to them , “ You have come to you , and you , and you have come to you , and you , and you have be be the Lord . ”\n",
      "2021-07-01 09:32:00,685 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:32:00,686 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:32:00,686 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:32:00,686 - INFO - joeynmt.training - \tHypothesis: And when they had been been been been been been been been been been been given to the temple , and they had been been been been given to the Lord .\n",
      "2021-07-01 09:32:00,686 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:32:00,687 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:32:00,687 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:32:00,688 - INFO - joeynmt.training - \tHypothesis: And when they had been been been been been been been been been a great man , and they had been been been been been been been been been been been been been given to the temple , and they had been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been been given to the temple .\n",
      "2021-07-01 09:32:00,688 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     1400: bleu:   1.68, loss: 127497.9141, ppl:  49.9418, duration: 108.4787s\n",
      "2021-07-01 09:32:14,507 - INFO - joeynmt.training - Epoch   6, Step:     1500, Batch Loss:     3.816556, Tokens per Sec:     5231, Lr: 0.000300\n",
      "2021-07-01 09:32:27,686 - INFO - joeynmt.training - Epoch   6, Step:     1600, Batch Loss:     3.861883, Tokens per Sec:     5384, Lr: 0.000300\n",
      "2021-07-01 09:33:39,948 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:33:39,949 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:33:39,949 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:33:40,344 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:33:40,344 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:33:41,637 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:33:41,638 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:33:41,639 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:33:41,639 - INFO - joeynmt.training - \tHypothesis: Then the disciples had come to the disciples , and the disciples , and the disciples , and the disciples , and the disciples , and the disciples , and the disciples said to them , “ You have come to the Lord . ”\n",
      "2021-07-01 09:33:41,639 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:33:41,640 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:33:41,640 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:33:41,640 - INFO - joeynmt.training - \tHypothesis: Then the disciples came to them , “ Seean of the disciples , and said to them , “ You are the Lord , and the Lord , and the Lord , and the Lord , and the Lord . ”\n",
      "2021-07-01 09:33:41,641 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:33:41,642 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:33:41,642 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:33:41,642 - INFO - joeynmt.training - \tHypothesis: And when they had come to the people , they had been been been been been been been been been given to the people .\n",
      "2021-07-01 09:33:41,642 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:33:41,643 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:33:41,643 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:33:41,643 - INFO - joeynmt.training - \tHypothesis: And when they had come to the people , they had been been been been been been been been been been been been been been been been been been been been been given to the people .\n",
      "2021-07-01 09:33:41,644 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     1600: bleu:   2.38, loss: 124984.1406, ppl:  46.2357, duration: 73.9572s\n",
      "2021-07-01 09:33:43,371 - INFO - joeynmt.training - Epoch   6: total training loss 1042.50\n",
      "2021-07-01 09:33:43,371 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-01 09:33:55,518 - INFO - joeynmt.training - Epoch   7, Step:     1700, Batch Loss:     3.761484, Tokens per Sec:     5159, Lr: 0.000300\n",
      "2021-07-01 09:34:08,712 - INFO - joeynmt.training - Epoch   7, Step:     1800, Batch Loss:     3.938340, Tokens per Sec:     5411, Lr: 0.000300\n",
      "2021-07-01 09:35:00,520 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:35:00,521 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:35:00,521 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:35:00,923 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:35:00,924 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:35:01,732 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:35:01,733 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:35:01,733 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:35:01,734 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , they had been been been taken to them , and they had come to them , and they had come to them , and they had come to them , and they had come to them .\n",
      "2021-07-01 09:35:01,734 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:35:01,734 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:35:01,735 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:35:01,735 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , they had come to them , and they had come to them , saying , “ You have been been been taken to you . ”\n",
      "2021-07-01 09:35:01,735 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:35:01,736 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:35:01,736 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:35:01,736 - INFO - joeynmt.training - \tHypothesis: And when they had been been been taken to the Jews , they had been been been taken to the Jews .\n",
      "2021-07-01 09:35:01,736 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:35:01,737 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:35:01,738 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:35:01,738 - INFO - joeynmt.training - \tHypothesis: And when they had been been been taken up , they had been been been taken to the Jews , and they had been been been been taken to the Jews .\n",
      "2021-07-01 09:35:01,738 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     1800: bleu:   2.48, loss: 123692.5312, ppl:  44.4397, duration: 53.0253s\n",
      "2021-07-01 09:35:12,913 - INFO - joeynmt.training - Epoch   7: total training loss 1016.13\n",
      "2021-07-01 09:35:12,913 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-01 09:35:15,299 - INFO - joeynmt.training - Epoch   8, Step:     1900, Batch Loss:     3.740321, Tokens per Sec:     5331, Lr: 0.000300\n",
      "2021-07-01 09:35:28,490 - INFO - joeynmt.training - Epoch   8, Step:     2000, Batch Loss:     3.702153, Tokens per Sec:     5472, Lr: 0.000300\n",
      "2021-07-01 09:36:54,032 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:36:54,032 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:36:54,033 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:36:54,417 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:36:54,418 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:36:55,265 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:36:55,267 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:36:55,267 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:36:55,267 - INFO - joeynmt.training - \tHypothesis: Then they had said to them , “ What is the temple , and the temple , and the son of the son of the son of the son of the temple , and the people , and the people , and the people , and the Son of the people of the people . ”\n",
      "2021-07-01 09:36:55,267 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:36:55,268 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:36:55,268 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:36:55,269 - INFO - joeynmt.training - \tHypothesis: Then they had said to them , “ The Lord , and the Lord , “ You are the Lord , and the Lord , and the Lord , and the Lord of the Lord , and the Lord , and the Lord , and the Lord , and the Lord . ”\n",
      "2021-07-01 09:36:55,269 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:36:55,269 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:36:55,270 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:36:55,270 - INFO - joeynmt.training - \tHypothesis: Then the multitude of the multitude of the temple , and the temple , and the temple of the people ,\n",
      "2021-07-01 09:36:55,270 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:36:55,271 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:36:55,271 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:36:55,271 - INFO - joeynmt.training - \tHypothesis: Then they had come to them , and they went out of the temple , and the temple , and the city of the city of the city of the temple , and the city of the city of the city of the people .\n",
      "2021-07-01 09:36:55,271 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     2000: bleu:   2.45, loss: 121792.1172, ppl:  41.9232, duration: 86.7812s\n",
      "2021-07-01 09:37:09,018 - INFO - joeynmt.training - Epoch   8, Step:     2100, Batch Loss:     3.937224, Tokens per Sec:     5204, Lr: 0.000300\n",
      "2021-07-01 09:37:15,864 - INFO - joeynmt.training - Epoch   8: total training loss 992.48\n",
      "2021-07-01 09:37:15,865 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-01 09:37:22,439 - INFO - joeynmt.training - Epoch   9, Step:     2200, Batch Loss:     3.416490, Tokens per Sec:     5329, Lr: 0.000300\n",
      "2021-07-01 09:38:54,602 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:38:54,603 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:38:54,603 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:38:54,990 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:38:54,990 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:38:55,828 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:38:55,829 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:38:55,830 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:38:55,830 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , they had come to him , He went to him , saying , “ You have no one of the son of the Son of the world , and the Son of the world , and you have been taken to you . ”\n",
      "2021-07-01 09:38:55,830 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:38:55,831 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:38:55,831 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:38:55,831 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , He had said to them , “ You have been given to you , and you have been given to you , ”\n",
      "2021-07-01 09:38:55,832 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:38:55,832 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:38:55,832 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:38:55,833 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , they had come to the temple , and they went out of the temple .\n",
      "2021-07-01 09:38:55,833 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:38:55,834 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:38:55,834 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:38:55,835 - INFO - joeynmt.training - \tHypothesis: And when they had come to the multitude , they had come to the temple , and the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the people .\n",
      "2021-07-01 09:38:55,835 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     2200: bleu:   2.57, loss: 119842.6797, ppl:  39.4898, duration: 93.3952s\n",
      "2021-07-01 09:39:09,613 - INFO - joeynmt.training - Epoch   9, Step:     2300, Batch Loss:     3.589273, Tokens per Sec:     5193, Lr: 0.000300\n",
      "2021-07-01 09:39:22,937 - INFO - joeynmt.training - Epoch   9, Step:     2400, Batch Loss:     3.504075, Tokens per Sec:     5408, Lr: 0.000300\n",
      "2021-07-01 09:40:51,866 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:40:51,867 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:40:51,867 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:40:52,299 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:40:52,300 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:40:53,134 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:40:53,135 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:40:53,135 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:40:53,135 - INFO - joeynmt.training - \tHypothesis: And when they had come to the disciples , they had said to them , “ I am not of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son of the son\n",
      "2021-07-01 09:40:53,135 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:40:53,136 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:40:53,136 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:40:53,137 - INFO - joeynmt.training - \tHypothesis: And when they had said to them , “ I am the temple of the temple , and the Jews , and the world , and the world will be called the world . ”\n",
      "2021-07-01 09:40:53,137 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:40:53,137 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:40:53,138 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:40:53,138 - INFO - joeynmt.training - \tHypothesis: And when he had come to the temple , he had been been brought them to the temple , and they had been been brought them to the sea .\n",
      "2021-07-01 09:40:53,138 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:40:53,139 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:40:53,139 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:40:53,139 - INFO - joeynmt.training - \tHypothesis: And when they had come to the temple , they had come to the sea , and the sea , and the sea , and the sea , and the sea , and the sea , and the sea , and the sea .\n",
      "2021-07-01 09:40:53,140 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     2400: bleu:   2.66, loss: 117787.9844, ppl:  37.0778, duration: 90.2019s\n",
      "2021-07-01 09:40:55,703 - INFO - joeynmt.training - Epoch   9: total training loss 963.48\n",
      "2021-07-01 09:40:55,703 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-01 09:41:06,975 - INFO - joeynmt.training - Epoch  10, Step:     2500, Batch Loss:     3.752923, Tokens per Sec:     5220, Lr: 0.000300\n",
      "2021-07-01 09:41:20,279 - INFO - joeynmt.training - Epoch  10, Step:     2600, Batch Loss:     3.486787, Tokens per Sec:     5368, Lr: 0.000300\n",
      "2021-07-01 09:42:38,647 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:42:38,648 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:42:38,648 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:42:39,050 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:42:39,050 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:42:40,444 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:42:40,445 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:42:40,446 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:42:40,446 - INFO - joeynmt.training - \tHypothesis: And when they had come to the temple , they saw the disciples , they saw him , and said to them , “ I have been done to you , and you , and have been done to you . ”\n",
      "2021-07-01 09:42:40,446 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:42:40,447 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:42:40,447 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:42:40,447 - INFO - joeynmt.training - \tHypothesis: Then the multitude of the Jews , and the multitude of the Jews , saying , “ I am the kingdom of the world , and the world , and the world . ”\n",
      "2021-07-01 09:42:40,448 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:42:40,448 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:42:40,449 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:42:40,449 - INFO - joeynmt.training - \tHypothesis: And when they had come to the temple , they had been been given to the temple , and they were healed .\n",
      "2021-07-01 09:42:40,449 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:42:40,450 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:42:40,450 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:42:40,450 - INFO - joeynmt.training - \tHypothesis: And when they had come to the temple , they had been been taken up , and the city of the temple , and the sea , and the sea , and the sea , and the sea , and the sea .\n",
      "2021-07-01 09:42:40,451 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     2600: bleu:   3.18, loss: 116235.1953, ppl:  35.3531, duration: 80.1715s\n",
      "2021-07-01 09:42:52,463 - INFO - joeynmt.training - Epoch  10: total training loss 938.96\n",
      "2021-07-01 09:42:52,464 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-01 09:42:54,362 - INFO - joeynmt.training - Epoch  11, Step:     2700, Batch Loss:     3.530386, Tokens per Sec:     5333, Lr: 0.000300\n",
      "2021-07-01 09:43:07,653 - INFO - joeynmt.training - Epoch  11, Step:     2800, Batch Loss:     3.528154, Tokens per Sec:     5420, Lr: 0.000300\n",
      "2021-07-01 09:44:05,688 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:44:05,688 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:44:05,689 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:44:06,120 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:44:06,121 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:44:06,956 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:44:06,957 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:44:06,957 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:44:06,957 - INFO - joeynmt.training - \tHypothesis: Then the Jews came to the Jews , and said to them , “ I am the Jews , and I am not of the Jews , and I will be afraid , and I will be afraid . ”\n",
      "2021-07-01 09:44:06,958 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:44:06,958 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:44:06,959 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:44:06,959 - INFO - joeynmt.training - \tHypothesis: Then the Jews came to the Jews , saying , “ I am the Jews , ” And when they had come to the Jews , saying , “ You have been done in the things of the things of the things of the things of the things of the things of God . ”\n",
      "2021-07-01 09:44:06,959 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:44:06,960 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:44:06,960 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:44:06,960 - INFO - joeynmt.training - \tHypothesis: Then he was a certain man , he who was a certain man who sat on the city .\n",
      "2021-07-01 09:44:06,960 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:44:06,961 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:44:06,961 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:44:06,961 - INFO - joeynmt.training - \tHypothesis: And when they had come to the city , they saw the city , they saw the city , they saw the city of the city , and the city of the city , and the Jews , and the Jews .\n",
      "2021-07-01 09:44:06,962 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step     2800: bleu:   3.57, loss: 114767.9297, ppl:  33.7973, duration: 59.3076s\n",
      "2021-07-01 09:44:20,631 - INFO - joeynmt.training - Epoch  11, Step:     2900, Batch Loss:     3.246042, Tokens per Sec:     5255, Lr: 0.000300\n",
      "2021-07-01 09:44:28,002 - INFO - joeynmt.training - Epoch  11: total training loss 930.50\n",
      "2021-07-01 09:44:28,002 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-01 09:44:33,824 - INFO - joeynmt.training - Epoch  12, Step:     3000, Batch Loss:     3.512256, Tokens per Sec:     5257, Lr: 0.000300\n",
      "2021-07-01 09:45:18,985 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:45:18,985 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:45:18,986 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:45:19,406 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:45:19,407 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:45:20,705 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:45:20,706 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:45:20,706 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:45:20,706 - INFO - joeynmt.training - \tHypothesis: And they went out of the disciples , and said to Him , “ What is the Son of Man , and you have been been taken up and have been taken to me , and they will be afraid . ”\n",
      "2021-07-01 09:45:20,707 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:45:20,708 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:45:20,708 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:45:20,708 - INFO - joeynmt.training - \tHypothesis: Then the disciples came to Him , and said to them , “ You are the kingdom of the world , and the kingdom of the kingdom of the kingdom of the world . ”\n",
      "2021-07-01 09:45:20,708 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:45:20,709 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:45:20,709 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:45:20,709 - INFO - joeynmt.training - \tHypothesis: And when he had come to the city , he was a great man , and the city of the city of the city .\n",
      "2021-07-01 09:45:20,710 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:45:20,710 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:45:20,710 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:45:20,711 - INFO - joeynmt.training - \tHypothesis: And when they had come to them , they saw the city , they saw the city , they saw the city , and they saw the city , and they went out of the sea , and they were healed .\n",
      "2021-07-01 09:45:20,711 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     3000: bleu:   3.52, loss: 114136.1484, ppl:  33.1486, duration: 46.8868s\n",
      "2021-07-01 09:45:34,623 - INFO - joeynmt.training - Epoch  12, Step:     3100, Batch Loss:     3.354867, Tokens per Sec:     5226, Lr: 0.000300\n",
      "2021-07-01 09:45:47,928 - INFO - joeynmt.training - Epoch  12, Step:     3200, Batch Loss:     3.259437, Tokens per Sec:     5357, Lr: 0.000300\n",
      "2021-07-01 09:46:45,459 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:46:45,459 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:46:45,459 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:46:45,889 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:46:45,889 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:46:46,716 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:46:46,718 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:46:46,718 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:46:46,718 - INFO - joeynmt.training - \tHypothesis: And when they had come to the Pharisees , He said to them , “ I am the Son of the Son of the Jews , I am not to you , and I am not afraid . ”\n",
      "2021-07-01 09:46:46,719 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:46:46,719 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:46:46,720 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:46:46,720 - INFO - joeynmt.training - \tHypothesis: Then the multitude came to the multitude , saying , “ I am the Jews , and I am the Son of the Jews , and I am not of the law of the law . ”\n",
      "2021-07-01 09:46:46,720 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:46:46,721 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:46:46,721 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:46:46,721 - INFO - joeynmt.training - \tHypothesis: And when he had come to the temple , he was called the temple of the Lord ,\n",
      "2021-07-01 09:46:46,721 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:46:46,723 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:46:46,724 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:46:46,724 - INFO - joeynmt.training - \tHypothesis: And when they had come to the multitude , they had come to the multitude , they went out of the sea , and they went out the tomb , and the third of the sea .\n",
      "2021-07-01 09:46:46,724 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     3200: bleu:   3.79, loss: 112001.7500, ppl:  31.0479, duration: 58.7958s\n",
      "2021-07-01 09:46:50,072 - INFO - joeynmt.training - Epoch  12: total training loss 906.77\n",
      "2021-07-01 09:46:50,073 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-01 09:47:00,506 - INFO - joeynmt.training - Epoch  13, Step:     3300, Batch Loss:     3.454672, Tokens per Sec:     5175, Lr: 0.000300\n",
      "2021-07-01 09:47:13,826 - INFO - joeynmt.training - Epoch  13, Step:     3400, Batch Loss:     3.466449, Tokens per Sec:     5386, Lr: 0.000300\n",
      "2021-07-01 09:48:02,999 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:48:03,000 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:48:03,000 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:48:03,397 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:48:03,397 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:48:04,244 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:48:04,246 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:48:04,246 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:48:04,246 - INFO - joeynmt.training - \tHypothesis: Then they went out to the Pharisees , and said to them , “ I am the man , and the other of the other man , and the other of the other of the other . ”\n",
      "2021-07-01 09:48:04,247 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:48:04,248 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:48:04,248 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:48:04,249 - INFO - joeynmt.training - \tHypothesis: Then the multitude came to the multitude , and said , “ It is the man , and the other of the people , and the things which is written , “ You are the things of the things of the things of God . ”\n",
      "2021-07-01 09:48:04,249 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:48:04,250 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:48:04,250 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:48:04,250 - INFO - joeynmt.training - \tHypothesis: And he who had been a certain man named a man , and the third of the people .\n",
      "2021-07-01 09:48:04,250 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:48:04,251 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:48:04,251 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:48:04,251 - INFO - joeynmt.training - \tHypothesis: And when they had come to the tomb , they were a great man , and the third of the soldiers , and the third of the tomb , and the other other .\n",
      "2021-07-01 09:48:04,252 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step     3400: bleu:   4.19, loss: 110637.7500, ppl:  29.7757, duration: 50.4252s\n",
      "2021-07-01 09:48:17,177 - INFO - joeynmt.training - Epoch  13: total training loss 887.51\n",
      "2021-07-01 09:48:17,177 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-01 09:48:17,999 - INFO - joeynmt.training - Epoch  14, Step:     3500, Batch Loss:     3.094635, Tokens per Sec:     5239, Lr: 0.000300\n",
      "2021-07-01 09:48:31,368 - INFO - joeynmt.training - Epoch  14, Step:     3600, Batch Loss:     3.386297, Tokens per Sec:     5399, Lr: 0.000300\n",
      "2021-07-01 09:49:20,041 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:49:20,042 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:49:20,042 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:49:20,455 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:49:20,456 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:49:21,280 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:49:21,281 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:49:21,281 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:49:21,281 - INFO - joeynmt.training - \tHypothesis: And when they had come to the Pharisees , He said to them , “ I am the Son of the Jews , and the other of the disciples , and I am not afraid , and I will be afraid . ”\n",
      "2021-07-01 09:49:21,282 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:49:21,282 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:49:21,283 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:49:21,283 - INFO - joeynmt.training - \tHypothesis: And when he had come to the disciples , he saw him , he said to him , “ I am the law of God , ”\n",
      "2021-07-01 09:49:21,283 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:49:21,284 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:49:21,284 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:49:21,284 - INFO - joeynmt.training - \tHypothesis: And he who had been given to the third day , and he was called to the prophet .\n",
      "2021-07-01 09:49:21,284 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:49:21,285 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:49:21,285 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:49:21,286 - INFO - joeynmt.training - \tHypothesis: And when he had come to the third day , he was a certain man named Jew , and he went out into the house of the house of the tomb .\n",
      "2021-07-01 09:49:21,286 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step     3600: bleu:   4.48, loss: 110054.6094, ppl:  29.2478, duration: 49.9173s\n",
      "2021-07-01 09:49:34,948 - INFO - joeynmt.training - Epoch  14, Step:     3700, Batch Loss:     2.950232, Tokens per Sec:     5246, Lr: 0.000300\n",
      "2021-07-01 09:49:43,343 - INFO - joeynmt.training - Epoch  14: total training loss 869.66\n",
      "2021-07-01 09:49:43,344 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-01 09:49:48,266 - INFO - joeynmt.training - Epoch  15, Step:     3800, Batch Loss:     3.536468, Tokens per Sec:     5272, Lr: 0.000300\n",
      "2021-07-01 09:50:45,590 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:50:45,591 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:50:45,591 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:50:45,993 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:50:45,993 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:50:46,835 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:50:46,836 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:50:46,837 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:50:46,837 - INFO - joeynmt.training - \tHypothesis: And they went out to the temple , and said to them , “ Go and go to the centurion , and go to the door of the door of the door , and the door of the door of the door . ”\n",
      "2021-07-01 09:50:46,837 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:50:46,838 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:50:46,838 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:50:46,838 - INFO - joeynmt.training - \tHypothesis: Then the Jews came to the synagogue , and said , “ I am the Jews , and the Jews , and the Jews who are of the things which is written , “ You are the things of the things which are of the things which are of the things which are of the things which are of the things . ”\n",
      "2021-07-01 09:50:46,839 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:50:46,840 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:50:46,840 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:50:46,841 - INFO - joeynmt.training - \tHypothesis: And he was a certain man named Jeeeus , who was a man .\n",
      "2021-07-01 09:50:46,841 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:50:46,842 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:50:46,842 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:50:46,842 - INFO - joeynmt.training - \tHypothesis: Then he went out to the synagogue , and went out to the synagogue , and went out of the sea , and went out of the sea , and the sea .\n",
      "2021-07-01 09:50:46,842 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step     3800: bleu:   4.29, loss: 108857.1562, ppl:  28.1930, duration: 58.5759s\n",
      "2021-07-01 09:51:00,364 - INFO - joeynmt.training - Epoch  15, Step:     3900, Batch Loss:     3.257005, Tokens per Sec:     5346, Lr: 0.000300\n",
      "2021-07-01 09:51:13,319 - INFO - joeynmt.training - Epoch  15, Step:     4000, Batch Loss:     3.153321, Tokens per Sec:     5481, Lr: 0.000300\n",
      "2021-07-01 09:52:07,022 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:52:07,022 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:52:07,023 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:52:07,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:52:07,428 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:52:08,724 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:52:08,726 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:52:08,726 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:52:08,726 - INFO - joeynmt.training - \tHypothesis: And the Pharisees came to the Pharisees , and the Pharisees came to him , saying , “ I am a little while while , and I will see you . ”\n",
      "2021-07-01 09:52:08,727 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:52:08,727 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:52:08,728 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:52:08,728 - INFO - joeynmt.training - \tHypothesis: And when He had come to the multitude , He said to them , “ I am the son of the son of the law , and you are the things which are in the things which you have spoken . ”\n",
      "2021-07-01 09:52:08,728 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:52:08,729 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:52:08,729 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:52:08,729 - INFO - joeynmt.training - \tHypothesis: And he was a certain man named Jeeus , and the Jews who had been given to the Jews .\n",
      "2021-07-01 09:52:08,729 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:52:08,730 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:52:08,730 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:52:08,731 - INFO - joeynmt.training - \tHypothesis: And when he had come to the multitude , he had come to the city , he went out of the city , and saw the city , and the city of the tomb .\n",
      "2021-07-01 09:52:08,731 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step     4000: bleu:   4.45, loss: 107955.1250, ppl:  27.4236, duration: 55.4115s\n",
      "2021-07-01 09:52:13,107 - INFO - joeynmt.training - Epoch  15: total training loss 856.99\n",
      "2021-07-01 09:52:13,108 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-01 09:52:22,328 - INFO - joeynmt.training - Epoch  16, Step:     4100, Batch Loss:     3.006569, Tokens per Sec:     5090, Lr: 0.000300\n",
      "2021-07-01 09:52:35,506 - INFO - joeynmt.training - Epoch  16, Step:     4200, Batch Loss:     3.252230, Tokens per Sec:     5557, Lr: 0.000300\n",
      "2021-07-01 09:53:31,115 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:53:31,115 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:53:31,116 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:53:31,518 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:53:31,518 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:53:32,336 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:53:32,337 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:53:32,338 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:53:32,339 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees came to the Pharisees , and said to them , “ The Pharisees of the Pharisees , and the ruler of the water , and the other will be afraid , and the other . ”\n",
      "2021-07-01 09:53:32,339 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:53:32,339 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:53:32,340 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:53:32,340 - INFO - joeynmt.training - \tHypothesis: Then the multitude of the Jews , when he had come to the city , he said to him , “ I am not afraid , but the things which is called in the Sabbath . ”\n",
      "2021-07-01 09:53:32,340 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:53:32,341 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:53:32,341 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:53:32,341 - INFO - joeynmt.training - \tHypothesis: And he was a certain man named Jew , and the Jews who was in the city .\n",
      "2021-07-01 09:53:32,342 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:53:32,342 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:53:32,342 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:53:32,343 - INFO - joeynmt.training - \tHypothesis: And when he had come to the city , he went out and saw the stay , and went out of the sea , and the sea , and the sea was filled .\n",
      "2021-07-01 09:53:32,343 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     4200: bleu:   4.59, loss: 106873.6719, ppl:  26.5289, duration: 56.8364s\n",
      "2021-07-01 09:53:45,793 - INFO - joeynmt.training - Epoch  16, Step:     4300, Batch Loss:     3.263824, Tokens per Sec:     5300, Lr: 0.000300\n",
      "2021-07-01 09:53:46,022 - INFO - joeynmt.training - Epoch  16: total training loss 835.90\n",
      "2021-07-01 09:53:46,022 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-01 09:53:58,696 - INFO - joeynmt.training - Epoch  17, Step:     4400, Batch Loss:     3.101264, Tokens per Sec:     5429, Lr: 0.000300\n",
      "2021-07-01 09:54:47,001 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:54:47,002 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:54:47,002 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:54:47,394 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:54:47,395 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:54:48,681 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:54:48,682 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:54:48,683 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:54:48,683 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees came to the Pharisees , and said to him , “ Go and go and go to the rock , and go to you , and go to you . ”\n",
      "2021-07-01 09:54:48,683 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:54:48,684 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:54:48,684 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:54:48,685 - INFO - joeynmt.training - \tHypothesis: Then he went out to the tomb , and said to him , “ Sir , I am not afraid , but have been afraid . ”\n",
      "2021-07-01 09:54:48,685 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:54:48,685 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:54:48,686 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:54:48,686 - INFO - joeynmt.training - \tHypothesis: And when he had come to the city of the Jews , he was in the city .\n",
      "2021-07-01 09:54:48,686 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:54:48,687 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:54:48,687 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:54:48,687 - INFO - joeynmt.training - \tHypothesis: And when he had come to him , he was a man named Prequicly . And he went out and went out to the tomb .\n",
      "2021-07-01 09:54:48,687 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step     4400: bleu:   5.13, loss: 106093.6172, ppl:  25.9016, duration: 49.9907s\n",
      "2021-07-01 09:55:02,508 - INFO - joeynmt.training - Epoch  17, Step:     4500, Batch Loss:     3.162512, Tokens per Sec:     5304, Lr: 0.000300\n",
      "2021-07-01 09:55:11,880 - INFO - joeynmt.training - Epoch  17: total training loss 817.95\n",
      "2021-07-01 09:55:11,880 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-01 09:55:15,940 - INFO - joeynmt.training - Epoch  18, Step:     4600, Batch Loss:     2.913657, Tokens per Sec:     5344, Lr: 0.000300\n",
      "2021-07-01 09:55:59,829 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:55:59,829 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:55:59,830 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:56:00,214 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:56:00,215 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:56:01,046 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:56:01,047 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:56:01,048 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:56:01,048 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees came to Him , and said to Him , “ Go and go into the ground , and go to the water , and go to you , and go to the door . ”\n",
      "2021-07-01 09:56:01,048 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:56:01,049 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:56:01,049 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:56:01,049 - INFO - joeynmt.training - \tHypothesis: Then he went out to the synagogue , and said to Him , “ Go and go to me , and do not know that you may be called . ”\n",
      "2021-07-01 09:56:01,050 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:56:01,050 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:56:01,050 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:56:01,051 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the city , he was called to the city of the Jews .\n",
      "2021-07-01 09:56:01,051 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:56:01,051 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:56:01,052 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:56:01,052 - INFO - joeynmt.training - \tHypothesis: And when he had come to him , he went out to the city , and went out to the city , and went out to the sea .\n",
      "2021-07-01 09:56:01,052 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     4600: bleu:   5.20, loss: 105540.1797, ppl:  25.4656, duration: 45.1120s\n",
      "2021-07-01 09:56:14,799 - INFO - joeynmt.training - Epoch  18, Step:     4700, Batch Loss:     2.826008, Tokens per Sec:     5172, Lr: 0.000300\n",
      "2021-07-01 09:56:28,122 - INFO - joeynmt.training - Epoch  18, Step:     4800, Batch Loss:     3.302501, Tokens per Sec:     5373, Lr: 0.000300\n",
      "2021-07-01 09:57:11,839 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:57:11,839 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:57:11,839 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:57:12,225 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:57:12,225 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:57:13,038 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:57:13,039 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:57:13,040 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:57:13,040 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees took him up and said to them , “ Go and go into the ground , and go to the door , and go to the door of the door , and to be afraid . ”\n",
      "2021-07-01 09:57:13,040 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:57:13,041 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:57:13,041 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:57:13,041 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the synagogue , he went out to the door , and said to him , “ I am the son of the Lord , and you have been made to be afraid . ”\n",
      "2021-07-01 09:57:13,042 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:57:13,042 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:57:13,043 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:57:13,043 - INFO - joeynmt.training - \tHypothesis: Now a certain man was a certain man named Peus ,\n",
      "2021-07-01 09:57:13,043 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:57:13,044 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:57:13,044 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:57:13,045 - INFO - joeynmt.training - \tHypothesis: Then he went out to the tomb , and went out of the city , and went out of the city . And he went out and went out of the tomb .\n",
      "2021-07-01 09:57:13,045 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     4800: bleu:   5.22, loss: 104733.0000, ppl:  24.8429, duration: 44.9228s\n",
      "2021-07-01 09:57:18,256 - INFO - joeynmt.training - Epoch  18: total training loss 809.80\n",
      "2021-07-01 09:57:18,257 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-01 09:57:26,714 - INFO - joeynmt.training - Epoch  19, Step:     4900, Batch Loss:     2.833150, Tokens per Sec:     5267, Lr: 0.000300\n",
      "2021-07-01 09:57:39,852 - INFO - joeynmt.training - Epoch  19, Step:     5000, Batch Loss:     3.078354, Tokens per Sec:     5456, Lr: 0.000300\n",
      "2021-07-01 09:58:26,556 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:58:26,556 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:58:26,556 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:58:26,972 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:58:26,983 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:58:27,839 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:58:27,841 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:58:27,841 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:58:27,841 - INFO - joeynmt.training - \tHypothesis: And the Pharisees saw it , and He said to them , “ Go and go and go into the house of the door , and go to him , and go to him , and to take it . ”\n",
      "2021-07-01 09:58:27,842 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:58:27,842 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:58:27,843 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:58:27,843 - INFO - joeynmt.training - \tHypothesis: Now when he had come , he saw that He had come to Peter , saying , “ Master , we have been done in your house , and have been done for us . ”\n",
      "2021-07-01 09:58:27,844 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:58:27,844 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:58:27,844 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:58:27,845 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the city , he was called to the Jews .\n",
      "2021-07-01 09:58:27,845 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:58:27,846 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:58:27,846 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:58:27,846 - INFO - joeynmt.training - \tHypothesis: And when he had come to the tomb , he was brought to him to him , and he went out of the city , and went out of the tomb .\n",
      "2021-07-01 09:58:27,846 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     5000: bleu:   5.54, loss: 104148.3672, ppl:  24.4013, duration: 47.9934s\n",
      "2021-07-01 09:58:41,538 - INFO - joeynmt.training - Epoch  19, Step:     5100, Batch Loss:     2.847332, Tokens per Sec:     5173, Lr: 0.000300\n",
      "2021-07-01 09:58:42,573 - INFO - joeynmt.training - Epoch  19: total training loss 788.55\n",
      "2021-07-01 09:58:42,573 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-01 09:58:54,704 - INFO - joeynmt.training - Epoch  20, Step:     5200, Batch Loss:     2.695895, Tokens per Sec:     5442, Lr: 0.000300\n",
      "2021-07-01 09:59:30,965 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 09:59:30,965 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 09:59:30,965 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 09:59:31,354 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 09:59:31,355 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 09:59:32,186 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 09:59:32,188 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 09:59:32,188 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 09:59:32,188 - INFO - joeynmt.training - \tHypothesis: And when the Pharisees had come , He said to them , “ Go , and you have compassion on the door , and you will see Me , and you will see Me . ”\n",
      "2021-07-01 09:59:32,188 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 09:59:32,189 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 09:59:32,189 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 09:59:32,189 - INFO - joeynmt.training - \tHypothesis: Now when he had come , he went out to the city , and said to him , “ You are the son of your father , and you are your way . ”\n",
      "2021-07-01 09:59:32,190 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 09:59:32,190 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 09:59:32,191 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 09:59:32,191 - INFO - joeynmt.training - \tHypothesis: Now it was a certain certain man named Joseph , who was a certain certain man named .\n",
      "2021-07-01 09:59:32,191 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 09:59:32,192 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 09:59:32,193 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 09:59:32,193 - INFO - joeynmt.training - \tHypothesis: And when he had come , he went out to the city , he went out to the city , and went out to the city , and went out of the city .\n",
      "2021-07-01 09:59:32,193 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     5200: bleu:   5.82, loss: 103974.9297, ppl:  24.2719, duration: 37.4881s\n",
      "2021-07-01 09:59:45,720 - INFO - joeynmt.training - Epoch  20, Step:     5300, Batch Loss:     2.848451, Tokens per Sec:     5280, Lr: 0.000300\n",
      "2021-07-01 09:59:55,924 - INFO - joeynmt.training - Epoch  20: total training loss 781.09\n",
      "2021-07-01 09:59:55,924 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-01 09:59:58,792 - INFO - joeynmt.training - Epoch  21, Step:     5400, Batch Loss:     2.843494, Tokens per Sec:     5336, Lr: 0.000300\n",
      "2021-07-01 10:00:39,014 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:00:39,014 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:00:39,014 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:00:39,425 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:00:39,426 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:00:40,700 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:00:40,701 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:00:40,701 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:00:40,702 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees came to the Pharisees , and when they had come out of the boat , He said to them , “ Go and go and tell you , and go to you . ”\n",
      "2021-07-01 10:00:40,702 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:00:40,703 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:00:40,703 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:00:40,703 - INFO - joeynmt.training - \tHypothesis: Then when he had come , he came to the house of Peter , and said to him , “ Go and tell you , and you are willing ; for you are willing . ”\n",
      "2021-07-01 10:00:40,703 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:00:40,704 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:00:40,704 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:00:40,705 - INFO - joeynmt.training - \tHypothesis: Now a certain man was a certain certain man named Jew , and the Jews .\n",
      "2021-07-01 10:00:40,705 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:00:40,706 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:00:40,706 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:00:40,706 - INFO - joeynmt.training - \tHypothesis: Then he went out to the tomb , and went out of the tomb , and went out of the tomb and went into the tomb .\n",
      "2021-07-01 10:00:40,706 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step     5400: bleu:   5.71, loss: 102869.1797, ppl:  23.4624, duration: 41.9140s\n",
      "2021-07-01 10:00:54,270 - INFO - joeynmt.training - Epoch  21, Step:     5500, Batch Loss:     2.759214, Tokens per Sec:     5301, Lr: 0.000300\n",
      "2021-07-01 10:01:07,387 - INFO - joeynmt.training - Epoch  21, Step:     5600, Batch Loss:     2.636731, Tokens per Sec:     5485, Lr: 0.000300\n",
      "2021-07-01 10:01:58,013 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:01:58,014 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:01:58,014 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:01:58,418 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:01:58,418 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:01:59,271 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:01:59,272 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:01:59,272 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:01:59,273 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw it , and when He had come to the disciples , He said to them , “ I am going to you , and go and go to the door of the door of the door . ”\n",
      "2021-07-01 10:01:59,273 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:01:59,274 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:01:59,274 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:01:59,274 - INFO - joeynmt.training - \tHypothesis: And when He had come to the house , He came to the house of Peter , and said to him , “ Go and go to your way , and you are willing . ”\n",
      "2021-07-01 10:01:59,274 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:01:59,275 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:01:59,275 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:01:59,275 - INFO - joeynmt.training - \tHypothesis: Now a certain man was a certain man named Jew , and was a great great .\n",
      "2021-07-01 10:01:59,276 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:01:59,276 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:01:59,277 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:01:59,277 - INFO - joeynmt.training - \tHypothesis: And when he had come to the city , he was taken away , and he was brought to him to the tomb . And he went out and went out of the tomb .\n",
      "2021-07-01 10:01:59,277 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step     5600: bleu:   5.71, loss: 102226.4922, ppl:  23.0044, duration: 51.8898s\n",
      "2021-07-01 10:02:05,501 - INFO - joeynmt.training - Epoch  21: total training loss 765.31\n",
      "2021-07-01 10:02:05,501 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-01 10:02:13,028 - INFO - joeynmt.training - Epoch  22, Step:     5700, Batch Loss:     2.764832, Tokens per Sec:     5067, Lr: 0.000300\n",
      "2021-07-01 10:02:26,316 - INFO - joeynmt.training - Epoch  22, Step:     5800, Batch Loss:     3.057881, Tokens per Sec:     5333, Lr: 0.000300\n",
      "2021-07-01 10:03:08,688 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:03:08,689 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:03:08,689 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:03:09,119 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:03:09,120 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:03:09,964 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:03:09,965 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:03:09,965 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:03:09,966 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees came to Him , and said to him , “ I am going to see the water , and go and go into the house of the water . ”\n",
      "2021-07-01 10:03:09,966 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:03:09,967 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:03:09,967 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:03:09,967 - INFO - joeynmt.training - \tHypothesis: Then the next day , when he had come , he came to Peter , and said to him , “ Go and go into your house , and go to your way . ”\n",
      "2021-07-01 10:03:09,967 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:03:09,968 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:03:09,968 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:03:09,968 - INFO - joeynmt.training - \tHypothesis: Now a certain man was a certain man named Jew , and was near Jerusalem .\n",
      "2021-07-01 10:03:09,969 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:03:09,969 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:03:09,969 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:03:09,970 - INFO - joeynmt.training - \tHypothesis: Then he went out to the city , and was troubled by the root of the swine . And he went out and went into the sea .\n",
      "2021-07-01 10:03:09,970 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step     5800: bleu:   5.85, loss: 101839.5625, ppl:  22.7330, duration: 43.6534s\n",
      "2021-07-01 10:03:24,214 - INFO - joeynmt.training - Epoch  22, Step:     5900, Batch Loss:     2.980526, Tokens per Sec:     4975, Lr: 0.000300\n",
      "2021-07-01 10:03:26,482 - INFO - joeynmt.training - Epoch  22: total training loss 756.52\n",
      "2021-07-01 10:03:26,482 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-01 10:03:37,504 - INFO - joeynmt.training - Epoch  23, Step:     6000, Batch Loss:     2.567569, Tokens per Sec:     5394, Lr: 0.000300\n",
      "2021-07-01 10:04:25,040 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:04:25,040 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:04:25,041 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:04:25,455 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:04:25,455 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:04:26,296 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:04:26,297 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:04:26,298 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:04:26,299 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees came to Him , and He was going up to the boat , and said to them , “ Go out of the vineyard , and go to the vineyard , and take it to Me . ”\n",
      "2021-07-01 10:04:26,299 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:04:26,299 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:04:26,300 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:04:26,300 - INFO - joeynmt.training - \tHypothesis: And when he had come to the house , he went out to the house of Peter , and said to him , “ Rabbi , we have compassion on your way . ”\n",
      "2021-07-01 10:04:26,300 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:04:26,301 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:04:26,301 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:04:26,301 - INFO - joeynmt.training - \tHypothesis: Now there was a certain man named Jew , and there was a great day .\n",
      "2021-07-01 10:04:26,302 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:04:26,302 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:04:26,302 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:04:26,303 - INFO - joeynmt.training - \tHypothesis: And when he had come to the ship , he went out to the tomb , and found it on the sea , and went out to the tomb . And he went out of the tomb .\n",
      "2021-07-01 10:04:26,303 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step     6000: bleu:   5.87, loss: 101548.7500, ppl:  22.5311, duration: 48.7988s\n",
      "2021-07-01 10:04:40,123 - INFO - joeynmt.training - Epoch  23, Step:     6100, Batch Loss:     2.573166, Tokens per Sec:     5163, Lr: 0.000300\n",
      "2021-07-01 10:04:51,559 - INFO - joeynmt.training - Epoch  23: total training loss 741.81\n",
      "2021-07-01 10:04:51,559 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-01 10:04:53,416 - INFO - joeynmt.training - Epoch  24, Step:     6200, Batch Loss:     2.351323, Tokens per Sec:     5169, Lr: 0.000300\n",
      "2021-07-01 10:05:33,557 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:05:33,557 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:05:33,558 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:05:33,975 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:05:33,975 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:05:34,825 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:05:34,826 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:05:34,826 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:05:34,826 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood up , and when He had said to them , “ I have received the door , and you will see Me , and you will see Me . ”\n",
      "2021-07-01 10:05:34,827 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:05:34,827 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:05:34,828 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:05:34,828 - INFO - joeynmt.training - \tHypothesis: And when he had come , he went out into the house of Peter , and said to him , “ Rabbi , we have compassion on your way . ”\n",
      "2021-07-01 10:05:34,828 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:05:34,829 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:05:34,829 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:05:34,829 - INFO - joeynmt.training - \tHypothesis: And he was a certain man named Perus , who was near the day of the Passover .\n",
      "2021-07-01 10:05:34,829 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:05:34,830 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:05:34,830 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:05:34,831 - INFO - joeynmt.training - \tHypothesis: And when he had come , he was brought to him into the midst of the city , and he went out into the finging .\n",
      "2021-07-01 10:05:34,831 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step     6200: bleu:   6.47, loss: 100851.5000, ppl:  22.0543, duration: 41.4146s\n",
      "2021-07-01 10:05:48,591 - INFO - joeynmt.training - Epoch  24, Step:     6300, Batch Loss:     2.244322, Tokens per Sec:     5232, Lr: 0.000300\n",
      "2021-07-01 10:06:01,935 - INFO - joeynmt.training - Epoch  24, Step:     6400, Batch Loss:     2.978971, Tokens per Sec:     5394, Lr: 0.000300\n",
      "2021-07-01 10:06:42,633 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:06:42,633 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:06:42,634 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:06:43,059 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:06:43,059 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:06:43,898 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:06:43,900 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:06:43,900 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:06:43,900 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw the Pharisees , and when He had come out of the boat , He said to them , “ Go into the midst of the cut of the water of the cut of the door . ”\n",
      "2021-07-01 10:06:43,901 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:06:43,901 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:06:43,901 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:06:43,902 - INFO - joeynmt.training - \tHypothesis: And when He had come to the city , He came to the hand of Peter , and said to him , “ Go and go to your way . ”\n",
      "2021-07-01 10:06:43,902 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:06:43,903 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:06:43,903 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:06:43,903 - INFO - joeynmt.training - \tHypothesis: Now a certain man was there was a great day . And the first was near .\n",
      "2021-07-01 10:06:43,903 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:06:43,904 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:06:43,904 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:06:43,904 - INFO - joeynmt.training - \tHypothesis: And when he had come to the city , he was met , and the stay was like a swine was thrown into the sea . And he went out of the tomb .\n",
      "2021-07-01 10:06:43,905 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step     6400: bleu:   6.25, loss: 100515.0547, ppl:  21.8279, duration: 41.9685s\n",
      "2021-07-01 10:06:51,111 - INFO - joeynmt.training - Epoch  24: total training loss 727.43\n",
      "2021-07-01 10:06:51,112 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-07-01 10:06:57,704 - INFO - joeynmt.training - Epoch  25, Step:     6500, Batch Loss:     2.559196, Tokens per Sec:     4913, Lr: 0.000300\n",
      "2021-07-01 10:07:11,108 - INFO - joeynmt.training - Epoch  25, Step:     6600, Batch Loss:     2.711006, Tokens per Sec:     5410, Lr: 0.000300\n",
      "2021-07-01 10:07:51,248 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:07:51,249 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:07:51,249 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:07:51,670 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:07:51,670 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:07:52,526 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:07:52,528 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:07:52,528 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:07:52,528 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw it , and when the Pharisees saw it , he said to them , “ Go and go out to the cut of the cut of the door , and go to the door . ”\n",
      "2021-07-01 10:07:52,528 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:07:52,529 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:07:52,529 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:07:52,530 - INFO - joeynmt.training - \tHypothesis: Now when He had come to the house , He went out to the hand of the girl . And He said to her , “ Rabbi ! ”\n",
      "2021-07-01 10:07:52,530 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:07:52,531 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:07:52,531 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:07:52,531 - INFO - joeynmt.training - \tHypothesis: Now there was a certain man named Pernacle , the day of the Sabbath .\n",
      "2021-07-01 10:07:52,532 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:07:52,532 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:07:52,533 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:07:52,533 - INFO - joeynmt.training - \tHypothesis: And when he had come to the stay , he went out to the stay , he went out and found a great city . And he went out to the tomb .\n",
      "2021-07-01 10:07:52,533 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step     6600: bleu:   6.39, loss: 100220.9844, ppl:  21.6319, duration: 41.4243s\n",
      "2021-07-01 10:08:06,318 - INFO - joeynmt.training - Epoch  25, Step:     6700, Batch Loss:     2.753752, Tokens per Sec:     5250, Lr: 0.000300\n",
      "2021-07-01 10:08:09,197 - INFO - joeynmt.training - Epoch  25: total training loss 716.77\n",
      "2021-07-01 10:08:09,197 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-07-01 10:08:19,567 - INFO - joeynmt.training - Epoch  26, Step:     6800, Batch Loss:     2.476420, Tokens per Sec:     5356, Lr: 0.000300\n",
      "2021-07-01 10:09:01,628 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:09:01,628 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:09:01,629 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:09:02,817 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:09:02,818 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:09:02,818 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:09:02,819 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw it , and when He had come to the disciples , He said to them , “ I am going out of the rooster of the water of the water . ”\n",
      "2021-07-01 10:09:02,819 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:09:02,819 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:09:02,820 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:09:02,820 - INFO - joeynmt.training - \tHypothesis: Now when He had come to the house , He came to Peter , and said to him , “ Go , go your way , and see your way . ”\n",
      "2021-07-01 10:09:02,820 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:09:02,821 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:09:02,821 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:09:02,821 - INFO - joeynmt.training - \tHypothesis: Now it was a certain man named Perus , that He was near the Sabbath .\n",
      "2021-07-01 10:09:02,822 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:09:02,822 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:09:02,824 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:09:02,824 - INFO - joeynmt.training - \tHypothesis: So when he had come to the stay , he went out to the stay , and went out of the stay .\n",
      "2021-07-01 10:09:02,824 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step     6800: bleu:   6.49, loss: 100360.6875, ppl:  21.7248, duration: 43.2568s\n",
      "2021-07-01 10:09:17,049 - INFO - joeynmt.training - Epoch  26, Step:     6900, Batch Loss:     2.842410, Tokens per Sec:     5065, Lr: 0.000300\n",
      "2021-07-01 10:09:29,103 - INFO - joeynmt.training - Epoch  26: total training loss 704.64\n",
      "2021-07-01 10:09:29,104 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-07-01 10:09:30,470 - INFO - joeynmt.training - Epoch  27, Step:     7000, Batch Loss:     2.320842, Tokens per Sec:     5315, Lr: 0.000300\n",
      "2021-07-01 10:10:10,256 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:10:10,257 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:10:10,257 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:10:10,675 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:10:10,675 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:10:11,623 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:10:11,626 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:10:11,626 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:10:11,626 - INFO - joeynmt.training - \tHypothesis: And the Pharisees saw it , and when the Pharisees had come , He said to them , “ Go out into the house of the cut down and drink , and I will see you will see Me . ”\n",
      "2021-07-01 10:10:11,626 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:10:11,627 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:10:11,627 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:10:11,628 - INFO - joeynmt.training - \tHypothesis: Now when He had come to the house , He went out to the house of Mary , and said to him , “ Go , go your way , and see your way . ”\n",
      "2021-07-01 10:10:11,628 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:10:11,629 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:10:11,629 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:10:11,629 - INFO - joeynmt.training - \tHypothesis: Now a certain man was in the day of the Sabbath .\n",
      "2021-07-01 10:10:11,629 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:10:11,630 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:10:11,630 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:10:11,631 - INFO - joeynmt.training - \tHypothesis: And when he had come , he was fled , and was a great millstone was thrown out into the sea . And he went out and went out into the city .\n",
      "2021-07-01 10:10:11,631 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step     7000: bleu:   6.48, loss: 99407.5234, ppl:  21.0988, duration: 41.1602s\n",
      "2021-07-01 10:10:25,330 - INFO - joeynmt.training - Epoch  27, Step:     7100, Batch Loss:     2.555432, Tokens per Sec:     5154, Lr: 0.000300\n",
      "2021-07-01 10:10:38,649 - INFO - joeynmt.training - Epoch  27, Step:     7200, Batch Loss:     2.793516, Tokens per Sec:     5399, Lr: 0.000300\n",
      "2021-07-01 10:11:18,391 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:11:18,392 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:11:18,392 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:11:19,606 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:11:19,607 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:11:19,607 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:11:19,608 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw it , and when he had come to the disciples , he said to them , “ I will see the rock of the cut of the vineyard , and you will see Me . ”\n",
      "2021-07-01 10:11:19,608 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:11:19,609 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:11:19,609 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:11:19,609 - INFO - joeynmt.training - \tHypothesis: Now when He had come to Bethany , He came to the house of Peter , and said to him , “ Sir , go your way , and let us go your way . ”\n",
      "2021-07-01 10:11:19,609 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:11:19,610 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:11:19,610 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:11:19,611 - INFO - joeynmt.training - \tHypothesis: Now there was a certain man named Perus , who was near the day .\n",
      "2021-07-01 10:11:19,611 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:11:19,611 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:11:19,612 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:11:19,612 - INFO - joeynmt.training - \tHypothesis: And he went out to the tomb , and was filled with a great millage ; and he was thrown into the sea .\n",
      "2021-07-01 10:11:19,612 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step     7200: bleu:   6.78, loss: 99432.1641, ppl:  21.1148, duration: 40.9621s\n",
      "2021-07-01 10:11:27,429 - INFO - joeynmt.training - Epoch  27: total training loss 694.91\n",
      "2021-07-01 10:11:27,429 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-07-01 10:11:33,469 - INFO - joeynmt.training - Epoch  28, Step:     7300, Batch Loss:     2.257709, Tokens per Sec:     5081, Lr: 0.000300\n",
      "2021-07-01 10:11:46,810 - INFO - joeynmt.training - Epoch  28, Step:     7400, Batch Loss:     2.238329, Tokens per Sec:     5370, Lr: 0.000300\n",
      "2021-07-01 10:12:34,397 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:12:34,398 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:12:34,398 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:12:34,807 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:12:34,807 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:12:36,083 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:12:36,084 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:12:36,084 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:12:36,084 - INFO - joeynmt.training - \tHypothesis: And the Pharisees stood by the Pharisees , and when she had come , He said to them , “ Go , go and go into the house of the house , and see I will see you . ”\n",
      "2021-07-01 10:12:36,085 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:12:36,085 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:12:36,086 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:12:36,086 - INFO - joeynmt.training - \tHypothesis: Now when He had come , He went out to the house of Peter , and said to him , “ Go , go your way , and take you , and follow us . ”\n",
      "2021-07-01 10:12:36,086 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:12:36,087 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:12:36,087 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:12:36,087 - INFO - joeynmt.training - \tHypothesis: Now there was a certain day , a great time , that was near the day .\n",
      "2021-07-01 10:12:36,087 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:12:36,088 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:12:36,088 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:12:36,089 - INFO - joeynmt.training - \tHypothesis: And he went out to the tomb , and was a great millage ; and he was a great swine , and a great great lake , and his bolden .\n",
      "2021-07-01 10:12:36,089 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step     7400: bleu:   6.61, loss: 99112.0547, ppl:  20.9085, duration: 49.2790s\n",
      "2021-07-01 10:12:49,863 - INFO - joeynmt.training - Epoch  28, Step:     7500, Batch Loss:     2.564326, Tokens per Sec:     5247, Lr: 0.000300\n",
      "2021-07-01 10:12:53,254 - INFO - joeynmt.training - Epoch  28: total training loss 685.01\n",
      "2021-07-01 10:12:53,254 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-07-01 10:13:03,058 - INFO - joeynmt.training - Epoch  29, Step:     7600, Batch Loss:     2.386885, Tokens per Sec:     5378, Lr: 0.000300\n",
      "2021-07-01 10:13:44,740 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:13:44,741 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:13:44,741 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:13:45,966 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:13:45,967 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:13:45,967 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:13:45,967 - INFO - joeynmt.training - \tHypothesis: And the Pharisees stood up , and saw it , and said to them , “ I am coming to you , and behold , I will see you the cut of the cup of the cut of the door . ”\n",
      "2021-07-01 10:13:45,967 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:13:45,968 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:13:45,969 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:13:45,969 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the house , he went out to the hand of Peter , and said to him , “ Rabbi , we have compassion on your way . ”\n",
      "2021-07-01 10:13:45,969 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:13:45,970 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:13:45,970 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:13:45,970 - INFO - joeynmt.training - \tHypothesis: Now a man was about to the feast , that he was near the feast .\n",
      "2021-07-01 10:13:45,971 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:13:45,971 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:13:45,972 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:13:45,972 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a great multitude . And when he had taken a great multitude , he was cast out of the rock and swine .\n",
      "2021-07-01 10:13:45,973 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step     7600: bleu:   6.75, loss: 99203.9219, ppl:  20.9675, duration: 42.9142s\n",
      "2021-07-01 10:13:59,792 - INFO - joeynmt.training - Epoch  29, Step:     7700, Batch Loss:     2.583104, Tokens per Sec:     5260, Lr: 0.000300\n",
      "2021-07-01 10:14:12,315 - INFO - joeynmt.training - Epoch  29: total training loss 678.21\n",
      "2021-07-01 10:14:12,316 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-07-01 10:14:13,029 - INFO - joeynmt.training - Epoch  30, Step:     7800, Batch Loss:     2.539336, Tokens per Sec:     5506, Lr: 0.000300\n",
      "2021-07-01 10:14:56,087 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:14:56,088 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:14:56,088 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:14:56,478 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:14:56,478 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:14:57,342 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:14:57,343 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:14:57,343 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:14:57,343 - INFO - joeynmt.training - \tHypothesis: And the Pharisees saw it , and he was still still still still still still still . And He said to them , “ I have compassion on you , and go down to see my feet . ”\n",
      "2021-07-01 10:14:57,344 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:14:57,344 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:14:57,344 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:14:57,345 - INFO - joeynmt.training - \tHypothesis: And when he had come to the house , he came to the girl , and said to him , “ Sir , go your way . ”\n",
      "2021-07-01 10:14:57,345 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:14:57,346 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:14:57,346 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:14:57,346 - INFO - joeynmt.training - \tHypothesis: And he was a great time , that he was in the day of the Sabbath .\n",
      "2021-07-01 10:14:57,346 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:14:57,347 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:14:57,347 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:14:57,347 - INFO - joeynmt.training - \tHypothesis: And he was hungry , and he was a great millage . And he was thrown down into the sea . And he was thrown into the fire .\n",
      "2021-07-01 10:14:57,348 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step     7800: bleu:   7.08, loss: 98835.9688, ppl:  20.7321, duration: 44.3185s\n",
      "2021-07-01 10:15:11,091 - INFO - joeynmt.training - Epoch  30, Step:     7900, Batch Loss:     2.265331, Tokens per Sec:     5139, Lr: 0.000300\n",
      "2021-07-01 10:15:24,254 - INFO - joeynmt.training - Epoch  30, Step:     8000, Batch Loss:     2.324389, Tokens per Sec:     5451, Lr: 0.000300\n",
      "2021-07-01 10:16:05,370 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:16:05,371 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:16:05,371 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:16:05,768 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-01 10:16:05,769 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-01 10:16:06,585 - INFO - joeynmt.training - Example #0\n",
      "2021-07-01 10:16:06,587 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-01 10:16:06,587 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-01 10:16:06,587 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw it , and when he had said to them , “ I have been afraid , and behold , I will see you up the door of the water . ”\n",
      "2021-07-01 10:16:06,588 - INFO - joeynmt.training - Example #1\n",
      "2021-07-01 10:16:06,588 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-01 10:16:06,589 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-01 10:16:06,589 - INFO - joeynmt.training - \tHypothesis: Now when He had come to him , He came to the girl , and said to him , “ Rabbi , you are willing ; and you are willing . ”\n",
      "2021-07-01 10:16:06,589 - INFO - joeynmt.training - Example #2\n",
      "2021-07-01 10:16:06,590 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-01 10:16:06,590 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-01 10:16:06,590 - INFO - joeynmt.training - \tHypothesis: Now a certain man was about to the Sabbath , and was near the third day .\n",
      "2021-07-01 10:16:06,590 - INFO - joeynmt.training - Example #3\n",
      "2021-07-01 10:16:06,591 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-01 10:16:06,591 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-01 10:16:06,592 - INFO - joeynmt.training - \tHypothesis: And he went out and found it , and was a great possessed . And he went out and sat down and sat down and sat down and sat down .\n",
      "2021-07-01 10:16:06,592 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step     8000: bleu:   7.09, loss: 98705.7812, ppl:  20.6495, duration: 42.3378s\n",
      "2021-07-01 10:16:15,240 - INFO - joeynmt.training - Epoch  30: total training loss 669.87\n",
      "2021-07-01 10:16:15,240 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-07-01 10:16:15,240 - INFO - joeynmt.training - Best validation result (greedy) at step     8000:  20.65 ppl.\n",
      "2021-07-01 10:16:15,284 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-07-01 10:16:15,814 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-01 10:16:16,110 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-01 10:16:16,184 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.en)...\n",
      "2021-07-01 10:17:16,510 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:17:16,510 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:17:16,511 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:17:16,889 - INFO - joeynmt.prediction -  dev bleu[13a]:   7.58 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-01 10:17:16,895 - INFO - joeynmt.prediction - Translations saved to: models/lhen_reverse_transformer/00008000.hyps.dev\n",
      "2021-07-01 10:17:16,896 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.en)...\n",
      "2021-07-01 10:18:18,321 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:18:18,322 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:18:18,322 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:18:18,710 - INFO - joeynmt.prediction - test bleu[13a]:   7.45 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-01 10:18:18,716 - INFO - joeynmt.prediction - Translations saved to: models/lhen_reverse_transformer/00008000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt3$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKD3TP1cevE4",
    "outputId": "15b42bae-e78e-48d0-d327-e802f24eb950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 200\tLoss: 165704.70312\tPPL: 161.22653\tbleu: 0.00716\tLR: 0.00030000\t*\n",
      "Steps: 400\tLoss: 154830.34375\tPPL: 115.49778\tbleu: 0.00821\tLR: 0.00030000\t*\n",
      "Steps: 600\tLoss: 142989.31250\tPPL: 80.32177\tbleu: 0.13588\tLR: 0.00030000\t*\n",
      "Steps: 800\tLoss: 137030.95312\tPPL: 66.90507\tbleu: 0.69859\tLR: 0.00030000\t*\n",
      "Steps: 1000\tLoss: 133380.62500\tPPL: 59.81789\tbleu: 0.93283\tLR: 0.00030000\t*\n",
      "Steps: 1200\tLoss: 129902.87500\tPPL: 53.76530\tbleu: 1.87698\tLR: 0.00030000\t*\n",
      "Steps: 1400\tLoss: 127497.91406\tPPL: 49.94184\tbleu: 1.68085\tLR: 0.00030000\t*\n",
      "Steps: 1600\tLoss: 124984.14062\tPPL: 46.23567\tbleu: 2.37874\tLR: 0.00030000\t*\n",
      "Steps: 1800\tLoss: 123692.53125\tPPL: 44.43969\tbleu: 2.47989\tLR: 0.00030000\t*\n",
      "Steps: 2000\tLoss: 121792.11719\tPPL: 41.92322\tbleu: 2.44680\tLR: 0.00030000\t*\n",
      "Steps: 2200\tLoss: 119842.67969\tPPL: 39.48982\tbleu: 2.57330\tLR: 0.00030000\t*\n",
      "Steps: 2400\tLoss: 117787.98438\tPPL: 37.07777\tbleu: 2.65778\tLR: 0.00030000\t*\n",
      "Steps: 2600\tLoss: 116235.19531\tPPL: 35.35315\tbleu: 3.17998\tLR: 0.00030000\t*\n",
      "Steps: 2800\tLoss: 114767.92969\tPPL: 33.79729\tbleu: 3.56650\tLR: 0.00030000\t*\n",
      "Steps: 3000\tLoss: 114136.14844\tPPL: 33.14863\tbleu: 3.52482\tLR: 0.00030000\t*\n",
      "Steps: 3200\tLoss: 112001.75000\tPPL: 31.04790\tbleu: 3.78960\tLR: 0.00030000\t*\n",
      "Steps: 3400\tLoss: 110637.75000\tPPL: 29.77568\tbleu: 4.18682\tLR: 0.00030000\t*\n",
      "Steps: 3600\tLoss: 110054.60938\tPPL: 29.24781\tbleu: 4.48382\tLR: 0.00030000\t*\n",
      "Steps: 3800\tLoss: 108857.15625\tPPL: 28.19301\tbleu: 4.28894\tLR: 0.00030000\t*\n",
      "Steps: 4000\tLoss: 107955.12500\tPPL: 27.42364\tbleu: 4.45314\tLR: 0.00030000\t*\n",
      "Steps: 4200\tLoss: 106873.67188\tPPL: 26.52886\tbleu: 4.58587\tLR: 0.00030000\t*\n",
      "Steps: 4400\tLoss: 106093.61719\tPPL: 25.90162\tbleu: 5.12688\tLR: 0.00030000\t*\n",
      "Steps: 4600\tLoss: 105540.17969\tPPL: 25.46563\tbleu: 5.19654\tLR: 0.00030000\t*\n",
      "Steps: 4800\tLoss: 104733.00000\tPPL: 24.84286\tbleu: 5.22400\tLR: 0.00030000\t*\n",
      "Steps: 5000\tLoss: 104148.36719\tPPL: 24.40132\tbleu: 5.54413\tLR: 0.00030000\t*\n",
      "Steps: 5200\tLoss: 103974.92969\tPPL: 24.27185\tbleu: 5.81875\tLR: 0.00030000\t*\n",
      "Steps: 5400\tLoss: 102869.17969\tPPL: 23.46241\tbleu: 5.70557\tLR: 0.00030000\t*\n",
      "Steps: 5600\tLoss: 102226.49219\tPPL: 23.00441\tbleu: 5.71134\tLR: 0.00030000\t*\n",
      "Steps: 5800\tLoss: 101839.56250\tPPL: 22.73299\tbleu: 5.84991\tLR: 0.00030000\t*\n",
      "Steps: 6000\tLoss: 101548.75000\tPPL: 22.53111\tbleu: 5.87056\tLR: 0.00030000\t*\n",
      "Steps: 6200\tLoss: 100851.50000\tPPL: 22.05434\tbleu: 6.47163\tLR: 0.00030000\t*\n",
      "Steps: 6400\tLoss: 100515.05469\tPPL: 21.82791\tbleu: 6.24815\tLR: 0.00030000\t*\n",
      "Steps: 6600\tLoss: 100220.98438\tPPL: 21.63190\tbleu: 6.39357\tLR: 0.00030000\t*\n",
      "Steps: 6800\tLoss: 100360.68750\tPPL: 21.72479\tbleu: 6.49462\tLR: 0.00030000\t\n",
      "Steps: 7000\tLoss: 99407.52344\tPPL: 21.09882\tbleu: 6.47686\tLR: 0.00030000\t*\n",
      "Steps: 7200\tLoss: 99432.16406\tPPL: 21.11477\tbleu: 6.78016\tLR: 0.00030000\t\n",
      "Steps: 7400\tLoss: 99112.05469\tPPL: 20.90846\tbleu: 6.61030\tLR: 0.00030000\t*\n",
      "Steps: 7600\tLoss: 99203.92188\tPPL: 20.96746\tbleu: 6.74511\tLR: 0.00030000\t\n",
      "Steps: 7800\tLoss: 98835.96875\tPPL: 20.73214\tbleu: 7.07651\tLR: 0.00030000\t*\n",
      "Steps: 8000\tLoss: 98705.78125\tPPL: 20.64952\tbleu: 7.09462\tLR: 0.00030000\t*\n"
     ]
    }
   ],
   "source": [
    "# Output our validation accuracy\n",
    "! cat \"joeynmt/models/lhen_reverse_transformer/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVLbG8ITokd7",
    "outputId": "6e0343a9-a281-4630-e0dd-f96aaed585b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 10:35:11,527 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-01 10:35:11,534 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-01 10:35:11,800 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-01 10:35:11,818 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-01 10:35:11,833 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-01 10:35:11,860 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-07-01 10:35:14,580 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-01 10:35:14,837 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-01 10:35:14,914 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.en)...\n",
      "2021-07-01 10:36:15,943 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:36:15,943 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:36:15,943 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:36:16,330 - INFO - joeynmt.prediction -  dev bleu[13a]:   7.58 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-01 10:36:16,331 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.en)...\n",
      "2021-07-01 10:37:18,556 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-01 10:37:18,556 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-01 10:37:18,556 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-01 10:37:18,940 - INFO - joeynmt.prediction - test bleu[13a]:   7.45 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt test 'models/lhen_reverse_transformer/config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "romEqJUSjqGa"
   },
   "outputs": [],
   "source": [
    "!python3 joeynmt/scripts/plot_validations.py joeynmt/models/lhen_reverse_transformer --plot_values bleu PPL  --output_path joeynmt/models/lhen_reverse_transformer/bleu-ppl.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK8JT2qqMK-h"
   },
   "source": [
    "![blue](https://drive.google.com/uc?id=1M0r8iSUYCyasJNO7yClAbvq_0hDencW-)![blue2](https://drive.google.com/uc?id=15Wbe6_ThVra_wSkdsELv9YuDyX_1axoh)\n",
    "\n",
    "\n",
    "https://drive.google.com/file/d/1M0r8iSUYCyasJNO7yClAbvq_0hDencW-/view?usp=sharing\n",
    "\n",
    "https://drive.google.com/file/d/15Wbe6_ThVra_wSkdsELv9YuDyX_1axoh/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spYBzAmDEDlO"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 8000\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/models/lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/{name}_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/lhen_reverse_transformer\"', f'model_dir: \"models/lhen_reverse_transformer_continued\"')\n",
    "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "W0rEiN4PyzXo",
    "outputId": "db0759ca-d59d-434e-ec61-4f1982ca6d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/lhen_reverse_transformer/8000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 1096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 200         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/lhen_reverse_transformer_continued\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_lhen_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tM-9uxZFigO",
    "outputId": "254f0fed-973c-4251-e9ed-0c1f33d5165e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-02 07:21:54,842 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-02 07:21:54,907 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-02 07:21:55,841 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-02 07:21:56,514 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-02 07:21:57,243 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-02 07:21:58,205 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-02 07:21:58,205 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-02 07:21:58,425 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-02 07:21:58.674452: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-02 07:22:00,502 - INFO - joeynmt.training - Total params: 12097024\n",
      "2021-07-02 07:22:03,979 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/lhen_reverse_transformer/8000.ckpt\n",
      "2021-07-02 07:22:04,419 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-02 07:22:04,419 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-02 07:22:04,419 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-02 07:22:04,419 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\n",
      "2021-07-02 07:22:04,420 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\n",
      "2021-07-02 07:22:04,420 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\n",
      "2021-07-02 07:22:04,420 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-02 07:22:04,420 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-02 07:22:04,420 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-02 07:22:04,420 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-02 07:22:04,420 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-02 07:22:04,421 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-02 07:22:04,421 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-02 07:22:04,421 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/lhen_reverse_transformer/8000.ckpt\n",
      "2021-07-02 07:22:04,421 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-02 07:22:04,421 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-02 07:22:04,421 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-02 07:22:04,421 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-02 07:22:04,422 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-02 07:22:04,422 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-02 07:22:04,422 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-02 07:22:04,422 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-02 07:22:04,422 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-02 07:22:04,422 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-02 07:22:04,422 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-02 07:22:04,423 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-02 07:22:04,423 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-02 07:22:04,423 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-02 07:22:04,423 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1096\n",
      "2021-07-02 07:22:04,423 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-02 07:22:04,423 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-02 07:22:04,423 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-02 07:22:04,424 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-02 07:22:04,424 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-02 07:22:04,424 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-02 07:22:04,424 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 200\n",
      "2021-07-02 07:22:04,424 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-02 07:22:04,424 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-02 07:22:04,424 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lhen_reverse_transformer_continued\n",
      "2021-07-02 07:22:04,424 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-02 07:22:04,425 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-02 07:22:04,425 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-02 07:22:04,425 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-02 07:22:04,425 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-02 07:22:04,425 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-02 07:22:04,425 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-02 07:22:04,425 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-02 07:22:04,425 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-02 07:22:04,426 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-02 07:22:04,426 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-02 07:22:04,426 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-02 07:22:04,426 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-02 07:22:04,426 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-02 07:22:04,426 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-02 07:22:04,426 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-02 07:22:04,427 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-02 07:22:04,427 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-02 07:22:04,427 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-02 07:22:04,427 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-02 07:22:04,427 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-02 07:22:04,427 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-02 07:22:04,427 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-02 07:22:04,427 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-02 07:22:04,428 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-02 07:22:04,428 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-02 07:22:04,428 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-02 07:22:04,428 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-02 07:22:04,428 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-02 07:22:04,428 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-02 07:22:04,428 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-02 07:22:04,428 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 5904,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-02 07:22:04,429 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-02 07:22:04,429 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-02 07:22:04,429 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-02 07:22:04,429 - INFO - joeynmt.helpers - Number of Src words (types): 4050\n",
      "2021-07-02 07:22:04,430 - INFO - joeynmt.helpers - Number of Trg words (types): 4050\n",
      "2021-07-02 07:22:04,430 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4050),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4050))\n",
      "2021-07-02 07:22:04,439 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 1096\n",
      "\ttotal batch size (w. parallel & accumulation): 1096\n",
      "2021-07-02 07:22:04,439 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-02 07:22:08,316 - INFO - joeynmt.training - Epoch   1: total training loss 164.06\n",
      "2021-07-02 07:22:08,316 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-02 07:22:10,359 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     2.635019, Tokens per Sec:    12355, Lr: 0.000300\n",
      "2021-07-02 07:22:16,165 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     2.415788, Tokens per Sec:    12060, Lr: 0.000300\n",
      "2021-07-02 07:22:37,364 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:22:37,365 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:22:37,365 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:22:38,410 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:22:38,411 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:22:38,411 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:22:38,411 - INFO - joeynmt.training - \tHypothesis: And the Pharisees stood by the Pharisees , and said to them , “ I am not afraid ; and when you see the young men , you will see Me , you will see Me . ”\n",
      "2021-07-02 07:22:38,412 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:22:38,412 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:22:38,412 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:22:38,412 - INFO - joeynmt.training - \tHypothesis: Now when he had come to him , he came to the girl , and said to him , “ Send , come here here here ! ”\n",
      "2021-07-02 07:22:38,412 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:22:38,413 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:22:38,414 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:22:38,414 - INFO - joeynmt.training - \tHypothesis: Now there was a certain day , a great day of the Sabbath .\n",
      "2021-07-02 07:22:38,414 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:22:38,414 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:22:38,415 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:22:38,415 - INFO - joeynmt.training - \tHypothesis: And when he had come , he was hungry , and there was a great lake , and the swine was thrown into the sea .\n",
      "2021-07-02 07:22:38,415 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     8200: bleu:   7.14, loss: 98958.7109, ppl:  20.8103, duration: 22.2494s\n",
      "2021-07-02 07:22:44,304 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     2.389243, Tokens per Sec:    12367, Lr: 0.000300\n",
      "2021-07-02 07:22:46,557 - INFO - joeynmt.training - Epoch   2: total training loss 659.91\n",
      "2021-07-02 07:22:46,558 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-02 07:22:50,660 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     2.699636, Tokens per Sec:    11478, Lr: 0.000300\n",
      "2021-07-02 07:23:12,234 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:23:12,234 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:23:12,234 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:23:12,554 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-02 07:23:12,554 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-02 07:23:13,272 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:23:13,273 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:23:13,273 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:23:13,273 - INFO - joeynmt.training - \tHypothesis: And the Pharisees stood up , and the Pharisees stood up with the eyes , saying , “ I am coming with you , and you will see Me ; and I will see the rooster of the water . ”\n",
      "2021-07-02 07:23:13,273 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:23:13,274 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:23:13,275 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:23:13,275 - INFO - joeynmt.training - \tHypothesis: And when he had come to the house , he went out and sat down with a loud voice , and said to him , “ Rabbi , we have compassion on your way . ”\n",
      "2021-07-02 07:23:13,275 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:23:13,276 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:23:13,276 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:23:13,276 - INFO - joeynmt.training - \tHypothesis: Now a certain man was about to be called in the day of the Sabbath .\n",
      "2021-07-02 07:23:13,276 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:23:13,277 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:23:13,277 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:23:13,277 - INFO - joeynmt.training - \tHypothesis: And he went out and found it , and was a great possessed . And he was a great possessed , and a great great possession fell on the ground .\n",
      "2021-07-02 07:23:13,277 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     8400: bleu:   7.27, loss: 98222.4453, ppl:  20.3456, duration: 22.6170s\n",
      "2021-07-02 07:23:19,142 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     2.522285, Tokens per Sec:    12364, Lr: 0.000300\n",
      "2021-07-02 07:23:25,651 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     2.530014, Tokens per Sec:    10967, Lr: 0.000300\n",
      "2021-07-02 07:23:44,023 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:23:44,023 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:23:44,023 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:23:44,338 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-02 07:23:44,338 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-02 07:23:45,465 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:23:45,466 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:23:45,466 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:23:45,466 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees heard that it was spoken by the disciples , and said to them , “ I will see the cut of the cut of the cut of the cup of the table , and you will see Me . ”\n",
      "2021-07-02 07:23:45,466 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:23:45,467 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:23:45,467 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:23:45,468 - INFO - joeynmt.training - \tHypothesis: Now when He had come to Peter , He came to Peter , and said to him , “ Rabbi , you are your way . ”\n",
      "2021-07-02 07:23:45,468 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:23:45,468 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:23:45,468 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:23:45,468 - INFO - joeynmt.training - \tHypothesis: Now there was a certain day , a day of the Sabbath , that the day was near the Sabbath .\n",
      "2021-07-02 07:23:45,468 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:23:45,469 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:23:45,469 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:23:45,469 - INFO - joeynmt.training - \tHypothesis: Then he went out to the ground , and was not like a great lame ; and he went out and saw it and sat on the ground .\n",
      "2021-07-02 07:23:45,470 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     8600: bleu:   7.29, loss: 97980.1016, ppl:  20.1949, duration: 19.8185s\n",
      "2021-07-02 07:23:45,596 - INFO - joeynmt.training - Epoch   3: total training loss 647.66\n",
      "2021-07-02 07:23:45,597 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-02 07:23:51,800 - INFO - joeynmt.training - Epoch   4, Step:     8700, Batch Loss:     2.303867, Tokens per Sec:    11493, Lr: 0.000300\n",
      "2021-07-02 07:23:57,967 - INFO - joeynmt.training - Epoch   4, Step:     8800, Batch Loss:     2.745318, Tokens per Sec:    11516, Lr: 0.000300\n",
      "2021-07-02 07:24:17,224 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:24:17,224 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:24:17,224 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:24:18,214 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:24:18,215 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:24:18,215 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:24:18,215 - INFO - joeynmt.training - \tHypothesis: And the Pharisees stood that he was still still still , and said to them , “ I have compassion on the right hand of the customs , and you will see Me . ”\n",
      "2021-07-02 07:24:18,215 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:24:18,216 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:24:18,216 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:24:18,216 - INFO - joeynmt.training - \tHypothesis: And when he had come to Him , he came to the house of Mary , and said to her , “ Rabbi , we have compassion on your way . ”\n",
      "2021-07-02 07:24:18,216 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:24:18,217 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:24:18,217 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:24:18,217 - INFO - joeynmt.training - \tHypothesis: Now a certain man was about the day of the Sabbath , that he might be taken away .\n",
      "2021-07-02 07:24:18,217 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:24:18,217 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:24:18,218 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:24:18,218 - INFO - joeynmt.training - \tHypothesis: And he was hungry , and was minded , and was made a great possession ; and he was thrown into the sea .\n",
      "2021-07-02 07:24:18,218 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     8800: bleu:   7.51, loss: 98038.9453, ppl:  20.2314, duration: 20.2503s\n",
      "2021-07-02 07:24:22,365 - INFO - joeynmt.training - Epoch   4: total training loss 636.06\n",
      "2021-07-02 07:24:22,366 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-02 07:24:24,202 - INFO - joeynmt.training - Epoch   5, Step:     8900, Batch Loss:     2.303290, Tokens per Sec:    12318, Lr: 0.000300\n",
      "2021-07-02 07:24:30,620 - INFO - joeynmt.training - Epoch   5, Step:     9000, Batch Loss:     2.210817, Tokens per Sec:    11237, Lr: 0.000300\n",
      "2021-07-02 07:24:51,099 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:24:51,099 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:24:51,100 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:24:52,110 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:24:52,110 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:24:52,111 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:24:52,111 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood by the Pharisees , and said to them , “ This Man is coming in the midst of the table , and you will see Me ; and I will see the cut of the door . ”\n",
      "2021-07-02 07:24:52,111 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:24:52,111 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:24:52,111 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:24:52,112 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the house , he went out to the girl , and said to him , “ Rabbi , we are willing ; be afraid . ”\n",
      "2021-07-02 07:24:52,112 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:24:52,112 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:24:52,112 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:24:52,112 - INFO - joeynmt.training - \tHypothesis: Now a certain man was about to be in the day of the Sabbath .\n",
      "2021-07-02 07:24:52,113 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:24:52,113 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:24:52,113 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:24:52,113 - INFO - joeynmt.training - \tHypothesis: Then Paul was hungry , and behold , a great wind was full of a swine . And he went out and sat down and sat down .\n",
      "2021-07-02 07:24:52,114 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     9000: bleu:   7.46, loss: 98010.0781, ppl:  20.2135, duration: 21.4934s\n",
      "2021-07-02 07:24:58,044 - INFO - joeynmt.training - Epoch   5, Step:     9100, Batch Loss:     2.381145, Tokens per Sec:    12158, Lr: 0.000300\n",
      "2021-07-02 07:25:00,184 - INFO - joeynmt.training - Epoch   5: total training loss 631.51\n",
      "2021-07-02 07:25:00,185 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-02 07:25:04,432 - INFO - joeynmt.training - Epoch   6, Step:     9200, Batch Loss:     2.621389, Tokens per Sec:    10850, Lr: 0.000300\n",
      "2021-07-02 07:25:24,035 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:25:24,035 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:25:24,035 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:25:24,362 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-02 07:25:24,362 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-02 07:25:25,015 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:25:25,016 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:25:25,016 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:25:25,016 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw that it was a man who had been in the boat , and said to them , “ This Man is a little while while , I will see you . ”\n",
      "2021-07-02 07:25:25,016 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:25:25,017 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:25:25,017 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:25:25,017 - INFO - joeynmt.training - \tHypothesis: When he had come to him , he came to the house of Mary , and said to him , “ Rabbi , your son ! ”\n",
      "2021-07-02 07:25:25,017 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:25:25,018 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:25:25,018 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:25:25,018 - INFO - joeynmt.training - \tHypothesis: Now there was a certain man who had been in the Sabbath , and was near the Sabbath .\n",
      "2021-07-02 07:25:25,018 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:25:25,019 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:25:25,019 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:25:25,019 - INFO - joeynmt.training - \tHypothesis: And when he had come , he was carried up , and there was a great lake , and it was a great great lake . And he was a great heads .\n",
      "2021-07-02 07:25:25,019 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     9200: bleu:   7.71, loss: 97764.7578, ppl:  20.0620, duration: 20.5864s\n",
      "2021-07-02 07:25:31,522 - INFO - joeynmt.training - Epoch   6, Step:     9300, Batch Loss:     2.630607, Tokens per Sec:    11024, Lr: 0.000300\n",
      "2021-07-02 07:25:37,703 - INFO - joeynmt.training - Epoch   6, Step:     9400, Batch Loss:     1.968445, Tokens per Sec:    11551, Lr: 0.000300\n",
      "2021-07-02 07:25:55,696 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:25:55,696 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:25:55,696 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:25:56,010 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-02 07:25:56,011 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-02 07:25:56,696 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:25:56,697 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:25:56,698 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:25:56,698 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw that he was still still still still , and said to them , “ I have been afraid , and you will see Me in the midst of the water . ”\n",
      "2021-07-02 07:25:56,698 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:25:56,698 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:25:56,699 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:25:56,699 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the house , he came to the girl , and said to him , “ Rabbi , we have compassion on your way . ”\n",
      "2021-07-02 07:25:56,699 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:25:56,699 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:25:56,699 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:25:56,700 - INFO - joeynmt.training - \tHypothesis: Now a certain man was about to the feast , and was near .\n",
      "2021-07-02 07:25:56,700 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:25:56,700 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:25:56,700 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:25:56,700 - INFO - joeynmt.training - \tHypothesis: Then Paul was carried up , and behold , a great multitude was thrown into a garment . And he went out and shair .\n",
      "2021-07-02 07:25:56,701 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     9400: bleu:   7.86, loss: 97437.9062, ppl:  19.8619, duration: 18.9973s\n",
      "2021-07-02 07:25:57,015 - INFO - joeynmt.training - Epoch   6: total training loss 621.94\n",
      "2021-07-02 07:25:57,016 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-02 07:26:02,620 - INFO - joeynmt.training - Epoch   7, Step:     9500, Batch Loss:     2.272434, Tokens per Sec:    12340, Lr: 0.000300\n",
      "2021-07-02 07:26:09,004 - INFO - joeynmt.training - Epoch   7, Step:     9600, Batch Loss:     2.608491, Tokens per Sec:    10894, Lr: 0.000300\n",
      "2021-07-02 07:26:29,892 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:26:29,893 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:26:29,893 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:26:30,871 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:26:30,871 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:26:30,872 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:26:30,872 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood up , and when he had said to them , “ I saw that I saw the voice of the cut of the cut of the cut of the cup of the cloud and followed Him . ”\n",
      "2021-07-02 07:26:30,872 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:26:30,872 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:26:30,873 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:26:30,873 - INFO - joeynmt.training - \tHypothesis: Now when he had come to him , she came and sat at the midst of Mary , and said to him , “ Get us , and come here here here here . ”\n",
      "2021-07-02 07:26:30,873 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:26:30,874 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:26:30,874 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:26:30,874 - INFO - joeynmt.training - \tHypothesis: Now a certain man was about to come to the Sabbath , and was near the Sabbath .\n",
      "2021-07-02 07:26:30,874 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:26:30,875 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:26:30,875 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:26:30,875 - INFO - joeynmt.training - \tHypothesis: Then he went out and found it , and was a great lord . And when he had sowed it , he went out and found it on a rod , and departed .\n",
      "2021-07-02 07:26:30,875 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     9600: bleu:   7.43, loss: 97703.6562, ppl:  20.0244, duration: 21.8711s\n",
      "2021-07-02 07:26:35,270 - INFO - joeynmt.training - Epoch   7: total training loss 617.09\n",
      "2021-07-02 07:26:35,270 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-02 07:26:36,834 - INFO - joeynmt.training - Epoch   8, Step:     9700, Batch Loss:     2.436374, Tokens per Sec:    12012, Lr: 0.000300\n",
      "2021-07-02 07:26:43,247 - INFO - joeynmt.training - Epoch   8, Step:     9800, Batch Loss:     1.993890, Tokens per Sec:    11126, Lr: 0.000300\n",
      "2021-07-02 07:27:04,552 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:27:04,552 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:27:04,552 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:27:04,867 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-02 07:27:04,867 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-02 07:27:06,023 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:27:06,024 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:27:06,024 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:27:06,024 - INFO - joeynmt.training - \tHypothesis: And the Pharisees saw that he was still still still still ; and when he had said to them , “ I saw a little while , I will see you and see Me . ”\n",
      "2021-07-02 07:27:06,024 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:27:06,025 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:27:06,025 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:27:06,025 - INFO - joeynmt.training - \tHypothesis: When he had come to the house , he went out and saw him , and said to her , “ Rabbi , we have compassion on your feet . ”\n",
      "2021-07-02 07:27:06,025 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:27:06,026 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:27:06,026 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:27:06,026 - INFO - joeynmt.training - \tHypothesis: Now behold , there was a great earthquake of the Sabbath , and it was day day .\n",
      "2021-07-02 07:27:06,026 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:27:06,026 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:27:06,027 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:27:06,027 - INFO - joeynmt.training - \tHypothesis: And he was hungry , and behold , a great wind was full of a swine . And it was a great great great wind and swine .\n",
      "2021-07-02 07:27:06,027 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     9800: bleu:   7.80, loss: 97414.1250, ppl:  19.8474, duration: 22.7799s\n",
      "2021-07-02 07:27:12,081 - INFO - joeynmt.training - Epoch   8, Step:     9900, Batch Loss:     2.599916, Tokens per Sec:    11925, Lr: 0.000300\n",
      "2021-07-02 07:27:14,913 - INFO - joeynmt.training - Epoch   8: total training loss 608.58\n",
      "2021-07-02 07:27:14,913 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-02 07:27:18,298 - INFO - joeynmt.training - Epoch   9, Step:    10000, Batch Loss:     2.348042, Tokens per Sec:    12253, Lr: 0.000300\n",
      "2021-07-02 07:27:37,223 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:27:37,224 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:27:37,224 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:27:37,541 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-02 07:27:37,541 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-02 07:27:38,219 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:27:38,220 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:27:38,220 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:27:38,220 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood up , and when he had said these things , “ I saw the voice of the cloud and go into the midst of the hand of the cloud and see . ”\n",
      "2021-07-02 07:27:38,220 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:27:38,221 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:27:38,221 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:27:38,221 - INFO - joeynmt.training - \tHypothesis: And when he had come to him , he went out and sat at his hand , and said to him , “ Rabbi , we have compassion on us . ”\n",
      "2021-07-02 07:27:38,221 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:27:38,221 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:27:38,222 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:27:38,222 - INFO - joeynmt.training - \tHypothesis: Now a man was about to come through the Sabbath , that it was near the Sabbath .\n",
      "2021-07-02 07:27:38,222 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:27:38,222 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:27:38,223 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:27:38,223 - INFO - joeynmt.training - \tHypothesis: And when he had come to the ground , he was hungry , and was carried by the swine . And he went out and prayed . And he was cast out of his hand .\n",
      "2021-07-02 07:27:38,223 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    10000: bleu:   8.32, loss: 97413.6562, ppl:  19.8471, duration: 19.9246s\n",
      "2021-07-02 07:27:44,125 - INFO - joeynmt.training - Epoch   9, Step:    10100, Batch Loss:     2.307301, Tokens per Sec:    11983, Lr: 0.000300\n",
      "2021-07-02 07:27:50,526 - INFO - joeynmt.training - Epoch   9, Step:    10200, Batch Loss:     2.292569, Tokens per Sec:    11172, Lr: 0.000300\n",
      "2021-07-02 07:28:09,864 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:28:09,865 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:28:09,865 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:28:10,174 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-02 07:28:10,174 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-02 07:28:10,836 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:28:10,837 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:28:10,837 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:28:10,837 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw that a man was with the spirit , and said to them , “ Go , see the feet of the cup and see my feet , and I will see you . ”\n",
      "2021-07-02 07:28:10,837 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:28:10,838 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:28:10,838 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:28:10,838 - INFO - joeynmt.training - \tHypothesis: When he had said these things , he went out to the girl , and said to him , “ Rabbi , we have compassion on your feet , and follow us . ”\n",
      "2021-07-02 07:28:10,838 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:28:10,839 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:28:10,839 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:28:10,839 - INFO - joeynmt.training - \tHypothesis: Now a certain man was about the Sabbath , that he was near the Sabbath .\n",
      "2021-07-02 07:28:10,839 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:28:10,840 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:28:10,840 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:28:10,840 - INFO - joeynmt.training - \tHypothesis: Then he was hungry , and was carried up to the stay . And he was a swine and went out and fell on his hand .\n",
      "2021-07-02 07:28:10,840 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    10200: bleu:   8.29, loss: 97024.3594, ppl:  19.6115, duration: 20.3138s\n",
      "2021-07-02 07:28:11,593 - INFO - joeynmt.training - Epoch   9: total training loss 599.10\n",
      "2021-07-02 07:28:11,593 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-02 07:28:17,009 - INFO - joeynmt.training - Epoch  10, Step:    10300, Batch Loss:     2.084090, Tokens per Sec:    11881, Lr: 0.000300\n",
      "2021-07-02 07:28:23,146 - INFO - joeynmt.training - Epoch  10, Step:    10400, Batch Loss:     1.971283, Tokens per Sec:    11702, Lr: 0.000300\n",
      "2021-07-02 07:28:42,748 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:28:42,748 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:28:42,748 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:28:43,704 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:28:43,705 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:28:43,705 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:28:43,705 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw that he was still still still still , and said to them , “ I saw the voice of the cut down and go into the midst of the cloud and see . ”\n",
      "2021-07-02 07:28:43,705 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:28:43,706 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:28:43,706 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:28:43,706 - INFO - joeynmt.training - \tHypothesis: When he had come to him , he came to the girl , and went out to the girl , and said to him , “ Rabbi , we are willing . ”\n",
      "2021-07-02 07:28:43,706 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:28:43,707 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:28:43,707 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:28:43,707 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the Sabbath , he was near .\n",
      "2021-07-02 07:28:43,707 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:28:43,708 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:28:43,708 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:28:43,708 - INFO - joeynmt.training - \tHypothesis: And he was hungry , and was carried by the land of Syria . And he went out and sat down on the ground and sat down .\n",
      "2021-07-02 07:28:43,708 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    10400: bleu:   8.14, loss: 97337.3516, ppl:  19.8007, duration: 20.5616s\n",
      "2021-07-02 07:28:48,402 - INFO - joeynmt.training - Epoch  10: total training loss 588.42\n",
      "2021-07-02 07:28:48,403 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-02 07:28:49,639 - INFO - joeynmt.training - Epoch  11, Step:    10500, Batch Loss:     2.236662, Tokens per Sec:    11779, Lr: 0.000300\n",
      "2021-07-02 07:28:55,964 - INFO - joeynmt.training - Epoch  11, Step:    10600, Batch Loss:     2.120114, Tokens per Sec:    11371, Lr: 0.000300\n",
      "2021-07-02 07:29:14,576 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:29:14,577 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:29:14,577 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:29:15,540 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:29:15,541 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:29:15,541 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:29:15,542 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood up , and when the Pharisees saw it , he said to them , “ I will see you the mine and finger of the cut down and see me . ”\n",
      "2021-07-02 07:29:15,542 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:29:15,542 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:29:15,542 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:29:15,542 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the house , he went out to the girl , and said to him , “ Rabbi , we have compassion on your way . ”\n",
      "2021-07-02 07:29:15,543 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:29:15,543 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:29:15,543 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:29:15,543 - INFO - joeynmt.training - \tHypothesis: Now a certain day , when he had come near the Sabbath ,\n",
      "2021-07-02 07:29:15,544 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:29:15,544 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:29:15,544 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:29:15,544 - INFO - joeynmt.training - \tHypothesis: Then he was filled with a fig tree . And when he had taken a great great great multitude , he was cast out his hand and sat down on his hand .\n",
      "2021-07-02 07:29:15,545 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    10600: bleu:   8.56, loss: 97628.6328, ppl:  19.9784, duration: 19.5802s\n",
      "2021-07-02 07:29:21,510 - INFO - joeynmt.training - Epoch  11, Step:    10700, Batch Loss:     1.859548, Tokens per Sec:    12072, Lr: 0.000300\n",
      "2021-07-02 07:29:24,748 - INFO - joeynmt.training - Epoch  11: total training loss 583.14\n",
      "2021-07-02 07:29:24,749 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-02 07:29:27,875 - INFO - joeynmt.training - Epoch  12, Step:    10800, Batch Loss:     1.884571, Tokens per Sec:    12434, Lr: 0.000300\n",
      "2021-07-02 07:29:47,783 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:29:47,784 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:29:47,784 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:29:48,780 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:29:48,780 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:29:48,781 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:29:48,781 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood up , and said to them , “ If anyone had been demon-possessed , he said to them , “ I will see you in the midst of the cloud and finging . ”\n",
      "2021-07-02 07:29:48,781 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:29:48,781 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:29:48,782 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:29:48,782 - INFO - joeynmt.training - \tHypothesis: When he had seen the servants , he went out to the girl , and said to him , “ Rabbi , we have compassion on your feet . ”\n",
      "2021-07-02 07:29:48,782 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:29:48,782 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:29:48,783 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:29:48,783 - INFO - joeynmt.training - \tHypothesis: Now there was a certain day , that He was a great earthly day in the day of the Sabbath .\n",
      "2021-07-02 07:29:48,783 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:29:48,783 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:29:48,784 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:29:48,784 - INFO - joeynmt.training - \tHypothesis: And he was hungry , and was carried up with a great fear ; and he was shair , and a great great great grain .\n",
      "2021-07-02 07:29:48,784 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    10800: bleu:   8.49, loss: 97078.1328, ppl:  19.6439, duration: 20.9088s\n",
      "2021-07-02 07:29:54,772 - INFO - joeynmt.training - Epoch  12, Step:    10900, Batch Loss:     2.128380, Tokens per Sec:    11991, Lr: 0.000300\n",
      "2021-07-02 07:30:01,201 - INFO - joeynmt.training - Epoch  12, Step:    11000, Batch Loss:     2.263728, Tokens per Sec:    11170, Lr: 0.000300\n",
      "2021-07-02 07:30:20,761 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:30:20,762 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:30:20,762 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:30:22,181 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:30:22,182 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:30:22,182 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:30:22,183 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood up , and saw that he was still still still still still still with his eyes , saying , “ Come and see your feet , I will see you up and see my face . ”\n",
      "2021-07-02 07:30:22,183 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:30:22,183 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:30:22,183 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:30:22,184 - INFO - joeynmt.training - \tHypothesis: When He had spoken these things , he went out to the house of Mary , and said to him , “ Send , come here here here , and come to us . ”\n",
      "2021-07-02 07:30:22,184 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:30:22,184 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:30:22,184 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:30:22,184 - INFO - joeynmt.training - \tHypothesis: Now a certain man was sick , and behold , a great city was near .\n",
      "2021-07-02 07:30:22,185 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:30:22,185 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:30:22,185 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:30:22,185 - INFO - joeynmt.training - \tHypothesis: And he was filled with a fig tree . And when he was taken up , he was thrown into a villstone . And he was thrown down and sat down with a heads of the heads of the heads .\n",
      "2021-07-02 07:30:22,185 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    11000: bleu:   8.23, loss: 97145.1250, ppl:  19.6843, duration: 20.9838s\n",
      "2021-07-02 07:30:23,003 - INFO - joeynmt.training - Epoch  12: total training loss 572.92\n",
      "2021-07-02 07:30:23,004 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-02 07:30:28,079 - INFO - joeynmt.training - Epoch  13, Step:    11100, Batch Loss:     1.832921, Tokens per Sec:    12112, Lr: 0.000300\n",
      "2021-07-02 07:30:34,416 - INFO - joeynmt.training - Epoch  13, Step:    11200, Batch Loss:     2.096437, Tokens per Sec:    11475, Lr: 0.000300\n",
      "2021-07-02 07:30:53,574 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:30:53,575 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:30:53,575 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:30:54,547 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:30:54,547 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:30:54,548 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:30:54,548 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees asked him , saying , “ If I am coming quickly , I have compassion on you , I will see the feet of the cut of the cup of the clay . ”\n",
      "2021-07-02 07:30:54,548 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:30:54,548 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:30:54,549 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:30:54,549 - INFO - joeynmt.training - \tHypothesis: When he had spoken these things , she came to her , and said to her , “ Send us , come to your son ! ”\n",
      "2021-07-02 07:30:54,549 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:30:54,549 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:30:54,549 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:30:54,550 - INFO - joeynmt.training - \tHypothesis: Now when he had come to the feast , he was going to the feast .\n",
      "2021-07-02 07:30:54,550 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:30:54,550 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:30:54,550 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:30:54,551 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a fig around his feet , and it was fled by the star . And he was thrown into a fire .\n",
      "2021-07-02 07:30:54,551 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    11200: bleu:   8.47, loss: 97275.3047, ppl:  19.7630, duration: 20.1347s\n",
      "2021-07-02 07:30:59,449 - INFO - joeynmt.training - Epoch  13: total training loss 570.06\n",
      "2021-07-02 07:30:59,449 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-02 07:31:00,462 - INFO - joeynmt.training - Epoch  14, Step:    11300, Batch Loss:     1.943153, Tokens per Sec:    12501, Lr: 0.000300\n",
      "2021-07-02 07:31:06,997 - INFO - joeynmt.training - Epoch  14, Step:    11400, Batch Loss:     2.174954, Tokens per Sec:    11091, Lr: 0.000300\n",
      "2021-07-02 07:31:25,794 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:31:25,795 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:31:25,795 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:31:26,811 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:31:26,812 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:31:26,812 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:31:26,812 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood up , and saw that he was still still afraid , saying , “ Come and see the pool of the feet of the finger , and bring me before me . ”\n",
      "2021-07-02 07:31:26,812 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:31:26,813 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:31:26,813 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:31:26,813 - INFO - joeynmt.training - \tHypothesis: When He had said these things , she went out to the house of Mary , and Mary his mother , and said to him , “ Rabbi , we have compassion on us . ”\n",
      "2021-07-02 07:31:26,813 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:31:26,814 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:31:26,814 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:31:26,814 - INFO - joeynmt.training - \tHypothesis: Now there was a certain day of the Jews , that He was near the day of the Sabbath .\n",
      "2021-07-02 07:31:26,815 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:31:26,815 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:31:26,815 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:31:26,815 - INFO - joeynmt.training - \tHypothesis: And when he had said these things , he was a great goll . And he went out and swine was thrown into the sea . And he went out of the rod and sat on his hand .\n",
      "2021-07-02 07:31:26,815 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    11400: bleu:   8.49, loss: 97219.1250, ppl:  19.7290, duration: 19.8181s\n",
      "2021-07-02 07:31:32,904 - INFO - joeynmt.training - Epoch  14, Step:    11500, Batch Loss:     2.098371, Tokens per Sec:    11630, Lr: 0.000210\n",
      "2021-07-02 07:31:36,102 - INFO - joeynmt.training - Epoch  14: total training loss 559.58\n",
      "2021-07-02 07:31:36,103 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-02 07:31:39,548 - INFO - joeynmt.training - Epoch  15, Step:    11600, Batch Loss:     2.099445, Tokens per Sec:     9855, Lr: 0.000210\n",
      "2021-07-02 07:31:59,931 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:31:59,931 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:31:59,931 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:32:00,966 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:32:00,967 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:32:00,967 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:32:00,968 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees stood up , and saw that he was going up to see ; and He said to them , “ Take the way of the cup and see ; and I have compassion on me . ”\n",
      "2021-07-02 07:32:00,968 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:32:00,968 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:32:00,968 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:32:00,969 - INFO - joeynmt.training - \tHypothesis: When He heard that he had said to him , Mary Magdalene , Mary Magdalene , she said to her , “ Send us , come here ! ”\n",
      "2021-07-02 07:32:00,969 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:32:00,969 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:32:00,969 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:32:00,969 - INFO - joeynmt.training - \tHypothesis: Now a certain man was about to come to the Sabbath , that it was day .\n",
      "2021-07-02 07:32:00,970 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:32:00,970 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:32:00,970 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:32:00,970 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a root of the root of the bridegroom . And he went out to a rod , and went out of the west place .\n",
      "2021-07-02 07:32:00,970 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    11600: bleu:   8.46, loss: 97157.9375, ppl:  19.6920, duration: 21.4216s\n",
      "2021-07-02 07:32:07,460 - INFO - joeynmt.training - Epoch  15, Step:    11700, Batch Loss:     1.930363, Tokens per Sec:    11227, Lr: 0.000210\n",
      "2021-07-02 07:32:13,719 - INFO - joeynmt.training - Epoch  15, Step:    11800, Batch Loss:     1.952616, Tokens per Sec:    11285, Lr: 0.000210\n",
      "2021-07-02 07:32:33,170 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:32:33,170 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:32:33,171 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:32:33,484 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-02 07:32:33,484 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-02 07:32:34,197 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:32:34,198 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:32:34,198 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:32:34,198 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to him , “ If anyone had been coming , he came and found it at the right hand and announted his feet , and he will see my faste . ”\n",
      "2021-07-02 07:32:34,199 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:32:34,199 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:32:34,200 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:32:34,200 - INFO - joeynmt.training - \tHypothesis: When He had said these things , he went out to the house of Mary , and said to her , “ Send us , and come here here here ! ”\n",
      "2021-07-02 07:32:34,200 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:32:34,200 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:32:34,200 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:32:34,201 - INFO - joeynmt.training - \tHypothesis: Now a certain man was near , a great day was near .\n",
      "2021-07-02 07:32:34,201 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:32:34,201 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:32:34,201 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:32:34,202 - INFO - joeynmt.training - \tHypothesis: Then he was filled with a skney . And when he was taken up , he was a scorrrant , and a great horses fell on his head was burned .\n",
      "2021-07-02 07:32:34,204 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    11800: bleu:   8.94, loss: 96984.3125, ppl:  19.5874, duration: 20.4840s\n",
      "2021-07-02 07:32:35,509 - INFO - joeynmt.training - Epoch  15: total training loss 547.96\n",
      "2021-07-02 07:32:35,509 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-02 07:32:40,149 - INFO - joeynmt.training - Epoch  16, Step:    11900, Batch Loss:     2.176060, Tokens per Sec:    11856, Lr: 0.000210\n",
      "2021-07-02 07:32:46,488 - INFO - joeynmt.training - Epoch  16, Step:    12000, Batch Loss:     2.231371, Tokens per Sec:    11360, Lr: 0.000210\n",
      "2021-07-02 07:33:05,489 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:33:05,489 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:33:05,489 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:33:06,475 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:33:06,475 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:33:06,475 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:33:06,476 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to him , “ If anyone had been coming , he came and found it in the midst of the midst of the cities , and said to them , “ I will see you and see Me . ”\n",
      "2021-07-02 07:33:06,476 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:33:06,476 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:33:06,477 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:33:06,477 - INFO - joeynmt.training - \tHypothesis: When He had spoken these things , he went out to the house of Mary , and said to him , “ Send , come here here here and follow us . ”\n",
      "2021-07-02 07:33:06,477 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:33:06,478 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:33:06,478 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:33:06,478 - INFO - joeynmt.training - \tHypothesis: Now a man was sick , that he was near the Sabbath .\n",
      "2021-07-02 07:33:06,478 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:33:06,479 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:33:06,479 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:33:06,479 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a vision . And when he was taken up , he was a scorrant , and a great fire was given to him .\n",
      "2021-07-02 07:33:06,479 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    12000: bleu:   8.66, loss: 97259.5703, ppl:  19.7535, duration: 19.9910s\n",
      "2021-07-02 07:33:12,005 - INFO - joeynmt.training - Epoch  16: total training loss 540.70\n",
      "2021-07-02 07:33:12,005 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-02 07:33:12,564 - INFO - joeynmt.training - Epoch  17, Step:    12100, Batch Loss:     1.868705, Tokens per Sec:    12306, Lr: 0.000210\n",
      "2021-07-02 07:33:18,974 - INFO - joeynmt.training - Epoch  17, Step:    12200, Batch Loss:     2.179623, Tokens per Sec:    11170, Lr: 0.000210\n",
      "2021-07-02 07:33:38,674 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:33:38,674 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:33:38,675 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:33:39,736 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:33:39,737 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:33:39,737 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:33:39,737 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ If anyone had been baptized with me , he came and said to them , “ Take up your bed and see my cut . ”\n",
      "2021-07-02 07:33:39,737 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:33:39,738 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:33:39,738 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:33:39,738 - INFO - joeynmt.training - \tHypothesis: When He heard it , he came to Mary , Mary Magdalene , Mary the mother of Mary , and said to him , “ Get , we are willing to be afraid . ”\n",
      "2021-07-02 07:33:39,738 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:33:39,739 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:33:39,739 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:33:39,739 - INFO - joeynmt.training - \tHypothesis: Now behold , there was a great earthly , a great day of the Sabbath .\n",
      "2021-07-02 07:33:39,740 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:33:39,740 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:33:39,741 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:33:39,741 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a great garments . And when he had taken a great great garment , he went out to a great great fire . And he departed and departed .\n",
      "2021-07-02 07:33:39,741 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    12200: bleu:   8.72, loss: 97523.3125, ppl:  19.9140, duration: 20.7664s\n",
      "2021-07-02 07:33:45,768 - INFO - joeynmt.training - Epoch  17, Step:    12300, Batch Loss:     1.952315, Tokens per Sec:    11872, Lr: 0.000210\n",
      "2021-07-02 07:33:49,924 - INFO - joeynmt.training - Epoch  17: total training loss 532.53\n",
      "2021-07-02 07:33:49,925 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-02 07:33:52,410 - INFO - joeynmt.training - Epoch  18, Step:    12400, Batch Loss:     1.909682, Tokens per Sec:    11592, Lr: 0.000210\n",
      "2021-07-02 07:34:11,597 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:34:11,597 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:34:11,598 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:34:12,582 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:34:12,585 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:34:12,585 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:34:12,585 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ If anyone had been baptized with me , I came and saw the lame of the cup and see. ” And he said to them , “ I will see you . ”\n",
      "2021-07-02 07:34:12,585 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:34:12,586 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:34:12,586 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:34:12,586 - INFO - joeynmt.training - \tHypothesis: When he had spoken these things , he went out to the house of Mary , and said to him , “ Send your mother , and come here here here ! ”\n",
      "2021-07-02 07:34:12,586 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:34:12,587 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:34:12,587 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:34:12,587 - INFO - joeynmt.training - \tHypothesis: Now a certain man was sick , and a day of the Sabbath .\n",
      "2021-07-02 07:34:12,587 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:34:12,588 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:34:12,588 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:34:12,588 - INFO - joeynmt.training - \tHypothesis: And he was hungry , and was blinded . And when he was cast out of the garment of a vision . And he was taken up and sat down .\n",
      "2021-07-02 07:34:12,588 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    12400: bleu:   8.87, loss: 97310.8281, ppl:  19.7846, duration: 20.1779s\n",
      "2021-07-02 07:34:18,623 - INFO - joeynmt.training - Epoch  18, Step:    12500, Batch Loss:     2.188809, Tokens per Sec:    11842, Lr: 0.000210\n",
      "2021-07-02 07:34:25,174 - INFO - joeynmt.training - Epoch  18, Step:    12600, Batch Loss:     1.998006, Tokens per Sec:    10989, Lr: 0.000210\n",
      "2021-07-02 07:34:44,972 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:34:44,972 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:34:44,973 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:34:45,939 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:34:45,940 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:34:45,940 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:34:45,940 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to them , “ When I saw that he was coming in the midst of the cloud , I will give you a patter of water . ”\n",
      "2021-07-02 07:34:45,940 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:34:45,941 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:34:45,941 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:34:45,941 - INFO - joeynmt.training - \tHypothesis: When he had spoken these things , he went to the house of Mary , and Mary Magdalene , and said to him , “ Send us , and come here here . ”\n",
      "2021-07-02 07:34:45,941 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:34:45,942 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:34:45,942 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:34:45,942 - INFO - joeynmt.training - \tHypothesis: Now it was , as he was near the Sabbath , that He was near the day .\n",
      "2021-07-02 07:34:45,942 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:34:45,943 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:34:45,943 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:34:45,943 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a vision . And when he was taken by the scorners of the star , and it was given to him by a fire .\n",
      "2021-07-02 07:34:45,943 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    12600: bleu:   8.76, loss: 97050.7969, ppl:  19.6274, duration: 20.7688s\n",
      "2021-07-02 07:34:47,628 - INFO - joeynmt.training - Epoch  18: total training loss 526.41\n",
      "2021-07-02 07:34:47,629 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-02 07:34:51,891 - INFO - joeynmt.training - Epoch  19, Step:    12700, Batch Loss:     2.132071, Tokens per Sec:    11964, Lr: 0.000210\n",
      "2021-07-02 07:34:58,292 - INFO - joeynmt.training - Epoch  19, Step:    12800, Batch Loss:     2.158274, Tokens per Sec:    11379, Lr: 0.000210\n",
      "2021-07-02 07:35:18,089 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:35:18,089 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:35:18,090 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:35:19,082 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:35:19,083 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:35:19,083 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:35:19,083 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ If anyone had been in the boat , he would see Me ; and when I heard it , I have compassion on my feet , and I will see it . ”\n",
      "2021-07-02 07:35:19,083 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:35:19,084 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:35:19,084 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:35:19,084 - INFO - joeynmt.training - \tHypothesis: When he heard that he had seen him , he went and called Mary , and said to him , “ Rabbi , we have compassion on your left and follow You . ”\n",
      "2021-07-02 07:35:19,084 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:35:19,085 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:35:19,085 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:35:19,085 - INFO - joeynmt.training - \tHypothesis: Now a man was sick , and a day was near .\n",
      "2021-07-02 07:35:19,085 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:35:19,086 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:35:19,086 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:35:19,086 - INFO - joeynmt.training - \tHypothesis: And he was hungry , and was like a scorryl . And when he was thrown into the oven , and he went out of the wages .\n",
      "2021-07-02 07:35:19,086 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    12800: bleu:   8.72, loss: 97525.4922, ppl:  19.9153, duration: 20.7937s\n",
      "2021-07-02 07:35:24,924 - INFO - joeynmt.training - Epoch  19: total training loss 523.52\n",
      "2021-07-02 07:35:24,924 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-02 07:35:25,062 - INFO - joeynmt.training - Epoch  20, Step:    12900, Batch Loss:     1.886818, Tokens per Sec:    12006, Lr: 0.000210\n",
      "2021-07-02 07:35:31,531 - INFO - joeynmt.training - Epoch  20, Step:    13000, Batch Loss:     2.162636, Tokens per Sec:    11014, Lr: 0.000210\n",
      "2021-07-02 07:35:51,011 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:35:51,012 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:35:51,012 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:35:52,012 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:35:52,015 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:35:52,015 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:35:52,015 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees also spoke to the Pharisees , and when he had been coming , He said to them , “ Go into the pool and beat my feet , and I will see you . ”\n",
      "2021-07-02 07:35:52,015 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:35:52,016 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:35:52,016 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:35:52,016 - INFO - joeynmt.training - \tHypothesis: When he had spoken these things , he went out to the girl , and said to him , “ Send to us , and come here here here ! ”\n",
      "2021-07-02 07:35:52,016 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:35:52,017 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:35:52,017 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:35:52,017 - INFO - joeynmt.training - \tHypothesis: Now a certain day , because he was near the Sabbath .\n",
      "2021-07-02 07:35:52,017 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:35:52,018 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:35:52,018 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:35:52,018 - INFO - joeynmt.training - \tHypothesis: So Paul was filled with a little while . And when he was taken up , and a scorrant was cast out of the fire . And he arose and sat down .\n",
      "2021-07-02 07:35:52,019 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    13000: bleu:   9.20, loss: 97521.9922, ppl:  19.9132, duration: 20.4868s\n",
      "2021-07-02 07:35:57,960 - INFO - joeynmt.training - Epoch  20, Step:    13100, Batch Loss:     1.836345, Tokens per Sec:    11977, Lr: 0.000147\n",
      "2021-07-02 07:36:01,933 - INFO - joeynmt.training - Epoch  20: total training loss 513.68\n",
      "2021-07-02 07:36:01,934 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-02 07:36:04,363 - INFO - joeynmt.training - Epoch  21, Step:    13200, Batch Loss:     1.821473, Tokens per Sec:     9897, Lr: 0.000147\n",
      "2021-07-02 07:36:23,180 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:36:23,181 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:36:23,181 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:36:24,155 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:36:24,156 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:36:24,156 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:36:24,156 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees also said to him , “ When I saw that he was going up to the pool , and said to them , “ I will see you in the hands of the hands of the Pharisees and finging . ”\n",
      "2021-07-02 07:36:24,156 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:36:24,157 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:36:24,157 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:36:24,157 - INFO - joeynmt.training - \tHypothesis: When he had heard it , he went to the house of Mary , and called Mary , and said to her , “ Rabbi , we have compassion on your left . ”\n",
      "2021-07-02 07:36:24,157 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:36:24,158 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:36:24,158 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:36:24,158 - INFO - joeynmt.training - \tHypothesis: Now when he had come , there was a great earthquestion of the Sabbath .\n",
      "2021-07-02 07:36:24,159 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:36:24,159 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:36:24,159 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:36:24,159 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a vision . And when he was taken up , and a scorrant fell on his right hand was cast out of his hand .\n",
      "2021-07-02 07:36:24,159 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    13200: bleu:   9.43, loss: 97359.6719, ppl:  19.8142, duration: 19.7964s\n",
      "2021-07-02 07:36:30,092 - INFO - joeynmt.training - Epoch  21, Step:    13300, Batch Loss:     2.196275, Tokens per Sec:    12077, Lr: 0.000147\n",
      "2021-07-02 07:36:36,471 - INFO - joeynmt.training - Epoch  21, Step:    13400, Batch Loss:     2.206511, Tokens per Sec:    11331, Lr: 0.000147\n",
      "2021-07-02 07:36:55,805 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:36:55,805 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:36:55,806 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:36:56,779 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:36:56,780 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:36:56,780 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:36:56,780 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees had the Pharisees perceived that he was going to see , and said to them , “ Come and see the pool of the cup of the cup and stones , and I will see you . ”\n",
      "2021-07-02 07:36:56,781 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:36:56,781 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:36:56,781 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:36:56,782 - INFO - joeynmt.training - \tHypothesis: When he had heard that he had seen the house of Mary , Mary called Mary and said to him , “ Send us , and come here here here ! ”\n",
      "2021-07-02 07:36:56,782 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:36:56,782 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:36:56,783 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:36:56,784 - INFO - joeynmt.training - \tHypothesis: Now a certain man was sick , and a great day of the Sabbath .\n",
      "2021-07-02 07:36:56,784 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:36:56,785 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:36:56,785 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:36:56,785 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a great smoke . And when he was taken up , and a scorpions was cast out of fire . And he arose and sat down .\n",
      "2021-07-02 07:36:56,785 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    13400: bleu:   9.39, loss: 97444.2188, ppl:  19.8657, duration: 20.3138s\n",
      "2021-07-02 07:36:58,883 - INFO - joeynmt.training - Epoch  21: total training loss 502.74\n",
      "2021-07-02 07:36:58,883 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-02 07:37:02,684 - INFO - joeynmt.training - Epoch  22, Step:    13500, Batch Loss:     1.790402, Tokens per Sec:    12315, Lr: 0.000147\n",
      "2021-07-02 07:37:09,019 - INFO - joeynmt.training - Epoch  22, Step:    13600, Batch Loss:     1.737016, Tokens per Sec:    11289, Lr: 0.000147\n",
      "2021-07-02 07:37:28,385 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:37:28,386 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:37:28,386 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:37:29,831 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:37:29,832 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:37:29,832 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:37:29,832 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to him , “ If anyone had been taken up , he was going up to the seize ; and when I saw it , I was lost . ”\n",
      "2021-07-02 07:37:29,832 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:37:29,833 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:37:29,833 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:37:29,833 - INFO - joeynmt.training - \tHypothesis: When he had said these things , he went out to the girl , Mary Magdalene , and said to him , “ Teacher , we have compassion on Your way . ”\n",
      "2021-07-02 07:37:29,833 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:37:29,834 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:37:29,834 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:37:29,834 - INFO - joeynmt.training - \tHypothesis: Now it was , when he had come , a great city was near .\n",
      "2021-07-02 07:37:29,834 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:37:29,835 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:37:29,835 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:37:29,835 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a fig around his news ; and when he was cast out of the star , he departed and sat down with a fire .\n",
      "2021-07-02 07:37:29,835 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step    13600: bleu:   9.48, loss: 97609.9766, ppl:  19.9670, duration: 20.8155s\n",
      "2021-07-02 07:37:35,701 - INFO - joeynmt.training - Epoch  22, Step:    13700, Batch Loss:     1.780187, Tokens per Sec:    12244, Lr: 0.000147\n",
      "2021-07-02 07:37:35,927 - INFO - joeynmt.training - Epoch  22: total training loss 501.08\n",
      "2021-07-02 07:37:35,927 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-02 07:37:42,041 - INFO - joeynmt.training - Epoch  23, Step:    13800, Batch Loss:     1.782987, Tokens per Sec:    11121, Lr: 0.000147\n",
      "2021-07-02 07:38:01,896 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:38:01,897 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:38:01,897 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:38:02,879 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:38:02,880 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:38:02,881 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:38:02,881 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to him , “ When I saw that he had seen the lame , he came and said to them , “ Come and see the pool of the cup and see my feet . ”\n",
      "2021-07-02 07:38:02,881 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:38:02,882 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:38:02,882 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:38:02,882 - INFO - joeynmt.training - \tHypothesis: When he had spoken these things , he went to the house of Mary , and called Mary , and said to her , “ Rabbi , we have compassion on us . ”\n",
      "2021-07-02 07:38:02,882 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:38:02,883 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:38:02,883 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:38:02,883 - INFO - joeynmt.training - \tHypothesis: Now when he had come , a city was near the Sabbath .\n",
      "2021-07-02 07:38:02,883 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:38:02,884 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:38:02,884 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:38:02,884 - INFO - joeynmt.training - \tHypothesis: So he was hungry , and was thirty years . And when he was cast out his bowing , and a fire was cast out of his hand .\n",
      "2021-07-02 07:38:02,884 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    13800: bleu:   9.21, loss: 97884.0391, ppl:  20.1355, duration: 20.8427s\n",
      "2021-07-02 07:38:08,821 - INFO - joeynmt.training - Epoch  23, Step:    13900, Batch Loss:     2.132252, Tokens per Sec:    12046, Lr: 0.000147\n",
      "2021-07-02 07:38:13,614 - INFO - joeynmt.training - Epoch  23: total training loss 497.55\n",
      "2021-07-02 07:38:13,614 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-02 07:38:15,163 - INFO - joeynmt.training - Epoch  24, Step:    14000, Batch Loss:     1.655343, Tokens per Sec:    12132, Lr: 0.000147\n",
      "2021-07-02 07:38:35,767 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:38:35,768 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:38:35,768 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:38:36,780 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:38:36,797 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:38:36,797 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:38:36,798 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to him , “ When he had been taken up , he came and said to them , “ Come and see my feet , and I will see you up and see my feet . ”\n",
      "2021-07-02 07:38:36,798 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:38:36,798 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:38:36,799 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:38:36,799 - INFO - joeynmt.training - \tHypothesis: When he had heard that he had seen Peter , she came to her , and said to her , “ Send us , and come here here here ! ”\n",
      "2021-07-02 07:38:36,799 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:38:36,799 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:38:36,799 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:38:36,800 - INFO - joeynmt.training - \tHypothesis: Now when he had come , a city was near to the Sabbath .\n",
      "2021-07-02 07:38:36,800 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:38:36,800 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:38:36,800 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:38:36,800 - INFO - joeynmt.training - \tHypothesis: So Paul was hungry , and it was a great calling ; and as he was scorrant , and it was a fire fell on his right hand . And he went out and sat on his hand .\n",
      "2021-07-02 07:38:36,801 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step    14000: bleu:   9.26, loss: 97753.4922, ppl:  20.0551, duration: 21.6373s\n",
      "2021-07-02 07:38:43,094 - INFO - joeynmt.training - Epoch  24, Step:    14100, Batch Loss:     1.684825, Tokens per Sec:    11515, Lr: 0.000147\n",
      "2021-07-02 07:38:49,161 - INFO - joeynmt.training - Epoch  24, Step:    14200, Batch Loss:     1.798621, Tokens per Sec:    11750, Lr: 0.000147\n",
      "2021-07-02 07:39:08,162 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:39:08,163 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:39:08,163 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:39:09,132 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:39:09,136 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:39:09,136 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:39:09,136 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to them , “ When I saw that he was going out to see , and found His feet and washed me at the feet of the water . ”\n",
      "2021-07-02 07:39:09,136 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:39:09,137 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:39:09,137 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:39:09,137 - INFO - joeynmt.training - \tHypothesis: When he had said these things , he went to the girl , and Mary Magdalene , and said to her , “ Rabbi , we have compassion on us . ”\n",
      "2021-07-02 07:39:09,137 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:39:09,138 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:39:09,139 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:39:09,139 - INFO - joeynmt.training - \tHypothesis: Now when he had come , he was near the Sabbath , he was near .\n",
      "2021-07-02 07:39:09,139 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:39:09,139 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:39:09,140 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:39:09,140 - INFO - joeynmt.training - \tHypothesis: So he was hungry , and was a ween ; and when he was scorched , he was thrown into the fire of his hand . And he departed and sat down .\n",
      "2021-07-02 07:39:09,140 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step    14200: bleu:   9.41, loss: 97866.9844, ppl:  20.1250, duration: 19.9784s\n",
      "2021-07-02 07:39:12,084 - INFO - joeynmt.training - Epoch  24: total training loss 491.57\n",
      "2021-07-02 07:39:12,084 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-07-02 07:39:15,461 - INFO - joeynmt.training - Epoch  25, Step:    14300, Batch Loss:     1.647709, Tokens per Sec:    12194, Lr: 0.000103\n",
      "2021-07-02 07:39:21,778 - INFO - joeynmt.training - Epoch  25, Step:    14400, Batch Loss:     1.781842, Tokens per Sec:    11132, Lr: 0.000103\n",
      "2021-07-02 07:39:40,832 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:39:40,832 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:39:40,832 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:39:41,805 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:39:41,806 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:39:41,806 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:39:41,806 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ When I saw that he was going out to the pool , and washed with water with water , I will put my hands on my side . ”\n",
      "2021-07-02 07:39:41,806 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:39:41,807 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:39:41,807 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:39:41,807 - INFO - joeynmt.training - \tHypothesis: When he heard that he had said to him , he went out and called Mary , and said to him , “ Rabbi , here , and come here here here . ”\n",
      "2021-07-02 07:39:41,808 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:39:41,808 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:39:41,808 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:39:41,808 - INFO - joeynmt.training - \tHypothesis: Now when he had come , a city was near the Sabbath ,\n",
      "2021-07-02 07:39:41,809 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:39:41,809 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:39:41,809 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:39:41,809 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a little farther . And when he was taken up , he was scorrred by the fire of the fire . And he departed and departed .\n",
      "2021-07-02 07:39:41,809 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    14400: bleu:   9.56, loss: 97846.3203, ppl:  20.1122, duration: 20.0306s\n",
      "2021-07-02 07:39:47,960 - INFO - joeynmt.training - Epoch  25, Step:    14500, Batch Loss:     1.732011, Tokens per Sec:    11589, Lr: 0.000103\n",
      "2021-07-02 07:39:49,035 - INFO - joeynmt.training - Epoch  25: total training loss 486.72\n",
      "2021-07-02 07:39:49,035 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-07-02 07:39:54,109 - INFO - joeynmt.training - Epoch  26, Step:    14600, Batch Loss:     1.530893, Tokens per Sec:    12176, Lr: 0.000103\n",
      "2021-07-02 07:40:13,929 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:40:13,930 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:40:13,930 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:40:14,939 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:40:14,940 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:40:14,940 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:40:14,940 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ When I saw that he was going out to the lake , and washed with water and put my hands on my feet . ”\n",
      "2021-07-02 07:40:14,940 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:40:14,941 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:40:14,941 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:40:14,941 - INFO - joeynmt.training - \tHypothesis: When he heard that he had said these things , Mary took her out , and went out to her , and said to her , “ Rabbi , we have compassion on us . ”\n",
      "2021-07-02 07:40:14,941 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:40:14,942 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:40:14,942 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:40:14,942 - INFO - joeynmt.training - \tHypothesis: Now a certain man was near , a day of the Sabbath was near .\n",
      "2021-07-02 07:40:14,942 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:40:14,943 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:40:14,943 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:40:14,943 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a vision . And when he was taken up , he was a scorrant and fell on his right hand . And he departed and sat down .\n",
      "2021-07-02 07:40:14,943 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step    14600: bleu:   9.37, loss: 97890.8828, ppl:  20.1398, duration: 20.8338s\n",
      "2021-07-02 07:40:20,870 - INFO - joeynmt.training - Epoch  26, Step:    14700, Batch Loss:     1.940008, Tokens per Sec:    12135, Lr: 0.000103\n",
      "2021-07-02 07:40:26,175 - INFO - joeynmt.training - Epoch  26: total training loss 479.09\n",
      "2021-07-02 07:40:26,176 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-07-02 07:40:27,283 - INFO - joeynmt.training - Epoch  27, Step:    14800, Batch Loss:     1.611627, Tokens per Sec:    12008, Lr: 0.000103\n",
      "2021-07-02 07:40:46,480 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:40:46,480 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:40:46,480 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:40:47,465 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:40:47,465 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:40:47,466 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:40:47,466 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ When He had come , I saw the lord and story in the midst of the cloud , I will send you up my feet . ”\n",
      "2021-07-02 07:40:47,466 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:40:47,467 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:40:47,467 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:40:47,467 - INFO - joeynmt.training - \tHypothesis: When he had said these things , he went out to the girl , and Mary Magdalene , and said to her , “ Rabbi , we have compassion on us . ”\n",
      "2021-07-02 07:40:47,467 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:40:47,468 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:40:47,468 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:40:47,468 - INFO - joeynmt.training - \tHypothesis: Now it was , when he had taken a day , the next day , that he was near .\n",
      "2021-07-02 07:40:47,468 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:40:47,468 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:40:47,469 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:40:47,469 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a vision . And when he was taken up , he was scorched , and a fire was cast out of his hand .\n",
      "2021-07-02 07:40:47,469 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    14800: bleu:   9.43, loss: 97739.0000, ppl:  20.0461, duration: 20.1854s\n",
      "2021-07-02 07:40:53,379 - INFO - joeynmt.training - Epoch  27, Step:    14900, Batch Loss:     1.419625, Tokens per Sec:    12123, Lr: 0.000103\n",
      "2021-07-02 07:40:59,770 - INFO - joeynmt.training - Epoch  27, Step:    15000, Batch Loss:     1.886205, Tokens per Sec:    11354, Lr: 0.000103\n",
      "2021-07-02 07:41:19,110 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:41:19,110 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:41:19,110 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:41:20,088 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:41:20,089 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:41:20,089 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:41:20,089 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ If anyone had been in the boat , he would have power to you in the midst of the cut of the cup and finging . ”\n",
      "2021-07-02 07:41:20,089 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:41:20,090 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:41:20,090 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:41:20,090 - INFO - joeynmt.training - \tHypothesis: When he heard that he had said to him , Mary Magdalene , Mary the mother of her mother , and said to him , “ Rabbi , we have compassion on your left . ”\n",
      "2021-07-02 07:41:20,090 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:41:20,091 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:41:20,091 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:41:20,091 - INFO - joeynmt.training - \tHypothesis: Now a certain man was sick , that he was near the Sabbath .\n",
      "2021-07-02 07:41:20,091 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:41:20,091 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:41:20,092 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:41:20,092 - INFO - joeynmt.training - \tHypothesis: And he was hungry , and was thirty years . And when he was cast out his heads , and it was given to his heads . And he went out of his hand and sat down .\n",
      "2021-07-02 07:41:20,092 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    15000: bleu:   9.49, loss: 97962.0078, ppl:  20.1837, duration: 20.3212s\n",
      "2021-07-02 07:41:22,962 - INFO - joeynmt.training - Epoch  27: total training loss 472.19\n",
      "2021-07-02 07:41:22,962 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-07-02 07:41:26,065 - INFO - joeynmt.training - Epoch  28, Step:    15100, Batch Loss:     1.984432, Tokens per Sec:    11890, Lr: 0.000103\n",
      "2021-07-02 07:41:32,432 - INFO - joeynmt.training - Epoch  28, Step:    15200, Batch Loss:     2.022751, Tokens per Sec:    11327, Lr: 0.000103\n",
      "2021-07-02 07:41:51,459 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:41:51,460 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:41:51,460 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:41:52,478 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:41:52,479 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:41:52,479 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:41:52,479 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ When he had come , he fell down and found it in a pool , and said to them , “ Come and see me , and I will see my finger . ”\n",
      "2021-07-02 07:41:52,480 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:41:52,480 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:41:52,480 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:41:52,480 - INFO - joeynmt.training - \tHypothesis: When he heard that he had seen the servants , he went out to the girl , and said to her , “ Rabbi , here ! ”\n",
      "2021-07-02 07:41:52,481 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:41:52,481 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:41:52,481 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:41:52,481 - INFO - joeynmt.training - \tHypothesis: Now a certain day was about to the Jews , of the Day was near .\n",
      "2021-07-02 07:41:52,482 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:41:52,482 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:41:52,482 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:41:52,482 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a little while . And when he was taken up , he was scorrayed with fire . And he was cast out of his hand and departed .\n",
      "2021-07-02 07:41:52,483 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step    15200: bleu:   9.44, loss: 98041.2969, ppl:  20.2329, duration: 20.0504s\n",
      "2021-07-02 07:41:58,379 - INFO - joeynmt.training - Epoch  28, Step:    15300, Batch Loss:     1.964754, Tokens per Sec:    12390, Lr: 0.000103\n",
      "2021-07-02 07:41:59,264 - INFO - joeynmt.training - Epoch  28: total training loss 470.07\n",
      "2021-07-02 07:41:59,264 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-07-02 07:42:04,738 - INFO - joeynmt.training - Epoch  29, Step:    15400, Batch Loss:     1.500076, Tokens per Sec:    11186, Lr: 0.000103\n",
      "2021-07-02 07:42:24,066 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:42:24,067 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:42:24,067 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:42:25,036 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:42:25,037 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:42:25,037 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:42:25,037 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ When He had come , he fell down at the feet of the feet and annount , I will see you with compassion on my feet . ”\n",
      "2021-07-02 07:42:25,038 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:42:25,038 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:42:25,038 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:42:25,039 - INFO - joeynmt.training - \tHypothesis: When he had said these things , he went to the house of Mary , and sat down with her mother , and said to her , “ Rabbi , we are with compassion . ”\n",
      "2021-07-02 07:42:25,039 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:42:25,039 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:42:25,039 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:42:25,040 - INFO - joeynmt.training - \tHypothesis: Now a certain day was about to the week , that day of the Sabbath was near .\n",
      "2021-07-02 07:42:25,040 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:42:25,040 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:42:25,040 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:42:25,040 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a fig around his hand . And when he was cast out of his garment , and it was given to him by a fire . And he went out of his hand and healed him .\n",
      "2021-07-02 07:42:25,041 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    15400: bleu:   9.49, loss: 98053.4609, ppl:  20.2404, duration: 20.3021s\n",
      "2021-07-02 07:42:30,947 - INFO - joeynmt.training - Epoch  29, Step:    15500, Batch Loss:     1.962438, Tokens per Sec:    12200, Lr: 0.000072\n",
      "2021-07-02 07:42:36,292 - INFO - joeynmt.training - Epoch  29: total training loss 467.99\n",
      "2021-07-02 07:42:36,292 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-07-02 07:42:37,333 - INFO - joeynmt.training - Epoch  30, Step:    15600, Batch Loss:     1.845224, Tokens per Sec:    11926, Lr: 0.000072\n",
      "2021-07-02 07:42:56,959 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:42:56,959 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:42:56,960 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:42:58,409 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:42:58,409 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:42:58,409 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:42:58,410 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to them , “ When He had come , He will send them out to the pool , and washed with water ; and when I come , I will see you . ”\n",
      "2021-07-02 07:42:58,410 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:42:58,410 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:42:58,410 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:42:58,410 - INFO - joeynmt.training - \tHypothesis: When he had said these things , he went to the house and sat at the house of Mary , and said to her , “ Rabbi , we have compassion on us . ”\n",
      "2021-07-02 07:42:58,411 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:42:58,411 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:42:58,411 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:42:58,411 - INFO - joeynmt.training - \tHypothesis: Now when he had come , a city was near to the Sabbath .\n",
      "2021-07-02 07:42:58,412 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:42:58,412 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:42:58,412 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:42:58,413 - INFO - joeynmt.training - \tHypothesis: Then Paul stood by the fig tree . And when he was taken up , he was a scorrant oil of fire . And he went out of his hand and sat down and sat down .\n",
      "2021-07-02 07:42:58,413 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    15600: bleu:   9.55, loss: 98066.8125, ppl:  20.2487, duration: 21.0787s\n",
      "2021-07-02 07:43:04,229 - INFO - joeynmt.training - Epoch  30, Step:    15700, Batch Loss:     1.497919, Tokens per Sec:    11840, Lr: 0.000072\n",
      "2021-07-02 07:43:10,686 - INFO - joeynmt.training - Epoch  30, Step:    15800, Batch Loss:     1.817172, Tokens per Sec:    11481, Lr: 0.000072\n",
      "2021-07-02 07:43:29,365 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:43:29,365 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:43:29,365 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:43:30,335 - INFO - joeynmt.training - Example #0\n",
      "2021-07-02 07:43:30,336 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-02 07:43:30,336 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-02 07:43:30,336 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees answered and said to him , “ When He had come , he fell down and found it in the midst of the stretch , and washed me at my feet . ”\n",
      "2021-07-02 07:43:30,337 - INFO - joeynmt.training - Example #1\n",
      "2021-07-02 07:43:30,337 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-02 07:43:30,337 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-02 07:43:30,337 - INFO - joeynmt.training - \tHypothesis: When he had said these things , he went to the house of Mary , and called Mary , and said to her , “ Rabbi , we have compassion on us . ”\n",
      "2021-07-02 07:43:30,338 - INFO - joeynmt.training - Example #2\n",
      "2021-07-02 07:43:30,338 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-02 07:43:30,338 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-02 07:43:30,338 - INFO - joeynmt.training - \tHypothesis: Now it was , when he had passed through the Sabbath , that He was near the Sabbath .\n",
      "2021-07-02 07:43:30,339 - INFO - joeynmt.training - Example #3\n",
      "2021-07-02 07:43:30,339 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-02 07:43:30,339 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-02 07:43:30,339 - INFO - joeynmt.training - \tHypothesis: Then Paul was filled with a fig around his news . And when he was cast out of the fire , he departed and sat down and sat down with his hand .\n",
      "2021-07-02 07:43:30,340 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    15800: bleu:   9.61, loss: 98186.2812, ppl:  20.3231, duration: 19.6535s\n",
      "2021-07-02 07:43:33,516 - INFO - joeynmt.training - Epoch  30: total training loss 465.75\n",
      "2021-07-02 07:43:33,516 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-07-02 07:43:33,517 - INFO - joeynmt.training - Best validation result (greedy) at step    11800:  19.59 ppl.\n",
      "2021-07-02 07:43:33,536 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-07-02 07:43:33,893 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-02 07:43:34,077 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-02 07:43:34,138 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.en)...\n",
      "2021-07-02 07:44:01,422 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:44:01,423 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:44:01,423 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:44:01,724 - INFO - joeynmt.prediction -  dev bleu[13a]:   9.40 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-02 07:44:01,729 - INFO - joeynmt.prediction - Translations saved to: models/lhen_reverse_transformer_continued/00011800.hyps.dev\n",
      "2021-07-02 07:44:01,729 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.en)...\n",
      "2021-07-02 07:44:28,845 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:44:28,845 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:44:28,846 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:44:29,140 - INFO - joeynmt.prediction - test bleu[13a]:   8.98 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-02 07:44:29,145 - INFO - joeynmt.prediction - Translations saved to: models/lhen_reverse_transformer_continued/00011800.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_lhen_reload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bgrg-2UJGGZp",
    "outputId": "71f21ffa-923a-4d34-c2d2-04cab276489a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 8200\tLoss: 98958.71094\tPPL: 20.81034\tbleu: 7.14155\tLR: 0.00030000\t\n",
      "Steps: 8400\tLoss: 98222.44531\tPPL: 20.34563\tbleu: 7.26749\tLR: 0.00030000\t*\n",
      "Steps: 8600\tLoss: 97980.10156\tPPL: 20.19495\tbleu: 7.29421\tLR: 0.00030000\t*\n",
      "Steps: 8800\tLoss: 98038.94531\tPPL: 20.23143\tbleu: 7.51194\tLR: 0.00030000\t\n",
      "Steps: 9000\tLoss: 98010.07812\tPPL: 20.21353\tbleu: 7.45777\tLR: 0.00030000\t\n",
      "Steps: 9200\tLoss: 97764.75781\tPPL: 20.06199\tbleu: 7.71418\tLR: 0.00030000\t*\n",
      "Steps: 9400\tLoss: 97437.90625\tPPL: 19.86186\tbleu: 7.85631\tLR: 0.00030000\t*\n",
      "Steps: 9600\tLoss: 97703.65625\tPPL: 20.02443\tbleu: 7.42886\tLR: 0.00030000\t\n",
      "Steps: 9800\tLoss: 97414.12500\tPPL: 19.84737\tbleu: 7.80404\tLR: 0.00030000\t*\n",
      "Steps: 10000\tLoss: 97413.65625\tPPL: 19.84709\tbleu: 8.32098\tLR: 0.00030000\t*\n",
      "Steps: 10200\tLoss: 97024.35938\tPPL: 19.61150\tbleu: 8.29478\tLR: 0.00030000\t*\n",
      "Steps: 10400\tLoss: 97337.35156\tPPL: 19.80069\tbleu: 8.13847\tLR: 0.00030000\t\n",
      "Steps: 10600\tLoss: 97628.63281\tPPL: 19.97840\tbleu: 8.56480\tLR: 0.00030000\t\n",
      "Steps: 10800\tLoss: 97078.13281\tPPL: 19.64387\tbleu: 8.48819\tLR: 0.00030000\t\n",
      "Steps: 11000\tLoss: 97145.12500\tPPL: 19.68428\tbleu: 8.23039\tLR: 0.00030000\t\n",
      "Steps: 11200\tLoss: 97275.30469\tPPL: 19.76304\tbleu: 8.46530\tLR: 0.00030000\t\n",
      "Steps: 11400\tLoss: 97219.12500\tPPL: 19.72901\tbleu: 8.48884\tLR: 0.00021000\t\n",
      "Steps: 11600\tLoss: 97157.93750\tPPL: 19.69202\tbleu: 8.46411\tLR: 0.00021000\t\n",
      "Steps: 11800\tLoss: 96984.31250\tPPL: 19.58742\tbleu: 8.94221\tLR: 0.00021000\t*\n",
      "Steps: 12000\tLoss: 97259.57031\tPPL: 19.75351\tbleu: 8.66493\tLR: 0.00021000\t\n",
      "Steps: 12200\tLoss: 97523.31250\tPPL: 19.91396\tbleu: 8.72192\tLR: 0.00021000\t\n",
      "Steps: 12400\tLoss: 97310.82812\tPPL: 19.78458\tbleu: 8.87165\tLR: 0.00021000\t\n",
      "Steps: 12600\tLoss: 97050.79688\tPPL: 19.62741\tbleu: 8.76209\tLR: 0.00021000\t\n",
      "Steps: 12800\tLoss: 97525.49219\tPPL: 19.91529\tbleu: 8.72476\tLR: 0.00021000\t\n",
      "Steps: 13000\tLoss: 97521.99219\tPPL: 19.91315\tbleu: 9.20420\tLR: 0.00014700\t\n",
      "Steps: 13200\tLoss: 97359.67188\tPPL: 19.81425\tbleu: 9.42658\tLR: 0.00014700\t\n",
      "Steps: 13400\tLoss: 97444.21875\tPPL: 19.86570\tbleu: 9.38554\tLR: 0.00014700\t\n",
      "Steps: 13600\tLoss: 97609.97656\tPPL: 19.96696\tbleu: 9.47792\tLR: 0.00014700\t\n",
      "Steps: 13800\tLoss: 97884.03906\tPPL: 20.13553\tbleu: 9.21236\tLR: 0.00014700\t\n",
      "Steps: 14000\tLoss: 97753.49219\tPPL: 20.05506\tbleu: 9.26378\tLR: 0.00014700\t\n",
      "Steps: 14200\tLoss: 97866.98438\tPPL: 20.12500\tbleu: 9.40614\tLR: 0.00010290\t\n",
      "Steps: 14400\tLoss: 97846.32031\tPPL: 20.11224\tbleu: 9.55502\tLR: 0.00010290\t\n",
      "Steps: 14600\tLoss: 97890.88281\tPPL: 20.13976\tbleu: 9.36628\tLR: 0.00010290\t\n",
      "Steps: 14800\tLoss: 97739.00000\tPPL: 20.04615\tbleu: 9.42833\tLR: 0.00010290\t\n",
      "Steps: 15000\tLoss: 97962.00781\tPPL: 20.18374\tbleu: 9.49214\tLR: 0.00010290\t\n",
      "Steps: 15200\tLoss: 98041.29688\tPPL: 20.23289\tbleu: 9.44077\tLR: 0.00010290\t\n",
      "Steps: 15400\tLoss: 98053.46094\tPPL: 20.24044\tbleu: 9.48913\tLR: 0.00007203\t\n",
      "Steps: 15600\tLoss: 98066.81250\tPPL: 20.24873\tbleu: 9.54606\tLR: 0.00007203\t\n",
      "Steps: 15800\tLoss: 98186.28125\tPPL: 20.32307\tbleu: 9.60719\tLR: 0.00007203\t\n"
     ]
    }
   ],
   "source": [
    "! cat \"joeynmt/models/lhen_reverse_transformer_continued/validations.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_LgpPwjNPd0",
    "outputId": "580e7893-afed-4707-8f43-decdabf78146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-02 07:54:01,679 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-02 07:54:01,684 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-02 07:54:01,917 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-02 07:54:01,930 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-02 07:54:01,942 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-02 07:54:01,972 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-07-02 07:54:05,647 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-02 07:54:05,838 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-02 07:54:05,903 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.en)...\n",
      "2021-07-02 07:54:31,781 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:54:31,781 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:54:31,781 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:54:32,096 - INFO - joeynmt.prediction -  dev bleu[13a]:  10.47 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-02 07:54:32,096 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.en)...\n",
      "2021-07-02 07:54:57,916 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-02 07:54:57,917 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-02 07:54:57,917 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-02 07:54:58,206 - INFO - joeynmt.prediction - test bleu[13a]:  10.39 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt test 'models/lhen_reverse_transformer_continued/config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhpFL7a8yfqb"
   },
   "source": [
    "### Reverse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "vkwaVphVNc2T"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (source_language, target_language3)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{source_language}{target_language3}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language3}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"joeynmt/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 1096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 200         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: False\n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\", source_language=source_language, target_language3=target_language3)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXxz_b-21ZY6",
    "outputId": "2674fd41-8f85-4f15-e22a-8921ba249c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-10 10:01:14,809 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-10 10:01:14,859 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-10 10:01:16,638 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-10 10:01:17,391 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-10 10:01:18,749 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-10 10:01:20,134 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-10 10:01:20,134 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 10:01:20,341 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 10:01:20.573978: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-10 10:01:22,413 - INFO - joeynmt.training - Total params: 12097024\n",
      "2021-07-10 10:01:25,778 - INFO - joeynmt.helpers - cfg.name                           : enlh_transformer\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.trg                       : lh\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\n",
      "2021-07-10 10:01:25,779 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-10 10:01:25,780 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-10 10:01:25,781 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1096\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-10 10:01:25,782 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 200\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/enlh_transformer\n",
      "2021-07-10 10:01:25,783 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-10 10:01:25,784 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-10 10:01:25,785 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-10 10:01:25,786 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-10 10:01:25,787 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 5904,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "\t[TRG] Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - Number of Src words (types): 4050\n",
      "2021-07-10 10:01:25,788 - INFO - joeynmt.helpers - Number of Trg words (types): 4050\n",
      "2021-07-10 10:01:25,789 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4050),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4050))\n",
      "2021-07-10 10:01:25,807 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 1096\n",
      "\ttotal batch size (w. parallel & accumulation): 1096\n",
      "2021-07-10 10:01:25,808 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-10 10:01:31,873 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.710139, Tokens per Sec:    11926, Lr: 0.000300\n",
      "2021-07-10 10:01:37,775 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.553401, Tokens per Sec:    12046, Lr: 0.000300\n",
      "2021-07-10 10:02:17,566 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:02:17,567 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:02:18,261 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:02:18,262 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:02:18,262 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:02:18,262 - INFO - joeynmt.training - \tHypothesis: Ne “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ , “ , “ “ “ “ , “ , “ , “ , “ , “ , “ , “ , “ , “ , “ , “ “ , “ , “ , Ne , “ , Ne , , , Ne , “ , “ , “ , “ , “ , “ , , , , , , , , Ne , “ , “ , “ , “ , “ , , , “ “ , “ , “\n",
      "2021-07-10 10:02:18,262 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:02:18,263 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:02:18,264 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:02:18,264 - INFO - joeynmt.training - \tHypothesis: Ne “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ , “ , “ “ “ “ , “ , “ , “ , “ , “ , “ , “ , “ , Ne , “ , “ , “ , “ , Ne , Ne , , , , Ne , Ne , Ne , Ne , Ne , Ne , Ne , , ,\n",
      "2021-07-10 10:02:18,264 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:02:18,265 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:02:18,265 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:02:18,265 - INFO - joeynmt.training - \tHypothesis: Ne “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ , “ “ “ “ “ ,\n",
      "2021-07-10 10:02:18,265 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:02:18,266 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:02:18,266 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:02:18,266 - INFO - joeynmt.training - \tHypothesis: Ne “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ , “ , “ “ “ “ , “ , “ , “ , “ , “ , “ , “ , “ , “ , “ , “ “ , “ , “ , “ , Ne , , , , Ne , “ , “ , “ , “ , “ , “ , , , , , , , , , “ , “ “ , “ , “ , “ , , , “ “ “ , “ , “\n",
      "2021-07-10 10:02:18,266 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      200: bleu:   0.04, loss: 204323.4844, ppl: 250.9473, duration: 40.4913s\n",
      "2021-07-10 10:02:24,502 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.236579, Tokens per Sec:    11311, Lr: 0.000300\n",
      "2021-07-10 10:02:24,729 - INFO - joeynmt.training - Epoch   1: total training loss 1716.58\n",
      "2021-07-10 10:02:24,729 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-10 10:02:30,935 - INFO - joeynmt.training - Epoch   2, Step:      400, Batch Loss:     5.197055, Tokens per Sec:    11092, Lr: 0.000300\n",
      "2021-07-10 10:03:13,946 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:03:13,947 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:03:14,652 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:03:14,654 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:03:14,654 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:03:14,654 - INFO - joeynmt.training - \tHypothesis: Ne , “ “ “ “ “ Ne , “ “ Ne , “ Ne , “ Ne , “ “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , Ne , “ Ne , “ Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , “ Ne\n",
      "2021-07-10 10:03:14,654 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:03:14,655 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:03:14,655 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:03:14,655 - INFO - joeynmt.training - \tHypothesis: Ne , “ “ “ “ “ Ne , “ “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne , Ne , “ Ne , Ne , Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , Ne , Ne , Ne , Ne , “ Ne , Ne , Ne , “ Ne , Ne , Ne , Ne , “ Ne , “ Ne ,\n",
      "2021-07-10 10:03:14,655 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:03:14,656 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:03:14,656 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:03:14,656 - INFO - joeynmt.training - \tHypothesis: Ne , “ “ “ Ne , “ “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne ,\n",
      "2021-07-10 10:03:14,656 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:03:14,657 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:03:14,657 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:03:14,657 - INFO - joeynmt.training - \tHypothesis: Ne , “ “ “ “ “ “ Ne , “ “ “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne , “ Ne , “ Ne , Ne , “ Ne , “ Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , “ Ne , “ Ne , Ne , Ne , “ Ne , “ Ne , “\n",
      "2021-07-10 10:03:14,657 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      400: bleu:   0.03, loss: 196611.8594, ppl: 203.7122, duration: 43.7214s\n",
      "2021-07-10 10:03:20,614 - INFO - joeynmt.training - Epoch   2, Step:      500, Batch Loss:     5.240234, Tokens per Sec:    11718, Lr: 0.000300\n",
      "2021-07-10 10:03:27,086 - INFO - joeynmt.training - Epoch   2, Step:      600, Batch Loss:     5.081684, Tokens per Sec:    10842, Lr: 0.000300\n",
      "2021-07-10 10:04:10,467 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:04:10,468 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:04:11,158 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:04:11,159 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:04:11,159 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:04:11,160 - INFO - joeynmt.training - \tHypothesis: Kho ari , “ “ “ “ “ “ “ “ “ “ “ Kho ari , “ “ “ “ “ “ Kho ari , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ Kho , “ “ “ Kho , “ Kho , “ “ Kho , “ “ “ “ “ Kho , “ Kho , “ “ Kho , “ Kho , “ Kho , “ “ Kho , “ Kho , “ “ “ Kho , “ Kho ari , “ “ “ “ “ “ Kho ari\n",
      "2021-07-10 10:04:11,160 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:04:11,160 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:04:11,160 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - \tHypothesis: Kho ari , “ “ “ “ “ “ “ “ “ “ “ “ Kho ari , “ “ “ “ “ “ Kho ari , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ Kho , “ “ “ “ Kho , “ “ “ “ “ Kho , “ “ “ “ Kho , “ Kho , “ “ Kho , “ Kho , “ Kho , “ “ “ Kho , “ Kho ari , “ Kho , “ Kho ari , “ “ “ “ “ “ Kho ari\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:04:11,161 - INFO - joeynmt.training - \tHypothesis: Ne , , , , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , ne , ne , ne , ne , ne , ne , ne , ne ,\n",
      "2021-07-10 10:04:11,162 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:04:11,162 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:04:11,162 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:04:11,162 - INFO - joeynmt.training - \tHypothesis: Ne , , , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , “ Kho , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne ,\n",
      "2021-07-10 10:04:11,163 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      600: bleu:   0.20, loss: 185051.2031, ppl: 149.0213, duration: 44.0762s\n",
      "2021-07-10 10:04:11,850 - INFO - joeynmt.training - Epoch   2: total training loss 1609.12\n",
      "2021-07-10 10:04:11,850 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-10 10:04:17,114 - INFO - joeynmt.training - Epoch   3, Step:      700, Batch Loss:     4.843796, Tokens per Sec:    12053, Lr: 0.000300\n",
      "2021-07-10 10:04:23,508 - INFO - joeynmt.training - Epoch   3, Step:      800, Batch Loss:     4.941718, Tokens per Sec:    11057, Lr: 0.000300\n",
      "2021-07-10 10:05:06,621 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:05:06,621 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:05:06,621 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:05:06,870 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:05:06,871 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:05:08,004 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:05:08,005 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:05:08,005 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:05:08,005 - INFO - joeynmt.training - \tHypothesis: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-10 10:05:08,005 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - \tHypothesis: “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:05:08,006 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , Nyasaye ,\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:05:08,007 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa ,\n",
      "2021-07-10 10:05:08,008 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step      800: bleu:   0.04, loss: 180492.9688, ppl: 131.7395, duration: 44.4991s\n",
      "2021-07-10 10:05:14,033 - INFO - joeynmt.training - Epoch   3, Step:      900, Batch Loss:     4.920640, Tokens per Sec:    12033, Lr: 0.000300\n",
      "2021-07-10 10:05:15,228 - INFO - joeynmt.training - Epoch   3: total training loss 1494.00\n",
      "2021-07-10 10:05:15,228 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-10 10:05:20,459 - INFO - joeynmt.training - Epoch   4, Step:     1000, Batch Loss:     4.775295, Tokens per Sec:    11661, Lr: 0.000300\n",
      "2021-07-10 10:06:03,729 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:06:03,730 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:06:03,730 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:06:03,825 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:06:03,825 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:06:04,504 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:06:04,505 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:06:04,505 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:06:04,505 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu\n",
      "2021-07-10 10:06:04,505 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:06:04,506 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:06:04,506 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:06:04,506 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu , nababoolela ari , “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “ “\n",
      "2021-07-10 10:06:04,506 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:06:04,507 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:06:04,507 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:06:04,507 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa ,\n",
      "2021-07-10 10:06:04,507 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:06:04,508 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:06:04,508 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:06:04,508 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa , olwa ,\n",
      "2021-07-10 10:06:04,508 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     1000: bleu:   0.05, loss: 177280.9688, ppl: 120.7798, duration: 44.0486s\n",
      "2021-07-10 10:06:10,456 - INFO - joeynmt.training - Epoch   4, Step:     1100, Batch Loss:     4.727084, Tokens per Sec:    11941, Lr: 0.000300\n",
      "2021-07-10 10:06:16,953 - INFO - joeynmt.training - Epoch   4, Step:     1200, Batch Loss:     4.852685, Tokens per Sec:    10960, Lr: 0.000300\n",
      "2021-07-10 10:07:00,015 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:07:00,015 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:07:00,016 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:07:00,377 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:07:00,377 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:07:01,056 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:07:01,056 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:07:01,057 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:07:01,057 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu\n",
      "2021-07-10 10:07:01,057 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:07:01,058 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:07:01,058 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:07:01,058 - INFO - joeynmt.training - \tHypothesis: Mana , nababoolela ari , “ Nranga , ne , “ Ntsanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne\n",
      "2021-07-10 10:07:01,058 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , olwa , olwa , nibayanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne ,\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:07:01,059 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:07:01,060 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:07:01,060 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , olwa , btsanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , btsilililililililililililililililililililililililililililililililililililililililil@@\n",
      "2021-07-10 10:07:01,060 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     1200: bleu:   0.14, loss: 174162.8906, ppl: 111.0134, duration: 44.1068s\n",
      "2021-07-10 10:07:02,301 - INFO - joeynmt.training - Epoch   4: total training loss 1464.13\n",
      "2021-07-10 10:07:02,301 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-10 10:07:07,077 - INFO - joeynmt.training - Epoch   5, Step:     1300, Batch Loss:     4.396784, Tokens per Sec:    11852, Lr: 0.000300\n",
      "2021-07-10 10:07:13,526 - INFO - joeynmt.training - Epoch   5, Step:     1400, Batch Loss:     4.548765, Tokens per Sec:    11202, Lr: 0.000300\n",
      "2021-07-10 10:07:56,671 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:07:56,672 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:07:56,672 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:07:56,994 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:07:56,994 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:07:57,665 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:07:57,666 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:07:57,666 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:07:57,666 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu Yesu Yesu Yesu Yesu Yesu , Yesu , Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu ari , “ Nboolela , “ Omwami , ne , wwalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalal@@\n",
      "2021-07-10 10:07:57,666 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:07:57,667 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:07:57,667 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:07:57,667 - INFO - joeynmt.training - \tHypothesis: Mana , nibakalusia ari , “ Nranga , ne , owenya , ne , ne , owranga , ne , ne , owranga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , owranga , ne , owranga , owranga . ”\n",
      "2021-07-10 10:07:57,667 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - \tHypothesis: Mana , nibenya , nibenya , ne , nibenya , ne , ne , nibalalalalalalalalalalalalalalalalalalalalalalalalalala. ,\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:07:57,668 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:07:57,669 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:07:57,669 - INFO - joeynmt.training - \tHypothesis: Ne , olwa , basanga , ne , nibasanga , ne , ne , nibasanga , ne , ne , nibasanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , basanga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , branga ,\n",
      "2021-07-10 10:07:57,669 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     1400: bleu:   0.86, loss: 171274.1875, ppl: 102.6716, duration: 44.1429s\n",
      "2021-07-10 10:08:03,649 - INFO - joeynmt.training - Epoch   5, Step:     1500, Batch Loss:     4.469375, Tokens per Sec:    11930, Lr: 0.000300\n",
      "2021-07-10 10:08:05,020 - INFO - joeynmt.training - Epoch   5: total training loss 1409.95\n",
      "2021-07-10 10:08:05,021 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-10 10:08:10,176 - INFO - joeynmt.training - Epoch   6, Step:     1600, Batch Loss:     4.660630, Tokens per Sec:    10445, Lr: 0.000300\n",
      "2021-07-10 10:08:53,189 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:08:53,189 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:08:53,190 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:08:53,503 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:08:53,504 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:08:54,203 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:08:54,204 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:08:54,204 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:08:54,204 - INFO - joeynmt.training - \tHypothesis: Mana Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu Yesu , nababoolela ari , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi , “ N. ”\n",
      "2021-07-10 10:08:54,204 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:08:54,205 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:08:54,205 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:08:54,205 - INFO - joeynmt.training - \tHypothesis: Mana Yesu nababoolela ari , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi\n",
      "2021-07-10 10:08:54,205 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:08:54,206 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:08:54,206 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:08:54,206 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibamanya mbu , “ Omundu , baboolile , ne , ne , ne , ne , ne , nibaranga .\n",
      "2021-07-10 10:08:54,206 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:08:54,207 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:08:54,207 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:08:54,207 - INFO - joeynmt.training - \tHypothesis: Ne olwa , olwa , nibaranga , ne , ne , ne , ne , nibaranga , ne , ne , ne , ne , ne , ne , nibaranga , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , ne , nibaranga , baranga .\n",
      "2021-07-10 10:08:54,207 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     1600: bleu:   0.78, loss: 168059.3906, ppl:  94.1230, duration: 44.0312s\n",
      "2021-07-10 10:09:00,263 - INFO - joeynmt.training - Epoch   6, Step:     1700, Batch Loss:     4.814085, Tokens per Sec:    11716, Lr: 0.000300\n",
      "2021-07-10 10:09:06,775 - INFO - joeynmt.training - Epoch   6, Step:     1800, Batch Loss:     4.419515, Tokens per Sec:    10809, Lr: 0.000300\n",
      "2021-07-10 10:09:48,977 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:09:48,978 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:09:48,978 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:09:49,257 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:09:49,257 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:09:50,342 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:09:50,343 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:09:50,343 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:09:50,343 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali yali yali yali yali yali yali , nababoolela ari , “ Neeeeeeeeeeeeeeeiiiio. ”\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali yali yali yali yali yali yali yali yali yali yali yali yali yali yali yali yali yali , naboola ari , “ Nalalalalalalalalalalalo. ”\n",
      "2021-07-10 10:09:50,344 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:09:50,345 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:09:50,345 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:09:50,345 - INFO - joeynmt.training - \tHypothesis: Mana , nibaboolela mbu , “ Neeeeeayo , nibirila .\n",
      "2021-07-10 10:09:50,345 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:09:50,346 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:09:50,346 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:09:50,346 - INFO - joeynmt.training - \tHypothesis: Mana , nibaria , nibiria , ne , nibiria , ne , nibiria , ne , nibiria , ne , nibiria , ne , nibaria .\n",
      "2021-07-10 10:09:50,346 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     1800: bleu:   1.46, loss: 165015.8750, ppl:  86.6867, duration: 43.5705s\n",
      "2021-07-10 10:09:52,123 - INFO - joeynmt.training - Epoch   6: total training loss 1388.87\n",
      "2021-07-10 10:09:52,123 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-10 10:09:56,229 - INFO - joeynmt.training - Epoch   7, Step:     1900, Batch Loss:     4.167235, Tokens per Sec:    12083, Lr: 0.000300\n",
      "2021-07-10 10:10:02,623 - INFO - joeynmt.training - Epoch   7, Step:     2000, Batch Loss:     4.246118, Tokens per Sec:    11227, Lr: 0.000300\n",
      "2021-07-10 10:10:45,607 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:10:45,608 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:10:45,608 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:10:45,911 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:10:45,912 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:10:46,605 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:10:46,606 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:10:46,606 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:10:46,606 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali , nibareeba ari , “ Nalalalaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaawe . ”\n",
      "2021-07-10 10:10:46,606 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:10:46,607 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:10:46,607 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:10:46,607 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali yali yali yali yali , naboolela , naboola ari , “ Nalalaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaawe . ”\n",
      "2021-07-10 10:10:46,608 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:10:46,609 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:10:46,609 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:10:46,609 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibaboolanga , ne , nibaboola , ne , nibahelesia .\n",
      "2021-07-10 10:10:46,609 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:10:46,610 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:10:46,610 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:10:46,610 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibaranga , nibiri , ne , nibiri , ne , nibaria , ne , nibaria , ne , nibaria , ne , nibaria , ne , nibaria , ne , nibaria .\n",
      "2021-07-10 10:10:46,610 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     2000: bleu:   1.23, loss: 162329.7500, ppl:  80.6133, duration: 43.9875s\n",
      "2021-07-10 10:10:52,618 - INFO - joeynmt.training - Epoch   7, Step:     2100, Batch Loss:     4.309657, Tokens per Sec:    12012, Lr: 0.000300\n",
      "2021-07-10 10:10:54,572 - INFO - joeynmt.training - Epoch   7: total training loss 1332.86\n",
      "2021-07-10 10:10:54,572 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-10 10:10:59,134 - INFO - joeynmt.training - Epoch   8, Step:     2200, Batch Loss:     4.481798, Tokens per Sec:    10428, Lr: 0.000300\n",
      "2021-07-10 10:11:42,278 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:11:42,278 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:11:42,279 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:11:42,603 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:11:42,604 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:11:43,285 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:11:43,285 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:11:43,286 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:11:43,286 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali , nibareeba ari , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi\n",
      "2021-07-10 10:11:43,286 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:11:43,286 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - \tHypothesis: Mana , nibareeba ari , “ Nolwa , olwa , bareeba mbu , “ Nalile , ne , ne , ne , olwa , owanyanga , ne , ne , ne , ne olwa olwa , owanyie . ”\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:11:43,287 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibaana befwe , nibirila , nibirila , nibirila , nibirila .\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:11:43,288 - INFO - joeynmt.training - \tHypothesis: Mana , nibaana befwe , nibiri , nibiri , ne , nibiri , nibiru , ne , nibiru , ne , nibiru , ne , nibiru , ne , nibiru .\n",
      "2021-07-10 10:11:43,289 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     2200: bleu:   1.33, loss: 159668.2812, ppl:  75.0154, duration: 44.1539s\n",
      "2021-07-10 10:11:49,208 - INFO - joeynmt.training - Epoch   8, Step:     2300, Batch Loss:     4.213748, Tokens per Sec:    11908, Lr: 0.000300\n",
      "2021-07-10 10:11:55,842 - INFO - joeynmt.training - Epoch   8, Step:     2400, Batch Loss:     4.350402, Tokens per Sec:    10703, Lr: 0.000300\n",
      "2021-07-10 10:12:38,943 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:12:38,943 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:12:38,943 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:12:39,261 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:12:39,262 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:12:39,964 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:12:39,965 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:12:39,965 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:12:39,965 - INFO - joeynmt.training - \tHypothesis: Ne olwa , nibareeba bari , “ Nolwa , , , , Omwana wa , womuboolela mbu , “ Nalalalalalolie , womuboolela mbu , ‘ Nalalalalalalalie , womuboolela mbu , ‘ Nalalalalalalalalalalalalalalalalalalalalalalalalalalalalalalo. ”\n",
      "2021-07-10 10:12:39,965 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - \tHypothesis: Ne olwa , yali , yali , namuboolela ari , “ Nalalalile , mbu , ‘ Omwami , womuboolela mbu , ‘ Omwami , womuboolela mbu , ‘ Omwami . ”\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:12:39,966 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - \tHypothesis: Ne olwa , bali , nibareeba mbu , “ Abalila , abandu bandi , nibatiile .\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:12:39,967 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:12:39,968 - INFO - joeynmt.training - \tHypothesis: Ne olwa , yali , niyiri , yamini , ne , niyiri , niyiri , ne , niyiria , ne , niyiria , ne , nibarumi , nibaranga , ne , nibaranga .\n",
      "2021-07-10 10:12:39,968 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     2400: bleu:   2.22, loss: 157068.5938, ppl:  69.9229, duration: 44.1252s\n",
      "2021-07-10 10:12:42,291 - INFO - joeynmt.training - Epoch   8: total training loss 1307.90\n",
      "2021-07-10 10:12:42,291 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-10 10:12:46,047 - INFO - joeynmt.training - Epoch   9, Step:     2500, Batch Loss:     4.650863, Tokens per Sec:    11731, Lr: 0.000300\n",
      "2021-07-10 10:12:52,569 - INFO - joeynmt.training - Epoch   9, Step:     2600, Batch Loss:     4.045369, Tokens per Sec:    10815, Lr: 0.000300\n",
      "2021-07-10 10:13:35,497 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:13:35,498 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:13:35,498 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:13:35,806 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:13:35,806 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:13:36,475 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:13:36,476 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:13:36,476 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:13:36,476 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali , niyareeba ari , “ Omundu yesi yesi , ouranga , ne , owenya , ne , ne , ne , niyaboolela ari , “ Shimulenya , okhuranga , okhurula , okhuranga , ne , mulenya , okhuranga , ne , mulenya , ne , okhuranga . ”\n",
      "2021-07-10 10:13:36,476 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:13:36,477 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:13:36,477 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:13:36,477 - INFO - joeynmt.training - \tHypothesis: Mana , Petero , nasinjiile , ne , nababoolela ari , “ Nalie , ” Yesu , namuboolela ari , “ Omwami , ne , shiyamanya mbu , “ Omwami . ”\n",
      "2021-07-10 10:13:36,477 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - \tHypothesis: Ne olwa , bali nibareeba , nibareeba mbu , baliho , nibareeba , nibareeba bari ,\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:13:36,478 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:13:36,479 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:13:36,479 - INFO - joeynmt.training - \tHypothesis: Mana , yali niyenjile , ne , niyiria , ne , niyiria , ne , nibibili , ne , nibibili , ne , nibibili , ne , nibibili , nibibili , ne , nibibili .\n",
      "2021-07-10 10:13:36,479 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     2600: bleu:   2.16, loss: 154639.6875, ppl:  65.4778, duration: 43.9100s\n",
      "2021-07-10 10:13:42,362 - INFO - joeynmt.training - Epoch   9, Step:     2700, Batch Loss:     4.370677, Tokens per Sec:    12076, Lr: 0.000300\n",
      "2021-07-10 10:13:45,369 - INFO - joeynmt.training - Epoch   9: total training loss 1272.80\n",
      "2021-07-10 10:13:45,369 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-10 10:13:48,777 - INFO - joeynmt.training - Epoch  10, Step:     2800, Batch Loss:     3.925977, Tokens per Sec:    11813, Lr: 0.000300\n",
      "2021-07-10 10:14:31,847 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:14:31,848 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:14:31,848 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:14:32,168 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:14:32,169 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:14:33,271 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:14:33,272 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:14:33,272 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:14:33,272 - INFO - joeynmt.training - \tHypothesis: Mana Yesu , nahulilakhwo , ari , “ Omwami , ne , namuboolela ari , “ Omwami , shiboolela ari , “ Omwami , shiboolanga mbu , ‘ Omwami , ndaboolile , mbu , ‘ Omwami . ”\n",
      "2021-07-10 10:14:33,272 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tHypothesis: Mana Yesu , namuboolela ari , “ Omwami , namuboolela ari , “ Omwami , namuboolela ari , “ Omwami , iwe , iwe , iwe , iwe , iwe , wamwinjisia . ”\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:14:33,273 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - \tHypothesis: Ne olwa , abandu bahulilakhwo amakhuwa ako , banyoola , ne , nibaboolile .\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:14:33,274 - INFO - joeynmt.training - \tHypothesis: Mana Petero , nahulilakhwo amakhuwa ako , ne , yanyoola , ne , nanyoola , ne , nahonile , ne , nibaboolela ari , “ Abarula , ne , banyanga , banyanga , ne , bahonile .\n",
      "2021-07-10 10:14:33,275 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     2800: bleu:   2.34, loss: 152782.4688, ppl:  62.2706, duration: 44.4973s\n",
      "2021-07-10 10:14:39,318 - INFO - joeynmt.training - Epoch  10, Step:     2900, Batch Loss:     3.930479, Tokens per Sec:    11664, Lr: 0.000300\n",
      "2021-07-10 10:14:45,942 - INFO - joeynmt.training - Epoch  10, Step:     3000, Batch Loss:     4.256003, Tokens per Sec:    10974, Lr: 0.000300\n",
      "2021-07-10 10:15:29,079 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:15:29,080 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:15:29,080 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:15:29,393 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:15:29,394 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:15:30,077 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:15:30,078 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:15:30,078 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:15:30,078 - INFO - joeynmt.training - \tHypothesis: Mana Yesu , nabakalusia bari , “ Niwe , iwe , iwe , ne , ndenya , mbu , ‘ Niwe , ’ , ndenya , ne , ndenya , okhuboolela mbu , ‘ Shimulola , ’ , mwenya , okhuboolela mbu , ndenya . ”\n",
      "2021-07-10 10:15:30,079 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:15:30,079 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:15:30,079 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:15:30,080 - INFO - joeynmt.training - \tHypothesis: Mana , Petero , nasinjila , ne , naboolela Petero ari , “ Niwe , Omwana womundu , womundu , witsa , ’ ”\n",
      "2021-07-10 10:15:30,080 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:15:30,080 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:15:30,080 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:15:30,082 - INFO - joeynmt.training - \tHypothesis: Ne olwa , abandu batiila , nibatiila , ne nibanyoola , ne nibanyoola , ne nibanyoola .\n",
      "2021-07-10 10:15:30,082 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:15:30,084 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:15:30,084 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:15:30,085 - INFO - joeynmt.training - \tHypothesis: Mana , Paulo yali niyiria , ne , niyiria , ne , niyiria , ne , nibarula , ne , nibarula , nibarula , ne nibarula , nibarula , ne , nibarula , nibarula , okhusaala .\n",
      "2021-07-10 10:15:30,085 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     3000: bleu:   2.10, loss: 149558.0156, ppl:  57.0709, duration: 44.1420s\n",
      "2021-07-10 10:15:32,977 - INFO - joeynmt.training - Epoch  10: total training loss 1239.03\n",
      "2021-07-10 10:15:32,977 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-10 10:15:36,035 - INFO - joeynmt.training - Epoch  11, Step:     3100, Batch Loss:     3.824946, Tokens per Sec:    11748, Lr: 0.000300\n",
      "2021-07-10 10:15:42,500 - INFO - joeynmt.training - Epoch  11, Step:     3200, Batch Loss:     4.099649, Tokens per Sec:    10921, Lr: 0.000300\n",
      "2021-07-10 10:16:25,556 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:16:25,557 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:16:25,557 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:16:25,874 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:16:25,874 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:16:26,563 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:16:26,564 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:16:26,564 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:16:26,564 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyamanya mbu , Yesu , nababoolela ari , “ Niwe , iwe , wanje , ne , ndaboolile , mbu , ndaboolile , mbu , “ Omwami , ndaboolile . ”\n",
      "2021-07-10 10:16:26,564 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyamanya mbu , Yesu yali , niyaboolile , ne , nabareeba ari , “ Omwami , ”\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:16:26,566 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - \tHypothesis: Ne olwa bali nibaboolile , abandu bahonibwa , nibahonibwe .\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:16:26,567 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:16:26,568 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yali niyiranga , ne , nabiranga , ne , nabiranga , nabirukha , ne , nabirukha , nabirukha , nibahelesia , ne , nibahelesia , nibahelesia , okhusaala .\n",
      "2021-07-10 10:16:26,568 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step     3200: bleu:   2.16, loss: 147525.4375, ppl:  54.0187, duration: 44.0680s\n",
      "2021-07-10 10:16:32,454 - INFO - joeynmt.training - Epoch  11, Step:     3300, Batch Loss:     4.153135, Tokens per Sec:    12159, Lr: 0.000300\n",
      "2021-07-10 10:16:35,654 - INFO - joeynmt.training - Epoch  11: total training loss 1216.30\n",
      "2021-07-10 10:16:35,655 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-10 10:16:38,786 - INFO - joeynmt.training - Epoch  12, Step:     3400, Batch Loss:     3.855164, Tokens per Sec:     9793, Lr: 0.000300\n",
      "2021-07-10 10:17:21,771 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:17:21,772 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:17:21,772 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:17:22,051 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:17:22,052 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:17:22,732 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:17:22,733 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:17:22,733 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:17:22,733 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali niyaboolile , ne nababoolela ari , “ Omwami , ndaboolile , mbu , “ Niwe , ndenya , akanje , akanje , ne , ndaboolile . ”\n",
      "2021-07-10 10:17:22,733 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:17:22,734 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:17:22,734 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:17:22,734 - INFO - joeynmt.training - \tHypothesis: Mana Petero , niyaboole mbu , “ Niwe , owitsa , owomulakusi , womulakusi , womulakusi , womubatiisi . ”\n",
      "2021-07-10 10:17:22,734 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:17:22,735 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:17:22,735 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:17:22,735 - INFO - joeynmt.training - \tHypothesis: Mana , nibanyoola , mwitookho lia Yerusalemu , Yerusalemu , ne , nibenjile .\n",
      "2021-07-10 10:17:22,735 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:17:22,736 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:17:22,736 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:17:22,736 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yali niyiria , owali niyiria , niyiria , ne , niyiria , niyiria , nibenjile , ne , nibenjile , nibenjile .\n",
      "2021-07-10 10:17:22,736 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     3400: bleu:   2.61, loss: 146090.9688, ppl:  51.9634, duration: 43.9499s\n",
      "2021-07-10 10:17:28,765 - INFO - joeynmt.training - Epoch  12, Step:     3500, Batch Loss:     3.602729, Tokens per Sec:    11998, Lr: 0.000300\n",
      "2021-07-10 10:17:35,274 - INFO - joeynmt.training - Epoch  12, Step:     3600, Batch Loss:     3.857409, Tokens per Sec:    10963, Lr: 0.000300\n",
      "2021-07-10 10:18:16,089 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:18:16,089 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:18:16,090 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:18:16,396 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:18:16,397 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:18:17,126 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:18:17,127 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:18:17,127 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:18:17,127 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali , nababoolela ari , “ Omwechesia , shiendi , mbu , , ndenya , mbu , ndenya , mbu , ndenya , okhuboolela , mbu , ‘ Omboolile , okhuboolela . ”\n",
      "2021-07-10 10:18:17,128 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:18:17,128 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:18:17,128 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:18:17,128 - INFO - joeynmt.training - \tHypothesis: Mana , Petero yali niyareeba ari , “ Niwe , omwana wa , Yohana , ne , , yamwinjisia , omwana oyo , wamwinjisia . ”\n",
      "2021-07-10 10:18:17,129 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:18:17,129 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:18:17,129 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:18:17,129 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyiranga , abandu , bobushuru , bahona , ne , nibahona .\n",
      "2021-07-10 10:18:17,130 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:18:17,130 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:18:17,131 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:18:17,133 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yali niyirula , ne , nabirula , ne , nabirula , nibibili , nibibili , ne , nibirula , nibibili , nibibili , ne , nibibili .\n",
      "2021-07-10 10:18:17,133 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     3600: bleu:   2.40, loss: 143505.5312, ppl:  48.4545, duration: 41.8580s\n",
      "2021-07-10 10:18:20,727 - INFO - joeynmt.training - Epoch  12: total training loss 1179.26\n",
      "2021-07-10 10:18:20,728 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-10 10:18:23,142 - INFO - joeynmt.training - Epoch  13, Step:     3700, Batch Loss:     3.952638, Tokens per Sec:    11801, Lr: 0.000300\n",
      "2021-07-10 10:18:29,630 - INFO - joeynmt.training - Epoch  13, Step:     3800, Batch Loss:     3.736598, Tokens per Sec:    11070, Lr: 0.000300\n",
      "2021-07-10 10:19:12,795 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:19:12,796 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:19:12,796 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:19:13,115 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:19:13,116 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:19:14,218 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:19:14,219 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:19:14,220 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:19:14,220 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyaboolela ari , “ Omwechesia , mbu , ndenya , mbu , ndenya , mbu , ndenya , okhuboolela mbu , “ Nisie , ndenya , okhureere , okhureere . ”\n",
      "2021-07-10 10:19:14,220 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:19:14,220 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:19:14,221 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:19:14,221 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyenjile , ne , nalekha , ne , nababoolela ari , “ Omwechesia , mbu , niwe , Omwami , Omwami , witsa , wanje , ne , ndenya . ”\n",
      "2021-07-10 10:19:14,221 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:19:14,222 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:19:14,222 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:19:14,222 - INFO - joeynmt.training - \tHypothesis: Ne olwa , abandu baliho , baliho , baliho , Isabato , ne , nibenjile , abandu baliho .\n",
      "2021-07-10 10:19:14,222 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:19:14,223 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:19:14,223 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:19:14,223 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yali niyenjile , ne , nabibili , ne , nabibili , ne nabibili , nibenjela , ne , nibenjela , nibenjela , ne , nibenjela , nibenjela , ne , nibenjela .\n",
      "2021-07-10 10:19:14,223 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step     3800: bleu:   2.55, loss: 142290.7031, ppl:  46.8886, duration: 44.5923s\n",
      "2021-07-10 10:19:20,181 - INFO - joeynmt.training - Epoch  13, Step:     3900, Batch Loss:     3.605079, Tokens per Sec:    11667, Lr: 0.000300\n",
      "2021-07-10 10:19:24,530 - INFO - joeynmt.training - Epoch  13: total training loss 1153.43\n",
      "2021-07-10 10:19:24,531 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-10 10:19:26,616 - INFO - joeynmt.training - Epoch  14, Step:     4000, Batch Loss:     3.736794, Tokens per Sec:    11842, Lr: 0.000300\n",
      "2021-07-10 10:20:09,710 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:20:09,711 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:20:09,711 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:20:10,051 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:20:10,051 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:20:10,731 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:20:10,734 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:20:10,734 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:20:10,734 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyenya , ne , nababoolela ari , “ Omwechesia , ndenya , mbu , ndenya , mbu , ndenya , okhuboolela mbu , “ Enzenya , okhuboolela , mbu , ndenya , ndenya . ”\n",
      "2021-07-10 10:20:10,734 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:20:10,735 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:20:10,735 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:20:10,735 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyenjile , Yesu , yamanya mbu , Yesu , yamanya mbu , “ Omwechesia , wamwinjisia , ”\n",
      "2021-07-10 10:20:10,735 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyenjile , yanyoola , abandu boosi , banyoola , ne , nibanyoola , abandu boosi .\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:20:10,736 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:20:10,737 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:20:10,737 - INFO - joeynmt.training - \tHypothesis: Mana Paulo , nabaana befwe , bachendela , ne , nabibili , ne nabibili , ne nabibili , ne nabibili . Ne olwa yali niyenjile , niyenjile , ne nabasilia , nabibili .\n",
      "2021-07-10 10:20:10,737 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step     4000: bleu:   2.48, loss: 140957.5625, ppl:  45.2284, duration: 44.1203s\n",
      "2021-07-10 10:20:16,670 - INFO - joeynmt.training - Epoch  14, Step:     4100, Batch Loss:     3.925256, Tokens per Sec:    11941, Lr: 0.000300\n",
      "2021-07-10 10:20:23,090 - INFO - joeynmt.training - Epoch  14, Step:     4200, Batch Loss:     3.685305, Tokens per Sec:    11259, Lr: 0.000300\n",
      "2021-07-10 10:21:03,699 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:21:03,699 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:21:03,699 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:21:04,023 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:21:04,024 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:21:04,706 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:21:04,706 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:21:04,707 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:21:04,707 - INFO - joeynmt.training - \tHypothesis: Ne olwa , Abafarisayo , bahulilakhwo , Yesu , nababoolela ari , “ Omwechesia , ndenya , ne , ndenya , okhuboolela mbu , “ Endenya , okhuboolela mbu , ekhwo , isie , ndenya , okhuboolela . ”\n",
      "2021-07-10 10:21:04,707 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:21:04,707 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyechesinjia , abeechibe , ne , yakalukha munzu , ne , namuboolela ari , “ Omwechesia , obe , iwe , iwe , wanje , ”\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:21:04,708 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - \tHypothesis: Mana , abandu boosi boosi bali nibaliho , nibaliho , omundu yesi , owomukhonokwe .\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:21:04,709 - INFO - joeynmt.training - \tHypothesis: Mana Paulo nende Sila nende Sila , nibabiria , ne nabibili , ne nabibili , ne nabibili , ne , nabibili , ne nabibili . Ne olwa bali nibanywa , nibanywe , nibenjile .\n",
      "2021-07-10 10:21:04,710 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step     4200: bleu:   2.81, loss: 139297.2969, ppl:  43.2427, duration: 41.6188s\n",
      "2021-07-10 10:21:08,868 - INFO - joeynmt.training - Epoch  14: total training loss 1130.41\n",
      "2021-07-10 10:21:08,869 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-10 10:21:10,643 - INFO - joeynmt.training - Epoch  15, Step:     4300, Batch Loss:     3.675772, Tokens per Sec:    11731, Lr: 0.000300\n",
      "2021-07-10 10:21:17,251 - INFO - joeynmt.training - Epoch  15, Step:     4400, Batch Loss:     3.507568, Tokens per Sec:    10898, Lr: 0.000300\n",
      "2021-07-10 10:21:54,961 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:21:54,961 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:21:54,962 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:21:55,286 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:21:55,286 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:21:56,434 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:21:56,435 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:21:56,435 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:21:56,435 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyechesinjia abeechibe , ne , nababoolela ari , “ Omwechesia , nisie , owitsa , wambelesia , okhuboolela , mbu , “ Enzenya , okhuboolela , mbu , ekhwo , nemboolile , okhukholelie . ”\n",
      "2021-07-10 10:21:56,435 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:21:56,436 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:21:56,436 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:21:56,436 - INFO - joeynmt.training - \tHypothesis: Mana Yesu yali niyechesinjia , ne , niyechesinjia abeechibe , ne , namuboolela ari , “ Omwechesia , ” , ”\n",
      "2021-07-10 10:21:56,436 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:21:56,437 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:21:56,437 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:21:56,437 - INFO - joeynmt.training - \tHypothesis: Mana , khunyanga eya Isabato , Isabato , yaliho , Isabato , ne , khunyanga eya Isabato .\n",
      "2021-07-10 10:21:56,437 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:21:56,438 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:21:56,438 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:21:56,438 - INFO - joeynmt.training - \tHypothesis: Mana Paulo natsia , ne , natiila , omuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwashio , kwomuyeka kwomuyeka , kwomuyeka kwashio , kwomuyeka , kwashio .\n",
      "2021-07-10 10:21:56,438 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step     4400: bleu:   3.08, loss: 138373.4219, ppl:  42.1757, duration: 39.1864s\n",
      "2021-07-10 10:22:02,347 - INFO - joeynmt.training - Epoch  15, Step:     4500, Batch Loss:     3.543476, Tokens per Sec:    12041, Lr: 0.000300\n",
      "2021-07-10 10:22:07,180 - INFO - joeynmt.training - Epoch  15: total training loss 1102.02\n",
      "2021-07-10 10:22:07,180 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-10 10:22:08,781 - INFO - joeynmt.training - Epoch  16, Step:     4600, Batch Loss:     3.689896, Tokens per Sec:    12181, Lr: 0.000300\n",
      "2021-07-10 10:22:42,416 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:22:42,416 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:22:42,417 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:22:42,731 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:22:42,731 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:22:43,413 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:22:43,414 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:22:43,414 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:22:43,414 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo abo bamureeba , bari , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi yesi , ouboola , mbu , “ Enzenya , okhuboolela , mbu , ‘ Enzenya , nasi , ndenya , okhunzenya . ”\n",
      "2021-07-10 10:22:43,415 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:22:43,415 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:22:43,415 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:22:43,415 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali niyechesinjia , omundu undi , undi , owali niyechesinjia , ne , naboolela Petero ari , “ Omwechesia , ”\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - \tHypothesis: Ne olwa bali nibanyoola , Yerusalemu , baliho , ahambi , tsinyanga etsio .\n",
      "2021-07-10 10:22:43,416 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:22:43,417 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:22:43,417 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:22:43,417 - INFO - joeynmt.training - \tHypothesis: Mana Paulo , naboolela Paulo ari , “ Itaru , ne , olwa , ndasasaba , tsinyanga tsiayo , tsiomunyu , tsiomunyu , tsiomunyu , tsiomunyu , tsiomunyu , tsiomunyu .\n",
      "2021-07-10 10:22:43,417 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     4600: bleu:   3.04, loss: 137582.2344, ppl:  41.2830, duration: 34.6364s\n",
      "2021-07-10 10:22:49,440 - INFO - joeynmt.training - Epoch  16, Step:     4700, Batch Loss:     3.641934, Tokens per Sec:    11802, Lr: 0.000300\n",
      "2021-07-10 10:22:55,823 - INFO - joeynmt.training - Epoch  16, Step:     4800, Batch Loss:     3.792781, Tokens per Sec:    11187, Lr: 0.000300\n",
      "2021-07-10 10:23:35,897 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:23:35,898 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:23:35,898 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:23:36,218 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:23:36,219 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:23:36,945 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:23:36,946 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:23:36,946 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:23:36,946 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo , bamusaaya mbu , “ Omundu yesi yesi yesi yesi yesi yesi yesi yesi , ouboola , ne , namuboolela ari , “ Ndakhubooleele , , okhunzenya , okhunzenya , okhunzenya . ”\n",
      "2021-07-10 10:23:36,946 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:23:36,947 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:23:36,947 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:23:36,947 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjile , ne , niyenjile , Petero , ne namuboolela ari , “ Omwechesia , obe , ”\n",
      "2021-07-10 10:23:36,947 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:23:36,948 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:23:36,948 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:23:36,948 - INFO - joeynmt.training - \tHypothesis: Ne olwa , khunyanga eya Isabato , yalimwo , yalimwo , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:23:36,948 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:23:36,949 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:23:36,949 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:23:36,949 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali niyenjile , ne , nabula , ne nabibili , ne nabibili , ne , nabibili , ne , nabibili , ne nabibili , ne nabibili .\n",
      "2021-07-10 10:23:36,949 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     4800: bleu:   3.27, loss: 136052.0156, ppl:  39.6096, duration: 41.1264s\n",
      "2021-07-10 10:23:41,834 - INFO - joeynmt.training - Epoch  16: total training loss 1082.23\n",
      "2021-07-10 10:23:41,835 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-10 10:23:43,662 - INFO - joeynmt.training - Epoch  17, Step:     4900, Batch Loss:     3.621493, Tokens per Sec:     9392, Lr: 0.000300\n",
      "2021-07-10 10:23:49,727 - INFO - joeynmt.training - Epoch  17, Step:     5000, Batch Loss:     3.626518, Tokens per Sec:    11719, Lr: 0.000300\n",
      "2021-07-10 10:24:22,867 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:24:22,868 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:24:22,868 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:24:23,171 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:24:23,171 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:24:23,874 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:24:23,875 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:24:23,875 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:24:23,875 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo , nibaboolela Yesu ari , “ Omwechesia , ndenya , okhuboolela mbu , ndekalushe , ne , ndahulila . ”\n",
      "2021-07-10 10:24:23,875 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:24:23,876 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:24:23,876 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:24:23,876 - INFO - joeynmt.training - \tHypothesis: Mana Yesu , namurumile , ne , olwa yali niyenjile , Yesu , namuboolela ari , “ Omwami , ”\n",
      "2021-07-10 10:24:23,876 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:24:23,877 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:24:23,877 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:24:23,877 - INFO - joeynmt.training - \tHypothesis: Ne olwa , khunyanga yaliho , Isabato , yaliho , khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:24:23,877 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:24:23,878 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:24:23,878 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:24:23,878 - INFO - joeynmt.training - \tHypothesis: Mana Paulo yatiila , Paulo , ne nabaranjilila , okhuyila , ne , nabibili , ne , nabaranjilila , okhwisasaba , ne , nabaranjilila , okhwisukunwa , ne nabarwi , nabarwi .\n",
      "2021-07-10 10:24:23,878 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step     5000: bleu:   3.22, loss: 135581.5156, ppl:  39.1088, duration: 34.1510s\n",
      "2021-07-10 10:24:29,744 - INFO - joeynmt.training - Epoch  17, Step:     5100, Batch Loss:     3.146760, Tokens per Sec:    12146, Lr: 0.000300\n",
      "2021-07-10 10:24:34,954 - INFO - joeynmt.training - Epoch  17: total training loss 1065.02\n",
      "2021-07-10 10:24:34,955 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-10 10:24:36,151 - INFO - joeynmt.training - Epoch  18, Step:     5200, Batch Loss:     3.364739, Tokens per Sec:    11958, Lr: 0.000300\n",
      "2021-07-10 10:25:16,690 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:25:16,691 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:25:16,691 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:25:17,000 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:25:17,000 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:25:17,698 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:25:17,699 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:25:17,699 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:25:17,700 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo bandi bandi bandi , bamureeba bari , “ Omwechesia , ndakhuboolela mbu , ndeela , ndeela , ne , ndemulola , ndakhura , ne , ndakhura , ne , ndakhuranjilila okhuboolela . ”\n",
      "2021-07-10 10:25:17,700 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:25:17,700 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjile , yamwitsila , ne , namuboolela ari , “ Omwechesia , omanyile mbu , iwe , iwe , omanyile mbu , iwe , omanyile mbu , omanyile mbu , omanyile mbu , “ Omwechesia . ”\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:25:17,701 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:25:17,702 - INFO - joeynmt.training - \tHypothesis: Ne olwa , khunyanga eya Isabato , yaliho , khunyanga ya Isabato , Isabato , ne khunyanga ya Isabato , Isabato .\n",
      "2021-07-10 10:25:17,702 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:25:17,702 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:25:17,702 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:25:17,703 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yatiila Paulo , ne , natiila Paulo , ne , natsukuuna , ne , natsukunwa , natsukunwa , ne , natsukunwa , natsukwe , ne , natsukunwa mumulilo .\n",
      "2021-07-10 10:25:17,703 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     5200: bleu:   3.66, loss: 134399.4062, ppl:  37.8784, duration: 41.5509s\n",
      "2021-07-10 10:25:23,605 - INFO - joeynmt.training - Epoch  18, Step:     5300, Batch Loss:     2.991381, Tokens per Sec:    12022, Lr: 0.000300\n",
      "2021-07-10 10:25:29,999 - INFO - joeynmt.training - Epoch  18, Step:     5400, Batch Loss:     3.248828, Tokens per Sec:    11154, Lr: 0.000300\n",
      "2021-07-10 10:26:04,190 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:26:04,191 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:26:04,191 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:26:04,499 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:26:04,500 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:26:05,598 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:26:05,599 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:26:05,599 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:26:05,599 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo bandi bandi bali nibaboolelakhwo Yesu , nababoolela ari , “ Omwechesia , ndenya , okhuboolela mbu , ndamile , okhunzu , ne olwa ndakhuboolela . ”\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyebula , omwana wabwe , owali niyenjile , omwana wabwe , namuboolela ari , “ Omwechesia , ” ,\n",
      "2021-07-10 10:26:05,600 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:26:05,601 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:26:05,601 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:26:05,601 - INFO - joeynmt.training - \tHypothesis: Ne olwa , Yerusalemu , yaliho , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:26:05,601 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:26:05,602 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:26:05,602 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:26:05,602 - INFO - joeynmt.training - \tHypothesis: Mana Paulo natsia , tsimbilo natsia , tsimbilo , ne , olwa yali niyenjile , niyenjilamwo , ne , niyenjile , ne , niyenjilamwo , ne , niyenjile .\n",
      "2021-07-10 10:26:05,602 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     5400: bleu:   3.71, loss: 133931.3438, ppl:  37.4020, duration: 35.6028s\n",
      "2021-07-10 10:26:10,572 - INFO - joeynmt.training - Epoch  18: total training loss 1049.73\n",
      "2021-07-10 10:26:10,572 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-10 10:26:11,453 - INFO - joeynmt.training - Epoch  19, Step:     5500, Batch Loss:     3.479048, Tokens per Sec:    11280, Lr: 0.000300\n",
      "2021-07-10 10:26:17,856 - INFO - joeynmt.training - Epoch  19, Step:     5600, Batch Loss:     3.286332, Tokens per Sec:    11170, Lr: 0.000300\n",
      "2021-07-10 10:26:56,539 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:26:56,539 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:26:56,539 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:26:56,862 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:26:56,862 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:26:57,557 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:26:57,559 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:26:57,559 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:26:57,560 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo bandi bandi , bamureeba ari , “ Omwechesia , ndeebula , ne , ndaheela , ndakhurumile , ne , ndakhurumile . ”\n",
      "2021-07-10 10:26:57,560 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:26:57,560 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:26:57,560 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:26:57,561 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , omukhasi undi undi , yalimwo , ne , namuboolela ari , “ Omwechesia , ” , ”\n",
      "2021-07-10 10:26:57,561 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:26:57,561 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:26:57,561 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:26:57,562 - INFO - joeynmt.training - \tHypothesis: Ne olwa , Yerusalemu , yaliho abandu abanji , baliho , khunyanga eya Isabato , Isabato , bali , nibanyoola , eshiokhulia .\n",
      "2021-07-10 10:26:57,562 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:26:57,562 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:26:57,562 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:26:57,563 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yalimwo , yakwa , ne , olwa Sila , yakwa , ne , nabarumile , ne , nabarumile , ne , nabarumile , ne , nabarumile .\n",
      "2021-07-10 10:26:57,563 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     5600: bleu:   3.39, loss: 133174.1094, ppl:  36.6439, duration: 39.7067s\n",
      "2021-07-10 10:27:03,468 - INFO - joeynmt.training - Epoch  19, Step:     5700, Batch Loss:     3.517236, Tokens per Sec:    11975, Lr: 0.000300\n",
      "2021-07-10 10:27:09,027 - INFO - joeynmt.training - Epoch  19: total training loss 1035.27\n",
      "2021-07-10 10:27:09,027 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-10 10:27:09,692 - INFO - joeynmt.training - Epoch  20, Step:     5800, Batch Loss:     3.369959, Tokens per Sec:     8731, Lr: 0.000300\n",
      "2021-07-10 10:27:44,432 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:27:44,432 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:27:44,432 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:27:44,751 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:27:44,752 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:27:45,428 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:27:45,429 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:27:45,429 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:27:45,430 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo bandi bandi , bamureeba Yesu ari , “ Isie , ndenya , okhunzinjisia , ne , ndenya , okhuboolela mbu , “ Isie ndenya , okhunzenya , okhunzenya , okhunzenya . ”\n",
      "2021-07-10 10:27:45,430 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:27:45,430 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:27:45,431 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:27:45,431 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , yamanya , Petero , ne , namuboolela ari , “ Omwechesia , omanyile mbu , Yesu , omanyile mbu , Yesu , omanyile mbu , “ Omwechesia , ”\n",
      "2021-07-10 10:27:45,431 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:27:45,431 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:27:45,432 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:27:45,432 - INFO - joeynmt.training - \tHypothesis: Ne olwa tsinyanga etsio , Isabato , yaliho , abandu abanji , baliho , khunyanga eya Isabato .\n",
      "2021-07-10 10:27:45,432 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:27:45,432 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:27:45,433 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:27:45,433 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yatiila , omuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka kwomuyeka , kwomuyeka kwomuyeka kwako , ne , nahonokokha .\n",
      "2021-07-10 10:27:45,433 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     5800: bleu:   3.19, loss: 132333.7500, ppl:  35.8206, duration: 35.7408s\n",
      "2021-07-10 10:27:51,302 - INFO - joeynmt.training - Epoch  20, Step:     5900, Batch Loss:     3.457117, Tokens per Sec:    12065, Lr: 0.000300\n",
      "2021-07-10 10:27:57,743 - INFO - joeynmt.training - Epoch  20, Step:     6000, Batch Loss:     3.060367, Tokens per Sec:    10986, Lr: 0.000300\n",
      "2021-07-10 10:28:34,596 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:28:34,597 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:28:34,597 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:28:34,927 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:28:34,930 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:28:35,604 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:28:35,605 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:28:35,605 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:28:35,605 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo bandi bandi , bamureeba bari , “ Omundu yesi yesi yesi oumutiile , ne , namuboolela ari , “ Ekhusaaya , mutiile , ne , olwa ndakhubooleele . ”\n",
      "2021-07-10 10:28:35,606 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:28:35,606 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:28:35,606 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:28:35,606 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , ne , namuboolela ari , “ Omwechesia , ” , Yesu , namuboolela ari , “ Omwechesia , ” ,\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:28:35,607 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:28:35,608 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:28:35,608 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:28:35,608 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yatiila Paulo , ne , nabuukha , nabirula , ne , nabirula , nibamuyila , mushilibwa , ne , nabuukha , nibamunywekha . Ne olwa yenjela , nibamunyeka , nibamunyanza , mana , nabirulamwo .\n",
      "2021-07-10 10:28:35,608 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     6000: bleu:   3.35, loss: 131670.9531, ppl:  35.1843, duration: 37.8648s\n",
      "2021-07-10 10:28:41,351 - INFO - joeynmt.training - Epoch  20: total training loss 1018.80\n",
      "2021-07-10 10:28:41,352 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-10 10:28:41,538 - INFO - joeynmt.training - Epoch  21, Step:     6100, Batch Loss:     3.430549, Tokens per Sec:    11357, Lr: 0.000300\n",
      "2021-07-10 10:28:47,882 - INFO - joeynmt.training - Epoch  21, Step:     6200, Batch Loss:     3.375226, Tokens per Sec:    11131, Lr: 0.000300\n",
      "2021-07-10 10:29:19,986 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:29:19,986 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:29:19,986 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:29:20,290 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:29:20,291 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:29:20,982 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:29:20,982 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo nende Abafarisayo , naboolela Yesu ari , “ Omundu yesi oumutiile , ne , niyenya , okhuboolela mbu , “ Isie ndemulole , ne , ndenya . ”\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:29:20,983 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyechesinjia , ne , naboolela Petero ari , “ Omwechesia , ” , namuboolela ari , “ Omwechesia , ” , ”\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:29:20,984 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:29:20,985 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:29:20,985 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:29:20,985 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yali niyenjisia , ne , nabahenga , ne , nabahenga , ne , nabahenga , ne , nabahenga , ne , nabahenga , ne , nabahenga , nibachinjile .\n",
      "2021-07-10 10:29:20,985 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step     6200: bleu:   3.63, loss: 131429.0312, ppl:  34.9548, duration: 33.1035s\n",
      "2021-07-10 10:29:26,882 - INFO - joeynmt.training - Epoch  21, Step:     6300, Batch Loss:     3.394372, Tokens per Sec:    11793, Lr: 0.000300\n",
      "2021-07-10 10:29:33,293 - INFO - joeynmt.training - Epoch  21, Step:     6400, Batch Loss:     3.191988, Tokens per Sec:    11121, Lr: 0.000300\n",
      "2021-07-10 10:29:58,974 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:29:58,975 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:29:58,975 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:29:59,291 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:29:59,292 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:30:00,400 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:30:00,401 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:30:00,401 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:30:00,401 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo bandi bandi , Abafarisayo nibetsa khu Yesu , ne nababoolela ari , “ Ndenya okhukholeele , ne , nditsulilanga mbu , enzenya . ”\n",
      "2021-07-10 10:30:00,401 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:30:00,402 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:30:00,402 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:30:00,402 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyechesinjia , amakhuwa ako , yamwitsila , namuboolela ari , “ Omwechesia , ” , Yesu namuboolela ari , “ Omwami , ”\n",
      "2021-07-10 10:30:00,402 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:30:00,403 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:30:00,403 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:30:00,403 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato , yaho .\n",
      "2021-07-10 10:30:00,403 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:30:00,404 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:30:00,404 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:30:00,404 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yamanya mbu , batsie , ne nabaroma , nabarunda , ne nabarenga , nabarunda , ne nabarunda , nabarakwe .\n",
      "2021-07-10 10:30:00,404 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step     6400: bleu:   3.73, loss: 130217.8984, ppl:  33.8286, duration: 27.1102s\n",
      "2021-07-10 10:30:00,705 - INFO - joeynmt.training - Epoch  21: total training loss 1009.63\n",
      "2021-07-10 10:30:00,705 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-10 10:30:06,327 - INFO - joeynmt.training - Epoch  22, Step:     6500, Batch Loss:     3.235496, Tokens per Sec:    12043, Lr: 0.000300\n",
      "2021-07-10 10:30:12,727 - INFO - joeynmt.training - Epoch  22, Step:     6600, Batch Loss:     3.029850, Tokens per Sec:    11125, Lr: 0.000300\n",
      "2021-07-10 10:30:46,725 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:30:46,725 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:30:46,725 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:30:47,047 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:30:47,048 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:30:47,739 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:30:47,740 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:30:47,740 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:30:47,740 - INFO - joeynmt.training - \tHypothesis: Ne olwa Abafarisayo , bamuboolela mbu , “ Omundu yesi yesi yesi yesi yesi , oumuboolela , mbu , “ Isie , enzenya , okhutiilakhwo , ne , ndalola . ”\n",
      "2021-07-10 10:30:47,740 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:30:47,744 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:30:47,744 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:30:47,744 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , yamanya , mbu , Yesu yali niyaheela . Ne namuboolela ari , “ Omwechesia , obe , ninenywe . ”\n",
      "2021-07-10 10:30:47,744 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:30:47,745 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:30:47,745 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:30:47,745 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho omundu undi , weshialo , shialiho , khunyanga eya Isabato .\n",
      "2021-07-10 10:30:47,745 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:30:47,746 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:30:47,746 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:30:47,746 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjelesie , ne , naboha , ne nabiria , mana nabiria , mana nabiria , ne nabiria , nabiria , ne nabiria , nabiria , mana nabiria .\n",
      "2021-07-10 10:30:47,746 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step     6600: bleu:   3.46, loss: 129949.4844, ppl:  33.5839, duration: 35.0188s\n",
      "2021-07-10 10:30:53,609 - INFO - joeynmt.training - Epoch  22, Step:     6700, Batch Loss:     3.186085, Tokens per Sec:    11938, Lr: 0.000300\n",
      "2021-07-10 10:30:54,315 - INFO - joeynmt.training - Epoch  22: total training loss 990.94\n",
      "2021-07-10 10:30:54,316 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-10 10:30:59,999 - INFO - joeynmt.training - Epoch  23, Step:     6800, Batch Loss:     3.140948, Tokens per Sec:    10820, Lr: 0.000300\n",
      "2021-07-10 10:31:29,344 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:31:29,345 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:31:29,345 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:31:30,304 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:31:30,304 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:31:30,304 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:31:30,305 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , Abafarisayo , bamureeba bari , “ Omundu yesi yesi ousuubila , ne , niyenjilamwo , nasi , ndalola , ne , olwa ndola , enzenya . ”\n",
      "2021-07-10 10:31:30,305 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:31:30,305 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:31:30,305 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:31:30,306 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , yasinjila , ne , namuboolela ari , “ Omwechesia , ” , Yesu namuboolela ari , “ Omwami , ”\n",
      "2021-07-10 10:31:30,306 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:31:30,306 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yaliho , khunyanga eya Isabato , ne khunyanga eya Isabato , Isabato ,\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:31:30,307 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:31:30,308 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yenjilamwo Paulo , ne , naboha , ne nabaroma abo , nibatseshelela , ne , nibatsushilushilamwo , ne nabarakhwo , nibachitiilakhwo , mana nabenjilamwo , nibenjilamwo .\n",
      "2021-07-10 10:31:30,308 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step     6800: bleu:   3.61, loss: 129991.0156, ppl:  33.6217, duration: 30.3080s\n",
      "2021-07-10 10:31:36,212 - INFO - joeynmt.training - Epoch  23, Step:     6900, Batch Loss:     3.529071, Tokens per Sec:    12073, Lr: 0.000300\n",
      "2021-07-10 10:31:42,568 - INFO - joeynmt.training - Epoch  23, Step:     7000, Batch Loss:     3.133763, Tokens per Sec:    11056, Lr: 0.000300\n",
      "2021-07-10 10:32:11,116 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:32:11,117 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:32:11,117 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:32:11,428 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:32:11,429 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:32:12,651 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:32:12,652 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:32:12,652 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:32:12,652 - INFO - joeynmt.training - \tHypothesis: Ne Abafarisayo bandi bandi bandi , bamureeba mbu , “ Omundu yesi yesi oumutiilakhwo , ne , niyenjela , ne , namuboolela ari , “ Isie nditsa , ndachirakhwo , ndachinjile . ”\n",
      "2021-07-10 10:32:12,652 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:32:12,653 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:32:12,653 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:32:12,653 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali niyenjilamwo , yachaaka okhuboola , amakhuwa ako , yamwitsila naboola ari , “ Omwechesia , ”\n",
      "2021-07-10 10:32:12,653 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:32:12,654 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:32:12,654 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:32:12,654 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , ne khunyanga eya Isabato , abandu abanji , baliho .\n",
      "2021-07-10 10:32:12,654 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:32:12,655 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:32:12,655 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:32:12,655 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yatiila Paulo , ne natiila , emiesi chiali , chiali nichitaru , mana , nibenjila muliaro , ne nanyoola , ne nanyoola , eshiselelo , shituutu , ne , nabarulukha , ne , nabaremwa .\n",
      "2021-07-10 10:32:12,655 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step     7000: bleu:   3.86, loss: 128688.9922, ppl:  32.4585, duration: 30.0872s\n",
      "2021-07-10 10:32:13,782 - INFO - joeynmt.training - Epoch  23: total training loss 975.37\n",
      "2021-07-10 10:32:13,783 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-10 10:32:18,895 - INFO - joeynmt.training - Epoch  24, Step:     7100, Batch Loss:     3.327407, Tokens per Sec:    11070, Lr: 0.000300\n",
      "2021-07-10 10:32:25,065 - INFO - joeynmt.training - Epoch  24, Step:     7200, Batch Loss:     3.092860, Tokens per Sec:    11688, Lr: 0.000300\n",
      "2021-07-10 10:32:50,101 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:32:50,101 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:32:50,102 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:32:50,413 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:32:50,414 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:32:51,126 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:32:51,128 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:32:51,128 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:32:51,128 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi yesi oumutiile , ne , niyenjekhwo , ne , ndalola . ”\n",
      "2021-07-10 10:32:51,128 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:32:51,129 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:32:51,129 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:32:51,129 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , omundu undi , yahulila amakhuwa ako , yamwitsila , namuboolela ari , “ Omwechesia , witse , ”\n",
      "2021-07-10 10:32:51,129 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho omundu , womubatiisi , weshialo , shialiho , khunyanga eya Isabato .\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:32:51,130 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:32:51,131 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:32:51,131 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yakwa , ne , nanyoola oluchendo lwomukhuyu okwo , mana , nabarenga , mana nabarunda , nikukunwa mumulilo okulasilia , eshing'ari , nikasilie , ne , nahonokokha .\n",
      "2021-07-10 10:32:51,131 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step     7200: bleu:   4.14, loss: 128601.6172, ppl:  32.3819, duration: 26.0658s\n",
      "2021-07-10 10:32:57,066 - INFO - joeynmt.training - Epoch  24, Step:     7300, Batch Loss:     3.137117, Tokens per Sec:    11932, Lr: 0.000300\n",
      "2021-07-10 10:32:58,672 - INFO - joeynmt.training - Epoch  24: total training loss 955.35\n",
      "2021-07-10 10:32:58,673 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-07-10 10:33:03,525 - INFO - joeynmt.training - Epoch  25, Step:     7400, Batch Loss:     2.755885, Tokens per Sec:    11257, Lr: 0.000300\n",
      "2021-07-10 10:33:36,066 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:33:36,066 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:33:36,067 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:33:36,387 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:33:36,387 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:33:37,064 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:33:37,065 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:33:37,065 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:33:37,065 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Isie nisie , omutiiti , ne , ndarecheresia , ne , ndaranjilila okhunzira . ”\n",
      "2021-07-10 10:33:37,066 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:33:37,066 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:33:37,066 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:33:37,067 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , omundu oyo , nasinjila , ne , namuboolela ari , “ Omwechesia , ” , yamuboolela ari , “ Omwechesia , oboolile . ”\n",
      "2021-07-10 10:33:37,067 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:33:37,067 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:33:37,067 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:33:37,068 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho , khunyanga eya Isabato , yalimwo , khunyanga eya Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:33:37,068 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:33:37,068 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:33:37,068 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:33:37,069 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , mana nabarumbula , tsingubo tsiabwe , mana nabarumbetile , mana , nabarumbete , mana nabaremwa , mana , nabaremwa .\n",
      "2021-07-10 10:33:37,069 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step     7400: bleu:   4.07, loss: 127956.1016, ppl:  31.8215, duration: 33.5428s\n",
      "2021-07-10 10:33:43,000 - INFO - joeynmt.training - Epoch  25, Step:     7500, Batch Loss:     3.027595, Tokens per Sec:    11756, Lr: 0.000300\n",
      "2021-07-10 10:33:49,440 - INFO - joeynmt.training - Epoch  25, Step:     7600, Batch Loss:     3.068221, Tokens per Sec:    11026, Lr: 0.000300\n",
      "2021-07-10 10:34:13,071 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:34:13,071 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:34:13,072 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:34:13,373 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:34:13,373 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:34:14,064 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:34:14,065 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:34:14,065 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:34:14,066 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi yesi yesi , niyamile , ne , ndalola . ”\n",
      "2021-07-10 10:34:14,066 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:34:14,066 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:34:14,067 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:34:14,067 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali nashiboolanga amakhuwa ako , yasinjila , ne , namuboolela ari , “ Omwechesia , oleele ! ”\n",
      "2021-07-10 10:34:14,067 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaliho omundu , weshialo , shialiho , khunyanga eya Isabato .\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:34:14,068 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:34:14,069 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:34:14,069 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yakwa , ne , niyenjisia , ne , niyenjilamwo , niyenjilamwo , ne niyenjilamwo , ne , niyenjilamwo , ne niyenjilamwo , ne niyenjilamwo , niyenjilamwo .\n",
      "2021-07-10 10:34:14,069 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step     7600: bleu:   3.88, loss: 127914.7109, ppl:  31.7859, duration: 24.6289s\n",
      "2021-07-10 10:34:15,889 - INFO - joeynmt.training - Epoch  25: total training loss 944.30\n",
      "2021-07-10 10:34:15,889 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-07-10 10:34:19,993 - INFO - joeynmt.training - Epoch  26, Step:     7700, Batch Loss:     2.865616, Tokens per Sec:    11996, Lr: 0.000300\n",
      "2021-07-10 10:34:26,342 - INFO - joeynmt.training - Epoch  26, Step:     7800, Batch Loss:     2.879114, Tokens per Sec:    11132, Lr: 0.000300\n",
      "2021-07-10 10:34:56,011 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:34:56,011 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:34:56,011 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:34:56,333 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:34:56,333 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:34:57,017 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:34:57,018 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:34:57,018 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:34:57,018 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Oyo , nditsa , ne , nditsa , nditsa , ne , olwa nditsa , nditsa , nditsa , ndachinjia . ”\n",
      "2021-07-10 10:34:57,018 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:34:57,019 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:34:57,019 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:34:57,019 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjisia , yalola , Petero , yabukula Petero , Petero , namuboolela ari , “ Omwechesia , obe , ”\n",
      "2021-07-10 10:34:57,019 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yachaaka okhuchinga , lweinyanza , ne khunyanga eya Isabato , Isabato , yalimwo .\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:34:57,020 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:34:57,021 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yakwa , ne , nabarasi , nabarunda , mana nabarulamwo , mana nabarulamwo , mana nabararulamwo , bwisolo , mana nabarulukha , nibachitiilwa .\n",
      "2021-07-10 10:34:57,021 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step     7800: bleu:   3.76, loss: 127741.4375, ppl:  31.6374, duration: 30.6781s\n",
      "2021-07-10 10:35:02,994 - INFO - joeynmt.training - Epoch  26, Step:     7900, Batch Loss:     3.120385, Tokens per Sec:    11807, Lr: 0.000300\n",
      "2021-07-10 10:35:05,640 - INFO - joeynmt.training - Epoch  26: total training loss 931.06\n",
      "2021-07-10 10:35:05,640 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-07-10 10:35:09,475 - INFO - joeynmt.training - Epoch  27, Step:     8000, Batch Loss:     3.154422, Tokens per Sec:    12028, Lr: 0.000300\n",
      "2021-07-10 10:35:34,526 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:35:34,527 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:35:34,527 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:35:34,843 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:35:34,844 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:35:35,974 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:35:35,975 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:35:35,976 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:35:35,976 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yamanya Yesu mbu , “ Omundu yesi yesi yesi , oumuboolela ari , “ Enzu , nditsulilanga , ne ndalola , ebiamo bianje bianje . ”\n",
      "2021-07-10 10:35:35,976 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:35:35,976 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjilamwo , yalola , mbu , Yesu , yamwitsila namubooleele ari , “ Omwechesia , ”\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:35:35,977 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yaliho omundu , wambeli , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , Isabato .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjilamwo , Paulo , yenjilamwo , mana , naboha , tsingubo tsiomunyanza , mana , niyitsulamwo olusimbi lwomuyeka , kwomuyeka .\n",
      "2021-07-10 10:35:35,978 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step     8000: bleu:   4.07, loss: 127383.2344, ppl:  31.3324, duration: 26.5029s\n",
      "2021-07-10 10:35:41,816 - INFO - joeynmt.training - Epoch  27, Step:     8100, Batch Loss:     3.087347, Tokens per Sec:    12211, Lr: 0.000300\n",
      "2021-07-10 10:35:48,182 - INFO - joeynmt.training - Epoch  27, Step:     8200, Batch Loss:     3.020096, Tokens per Sec:    11173, Lr: 0.000300\n",
      "2021-07-10 10:36:19,824 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:36:19,824 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:36:19,824 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:36:20,159 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:36:20,159 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:36:20,837 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:36:20,838 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:36:20,839 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:36:20,839 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi oumutiile , ne , niyenjilamwo , ne , niyenjilamwo , ne , siesi endi omukhuyu kwanje . ”\n",
      "2021-07-10 10:36:20,839 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:36:20,839 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:36:20,840 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:36:20,840 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali nashiboolanga amakhuwa ako , yasinjila , ne , namuboolela ari , “ Omwechesia , olole , ” , Yesu namuboolela ari , “ Niwe , omuhonia watoto . ”\n",
      "2021-07-10 10:36:20,840 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:36:20,840 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:36:20,841 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:36:20,841 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yaliho , khunyanga eya Isabato , Isabato , abandu babili babili babili babili babili babili babili , bataru .\n",
      "2021-07-10 10:36:20,841 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:36:20,842 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:36:20,842 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:36:20,842 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yenjilamwo , ne , naboha , tsingubo tsiomunyiri . Ne olwa Paulo yali niyenjilamwo , niyenjilamwo , ne nafwa , ne nafwa .\n",
      "2021-07-10 10:36:20,842 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step     8200: bleu:   4.22, loss: 126984.2031, ppl:  30.9961, duration: 32.6596s\n",
      "2021-07-10 10:36:23,331 - INFO - joeynmt.training - Epoch  27: total training loss 918.25\n",
      "2021-07-10 10:36:23,332 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-07-10 10:36:26,729 - INFO - joeynmt.training - Epoch  28, Step:     8300, Batch Loss:     2.923501, Tokens per Sec:    11894, Lr: 0.000300\n",
      "2021-07-10 10:36:33,119 - INFO - joeynmt.training - Epoch  28, Step:     8400, Batch Loss:     3.279203, Tokens per Sec:    10979, Lr: 0.000300\n",
      "2021-07-10 10:37:10,139 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:37:10,140 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:37:10,140 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:37:10,457 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:37:10,457 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:37:11,149 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:37:11,151 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:37:11,151 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:37:11,151 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yareeba Yesu ari , “ Omundu yesi ouboola , mbu , yambwe , ne , yambambo , ne , ndalola , ne , nemalile okhuchiakhwo . ”\n",
      "2021-07-10 10:37:11,151 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:37:11,153 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:37:11,153 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:37:11,153 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali nashisinjiileho , yalola , Mariamu , yalola mbu , Yesu , yamanya mbu , “ Omwami , ” , namuboolela ari , “ Wenya , okhukholeele . ”\n",
      "2021-07-10 10:37:11,153 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yachaaka okhuchinga , khunyanga eya Isabato , abandu babili bataru , bataru , bataru .\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:37:11,154 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:37:11,155 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:37:11,155 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yenjilamwo , mana naboha , tsingubo tsiomunyanza , mana nabiria , mana nabirila , nibachibotokhana , ne , nibachinjile , okhwinyoola .\n",
      "2021-07-10 10:37:11,155 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step     8400: bleu:   4.21, loss: 126799.0234, ppl:  30.8413, duration: 38.0352s\n",
      "2021-07-10 10:37:17,108 - INFO - joeynmt.training - Epoch  28, Step:     8500, Batch Loss:     3.149360, Tokens per Sec:    12161, Lr: 0.000300\n",
      "2021-07-10 10:37:20,413 - INFO - joeynmt.training - Epoch  28: total training loss 903.76\n",
      "2021-07-10 10:37:20,413 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-07-10 10:37:23,544 - INFO - joeynmt.training - Epoch  29, Step:     8600, Batch Loss:     2.965389, Tokens per Sec:    11581, Lr: 0.000300\n",
      "2021-07-10 10:37:53,595 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:37:53,595 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:37:53,595 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:37:54,554 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:37:54,555 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:37:54,555 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:37:54,555 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu yesi oufukula , ne , niyamboolela ari , “ Enditsa , ne , ndalola , ne , ndalola , ndalola . ”\n",
      "2021-07-10 10:37:54,556 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:37:54,557 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:37:54,557 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:37:54,557 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , Petero yali nashiboolanga amakhuwa ako , yasinjila , namurusia , namuboolela ari , “ Omwami , olola ! ”\n",
      "2021-07-10 10:37:54,557 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , Isabato , yalimwo lisabo lia Pasaka .\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:37:54,558 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:37:54,559 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:37:54,559 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjilamwo , ne , niyenjilamwo , ne , niyenjilamwo , ne , niyenjilamwo , ne , niyenjilamwo , ne natsayi , boosi , nibachingwa , ne , nibachitiilakhwo .\n",
      "2021-07-10 10:37:54,559 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step     8600: bleu:   4.17, loss: 126825.8359, ppl:  30.8636, duration: 31.0145s\n",
      "2021-07-10 10:38:00,488 - INFO - joeynmt.training - Epoch  29, Step:     8700, Batch Loss:     2.795248, Tokens per Sec:    12069, Lr: 0.000300\n",
      "2021-07-10 10:38:06,859 - INFO - joeynmt.training - Epoch  29, Step:     8800, Batch Loss:     2.980321, Tokens per Sec:    11426, Lr: 0.000300\n",
      "2021-07-10 10:38:33,356 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:38:33,357 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:38:33,357 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:38:33,661 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:38:33,661 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:38:34,376 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:38:34,377 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:38:34,377 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:38:34,377 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi oumutiile , ne , niyamboolela ari , “ Endi namarwi kanje , nditsule , ne nditsula , ndalachinjia . ”\n",
      "2021-07-10 10:38:34,378 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:38:34,378 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:38:34,378 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:38:34,378 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali niyenjisinjia , yalola , mbu , Mariamu , namurume , ne namuboolela ari , “ Omwami , olole ! ”\n",
      "2021-07-10 10:38:34,379 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:38:34,379 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:38:34,379 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:38:34,379 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yalimwo , khunyanga eya Isabato , Isabato , ne khunyanga eya Isabato , yaho , khunyanga eya Isabato .\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjilamwo , mana nabarenga , ne , nabarunda , mana nabarumbula , mana , nabarumbete , ne nibachinjile , okhwikanzu , mana yenjilamwo , mana yenjilamwo .\n",
      "2021-07-10 10:38:34,380 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step     8800: bleu:   4.32, loss: 126406.0469, ppl:  30.5153, duration: 27.5205s\n",
      "2021-07-10 10:38:37,487 - INFO - joeynmt.training - Epoch  29: total training loss 891.35\n",
      "2021-07-10 10:38:37,488 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-07-10 10:38:40,336 - INFO - joeynmt.training - Epoch  30, Step:     8900, Batch Loss:     2.915074, Tokens per Sec:    12269, Lr: 0.000300\n",
      "2021-07-10 10:38:46,668 - INFO - joeynmt.training - Epoch  30, Step:     9000, Batch Loss:     2.881948, Tokens per Sec:    11096, Lr: 0.000300\n",
      "2021-07-10 10:39:14,877 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:39:14,878 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:39:14,878 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:39:15,190 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:39:15,190 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:39:16,317 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:39:16,318 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:39:16,318 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:39:16,319 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo , naboolela Yesu ari , “ Omundu yesi oumutiile , ne , niyamboolela ari , “ Isie nisie , nditsa , ne , nditsa , ndalola , ne ndalola , enzia , ne siesi enzia . ”\n",
      "2021-07-10 10:39:16,319 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:39:16,319 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:39:16,319 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:39:16,320 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , omwitookho elo , yamwitsila , yasinjila , ne namuboolela ari , “ Omwami , olole , olole , olole , olole , olole , olole . ”\n",
      "2021-07-10 10:39:16,320 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:39:16,320 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaho khunyanga eya Isabato , yaho , khunyanga eya Isabato , yaho , khunyanga eya Isabato ,\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:39:16,321 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:39:16,322 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo , yenjisia , ne , niyenjisia , mana , naboha , ne naniina , mana , naboha , ne naniala , olusimbi lwomulwomuyeka okwo .\n",
      "2021-07-10 10:39:16,322 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step     9000: bleu:   4.05, loss: 126229.0703, ppl:  30.3696, duration: 29.6530s\n",
      "2021-07-10 10:39:22,186 - INFO - joeynmt.training - Epoch  30, Step:     9100, Batch Loss:     2.785678, Tokens per Sec:    11888, Lr: 0.000300\n",
      "2021-07-10 10:39:26,113 - INFO - joeynmt.training - Epoch  30: total training loss 883.99\n",
      "2021-07-10 10:39:26,113 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-07-10 10:39:26,114 - INFO - joeynmt.training - Best validation result (greedy) at step     9000:  30.37 ppl.\n",
      "2021-07-10 10:39:26,133 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-10 10:39:26,494 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 10:39:26,678 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 10:39:26,751 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.lh)...\n",
      "2021-07-10 10:39:56,542 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:39:56,542 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:39:56,543 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:39:56,819 - INFO - joeynmt.prediction -  dev bleu[13a]:   4.64 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 10:39:56,823 - INFO - joeynmt.prediction - Translations saved to: models/enlh_transformer/00009000.hyps.dev\n",
      "2021-07-10 10:39:56,824 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.lh)...\n",
      "2021-07-10 10:40:27,097 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:40:27,097 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:40:27,097 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:40:27,391 - INFO - joeynmt.prediction - test bleu[13a]:   4.49 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 10:40:27,396 - INFO - joeynmt.prediction - Translations saved to: models/enlh_transformer/00009000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ93HroS3Saq"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 9000\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"joeynmt/models/enlh_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/{name}_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/enlh_transformer\"', f'model_dir: \"models/enlh_transformer_continued\"')\n",
    "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-0GBGh0B_vi",
    "outputId": "1209d812-f4f3-4a76-fb12-47a4abd253b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"enlh_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"en\"\n",
      "    trg: \"lh\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/enlh_transformer/9000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 1096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 3600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 200         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/enlh_transformer_continued\"\n",
      "    overwrite: False\n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/transformer_enlh_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxHfalSrCHj6",
    "outputId": "fb32f6bb-03f4-45a6-ebf1-38921cdb0ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-10 10:58:03,748 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-10 10:58:03,782 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-10 10:58:03,854 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-10 10:58:04,095 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-10 10:58:04,114 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-10 10:58:04,127 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-10 10:58:04,127 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 10:58:04,331 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 10:58:04.499348: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-10 10:58:05,951 - INFO - joeynmt.training - Total params: 12097024\n",
      "2021-07-10 10:58:09,252 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/enlh_transformer/9000.ckpt\n",
      "2021-07-10 10:58:09,707 - INFO - joeynmt.helpers - cfg.name                           : enlh_transformer\n",
      "2021-07-10 10:58:09,707 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.trg                       : lh\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/train.bpe\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-10 10:58:09,708 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab.txt\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/enlh_transformer/9000.ckpt\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-10 10:58:09,709 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-10 10:58:09,710 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.batch_size            : 1096\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-10 10:58:09,711 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 200\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/enlh_transformer_continued\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
      "2021-07-10 10:58:09,712 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-10 10:58:09,713 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-10 10:58:09,714 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-10 10:58:09,715 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-10 10:58:09,716 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 5904,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "\t[TRG] Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) and (8) to (9) of\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - Number of Src words (types): 4050\n",
      "2021-07-10 10:58:09,717 - INFO - joeynmt.helpers - Number of Trg words (types): 4050\n",
      "2021-07-10 10:58:09,718 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4050),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4050))\n",
      "2021-07-10 10:58:09,729 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 1096\n",
      "\ttotal batch size (w. parallel & accumulation): 1096\n",
      "2021-07-10 10:58:09,729 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-10 10:58:15,633 - INFO - joeynmt.training - Epoch   1, Step:     9100, Batch Loss:     2.815874, Tokens per Sec:    11812, Lr: 0.000300\n",
      "2021-07-10 10:58:18,982 - INFO - joeynmt.training - Epoch   1: total training loss 461.35\n",
      "2021-07-10 10:58:18,982 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-10 10:58:21,432 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     3.022842, Tokens per Sec:    11915, Lr: 0.000300\n",
      "2021-07-10 10:58:49,659 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:58:49,660 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:58:49,660 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:58:49,975 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:58:49,976 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:58:50,611 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:58:50,612 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:58:50,612 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:58:50,612 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , bamusila , ne , namuboolela ari , “ Omundu yesi oumutiile , okhutiilakhwo , ne , ndalamuhelesia , eshitabu eshikanye shianje . ”\n",
      "2021-07-10 10:58:50,612 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyenjisinjia , amakhuwa ako , yamwitsila namurusia , ne namuboolela ari , “ Omwami , witsulilanga , mbu , olole . ”\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:58:50,613 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo , khunyanga eya Isabato , yalimwo lisabo liabandu , abandu abanji .\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:58:50,614 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:58:50,615 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , mana , nabarechelesia , olukhongo lwomuyeka kwomukhuyu kwkwarulukha , mana , nakusilia , omuyeka kwako nikarulukha , mana , nakusilia .\n",
      "2021-07-10 10:58:50,615 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     9200: bleu:   4.09, loss: 126204.0547, ppl:  30.3490, duration: 29.1821s\n",
      "2021-07-10 10:58:56,466 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     2.703839, Tokens per Sec:    12327, Lr: 0.000300\n",
      "2021-07-10 10:59:02,807 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     2.915945, Tokens per Sec:    11221, Lr: 0.000300\n",
      "2021-07-10 10:59:25,785 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 10:59:25,786 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 10:59:25,786 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 10:59:26,101 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 10:59:26,101 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 10:59:26,742 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 10:59:26,744 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 10:59:26,744 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 10:59:26,744 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , nababoolela ari , “ Omundu yesi oumutiile , okhutiilakhwo , ne , ndalola , shinga olwa nditsa , ndachinjile . ”\n",
      "2021-07-10 10:59:26,744 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 10:59:26,745 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 10:59:26,745 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 10:59:26,745 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yesu yali niyenjisinjia amakhuwa ako , yasinjila , natsia , tsimbilo , naboola ari , “ Omwami , ”\n",
      "2021-07-10 10:59:26,745 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga yambeli , khunyanga yambeli yaho , khunyanga eya Isabato , abandu bataru bataru .\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 10:59:26,746 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 10:59:26,747 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 10:59:26,747 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabohwa , tsingubo tsiomunyiri . Mana Paulo , niyenjilamwo , ne natiila mushiina shiayo , ne natsayi eyo , yenyokha .\n",
      "2021-07-10 10:59:26,747 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     9400: bleu:   4.29, loss: 126050.9922, ppl:  30.2237, duration: 23.9392s\n",
      "2021-07-10 10:59:30,572 - INFO - joeynmt.training - Epoch   2: total training loss 874.14\n",
      "2021-07-10 10:59:30,572 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-10 10:59:32,643 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     2.666778, Tokens per Sec:    12110, Lr: 0.000300\n",
      "2021-07-10 10:59:39,012 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     2.913727, Tokens per Sec:    10806, Lr: 0.000300\n",
      "2021-07-10 11:00:10,864 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:00:10,865 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:00:10,865 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:00:11,846 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:00:11,847 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:00:11,847 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:00:11,847 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo nende Abafarisayo , nibaboolanga mbu , “ Omundu yesi yesi oumutiile , ne , niyamboolela ari , “ Isie nditsa , nditsa , ne , nditsa , enzia . ”\n",
      "2021-07-10 11:00:11,847 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali niyenjisinjia amakhuwa ako , yasinjila , natsia , tsimbilo , naboolela Yesu ari , “ Omwechesia , olole , olole . ”\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:00:11,848 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , abandu ba Kaisaria , baliho khunyanga eya Isabato ,\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:00:11,849 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabohwa , tsingubo tsiomunyiri , mana nibatsuuuna , mana nibachinjia , okhwinwa . Ne olwa yenjilamwo , yenjilamwo yinyoola yinyoola .\n",
      "2021-07-10 11:00:11,850 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     9600: bleu:   4.32, loss: 126058.9141, ppl:  30.2302, duration: 32.8369s\n",
      "2021-07-10 11:00:17,781 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     2.961181, Tokens per Sec:    12174, Lr: 0.000300\n",
      "2021-07-10 11:00:22,445 - INFO - joeynmt.training - Epoch   3: total training loss 860.57\n",
      "2021-07-10 11:00:22,445 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-10 11:00:24,260 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     2.845104, Tokens per Sec:    11519, Lr: 0.000300\n",
      "2021-07-10 11:00:48,993 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:00:48,994 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:00:48,994 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:00:49,312 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:00:49,312 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:00:50,060 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:00:50,061 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:00:50,061 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:00:50,061 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Omundu yesi oumutiile , okhutiilakhwo , ne , ndalola , ebiamo ebinji . ”\n",
      "2021-07-10 11:00:50,061 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:00:50,062 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:00:50,062 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:00:50,062 - INFO - joeynmt.training - \tHypothesis: Ne olwa Herode yali nashiboolanga amakhuwa ako , yamwitsila , yasinjila , ne namuboolela ari , “ Omwechesia , iwe , Omusamaria , ”\n",
      "2021-07-10 11:00:50,062 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:00:50,063 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:00:50,064 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:00:50,064 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo , khunyanga eya Isabato , abandu babili bataru bataru , baliho , khunyanga eya Isabato .\n",
      "2021-07-10 11:00:50,064 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:00:50,065 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:00:50,065 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:00:50,065 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , ne , nabohwa , ne , nabiria , mana , nabiria , mana , nabiria , nibachinjile , okhwibishilo . Ne olwa yenjilamwo , yeniya .\n",
      "2021-07-10 11:00:50,065 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     9800: bleu:   4.41, loss: 125691.2031, ppl:  29.9311, duration: 25.8044s\n",
      "2021-07-10 11:00:55,954 - INFO - joeynmt.training - Epoch   4, Step:     9900, Batch Loss:     2.790478, Tokens per Sec:    12052, Lr: 0.000300\n",
      "2021-07-10 11:01:02,287 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     2.732747, Tokens per Sec:    11368, Lr: 0.000300\n",
      "2021-07-10 11:01:32,220 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:01:32,220 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:01:32,220 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:01:32,528 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:01:32,528 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:01:33,253 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:01:33,255 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:01:33,255 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:01:33,255 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi oumutiile , okhutiilakhwo , ne , ndalola , ne nenzia , ne , nenzia . ”\n",
      "2021-07-10 11:01:33,256 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:01:33,256 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:01:33,256 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:01:33,256 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali nashiboolanga amakhuwa ako , yasinjila , namusinjila namusinjila , namuboolela ari , “ Omwechesia , olole ! ”\n",
      "2021-07-10 11:01:33,257 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:01:33,257 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:01:33,257 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:01:33,257 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo , khunyanga eya Isabato , yalimwo lisabo lia Pasaka .\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabohela , tsingubo tsiabwe , mana , nibaboha , tsingubo tsiomunyiri . Ne , yenyokha okwo , nikwolulimi lwokhwibohwa , mana , nibachitiilwa .\n",
      "2021-07-10 11:01:33,258 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    10000: bleu:   4.43, loss: 125599.3281, ppl:  29.8568, duration: 30.9707s\n",
      "2021-07-10 11:01:37,759 - INFO - joeynmt.training - Epoch   4: total training loss 848.22\n",
      "2021-07-10 11:01:37,760 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-10 11:01:39,200 - INFO - joeynmt.training - Epoch   5, Step:    10100, Batch Loss:     2.561778, Tokens per Sec:    11834, Lr: 0.000300\n",
      "2021-07-10 11:01:45,626 - INFO - joeynmt.training - Epoch   5, Step:    10200, Batch Loss:     2.706297, Tokens per Sec:    10987, Lr: 0.000300\n",
      "2021-07-10 11:02:14,653 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:02:14,654 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:02:14,654 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:02:14,972 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:02:14,972 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:02:15,683 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:02:15,684 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:02:15,684 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:02:15,684 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibareeba Yesu ari , “ Omundu yesi oumutiile , ne , niyamboolela ari , “ Enzenya , enzenya , enzenya , okhunzira . ”\n",
      "2021-07-10 11:02:15,684 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:02:15,685 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:02:15,685 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:02:15,685 - INFO - joeynmt.training - \tHypothesis: Ne olwa Yosefu yalola , amakhuwa ako , yamwitsila namulola , namuboolela ari , “ Mariamu , witse , ”\n",
      "2021-07-10 11:02:15,685 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:02:15,686 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:02:15,686 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:02:15,686 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu abanji bataru , baliho khunyanga eya Isabato ,\n",
      "2021-07-10 11:02:15,686 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:02:15,687 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:02:15,687 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:02:15,687 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yamenya Korinelio , mana , yakwa mumeeli , mana , niyefwalile tsingubo tsiomunyiri , mana , nibakwa mumeeli , mana , niyefwalile tsingubo tsiomunyiri .\n",
      "2021-07-10 11:02:15,687 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    10200: bleu:   4.39, loss: 125502.9375, ppl:  29.7791, duration: 30.0607s\n",
      "2021-07-10 11:02:21,594 - INFO - joeynmt.training - Epoch   5, Step:    10300, Batch Loss:     2.588690, Tokens per Sec:    12132, Lr: 0.000300\n",
      "2021-07-10 11:02:26,701 - INFO - joeynmt.training - Epoch   5: total training loss 834.76\n",
      "2021-07-10 11:02:26,701 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-10 11:02:27,959 - INFO - joeynmt.training - Epoch   6, Step:    10400, Batch Loss:     2.486108, Tokens per Sec:    11262, Lr: 0.000300\n",
      "2021-07-10 11:02:52,623 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:02:52,623 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:02:52,624 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:02:52,937 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:02:52,937 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:02:53,609 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:02:53,610 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:02:53,610 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:02:53,610 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , bamureeba mbu , “ Omundu yesi oumutiile , ne , nenzie , ne , ndalola . ”\n",
      "2021-07-10 11:02:53,610 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:02:53,611 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:02:53,611 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:02:53,611 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , amakhuwa ako , yalimwo Mariamu , yamwitsila namulola , namuboolela ari , “ Omwami , witse , mukholeele ! ”\n",
      "2021-07-10 11:02:53,611 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:02:53,612 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:02:53,612 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:02:53,612 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu abanji , bahonokooshe khunyanga eya Isabato .\n",
      "2021-07-10 11:02:53,612 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:02:53,613 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:02:53,613 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:02:53,613 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabarusia , mana , nabaremwa nikarusibwemwo , mana , nabaremwa nikaremwa nikasukunwa munyanza .\n",
      "2021-07-10 11:02:53,613 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    10400: bleu:   4.71, loss: 125289.6797, ppl:  29.6078, duration: 25.6532s\n",
      "2021-07-10 11:02:59,521 - INFO - joeynmt.training - Epoch   6, Step:    10500, Batch Loss:     2.754767, Tokens per Sec:    12128, Lr: 0.000300\n",
      "2021-07-10 11:03:05,962 - INFO - joeynmt.training - Epoch   6, Step:    10600, Batch Loss:     2.872789, Tokens per Sec:    11186, Lr: 0.000300\n",
      "2021-07-10 11:03:31,535 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:03:31,536 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:03:31,536 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:03:31,842 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:03:31,842 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:03:32,530 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:03:32,530 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:03:32,531 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:03:32,531 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu outiile , okhutiilakhwo , ne , niyamboolela ari , “ Nditsulilanga , shinga olwa nditsa , ndalola , ne ndalola , enzia . ”\n",
      "2021-07-10 11:03:32,531 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - \tHypothesis: Ne olwa yali nashiboolanga amakhuwa ako , yalimwo Mariamu , namurumile . Mana Yesu namuboolela ari , “ Omwami , olola , olole , olaba , ninashio . ”\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:03:32,532 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , yalimwo lisabo liabandu ebikhumila bibili .\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:03:32,533 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:03:32,535 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , yatsukha , ne naniina , mana nabarusia , mana nabarunda , nibakwa hasi . Mana , nibatsuuushilo eshibia yali niyiboha .\n",
      "2021-07-10 11:03:32,535 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    10600: bleu:   4.58, loss: 125227.2031, ppl:  29.5578, duration: 26.5723s\n",
      "2021-07-10 11:03:37,827 - INFO - joeynmt.training - Epoch   6: total training loss 822.21\n",
      "2021-07-10 11:03:37,827 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-10 11:03:38,839 - INFO - joeynmt.training - Epoch   7, Step:    10700, Batch Loss:     2.621115, Tokens per Sec:    11790, Lr: 0.000300\n",
      "2021-07-10 11:03:45,271 - INFO - joeynmt.training - Epoch   7, Step:    10800, Batch Loss:     2.478525, Tokens per Sec:    11152, Lr: 0.000300\n",
      "2021-07-10 11:04:08,309 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:04:08,310 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:04:08,310 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:04:09,265 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yamanya Yesu mbu , yaboolelakhwo omundu yesi yesi yesi , ne niyamboolela ari , “ Enzenya , okhunzira , ne ndalola , omukhuyu kuno nikulukha . ”\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:04:09,266 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , amakhuwa ako , yakalukha munzu , nyinamwana , namusaaya ari , “ Omwami , yambile , ”\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:04:09,267 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:04:09,268 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo lisabo liabandu , abandu bataru , abandu bataru , bali ahambi ahambi okhuula .\n",
      "2021-07-10 11:04:09,268 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:04:09,268 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:04:09,268 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:04:09,269 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , mana nabohwa , ne , nabohwa hasi , mana , nibahelesia eshinjia , mana nibatsuuushilamwo , eshibia , mana yenjilamwo .\n",
      "2021-07-10 11:04:09,269 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    10800: bleu:   4.65, loss: 125648.3828, ppl:  29.8964, duration: 23.9970s\n",
      "2021-07-10 11:04:15,312 - INFO - joeynmt.training - Epoch   7, Step:    10900, Batch Loss:     2.407380, Tokens per Sec:    11592, Lr: 0.000300\n",
      "2021-07-10 11:04:21,069 - INFO - joeynmt.training - Epoch   7: total training loss 818.83\n",
      "2021-07-10 11:04:21,070 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-10 11:04:21,728 - INFO - joeynmt.training - Epoch   8, Step:    11000, Batch Loss:     2.361984, Tokens per Sec:    11776, Lr: 0.000300\n",
      "2021-07-10 11:04:48,674 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:04:48,674 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:04:48,674 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:04:48,987 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:04:48,987 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:04:49,732 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:04:49,733 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:04:49,733 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:04:49,733 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , bamureeba mbu , “ Omundu yesi yesi , niyamboolela mbu , anywe , ne , ndalamutiilakhwo , nasi siesi , ndalola . ”\n",
      "2021-07-10 11:04:49,733 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:04:49,734 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:04:49,734 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:04:49,734 - INFO - joeynmt.training - \tHypothesis: Ne olwa Petero yali nashiboolanga amakhuwa ako , yasinjila , ne , namuboolela ari , “ Omwechesia , wenya , okhukhwitsulila . ”\n",
      "2021-07-10 11:04:49,734 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:04:49,735 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:04:49,735 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:04:49,735 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu abanji bataru , bataru , bataru , bataru .\n",
      "2021-07-10 11:04:49,735 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:04:49,736 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:04:49,736 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:04:49,736 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , yafimbwa , mana , nibaboha , mana nibachirusia , mana nibachirusia , mana nibachinjile , mana yebohwe , mana yebohwe .\n",
      "2021-07-10 11:04:49,736 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    11000: bleu:   4.81, loss: 125224.7500, ppl:  29.5559, duration: 28.0075s\n",
      "2021-07-10 11:04:55,791 - INFO - joeynmt.training - Epoch   8, Step:    11100, Batch Loss:     2.363841, Tokens per Sec:    11848, Lr: 0.000300\n",
      "2021-07-10 11:05:02,054 - INFO - joeynmt.training - Epoch   8, Step:    11200, Batch Loss:     2.571990, Tokens per Sec:    11283, Lr: 0.000300\n",
      "2021-07-10 11:05:30,828 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:05:30,828 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:05:30,828 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:05:31,771 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:05:31,771 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibakalukha khu Yesu , ne , naboolela Yesu ari , “ Endeebula , shinga olwa nditsa , nditsule , ne , ndikomba nindikhulanga . ”\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:05:31,772 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - \tHypothesis: Olwa Yesu yali nashiboolanga amakhuwa ako , yasinjila elwanyi , ne , namuboolela ari , “ Omwechesia , olole , olole , olole , olole ! ”\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu abanji barechekha , lisabo lia Pasaka .\n",
      "2021-07-10 11:05:31,773 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:05:31,774 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:05:31,775 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:05:31,775 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo , yatsuhanda , ne , nabarakhwo , tsingubo tsindiiti , tsiomunyiri . Ne olwa yakwa , mutsimbeka tsiosi tsiali nitsihwele , mana yinwa .\n",
      "2021-07-10 11:05:31,775 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    11200: bleu:   4.94, loss: 125290.9062, ppl:  29.6088, duration: 29.7213s\n",
      "2021-07-10 11:05:37,742 - INFO - joeynmt.training - Epoch   8: total training loss 809.72\n",
      "2021-07-10 11:05:37,742 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-10 11:05:38,051 - INFO - joeynmt.training - Epoch   9, Step:    11300, Batch Loss:     2.410297, Tokens per Sec:    11464, Lr: 0.000300\n",
      "2021-07-10 11:05:44,422 - INFO - joeynmt.training - Epoch   9, Step:    11400, Batch Loss:     2.827872, Tokens per Sec:    11158, Lr: 0.000300\n",
      "2021-07-10 11:06:11,742 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:06:11,743 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:06:11,743 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:06:12,743 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:06:12,743 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:06:12,744 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:06:12,745 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo bandi nibetsa khu Yesu , ne , nababoolela ari , “ Endeebula , shinga olwa ndebukwe , ne nenzia , nenzia , ne nenzia . ”\n",
      "2021-07-10 11:06:12,746 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:06:12,746 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:06:12,746 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:06:12,746 - INFO - joeynmt.training - \tHypothesis: Ne olwa Paulo yali niyenjisinjia amakhuwa ako , yamwitsila namubooleele ari , “ Omwechesia , witse , munzu mwa Yesu , yamwitsila , mana , namubooleele . ”\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu bataru bataru bataru bataru , bahoniibwe .\n",
      "2021-07-10 11:06:12,747 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:06:12,748 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:06:12,748 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:06:12,748 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yamenya Korinelio , yatarandula tsingubo tsiabwe , mana , niyefwalile tsingubo tsiomunyiri , mana , niyefwalile tsingubo tsiomunyiri . Ne olwa yenjilamwo , yenoosia okhwinoosia shinga lwomuyeka kwomumioyo , kwatsayi .\n",
      "2021-07-10 11:06:12,748 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    11400: bleu:   5.04, loss: 125270.7109, ppl:  29.5927, duration: 28.3260s\n",
      "2021-07-10 11:06:18,616 - INFO - joeynmt.training - Epoch   9, Step:    11500, Batch Loss:     2.530859, Tokens per Sec:    12122, Lr: 0.000300\n",
      "2021-07-10 11:06:25,059 - INFO - joeynmt.training - Epoch   9, Step:    11600, Batch Loss:     2.617125, Tokens per Sec:    10899, Lr: 0.000300\n",
      "2021-07-10 11:06:49,280 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:06:49,280 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:06:49,280 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:06:50,255 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:06:50,256 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:06:50,256 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:06:50,256 - INFO - joeynmt.training - \tHypothesis: Mana Abafarisayo bandi nibetsa khu Yesu , ne , nababoolela ari , “ Omundu yesi yesi , niyamboolela mbu , enzie , ndalola , ne ndalola , shinga olwa nditsa . ”\n",
      "2021-07-10 11:06:50,256 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:06:50,257 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:06:50,257 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:06:50,257 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , amakhuwa ako , Petero yali niyenjisinjia , omukhaana wabwe , namuboolela ari , “ Omwechesia , omwamwiwibwi , iwe witsa . ”\n",
      "2021-07-10 11:06:50,257 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:06:50,258 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:06:50,258 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:06:50,258 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , yalimwo khunyanga eya Isabato , khunyanga eya Isabato , yalimwo khunyanga eya Isabato ,\n",
      "2021-07-10 11:06:50,258 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:06:50,259 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:06:50,259 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:06:50,259 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , nibabohela tsingubo tsiomunyiri , mana nabarulula , mana nabarulula , emiandu chitaru , ne nabarulula , eminyoola .\n",
      "2021-07-10 11:06:50,259 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    11600: bleu:   4.68, loss: 125587.0625, ppl:  29.8469, duration: 25.2000s\n",
      "2021-07-10 11:06:50,335 - INFO - joeynmt.training - Epoch   9: total training loss 799.19\n",
      "2021-07-10 11:06:50,336 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-10 11:06:56,173 - INFO - joeynmt.training - Epoch  10, Step:    11700, Batch Loss:     2.498553, Tokens per Sec:    12146, Lr: 0.000300\n",
      "2021-07-10 11:07:02,487 - INFO - joeynmt.training - Epoch  10, Step:    11800, Batch Loss:     2.479457, Tokens per Sec:    10868, Lr: 0.000300\n",
      "2021-07-10 11:07:37,887 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:07:37,887 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:07:37,888 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:07:38,867 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:07:38,868 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:07:38,868 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:07:38,868 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , Yesu yamanya mbu , Yesu yambilwe , ne nababoolela ari , “ Endeebula , ne , ndalamutiilakhwo , ne nenzie . ”\n",
      "2021-07-10 11:07:38,868 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:07:38,869 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:07:38,869 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:07:38,869 - INFO - joeynmt.training - \tHypothesis: Ne olwa Pilato yali nashiboolanga amakhuwa ako , yasinjila , natsia ewa Mariamu yali namusaaya ari , “ Omwechesia , ”\n",
      "2021-07-10 11:07:38,869 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:07:38,870 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:07:38,870 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:07:38,870 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Isabato , khunyanga eya Isabato , abandu bataru bataru bahoniibwe .\n",
      "2021-07-10 11:07:38,870 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:07:38,871 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:07:38,871 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:07:38,871 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohela , tsingubo tsiabwe mana , nibabukula , tsimbeka tsiosi . Mana Paulo nalaka mbu , “ Ikonika , kwomuyeka kwasimeeli yakwa , mutsimbwa tsiomunyiri . ”\n",
      "2021-07-10 11:07:38,871 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    11800: bleu:   4.82, loss: 125947.5078, ppl:  30.1392, duration: 36.3842s\n",
      "2021-07-10 11:07:44,792 - INFO - joeynmt.training - Epoch  10, Step:    11900, Batch Loss:     2.730109, Tokens per Sec:    12287, Lr: 0.000300\n",
      "2021-07-10 11:07:45,216 - INFO - joeynmt.training - Epoch  10: total training loss 791.60\n",
      "2021-07-10 11:07:45,216 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-10 11:07:51,191 - INFO - joeynmt.training - Epoch  11, Step:    12000, Batch Loss:     2.633260, Tokens per Sec:    11228, Lr: 0.000300\n",
      "2021-07-10 11:08:15,378 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:08:15,378 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:08:15,378 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:08:16,380 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:08:16,381 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:08:16,381 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:08:16,381 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi bandi , bamureeba mbu , “ Omundu yesi yesi outiilakhwo , ne , niyenjiile , ne nemirwe chianje . ”\n",
      "2021-07-10 11:08:16,381 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:08:16,382 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:08:16,382 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:08:16,382 - INFO - joeynmt.training - \tHypothesis: Ne olwa Pilato yali nashiboolanga amakhuwa ako , yasinjila elwanyi , ne , namuboolela ari , “ Omwechesia , wabeele , omwitsawo . ”\n",
      "2021-07-10 11:08:16,382 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu bataru bataru , bataru bataru , bachesi .\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:08:16,383 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:08:16,384 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:08:16,384 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , mana , nabuukha , nabaremwa nikarulamwo , mana , nibakwa mumeeli . Mana nibakwa mumeeli , mutsimbeka tsiosi tsiali nitsihutsa .\n",
      "2021-07-10 11:08:16,384 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    12000: bleu:   4.88, loss: 125565.1484, ppl:  29.8292, duration: 25.1926s\n",
      "2021-07-10 11:08:22,264 - INFO - joeynmt.training - Epoch  11, Step:    12100, Batch Loss:     2.692736, Tokens per Sec:    12185, Lr: 0.000300\n",
      "2021-07-10 11:08:28,582 - INFO - joeynmt.training - Epoch  11, Step:    12200, Batch Loss:     2.529123, Tokens per Sec:    10971, Lr: 0.000300\n",
      "2021-07-10 11:08:58,858 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:08:58,858 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:08:58,859 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:08:59,854 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:08:59,855 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:08:59,855 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:08:59,855 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , Abafarisayo bamureeba mbu , “ Ounzikaane , ” Yesu nababoolela ari , “ Enditsa , ne , ndalamuhelesia , ne , emirwe chienywe . ”\n",
      "2021-07-10 11:08:59,855 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:08:59,856 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:08:59,856 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:08:59,856 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yamwitsila , yasinjila elwanyi , namuboolela ari , “ Omwami , witse , mwitsile . ”\n",
      "2021-07-10 11:08:59,856 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:08:59,857 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:08:59,857 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:08:59,857 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo Isabato , khunyanga eya Isabato , abandu abanji bahonokokha .\n",
      "2021-07-10 11:08:59,857 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:08:59,858 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:08:59,858 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:08:59,858 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana , nibabohela , tsingubo tsiomunyiri . Mana , nibenjila muliaro , nibachinjile , okhwinia , mana nibachirusia , mana nibachinjile .\n",
      "2021-07-10 11:08:59,858 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    12200: bleu:   4.75, loss: 125384.8281, ppl:  29.6841, duration: 31.2754s\n",
      "2021-07-10 11:09:00,679 - INFO - joeynmt.training - Epoch  11: total training loss 781.74\n",
      "2021-07-10 11:09:00,679 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-10 11:09:05,925 - INFO - joeynmt.training - Epoch  12, Step:    12300, Batch Loss:     2.622324, Tokens per Sec:    11967, Lr: 0.000210\n",
      "2021-07-10 11:09:12,240 - INFO - joeynmt.training - Epoch  12, Step:    12400, Batch Loss:     2.682939, Tokens per Sec:    11245, Lr: 0.000210\n",
      "2021-07-10 11:09:40,498 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:09:40,499 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:09:40,499 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:09:40,815 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:09:40,816 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:09:41,510 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:09:41,511 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:09:41,511 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:09:41,511 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo bamureeba bari , “ Omundu yesi , ouboola mbu , anyalilwe okhulola , ne nemirwe chianje . ”\n",
      "2021-07-10 11:09:41,512 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:09:41,513 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:09:41,513 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:09:41,513 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola amakhuwa ako , yakalukha ingo , yalola Mariamu , namusinjila namusinjila naboola ari , “ Omwechesia , witse , muchele ! ”\n",
      "2021-07-10 11:09:41,514 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:09:41,514 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:09:41,514 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:09:41,514 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , abandu bataru bataru bahoniibwe .\n",
      "2021-07-10 11:09:41,515 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:09:41,515 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:09:41,515 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:09:41,516 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , nibabohwa hasi , mana nibaboholela , tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluyoka lwolufwa lwomutarakwe .\n",
      "2021-07-10 11:09:41,516 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    12400: bleu:   5.24, loss: 125131.1797, ppl:  29.4812, duration: 29.2752s\n",
      "2021-07-10 11:09:47,387 - INFO - joeynmt.training - Epoch  12, Step:    12500, Batch Loss:     2.704675, Tokens per Sec:    12143, Lr: 0.000210\n",
      "2021-07-10 11:09:48,412 - INFO - joeynmt.training - Epoch  12: total training loss 755.69\n",
      "2021-07-10 11:09:48,413 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-10 11:09:53,819 - INFO - joeynmt.training - Epoch  13, Step:    12600, Batch Loss:     2.553833, Tokens per Sec:    10869, Lr: 0.000210\n",
      "2021-07-10 11:10:18,989 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:10:18,989 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:10:18,989 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:10:19,301 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:10:19,301 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:10:19,996 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:10:19,997 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:10:19,997 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:10:19,997 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Enditsa , ne , nemekha , ne nemalile , okhutiilakhwo . ”\n",
      "2021-07-10 11:10:19,997 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:10:19,998 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:10:19,998 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:10:19,998 - INFO - joeynmt.training - \tHypothesis: Ne olwa yoola ako , yalimwo omundu undi yetsa , nashilondakhwo , yaboola ari , “ Omwechesia , witse , injelekha wa Yesu , wabali. ”\n",
      "2021-07-10 11:10:19,998 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yelilukha , khunyanga eya Isabato , ne khunyanga eya Isabato , yanyoolamwo .\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:10:19,999 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:10:20,000 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yamenya , Paulo nende Sila , mana nibabohwa hasi , mana , nibabohololo , mana nibenjilamwo , shinga lwoluyoka lwelu lwomuyeka . Ne olwa yenjilamwo , yafimbwa omurwe .\n",
      "2021-07-10 11:10:20,000 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12600: bleu:   5.48, loss: 125061.5156, ppl:  29.4257, duration: 26.1804s\n",
      "2021-07-10 11:10:25,945 - INFO - joeynmt.training - Epoch  13, Step:    12700, Batch Loss:     2.373684, Tokens per Sec:    11999, Lr: 0.000210\n",
      "2021-07-10 11:10:32,230 - INFO - joeynmt.training - Epoch  13, Step:    12800, Batch Loss:     2.581828, Tokens per Sec:    11260, Lr: 0.000210\n",
      "2021-07-10 11:10:55,667 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:10:55,667 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:10:55,667 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:10:55,973 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-10 11:10:55,973 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-10 11:10:56,651 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:10:56,652 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:10:56,652 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:10:56,652 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , nibakalukha khu Yesu , ne , naboolela Yesu ari , “ Enjamile , amombakho , ne , ndalola amombakhwa , ne ndalola . ”\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila elwanyi , ne , naboolela Mariamu ari , “ Chende khutsie ewanje , ” ,\n",
      "2021-07-10 11:10:56,653 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:10:56,654 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:10:56,654 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:10:56,654 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , khunyanga eya Isabato , abandu bali , nibarechekha lisabo lia Pasaka .\n",
      "2021-07-10 11:10:56,654 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:10:56,655 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:10:56,655 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:10:56,655 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana , nibabohela olusimbi lwomuyeka kwarula. , Mana nibakwa hasi , mutsimbwa tsiali tsiali nitsimutsimbeka tsiosi .\n",
      "2021-07-10 11:10:56,655 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12800: bleu:   5.17, loss: 124865.2812, ppl:  29.2700, duration: 24.4245s\n",
      "2021-07-10 11:10:58,031 - INFO - joeynmt.training - Epoch  13: total training loss 750.81\n",
      "2021-07-10 11:10:58,032 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-10 11:11:02,558 - INFO - joeynmt.training - Epoch  14, Step:    12900, Batch Loss:     2.254772, Tokens per Sec:    12016, Lr: 0.000210\n",
      "2021-07-10 11:11:08,894 - INFO - joeynmt.training - Epoch  14, Step:    13000, Batch Loss:     2.652177, Tokens per Sec:    11268, Lr: 0.000210\n",
      "2021-07-10 11:11:34,540 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:11:34,540 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:11:34,541 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:11:35,512 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:11:35,513 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:11:35,513 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:11:35,513 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi , nibamuboolela bari , “ Omundu yesi outiile , okhumutiilakhwo , ne , namarwi kura , ne , nenzia . ”\n",
      "2021-07-10 11:11:35,513 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:11:35,514 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:11:35,514 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:11:35,514 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo , natsia , nibaboolela Mariamu ari , “ Omwechesia , wabeele , omwamiwe ! ”\n",
      "2021-07-10 11:11:35,514 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:11:35,515 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:11:35,515 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:11:35,515 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , eliuba nilihwele , khunyanga yambeli , yaho , khunyanga eya Isabato ,\n",
      "2021-07-10 11:11:35,515 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:11:35,516 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:11:35,516 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:11:35,516 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana nibahelesia , olukhongo lwabwe lwomuyeka , kwarula mumunwa kwkwkwasimwa . Ne olwa yali , niyenjilamwo , niyenjilamwo .\n",
      "2021-07-10 11:11:35,516 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    13000: bleu:   5.28, loss: 125309.5391, ppl:  29.6237, duration: 26.6220s\n",
      "2021-07-10 11:11:41,619 - INFO - joeynmt.training - Epoch  14, Step:    13100, Batch Loss:     2.663562, Tokens per Sec:    11532, Lr: 0.000210\n",
      "2021-07-10 11:11:43,585 - INFO - joeynmt.training - Epoch  14: total training loss 744.73\n",
      "2021-07-10 11:11:43,585 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-10 11:11:47,779 - INFO - joeynmt.training - Epoch  15, Step:    13200, Batch Loss:     2.669472, Tokens per Sec:    12064, Lr: 0.000210\n",
      "2021-07-10 11:12:11,568 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:12:11,568 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:12:11,568 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:12:12,537 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:12:12,538 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:12:12,538 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:12:12,538 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo , bamureeba bari , “ Omundu uno , yambelesie , ne , namarwi kanje , ndalola , ne , ndalola , shinga olwa ndikomba . ”\n",
      "2021-07-10 11:12:12,539 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:12:12,539 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:12:12,539 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:12:12,539 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yachaaka okhuboola , amakhuwa ako , yasinjila hakari wabwe , namuboolela ari , “ Omwami , witse , muno. ” Yesu ahonia omundu oyo ,\n",
      "2021-07-10 11:12:12,540 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:12:12,540 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:12:12,540 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:12:12,541 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , yaho , khunyanga eya Isabato , abandu bataru bataru bataru bahoniibwe .\n",
      "2021-07-10 11:12:12,541 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:12:12,542 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:12:12,542 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:12:12,542 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , mana , nabohwa hasi , mana , nibeluuuuuyia . Mana nibafimbwa , ne nibafimbwa omurwe .\n",
      "2021-07-10 11:12:12,542 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    13200: bleu:   5.07, loss: 125345.5469, ppl:  29.6526, duration: 24.7632s\n",
      "2021-07-10 11:12:18,433 - INFO - joeynmt.training - Epoch  15, Step:    13300, Batch Loss:     2.589403, Tokens per Sec:    12181, Lr: 0.000210\n",
      "2021-07-10 11:12:24,746 - INFO - joeynmt.training - Epoch  15, Step:    13400, Batch Loss:     2.785532, Tokens per Sec:    11125, Lr: 0.000210\n",
      "2021-07-10 11:12:52,402 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:12:52,403 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:12:52,403 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:12:53,377 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:12:53,377 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:12:53,378 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:12:53,378 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno , ali namarwi , ne , nanyiekhwo , ne , nenzie , ne olwa nditsulilanga , ebindu biosi . ”\n",
      "2021-07-10 11:12:53,378 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yachaaka okhuboola , akali nikekholeeshe , Yesu , namurumila naboola ari , “ Omwechesia , iwe , wabeele , okhukholeele tsimbabasi ! ”\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:12:53,379 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:12:53,380 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:12:53,380 - INFO - joeynmt.training - \tHypothesis: Ne khunyanga eya Isabato , Isabato , liliho , khunyanga eya Isabato , ne khunyanga eya Isabato , yirulangamwo abandu babili .\n",
      "2021-07-10 11:12:53,380 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:12:53,380 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:12:53,381 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:12:53,381 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana , nibaboholela shinga oluyoka lwomulilo . Mana , nibatsuuushilamwo , oluyoka lwelu lwomuyeka kwarakwe .\n",
      "2021-07-10 11:12:53,381 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    13400: bleu:   5.34, loss: 125366.8906, ppl:  29.6697, duration: 28.6345s\n",
      "2021-07-10 11:12:55,460 - INFO - joeynmt.training - Epoch  15: total training loss 737.49\n",
      "2021-07-10 11:12:55,461 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-10 11:12:59,294 - INFO - joeynmt.training - Epoch  16, Step:    13500, Batch Loss:     2.462835, Tokens per Sec:    12422, Lr: 0.000210\n",
      "2021-07-10 11:13:05,687 - INFO - joeynmt.training - Epoch  16, Step:    13600, Batch Loss:     2.268238, Tokens per Sec:    10940, Lr: 0.000210\n",
      "2021-07-10 11:13:29,504 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:13:29,504 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:13:29,505 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:13:30,533 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:13:30,534 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:13:30,534 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:13:30,534 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Eshilenje shianje , ne , nesambambakhwo , ne olwa nditsulilanga , ebindu biosi . ”\n",
      "2021-07-10 11:13:30,534 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:13:30,535 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:13:30,535 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:13:30,535 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola amakhuwa ako , yalimwo , yalimwo , Mariamu natsia elwanyi , naboola ari , “ Omwechesia , wabeele , omwamiwe ! ”\n",
      "2021-07-10 11:13:30,535 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:13:30,536 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:13:30,536 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:13:30,536 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Isabato , yiramwo inyanga , eyilangwa mbu , khunyanga eya Isabato .\n",
      "2021-07-10 11:13:30,536 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:13:30,537 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:13:30,537 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:13:30,537 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Sila , mana , nabohwa hasi , mana , nibafimbula tsingubo tsiomunyiri . Mana , nibakwa hasi khulwa oluyoka lwomutarakwe , shichila yali niyirula. ,\n",
      "2021-07-10 11:13:30,537 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    13600: bleu:   5.23, loss: 125629.2578, ppl:  29.8810, duration: 24.8500s\n",
      "2021-07-10 11:13:36,525 - INFO - joeynmt.training - Epoch  16, Step:    13700, Batch Loss:     2.373380, Tokens per Sec:    11795, Lr: 0.000210\n",
      "2021-07-10 11:13:39,222 - INFO - joeynmt.training - Epoch  16: total training loss 727.65\n",
      "2021-07-10 11:13:39,222 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-10 11:13:42,825 - INFO - joeynmt.training - Epoch  17, Step:    13800, Batch Loss:     2.388986, Tokens per Sec:    12119, Lr: 0.000210\n",
      "2021-07-10 11:14:08,390 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:14:08,390 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:14:08,390 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:14:09,829 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:14:09,829 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nibamuboolela bari , “ Omundu uno , ali namarwi , ne , nanyoola olusimbi lwanje lwene , lwene , lwene , lwene olwo , ndaboolela , mbu , enzie . ”\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:14:09,830 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:14:09,831 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola , amakhuwa ako , yalimwo , yalimwo Mariamu natsia , tsimbilo naboola ari , “ Omwechesia , wabeele , Omwami ! ”\n",
      "2021-07-10 11:14:09,831 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:14:09,831 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , eliuba liebunganga , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:14:09,832 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yebohwa hasi , mana , nibaboha olusimbi lwomuyeka , kwarulula tsingubo tsiomunyiri . Ne olwa yali niyefwalile tsingubo tsiomunyiri , tsiomunyiri , kwokhwilukha .\n",
      "2021-07-10 11:14:09,833 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    13800: bleu:   5.36, loss: 125561.7891, ppl:  29.8265, duration: 27.0076s\n",
      "2021-07-10 11:14:15,770 - INFO - joeynmt.training - Epoch  17, Step:    13900, Batch Loss:     2.522278, Tokens per Sec:    12061, Lr: 0.000210\n",
      "2021-07-10 11:14:22,092 - INFO - joeynmt.training - Epoch  17, Step:    14000, Batch Loss:     2.430365, Tokens per Sec:    11027, Lr: 0.000210\n",
      "2021-07-10 11:14:47,321 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:14:47,322 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:14:47,322 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:14:48,289 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:14:48,290 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:14:48,290 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:14:48,290 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno ali namarwi , ne , namanyilekhwo mbu , ali shinga olwa nditsa , ne nemisi , ndalola , ne nenzie . ”\n",
      "2021-07-10 11:14:48,290 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:14:48,291 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:14:48,291 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:14:48,291 - INFO - joeynmt.training - \tHypothesis: Ne olwa yoola ako , yalimwo , yalimwo Mariamu , yakalukha munzu , ne namuboolela ari , “ Omwami , witse , mulole . ”\n",
      "2021-07-10 11:14:48,291 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , khunyanga eya Isabato , abandu bataru balilungwa , khunyanga eya Isabato ,\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:14:48,292 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:14:48,293 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:14:48,293 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yenjisia , mana , nabohwa hasi , mana , nibafimbwa hasi , mana , nibafimbwa hasi khulwa oluyoka lwelelikulu .\n",
      "2021-07-10 11:14:48,293 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    14000: bleu:   5.42, loss: 125649.4688, ppl:  29.8973, duration: 26.2003s\n",
      "2021-07-10 11:14:50,910 - INFO - joeynmt.training - Epoch  17: total training loss 717.15\n",
      "2021-07-10 11:14:50,911 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-10 11:14:54,189 - INFO - joeynmt.training - Epoch  18, Step:    14100, Batch Loss:     2.105112, Tokens per Sec:    12014, Lr: 0.000147\n",
      "2021-07-10 11:15:00,536 - INFO - joeynmt.training - Epoch  18, Step:    14200, Batch Loss:     2.347688, Tokens per Sec:    11238, Lr: 0.000147\n",
      "2021-07-10 11:15:23,985 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:15:23,985 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:15:23,985 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:15:24,963 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:15:24,964 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:15:24,964 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:15:24,964 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi , nibamuboolela bari , “ Omundu uno , naba nakhwemekha , ne , namarwi kobukusi bwarulula , ne , nemalile okhutiila . ”\n",
      "2021-07-10 11:15:24,965 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:15:24,966 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:15:24,966 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:15:24,966 - INFO - joeynmt.training - \tHypothesis: Ne olwa , Yosefu yali niyenjisinjia amakhuwa ako , yasinjila , natsia elwanyi , naboola ari , “ Omwechesia , witsile ! ”\n",
      "2021-07-10 11:15:24,967 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , khunyanga yambeli yambeli , yalimwo khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:15:24,968 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:15:24,969 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:15:24,969 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yenjisia , mana nabenama , mana nibafukula , tsingubo tsiomunyiri . Mana nibafukula , oluyoka lwoluyoka lwelu eyo , mana , niyefwalile olusimbi lwokhwikulu .\n",
      "2021-07-10 11:15:24,969 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    14200: bleu:   5.54, loss: 125612.6094, ppl:  29.8675, duration: 24.4329s\n",
      "2021-07-10 11:15:30,920 - INFO - joeynmt.training - Epoch  18, Step:    14300, Batch Loss:     2.474216, Tokens per Sec:    12188, Lr: 0.000147\n",
      "2021-07-10 11:15:34,166 - INFO - joeynmt.training - Epoch  18: total training loss 700.84\n",
      "2021-07-10 11:15:34,167 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-10 11:15:37,293 - INFO - joeynmt.training - Epoch  19, Step:    14400, Batch Loss:     2.051602, Tokens per Sec:    12240, Lr: 0.000147\n",
      "2021-07-10 11:15:58,807 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:15:58,808 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:15:58,808 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:15:59,765 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:15:59,766 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:15:59,766 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:15:59,766 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Lekha , eebula , ne , nenzie mwikanzu ndalola . ”\n",
      "2021-07-10 11:15:59,767 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:15:59,767 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:15:59,767 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:15:59,768 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo , natsia ewa Mariamu yali namubooleele ari , “ Omwechesia , witsile ! ”\n",
      "2021-07-10 11:15:59,768 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:15:59,768 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:15:59,768 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:15:59,769 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , lili ahambi okhuula , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:15:59,769 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:15:59,769 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:15:59,769 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:15:59,770 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia , Paulo nende Sila , mana nibaboha tsingubo tsiabwe , mana nibafukula , tsimbeka tsiosi . Mana nibafukula , oluyoka lwoluyoka lwelu eyo , mana , niyefwalile olusimbi lwomuyeka .\n",
      "2021-07-10 11:15:59,770 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    14400: bleu:   5.18, loss: 125494.4062, ppl:  29.7722, duration: 22.4764s\n",
      "2021-07-10 11:16:06,059 - INFO - joeynmt.training - Epoch  19, Step:    14500, Batch Loss:     2.284168, Tokens per Sec:    11299, Lr: 0.000147\n",
      "2021-07-10 11:16:12,425 - INFO - joeynmt.training - Epoch  19, Step:    14600, Batch Loss:     2.306697, Tokens per Sec:    11055, Lr: 0.000147\n",
      "2021-07-10 11:16:36,678 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:16:36,679 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:16:36,679 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:16:37,642 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:16:37,644 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:16:37,644 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:16:37,644 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yamanya aka Yesu yali , namubooleele ari , “ Lekha , yambelesiekhwo , ne , nenzie . ”\n",
      "2021-07-10 11:16:37,644 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:16:37,645 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:16:37,645 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:16:37,645 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila , natsia ewa Mariamu yali namubooleele ari , “ Omwechesia , omwitsawo , kube ninenywe. ” ,\n",
      "2021-07-10 11:16:37,645 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:16:37,646 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:16:37,646 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:16:37,646 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , ne khunyanga eya Isabato , yiraho .\n",
      "2021-07-10 11:16:37,646 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:16:37,647 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:16:37,647 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:16:37,647 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yafukula induli , mana , nibabohwa hasi , mana , boosi nibafimbwa hasi . Ne olwa yali niyemilila , okhwinia bieyali niyibohwa .\n",
      "2021-07-10 11:16:37,647 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    14600: bleu:   5.40, loss: 125533.8672, ppl:  29.8040, duration: 25.2214s\n",
      "2021-07-10 11:16:40,734 - INFO - joeynmt.training - Epoch  19: total training loss 700.29\n",
      "2021-07-10 11:16:40,735 - INFO - joeynmt.training - EPOCH 20\n",
      "2021-07-10 11:16:43,574 - INFO - joeynmt.training - Epoch  20, Step:    14700, Batch Loss:     2.422806, Tokens per Sec:    11947, Lr: 0.000147\n",
      "2021-07-10 11:16:49,955 - INFO - joeynmt.training - Epoch  20, Step:    14800, Batch Loss:     2.231076, Tokens per Sec:    11076, Lr: 0.000147\n",
      "2021-07-10 11:17:14,552 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:17:14,553 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:17:14,553 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:17:15,524 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:17:15,524 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:17:15,525 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:17:15,525 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nibamuboolela bari , “ Omundu uno ali , natseshelela , ne , namarwi kabili , ne , nemalile okhutiilakhwo , ne nimboola , mbu , enzie . ”\n",
      "2021-07-10 11:17:15,525 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila , natsia ewa Mariamu yali namubooleele ari , “ Omwechesia , wabeele , omwitsawe . ”\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:17:15,526 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:17:15,527 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:17:15,527 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , khunyanga eya Isabato , yiranga , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:17:15,527 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:17:15,527 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:17:15,528 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:17:15,528 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabalindi , mana , nibabohela tsingubo tsiomunyanza , mana , nibaboha amatsi nikarulula tsingubo tsiomunyiri . Ne olwa yali niyenjile , okhwinoosia okhwo mumaatsi , mana natsushilile .\n",
      "2021-07-10 11:17:15,528 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    14800: bleu:   5.50, loss: 125812.4688, ppl:  30.0294, duration: 25.5727s\n",
      "2021-07-10 11:17:21,619 - INFO - joeynmt.training - Epoch  20, Step:    14900, Batch Loss:     1.945742, Tokens per Sec:    11801, Lr: 0.000147\n",
      "2021-07-10 11:17:25,392 - INFO - joeynmt.training - Epoch  20: total training loss 695.92\n",
      "2021-07-10 11:17:25,393 - INFO - joeynmt.training - EPOCH 21\n",
      "2021-07-10 11:17:27,965 - INFO - joeynmt.training - Epoch  21, Step:    15000, Batch Loss:     2.404124, Tokens per Sec:    12060, Lr: 0.000147\n",
      "2021-07-10 11:17:51,107 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:17:51,107 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:17:51,107 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:17:52,077 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:17:52,078 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:17:52,078 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:17:52,078 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nibamuboolela bari , “ Omundu uno , ali namarwi , ne , naboolela Yesu ari , “ Enditsa , ndalamutiilakhwo , ne , ndalola neimba tsinzo , ne ndalola . ”\n",
      "2021-07-10 11:17:52,079 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:17:52,079 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:17:52,079 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:17:52,079 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo natsia , tsimbilo , naboolela Mariamu ari , “ Omwechesia , wabeele , omwamwibwi . ”\n",
      "2021-07-10 11:17:52,080 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:17:52,080 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:17:52,080 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:17:52,080 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yechesinjia , khunyanga eya Isabato , ne khunyanga eya Isabato , yiruungwa .\n",
      "2021-07-10 11:17:52,081 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:17:52,081 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:17:52,081 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:17:52,082 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe natsia , khung'asia halala nende Sila , nibaboha tsingubo tsiabwe , mana nibafukula , tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluchendo lwelibanzu eyo , mana , niyiboha .\n",
      "2021-07-10 11:17:52,082 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    15000: bleu:   5.48, loss: 126012.8984, ppl:  30.1926, duration: 24.1169s\n",
      "2021-07-10 11:17:58,005 - INFO - joeynmt.training - Epoch  21, Step:    15100, Batch Loss:     2.326992, Tokens per Sec:    12099, Lr: 0.000147\n",
      "2021-07-10 11:18:04,348 - INFO - joeynmt.training - Epoch  21, Step:    15200, Batch Loss:     2.484939, Tokens per Sec:    11226, Lr: 0.000147\n",
      "2021-07-10 11:18:29,357 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:18:29,358 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:18:29,358 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:18:30,338 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:18:30,339 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:18:30,339 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:18:30,339 - INFO - joeynmt.training - \tHypothesis: Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Omundu uno , yambelesie emusaaya mbu , emulambwe , ne olwa nditsulilanga , ebindu biosi . ”\n",
      "2021-07-10 11:18:30,339 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:18:30,340 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:18:30,340 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:18:30,340 - INFO - joeynmt.training - \tHypothesis: Ne olwa yalola amakhuwa ako , yatsukha natsia , tsimbilo , nasaaya Yesu ari , “ Omwechesia , wabeele , omwitsawo . ”\n",
      "2021-07-10 11:18:30,340 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , khunyanga eya Isabato , yirulayo , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:18:30,341 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:18:30,342 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:18:30,342 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi , mana , nibaboha hasi , mana , nibahelesia eshinyasio eshiraaka eshiyia eshiyia eshiraambi , mana , emeeli yebohwa .\n",
      "2021-07-10 11:18:30,342 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step    15200: bleu:   5.48, loss: 125897.8906, ppl:  30.0988, duration: 25.9936s\n",
      "2021-07-10 11:18:33,912 - INFO - joeynmt.training - Epoch  21: total training loss 686.00\n",
      "2021-07-10 11:18:33,913 - INFO - joeynmt.training - EPOCH 22\n",
      "2021-07-10 11:18:36,221 - INFO - joeynmt.training - Epoch  22, Step:    15300, Batch Loss:     2.342602, Tokens per Sec:    12307, Lr: 0.000103\n",
      "2021-07-10 11:18:42,585 - INFO - joeynmt.training - Epoch  22, Step:    15400, Batch Loss:     2.169968, Tokens per Sec:    11143, Lr: 0.000103\n",
      "2021-07-10 11:19:05,347 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:19:05,347 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:19:05,348 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:19:06,376 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:19:06,377 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:19:06,377 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:19:06,377 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nende Abafarisayo bamureeba bari , “ Omundu yesi , outiilakhwo , ne , namarwi ketsukhana , ne , namarwi kokhuhulila . ”\n",
      "2021-07-10 11:19:06,377 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:19:06,378 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:19:06,378 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:19:06,378 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , witse , witse ninaye , ”\n",
      "2021-07-10 11:19:06,378 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:19:06,379 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:19:06,379 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:19:06,379 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Isabato , khunyanga eya Isabato , yiraho , khunyanga eya Isabato , ne khunyanga eya Isabato ,\n",
      "2021-07-10 11:19:06,379 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:19:06,380 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:19:06,380 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:19:06,380 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , mana , nabohwa hasi weimeeli eyo , mana , naboha hasi , mushiina eshiraambi , mana nibahelesia , eshinyasio eshikhongo muno .\n",
      "2021-07-10 11:19:06,380 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step    15400: bleu:   5.60, loss: 125892.4453, ppl:  30.0944, duration: 23.7948s\n",
      "2021-07-10 11:19:12,344 - INFO - joeynmt.training - Epoch  22, Step:    15500, Batch Loss:     2.245534, Tokens per Sec:    12066, Lr: 0.000103\n",
      "2021-07-10 11:19:16,561 - INFO - joeynmt.training - Epoch  22: total training loss 673.96\n",
      "2021-07-10 11:19:16,561 - INFO - joeynmt.training - EPOCH 23\n",
      "2021-07-10 11:19:18,815 - INFO - joeynmt.training - Epoch  23, Step:    15600, Batch Loss:     2.213375, Tokens per Sec:    11326, Lr: 0.000103\n",
      "2021-07-10 11:19:44,927 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:19:44,927 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:19:44,928 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:19:45,888 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:19:45,889 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:19:45,889 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:19:45,889 - INFO - joeynmt.training - \tHypothesis: Kho Abafarisayo bandi nibetsa khu Yesu , ne , naboolela Yesu ari , “ Lekha , ingubo ikho mbu , emulambelesie , ne , ndalola , shinga olwa nditsulilanga . ”\n",
      "2021-07-10 11:19:45,890 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:19:45,890 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:19:45,890 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:19:45,890 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiluutsi , Mariamu yarula natsia elwanyi , naboolela omukhaana oyo ari , “ Chende khutsie ewanje , ” ,\n",
      "2021-07-10 11:19:45,891 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:19:45,891 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:19:45,892 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:19:45,892 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , ne khunyanga eya Isabato , yiruungwa , ahambi okhuula , khunyanga eya Isabato .\n",
      "2021-07-10 11:19:45,892 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:19:45,892 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:19:45,893 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:19:45,893 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , nabahelesia tsimbeka tsiosi , mana , nibitsula tsingubo tsiomunyiri . Ne olwa indiri eyo yali niyefwalile tsingubo tsiomunyiri , mana , niyibohwa .\n",
      "2021-07-10 11:19:45,893 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    15600: bleu:   5.54, loss: 125850.9297, ppl:  30.0606, duration: 27.0773s\n",
      "2021-07-10 11:19:51,885 - INFO - joeynmt.training - Epoch  23, Step:    15700, Batch Loss:     1.913919, Tokens per Sec:    11820, Lr: 0.000103\n",
      "2021-07-10 11:19:58,061 - INFO - joeynmt.training - Epoch  23, Step:    15800, Batch Loss:     2.222816, Tokens per Sec:    11514, Lr: 0.000103\n",
      "2021-07-10 11:20:22,398 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:20:22,398 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:20:22,399 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:20:23,829 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:20:23,829 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:20:23,830 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:20:23,830 - INFO - joeynmt.training - \tHypothesis: Ne Abafarisayo bandi bandi , bamureeba mbu , “ Omundu yesi , outiilakhwo , ne namarwi , ndalola , ne nenzia , ne , ndalola . ”\n",
      "2021-07-10 11:20:23,830 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:20:23,830 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , witse , bwangu , chama , enaayile . ”\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:20:23,831 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yechesinjia abandu ebikhumila biraano , ne khunyanga eya Isabato , yiruungwa .\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:20:23,832 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabenjisia , Paulo nende Sila , mana , nabohwa hasi , mana , nibaboholela tsingubo tsiomunyanza . Mana nibatsuuyia , oluchendo lwabwe lwelibishilo , mana , niyibohwa .\n",
      "2021-07-10 11:20:23,833 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step    15800: bleu:   5.61, loss: 125933.2812, ppl:  30.1276, duration: 25.7710s\n",
      "2021-07-10 11:20:27,855 - INFO - joeynmt.training - Epoch  23: total training loss 674.94\n",
      "2021-07-10 11:20:27,855 - INFO - joeynmt.training - EPOCH 24\n",
      "2021-07-10 11:20:29,659 - INFO - joeynmt.training - Epoch  24, Step:    15900, Batch Loss:     2.024278, Tokens per Sec:    11852, Lr: 0.000103\n",
      "2021-07-10 11:20:35,956 - INFO - joeynmt.training - Epoch  24, Step:    16000, Batch Loss:     2.007446, Tokens per Sec:    11273, Lr: 0.000103\n",
      "2021-07-10 11:21:00,337 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:21:00,338 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:21:00,338 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:21:01,319 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:21:01,319 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:21:01,320 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:21:01,320 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu yesi outiile , amapaaroke , ne nababoolela ari , “ Enditsa , ndalamutiilakhwo , ne ndalola , enzia , ne ndalola . ”\n",
      "2021-07-10 11:21:01,320 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:21:01,321 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:21:01,321 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:21:01,323 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo natsia , nibaboolela Mariamu ari , “ Omwechesia , witse , bwangu , witse muno. ” Mana namuboolela ari , “ Wenya mbu , onyala okhukhulola . ”\n",
      "2021-07-10 11:21:01,324 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:21:01,324 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:21:01,324 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:21:01,325 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yiraho khunyanga eya Isabato , ne khunyanga eya Isabato , yiraho .\n",
      "2021-07-10 11:21:01,325 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:21:01,325 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:21:01,325 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:21:01,326 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , naruma tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluyoka lwolutwa lwelu lwalukuku lweinyanza , mana yeniya .\n",
      "2021-07-10 11:21:01,326 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step    16000: bleu:   5.38, loss: 126211.1094, ppl:  30.3548, duration: 25.3705s\n",
      "2021-07-10 11:21:07,189 - INFO - joeynmt.training - Epoch  24, Step:    16100, Batch Loss:     2.094017, Tokens per Sec:    12188, Lr: 0.000103\n",
      "2021-07-10 11:21:11,651 - INFO - joeynmt.training - Epoch  24: total training loss 674.06\n",
      "2021-07-10 11:21:11,651 - INFO - joeynmt.training - EPOCH 25\n",
      "2021-07-10 11:21:13,552 - INFO - joeynmt.training - Epoch  25, Step:    16200, Batch Loss:     2.146438, Tokens per Sec:     8981, Lr: 0.000103\n",
      "2021-07-10 11:21:38,634 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:21:38,635 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:21:38,635 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:21:39,609 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:21:39,610 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:21:39,610 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:21:39,610 - INFO - joeynmt.training - \tHypothesis: Abafarisayo nibamuboolela bari , “ Omundu uno ali , nanyiekhwo , ne olwa yambelesiekhwo , ne , nenzia . ”\n",
      "2021-07-10 11:21:39,610 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:21:39,611 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:21:39,611 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:21:39,611 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo . ”\n",
      "2021-07-10 11:21:39,611 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:21:39,612 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:21:39,612 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:21:39,613 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Isabato , yiruka , khunyanga eya Isabato , ne khunyanga eya Isabato , yiraho .\n",
      "2021-07-10 11:21:39,613 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:21:39,614 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:21:39,614 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:21:39,614 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yakaasia , Paulo yabelihe , mana , nabarusia tsingubo tsiabwe , mana , nibatuuuyia lwa. , Yalasandibwa shinga omulilo kwalio , mana yebohwa , Paulo nende Sila .\n",
      "2021-07-10 11:21:39,614 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    16200: bleu:   5.58, loss: 126307.6797, ppl:  30.4342, duration: 26.0612s\n",
      "2021-07-10 11:21:45,487 - INFO - joeynmt.training - Epoch  25, Step:    16300, Batch Loss:     2.445061, Tokens per Sec:    12091, Lr: 0.000103\n",
      "2021-07-10 11:21:51,782 - INFO - joeynmt.training - Epoch  25, Step:    16400, Batch Loss:     2.286090, Tokens per Sec:    11282, Lr: 0.000103\n",
      "2021-07-10 11:22:18,252 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:22:18,252 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:22:18,252 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:22:19,220 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:22:19,221 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:22:19,221 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:22:19,221 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu yesi outiilanga , ne , yenjiile , ne nababoolela ari , “ Enditsa , nasi nditsa , ne ndalola , enzia . ”\n",
      "2021-07-10 11:22:19,222 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:22:19,223 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:22:19,223 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:22:19,223 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwemitsabibu . ”\n",
      "2021-07-10 11:22:19,223 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:22:19,224 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:22:19,224 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:22:19,225 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yalimwo khunyanga eya Isabato , ne khunyanga eya Isabato , yanyoola lichina lieshikhumila ,\n",
      "2021-07-10 11:22:19,225 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:22:19,225 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:22:19,225 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:22:19,226 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe , yabalindi , mana , nabohwa hasi , mana nibahelesia , eshinyasio eshikhongo eshikhongo eshikhongo eshikhongo eshikhongo muno . Ne olwa yali niyefwalile tsingubo tsiomunyiri , mana yebindi bieyali niyefwa .\n",
      "2021-07-10 11:22:19,226 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step    16400: bleu:   5.44, loss: 126065.6406, ppl:  30.2357, duration: 27.4439s\n",
      "2021-07-10 11:22:24,031 - INFO - joeynmt.training - Epoch  25: total training loss 667.05\n",
      "2021-07-10 11:22:24,032 - INFO - joeynmt.training - EPOCH 26\n",
      "2021-07-10 11:22:25,153 - INFO - joeynmt.training - Epoch  26, Step:    16500, Batch Loss:     2.293441, Tokens per Sec:    12072, Lr: 0.000072\n",
      "2021-07-10 11:22:31,515 - INFO - joeynmt.training - Epoch  26, Step:    16600, Batch Loss:     2.095215, Tokens per Sec:    11272, Lr: 0.000072\n",
      "2021-07-10 11:22:54,053 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:22:54,054 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:22:54,054 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:22:55,029 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:22:55,030 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:22:55,030 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:22:55,030 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno , naba nanyiekhwo , ne , namarwi kobukusi bwokhubuula tsingano . ”\n",
      "2021-07-10 11:22:55,032 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:22:55,032 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:22:55,032 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:22:55,032 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yasinjila bwangu , naboolela omukhaana oyo ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwemitsabibu . ”\n",
      "2021-07-10 11:22:55,033 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:22:55,033 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:22:55,033 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:22:55,033 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Kaperinaumu , khunyanga eya Isabato , yanyoola khunyanga eya Isabato ,\n",
      "2021-07-10 11:22:55,034 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:22:55,034 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:22:55,034 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:22:55,034 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , nibaboha hasi , mana , nibaboha hasi , mana nibahelesia eshitabu eshikanye eshio . Mana , shiokhwikasia ameetwe okhwirwa khwa Nyasaye ,\n",
      "2021-07-10 11:22:55,035 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step    16600: bleu:   5.50, loss: 126235.6094, ppl:  30.3750, duration: 23.5190s\n",
      "2021-07-10 11:23:00,914 - INFO - joeynmt.training - Epoch  26, Step:    16700, Batch Loss:     2.449845, Tokens per Sec:    11896, Lr: 0.000072\n",
      "2021-07-10 11:23:06,464 - INFO - joeynmt.training - Epoch  26: total training loss 660.18\n",
      "2021-07-10 11:23:06,464 - INFO - joeynmt.training - EPOCH 27\n",
      "2021-07-10 11:23:07,242 - INFO - joeynmt.training - Epoch  27, Step:    16800, Batch Loss:     2.141901, Tokens per Sec:    12063, Lr: 0.000072\n",
      "2021-07-10 11:23:31,052 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:23:31,052 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:23:31,052 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:23:32,016 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:23:32,017 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:23:32,017 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:23:32,017 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno , naba namanyile , mbu ali neshishieno eshio , ne , nahenga ikulu . Mana Yesu nenzia , ne nenzia . ”\n",
      "2021-07-10 11:23:32,018 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:23:32,018 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:23:32,018 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:23:32,018 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , witse , witse , bwangu ewa Yesu yali , namubooleele . ”\n",
      "2021-07-10 11:23:32,019 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:23:32,019 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:23:32,019 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:23:32,019 - INFO - joeynmt.training - \tHypothesis: Khunyanga eya Isabato , yechesinjia abandu ebikhumila biraano , ne khunyanga eya Isabato , yalimwo .\n",
      "2021-07-10 11:23:32,020 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:23:32,020 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:23:32,020 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:23:32,020 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe , yabalindi , mana , nabohwa hasi , mana nibahelesia , tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluyoka lwelu eyo , yatsushilamwo , mana niyenyokha .\n",
      "2021-07-10 11:23:32,021 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    16800: bleu:   5.62, loss: 126267.8828, ppl:  30.4015, duration: 24.7778s\n",
      "2021-07-10 11:23:37,914 - INFO - joeynmt.training - Epoch  27, Step:    16900, Batch Loss:     2.223786, Tokens per Sec:    12029, Lr: 0.000072\n",
      "2021-07-10 11:23:44,239 - INFO - joeynmt.training - Epoch  27, Step:    17000, Batch Loss:     2.364861, Tokens per Sec:    11144, Lr: 0.000072\n",
      "2021-07-10 11:24:10,450 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:24:10,450 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:24:10,450 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:24:11,450 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:24:11,451 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:24:11,451 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:24:11,451 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno , naba nanyiekhwo , ne , namarwi kobukusi bweikulu , bweisambaraaka . ”\n",
      "2021-07-10 11:24:11,451 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:24:11,452 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:24:11,452 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:24:11,452 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiluutsi , Mariamu , yasinjila elwanyi , naboola ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwa Yohana . ”\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Kaperinaumu , khunyanga eya Isabato , yanyoola khunyanga eya Isabato .\n",
      "2021-07-10 11:24:11,453 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:24:11,454 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:24:11,454 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:24:11,454 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , nibaboha tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluchendo lwomutarakwe kwasimeeli yenu eyo , mana yebohwa , okhwinoosia okhwikulu .\n",
      "2021-07-10 11:24:11,454 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step    17000: bleu:   5.66, loss: 126297.0000, ppl:  30.4254, duration: 27.2150s\n",
      "2021-07-10 11:24:16,891 - INFO - joeynmt.training - Epoch  27: total training loss 655.61\n",
      "2021-07-10 11:24:16,892 - INFO - joeynmt.training - EPOCH 28\n",
      "2021-07-10 11:24:17,375 - INFO - joeynmt.training - Epoch  28, Step:    17100, Batch Loss:     2.236680, Tokens per Sec:    12010, Lr: 0.000072\n",
      "2021-07-10 11:24:23,749 - INFO - joeynmt.training - Epoch  28, Step:    17200, Batch Loss:     2.159129, Tokens per Sec:    11082, Lr: 0.000072\n",
      "2021-07-10 11:24:46,555 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:24:46,556 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:24:46,556 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:24:47,981 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:24:47,982 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:24:47,982 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:24:47,982 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , yamanya aka Yesu yali namubooleele , shichila yali namubooleele ari , “ Eshilenje shianje , ne , nenzia mwanje ndalola . ”\n",
      "2021-07-10 11:24:47,982 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:24:47,983 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:24:47,983 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:24:47,983 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yalola amakhuwa ako , yakalukha ingo , natsia naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwefwe . ”\n",
      "2021-07-10 11:24:47,983 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:24:47,984 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:24:47,984 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:24:47,984 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Kaperinaumu , khunyanga eya Isabato , yanyoola khunyanga eya Isabato ,\n",
      "2021-07-10 11:24:47,984 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:24:47,985 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:24:47,985 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:24:47,985 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yakaasia , Paulo yabelihe , mana , nabarakhwo hasi wamakukho mbu , batsie , mutsimbwa tsiomunyiri . Ne olwa indiri eyo yali niyibohwa , neindiri eyo , niyibohwa .\n",
      "2021-07-10 11:24:47,985 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step    17200: bleu:   5.36, loss: 126373.8516, ppl:  30.4887, duration: 24.2352s\n",
      "2021-07-10 11:24:53,865 - INFO - joeynmt.training - Epoch  28, Step:    17300, Batch Loss:     2.321517, Tokens per Sec:    12366, Lr: 0.000072\n",
      "2021-07-10 11:24:59,987 - INFO - joeynmt.training - Epoch  28: total training loss 651.91\n",
      "2021-07-10 11:24:59,988 - INFO - joeynmt.training - EPOCH 29\n",
      "2021-07-10 11:25:00,235 - INFO - joeynmt.training - Epoch  29, Step:    17400, Batch Loss:     2.255209, Tokens per Sec:    11232, Lr: 0.000072\n",
      "2021-07-10 11:25:21,968 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:25:21,968 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:25:21,969 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:25:22,938 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:25:22,938 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno ali , nanyiekhwo , ne namusaaya mbu , amwikhoye , ne , ndalola khandi shinga olwa nditsa . ”\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:25:22,939 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwa Yesu . ”\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:25:22,940 - INFO - joeynmt.training - \tHypothesis: Khunyanga yambeli yambeli yalimwo , khunyanga eya Isabato , ne khunyanga eya Isabato , yalimwo .\n",
      "2021-07-10 11:25:22,941 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:25:22,941 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:25:22,941 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:25:22,942 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , naruma tsingubo tsiomunyiri . Ne olwa yali niyenjile , oluyoka lwelu eyo , yenyokha okwo , mana yafimbwa omurwe .\n",
      "2021-07-10 11:25:22,942 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    17400: bleu:   5.67, loss: 126363.7969, ppl:  30.4804, duration: 22.7064s\n",
      "2021-07-10 11:25:28,806 - INFO - joeynmt.training - Epoch  29, Step:    17500, Batch Loss:     2.194132, Tokens per Sec:    12101, Lr: 0.000072\n",
      "2021-07-10 11:25:35,135 - INFO - joeynmt.training - Epoch  29, Step:    17600, Batch Loss:     1.242991, Tokens per Sec:    11312, Lr: 0.000072\n",
      "2021-07-10 11:25:58,155 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:25:58,156 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:25:58,156 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:25:59,145 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:25:59,146 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:25:59,146 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:25:59,146 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno ali , natseshelela , ne , namarwi kabili mbu , alolanga , ne nenzie . ”\n",
      "2021-07-10 11:25:59,147 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:25:59,147 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:25:59,147 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila elwanyi , ne , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawe . ”\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:25:59,148 - INFO - joeynmt.training - \tHypothesis: Khunyanga yambeli eya Isabato , yiruka , khunyanga eya Isabato , ne khunyanga eya Isabato , yalimwo .\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe natsia ingongo yelihe eyo , mana , nabenoosia shinga oluyoka lwomutarakwe , mana , nabenoosia okhwirwa khwa Paulo , mana , nabarakhwo eshindu shiosi shiosi shiosi eshiyia .\n",
      "2021-07-10 11:25:59,149 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step    17600: bleu:   5.69, loss: 126435.2266, ppl:  30.5394, duration: 24.0141s\n",
      "2021-07-10 11:26:05,028 - INFO - joeynmt.training - Epoch  29, Step:    17700, Batch Loss:     2.339534, Tokens per Sec:    11994, Lr: 0.000050\n",
      "2021-07-10 11:26:05,087 - INFO - joeynmt.training - Epoch  29: total training loss 650.28\n",
      "2021-07-10 11:26:05,087 - INFO - joeynmt.training - EPOCH 30\n",
      "2021-07-10 11:26:11,354 - INFO - joeynmt.training - Epoch  30, Step:    17800, Batch Loss:     1.833692, Tokens per Sec:    11038, Lr: 0.000050\n",
      "2021-07-10 11:26:34,098 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:26:34,098 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:26:34,099 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:26:35,051 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:26:35,052 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:26:35,052 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela mbu , Yesu ali nanyiekhwo , ne namunyoola , nababoolela ari , “ Enditsa , ndalamutiilakhwo , ne ndalaba , neimba , ndalola . ”\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:26:35,053 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , ne namubooleele . ”\n",
      "2021-07-10 11:26:35,054 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:26:35,054 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:26:35,054 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:26:35,055 - INFO - joeynmt.training - \tHypothesis: Khunyanga yene eyo , Kaperinaumu , khunyanga eya Isabato , yarechekha , lisabo lia Pasaka .\n",
      "2021-07-10 11:26:35,055 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:26:35,056 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:26:35,056 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:26:35,056 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yenjisia Paulo , yabohwa hasi , mana , nibaboha hasi , mana , nibaboha amatsi amanji . Mana , nibatsushilamwo eshitabu eshikanye eshio , nishikalukhasibungwa .\n",
      "2021-07-10 11:26:35,056 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    17800: bleu:   5.86, loss: 126503.1641, ppl:  30.5955, duration: 23.7014s\n",
      "2021-07-10 11:26:40,935 - INFO - joeynmt.training - Epoch  30, Step:    17900, Batch Loss:     2.247837, Tokens per Sec:    11967, Lr: 0.000050\n",
      "2021-07-10 11:26:47,298 - INFO - joeynmt.training - Epoch  30, Step:    18000, Batch Loss:     1.954968, Tokens per Sec:    11228, Lr: 0.000050\n",
      "2021-07-10 11:27:11,190 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:27:11,191 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:27:11,191 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:27:12,162 - INFO - joeynmt.training - Example #0\n",
      "2021-07-10 11:27:12,162 - INFO - joeynmt.training - \tSource:     Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-10 11:27:12,163 - INFO - joeynmt.training - \tReference:  Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-10 11:27:12,163 - INFO - joeynmt.training - \tHypothesis: Abafarisayo , nibamuboolela bari , “ Omundu uno ali , nanyiekhwo , ne namusaaya mbu , amwikhoye , ne , ndalola khandi shinga olwa nditsa , enzia . ”\n",
      "2021-07-10 11:27:12,163 - INFO - joeynmt.training - Example #1\n",
      "2021-07-10 11:27:12,163 - INFO - joeynmt.training - \tSource:     And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - \tReference:  Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - \tHypothesis: Ne olwa Mariamu yali nashiboolanga amakhuwa ako , yasinjila bwangu , naboolela Mariamu ari , “ Omwechesia , wabeele , omwitsawo , womukanda kwa Yesu . ”\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - Example #2\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - \tSource:     That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-10 11:27:12,164 - INFO - joeynmt.training - \tReference:  Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-10 11:27:12,165 - INFO - joeynmt.training - \tHypothesis: Khunyanga yambeli eya Isabato , yarechekha , khunyanga eya Isabato , ne khunyanga eya Isabato , yirubeka .\n",
      "2021-07-10 11:27:12,165 - INFO - joeynmt.training - Example #3\n",
      "2021-07-10 11:27:12,165 - INFO - joeynmt.training - \tSource:     But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-10 11:27:12,165 - INFO - joeynmt.training - \tReference:  Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-10 11:27:12,166 - INFO - joeynmt.training - \tHypothesis: Nebutswa Paulo yabelihe , yenjisia mushiina , shiokhwinikha , mana nabarusia , ebiralie nibakwalwalie , mana , nabohwa hasi . Yalahelesia eshinyasio eshiyia , eshiyia .\n",
      "2021-07-10 11:27:12,166 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step    18000: bleu:   5.71, loss: 126447.5312, ppl:  30.5495, duration: 24.8670s\n",
      "2021-07-10 11:27:12,651 - INFO - joeynmt.training - Epoch  30: total training loss 647.99\n",
      "2021-07-10 11:27:12,652 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
      "2021-07-10 11:27:12,652 - INFO - joeynmt.training - Best validation result (greedy) at step    12800:  29.27 ppl.\n",
      "2021-07-10 11:27:12,672 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-10 11:27:13,041 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-10 11:27:13,232 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-10 11:27:13,295 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.lh)...\n",
      "2021-07-10 11:27:43,039 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:27:43,040 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:27:43,040 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:27:43,325 - INFO - joeynmt.prediction -  dev bleu[13a]:   5.69 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 11:27:43,335 - INFO - joeynmt.prediction - Translations saved to: models/enlh_transformer_continued/00012800.hyps.dev\n",
      "2021-07-10 11:27:43,335 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.lh)...\n",
      "2021-07-10 11:28:12,532 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-10 11:28:12,532 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-10 11:28:12,533 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-10 11:28:12,827 - INFO - joeynmt.prediction - test bleu[13a]:   5.05 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-10 11:28:12,832 - INFO - joeynmt.prediction - Translations saved to: models/enlh_transformer_continued/00012800.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_enlh_reload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSBIzdFg3TLK",
    "outputId": "ed55608d-9d13-4a28-aef6-535fbf11fcd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 09:06:34,004 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-18 09:06:34,802 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-18 09:06:35,535 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-18 09:06:36,854 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-18 09:06:38,258 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-18 09:06:38,317 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
      "2021-07-18 09:07:31,413 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 09:07:31,792 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-18 09:07:31,865 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/dev.bpe.lh)...\n",
      "2021-07-18 09:08:00,983 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 09:08:00,983 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 09:08:00,983 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 09:08:01,311 - INFO - joeynmt.prediction -  dev bleu[13a]:   6.39 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-18 09:08:01,311 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/test.bpe.lh)...\n",
      "2021-07-18 09:08:30,804 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 09:08:30,804 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 09:08:30,805 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 09:08:31,104 - INFO - joeynmt.prediction - test bleu[13a]:   5.31 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt test 'models/enlh_transformer_continued/config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTdKNjTu-Zni"
   },
   "source": [
    "# Backtranslation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzIHIEP8yKLB"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZwi9exL-xgG"
   },
   "outputs": [],
   "source": [
    "# Changing to Luganda directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luganda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koRVt8o8-hbQ"
   },
   "outputs": [],
   "source": [
    "# Getting English data from the Luganda dataset\n",
    "lug = pd.read_csv(\"Luganda.csv\")\n",
    "mon_en = pd.DataFrame(lug['source_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lybf6p-RdXvk"
   },
   "outputs": [],
   "source": [
    "mon_en.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Acz2W3Z8kI6p",
    "outputId": "d24e5525-d0f2-40e0-c451-d847cee57d82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249490</th>\n",
       "      <td>Among these publishers today are third - gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249491</th>\n",
       "      <td>We give thanks to Jehovah and to those early f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249492</th>\n",
       "      <td>15 : 15 , 16 . ​ — From our archives in Portug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249493</th>\n",
       "      <td>See “ There Is More Harvest Work to Be Done ” ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249494</th>\n",
       "      <td>31 - 32 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249495 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_sentence\n",
       "0                      This publication is not for sale .\n",
       "1                                           COVER SUBJECT\n",
       "2       The Bible was completed about two thousand yea...\n",
       "3       Since then , countless other books have come a...\n",
       "4                                     But not the Bible .\n",
       "...                                                   ...\n",
       "249490  Among these publishers today are third - gener...\n",
       "249491  We give thanks to Jehovah and to those early f...\n",
       "249492  15 : 15 , 16 . ​ — From our archives in Portug...\n",
       "249493  See “ There Is More Harvest Work to Be Done ” ...\n",
       "249494                                          31 - 32 .\n",
       "\n",
       "[249495 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MDnIMMeFOWt"
   },
   "outputs": [],
   "source": [
    "# Function to identify if a string has a number or not\n",
    "import re\n",
    "\n",
    "def hasNum(inputString):\n",
    "  input = str(inputString)\n",
    "  return not re.findall('\\d+', input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6A8vuksEQi_"
   },
   "outputs": [],
   "source": [
    "# Detecting numbers\n",
    "mon_en['has_num'] = mon_en['source_sentence'].apply(hasNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "uZivWkAfj3fr",
    "outputId": "f851d477-feda-4e40-877d-abe93a362b48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>has_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consider the following .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Bible has survived many vicious attacks by...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>For example , during the Middle Ages in certai...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scholars who translated the Bible into the ver...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Despite its many enemies , the Bible became ​ ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_sentence  has_num\n",
       "0                 This publication is not for sale .     True\n",
       "1                                      COVER SUBJECT     True\n",
       "2  The Bible was completed about two thousand yea...     True\n",
       "3  Since then , countless other books have come a...     True\n",
       "4                                But not the Bible .     True\n",
       "5                           Consider the following .     True\n",
       "6  The Bible has survived many vicious attacks by...     True\n",
       "7  For example , during the Middle Ages in certai...     True\n",
       "8  Scholars who translated the Bible into the ver...     True\n",
       "9  Despite its many enemies , the Bible became ​ ...     True"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_en.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-BayN_sGG4he",
    "outputId": "b1fc2253-b12a-4ee3-eda6-5a8dada28f69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "      <th>has_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>247121</td>\n",
       "      <td>249495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>231428</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>*</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>462</td>\n",
       "      <td>203930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source_sentence has_num\n",
       "count           247121  249495\n",
       "unique          231428       2\n",
       "top                  *    True\n",
       "freq               462  203930"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_en.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trlla8D4Jy_3"
   },
   "outputs": [],
   "source": [
    "mon_en = mon_en[mon_en['has_num'] == True] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsxdPilInOPT"
   },
   "outputs": [],
   "source": [
    "mon_en.drop(['has_num'], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "EWXmOVskoYJn",
    "outputId": "adc18304-1b1d-4ee0-9e08-53e23f2e5904"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249481</th>\n",
       "      <td>With the printing and distribution of Bible li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249483</th>\n",
       "      <td>However , the seeds of truth had been sown .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249484</th>\n",
       "      <td>Amid the upheaval in Europe during the Spanish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249486</th>\n",
       "      <td>After that , the growth in the number of Kingd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249491</th>\n",
       "      <td>We give thanks to Jehovah and to those early f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203930 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_sentence\n",
       "0                      This publication is not for sale .\n",
       "1                                           COVER SUBJECT\n",
       "2       The Bible was completed about two thousand yea...\n",
       "3       Since then , countless other books have come a...\n",
       "4                                     But not the Bible .\n",
       "...                                                   ...\n",
       "249481  With the printing and distribution of Bible li...\n",
       "249483       However , the seeds of truth had been sown .\n",
       "249484  Amid the upheaval in Europe during the Spanish...\n",
       "249486  After that , the growth in the number of Kingd...\n",
       "249491  We give thanks to Jehovah and to those early f...\n",
       "\n",
       "[203930 rows x 1 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data\n",
    "mon_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-kXQWTbowCM"
   },
   "outputs": [],
   "source": [
    "# Monolingual English\n",
    "#mon_en.to_csv('mon_en.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6MYI_S9opIwP",
    "outputId": "de6eb4c2-dac7-4b11-d126-9fe8a0867211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This publication is not for sale .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVER SUBJECT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Bible was completed about two thousand yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Since then , countless other books have come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But not the Bible .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_sentence\n",
       "0                 This publication is not for sale .\n",
       "1                                      COVER SUBJECT\n",
       "2  The Bible was completed about two thousand yea...\n",
       "3  Since then , countless other books have come a...\n",
       "4                                But not the Bible ."
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon = pd.read_csv(\"mon_en.csv\")\n",
    "mon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRhC2JcCxV7t",
    "outputId": "8a030490-9394-46cf-9bf2-6341da8c843e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source_sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqyMXv9nxbpD"
   },
   "outputs": [],
   "source": [
    "mon.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z23XFxhOzteI",
    "outputId": "b5b22670-fbe2-45ab-9d03-7ae71e776a17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shareddrives/NMT_for_African_Language/Luganda\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWxgyZ6kz4Fz"
   },
   "outputs": [],
   "source": [
    "# Changing to Luhyia directory\n",
    "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2G0Ux8mU1-YK",
    "outputId": "5cffac30-b018-428d-e51b-a57dd7194f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnfcSBKewTHd"
   },
   "outputs": [],
   "source": [
    "# Getting monolingual BPEs\n",
    "with open(\"mon.\"+source_language, \"w\") as src_file:\n",
    "  for index, row in mon.iterrows():\n",
    "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < mon.$src > mon.bpe.$src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJfALQDj2R8_",
    "outputId": "be6fad43-baa8-4878-ad95-b34ed492b593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> mon.bpe.en <==\n",
      "This p@@ ub@@ li@@ cation is not for s@@ ale .\n",
      "C@@ O@@ V@@ E@@ R S@@ U@@ B@@ J@@ E@@ C@@ T\n",
      "The B@@ ible was comple@@ ted about two thousand years a@@ go .\n",
      "S@@ in@@ ce then , coun@@ t@@ less other bo@@ ok@@ s have come and gone .\n",
      "But not the B@@ ible .\n",
      "C@@ on@@ si@@ der the follow@@ ing .\n",
      "The B@@ ible has sur@@ v@@ ived many vi@@ ci@@ ous at@@ ta@@ c@@ ks by p@@ ow@@ er@@ ful people .\n",
      "For exam@@ ple , d@@ ur@@ ing the M@@ id@@ d@@ le A@@ g@@ es in certain “ Chris@@ ti@@ an ” l@@ ands , “ the poss@@ ess@@ ion and read@@ ing of the B@@ ible in the ver@@ nac@@ ul@@ ar [ the l@@ ang@@ u@@ age of the comm@@ on people ] was in@@ cre@@ as@@ ingly as@@ so@@ ci@@ ated with her@@ es@@ y and dis@@ sent , ” says the book A@@ n I@@ n@@ t@@ ro@@ du@@ ction to the M@@ e@@ di@@ ev@@ al B@@ ible .\n",
      "S@@ ch@@ ol@@ ars who trans@@ l@@ ated the B@@ ible into the ver@@ nac@@ ul@@ ar or who prom@@ o@@ ted B@@ ible st@@ ud@@ y ris@@ ked their lives . S@@ ome were killed .\n",
      "D@@ es@@ p@@ ite its many enemi@@ es , the B@@ ible became ​ — and contin@@ ues to be — ​ the most wi@@ d@@ ely dis@@ tri@@ bu@@ ted book of all time .\n",
      "\n",
      "==> mon.en <==\n",
      "This publication is not for sale .\n",
      "COVER SUBJECT\n",
      "The Bible was completed about two thousand years ago .\n",
      "Since then , countless other books have come and gone .\n",
      "But not the Bible .\n",
      "Consider the following .\n",
      "The Bible has survived many vicious attacks by powerful people .\n",
      "For example , during the Middle Ages in certain “ Christian ” lands , “ the possession and reading of the Bible in the vernacular [ the language of the common people ] was increasingly associated with heresy and dissent , ” says the book An Introduction to the Medieval Bible .\n",
      "Scholars who translated the Bible into the vernacular or who promoted Bible study risked their lives . Some were killed .\n",
      "Despite its many enemies , the Bible became ​ — and continues to be — ​ the most widely distributed book of all time .\n"
     ]
    }
   ],
   "source": [
    "! head mon.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riF2uGlF2ksO",
    "outputId": "f699b307-40ff-4fb9-bd74-481d4688d29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> mon.bpe.en <==\n",
      "He sought per@@ mis@@ sion to use his h@@ ome for reg@@ ul@@ ar me@@ et@@ ings .\n",
      "In ad@@ d@@ ition , through tr@@ ac@@ ts and bo@@ ok@@ le@@ ts , the word of truth sp@@ read to the fa@@ r re@@ ac@@ hes of the P@@ or@@ t@@ u@@ gu@@ es@@ e Em@@ p@@ ire ​ — A@@ ng@@ ol@@ a , the A@@ z@@ or@@ es , C@@ ap@@ e V@@ er@@ de , E@@ ast T@@ im@@ or , G@@ o@@ a , M@@ a@@ de@@ ira , and M@@ o@@ z@@ am@@ bi@@ qu@@ e .\n",
      "W@@ hi@@ le living in B@@ ra@@ z@@ il , he had heard a p@@ ub@@ li@@ c tal@@ k given by B@@ ro@@ ther Y@@ oung .\n",
      "He read@@ ily re@@ c@@ og@@ ni@@ zed the r@@ ing of truth and was e@@ ag@@ er to hel@@ p B@@ ro@@ ther F@@ er@@ g@@ us@@ on to ex@@ p@@ and the pre@@ aching work .\n",
      "To do so , M@@ anu@@ el began to serve as a col@@ p@@ or@@ te@@ u@@ r , as pi@@ on@@ e@@ ers were then called .\n",
      "W@@ ith the pr@@ in@@ ting and dis@@ tri@@ bu@@ t@@ ion of B@@ ible l@@ it@@ er@@ at@@ ure now well - or@@ g@@ ani@@ zed , the f@@ led@@ gl@@ ing con@@ gre@@ g@@ ation in L@@ is@@ b@@ on th@@ ri@@ ved !\n",
      "H@@ owever , the see@@ ds of truth had been s@@ own .\n",
      "Am@@ id the up@@ heav@@ al in E@@ u@@ ro@@ pe d@@ ur@@ ing the Sp@@ an@@ ish C@@ iv@@ il W@@ ar and W@@ or@@ ld W@@ ar I@@ I , the faithful g@@ rou@@ p of brothers in P@@ or@@ t@@ u@@ ga@@ l man@@ ag@@ ed to sur@@ v@@ ive spirit@@ u@@ ally .\n",
      "After that , the gr@@ ow@@ th in the number of K@@ ing@@ dom pro@@ c@@ la@@ im@@ ers was un@@ st@@ op@@ p@@ able .\n",
      "We give thanks to J@@ e@@ ho@@ v@@ ah and to those ear@@ ly faithful brothers and sis@@ ters who cour@@ ag@@ e@@ ously took the le@@ ad in spe@@ ar@@ he@@ ad@@ ing the work as ‘ p@@ ub@@ li@@ c servants of Christ Jesus to the nations . ’ ​ — R@@ om .\n",
      "\n",
      "==> mon.en <==\n",
      "He sought permission to use his home for regular meetings .\n",
      "In addition , through tracts and booklets , the word of truth spread to the far reaches of the Portuguese Empire ​ — Angola , the Azores , Cape Verde , East Timor , Goa , Madeira , and Mozambique .\n",
      "While living in Brazil , he had heard a public talk given by Brother Young .\n",
      "He readily recognized the ring of truth and was eager to help Brother Ferguson to expand the preaching work .\n",
      "To do so , Manuel began to serve as a colporteur , as pioneers were then called .\n",
      "With the printing and distribution of Bible literature now well - organized , the fledgling congregation in Lisbon thrived !\n",
      "However , the seeds of truth had been sown .\n",
      "Amid the upheaval in Europe during the Spanish Civil War and World War II , the faithful group of brothers in Portugal managed to survive spiritually .\n",
      "After that , the growth in the number of Kingdom proclaimers was unstoppable .\n",
      "We give thanks to Jehovah and to those early faithful brothers and sisters who courageously took the lead in spearheading the work as ‘ public servants of Christ Jesus to the nations . ’ ​ — Rom .\n"
     ]
    }
   ],
   "source": [
    "!tail mon.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUTzXZMopQ7Q",
    "outputId": "ecad7ae3-c9b1-44b3-de21-183da046016e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 09:11:56,551 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-18 09:12:00,358 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 09:12:00,564 - INFO - joeynmt.model - Enc-dec model built.\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python -m joeynmt translate 'models/enlh_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/mon.bpe.en\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/mon.lh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56vQc6fbSZxB",
    "outputId": "07cf7f72-9e0d-44f7-9441-24e887c70adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This publication is not for sale .\n",
      "COVER SUBJECT\n",
      "The Bible was completed about two thousand years ago .\n",
      "Since then , countless other books have come and gone .\n",
      "But not the Bible .\n",
      "Consider the following .\n",
      "The Bible has survived many vicious attacks by powerful people .\n",
      "For example , during the Middle Ages in certain “ Christian ” lands , “ the possession and reading of the Bible in the vernacular [ the language of the common people ] was increasingly associated with heresy and dissent , ” says the book An Introduction to the Medieval Bible .\n",
      "Scholars who translated the Bible into the vernacular or who promoted Bible study risked their lives . Some were killed .\n",
      "Despite its many enemies , the Bible became ​ — and continues to be — ​ the most widely distributed book of all time .\n",
      "Oburume obwomundu shibuliho ta , habula nobwatoto\n",
      "Olunyuma lwetsinyanga tsitaru , Yorodani nende Siria .\n",
      "Yali ahambi isaa yashienda yemiyika , chibili .\n",
      "Abakhalabani bobubeeyi bakhetsukhana , nibakhupa ikha .\n",
      "Nebutswa , shimulamukhalachila eshiina tawe .\n",
      "Mutsinyanga etsio .\n",
      "Nyasaye yaranjilila okhulaasia abandu abanji , ne abandu abanji , nabo , nibahonibwe .\n",
      "Baana befwe , khurumanga khumatookho , keshilaamo mbu , “ Lekha ” ” , mbu , “ Oli ” ” , mbu , “ Oli owobuloho , ” , owobuloho , obuloho , khandi mbu , “ Olunyala , ” ta , ” , shichila buli shindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashindu shiashilaka ,\n",
      "Biliho ebifwanani biomwikulu , bilanyala okhuboola ta , shichila , Nyasaye yalonga likulu nende ebilimwoyo .\n",
      "Abo abahulilikha shinga abahulilikha , abasuku ba Nyasaye , baloba okhukhalachilwa eshiina ta , baloba okhuhulila , akali amalayi .\n"
     ]
    }
   ],
   "source": [
    "!head mon.en\n",
    "!head mon.lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h82xs4DU8t9K"
   },
   "outputs": [],
   "source": [
    "# Dev data source\n",
    "file1 = ['train.en', 'mon.en']\n",
    "\n",
    "# Dev data target\n",
    "file2 = ['train.lh', 'mon.lh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuEPOeF3zuj0"
   },
   "outputs": [],
   "source": [
    "# Procedure to create concatenated files\n",
    "def create_file(x,filename):\n",
    "  # Open filename in write mode\n",
    "  with open(filename, 'w') as outfile:\n",
    "      for names in x:\n",
    "          # Open each file in read mode\n",
    "          with open(names) as infile:\n",
    "              # read the data and write it in file3\n",
    "              outfile.write(infile.read())\n",
    "          outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfSsVReb9dDd"
   },
   "outputs": [],
   "source": [
    "create_file(file1,'back.en')\n",
    "create_file(file2,'back.lh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BG_IWkOE9qGp"
   },
   "outputs": [],
   "source": [
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt learn-joint-bpe-and-vocab --input back.$src back.$tgt3 -s 4000 -o bpe.codes.4000 --write-vocabulary vocab2.$src vocab2.$tgt3\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$src < back.$src > back.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$tgt3 < back.$tgt3 > back.bpe.$tgt3\n",
    "\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$src < dev.$src > back_dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$tgt3 < dev.$tgt3 > back_dev.bpe.$tgt3\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$src < test.$src > back_test.bpe.$src\n",
    "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab2.$tgt3 < test.$tgt3 > back_test.bpe.$tgt3\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py back.bpe.$src back.bpe.$tgt3 --output_path vocab2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVyKdTdF07yF"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "f-7K3051-_i2"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "name = '%s%s' % (target_language3, source_language)\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{target_language3}{source_language}_reverse_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{target_language3}\"\n",
    "    trg: \"{source_language}\"\n",
    "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\"\n",
    "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\"\n",
    "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
    "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "\n",
    "training:\n",
    "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.1\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 1600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"ppl\"\n",
    "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"\n",
    "    model_dir: \"models/back_{name}_reverse_transformer\"\n",
    "    overwrite: False              # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 100\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_last_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.3\n",
    "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia\", source_language=source_language, target_language3=target_language3)\n",
    "with open(\"joeynmt/configs/back_transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9PqOYzAACSx",
    "outputId": "46dc3534-c97f-47c2-85c4-5c4e0c133df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 11:13:02,325 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-18 11:13:02,785 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-18 11:13:05,985 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-18 11:13:06,237 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-18 11:13:06,262 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-18 11:13:06,831 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-18 11:13:06,831 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 11:13:07,034 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-18 11:13:07.277581: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-18 11:13:08,753 - INFO - joeynmt.training - Total params: 12138240\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\n",
      "2021-07-18 11:13:11,975 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-18 11:13:11,976 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-18 11:13:11,977 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-18 11:13:11,978 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/back_lhen_reverse_transformer\n",
      "2021-07-18 11:13:11,979 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-18 11:13:11,980 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-18 11:13:11,981 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-18 11:13:11,982 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 207289,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-18 11:13:11,983 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] P@@ il@@ a@@ to nakal@@ ukha itookho , ne nal@@ anga Yesu , namureeba , ari , “ Iwe ni@@ we omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then P@@ il@@ ate ent@@ ered the P@@ ra@@ et@@ or@@ i@@ u@@ m again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-18 11:13:11,984 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-18 11:13:11,984 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-18 11:13:11,984 - INFO - joeynmt.helpers - Number of Src words (types): 4211\n",
      "2021-07-18 11:13:11,984 - INFO - joeynmt.helpers - Number of Trg words (types): 4211\n",
      "2021-07-18 11:13:11,985 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4211),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4211))\n",
      "2021-07-18 11:13:11,994 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-18 11:13:11,995 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-18 11:13:24,863 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.652103, Tokens per Sec:    17280, Lr: 0.000300\n",
      "2021-07-18 11:13:37,544 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.488219, Tokens per Sec:    17748, Lr: 0.000300\n",
      "2021-07-18 11:13:50,212 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.178207, Tokens per Sec:    17995, Lr: 0.000300\n",
      "2021-07-18 11:14:03,139 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.167909, Tokens per Sec:    17535, Lr: 0.000300\n",
      "2021-07-18 11:14:16,125 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     4.638240, Tokens per Sec:    17573, Lr: 0.000300\n",
      "2021-07-18 11:14:28,867 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     5.034882, Tokens per Sec:    17596, Lr: 0.000300\n",
      "2021-07-18 11:14:41,804 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     4.660217, Tokens per Sec:    17435, Lr: 0.000300\n",
      "2021-07-18 11:14:54,731 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.592145, Tokens per Sec:    17411, Lr: 0.000300\n",
      "2021-07-18 11:15:08,014 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     4.464002, Tokens per Sec:    17149, Lr: 0.000300\n",
      "2021-07-18 11:15:21,299 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.979547, Tokens per Sec:    17241, Lr: 0.000300\n",
      "2021-07-18 11:15:34,475 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     4.514979, Tokens per Sec:    17143, Lr: 0.000300\n",
      "2021-07-18 11:15:47,832 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.464970, Tokens per Sec:    17239, Lr: 0.000300\n",
      "2021-07-18 11:16:01,084 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     4.146487, Tokens per Sec:    17110, Lr: 0.000300\n",
      "2021-07-18 11:16:14,631 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.352227, Tokens per Sec:    16841, Lr: 0.000300\n",
      "2021-07-18 11:16:28,100 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     4.125914, Tokens per Sec:    17007, Lr: 0.000300\n",
      "2021-07-18 11:16:41,494 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.183986, Tokens per Sec:    17045, Lr: 0.000300\n",
      "2021-07-18 11:16:54,988 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.885419, Tokens per Sec:    17211, Lr: 0.000300\n",
      "2021-07-18 11:17:08,353 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.942995, Tokens per Sec:    16801, Lr: 0.000300\n",
      "2021-07-18 11:17:22,085 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.685602, Tokens per Sec:    16800, Lr: 0.000300\n",
      "2021-07-18 11:17:35,595 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.807752, Tokens per Sec:    16637, Lr: 0.000300\n",
      "2021-07-18 11:17:49,214 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.794858, Tokens per Sec:    16676, Lr: 0.000300\n",
      "2021-07-18 11:18:02,775 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     4.047292, Tokens per Sec:    16827, Lr: 0.000300\n",
      "2021-07-18 11:18:16,226 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.761487, Tokens per Sec:    16861, Lr: 0.000300\n",
      "2021-07-18 11:18:19,439 - INFO - joeynmt.training - Epoch   1: total training loss 10391.30\n",
      "2021-07-18 11:18:19,439 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-18 11:18:30,331 - INFO - joeynmt.training - Epoch   2, Step:     2400, Batch Loss:     4.054616, Tokens per Sec:    16152, Lr: 0.000300\n",
      "2021-07-18 11:18:43,936 - INFO - joeynmt.training - Epoch   2, Step:     2500, Batch Loss:     4.055769, Tokens per Sec:    16547, Lr: 0.000300\n",
      "2021-07-18 11:18:57,786 - INFO - joeynmt.training - Epoch   2, Step:     2600, Batch Loss:     3.891661, Tokens per Sec:    16669, Lr: 0.000300\n",
      "2021-07-18 11:19:11,431 - INFO - joeynmt.training - Epoch   2, Step:     2700, Batch Loss:     3.873384, Tokens per Sec:    16385, Lr: 0.000300\n",
      "2021-07-18 11:19:24,906 - INFO - joeynmt.training - Epoch   2, Step:     2800, Batch Loss:     3.737725, Tokens per Sec:    16952, Lr: 0.000300\n",
      "2021-07-18 11:19:38,432 - INFO - joeynmt.training - Epoch   2, Step:     2900, Batch Loss:     3.451412, Tokens per Sec:    16745, Lr: 0.000300\n",
      "2021-07-18 11:19:51,972 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     3.619028, Tokens per Sec:    16764, Lr: 0.000300\n",
      "2021-07-18 11:20:05,706 - INFO - joeynmt.training - Epoch   2, Step:     3100, Batch Loss:     3.575587, Tokens per Sec:    16667, Lr: 0.000300\n",
      "2021-07-18 11:20:19,396 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     3.796655, Tokens per Sec:    16534, Lr: 0.000300\n",
      "2021-07-18 11:20:32,923 - INFO - joeynmt.training - Epoch   2, Step:     3300, Batch Loss:     3.196476, Tokens per Sec:    16537, Lr: 0.000300\n",
      "2021-07-18 11:20:46,482 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     3.207240, Tokens per Sec:    16809, Lr: 0.000300\n",
      "2021-07-18 11:20:59,977 - INFO - joeynmt.training - Epoch   2, Step:     3500, Batch Loss:     3.251864, Tokens per Sec:    16736, Lr: 0.000300\n",
      "2021-07-18 11:21:13,655 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     3.656019, Tokens per Sec:    16612, Lr: 0.000300\n",
      "2021-07-18 11:21:27,320 - INFO - joeynmt.training - Epoch   2, Step:     3700, Batch Loss:     3.539258, Tokens per Sec:    16699, Lr: 0.000300\n",
      "2021-07-18 11:21:40,944 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.729759, Tokens per Sec:    16650, Lr: 0.000300\n",
      "2021-07-18 11:21:54,636 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     3.369380, Tokens per Sec:    16817, Lr: 0.000300\n",
      "2021-07-18 11:22:08,261 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     3.270392, Tokens per Sec:    16700, Lr: 0.000300\n",
      "2021-07-18 11:22:22,048 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     3.304070, Tokens per Sec:    16329, Lr: 0.000300\n",
      "2021-07-18 11:22:35,767 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     3.306737, Tokens per Sec:    16626, Lr: 0.000300\n",
      "2021-07-18 11:22:49,338 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     3.490550, Tokens per Sec:    16649, Lr: 0.000300\n",
      "2021-07-18 11:23:02,991 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     3.330337, Tokens per Sec:    16841, Lr: 0.000300\n",
      "2021-07-18 11:23:16,576 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     3.476504, Tokens per Sec:    16906, Lr: 0.000300\n",
      "2021-07-18 11:23:30,309 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     3.262308, Tokens per Sec:    16687, Lr: 0.000300\n",
      "2021-07-18 11:23:36,467 - INFO - joeynmt.training - Epoch   2: total training loss 8195.23\n",
      "2021-07-18 11:23:36,468 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-18 11:23:44,128 - INFO - joeynmt.training - Epoch   3, Step:     4700, Batch Loss:     3.468995, Tokens per Sec:    15878, Lr: 0.000300\n",
      "2021-07-18 11:23:58,007 - INFO - joeynmt.training - Epoch   3, Step:     4800, Batch Loss:     3.497197, Tokens per Sec:    16601, Lr: 0.000300\n",
      "2021-07-18 11:24:11,707 - INFO - joeynmt.training - Epoch   3, Step:     4900, Batch Loss:     3.512869, Tokens per Sec:    16655, Lr: 0.000300\n",
      "2021-07-18 11:24:25,136 - INFO - joeynmt.training - Epoch   3, Step:     5000, Batch Loss:     3.397258, Tokens per Sec:    16587, Lr: 0.000300\n",
      "2021-07-18 11:24:48,572 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 11:24:48,572 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 11:24:48,572 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 11:24:48,905 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 11:24:48,906 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 11:24:49,590 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - \tHypothesis: The apostles were to be a man who said , “ If I have been a man , I have been a man , ” said , “ The man of the house of the heavens . ”\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 11:24:49,592 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - \tHypothesis: When he was a man , he was a man who had been reported to the son of the son of the son of the son of the son of the son of the son of the Mary . ”\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 11:24:49,593 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - \tHypothesis: He was a time to be a time when the day was a time .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - \tHypothesis: Paul was a brief , and he was a brief , and he was a brief , and he was a brief , and the brief of the wedding .\n",
      "2021-07-18 11:24:49,594 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     5000: bleu:   2.01, loss: 118329.8281, ppl:  29.3023, duration: 24.4579s\n",
      "2021-07-18 11:25:03,536 - INFO - joeynmt.training - Epoch   3, Step:     5100, Batch Loss:     3.289864, Tokens per Sec:    16383, Lr: 0.000300\n",
      "2021-07-18 11:25:17,254 - INFO - joeynmt.training - Epoch   3, Step:     5200, Batch Loss:     3.417833, Tokens per Sec:    16777, Lr: 0.000300\n",
      "2021-07-18 11:25:30,817 - INFO - joeynmt.training - Epoch   3, Step:     5300, Batch Loss:     3.418568, Tokens per Sec:    16822, Lr: 0.000300\n",
      "2021-07-18 11:25:44,443 - INFO - joeynmt.training - Epoch   3, Step:     5400, Batch Loss:     3.125911, Tokens per Sec:    16820, Lr: 0.000300\n",
      "2021-07-18 11:25:58,094 - INFO - joeynmt.training - Epoch   3, Step:     5500, Batch Loss:     3.278790, Tokens per Sec:    16774, Lr: 0.000300\n",
      "2021-07-18 11:26:11,888 - INFO - joeynmt.training - Epoch   3, Step:     5600, Batch Loss:     3.390987, Tokens per Sec:    16403, Lr: 0.000300\n",
      "2021-07-18 11:26:25,541 - INFO - joeynmt.training - Epoch   3, Step:     5700, Batch Loss:     3.461645, Tokens per Sec:    16501, Lr: 0.000300\n",
      "2021-07-18 11:26:39,252 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     3.386021, Tokens per Sec:    16958, Lr: 0.000300\n",
      "2021-07-18 11:26:52,856 - INFO - joeynmt.training - Epoch   3, Step:     5900, Batch Loss:     3.469775, Tokens per Sec:    16453, Lr: 0.000300\n",
      "2021-07-18 11:27:06,548 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     3.159902, Tokens per Sec:    16941, Lr: 0.000300\n",
      "2021-07-18 11:27:20,239 - INFO - joeynmt.training - Epoch   3, Step:     6100, Batch Loss:     3.132534, Tokens per Sec:    16627, Lr: 0.000300\n",
      "2021-07-18 11:27:33,846 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     3.480273, Tokens per Sec:    16505, Lr: 0.000300\n",
      "2021-07-18 11:27:47,430 - INFO - joeynmt.training - Epoch   3, Step:     6300, Batch Loss:     3.375218, Tokens per Sec:    16729, Lr: 0.000300\n",
      "2021-07-18 11:28:01,073 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     3.493947, Tokens per Sec:    16707, Lr: 0.000300\n",
      "2021-07-18 11:28:14,647 - INFO - joeynmt.training - Epoch   3, Step:     6500, Batch Loss:     3.207147, Tokens per Sec:    16713, Lr: 0.000300\n",
      "2021-07-18 11:28:28,327 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     3.245547, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 11:28:42,043 - INFO - joeynmt.training - Epoch   3, Step:     6700, Batch Loss:     3.206191, Tokens per Sec:    16686, Lr: 0.000300\n",
      "2021-07-18 11:28:55,731 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     3.404485, Tokens per Sec:    16933, Lr: 0.000300\n",
      "2021-07-18 11:29:09,159 - INFO - joeynmt.training - Epoch   3, Step:     6900, Batch Loss:     3.077116, Tokens per Sec:    16640, Lr: 0.000300\n",
      "2021-07-18 11:29:17,795 - INFO - joeynmt.training - Epoch   3: total training loss 7527.42\n",
      "2021-07-18 11:29:17,795 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-18 11:29:22,997 - INFO - joeynmt.training - Epoch   4, Step:     7000, Batch Loss:     3.296239, Tokens per Sec:    15407, Lr: 0.000300\n",
      "2021-07-18 11:29:36,794 - INFO - joeynmt.training - Epoch   4, Step:     7100, Batch Loss:     3.277412, Tokens per Sec:    16872, Lr: 0.000300\n",
      "2021-07-18 11:29:50,287 - INFO - joeynmt.training - Epoch   4, Step:     7200, Batch Loss:     3.302583, Tokens per Sec:    16696, Lr: 0.000300\n",
      "2021-07-18 11:30:03,942 - INFO - joeynmt.training - Epoch   4, Step:     7300, Batch Loss:     2.985823, Tokens per Sec:    16618, Lr: 0.000300\n",
      "2021-07-18 11:30:17,684 - INFO - joeynmt.training - Epoch   4, Step:     7400, Batch Loss:     3.331528, Tokens per Sec:    16919, Lr: 0.000300\n",
      "2021-07-18 11:30:31,392 - INFO - joeynmt.training - Epoch   4, Step:     7500, Batch Loss:     3.230357, Tokens per Sec:    16938, Lr: 0.000300\n",
      "2021-07-18 11:30:44,848 - INFO - joeynmt.training - Epoch   4, Step:     7600, Batch Loss:     2.926011, Tokens per Sec:    16844, Lr: 0.000300\n",
      "2021-07-18 11:30:58,474 - INFO - joeynmt.training - Epoch   4, Step:     7700, Batch Loss:     3.312961, Tokens per Sec:    16735, Lr: 0.000300\n",
      "2021-07-18 11:31:12,249 - INFO - joeynmt.training - Epoch   4, Step:     7800, Batch Loss:     2.835406, Tokens per Sec:    16489, Lr: 0.000300\n",
      "2021-07-18 11:31:25,975 - INFO - joeynmt.training - Epoch   4, Step:     7900, Batch Loss:     2.861612, Tokens per Sec:    16624, Lr: 0.000300\n",
      "2021-07-18 11:31:39,639 - INFO - joeynmt.training - Epoch   4, Step:     8000, Batch Loss:     2.718804, Tokens per Sec:    16788, Lr: 0.000300\n",
      "2021-07-18 11:31:53,182 - INFO - joeynmt.training - Epoch   4, Step:     8100, Batch Loss:     3.229550, Tokens per Sec:    16779, Lr: 0.000300\n",
      "2021-07-18 11:32:06,723 - INFO - joeynmt.training - Epoch   4, Step:     8200, Batch Loss:     3.479104, Tokens per Sec:    16741, Lr: 0.000300\n",
      "2021-07-18 11:32:20,381 - INFO - joeynmt.training - Epoch   4, Step:     8300, Batch Loss:     3.357882, Tokens per Sec:    16582, Lr: 0.000300\n",
      "2021-07-18 11:32:34,007 - INFO - joeynmt.training - Epoch   4, Step:     8400, Batch Loss:     3.202068, Tokens per Sec:    16654, Lr: 0.000300\n",
      "2021-07-18 11:32:47,666 - INFO - joeynmt.training - Epoch   4, Step:     8500, Batch Loss:     3.038518, Tokens per Sec:    16699, Lr: 0.000300\n",
      "2021-07-18 11:33:01,333 - INFO - joeynmt.training - Epoch   4, Step:     8600, Batch Loss:     3.215507, Tokens per Sec:    16733, Lr: 0.000300\n",
      "2021-07-18 11:33:14,897 - INFO - joeynmt.training - Epoch   4, Step:     8700, Batch Loss:     3.070414, Tokens per Sec:    16593, Lr: 0.000300\n",
      "2021-07-18 11:33:28,559 - INFO - joeynmt.training - Epoch   4, Step:     8800, Batch Loss:     3.164593, Tokens per Sec:    16596, Lr: 0.000300\n",
      "2021-07-18 11:33:42,137 - INFO - joeynmt.training - Epoch   4, Step:     8900, Batch Loss:     2.706897, Tokens per Sec:    16557, Lr: 0.000300\n",
      "2021-07-18 11:33:55,573 - INFO - joeynmt.training - Epoch   4, Step:     9000, Batch Loss:     3.207397, Tokens per Sec:    16870, Lr: 0.000300\n",
      "2021-07-18 11:34:09,165 - INFO - joeynmt.training - Epoch   4, Step:     9100, Batch Loss:     2.665085, Tokens per Sec:    16680, Lr: 0.000300\n",
      "2021-07-18 11:34:22,738 - INFO - joeynmt.training - Epoch   4, Step:     9200, Batch Loss:     2.971258, Tokens per Sec:    16464, Lr: 0.000300\n",
      "2021-07-18 11:34:34,416 - INFO - joeynmt.training - Epoch   4: total training loss 7190.07\n",
      "2021-07-18 11:34:34,416 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-18 11:34:36,720 - INFO - joeynmt.training - Epoch   5, Step:     9300, Batch Loss:     3.196618, Tokens per Sec:    14307, Lr: 0.000300\n",
      "2021-07-18 11:34:50,387 - INFO - joeynmt.training - Epoch   5, Step:     9400, Batch Loss:     2.766790, Tokens per Sec:    16800, Lr: 0.000300\n",
      "2021-07-18 11:35:04,086 - INFO - joeynmt.training - Epoch   5, Step:     9500, Batch Loss:     2.530447, Tokens per Sec:    16509, Lr: 0.000300\n",
      "2021-07-18 11:35:17,618 - INFO - joeynmt.training - Epoch   5, Step:     9600, Batch Loss:     3.336390, Tokens per Sec:    16876, Lr: 0.000300\n",
      "2021-07-18 11:35:31,086 - INFO - joeynmt.training - Epoch   5, Step:     9700, Batch Loss:     3.191032, Tokens per Sec:    16705, Lr: 0.000300\n",
      "2021-07-18 11:35:44,775 - INFO - joeynmt.training - Epoch   5, Step:     9800, Batch Loss:     3.259223, Tokens per Sec:    16632, Lr: 0.000300\n",
      "2021-07-18 11:35:58,537 - INFO - joeynmt.training - Epoch   5, Step:     9900, Batch Loss:     3.201852, Tokens per Sec:    16825, Lr: 0.000300\n",
      "2021-07-18 11:36:12,283 - INFO - joeynmt.training - Epoch   5, Step:    10000, Batch Loss:     3.243787, Tokens per Sec:    16407, Lr: 0.000300\n",
      "2021-07-18 11:36:45,633 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 11:36:45,633 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 11:36:45,634 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 11:36:45,981 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 11:36:45,981 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 11:36:46,663 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 11:36:46,664 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 11:36:46,664 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 11:36:46,665 - INFO - joeynmt.training - \tHypothesis: And the Pharisees , who was a man who had been given him , saying , “ He will be raised , and he will be silver , and he will be silver . ”\n",
      "2021-07-18 11:36:46,665 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 11:36:46,665 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 11:36:46,666 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 11:36:46,667 - INFO - joeynmt.training - \tHypothesis: When he was a great crowd , he was a man of the mother , and he said to Him , “ Lord , and he will be raised . ”\n",
      "2021-07-18 11:36:46,667 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the day .\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 11:36:46,668 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 11:36:46,669 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 11:36:46,669 - INFO - joeynmt.training - \tHypothesis: And he was raised , and he was raised , and he was raised , and he was a source of the seven seven of the seven seven seven of the wind .\n",
      "2021-07-18 11:36:46,669 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    10000: bleu:   3.14, loss: 107455.4219, ppl:  21.4830, duration: 34.3853s\n",
      "2021-07-18 11:37:00,389 - INFO - joeynmt.training - Epoch   5, Step:    10100, Batch Loss:     3.214835, Tokens per Sec:    16690, Lr: 0.000300\n",
      "2021-07-18 11:37:14,243 - INFO - joeynmt.training - Epoch   5, Step:    10200, Batch Loss:     3.125715, Tokens per Sec:    16703, Lr: 0.000300\n",
      "2021-07-18 11:37:27,808 - INFO - joeynmt.training - Epoch   5, Step:    10300, Batch Loss:     3.316646, Tokens per Sec:    16475, Lr: 0.000300\n",
      "2021-07-18 11:37:41,488 - INFO - joeynmt.training - Epoch   5, Step:    10400, Batch Loss:     3.055973, Tokens per Sec:    16609, Lr: 0.000300\n",
      "2021-07-18 11:37:55,129 - INFO - joeynmt.training - Epoch   5, Step:    10500, Batch Loss:     3.117477, Tokens per Sec:    16717, Lr: 0.000300\n",
      "2021-07-18 11:38:08,728 - INFO - joeynmt.training - Epoch   5, Step:    10600, Batch Loss:     2.911697, Tokens per Sec:    16705, Lr: 0.000300\n",
      "2021-07-18 11:38:22,530 - INFO - joeynmt.training - Epoch   5, Step:    10700, Batch Loss:     2.984868, Tokens per Sec:    16526, Lr: 0.000300\n",
      "2021-07-18 11:38:36,072 - INFO - joeynmt.training - Epoch   5, Step:    10800, Batch Loss:     3.322159, Tokens per Sec:    16617, Lr: 0.000300\n",
      "2021-07-18 11:38:49,660 - INFO - joeynmt.training - Epoch   5, Step:    10900, Batch Loss:     2.906811, Tokens per Sec:    16745, Lr: 0.000300\n",
      "2021-07-18 11:39:03,022 - INFO - joeynmt.training - Epoch   5, Step:    11000, Batch Loss:     2.893350, Tokens per Sec:    16677, Lr: 0.000300\n",
      "2021-07-18 11:39:16,713 - INFO - joeynmt.training - Epoch   5, Step:    11100, Batch Loss:     3.314165, Tokens per Sec:    16674, Lr: 0.000300\n",
      "2021-07-18 11:39:30,404 - INFO - joeynmt.training - Epoch   5, Step:    11200, Batch Loss:     2.942385, Tokens per Sec:    16613, Lr: 0.000300\n",
      "2021-07-18 11:39:44,031 - INFO - joeynmt.training - Epoch   5, Step:    11300, Batch Loss:     3.192435, Tokens per Sec:    16663, Lr: 0.000300\n",
      "2021-07-18 11:39:57,594 - INFO - joeynmt.training - Epoch   5, Step:    11400, Batch Loss:     2.880817, Tokens per Sec:    16633, Lr: 0.000300\n",
      "2021-07-18 11:40:11,154 - INFO - joeynmt.training - Epoch   5, Step:    11500, Batch Loss:     3.142082, Tokens per Sec:    16870, Lr: 0.000300\n",
      "2021-07-18 11:40:24,754 - INFO - joeynmt.training - Epoch   5, Step:    11600, Batch Loss:     3.189060, Tokens per Sec:    16663, Lr: 0.000300\n",
      "2021-07-18 11:40:25,827 - INFO - joeynmt.training - Epoch   5: total training loss 6979.33\n",
      "2021-07-18 11:40:25,827 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-18 11:40:38,792 - INFO - joeynmt.training - Epoch   6, Step:    11700, Batch Loss:     2.928677, Tokens per Sec:    16372, Lr: 0.000300\n",
      "2021-07-18 11:40:52,457 - INFO - joeynmt.training - Epoch   6, Step:    11800, Batch Loss:     2.441700, Tokens per Sec:    16741, Lr: 0.000300\n",
      "2021-07-18 11:41:06,065 - INFO - joeynmt.training - Epoch   6, Step:    11900, Batch Loss:     3.163107, Tokens per Sec:    16511, Lr: 0.000300\n",
      "2021-07-18 11:41:19,742 - INFO - joeynmt.training - Epoch   6, Step:    12000, Batch Loss:     3.225448, Tokens per Sec:    16616, Lr: 0.000300\n",
      "2021-07-18 11:41:33,284 - INFO - joeynmt.training - Epoch   6, Step:    12100, Batch Loss:     3.053762, Tokens per Sec:    16752, Lr: 0.000300\n",
      "2021-07-18 11:41:46,743 - INFO - joeynmt.training - Epoch   6, Step:    12200, Batch Loss:     2.944340, Tokens per Sec:    16686, Lr: 0.000300\n",
      "2021-07-18 11:42:00,274 - INFO - joeynmt.training - Epoch   6, Step:    12300, Batch Loss:     3.046337, Tokens per Sec:    16659, Lr: 0.000300\n",
      "2021-07-18 11:42:13,929 - INFO - joeynmt.training - Epoch   6, Step:    12400, Batch Loss:     2.950295, Tokens per Sec:    16345, Lr: 0.000300\n",
      "2021-07-18 11:42:27,668 - INFO - joeynmt.training - Epoch   6, Step:    12500, Batch Loss:     2.666849, Tokens per Sec:    16637, Lr: 0.000300\n",
      "2021-07-18 11:42:41,256 - INFO - joeynmt.training - Epoch   6, Step:    12600, Batch Loss:     2.830174, Tokens per Sec:    16727, Lr: 0.000300\n",
      "2021-07-18 11:42:54,647 - INFO - joeynmt.training - Epoch   6, Step:    12700, Batch Loss:     2.856973, Tokens per Sec:    16544, Lr: 0.000300\n",
      "2021-07-18 11:43:08,296 - INFO - joeynmt.training - Epoch   6, Step:    12800, Batch Loss:     2.728805, Tokens per Sec:    16683, Lr: 0.000300\n",
      "2021-07-18 11:43:21,903 - INFO - joeynmt.training - Epoch   6, Step:    12900, Batch Loss:     2.924922, Tokens per Sec:    16558, Lr: 0.000300\n",
      "2021-07-18 11:43:35,645 - INFO - joeynmt.training - Epoch   6, Step:    13000, Batch Loss:     2.544010, Tokens per Sec:    16926, Lr: 0.000300\n",
      "2021-07-18 11:43:49,279 - INFO - joeynmt.training - Epoch   6, Step:    13100, Batch Loss:     2.668169, Tokens per Sec:    16792, Lr: 0.000300\n",
      "2021-07-18 11:44:02,842 - INFO - joeynmt.training - Epoch   6, Step:    13200, Batch Loss:     2.896705, Tokens per Sec:    16727, Lr: 0.000300\n",
      "2021-07-18 11:44:16,750 - INFO - joeynmt.training - Epoch   6, Step:    13300, Batch Loss:     3.067722, Tokens per Sec:    16571, Lr: 0.000300\n",
      "2021-07-18 11:44:30,563 - INFO - joeynmt.training - Epoch   6, Step:    13400, Batch Loss:     2.941170, Tokens per Sec:    16565, Lr: 0.000300\n",
      "2021-07-18 11:44:44,196 - INFO - joeynmt.training - Epoch   6, Step:    13500, Batch Loss:     2.299691, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-18 11:44:57,973 - INFO - joeynmt.training - Epoch   6, Step:    13600, Batch Loss:     2.765565, Tokens per Sec:    16716, Lr: 0.000300\n",
      "2021-07-18 11:45:11,711 - INFO - joeynmt.training - Epoch   6, Step:    13700, Batch Loss:     2.785002, Tokens per Sec:    16774, Lr: 0.000300\n",
      "2021-07-18 11:45:25,506 - INFO - joeynmt.training - Epoch   6, Step:    13800, Batch Loss:     3.036847, Tokens per Sec:    16816, Lr: 0.000300\n",
      "2021-07-18 11:45:39,112 - INFO - joeynmt.training - Epoch   6, Step:    13900, Batch Loss:     2.611948, Tokens per Sec:    16601, Lr: 0.000300\n",
      "2021-07-18 11:45:42,666 - INFO - joeynmt.training - Epoch   6: total training loss 6793.23\n",
      "2021-07-18 11:45:42,667 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-18 11:45:52,900 - INFO - joeynmt.training - Epoch   7, Step:    14000, Batch Loss:     2.982269, Tokens per Sec:    15972, Lr: 0.000300\n",
      "2021-07-18 11:46:06,569 - INFO - joeynmt.training - Epoch   7, Step:    14100, Batch Loss:     2.837081, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-07-18 11:46:20,169 - INFO - joeynmt.training - Epoch   7, Step:    14200, Batch Loss:     2.618802, Tokens per Sec:    16711, Lr: 0.000300\n",
      "2021-07-18 11:46:33,695 - INFO - joeynmt.training - Epoch   7, Step:    14300, Batch Loss:     2.969814, Tokens per Sec:    16489, Lr: 0.000300\n",
      "2021-07-18 11:46:47,402 - INFO - joeynmt.training - Epoch   7, Step:    14400, Batch Loss:     2.815508, Tokens per Sec:    16875, Lr: 0.000300\n",
      "2021-07-18 11:47:01,155 - INFO - joeynmt.training - Epoch   7, Step:    14500, Batch Loss:     2.807916, Tokens per Sec:    16731, Lr: 0.000300\n",
      "2021-07-18 11:47:14,960 - INFO - joeynmt.training - Epoch   7, Step:    14600, Batch Loss:     2.808668, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-07-18 11:47:28,701 - INFO - joeynmt.training - Epoch   7, Step:    14700, Batch Loss:     2.939392, Tokens per Sec:    16813, Lr: 0.000300\n",
      "2021-07-18 11:47:42,304 - INFO - joeynmt.training - Epoch   7, Step:    14800, Batch Loss:     3.015333, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-18 11:47:55,848 - INFO - joeynmt.training - Epoch   7, Step:    14900, Batch Loss:     2.890965, Tokens per Sec:    16840, Lr: 0.000300\n",
      "2021-07-18 11:48:09,657 - INFO - joeynmt.training - Epoch   7, Step:    15000, Batch Loss:     2.783857, Tokens per Sec:    16793, Lr: 0.000300\n",
      "2021-07-18 11:48:39,225 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 11:48:39,225 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 11:48:39,225 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 11:48:39,564 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 11:48:39,564 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 11:48:40,312 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 11:48:40,313 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 11:48:40,313 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - \tHypothesis: And the Pharisees who had been given to Him , saying , “ He who was with Him , and he said to them , “ I shall go to the house of the house of the house . ”\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 11:48:40,314 - INFO - joeynmt.training - \tHypothesis: When she was studying the Bible , she was a mother , and she asked her to her , “ Mary , and she was a little man , and she was baptized . ”\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the day .\n",
      "2021-07-18 11:48:40,315 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 11:48:40,316 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 11:48:40,317 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 11:48:40,317 - INFO - joeynmt.training - \tHypothesis: And he who was baptized , and he saw Him , and he saw Him , and he was sleeping , and he was sleeping on the road of the road , and he was sleeping .\n",
      "2021-07-18 11:48:40,317 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    15000: bleu:   3.78, loss: 102888.7109, ppl:  18.8575, duration: 30.6592s\n",
      "2021-07-18 11:48:53,961 - INFO - joeynmt.training - Epoch   7, Step:    15100, Batch Loss:     3.057984, Tokens per Sec:    16551, Lr: 0.000300\n",
      "2021-07-18 11:49:07,703 - INFO - joeynmt.training - Epoch   7, Step:    15200, Batch Loss:     2.851904, Tokens per Sec:    16858, Lr: 0.000300\n",
      "2021-07-18 11:49:21,685 - INFO - joeynmt.training - Epoch   7, Step:    15300, Batch Loss:     2.973060, Tokens per Sec:    16619, Lr: 0.000300\n",
      "2021-07-18 11:49:35,447 - INFO - joeynmt.training - Epoch   7, Step:    15400, Batch Loss:     2.940523, Tokens per Sec:    16416, Lr: 0.000300\n",
      "2021-07-18 11:49:49,040 - INFO - joeynmt.training - Epoch   7, Step:    15500, Batch Loss:     2.966185, Tokens per Sec:    16412, Lr: 0.000300\n",
      "2021-07-18 11:50:02,559 - INFO - joeynmt.training - Epoch   7, Step:    15600, Batch Loss:     2.975596, Tokens per Sec:    16546, Lr: 0.000300\n",
      "2021-07-18 11:50:16,285 - INFO - joeynmt.training - Epoch   7, Step:    15700, Batch Loss:     2.970467, Tokens per Sec:    16453, Lr: 0.000300\n",
      "2021-07-18 11:50:30,274 - INFO - joeynmt.training - Epoch   7, Step:    15800, Batch Loss:     2.991073, Tokens per Sec:    16731, Lr: 0.000300\n",
      "2021-07-18 11:50:43,925 - INFO - joeynmt.training - Epoch   7, Step:    15900, Batch Loss:     2.941234, Tokens per Sec:    16460, Lr: 0.000300\n",
      "2021-07-18 11:50:57,633 - INFO - joeynmt.training - Epoch   7, Step:    16000, Batch Loss:     2.829274, Tokens per Sec:    16819, Lr: 0.000300\n",
      "2021-07-18 11:51:11,331 - INFO - joeynmt.training - Epoch   7, Step:    16100, Batch Loss:     2.946280, Tokens per Sec:    16695, Lr: 0.000300\n",
      "2021-07-18 11:51:25,094 - INFO - joeynmt.training - Epoch   7, Step:    16200, Batch Loss:     2.887893, Tokens per Sec:    16627, Lr: 0.000300\n",
      "2021-07-18 11:51:30,953 - INFO - joeynmt.training - Epoch   7: total training loss 6674.53\n",
      "2021-07-18 11:51:30,954 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-18 11:51:39,168 - INFO - joeynmt.training - Epoch   8, Step:    16300, Batch Loss:     2.986401, Tokens per Sec:    16059, Lr: 0.000300\n",
      "2021-07-18 11:51:52,837 - INFO - joeynmt.training - Epoch   8, Step:    16400, Batch Loss:     2.915144, Tokens per Sec:    16506, Lr: 0.000300\n",
      "2021-07-18 11:52:06,819 - INFO - joeynmt.training - Epoch   8, Step:    16500, Batch Loss:     2.760735, Tokens per Sec:    16760, Lr: 0.000300\n",
      "2021-07-18 11:52:20,516 - INFO - joeynmt.training - Epoch   8, Step:    16600, Batch Loss:     2.621777, Tokens per Sec:    16542, Lr: 0.000300\n",
      "2021-07-18 11:52:34,218 - INFO - joeynmt.training - Epoch   8, Step:    16700, Batch Loss:     3.189390, Tokens per Sec:    16513, Lr: 0.000300\n",
      "2021-07-18 11:52:47,985 - INFO - joeynmt.training - Epoch   8, Step:    16800, Batch Loss:     2.971002, Tokens per Sec:    16780, Lr: 0.000300\n",
      "2021-07-18 11:53:01,733 - INFO - joeynmt.training - Epoch   8, Step:    16900, Batch Loss:     3.136696, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-18 11:53:15,505 - INFO - joeynmt.training - Epoch   8, Step:    17000, Batch Loss:     2.779980, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-18 11:53:29,266 - INFO - joeynmt.training - Epoch   8, Step:    17100, Batch Loss:     2.436192, Tokens per Sec:    16549, Lr: 0.000300\n",
      "2021-07-18 11:53:42,897 - INFO - joeynmt.training - Epoch   8, Step:    17200, Batch Loss:     2.741244, Tokens per Sec:    16827, Lr: 0.000300\n",
      "2021-07-18 11:53:56,500 - INFO - joeynmt.training - Epoch   8, Step:    17300, Batch Loss:     2.854635, Tokens per Sec:    16734, Lr: 0.000300\n",
      "2021-07-18 11:54:10,241 - INFO - joeynmt.training - Epoch   8, Step:    17400, Batch Loss:     2.171211, Tokens per Sec:    16340, Lr: 0.000300\n",
      "2021-07-18 11:54:23,975 - INFO - joeynmt.training - Epoch   8, Step:    17500, Batch Loss:     2.886678, Tokens per Sec:    16306, Lr: 0.000300\n",
      "2021-07-18 11:54:37,740 - INFO - joeynmt.training - Epoch   8, Step:    17600, Batch Loss:     2.700441, Tokens per Sec:    16614, Lr: 0.000300\n",
      "2021-07-18 11:54:51,280 - INFO - joeynmt.training - Epoch   8, Step:    17700, Batch Loss:     2.702967, Tokens per Sec:    16606, Lr: 0.000300\n",
      "2021-07-18 11:55:04,741 - INFO - joeynmt.training - Epoch   8, Step:    17800, Batch Loss:     2.715117, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-07-18 11:55:18,398 - INFO - joeynmt.training - Epoch   8, Step:    17900, Batch Loss:     2.714948, Tokens per Sec:    16653, Lr: 0.000300\n",
      "2021-07-18 11:55:32,122 - INFO - joeynmt.training - Epoch   8, Step:    18000, Batch Loss:     2.979905, Tokens per Sec:    16478, Lr: 0.000300\n",
      "2021-07-18 11:55:45,806 - INFO - joeynmt.training - Epoch   8, Step:    18100, Batch Loss:     2.819937, Tokens per Sec:    16654, Lr: 0.000300\n",
      "2021-07-18 11:55:59,395 - INFO - joeynmt.training - Epoch   8, Step:    18200, Batch Loss:     3.098244, Tokens per Sec:    16586, Lr: 0.000300\n",
      "2021-07-18 11:56:12,996 - INFO - joeynmt.training - Epoch   8, Step:    18300, Batch Loss:     2.164204, Tokens per Sec:    16381, Lr: 0.000300\n",
      "2021-07-18 11:56:26,832 - INFO - joeynmt.training - Epoch   8, Step:    18400, Batch Loss:     3.014172, Tokens per Sec:    16487, Lr: 0.000300\n",
      "2021-07-18 11:56:40,643 - INFO - joeynmt.training - Epoch   8, Step:    18500, Batch Loss:     2.843159, Tokens per Sec:    16404, Lr: 0.000300\n",
      "2021-07-18 11:56:49,913 - INFO - joeynmt.training - Epoch   8: total training loss 6595.56\n",
      "2021-07-18 11:56:49,913 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-18 11:56:54,496 - INFO - joeynmt.training - Epoch   9, Step:    18600, Batch Loss:     2.932591, Tokens per Sec:    14736, Lr: 0.000300\n",
      "2021-07-18 11:57:08,244 - INFO - joeynmt.training - Epoch   9, Step:    18700, Batch Loss:     2.293607, Tokens per Sec:    16425, Lr: 0.000300\n",
      "2021-07-18 11:57:21,886 - INFO - joeynmt.training - Epoch   9, Step:    18800, Batch Loss:     2.666705, Tokens per Sec:    16738, Lr: 0.000300\n",
      "2021-07-18 11:57:35,557 - INFO - joeynmt.training - Epoch   9, Step:    18900, Batch Loss:     3.022337, Tokens per Sec:    16512, Lr: 0.000300\n",
      "2021-07-18 11:57:49,162 - INFO - joeynmt.training - Epoch   9, Step:    19000, Batch Loss:     2.551999, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-07-18 11:58:02,886 - INFO - joeynmt.training - Epoch   9, Step:    19100, Batch Loss:     2.876385, Tokens per Sec:    16750, Lr: 0.000300\n",
      "2021-07-18 11:58:16,695 - INFO - joeynmt.training - Epoch   9, Step:    19200, Batch Loss:     3.052240, Tokens per Sec:    16366, Lr: 0.000300\n",
      "2021-07-18 11:58:30,287 - INFO - joeynmt.training - Epoch   9, Step:    19300, Batch Loss:     2.737508, Tokens per Sec:    16679, Lr: 0.000300\n",
      "2021-07-18 11:58:43,817 - INFO - joeynmt.training - Epoch   9, Step:    19400, Batch Loss:     2.894639, Tokens per Sec:    16405, Lr: 0.000300\n",
      "2021-07-18 11:58:57,587 - INFO - joeynmt.training - Epoch   9, Step:    19500, Batch Loss:     2.522689, Tokens per Sec:    16754, Lr: 0.000300\n",
      "2021-07-18 11:59:11,498 - INFO - joeynmt.training - Epoch   9, Step:    19600, Batch Loss:     2.788762, Tokens per Sec:    16613, Lr: 0.000300\n",
      "2021-07-18 11:59:25,121 - INFO - joeynmt.training - Epoch   9, Step:    19700, Batch Loss:     3.152863, Tokens per Sec:    16645, Lr: 0.000300\n",
      "2021-07-18 11:59:38,804 - INFO - joeynmt.training - Epoch   9, Step:    19800, Batch Loss:     2.967395, Tokens per Sec:    16775, Lr: 0.000300\n",
      "2021-07-18 11:59:52,639 - INFO - joeynmt.training - Epoch   9, Step:    19900, Batch Loss:     2.857632, Tokens per Sec:    16769, Lr: 0.000300\n",
      "2021-07-18 12:00:06,309 - INFO - joeynmt.training - Epoch   9, Step:    20000, Batch Loss:     2.449187, Tokens per Sec:    16634, Lr: 0.000300\n",
      "2021-07-18 12:00:35,921 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 12:00:35,921 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 12:00:35,921 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 12:00:36,263 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 12:00:36,263 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 12:00:37,002 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 12:00:37,003 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 12:00:37,003 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 12:00:37,003 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw Him , and He said to Him , “ He who was with Him , and He said to Him , “ I shall be with you , and I will see you . ”\n",
      "2021-07-18 12:00:37,004 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 12:00:37,004 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 12:00:37,004 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 12:00:37,004 - INFO - joeynmt.training - \tHypothesis: When she was a man , she was able to see her mother , and she was asked to go to Mary , “ Mary , and she was a young man . ”\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the day .\n",
      "2021-07-18 12:00:37,005 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 12:00:37,006 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 12:00:37,006 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 12:00:37,006 - INFO - joeynmt.training - \tHypothesis: And he was with Him , and when He was in the midst of the sea , and he was raised , and he was sat down to the road of the road .\n",
      "2021-07-18 12:00:37,006 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    20000: bleu:   4.57, loss: 98859.5156, ppl:  16.8087, duration: 30.6970s\n",
      "2021-07-18 12:00:50,674 - INFO - joeynmt.training - Epoch   9, Step:    20100, Batch Loss:     2.747767, Tokens per Sec:    16386, Lr: 0.000300\n",
      "2021-07-18 12:01:04,185 - INFO - joeynmt.training - Epoch   9, Step:    20200, Batch Loss:     3.011094, Tokens per Sec:    16641, Lr: 0.000300\n",
      "2021-07-18 12:01:17,847 - INFO - joeynmt.training - Epoch   9, Step:    20300, Batch Loss:     2.848831, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-18 12:01:31,515 - INFO - joeynmt.training - Epoch   9, Step:    20400, Batch Loss:     2.855074, Tokens per Sec:    16603, Lr: 0.000300\n",
      "2021-07-18 12:01:45,087 - INFO - joeynmt.training - Epoch   9, Step:    20500, Batch Loss:     2.655996, Tokens per Sec:    16409, Lr: 0.000300\n",
      "2021-07-18 12:01:58,639 - INFO - joeynmt.training - Epoch   9, Step:    20600, Batch Loss:     2.717762, Tokens per Sec:    16845, Lr: 0.000300\n",
      "2021-07-18 12:02:12,005 - INFO - joeynmt.training - Epoch   9, Step:    20700, Batch Loss:     2.634065, Tokens per Sec:    16560, Lr: 0.000300\n",
      "2021-07-18 12:02:25,624 - INFO - joeynmt.training - Epoch   9, Step:    20800, Batch Loss:     2.901412, Tokens per Sec:    16841, Lr: 0.000300\n",
      "2021-07-18 12:02:38,804 - INFO - joeynmt.training - Epoch   9: total training loss 6518.89\n",
      "2021-07-18 12:02:38,804 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-18 12:02:39,645 - INFO - joeynmt.training - Epoch  10, Step:    20900, Batch Loss:     2.450933, Tokens per Sec:    10018, Lr: 0.000300\n",
      "2021-07-18 12:02:53,303 - INFO - joeynmt.training - Epoch  10, Step:    21000, Batch Loss:     2.891134, Tokens per Sec:    16886, Lr: 0.000300\n",
      "2021-07-18 12:03:07,044 - INFO - joeynmt.training - Epoch  10, Step:    21100, Batch Loss:     2.874460, Tokens per Sec:    16602, Lr: 0.000300\n",
      "2021-07-18 12:03:20,707 - INFO - joeynmt.training - Epoch  10, Step:    21200, Batch Loss:     2.702536, Tokens per Sec:    16662, Lr: 0.000300\n",
      "2021-07-18 12:03:34,309 - INFO - joeynmt.training - Epoch  10, Step:    21300, Batch Loss:     2.672738, Tokens per Sec:    16915, Lr: 0.000300\n",
      "2021-07-18 12:03:47,833 - INFO - joeynmt.training - Epoch  10, Step:    21400, Batch Loss:     3.028269, Tokens per Sec:    16527, Lr: 0.000300\n",
      "2021-07-18 12:04:01,461 - INFO - joeynmt.training - Epoch  10, Step:    21500, Batch Loss:     2.336853, Tokens per Sec:    16872, Lr: 0.000300\n",
      "2021-07-18 12:04:15,177 - INFO - joeynmt.training - Epoch  10, Step:    21600, Batch Loss:     2.730253, Tokens per Sec:    16311, Lr: 0.000300\n",
      "2021-07-18 12:04:28,789 - INFO - joeynmt.training - Epoch  10, Step:    21700, Batch Loss:     2.800956, Tokens per Sec:    16565, Lr: 0.000300\n",
      "2021-07-18 12:04:42,454 - INFO - joeynmt.training - Epoch  10, Step:    21800, Batch Loss:     2.541595, Tokens per Sec:    16836, Lr: 0.000300\n",
      "2021-07-18 12:04:56,001 - INFO - joeynmt.training - Epoch  10, Step:    21900, Batch Loss:     2.805495, Tokens per Sec:    16584, Lr: 0.000300\n",
      "2021-07-18 12:05:09,643 - INFO - joeynmt.training - Epoch  10, Step:    22000, Batch Loss:     2.706205, Tokens per Sec:    16764, Lr: 0.000300\n",
      "2021-07-18 12:05:23,352 - INFO - joeynmt.training - Epoch  10, Step:    22100, Batch Loss:     2.454398, Tokens per Sec:    16173, Lr: 0.000300\n",
      "2021-07-18 12:05:36,900 - INFO - joeynmt.training - Epoch  10, Step:    22200, Batch Loss:     2.686183, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-18 12:05:50,471 - INFO - joeynmt.training - Epoch  10, Step:    22300, Batch Loss:     2.545541, Tokens per Sec:    16726, Lr: 0.000300\n",
      "2021-07-18 12:06:03,849 - INFO - joeynmt.training - Epoch  10, Step:    22400, Batch Loss:     2.499347, Tokens per Sec:    16756, Lr: 0.000300\n",
      "2021-07-18 12:06:17,376 - INFO - joeynmt.training - Epoch  10, Step:    22500, Batch Loss:     2.250569, Tokens per Sec:    16674, Lr: 0.000300\n",
      "2021-07-18 12:06:31,056 - INFO - joeynmt.training - Epoch  10, Step:    22600, Batch Loss:     2.729260, Tokens per Sec:    16621, Lr: 0.000300\n",
      "2021-07-18 12:06:44,641 - INFO - joeynmt.training - Epoch  10, Step:    22700, Batch Loss:     2.634743, Tokens per Sec:    16748, Lr: 0.000300\n",
      "2021-07-18 12:06:58,281 - INFO - joeynmt.training - Epoch  10, Step:    22800, Batch Loss:     2.817023, Tokens per Sec:    16751, Lr: 0.000300\n",
      "2021-07-18 12:07:11,776 - INFO - joeynmt.training - Epoch  10, Step:    22900, Batch Loss:     2.212120, Tokens per Sec:    16897, Lr: 0.000300\n",
      "2021-07-18 12:07:25,476 - INFO - joeynmt.training - Epoch  10, Step:    23000, Batch Loss:     2.923583, Tokens per Sec:    16613, Lr: 0.000300\n",
      "2021-07-18 12:07:39,091 - INFO - joeynmt.training - Epoch  10, Step:    23100, Batch Loss:     2.960685, Tokens per Sec:    16610, Lr: 0.000300\n",
      "2021-07-18 12:07:52,797 - INFO - joeynmt.training - Epoch  10, Step:    23200, Batch Loss:     2.976733, Tokens per Sec:    16549, Lr: 0.000300\n",
      "2021-07-18 12:07:55,869 - INFO - joeynmt.training - Epoch  10: total training loss 6443.32\n",
      "2021-07-18 12:07:55,869 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-18 12:08:06,812 - INFO - joeynmt.training - Epoch  11, Step:    23300, Batch Loss:     2.581611, Tokens per Sec:    16136, Lr: 0.000300\n",
      "2021-07-18 12:08:20,398 - INFO - joeynmt.training - Epoch  11, Step:    23400, Batch Loss:     2.932013, Tokens per Sec:    16655, Lr: 0.000300\n",
      "2021-07-18 12:08:33,949 - INFO - joeynmt.training - Epoch  11, Step:    23500, Batch Loss:     2.771335, Tokens per Sec:    16758, Lr: 0.000300\n",
      "2021-07-18 12:08:47,467 - INFO - joeynmt.training - Epoch  11, Step:    23600, Batch Loss:     2.705549, Tokens per Sec:    16873, Lr: 0.000300\n",
      "2021-07-18 12:09:01,039 - INFO - joeynmt.training - Epoch  11, Step:    23700, Batch Loss:     2.906891, Tokens per Sec:    16644, Lr: 0.000300\n",
      "2021-07-18 12:09:14,708 - INFO - joeynmt.training - Epoch  11, Step:    23800, Batch Loss:     2.888039, Tokens per Sec:    16618, Lr: 0.000300\n",
      "2021-07-18 12:09:28,217 - INFO - joeynmt.training - Epoch  11, Step:    23900, Batch Loss:     2.676244, Tokens per Sec:    16911, Lr: 0.000300\n",
      "2021-07-18 12:09:41,757 - INFO - joeynmt.training - Epoch  11, Step:    24000, Batch Loss:     2.711621, Tokens per Sec:    16622, Lr: 0.000300\n",
      "2021-07-18 12:09:55,222 - INFO - joeynmt.training - Epoch  11, Step:    24100, Batch Loss:     2.699487, Tokens per Sec:    16824, Lr: 0.000300\n",
      "2021-07-18 12:10:08,941 - INFO - joeynmt.training - Epoch  11, Step:    24200, Batch Loss:     2.714545, Tokens per Sec:    16553, Lr: 0.000300\n",
      "2021-07-18 12:10:22,454 - INFO - joeynmt.training - Epoch  11, Step:    24300, Batch Loss:     2.898083, Tokens per Sec:    16398, Lr: 0.000300\n",
      "2021-07-18 12:10:35,967 - INFO - joeynmt.training - Epoch  11, Step:    24400, Batch Loss:     2.501831, Tokens per Sec:    16823, Lr: 0.000300\n",
      "2021-07-18 12:10:49,604 - INFO - joeynmt.training - Epoch  11, Step:    24500, Batch Loss:     2.801112, Tokens per Sec:    16670, Lr: 0.000300\n",
      "2021-07-18 12:11:03,172 - INFO - joeynmt.training - Epoch  11, Step:    24600, Batch Loss:     2.740498, Tokens per Sec:    16854, Lr: 0.000300\n",
      "2021-07-18 12:11:16,820 - INFO - joeynmt.training - Epoch  11, Step:    24700, Batch Loss:     2.582118, Tokens per Sec:    16684, Lr: 0.000300\n",
      "2021-07-18 12:11:30,592 - INFO - joeynmt.training - Epoch  11, Step:    24800, Batch Loss:     2.751882, Tokens per Sec:    16412, Lr: 0.000300\n",
      "2021-07-18 12:11:44,037 - INFO - joeynmt.training - Epoch  11, Step:    24900, Batch Loss:     2.641244, Tokens per Sec:    16807, Lr: 0.000300\n",
      "2021-07-18 12:11:57,591 - INFO - joeynmt.training - Epoch  11, Step:    25000, Batch Loss:     2.104525, Tokens per Sec:    16545, Lr: 0.000300\n",
      "2021-07-18 12:12:23,162 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 12:12:23,162 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 12:12:23,162 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 12:12:23,541 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 12:12:23,541 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 12:12:24,256 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 12:12:24,257 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 12:12:24,257 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 12:12:24,257 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to Him , “ He who is in the midst of the mouth , and He said to them , “ I shall come to you . ”\n",
      "2021-07-18 12:12:24,258 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 12:12:24,258 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 12:12:24,258 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 12:12:24,259 - INFO - joeynmt.training - \tHypothesis: When she was baptized , she went to Mary , and said to her , “ Mary , and she was a man , and said to him , “ You have come to me . ”\n",
      "2021-07-18 12:12:24,259 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 12:12:24,259 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 12:12:24,259 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 12:12:24,260 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day .\n",
      "2021-07-18 12:12:24,260 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 12:12:24,260 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 12:12:24,260 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 12:12:24,261 - INFO - joeynmt.training - \tHypothesis: And he had seen Him , and when He was in the midst of the bread , he was raised up , and he was a bread of the bread .\n",
      "2021-07-18 12:12:24,261 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    25000: bleu:   5.49, loss: 96824.3672, ppl:  15.8601, duration: 26.6690s\n",
      "2021-07-18 12:12:38,251 - INFO - joeynmt.training - Epoch  11, Step:    25100, Batch Loss:     2.567255, Tokens per Sec:    16665, Lr: 0.000300\n",
      "2021-07-18 12:12:51,814 - INFO - joeynmt.training - Epoch  11, Step:    25200, Batch Loss:     2.771243, Tokens per Sec:    16682, Lr: 0.000300\n",
      "2021-07-18 12:13:05,223 - INFO - joeynmt.training - Epoch  11, Step:    25300, Batch Loss:     2.407258, Tokens per Sec:    16799, Lr: 0.000300\n",
      "2021-07-18 12:13:18,789 - INFO - joeynmt.training - Epoch  11, Step:    25400, Batch Loss:     2.909930, Tokens per Sec:    16916, Lr: 0.000300\n",
      "2021-07-18 12:13:32,508 - INFO - joeynmt.training - Epoch  11, Step:    25500, Batch Loss:     2.830852, Tokens per Sec:    16390, Lr: 0.000300\n",
      "2021-07-18 12:13:39,371 - INFO - joeynmt.training - Epoch  11: total training loss 6394.96\n",
      "2021-07-18 12:13:39,371 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-18 12:13:46,318 - INFO - joeynmt.training - Epoch  12, Step:    25600, Batch Loss:     2.664860, Tokens per Sec:    15980, Lr: 0.000300\n",
      "2021-07-18 12:13:59,996 - INFO - joeynmt.training - Epoch  12, Step:    25700, Batch Loss:     2.713388, Tokens per Sec:    16712, Lr: 0.000300\n",
      "2021-07-18 12:14:13,541 - INFO - joeynmt.training - Epoch  12, Step:    25800, Batch Loss:     2.709301, Tokens per Sec:    16616, Lr: 0.000300\n",
      "2021-07-18 12:14:27,165 - INFO - joeynmt.training - Epoch  12, Step:    25900, Batch Loss:     3.020360, Tokens per Sec:    16684, Lr: 0.000300\n",
      "2021-07-18 12:14:40,428 - INFO - joeynmt.training - Epoch  12, Step:    26000, Batch Loss:     2.667823, Tokens per Sec:    16465, Lr: 0.000300\n",
      "2021-07-18 12:14:53,946 - INFO - joeynmt.training - Epoch  12, Step:    26100, Batch Loss:     2.581939, Tokens per Sec:    16562, Lr: 0.000300\n",
      "2021-07-18 12:15:07,646 - INFO - joeynmt.training - Epoch  12, Step:    26200, Batch Loss:     2.716557, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 12:15:21,377 - INFO - joeynmt.training - Epoch  12, Step:    26300, Batch Loss:     2.791682, Tokens per Sec:    16573, Lr: 0.000300\n",
      "2021-07-18 12:15:34,945 - INFO - joeynmt.training - Epoch  12, Step:    26400, Batch Loss:     2.664400, Tokens per Sec:    16870, Lr: 0.000300\n",
      "2021-07-18 12:15:48,467 - INFO - joeynmt.training - Epoch  12, Step:    26500, Batch Loss:     2.959663, Tokens per Sec:    16736, Lr: 0.000300\n",
      "2021-07-18 12:16:02,044 - INFO - joeynmt.training - Epoch  12, Step:    26600, Batch Loss:     2.373386, Tokens per Sec:    16663, Lr: 0.000300\n",
      "2021-07-18 12:16:15,713 - INFO - joeynmt.training - Epoch  12, Step:    26700, Batch Loss:     2.748428, Tokens per Sec:    16637, Lr: 0.000300\n",
      "2021-07-18 12:16:29,303 - INFO - joeynmt.training - Epoch  12, Step:    26800, Batch Loss:     2.450042, Tokens per Sec:    16729, Lr: 0.000300\n",
      "2021-07-18 12:16:42,975 - INFO - joeynmt.training - Epoch  12, Step:    26900, Batch Loss:     2.689230, Tokens per Sec:    16752, Lr: 0.000300\n",
      "2021-07-18 12:16:56,576 - INFO - joeynmt.training - Epoch  12, Step:    27000, Batch Loss:     2.582199, Tokens per Sec:    17001, Lr: 0.000300\n",
      "2021-07-18 12:17:10,130 - INFO - joeynmt.training - Epoch  12, Step:    27100, Batch Loss:     2.849659, Tokens per Sec:    17011, Lr: 0.000300\n",
      "2021-07-18 12:17:23,803 - INFO - joeynmt.training - Epoch  12, Step:    27200, Batch Loss:     2.827743, Tokens per Sec:    16476, Lr: 0.000300\n",
      "2021-07-18 12:17:37,393 - INFO - joeynmt.training - Epoch  12, Step:    27300, Batch Loss:     2.900068, Tokens per Sec:    16734, Lr: 0.000300\n",
      "2021-07-18 12:17:51,035 - INFO - joeynmt.training - Epoch  12, Step:    27400, Batch Loss:     2.972263, Tokens per Sec:    16619, Lr: 0.000300\n",
      "2021-07-18 12:18:04,665 - INFO - joeynmt.training - Epoch  12, Step:    27500, Batch Loss:     2.434264, Tokens per Sec:    16866, Lr: 0.000300\n",
      "2021-07-18 12:18:18,438 - INFO - joeynmt.training - Epoch  12, Step:    27600, Batch Loss:     2.841126, Tokens per Sec:    16669, Lr: 0.000300\n",
      "2021-07-18 12:18:32,118 - INFO - joeynmt.training - Epoch  12, Step:    27700, Batch Loss:     2.622101, Tokens per Sec:    16227, Lr: 0.000300\n",
      "2021-07-18 12:18:45,907 - INFO - joeynmt.training - Epoch  12, Step:    27800, Batch Loss:     2.859444, Tokens per Sec:    16550, Lr: 0.000300\n",
      "2021-07-18 12:18:56,298 - INFO - joeynmt.training - Epoch  12: total training loss 6327.44\n",
      "2021-07-18 12:18:56,298 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-18 12:19:00,099 - INFO - joeynmt.training - Epoch  13, Step:    27900, Batch Loss:     2.331542, Tokens per Sec:    15015, Lr: 0.000300\n",
      "2021-07-18 12:19:13,891 - INFO - joeynmt.training - Epoch  13, Step:    28000, Batch Loss:     2.738707, Tokens per Sec:    16398, Lr: 0.000300\n",
      "2021-07-18 12:19:27,427 - INFO - joeynmt.training - Epoch  13, Step:    28100, Batch Loss:     2.717178, Tokens per Sec:    16704, Lr: 0.000300\n",
      "2021-07-18 12:19:41,107 - INFO - joeynmt.training - Epoch  13, Step:    28200, Batch Loss:     2.659916, Tokens per Sec:    16802, Lr: 0.000300\n",
      "2021-07-18 12:19:54,846 - INFO - joeynmt.training - Epoch  13, Step:    28300, Batch Loss:     2.390097, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-07-18 12:20:08,635 - INFO - joeynmt.training - Epoch  13, Step:    28400, Batch Loss:     2.115096, Tokens per Sec:    16379, Lr: 0.000300\n",
      "2021-07-18 12:20:22,464 - INFO - joeynmt.training - Epoch  13, Step:    28500, Batch Loss:     2.841001, Tokens per Sec:    16488, Lr: 0.000300\n",
      "2021-07-18 12:20:36,183 - INFO - joeynmt.training - Epoch  13, Step:    28600, Batch Loss:     2.818252, Tokens per Sec:    16534, Lr: 0.000300\n",
      "2021-07-18 12:20:49,708 - INFO - joeynmt.training - Epoch  13, Step:    28700, Batch Loss:     2.778150, Tokens per Sec:    16622, Lr: 0.000300\n",
      "2021-07-18 12:21:03,402 - INFO - joeynmt.training - Epoch  13, Step:    28800, Batch Loss:     2.303812, Tokens per Sec:    16696, Lr: 0.000300\n",
      "2021-07-18 12:21:17,057 - INFO - joeynmt.training - Epoch  13, Step:    28900, Batch Loss:     2.753738, Tokens per Sec:    16390, Lr: 0.000300\n",
      "2021-07-18 12:21:30,634 - INFO - joeynmt.training - Epoch  13, Step:    29000, Batch Loss:     2.525260, Tokens per Sec:    16855, Lr: 0.000300\n",
      "2021-07-18 12:21:44,292 - INFO - joeynmt.training - Epoch  13, Step:    29100, Batch Loss:     2.815651, Tokens per Sec:    16906, Lr: 0.000300\n",
      "2021-07-18 12:21:58,002 - INFO - joeynmt.training - Epoch  13, Step:    29200, Batch Loss:     2.526947, Tokens per Sec:    16910, Lr: 0.000300\n",
      "2021-07-18 12:22:11,549 - INFO - joeynmt.training - Epoch  13, Step:    29300, Batch Loss:     2.975597, Tokens per Sec:    16530, Lr: 0.000300\n",
      "2021-07-18 12:22:25,149 - INFO - joeynmt.training - Epoch  13, Step:    29400, Batch Loss:     2.576011, Tokens per Sec:    16518, Lr: 0.000300\n",
      "2021-07-18 12:22:38,584 - INFO - joeynmt.training - Epoch  13, Step:    29500, Batch Loss:     2.927302, Tokens per Sec:    16702, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "!cd joeynmt; python3 -m joeynmt train configs/back_transformer_reverse_$tgt3$src.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWzqmGvLUcB_"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 25000\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/models/lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_{name}_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/back_lhen_reverse_transformer\"', f'model_dir: \"models/back_lhen_reverse_transformer_continued\"').replace(\n",
    "        f'epochs: 30', f'epochs: 17').replace(f'validation_freq: 5000', f'validation_freq: 2500')\n",
    "with open(\"joeynmt/configs/back_transformer_reverse_{name}_reload.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "eHd1hHFX0Cfi",
    "outputId": "0c68804c-ddb9-4a7c-b992-ea2ee9d30dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer/25000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 17                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/back_lhen_reverse_transformer_continued\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/back_transformer_reverse_lhen_reload.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4HPfdIf0Cfs",
    "outputId": "f295c31a-0e7b-45f1-c89b-fc96f12a061c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 17:10:05,612 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-18 17:10:05,687 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-18 17:10:09,772 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-18 17:10:10,273 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-18 17:10:10,939 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-18 17:10:12,029 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-18 17:10:12,029 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 17:10:12,411 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-18 17:10:12.672249: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-18 17:10:14,453 - INFO - joeynmt.training - Total params: 12138240\n",
      "2021-07-18 17:10:25,188 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer/25000.ckpt\n",
      "2021-07-18 17:10:25,657 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-18 17:10:25,657 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\n",
      "2021-07-18 17:10:25,658 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-18 17:10:25,659 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer/25000.ckpt\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-18 17:10:25,660 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-18 17:10:25,661 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-18 17:10:25,662 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.epochs                : 17\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-18 17:10:25,663 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/back_lhen_reverse_transformer_continued\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-18 17:10:25,664 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-18 17:10:25,665 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-18 17:10:25,666 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-18 17:10:25,667 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 207289,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-18 17:10:25,668 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] P@@ il@@ a@@ to nakal@@ ukha itookho , ne nal@@ anga Yesu , namureeba , ari , “ Iwe ni@@ we omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then P@@ il@@ ate ent@@ ered the P@@ ra@@ et@@ or@@ i@@ u@@ m again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.helpers - Number of Src words (types): 4211\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.helpers - Number of Trg words (types): 4211\n",
      "2021-07-18 17:10:25,669 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4211),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4211))\n",
      "2021-07-18 17:10:25,686 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-18 17:10:25,686 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-18 17:10:39,819 - INFO - joeynmt.training - Epoch   1, Step:    25100, Batch Loss:     2.523359, Tokens per Sec:    16499, Lr: 0.000300\n",
      "2021-07-18 17:10:52,782 - INFO - joeynmt.training - Epoch   1, Step:    25200, Batch Loss:     2.780542, Tokens per Sec:    17454, Lr: 0.000300\n",
      "2021-07-18 17:11:05,735 - INFO - joeynmt.training - Epoch   1, Step:    25300, Batch Loss:     2.421601, Tokens per Sec:    17389, Lr: 0.000300\n",
      "2021-07-18 17:11:18,885 - INFO - joeynmt.training - Epoch   1, Step:    25400, Batch Loss:     2.887430, Tokens per Sec:    17452, Lr: 0.000300\n",
      "2021-07-18 17:11:32,117 - INFO - joeynmt.training - Epoch   1, Step:    25500, Batch Loss:     2.857241, Tokens per Sec:    16993, Lr: 0.000300\n",
      "2021-07-18 17:11:38,727 - INFO - joeynmt.training - Epoch   1: total training loss 1514.16\n",
      "2021-07-18 17:11:38,728 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-18 17:11:45,559 - INFO - joeynmt.training - Epoch   2, Step:    25600, Batch Loss:     2.688238, Tokens per Sec:    16250, Lr: 0.000300\n",
      "2021-07-18 17:11:58,896 - INFO - joeynmt.training - Epoch   2, Step:    25700, Batch Loss:     2.683969, Tokens per Sec:    17139, Lr: 0.000300\n",
      "2021-07-18 17:12:12,272 - INFO - joeynmt.training - Epoch   2, Step:    25800, Batch Loss:     2.694610, Tokens per Sec:    16826, Lr: 0.000300\n",
      "2021-07-18 17:12:25,762 - INFO - joeynmt.training - Epoch   2, Step:    25900, Batch Loss:     2.996311, Tokens per Sec:    16849, Lr: 0.000300\n",
      "2021-07-18 17:12:38,984 - INFO - joeynmt.training - Epoch   2, Step:    26000, Batch Loss:     2.681271, Tokens per Sec:    16516, Lr: 0.000300\n",
      "2021-07-18 17:12:52,402 - INFO - joeynmt.training - Epoch   2, Step:    26100, Batch Loss:     2.552606, Tokens per Sec:    16685, Lr: 0.000300\n",
      "2021-07-18 17:13:06,010 - INFO - joeynmt.training - Epoch   2, Step:    26200, Batch Loss:     2.712375, Tokens per Sec:    16685, Lr: 0.000300\n",
      "2021-07-18 17:13:19,760 - INFO - joeynmt.training - Epoch   2, Step:    26300, Batch Loss:     2.797812, Tokens per Sec:    16550, Lr: 0.000300\n",
      "2021-07-18 17:13:33,382 - INFO - joeynmt.training - Epoch   2, Step:    26400, Batch Loss:     2.701579, Tokens per Sec:    16804, Lr: 0.000300\n",
      "2021-07-18 17:13:46,973 - INFO - joeynmt.training - Epoch   2, Step:    26500, Batch Loss:     2.954422, Tokens per Sec:    16650, Lr: 0.000300\n",
      "2021-07-18 17:14:00,656 - INFO - joeynmt.training - Epoch   2, Step:    26600, Batch Loss:     2.358116, Tokens per Sec:    16532, Lr: 0.000300\n",
      "2021-07-18 17:14:14,496 - INFO - joeynmt.training - Epoch   2, Step:    26700, Batch Loss:     2.731507, Tokens per Sec:    16433, Lr: 0.000300\n",
      "2021-07-18 17:14:28,293 - INFO - joeynmt.training - Epoch   2, Step:    26800, Batch Loss:     2.417904, Tokens per Sec:    16477, Lr: 0.000300\n",
      "2021-07-18 17:14:42,200 - INFO - joeynmt.training - Epoch   2, Step:    26900, Batch Loss:     2.665245, Tokens per Sec:    16470, Lr: 0.000300\n",
      "2021-07-18 17:14:55,961 - INFO - joeynmt.training - Epoch   2, Step:    27000, Batch Loss:     2.607755, Tokens per Sec:    16804, Lr: 0.000300\n",
      "2021-07-18 17:15:09,749 - INFO - joeynmt.training - Epoch   2, Step:    27100, Batch Loss:     2.908663, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-18 17:15:23,583 - INFO - joeynmt.training - Epoch   2, Step:    27200, Batch Loss:     2.831758, Tokens per Sec:    16284, Lr: 0.000300\n",
      "2021-07-18 17:15:37,313 - INFO - joeynmt.training - Epoch   2, Step:    27300, Batch Loss:     2.906028, Tokens per Sec:    16564, Lr: 0.000300\n",
      "2021-07-18 17:15:51,106 - INFO - joeynmt.training - Epoch   2, Step:    27400, Batch Loss:     2.977713, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-18 17:16:05,013 - INFO - joeynmt.training - Epoch   2, Step:    27500, Batch Loss:     2.432045, Tokens per Sec:    16530, Lr: 0.000300\n",
      "2021-07-18 17:16:38,607 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:16:38,608 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:16:38,608 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:16:38,997 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:16:38,997 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:16:39,741 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:16:39,743 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:16:39,743 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:16:39,743 - INFO - joeynmt.training - \tHypothesis: And the Pharisees saw Him , and He saw Him , and said to Him , “ He who was in the midst of the throne , and they came to Him . ”\n",
      "2021-07-18 17:16:39,743 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:16:39,744 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:16:39,744 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:16:39,745 - INFO - joeynmt.training - \tHypothesis: When Martha came to Mary , Mary came to Mary , and said to Him , “ Mary , and the disciples came to Him , and the Lord came to Him . ”\n",
      "2021-07-18 17:16:39,745 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:16:39,745 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:16:39,745 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:16:39,746 - INFO - joeynmt.training - \tHypothesis: He was a great day for the day of the day of the day .\n",
      "2021-07-18 17:16:39,746 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:16:39,747 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:16:39,747 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:16:39,747 - INFO - joeynmt.training - \tHypothesis: And when he was raised , he was raised up , and when he was raised up , and the bread of the bread , and the collector was raised up .\n",
      "2021-07-18 17:16:39,747 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    27500: bleu:   4.84, loss: 96084.9531, ppl:  15.5288, duration: 34.7342s\n",
      "2021-07-18 17:16:53,741 - INFO - joeynmt.training - Epoch   2, Step:    27600, Batch Loss:     2.834100, Tokens per Sec:    16406, Lr: 0.000300\n",
      "2021-07-18 17:17:07,333 - INFO - joeynmt.training - Epoch   2, Step:    27700, Batch Loss:     2.633597, Tokens per Sec:    16332, Lr: 0.000300\n",
      "2021-07-18 17:17:21,238 - INFO - joeynmt.training - Epoch   2, Step:    27800, Batch Loss:     2.893713, Tokens per Sec:    16411, Lr: 0.000300\n",
      "2021-07-18 17:17:31,731 - INFO - joeynmt.training - Epoch   2: total training loss 6326.45\n",
      "2021-07-18 17:17:31,732 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-18 17:17:35,531 - INFO - joeynmt.training - Epoch   3, Step:    27900, Batch Loss:     2.346241, Tokens per Sec:    15021, Lr: 0.000300\n",
      "2021-07-18 17:17:49,161 - INFO - joeynmt.training - Epoch   3, Step:    28000, Batch Loss:     2.763128, Tokens per Sec:    16593, Lr: 0.000300\n",
      "2021-07-18 17:18:02,841 - INFO - joeynmt.training - Epoch   3, Step:    28100, Batch Loss:     2.707178, Tokens per Sec:    16529, Lr: 0.000300\n",
      "2021-07-18 17:18:16,638 - INFO - joeynmt.training - Epoch   3, Step:    28200, Batch Loss:     2.645268, Tokens per Sec:    16659, Lr: 0.000300\n",
      "2021-07-18 17:18:30,329 - INFO - joeynmt.training - Epoch   3, Step:    28300, Batch Loss:     2.393321, Tokens per Sec:    16653, Lr: 0.000300\n",
      "2021-07-18 17:18:44,095 - INFO - joeynmt.training - Epoch   3, Step:    28400, Batch Loss:     2.090425, Tokens per Sec:    16406, Lr: 0.000300\n",
      "2021-07-18 17:18:57,790 - INFO - joeynmt.training - Epoch   3, Step:    28500, Batch Loss:     2.847047, Tokens per Sec:    16648, Lr: 0.000300\n",
      "2021-07-18 17:19:11,628 - INFO - joeynmt.training - Epoch   3, Step:    28600, Batch Loss:     2.780911, Tokens per Sec:    16392, Lr: 0.000300\n",
      "2021-07-18 17:19:25,374 - INFO - joeynmt.training - Epoch   3, Step:    28700, Batch Loss:     2.773043, Tokens per Sec:    16354, Lr: 0.000300\n",
      "2021-07-18 17:19:39,076 - INFO - joeynmt.training - Epoch   3, Step:    28800, Batch Loss:     2.307051, Tokens per Sec:    16688, Lr: 0.000300\n",
      "2021-07-18 17:19:52,681 - INFO - joeynmt.training - Epoch   3, Step:    28900, Batch Loss:     2.753651, Tokens per Sec:    16449, Lr: 0.000300\n",
      "2021-07-18 17:20:06,327 - INFO - joeynmt.training - Epoch   3, Step:    29000, Batch Loss:     2.535723, Tokens per Sec:    16770, Lr: 0.000300\n",
      "2021-07-18 17:20:20,146 - INFO - joeynmt.training - Epoch   3, Step:    29100, Batch Loss:     2.793529, Tokens per Sec:    16710, Lr: 0.000300\n",
      "2021-07-18 17:20:34,052 - INFO - joeynmt.training - Epoch   3, Step:    29200, Batch Loss:     2.532309, Tokens per Sec:    16671, Lr: 0.000300\n",
      "2021-07-18 17:20:47,855 - INFO - joeynmt.training - Epoch   3, Step:    29300, Batch Loss:     2.977476, Tokens per Sec:    16225, Lr: 0.000300\n",
      "2021-07-18 17:21:01,622 - INFO - joeynmt.training - Epoch   3, Step:    29400, Batch Loss:     2.572039, Tokens per Sec:    16317, Lr: 0.000300\n",
      "2021-07-18 17:21:15,215 - INFO - joeynmt.training - Epoch   3, Step:    29500, Batch Loss:     2.919913, Tokens per Sec:    16508, Lr: 0.000300\n",
      "2021-07-18 17:21:29,034 - INFO - joeynmt.training - Epoch   3, Step:    29600, Batch Loss:     2.828959, Tokens per Sec:    16420, Lr: 0.000300\n",
      "2021-07-18 17:21:42,710 - INFO - joeynmt.training - Epoch   3, Step:    29700, Batch Loss:     2.764737, Tokens per Sec:    16715, Lr: 0.000300\n",
      "2021-07-18 17:21:56,500 - INFO - joeynmt.training - Epoch   3, Step:    29800, Batch Loss:     2.714650, Tokens per Sec:    16406, Lr: 0.000300\n",
      "2021-07-18 17:22:10,348 - INFO - joeynmt.training - Epoch   3, Step:    29900, Batch Loss:     2.988300, Tokens per Sec:    16398, Lr: 0.000300\n",
      "2021-07-18 17:22:24,243 - INFO - joeynmt.training - Epoch   3, Step:    30000, Batch Loss:     2.770137, Tokens per Sec:    16400, Lr: 0.000300\n",
      "2021-07-18 17:22:49,146 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:22:49,147 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:22:49,147 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:22:49,515 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:22:49,515 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:22:50,690 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:22:50,691 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:22:50,691 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:22:50,692 - INFO - joeynmt.training - \tHypothesis: And the Pharisees and the Pharisees were like to be a source of the Pharisees , saying , “ I have come to you , and I have come to know . ”\n",
      "2021-07-18 17:22:50,692 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:22:50,692 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:22:50,692 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - \tHypothesis: When she was a great crowd , she was a mother , and her mother , and her mother , and her mother , and said to her , “ Mary , and she was a friend . ”\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:22:50,693 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day .\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - \tHypothesis: And when he was given to the body , he was raised up , and he was raised up , and the body of the body of the body , and the body of the body was raised up .\n",
      "2021-07-18 17:22:50,694 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    30000: bleu:   5.37, loss: 95258.6016, ppl:  15.1668, duration: 26.4505s\n",
      "2021-07-18 17:23:04,890 - INFO - joeynmt.training - Epoch   3, Step:    30100, Batch Loss:     2.856949, Tokens per Sec:    15892, Lr: 0.000300\n",
      "2021-07-18 17:23:18,639 - INFO - joeynmt.training - Epoch   3, Step:    30200, Batch Loss:     2.913859, Tokens per Sec:    16117, Lr: 0.000300\n",
      "2021-07-18 17:23:18,925 - INFO - joeynmt.training - Epoch   3: total training loss 6286.32\n",
      "2021-07-18 17:23:18,926 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-18 17:23:32,958 - INFO - joeynmt.training - Epoch   4, Step:    30300, Batch Loss:     2.750962, Tokens per Sec:    16201, Lr: 0.000300\n",
      "2021-07-18 17:23:46,719 - INFO - joeynmt.training - Epoch   4, Step:    30400, Batch Loss:     2.782564, Tokens per Sec:    16413, Lr: 0.000300\n",
      "2021-07-18 17:24:00,387 - INFO - joeynmt.training - Epoch   4, Step:    30500, Batch Loss:     2.692506, Tokens per Sec:    16456, Lr: 0.000300\n",
      "2021-07-18 17:24:14,122 - INFO - joeynmt.training - Epoch   4, Step:    30600, Batch Loss:     2.312006, Tokens per Sec:    16222, Lr: 0.000300\n",
      "2021-07-18 17:24:27,988 - INFO - joeynmt.training - Epoch   4, Step:    30700, Batch Loss:     2.553147, Tokens per Sec:    16748, Lr: 0.000300\n",
      "2021-07-18 17:24:41,643 - INFO - joeynmt.training - Epoch   4, Step:    30800, Batch Loss:     2.866960, Tokens per Sec:    16350, Lr: 0.000300\n",
      "2021-07-18 17:24:55,263 - INFO - joeynmt.training - Epoch   4, Step:    30900, Batch Loss:     2.499128, Tokens per Sec:    16717, Lr: 0.000300\n",
      "2021-07-18 17:25:08,943 - INFO - joeynmt.training - Epoch   4, Step:    31000, Batch Loss:     2.807083, Tokens per Sec:    16367, Lr: 0.000300\n",
      "2021-07-18 17:25:22,673 - INFO - joeynmt.training - Epoch   4, Step:    31100, Batch Loss:     2.810971, Tokens per Sec:    16284, Lr: 0.000300\n",
      "2021-07-18 17:25:36,513 - INFO - joeynmt.training - Epoch   4, Step:    31200, Batch Loss:     2.973747, Tokens per Sec:    16685, Lr: 0.000300\n",
      "2021-07-18 17:25:50,300 - INFO - joeynmt.training - Epoch   4, Step:    31300, Batch Loss:     2.585427, Tokens per Sec:    16581, Lr: 0.000300\n",
      "2021-07-18 17:26:04,104 - INFO - joeynmt.training - Epoch   4, Step:    31400, Batch Loss:     2.546406, Tokens per Sec:    16567, Lr: 0.000300\n",
      "2021-07-18 17:26:17,899 - INFO - joeynmt.training - Epoch   4, Step:    31500, Batch Loss:     2.591547, Tokens per Sec:    16446, Lr: 0.000300\n",
      "2021-07-18 17:26:31,500 - INFO - joeynmt.training - Epoch   4, Step:    31600, Batch Loss:     2.925022, Tokens per Sec:    16322, Lr: 0.000300\n",
      "2021-07-18 17:26:45,267 - INFO - joeynmt.training - Epoch   4, Step:    31700, Batch Loss:     2.417161, Tokens per Sec:    16556, Lr: 0.000300\n",
      "2021-07-18 17:26:59,312 - INFO - joeynmt.training - Epoch   4, Step:    31800, Batch Loss:     2.577895, Tokens per Sec:    16431, Lr: 0.000300\n",
      "2021-07-18 17:27:13,029 - INFO - joeynmt.training - Epoch   4, Step:    31900, Batch Loss:     2.618228, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-07-18 17:27:26,790 - INFO - joeynmt.training - Epoch   4, Step:    32000, Batch Loss:     2.297007, Tokens per Sec:    16514, Lr: 0.000300\n",
      "2021-07-18 17:27:40,539 - INFO - joeynmt.training - Epoch   4, Step:    32100, Batch Loss:     2.868172, Tokens per Sec:    16502, Lr: 0.000300\n",
      "2021-07-18 17:27:54,263 - INFO - joeynmt.training - Epoch   4, Step:    32200, Batch Loss:     2.727189, Tokens per Sec:    16467, Lr: 0.000300\n",
      "2021-07-18 17:28:08,181 - INFO - joeynmt.training - Epoch   4, Step:    32300, Batch Loss:     2.945050, Tokens per Sec:    16593, Lr: 0.000300\n",
      "2021-07-18 17:28:21,954 - INFO - joeynmt.training - Epoch   4, Step:    32400, Batch Loss:     2.305427, Tokens per Sec:    16415, Lr: 0.000300\n",
      "2021-07-18 17:28:35,649 - INFO - joeynmt.training - Epoch   4, Step:    32500, Batch Loss:     3.012115, Tokens per Sec:    16832, Lr: 0.000300\n",
      "2021-07-18 17:29:02,534 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:29:02,534 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:29:02,534 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:29:02,902 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:29:02,903 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:29:03,672 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:29:03,673 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:29:03,673 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:29:03,673 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to Him , “ He who has come to Him , and He who has come to me , and I will see the things I have seen . ”\n",
      "2021-07-18 17:29:03,674 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:29:03,674 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:29:03,674 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:29:03,674 - INFO - joeynmt.training - \tHypothesis: And when she was in the synagogue , she was a mother , and said to her , “ Mary , and the daughter of Mary , and the daughter of Mary . ”\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day .\n",
      "2021-07-18 17:29:03,675 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:29:03,676 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:29:03,676 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:29:03,676 - INFO - joeynmt.training - \tHypothesis: And he had been given up with the body , and when He was raised , he was raised up , and the body of the body of the body of the body of the body , and the body of the body of the body .\n",
      "2021-07-18 17:29:03,676 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    32500: bleu:   5.56, loss: 94901.8281, ppl:  15.0132, duration: 28.0275s\n",
      "2021-07-18 17:29:07,052 - INFO - joeynmt.training - Epoch   4: total training loss 6232.61\n",
      "2021-07-18 17:29:07,053 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-18 17:29:18,167 - INFO - joeynmt.training - Epoch   5, Step:    32600, Batch Loss:     2.944046, Tokens per Sec:    15804, Lr: 0.000300\n",
      "2021-07-18 17:29:31,873 - INFO - joeynmt.training - Epoch   5, Step:    32700, Batch Loss:     2.533552, Tokens per Sec:    16595, Lr: 0.000300\n",
      "2021-07-18 17:29:45,506 - INFO - joeynmt.training - Epoch   5, Step:    32800, Batch Loss:     2.795729, Tokens per Sec:    16424, Lr: 0.000300\n",
      "2021-07-18 17:29:59,210 - INFO - joeynmt.training - Epoch   5, Step:    32900, Batch Loss:     2.261023, Tokens per Sec:    16636, Lr: 0.000300\n",
      "2021-07-18 17:30:12,899 - INFO - joeynmt.training - Epoch   5, Step:    33000, Batch Loss:     2.596199, Tokens per Sec:    16731, Lr: 0.000300\n",
      "2021-07-18 17:30:26,557 - INFO - joeynmt.training - Epoch   5, Step:    33100, Batch Loss:     2.732780, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-07-18 17:30:40,264 - INFO - joeynmt.training - Epoch   5, Step:    33200, Batch Loss:     2.810383, Tokens per Sec:    16501, Lr: 0.000300\n",
      "2021-07-18 17:30:54,135 - INFO - joeynmt.training - Epoch   5, Step:    33300, Batch Loss:     2.747925, Tokens per Sec:    16686, Lr: 0.000300\n",
      "2021-07-18 17:31:07,801 - INFO - joeynmt.training - Epoch   5, Step:    33400, Batch Loss:     2.662137, Tokens per Sec:    16383, Lr: 0.000300\n",
      "2021-07-18 17:31:21,493 - INFO - joeynmt.training - Epoch   5, Step:    33500, Batch Loss:     2.661890, Tokens per Sec:    16414, Lr: 0.000300\n",
      "2021-07-18 17:31:35,252 - INFO - joeynmt.training - Epoch   5, Step:    33600, Batch Loss:     2.482535, Tokens per Sec:    16451, Lr: 0.000300\n",
      "2021-07-18 17:31:49,066 - INFO - joeynmt.training - Epoch   5, Step:    33700, Batch Loss:     2.712437, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-18 17:32:02,762 - INFO - joeynmt.training - Epoch   5, Step:    33800, Batch Loss:     2.592180, Tokens per Sec:    16493, Lr: 0.000300\n",
      "2021-07-18 17:32:16,612 - INFO - joeynmt.training - Epoch   5, Step:    33900, Batch Loss:     2.779282, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-07-18 17:32:30,241 - INFO - joeynmt.training - Epoch   5, Step:    34000, Batch Loss:     3.029863, Tokens per Sec:    16519, Lr: 0.000300\n",
      "2021-07-18 17:32:44,059 - INFO - joeynmt.training - Epoch   5, Step:    34100, Batch Loss:     2.783033, Tokens per Sec:    16756, Lr: 0.000300\n",
      "2021-07-18 17:32:57,854 - INFO - joeynmt.training - Epoch   5, Step:    34200, Batch Loss:     2.600207, Tokens per Sec:    16466, Lr: 0.000300\n",
      "2021-07-18 17:33:11,714 - INFO - joeynmt.training - Epoch   5, Step:    34300, Batch Loss:     2.610632, Tokens per Sec:    16652, Lr: 0.000300\n",
      "2021-07-18 17:33:25,550 - INFO - joeynmt.training - Epoch   5, Step:    34400, Batch Loss:     2.670703, Tokens per Sec:    16528, Lr: 0.000300\n",
      "2021-07-18 17:33:39,290 - INFO - joeynmt.training - Epoch   5, Step:    34500, Batch Loss:     2.806657, Tokens per Sec:    16419, Lr: 0.000300\n",
      "2021-07-18 17:33:52,974 - INFO - joeynmt.training - Epoch   5, Step:    34600, Batch Loss:     2.954773, Tokens per Sec:    16447, Lr: 0.000300\n",
      "2021-07-18 17:34:06,660 - INFO - joeynmt.training - Epoch   5, Step:    34700, Batch Loss:     2.423722, Tokens per Sec:    16301, Lr: 0.000300\n",
      "2021-07-18 17:34:20,415 - INFO - joeynmt.training - Epoch   5, Step:    34800, Batch Loss:     2.671597, Tokens per Sec:    16521, Lr: 0.000300\n",
      "2021-07-18 17:34:27,375 - INFO - joeynmt.training - Epoch   5: total training loss 6207.99\n",
      "2021-07-18 17:34:27,376 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-18 17:34:34,478 - INFO - joeynmt.training - Epoch   6, Step:    34900, Batch Loss:     2.813895, Tokens per Sec:    15946, Lr: 0.000300\n",
      "2021-07-18 17:34:48,257 - INFO - joeynmt.training - Epoch   6, Step:    35000, Batch Loss:     2.470519, Tokens per Sec:    16805, Lr: 0.000300\n",
      "2021-07-18 17:35:16,381 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:35:16,382 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:35:16,382 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:35:17,546 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:35:17,547 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:35:17,547 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:35:17,547 - INFO - joeynmt.training - \tHypothesis: And the Pharisees and the Pharisees were like to be a man , saying , “ Look ! ” And he said to them , “ Whoever is the city of the city , and they are going to see . ”\n",
      "2021-07-18 17:35:17,547 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:35:17,548 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:35:17,548 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:35:17,548 - INFO - joeynmt.training - \tHypothesis: And when she was at the same time , Mary and Mary , Mary , and Mary , and Mary , and Mary , and Mary , and Mary , “ Mary , and the one who is the one . ”\n",
      "2021-07-18 17:35:17,548 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:35:17,549 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:35:17,549 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:35:17,549 - INFO - joeynmt.training - \tHypothesis: He was a great day of the day of the day of the day .\n",
      "2021-07-18 17:35:17,549 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:35:17,550 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:35:17,550 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:35:17,550 - INFO - joeynmt.training - \tHypothesis: Paul also gave up the body , and when he was in the body , he was a brief , and the bridegroom , and the bridegroom were buried .\n",
      "2021-07-18 17:35:17,550 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    35000: bleu:   5.45, loss: 94904.0625, ppl:  15.0141, duration: 29.2925s\n",
      "2021-07-18 17:35:31,368 - INFO - joeynmt.training - Epoch   6, Step:    35100, Batch Loss:     2.621793, Tokens per Sec:    16373, Lr: 0.000300\n",
      "2021-07-18 17:35:44,992 - INFO - joeynmt.training - Epoch   6, Step:    35200, Batch Loss:     2.581375, Tokens per Sec:    16561, Lr: 0.000300\n",
      "2021-07-18 17:35:58,599 - INFO - joeynmt.training - Epoch   6, Step:    35300, Batch Loss:     2.176227, Tokens per Sec:    16408, Lr: 0.000300\n",
      "2021-07-18 17:36:12,403 - INFO - joeynmt.training - Epoch   6, Step:    35400, Batch Loss:     2.638392, Tokens per Sec:    16486, Lr: 0.000300\n",
      "2021-07-18 17:36:26,325 - INFO - joeynmt.training - Epoch   6, Step:    35500, Batch Loss:     2.894516, Tokens per Sec:    16301, Lr: 0.000300\n",
      "2021-07-18 17:36:40,146 - INFO - joeynmt.training - Epoch   6, Step:    35600, Batch Loss:     2.677186, Tokens per Sec:    16515, Lr: 0.000300\n",
      "2021-07-18 17:36:53,959 - INFO - joeynmt.training - Epoch   6, Step:    35700, Batch Loss:     2.672566, Tokens per Sec:    16388, Lr: 0.000300\n",
      "2021-07-18 17:37:07,846 - INFO - joeynmt.training - Epoch   6, Step:    35800, Batch Loss:     2.661989, Tokens per Sec:    16320, Lr: 0.000300\n",
      "2021-07-18 17:37:21,569 - INFO - joeynmt.training - Epoch   6, Step:    35900, Batch Loss:     2.292081, Tokens per Sec:    16579, Lr: 0.000300\n",
      "2021-07-18 17:37:35,338 - INFO - joeynmt.training - Epoch   6, Step:    36000, Batch Loss:     2.334862, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-18 17:37:48,893 - INFO - joeynmt.training - Epoch   6, Step:    36100, Batch Loss:     2.832502, Tokens per Sec:    16469, Lr: 0.000300\n",
      "2021-07-18 17:38:02,779 - INFO - joeynmt.training - Epoch   6, Step:    36200, Batch Loss:     2.808983, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-18 17:38:16,619 - INFO - joeynmt.training - Epoch   6, Step:    36300, Batch Loss:     2.505994, Tokens per Sec:    16506, Lr: 0.000300\n",
      "2021-07-18 17:38:30,403 - INFO - joeynmt.training - Epoch   6, Step:    36400, Batch Loss:     2.886223, Tokens per Sec:    16786, Lr: 0.000300\n",
      "2021-07-18 17:38:44,079 - INFO - joeynmt.training - Epoch   6, Step:    36500, Batch Loss:     2.740465, Tokens per Sec:    16588, Lr: 0.000300\n",
      "2021-07-18 17:38:57,632 - INFO - joeynmt.training - Epoch   6, Step:    36600, Batch Loss:     2.771126, Tokens per Sec:    16353, Lr: 0.000300\n",
      "2021-07-18 17:39:11,409 - INFO - joeynmt.training - Epoch   6, Step:    36700, Batch Loss:     2.829376, Tokens per Sec:    16767, Lr: 0.000300\n",
      "2021-07-18 17:39:25,115 - INFO - joeynmt.training - Epoch   6, Step:    36800, Batch Loss:     2.558002, Tokens per Sec:    16453, Lr: 0.000300\n",
      "2021-07-18 17:39:38,732 - INFO - joeynmt.training - Epoch   6, Step:    36900, Batch Loss:     2.136938, Tokens per Sec:    16580, Lr: 0.000300\n",
      "2021-07-18 17:39:52,340 - INFO - joeynmt.training - Epoch   6, Step:    37000, Batch Loss:     2.699796, Tokens per Sec:    16357, Lr: 0.000300\n",
      "2021-07-18 17:40:06,069 - INFO - joeynmt.training - Epoch   6, Step:    37100, Batch Loss:     2.671640, Tokens per Sec:    16484, Lr: 0.000300\n",
      "2021-07-18 17:40:16,585 - INFO - joeynmt.training - Epoch   6: total training loss 6172.57\n",
      "2021-07-18 17:40:16,585 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-18 17:40:20,093 - INFO - joeynmt.training - Epoch   7, Step:    37200, Batch Loss:     2.673325, Tokens per Sec:    14684, Lr: 0.000300\n",
      "2021-07-18 17:40:33,867 - INFO - joeynmt.training - Epoch   7, Step:    37300, Batch Loss:     2.837071, Tokens per Sec:    16585, Lr: 0.000300\n",
      "2021-07-18 17:40:47,576 - INFO - joeynmt.training - Epoch   7, Step:    37400, Batch Loss:     2.585966, Tokens per Sec:    16756, Lr: 0.000300\n",
      "2021-07-18 17:41:01,324 - INFO - joeynmt.training - Epoch   7, Step:    37500, Batch Loss:     2.237097, Tokens per Sec:    16455, Lr: 0.000300\n",
      "2021-07-18 17:41:32,381 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:41:32,381 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:41:32,381 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:41:32,732 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:41:32,732 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:41:33,909 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:41:33,910 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:41:33,910 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:41:33,910 - INFO - joeynmt.training - \tHypothesis: And the Pharisees and the Pharisees were like a man , saying , “ It is a great crowd , and the seven hour , and the hour of the wine . ”\n",
      "2021-07-18 17:41:33,911 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:41:33,911 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:41:33,912 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:41:33,912 - INFO - joeynmt.training - \tHypothesis: When she was a great crowd , Mary and Mary , Mary , and Mary , and Mary , and Mary , and Mary , and Mary .\n",
      "2021-07-18 17:41:33,912 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:41:33,914 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:41:33,914 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:41:33,914 - INFO - joeynmt.training - \tHypothesis: He was a great day of the day of the day of the day .\n",
      "2021-07-18 17:41:33,914 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:41:33,915 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:41:33,915 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:41:33,915 - INFO - joeynmt.training - \tHypothesis: And when he had been given to the body , he was sleeping , and the body of the body of the body of the body of the body , and the bread of the wheat of the heat .\n",
      "2021-07-18 17:41:33,915 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    37500: bleu:   6.02, loss: 93161.5859, ppl:  14.2856, duration: 32.5910s\n",
      "2021-07-18 17:41:47,920 - INFO - joeynmt.training - Epoch   7, Step:    37600, Batch Loss:     2.595340, Tokens per Sec:    16240, Lr: 0.000300\n",
      "2021-07-18 17:42:01,704 - INFO - joeynmt.training - Epoch   7, Step:    37700, Batch Loss:     2.582844, Tokens per Sec:    16442, Lr: 0.000300\n",
      "2021-07-18 17:42:15,595 - INFO - joeynmt.training - Epoch   7, Step:    37800, Batch Loss:     2.547504, Tokens per Sec:    16807, Lr: 0.000300\n",
      "2021-07-18 17:42:29,484 - INFO - joeynmt.training - Epoch   7, Step:    37900, Batch Loss:     2.415132, Tokens per Sec:    16546, Lr: 0.000300\n",
      "2021-07-18 17:42:43,080 - INFO - joeynmt.training - Epoch   7, Step:    38000, Batch Loss:     2.514435, Tokens per Sec:    16496, Lr: 0.000300\n",
      "2021-07-18 17:42:56,812 - INFO - joeynmt.training - Epoch   7, Step:    38100, Batch Loss:     2.835392, Tokens per Sec:    16506, Lr: 0.000300\n",
      "2021-07-18 17:43:10,569 - INFO - joeynmt.training - Epoch   7, Step:    38200, Batch Loss:     2.635521, Tokens per Sec:    16780, Lr: 0.000300\n",
      "2021-07-18 17:43:24,183 - INFO - joeynmt.training - Epoch   7, Step:    38300, Batch Loss:     2.520733, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 17:43:37,924 - INFO - joeynmt.training - Epoch   7, Step:    38400, Batch Loss:     2.424816, Tokens per Sec:    16662, Lr: 0.000300\n",
      "2021-07-18 17:43:51,619 - INFO - joeynmt.training - Epoch   7, Step:    38500, Batch Loss:     2.761295, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-18 17:44:05,382 - INFO - joeynmt.training - Epoch   7, Step:    38600, Batch Loss:     2.669761, Tokens per Sec:    16556, Lr: 0.000300\n",
      "2021-07-18 17:44:19,121 - INFO - joeynmt.training - Epoch   7, Step:    38700, Batch Loss:     2.629753, Tokens per Sec:    16503, Lr: 0.000300\n",
      "2021-07-18 17:44:32,801 - INFO - joeynmt.training - Epoch   7, Step:    38800, Batch Loss:     2.787641, Tokens per Sec:    16601, Lr: 0.000300\n",
      "2021-07-18 17:44:46,454 - INFO - joeynmt.training - Epoch   7, Step:    38900, Batch Loss:     2.205695, Tokens per Sec:    16429, Lr: 0.000300\n",
      "2021-07-18 17:45:00,243 - INFO - joeynmt.training - Epoch   7, Step:    39000, Batch Loss:     2.776600, Tokens per Sec:    16721, Lr: 0.000300\n",
      "2021-07-18 17:45:14,049 - INFO - joeynmt.training - Epoch   7, Step:    39100, Batch Loss:     2.980540, Tokens per Sec:    16287, Lr: 0.000300\n",
      "2021-07-18 17:45:27,739 - INFO - joeynmt.training - Epoch   7, Step:    39200, Batch Loss:     2.885010, Tokens per Sec:    16641, Lr: 0.000300\n",
      "2021-07-18 17:45:41,366 - INFO - joeynmt.training - Epoch   7, Step:    39300, Batch Loss:     2.643274, Tokens per Sec:    16707, Lr: 0.000300\n",
      "2021-07-18 17:45:54,941 - INFO - joeynmt.training - Epoch   7, Step:    39400, Batch Loss:     2.682798, Tokens per Sec:    16528, Lr: 0.000300\n",
      "2021-07-18 17:46:08,474 - INFO - joeynmt.training - Epoch   7: total training loss 6128.12\n",
      "2021-07-18 17:46:08,475 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-18 17:46:09,093 - INFO - joeynmt.training - Epoch   8, Step:    39500, Batch Loss:     2.175182, Tokens per Sec:     7376, Lr: 0.000300\n",
      "2021-07-18 17:46:22,852 - INFO - joeynmt.training - Epoch   8, Step:    39600, Batch Loss:     2.617034, Tokens per Sec:    16449, Lr: 0.000300\n",
      "2021-07-18 17:46:36,599 - INFO - joeynmt.training - Epoch   8, Step:    39700, Batch Loss:     2.576166, Tokens per Sec:    16555, Lr: 0.000300\n",
      "2021-07-18 17:46:50,261 - INFO - joeynmt.training - Epoch   8, Step:    39800, Batch Loss:     2.362172, Tokens per Sec:    16393, Lr: 0.000300\n",
      "2021-07-18 17:47:04,073 - INFO - joeynmt.training - Epoch   8, Step:    39900, Batch Loss:     2.690714, Tokens per Sec:    16757, Lr: 0.000300\n",
      "2021-07-18 17:47:17,845 - INFO - joeynmt.training - Epoch   8, Step:    40000, Batch Loss:     2.753013, Tokens per Sec:    16434, Lr: 0.000300\n",
      "2021-07-18 17:47:46,008 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:47:46,009 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:47:46,009 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:47:47,126 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:47:47,127 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:47:47,127 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:47:47,127 - INFO - joeynmt.training - \tHypothesis: And the Pharisees and the Pharisees saw Him , saying , “ I have come to the door , and I have come to them , and I have come to you . ”\n",
      "2021-07-18 17:47:47,128 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:47:47,128 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:47:47,128 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - \tHypothesis: And when she was a great crowd , He went to Mary , and said to Him , “ Mary , and He said to Him , “ Go , and see Him , and you are the one who is coming . ”\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:47:47,129 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the end .\n",
      "2021-07-18 17:47:47,130 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:47:47,130 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:47:47,130 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:47:47,130 - INFO - joeynmt.training - \tHypothesis: And he had come to Him , and He was like a fire , and when He was raised , He was raised up , and the fire of the sea , and the sea of the sea , and the sea of the sea .\n",
      "2021-07-18 17:47:47,131 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    40000: bleu:   5.96, loss: 93181.2969, ppl:  14.2936, duration: 29.2855s\n",
      "2021-07-18 17:48:01,390 - INFO - joeynmt.training - Epoch   8, Step:    40100, Batch Loss:     2.733800, Tokens per Sec:    16400, Lr: 0.000300\n",
      "2021-07-18 17:48:14,948 - INFO - joeynmt.training - Epoch   8, Step:    40200, Batch Loss:     2.678736, Tokens per Sec:    16571, Lr: 0.000300\n",
      "2021-07-18 17:48:28,561 - INFO - joeynmt.training - Epoch   8, Step:    40300, Batch Loss:     2.639434, Tokens per Sec:    16516, Lr: 0.000300\n",
      "2021-07-18 17:48:42,358 - INFO - joeynmt.training - Epoch   8, Step:    40400, Batch Loss:     2.326212, Tokens per Sec:    16424, Lr: 0.000300\n",
      "2021-07-18 17:48:56,120 - INFO - joeynmt.training - Epoch   8, Step:    40500, Batch Loss:     2.771110, Tokens per Sec:    16498, Lr: 0.000300\n",
      "2021-07-18 17:49:09,896 - INFO - joeynmt.training - Epoch   8, Step:    40600, Batch Loss:     2.640183, Tokens per Sec:    16619, Lr: 0.000300\n",
      "2021-07-18 17:49:23,573 - INFO - joeynmt.training - Epoch   8, Step:    40700, Batch Loss:     2.594287, Tokens per Sec:    16418, Lr: 0.000300\n",
      "2021-07-18 17:49:37,203 - INFO - joeynmt.training - Epoch   8, Step:    40800, Batch Loss:     2.432509, Tokens per Sec:    16766, Lr: 0.000300\n",
      "2021-07-18 17:49:50,886 - INFO - joeynmt.training - Epoch   8, Step:    40900, Batch Loss:     2.735499, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-07-18 17:50:04,575 - INFO - joeynmt.training - Epoch   8, Step:    41000, Batch Loss:     2.402596, Tokens per Sec:    16349, Lr: 0.000300\n",
      "2021-07-18 17:50:18,285 - INFO - joeynmt.training - Epoch   8, Step:    41100, Batch Loss:     2.736159, Tokens per Sec:    16527, Lr: 0.000300\n",
      "2021-07-18 17:50:31,925 - INFO - joeynmt.training - Epoch   8, Step:    41200, Batch Loss:     2.771257, Tokens per Sec:    16551, Lr: 0.000300\n",
      "2021-07-18 17:50:45,497 - INFO - joeynmt.training - Epoch   8, Step:    41300, Batch Loss:     2.426330, Tokens per Sec:    16533, Lr: 0.000300\n",
      "2021-07-18 17:50:59,196 - INFO - joeynmt.training - Epoch   8, Step:    41400, Batch Loss:     2.645169, Tokens per Sec:    16618, Lr: 0.000300\n",
      "2021-07-18 17:51:12,897 - INFO - joeynmt.training - Epoch   8, Step:    41500, Batch Loss:     2.804753, Tokens per Sec:    16455, Lr: 0.000300\n",
      "2021-07-18 17:51:26,697 - INFO - joeynmt.training - Epoch   8, Step:    41600, Batch Loss:     2.717630, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-07-18 17:51:40,407 - INFO - joeynmt.training - Epoch   8, Step:    41700, Batch Loss:     2.764007, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-07-18 17:51:53,918 - INFO - joeynmt.training - Epoch   8, Step:    41800, Batch Loss:     2.286472, Tokens per Sec:    16385, Lr: 0.000300\n",
      "2021-07-18 17:51:57,746 - INFO - joeynmt.training - Epoch   8: total training loss 6125.30\n",
      "2021-07-18 17:51:57,746 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-18 17:52:08,039 - INFO - joeynmt.training - Epoch   9, Step:    41900, Batch Loss:     2.705353, Tokens per Sec:    16249, Lr: 0.000300\n",
      "2021-07-18 17:52:21,691 - INFO - joeynmt.training - Epoch   9, Step:    42000, Batch Loss:     2.455576, Tokens per Sec:    16395, Lr: 0.000300\n",
      "2021-07-18 17:52:35,285 - INFO - joeynmt.training - Epoch   9, Step:    42100, Batch Loss:     2.451493, Tokens per Sec:    16526, Lr: 0.000300\n",
      "2021-07-18 17:52:48,819 - INFO - joeynmt.training - Epoch   9, Step:    42200, Batch Loss:     2.263031, Tokens per Sec:    16391, Lr: 0.000300\n",
      "2021-07-18 17:53:02,484 - INFO - joeynmt.training - Epoch   9, Step:    42300, Batch Loss:     2.000736, Tokens per Sec:    16409, Lr: 0.000300\n",
      "2021-07-18 17:53:16,192 - INFO - joeynmt.training - Epoch   9, Step:    42400, Batch Loss:     2.622065, Tokens per Sec:    16529, Lr: 0.000300\n",
      "2021-07-18 17:53:29,787 - INFO - joeynmt.training - Epoch   9, Step:    42500, Batch Loss:     2.774410, Tokens per Sec:    16408, Lr: 0.000300\n",
      "2021-07-18 17:53:57,273 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 17:53:57,273 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 17:53:57,273 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 17:53:57,651 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 17:53:57,652 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 17:53:58,480 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 17:53:58,483 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 17:53:58,483 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 17:53:58,483 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to Him , “ He who has been given me , and He who has given me a great multitude , and I have come to you . ”\n",
      "2021-07-18 17:53:58,484 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 17:53:58,484 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 17:53:58,484 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 17:53:58,484 - INFO - joeynmt.training - \tHypothesis: When Martha came to Him , He went to Mary , and said to Him , “ Mary , and He said to Him , ‘ Mary , and you are the Lord . ”\n",
      "2021-07-18 17:53:58,485 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 17:53:58,485 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 17:53:58,485 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 17:53:58,486 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the end .\n",
      "2021-07-18 17:53:58,486 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 17:53:58,486 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 17:53:58,486 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 17:53:58,487 - INFO - joeynmt.training - \tHypothesis: And when he had been given , He was a sound of fire , and when He was raised , He was raised up , and the sea of the sea , and the sea of the sea .\n",
      "2021-07-18 17:53:58,487 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    42500: bleu:   5.93, loss: 92938.8906, ppl:  14.1951, duration: 28.6991s\n",
      "2021-07-18 17:54:12,847 - INFO - joeynmt.training - Epoch   9, Step:    42600, Batch Loss:     2.598190, Tokens per Sec:    16020, Lr: 0.000300\n",
      "2021-07-18 17:54:26,558 - INFO - joeynmt.training - Epoch   9, Step:    42700, Batch Loss:     2.579519, Tokens per Sec:    16588, Lr: 0.000300\n",
      "2021-07-18 17:54:40,300 - INFO - joeynmt.training - Epoch   9, Step:    42800, Batch Loss:     2.663406, Tokens per Sec:    16647, Lr: 0.000300\n",
      "2021-07-18 17:54:54,218 - INFO - joeynmt.training - Epoch   9, Step:    42900, Batch Loss:     2.470744, Tokens per Sec:    16686, Lr: 0.000300\n",
      "2021-07-18 17:55:08,005 - INFO - joeynmt.training - Epoch   9, Step:    43000, Batch Loss:     2.596208, Tokens per Sec:    16635, Lr: 0.000300\n",
      "2021-07-18 17:55:21,945 - INFO - joeynmt.training - Epoch   9, Step:    43100, Batch Loss:     2.900149, Tokens per Sec:    16692, Lr: 0.000300\n",
      "2021-07-18 17:55:35,736 - INFO - joeynmt.training - Epoch   9, Step:    43200, Batch Loss:     2.752972, Tokens per Sec:    16462, Lr: 0.000300\n",
      "2021-07-18 17:55:49,343 - INFO - joeynmt.training - Epoch   9, Step:    43300, Batch Loss:     2.649529, Tokens per Sec:    16228, Lr: 0.000300\n",
      "2021-07-18 17:56:03,221 - INFO - joeynmt.training - Epoch   9, Step:    43400, Batch Loss:     2.572042, Tokens per Sec:    16429, Lr: 0.000300\n",
      "2021-07-18 17:56:17,009 - INFO - joeynmt.training - Epoch   9, Step:    43500, Batch Loss:     2.678832, Tokens per Sec:    16286, Lr: 0.000300\n",
      "2021-07-18 17:56:30,726 - INFO - joeynmt.training - Epoch   9, Step:    43600, Batch Loss:     2.894085, Tokens per Sec:    16257, Lr: 0.000300\n",
      "2021-07-18 17:56:44,608 - INFO - joeynmt.training - Epoch   9, Step:    43700, Batch Loss:     2.661068, Tokens per Sec:    16577, Lr: 0.000300\n",
      "2021-07-18 17:56:58,336 - INFO - joeynmt.training - Epoch   9, Step:    43800, Batch Loss:     2.676608, Tokens per Sec:    16785, Lr: 0.000300\n",
      "2021-07-18 17:57:12,171 - INFO - joeynmt.training - Epoch   9, Step:    43900, Batch Loss:     2.531440, Tokens per Sec:    16454, Lr: 0.000300\n",
      "2021-07-18 17:57:25,862 - INFO - joeynmt.training - Epoch   9, Step:    44000, Batch Loss:     2.678597, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 17:57:39,535 - INFO - joeynmt.training - Epoch   9, Step:    44100, Batch Loss:     2.498877, Tokens per Sec:    16445, Lr: 0.000300\n",
      "2021-07-18 17:57:46,978 - INFO - joeynmt.training - Epoch   9: total training loss 6082.81\n",
      "2021-07-18 17:57:46,979 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-18 17:57:53,632 - INFO - joeynmt.training - Epoch  10, Step:    44200, Batch Loss:     2.778127, Tokens per Sec:    15654, Lr: 0.000300\n",
      "2021-07-18 17:58:07,352 - INFO - joeynmt.training - Epoch  10, Step:    44300, Batch Loss:     2.623645, Tokens per Sec:    16479, Lr: 0.000300\n",
      "2021-07-18 17:58:21,111 - INFO - joeynmt.training - Epoch  10, Step:    44400, Batch Loss:     2.688815, Tokens per Sec:    16555, Lr: 0.000300\n",
      "2021-07-18 17:58:34,749 - INFO - joeynmt.training - Epoch  10, Step:    44500, Batch Loss:     2.629601, Tokens per Sec:    16432, Lr: 0.000300\n",
      "2021-07-18 17:58:48,488 - INFO - joeynmt.training - Epoch  10, Step:    44600, Batch Loss:     2.822423, Tokens per Sec:    16518, Lr: 0.000300\n",
      "2021-07-18 17:59:02,280 - INFO - joeynmt.training - Epoch  10, Step:    44700, Batch Loss:     2.502395, Tokens per Sec:    16694, Lr: 0.000300\n",
      "2021-07-18 17:59:15,966 - INFO - joeynmt.training - Epoch  10, Step:    44800, Batch Loss:     2.589581, Tokens per Sec:    16589, Lr: 0.000300\n",
      "2021-07-18 17:59:29,652 - INFO - joeynmt.training - Epoch  10, Step:    44900, Batch Loss:     2.362010, Tokens per Sec:    16554, Lr: 0.000300\n",
      "2021-07-18 17:59:43,394 - INFO - joeynmt.training - Epoch  10, Step:    45000, Batch Loss:     2.784768, Tokens per Sec:    16428, Lr: 0.000300\n",
      "2021-07-18 18:00:13,164 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:00:13,164 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:00:13,164 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:00:13,537 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 18:00:13,538 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 18:00:14,284 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:00:14,286 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:00:14,287 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:00:14,287 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw Him as a man , and said , “ He who is in the midst , and said to them , “ I am the Sabbath , and I am going to you . ”\n",
      "2021-07-18 18:00:14,287 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:00:14,287 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:00:14,288 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:00:14,288 - INFO - joeynmt.training - \tHypothesis: Now when she was born , Mary came to Mary , and said to Mary , “ Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary . ”\n",
      "2021-07-18 18:00:14,288 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:00:14,289 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:00:14,289 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:00:14,289 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day .\n",
      "2021-07-18 18:00:14,289 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:00:14,290 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:00:14,290 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:00:14,291 - INFO - joeynmt.training - \tHypothesis: And Paul was with them , and when he was raised , and when he was raised up , he was raised up , and the fire of the sea , and the sea of the sea .\n",
      "2021-07-18 18:00:14,291 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    45000: bleu:   6.21, loss: 92130.0391, ppl:  13.8711, duration: 30.8963s\n",
      "2021-07-18 18:00:28,110 - INFO - joeynmt.training - Epoch  10, Step:    45100, Batch Loss:     2.500345, Tokens per Sec:    16221, Lr: 0.000300\n",
      "2021-07-18 18:00:41,831 - INFO - joeynmt.training - Epoch  10, Step:    45200, Batch Loss:     2.757017, Tokens per Sec:    16488, Lr: 0.000300\n",
      "2021-07-18 18:00:55,469 - INFO - joeynmt.training - Epoch  10, Step:    45300, Batch Loss:     2.624921, Tokens per Sec:    16549, Lr: 0.000300\n",
      "2021-07-18 18:01:09,276 - INFO - joeynmt.training - Epoch  10, Step:    45400, Batch Loss:     2.784568, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-18 18:01:23,209 - INFO - joeynmt.training - Epoch  10, Step:    45500, Batch Loss:     2.923378, Tokens per Sec:    16632, Lr: 0.000300\n",
      "2021-07-18 18:01:36,982 - INFO - joeynmt.training - Epoch  10, Step:    45600, Batch Loss:     2.686592, Tokens per Sec:    16626, Lr: 0.000300\n",
      "2021-07-18 18:01:50,734 - INFO - joeynmt.training - Epoch  10, Step:    45700, Batch Loss:     2.440629, Tokens per Sec:    16647, Lr: 0.000300\n",
      "2021-07-18 18:02:04,524 - INFO - joeynmt.training - Epoch  10, Step:    45800, Batch Loss:     2.938115, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-18 18:02:18,400 - INFO - joeynmt.training - Epoch  10, Step:    45900, Batch Loss:     2.651367, Tokens per Sec:    16471, Lr: 0.000300\n",
      "2021-07-18 18:02:32,028 - INFO - joeynmt.training - Epoch  10, Step:    46000, Batch Loss:     2.271327, Tokens per Sec:    16716, Lr: 0.000300\n",
      "2021-07-18 18:02:45,701 - INFO - joeynmt.training - Epoch  10, Step:    46100, Batch Loss:     2.513967, Tokens per Sec:    16585, Lr: 0.000300\n",
      "2021-07-18 18:02:59,341 - INFO - joeynmt.training - Epoch  10, Step:    46200, Batch Loss:     2.330808, Tokens per Sec:    16126, Lr: 0.000300\n",
      "2021-07-18 18:03:13,088 - INFO - joeynmt.training - Epoch  10, Step:    46300, Batch Loss:     2.803338, Tokens per Sec:    16492, Lr: 0.000300\n",
      "2021-07-18 18:03:26,794 - INFO - joeynmt.training - Epoch  10, Step:    46400, Batch Loss:     2.645252, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-18 18:03:37,943 - INFO - joeynmt.training - Epoch  10: total training loss 6065.30\n",
      "2021-07-18 18:03:37,944 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-18 18:03:40,908 - INFO - joeynmt.training - Epoch  11, Step:    46500, Batch Loss:     2.795692, Tokens per Sec:    15231, Lr: 0.000300\n",
      "2021-07-18 18:03:54,437 - INFO - joeynmt.training - Epoch  11, Step:    46600, Batch Loss:     2.435587, Tokens per Sec:    16203, Lr: 0.000300\n",
      "2021-07-18 18:04:08,218 - INFO - joeynmt.training - Epoch  11, Step:    46700, Batch Loss:     2.482305, Tokens per Sec:    16780, Lr: 0.000300\n",
      "2021-07-18 18:04:22,044 - INFO - joeynmt.training - Epoch  11, Step:    46800, Batch Loss:     2.458282, Tokens per Sec:    16574, Lr: 0.000300\n",
      "2021-07-18 18:04:35,754 - INFO - joeynmt.training - Epoch  11, Step:    46900, Batch Loss:     2.575109, Tokens per Sec:    16683, Lr: 0.000300\n",
      "2021-07-18 18:04:49,589 - INFO - joeynmt.training - Epoch  11, Step:    47000, Batch Loss:     2.745617, Tokens per Sec:    16414, Lr: 0.000300\n",
      "2021-07-18 18:05:03,299 - INFO - joeynmt.training - Epoch  11, Step:    47100, Batch Loss:     2.588981, Tokens per Sec:    16642, Lr: 0.000300\n",
      "2021-07-18 18:05:17,004 - INFO - joeynmt.training - Epoch  11, Step:    47200, Batch Loss:     2.781968, Tokens per Sec:    16505, Lr: 0.000300\n",
      "2021-07-18 18:05:30,702 - INFO - joeynmt.training - Epoch  11, Step:    47300, Batch Loss:     2.501107, Tokens per Sec:    16266, Lr: 0.000300\n",
      "2021-07-18 18:05:44,493 - INFO - joeynmt.training - Epoch  11, Step:    47400, Batch Loss:     2.411270, Tokens per Sec:    16476, Lr: 0.000300\n",
      "2021-07-18 18:05:58,367 - INFO - joeynmt.training - Epoch  11, Step:    47500, Batch Loss:     2.392524, Tokens per Sec:    16636, Lr: 0.000300\n",
      "2021-07-18 18:06:24,982 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:06:24,983 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:06:24,983 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:06:26,463 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:06:26,464 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:06:26,464 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:06:26,464 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and said to them , “ He who is in the midst of the Pharisees , and said to them , “ I have come to you . ”\n",
      "2021-07-18 18:06:26,464 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - \tHypothesis: And when she had come to Him , He went to Mary , and said to Mary , “ Mary , and the mother of Mary , and He said to Him , “ Teacher , and see Him . ”\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:06:26,465 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - \tHypothesis: He was a long time for the day of the day of the day .\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:06:26,466 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:06:26,467 - INFO - joeynmt.training - \tHypothesis: And when he had come to Him , He saw the fire , and when He was raised , He was raised up , and the fire of the head , and the head of the head of the head .\n",
      "2021-07-18 18:06:26,467 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    47500: bleu:   6.14, loss: 92418.3906, ppl:  13.9857, duration: 28.0990s\n",
      "2021-07-18 18:06:40,349 - INFO - joeynmt.training - Epoch  11, Step:    47600, Batch Loss:     2.727108, Tokens per Sec:    16380, Lr: 0.000300\n",
      "2021-07-18 18:06:54,041 - INFO - joeynmt.training - Epoch  11, Step:    47700, Batch Loss:     2.617947, Tokens per Sec:    16725, Lr: 0.000300\n",
      "2021-07-18 18:07:07,677 - INFO - joeynmt.training - Epoch  11, Step:    47800, Batch Loss:     2.658020, Tokens per Sec:    16125, Lr: 0.000300\n",
      "2021-07-18 18:07:21,498 - INFO - joeynmt.training - Epoch  11, Step:    47900, Batch Loss:     2.474666, Tokens per Sec:    16607, Lr: 0.000300\n",
      "2021-07-18 18:07:35,086 - INFO - joeynmt.training - Epoch  11, Step:    48000, Batch Loss:     2.112014, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-07-18 18:07:48,764 - INFO - joeynmt.training - Epoch  11, Step:    48100, Batch Loss:     2.537533, Tokens per Sec:    16807, Lr: 0.000300\n",
      "2021-07-18 18:08:02,504 - INFO - joeynmt.training - Epoch  11, Step:    48200, Batch Loss:     2.678911, Tokens per Sec:    16736, Lr: 0.000300\n",
      "2021-07-18 18:08:16,317 - INFO - joeynmt.training - Epoch  11, Step:    48300, Batch Loss:     2.605086, Tokens per Sec:    16691, Lr: 0.000300\n",
      "2021-07-18 18:08:30,041 - INFO - joeynmt.training - Epoch  11, Step:    48400, Batch Loss:     2.792831, Tokens per Sec:    16428, Lr: 0.000300\n",
      "2021-07-18 18:08:43,826 - INFO - joeynmt.training - Epoch  11, Step:    48500, Batch Loss:     2.638405, Tokens per Sec:    16263, Lr: 0.000300\n",
      "2021-07-18 18:08:57,612 - INFO - joeynmt.training - Epoch  11, Step:    48600, Batch Loss:     2.449753, Tokens per Sec:    16547, Lr: 0.000300\n",
      "2021-07-18 18:09:11,403 - INFO - joeynmt.training - Epoch  11, Step:    48700, Batch Loss:     2.969850, Tokens per Sec:    16638, Lr: 0.000300\n",
      "2021-07-18 18:09:25,296 - INFO - joeynmt.training - Epoch  11, Step:    48800, Batch Loss:     2.622201, Tokens per Sec:    16548, Lr: 0.000300\n",
      "2021-07-18 18:09:25,705 - INFO - joeynmt.training - Epoch  11: total training loss 6028.67\n",
      "2021-07-18 18:09:25,705 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-18 18:09:39,400 - INFO - joeynmt.training - Epoch  12, Step:    48900, Batch Loss:     2.785305, Tokens per Sec:    16088, Lr: 0.000300\n",
      "2021-07-18 18:09:53,057 - INFO - joeynmt.training - Epoch  12, Step:    49000, Batch Loss:     2.599651, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-07-18 18:10:06,913 - INFO - joeynmt.training - Epoch  12, Step:    49100, Batch Loss:     2.622269, Tokens per Sec:    16371, Lr: 0.000300\n",
      "2021-07-18 18:10:20,582 - INFO - joeynmt.training - Epoch  12, Step:    49200, Batch Loss:     2.717268, Tokens per Sec:    16535, Lr: 0.000300\n",
      "2021-07-18 18:10:34,181 - INFO - joeynmt.training - Epoch  12, Step:    49300, Batch Loss:     2.590398, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-07-18 18:10:47,978 - INFO - joeynmt.training - Epoch  12, Step:    49400, Batch Loss:     2.324870, Tokens per Sec:    16926, Lr: 0.000300\n",
      "2021-07-18 18:11:01,793 - INFO - joeynmt.training - Epoch  12, Step:    49500, Batch Loss:     2.676963, Tokens per Sec:    16470, Lr: 0.000300\n",
      "2021-07-18 18:11:15,634 - INFO - joeynmt.training - Epoch  12, Step:    49600, Batch Loss:     2.752771, Tokens per Sec:    16606, Lr: 0.000300\n",
      "2021-07-18 18:11:29,549 - INFO - joeynmt.training - Epoch  12, Step:    49700, Batch Loss:     2.839061, Tokens per Sec:    16448, Lr: 0.000300\n",
      "2021-07-18 18:11:43,108 - INFO - joeynmt.training - Epoch  12, Step:    49800, Batch Loss:     2.551483, Tokens per Sec:    16272, Lr: 0.000300\n",
      "2021-07-18 18:11:56,875 - INFO - joeynmt.training - Epoch  12, Step:    49900, Batch Loss:     2.566016, Tokens per Sec:    16658, Lr: 0.000300\n",
      "2021-07-18 18:12:10,665 - INFO - joeynmt.training - Epoch  12, Step:    50000, Batch Loss:     2.676202, Tokens per Sec:    16484, Lr: 0.000300\n",
      "2021-07-18 18:12:37,195 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:12:37,195 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:12:37,196 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:12:37,570 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 18:12:37,570 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 18:12:38,376 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:12:38,376 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:12:38,377 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:12:38,377 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees said to them , “ He saw the seven sheep , and said to them , “ Where I have come , I have come to you , and I have come to you . ”\n",
      "2021-07-18 18:12:38,377 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:12:38,378 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:12:38,378 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:12:38,378 - INFO - joeynmt.training - \tHypothesis: And when Martha was at the same time , Mary and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary .\n",
      "2021-07-18 18:12:38,378 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:12:38,379 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:12:38,379 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:12:38,379 - INFO - joeynmt.training - \tHypothesis: He was a long time for the day of the day of the end .\n",
      "2021-07-18 18:12:38,379 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:12:38,380 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:12:38,380 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:12:38,380 - INFO - joeynmt.training - \tHypothesis: And Paul also spoke to Him , and when He was sold , He saw the body of the body , and the body of the body of the body , and the sound of the sea .\n",
      "2021-07-18 18:12:38,380 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    50000: bleu:   6.30, loss: 91502.1641, ppl:  13.6247, duration: 27.7151s\n",
      "2021-07-18 18:12:52,435 - INFO - joeynmt.training - Epoch  12, Step:    50100, Batch Loss:     2.770433, Tokens per Sec:    16105, Lr: 0.000300\n",
      "2021-07-18 18:13:06,023 - INFO - joeynmt.training - Epoch  12, Step:    50200, Batch Loss:     2.662262, Tokens per Sec:    16461, Lr: 0.000300\n",
      "2021-07-18 18:13:19,954 - INFO - joeynmt.training - Epoch  12, Step:    50300, Batch Loss:     2.737151, Tokens per Sec:    16670, Lr: 0.000300\n",
      "2021-07-18 18:13:33,714 - INFO - joeynmt.training - Epoch  12, Step:    50400, Batch Loss:     2.800951, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-18 18:13:47,511 - INFO - joeynmt.training - Epoch  12, Step:    50500, Batch Loss:     2.855441, Tokens per Sec:    16659, Lr: 0.000300\n",
      "2021-07-18 18:14:01,407 - INFO - joeynmt.training - Epoch  12, Step:    50600, Batch Loss:     2.653716, Tokens per Sec:    16698, Lr: 0.000300\n",
      "2021-07-18 18:14:15,175 - INFO - joeynmt.training - Epoch  12, Step:    50700, Batch Loss:     2.648063, Tokens per Sec:    16399, Lr: 0.000300\n",
      "2021-07-18 18:14:29,126 - INFO - joeynmt.training - Epoch  12, Step:    50800, Batch Loss:     2.561216, Tokens per Sec:    16720, Lr: 0.000300\n",
      "2021-07-18 18:14:42,797 - INFO - joeynmt.training - Epoch  12, Step:    50900, Batch Loss:     2.790103, Tokens per Sec:    16533, Lr: 0.000300\n",
      "2021-07-18 18:14:56,551 - INFO - joeynmt.training - Epoch  12, Step:    51000, Batch Loss:     2.840385, Tokens per Sec:    16522, Lr: 0.000300\n",
      "2021-07-18 18:15:10,299 - INFO - joeynmt.training - Epoch  12, Step:    51100, Batch Loss:     2.875169, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-18 18:15:13,079 - INFO - joeynmt.training - Epoch  12: total training loss 5989.42\n",
      "2021-07-18 18:15:13,080 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-18 18:15:24,416 - INFO - joeynmt.training - Epoch  13, Step:    51200, Batch Loss:     2.612722, Tokens per Sec:    16093, Lr: 0.000300\n",
      "2021-07-18 18:15:38,033 - INFO - joeynmt.training - Epoch  13, Step:    51300, Batch Loss:     2.575217, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-07-18 18:15:51,879 - INFO - joeynmt.training - Epoch  13, Step:    51400, Batch Loss:     2.389009, Tokens per Sec:    16655, Lr: 0.000300\n",
      "2021-07-18 18:16:05,513 - INFO - joeynmt.training - Epoch  13, Step:    51500, Batch Loss:     2.557447, Tokens per Sec:    16423, Lr: 0.000300\n",
      "2021-07-18 18:16:19,218 - INFO - joeynmt.training - Epoch  13, Step:    51600, Batch Loss:     2.729674, Tokens per Sec:    16354, Lr: 0.000300\n",
      "2021-07-18 18:16:32,824 - INFO - joeynmt.training - Epoch  13, Step:    51700, Batch Loss:     2.382482, Tokens per Sec:    16625, Lr: 0.000300\n",
      "2021-07-18 18:16:46,597 - INFO - joeynmt.training - Epoch  13, Step:    51800, Batch Loss:     2.635111, Tokens per Sec:    16377, Lr: 0.000300\n",
      "2021-07-18 18:17:00,346 - INFO - joeynmt.training - Epoch  13, Step:    51900, Batch Loss:     2.623150, Tokens per Sec:    16696, Lr: 0.000300\n",
      "2021-07-18 18:17:14,113 - INFO - joeynmt.training - Epoch  13, Step:    52000, Batch Loss:     2.507889, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-07-18 18:17:27,843 - INFO - joeynmt.training - Epoch  13, Step:    52100, Batch Loss:     2.554207, Tokens per Sec:    16555, Lr: 0.000300\n",
      "2021-07-18 18:17:41,536 - INFO - joeynmt.training - Epoch  13, Step:    52200, Batch Loss:     2.675241, Tokens per Sec:    16485, Lr: 0.000300\n",
      "2021-07-18 18:17:55,314 - INFO - joeynmt.training - Epoch  13, Step:    52300, Batch Loss:     2.504127, Tokens per Sec:    16543, Lr: 0.000300\n",
      "2021-07-18 18:18:09,044 - INFO - joeynmt.training - Epoch  13, Step:    52400, Batch Loss:     2.679950, Tokens per Sec:    16432, Lr: 0.000300\n",
      "2021-07-18 18:18:22,816 - INFO - joeynmt.training - Epoch  13, Step:    52500, Batch Loss:     2.532991, Tokens per Sec:    16343, Lr: 0.000300\n",
      "2021-07-18 18:18:50,500 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:18:50,500 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:18:50,501 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:18:51,986 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:18:51,986 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:18:51,986 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and said to them , “ He saw the sword , and said to them , “ Look ! And I have come to you , and I have come to you . ”\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:18:51,987 - INFO - joeynmt.training - \tHypothesis: And when Martha came to Mary , Mary and Mary and Mary , and Mary , and Mary , and Mary , “ came to Him , and the Lord was coming . ”\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - \tHypothesis: He was a man in the day of the day of the day .\n",
      "2021-07-18 18:18:51,988 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:18:51,989 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:18:51,989 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:18:51,989 - INFO - joeynmt.training - \tHypothesis: Then he took the steps of the sea , and when He was sleeping , He was sleeping , and the sea of the sea , and the sea of the sea , and the sea of the sea .\n",
      "2021-07-18 18:18:51,989 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    52500: bleu:   6.20, loss: 91796.3359, ppl:  13.7396, duration: 29.1727s\n",
      "2021-07-18 18:19:05,971 - INFO - joeynmt.training - Epoch  13, Step:    52600, Batch Loss:     2.784657, Tokens per Sec:    16103, Lr: 0.000300\n",
      "2021-07-18 18:19:19,778 - INFO - joeynmt.training - Epoch  13, Step:    52700, Batch Loss:     2.486948, Tokens per Sec:    16558, Lr: 0.000300\n",
      "2021-07-18 18:19:33,421 - INFO - joeynmt.training - Epoch  13, Step:    52800, Batch Loss:     2.356462, Tokens per Sec:    16632, Lr: 0.000300\n",
      "2021-07-18 18:19:47,367 - INFO - joeynmt.training - Epoch  13, Step:    52900, Batch Loss:     2.326673, Tokens per Sec:    16729, Lr: 0.000300\n",
      "2021-07-18 18:20:01,005 - INFO - joeynmt.training - Epoch  13, Step:    53000, Batch Loss:     2.559882, Tokens per Sec:    16575, Lr: 0.000300\n",
      "2021-07-18 18:20:14,771 - INFO - joeynmt.training - Epoch  13, Step:    53100, Batch Loss:     2.751396, Tokens per Sec:    16548, Lr: 0.000300\n",
      "2021-07-18 18:20:28,656 - INFO - joeynmt.training - Epoch  13, Step:    53200, Batch Loss:     2.517473, Tokens per Sec:    16715, Lr: 0.000300\n",
      "2021-07-18 18:20:42,265 - INFO - joeynmt.training - Epoch  13, Step:    53300, Batch Loss:     2.563397, Tokens per Sec:    16545, Lr: 0.000300\n",
      "2021-07-18 18:20:56,173 - INFO - joeynmt.training - Epoch  13, Step:    53400, Batch Loss:     2.775694, Tokens per Sec:    16373, Lr: 0.000300\n",
      "2021-07-18 18:21:02,274 - INFO - joeynmt.training - Epoch  13: total training loss 5991.60\n",
      "2021-07-18 18:21:02,275 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-18 18:21:10,237 - INFO - joeynmt.training - Epoch  14, Step:    53500, Batch Loss:     2.422273, Tokens per Sec:    16060, Lr: 0.000300\n",
      "2021-07-18 18:21:23,854 - INFO - joeynmt.training - Epoch  14, Step:    53600, Batch Loss:     2.488612, Tokens per Sec:    16565, Lr: 0.000300\n",
      "2021-07-18 18:21:37,572 - INFO - joeynmt.training - Epoch  14, Step:    53700, Batch Loss:     2.782307, Tokens per Sec:    16645, Lr: 0.000300\n",
      "2021-07-18 18:21:51,206 - INFO - joeynmt.training - Epoch  14, Step:    53800, Batch Loss:     2.585469, Tokens per Sec:    16400, Lr: 0.000300\n",
      "2021-07-18 18:22:04,915 - INFO - joeynmt.training - Epoch  14, Step:    53900, Batch Loss:     2.577602, Tokens per Sec:    16351, Lr: 0.000300\n",
      "2021-07-18 18:22:18,554 - INFO - joeynmt.training - Epoch  14, Step:    54000, Batch Loss:     2.635331, Tokens per Sec:    16407, Lr: 0.000300\n",
      "2021-07-18 18:22:32,129 - INFO - joeynmt.training - Epoch  14, Step:    54100, Batch Loss:     2.397973, Tokens per Sec:    16513, Lr: 0.000300\n",
      "2021-07-18 18:22:45,809 - INFO - joeynmt.training - Epoch  14, Step:    54200, Batch Loss:     2.540159, Tokens per Sec:    16557, Lr: 0.000300\n",
      "2021-07-18 18:22:59,667 - INFO - joeynmt.training - Epoch  14, Step:    54300, Batch Loss:     2.418574, Tokens per Sec:    16789, Lr: 0.000300\n",
      "2021-07-18 18:23:13,511 - INFO - joeynmt.training - Epoch  14, Step:    54400, Batch Loss:     2.798739, Tokens per Sec:    16430, Lr: 0.000300\n",
      "2021-07-18 18:23:27,284 - INFO - joeynmt.training - Epoch  14, Step:    54500, Batch Loss:     2.575266, Tokens per Sec:    16448, Lr: 0.000300\n",
      "2021-07-18 18:23:40,927 - INFO - joeynmt.training - Epoch  14, Step:    54600, Batch Loss:     2.630668, Tokens per Sec:    16328, Lr: 0.000300\n",
      "2021-07-18 18:23:54,542 - INFO - joeynmt.training - Epoch  14, Step:    54700, Batch Loss:     2.598145, Tokens per Sec:    16710, Lr: 0.000300\n",
      "2021-07-18 18:24:08,255 - INFO - joeynmt.training - Epoch  14, Step:    54800, Batch Loss:     2.642677, Tokens per Sec:    16531, Lr: 0.000300\n",
      "2021-07-18 18:24:22,092 - INFO - joeynmt.training - Epoch  14, Step:    54900, Batch Loss:     2.535192, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-18 18:24:35,685 - INFO - joeynmt.training - Epoch  14, Step:    55000, Batch Loss:     2.277778, Tokens per Sec:    16481, Lr: 0.000300\n",
      "2021-07-18 18:25:06,135 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:25:06,136 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:25:06,136 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:25:06,524 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-18 18:25:06,524 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-18 18:25:07,368 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:25:07,371 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:25:07,371 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:25:07,371 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He who is a man , and he who is in the midst of the seven , and he who is in the midst of the seven times . ”\n",
      "2021-07-18 18:25:07,371 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:25:07,372 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:25:07,372 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:25:07,372 - INFO - joeynmt.training - \tHypothesis: And when Martha was a great crowd , Mary and Mary , Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary .\n",
      "2021-07-18 18:25:07,372 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:25:07,373 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:25:07,373 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:25:07,373 - INFO - joeynmt.training - \tHypothesis: He was a long time for the day of the day of the day of the day .\n",
      "2021-07-18 18:25:07,373 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:25:07,374 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:25:07,374 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:25:07,374 - INFO - joeynmt.training - \tHypothesis: And Paul had spoken , and when He was in the midst of the sea , He was sat down and sat down , and the sea of the sea , and the sea of the sea .\n",
      "2021-07-18 18:25:07,374 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    55000: bleu:   6.39, loss: 90852.1328, ppl:  13.3742, duration: 31.6888s\n",
      "2021-07-18 18:25:21,316 - INFO - joeynmt.training - Epoch  14, Step:    55100, Batch Loss:     2.543422, Tokens per Sec:    16469, Lr: 0.000300\n",
      "2021-07-18 18:25:35,009 - INFO - joeynmt.training - Epoch  14, Step:    55200, Batch Loss:     2.774153, Tokens per Sec:    16491, Lr: 0.000300\n",
      "2021-07-18 18:25:48,822 - INFO - joeynmt.training - Epoch  14, Step:    55300, Batch Loss:     2.810834, Tokens per Sec:    16460, Lr: 0.000300\n",
      "2021-07-18 18:26:02,509 - INFO - joeynmt.training - Epoch  14, Step:    55400, Batch Loss:     2.676061, Tokens per Sec:    16415, Lr: 0.000300\n",
      "2021-07-18 18:26:16,348 - INFO - joeynmt.training - Epoch  14, Step:    55500, Batch Loss:     2.611952, Tokens per Sec:    16608, Lr: 0.000300\n",
      "2021-07-18 18:26:30,127 - INFO - joeynmt.training - Epoch  14, Step:    55600, Batch Loss:     2.501096, Tokens per Sec:    16528, Lr: 0.000300\n",
      "2021-07-18 18:26:43,883 - INFO - joeynmt.training - Epoch  14, Step:    55700, Batch Loss:     2.471441, Tokens per Sec:    16522, Lr: 0.000300\n",
      "2021-07-18 18:26:53,778 - INFO - joeynmt.training - Epoch  14: total training loss 5984.50\n",
      "2021-07-18 18:26:53,779 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-18 18:26:57,930 - INFO - joeynmt.training - Epoch  15, Step:    55800, Batch Loss:     2.687990, Tokens per Sec:    14945, Lr: 0.000300\n",
      "2021-07-18 18:27:11,652 - INFO - joeynmt.training - Epoch  15, Step:    55900, Batch Loss:     2.660452, Tokens per Sec:    16895, Lr: 0.000300\n",
      "2021-07-18 18:27:25,211 - INFO - joeynmt.training - Epoch  15, Step:    56000, Batch Loss:     2.649322, Tokens per Sec:    16414, Lr: 0.000300\n",
      "2021-07-18 18:27:38,893 - INFO - joeynmt.training - Epoch  15, Step:    56100, Batch Loss:     2.627172, Tokens per Sec:    16547, Lr: 0.000300\n",
      "2021-07-18 18:27:52,629 - INFO - joeynmt.training - Epoch  15, Step:    56200, Batch Loss:     2.576288, Tokens per Sec:    16640, Lr: 0.000300\n",
      "2021-07-18 18:28:06,464 - INFO - joeynmt.training - Epoch  15, Step:    56300, Batch Loss:     2.822463, Tokens per Sec:    16392, Lr: 0.000300\n",
      "2021-07-18 18:28:20,062 - INFO - joeynmt.training - Epoch  15, Step:    56400, Batch Loss:     2.715123, Tokens per Sec:    16570, Lr: 0.000300\n",
      "2021-07-18 18:28:33,716 - INFO - joeynmt.training - Epoch  15, Step:    56500, Batch Loss:     2.671602, Tokens per Sec:    16436, Lr: 0.000300\n",
      "2021-07-18 18:28:47,419 - INFO - joeynmt.training - Epoch  15, Step:    56600, Batch Loss:     2.770893, Tokens per Sec:    16612, Lr: 0.000300\n",
      "2021-07-18 18:29:01,179 - INFO - joeynmt.training - Epoch  15, Step:    56700, Batch Loss:     2.540199, Tokens per Sec:    16418, Lr: 0.000300\n",
      "2021-07-18 18:29:14,918 - INFO - joeynmt.training - Epoch  15, Step:    56800, Batch Loss:     2.643060, Tokens per Sec:    16299, Lr: 0.000300\n",
      "2021-07-18 18:29:28,591 - INFO - joeynmt.training - Epoch  15, Step:    56900, Batch Loss:     2.112634, Tokens per Sec:    16662, Lr: 0.000300\n",
      "2021-07-18 18:29:42,352 - INFO - joeynmt.training - Epoch  15, Step:    57000, Batch Loss:     2.242307, Tokens per Sec:    16528, Lr: 0.000300\n",
      "2021-07-18 18:29:55,916 - INFO - joeynmt.training - Epoch  15, Step:    57100, Batch Loss:     2.600213, Tokens per Sec:    16541, Lr: 0.000300\n",
      "2021-07-18 18:30:09,762 - INFO - joeynmt.training - Epoch  15, Step:    57200, Batch Loss:     2.434771, Tokens per Sec:    16521, Lr: 0.000300\n",
      "2021-07-18 18:30:23,656 - INFO - joeynmt.training - Epoch  15, Step:    57300, Batch Loss:     2.783355, Tokens per Sec:    16791, Lr: 0.000300\n",
      "2021-07-18 18:30:37,079 - INFO - joeynmt.training - Epoch  15, Step:    57400, Batch Loss:     2.754578, Tokens per Sec:    16304, Lr: 0.000300\n",
      "2021-07-18 18:30:50,836 - INFO - joeynmt.training - Epoch  15, Step:    57500, Batch Loss:     2.416583, Tokens per Sec:    16636, Lr: 0.000300\n",
      "2021-07-18 18:31:19,976 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:31:19,977 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:31:19,977 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:31:21,079 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:31:21,080 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:31:21,080 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:31:21,080 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw Him , and said to them , “ I have come to the city , and I have come to you . ”\n",
      "2021-07-18 18:31:21,080 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:31:21,081 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:31:21,081 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:31:21,081 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and Mary .\n",
      "2021-07-18 18:31:21,081 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:31:21,082 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:31:21,082 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:31:21,082 - INFO - joeynmt.training - \tHypothesis: He was a day of the day of the day of the day of the Sabbath .\n",
      "2021-07-18 18:31:21,082 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:31:21,083 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:31:21,083 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:31:21,083 - INFO - joeynmt.training - \tHypothesis: And when Paul had been given up , He was in the body , when He was sat down , He was sat down , and the sound of the sea , and the sound of the sea .\n",
      "2021-07-18 18:31:21,083 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    57500: bleu:   6.50, loss: 90869.3828, ppl:  13.3808, duration: 30.2473s\n",
      "2021-07-18 18:31:34,979 - INFO - joeynmt.training - Epoch  15, Step:    57600, Batch Loss:     2.471746, Tokens per Sec:    16455, Lr: 0.000300\n",
      "2021-07-18 18:31:48,676 - INFO - joeynmt.training - Epoch  15, Step:    57700, Batch Loss:     2.502301, Tokens per Sec:    16775, Lr: 0.000300\n",
      "2021-07-18 18:32:02,441 - INFO - joeynmt.training - Epoch  15, Step:    57800, Batch Loss:     2.517187, Tokens per Sec:    16148, Lr: 0.000300\n",
      "2021-07-18 18:32:16,158 - INFO - joeynmt.training - Epoch  15, Step:    57900, Batch Loss:     2.449076, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-18 18:32:29,791 - INFO - joeynmt.training - Epoch  15, Step:    58000, Batch Loss:     2.590324, Tokens per Sec:    16446, Lr: 0.000300\n",
      "2021-07-18 18:32:43,268 - INFO - joeynmt.training - Epoch  15, Step:    58100, Batch Loss:     2.510573, Tokens per Sec:    16457, Lr: 0.000300\n",
      "2021-07-18 18:32:43,817 - INFO - joeynmt.training - Epoch  15: total training loss 5975.85\n",
      "2021-07-18 18:32:43,818 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-18 18:32:57,418 - INFO - joeynmt.training - Epoch  16, Step:    58200, Batch Loss:     2.657489, Tokens per Sec:    16111, Lr: 0.000300\n",
      "2021-07-18 18:33:11,255 - INFO - joeynmt.training - Epoch  16, Step:    58300, Batch Loss:     2.643729, Tokens per Sec:    16641, Lr: 0.000300\n",
      "2021-07-18 18:33:25,013 - INFO - joeynmt.training - Epoch  16, Step:    58400, Batch Loss:     2.564490, Tokens per Sec:    16580, Lr: 0.000300\n",
      "2021-07-18 18:33:38,816 - INFO - joeynmt.training - Epoch  16, Step:    58500, Batch Loss:     2.566830, Tokens per Sec:    16800, Lr: 0.000300\n",
      "2021-07-18 18:33:52,399 - INFO - joeynmt.training - Epoch  16, Step:    58600, Batch Loss:     2.509922, Tokens per Sec:    16526, Lr: 0.000300\n",
      "2021-07-18 18:34:06,181 - INFO - joeynmt.training - Epoch  16, Step:    58700, Batch Loss:     2.606077, Tokens per Sec:    16409, Lr: 0.000300\n",
      "2021-07-18 18:34:20,126 - INFO - joeynmt.training - Epoch  16, Step:    58800, Batch Loss:     2.610657, Tokens per Sec:    16546, Lr: 0.000300\n",
      "2021-07-18 18:34:33,641 - INFO - joeynmt.training - Epoch  16, Step:    58900, Batch Loss:     2.751345, Tokens per Sec:    16452, Lr: 0.000300\n",
      "2021-07-18 18:34:47,399 - INFO - joeynmt.training - Epoch  16, Step:    59000, Batch Loss:     2.247504, Tokens per Sec:    16515, Lr: 0.000300\n",
      "2021-07-18 18:35:01,050 - INFO - joeynmt.training - Epoch  16, Step:    59100, Batch Loss:     2.665253, Tokens per Sec:    16477, Lr: 0.000300\n",
      "2021-07-18 18:35:15,066 - INFO - joeynmt.training - Epoch  16, Step:    59200, Batch Loss:     2.707827, Tokens per Sec:    16566, Lr: 0.000300\n",
      "2021-07-18 18:35:28,746 - INFO - joeynmt.training - Epoch  16, Step:    59300, Batch Loss:     2.691685, Tokens per Sec:    16656, Lr: 0.000300\n",
      "2021-07-18 18:35:42,521 - INFO - joeynmt.training - Epoch  16, Step:    59400, Batch Loss:     2.827472, Tokens per Sec:    16360, Lr: 0.000300\n",
      "2021-07-18 18:35:56,303 - INFO - joeynmt.training - Epoch  16, Step:    59500, Batch Loss:     2.412101, Tokens per Sec:    16358, Lr: 0.000300\n",
      "2021-07-18 18:36:09,945 - INFO - joeynmt.training - Epoch  16, Step:    59600, Batch Loss:     2.265986, Tokens per Sec:    16551, Lr: 0.000300\n",
      "2021-07-18 18:36:23,589 - INFO - joeynmt.training - Epoch  16, Step:    59700, Batch Loss:     2.730962, Tokens per Sec:    16442, Lr: 0.000300\n",
      "2021-07-18 18:36:37,212 - INFO - joeynmt.training - Epoch  16, Step:    59800, Batch Loss:     2.514449, Tokens per Sec:    16490, Lr: 0.000300\n",
      "2021-07-18 18:36:51,107 - INFO - joeynmt.training - Epoch  16, Step:    59900, Batch Loss:     2.542433, Tokens per Sec:    16676, Lr: 0.000300\n",
      "2021-07-18 18:37:04,941 - INFO - joeynmt.training - Epoch  16, Step:    60000, Batch Loss:     2.831256, Tokens per Sec:    16490, Lr: 0.000300\n",
      "2021-07-18 18:37:31,530 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:37:31,531 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:37:31,531 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:37:32,693 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:37:32,694 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:37:32,694 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:37:32,694 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees said to them , “ I saw the seven sheep , and I saw them , and I went to them . ”\n",
      "2021-07-18 18:37:32,695 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:37:32,695 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:37:32,695 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:37:32,695 - INFO - joeynmt.training - \tHypothesis: Now when Martha had heard the voice of Mary , Mary and Mary , and Mary , and Mary said to Him , “ Take the word of Mary . ”\n",
      "2021-07-18 18:37:32,696 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:37:32,696 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:37:32,696 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:37:32,697 - INFO - joeynmt.training - \tHypothesis: He was a day of service in the days of his day .\n",
      "2021-07-18 18:37:32,697 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:37:32,697 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:37:32,697 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:37:32,698 - INFO - joeynmt.training - \tHypothesis: And Paul had seen the sound of the sea , and when he was raised up , he was raised up , and the sea of the sea , and the bread of the sea .\n",
      "2021-07-18 18:37:32,698 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    60000: bleu:   6.43, loss: 90864.1484, ppl:  13.3788, duration: 27.7558s\n",
      "2021-07-18 18:37:46,729 - INFO - joeynmt.training - Epoch  16, Step:    60100, Batch Loss:     2.424499, Tokens per Sec:    16434, Lr: 0.000300\n",
      "2021-07-18 18:38:00,445 - INFO - joeynmt.training - Epoch  16, Step:    60200, Batch Loss:     2.705121, Tokens per Sec:    16465, Lr: 0.000300\n",
      "2021-07-18 18:38:14,251 - INFO - joeynmt.training - Epoch  16, Step:    60300, Batch Loss:     2.648430, Tokens per Sec:    16428, Lr: 0.000300\n",
      "2021-07-18 18:38:27,991 - INFO - joeynmt.training - Epoch  16, Step:    60400, Batch Loss:     2.422471, Tokens per Sec:    16229, Lr: 0.000300\n",
      "2021-07-18 18:38:31,647 - INFO - joeynmt.training - Epoch  16: total training loss 5935.66\n",
      "2021-07-18 18:38:31,648 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-18 18:38:42,022 - INFO - joeynmt.training - Epoch  17, Step:    60500, Batch Loss:     2.474560, Tokens per Sec:    16019, Lr: 0.000300\n",
      "2021-07-18 18:38:55,843 - INFO - joeynmt.training - Epoch  17, Step:    60600, Batch Loss:     2.507461, Tokens per Sec:    16636, Lr: 0.000300\n",
      "2021-07-18 18:39:09,625 - INFO - joeynmt.training - Epoch  17, Step:    60700, Batch Loss:     2.501543, Tokens per Sec:    16517, Lr: 0.000300\n",
      "2021-07-18 18:39:23,603 - INFO - joeynmt.training - Epoch  17, Step:    60800, Batch Loss:     2.428998, Tokens per Sec:    16541, Lr: 0.000300\n",
      "2021-07-18 18:39:37,393 - INFO - joeynmt.training - Epoch  17, Step:    60900, Batch Loss:     2.668244, Tokens per Sec:    16667, Lr: 0.000300\n",
      "2021-07-18 18:39:50,954 - INFO - joeynmt.training - Epoch  17, Step:    61000, Batch Loss:     2.384507, Tokens per Sec:    16372, Lr: 0.000300\n",
      "2021-07-18 18:40:04,658 - INFO - joeynmt.training - Epoch  17, Step:    61100, Batch Loss:     2.600641, Tokens per Sec:    16370, Lr: 0.000300\n",
      "2021-07-18 18:40:18,539 - INFO - joeynmt.training - Epoch  17, Step:    61200, Batch Loss:     2.326046, Tokens per Sec:    16293, Lr: 0.000300\n",
      "2021-07-18 18:40:32,179 - INFO - joeynmt.training - Epoch  17, Step:    61300, Batch Loss:     2.433665, Tokens per Sec:    16403, Lr: 0.000300\n",
      "2021-07-18 18:40:45,963 - INFO - joeynmt.training - Epoch  17, Step:    61400, Batch Loss:     2.548888, Tokens per Sec:    16539, Lr: 0.000300\n",
      "2021-07-18 18:40:59,687 - INFO - joeynmt.training - Epoch  17, Step:    61500, Batch Loss:     2.552166, Tokens per Sec:    16369, Lr: 0.000300\n",
      "2021-07-18 18:41:13,407 - INFO - joeynmt.training - Epoch  17, Step:    61600, Batch Loss:     2.625062, Tokens per Sec:    16365, Lr: 0.000300\n",
      "2021-07-18 18:41:27,020 - INFO - joeynmt.training - Epoch  17, Step:    61700, Batch Loss:     2.514465, Tokens per Sec:    16346, Lr: 0.000300\n",
      "2021-07-18 18:41:40,764 - INFO - joeynmt.training - Epoch  17, Step:    61800, Batch Loss:     2.608452, Tokens per Sec:    16353, Lr: 0.000300\n",
      "2021-07-18 18:41:54,422 - INFO - joeynmt.training - Epoch  17, Step:    61900, Batch Loss:     2.543049, Tokens per Sec:    16389, Lr: 0.000300\n",
      "2021-07-18 18:42:08,097 - INFO - joeynmt.training - Epoch  17, Step:    62000, Batch Loss:     2.810324, Tokens per Sec:    16384, Lr: 0.000300\n",
      "2021-07-18 18:42:21,943 - INFO - joeynmt.training - Epoch  17, Step:    62100, Batch Loss:     2.394086, Tokens per Sec:    16713, Lr: 0.000300\n",
      "2021-07-18 18:42:35,687 - INFO - joeynmt.training - Epoch  17, Step:    62200, Batch Loss:     2.783573, Tokens per Sec:    16510, Lr: 0.000300\n",
      "2021-07-18 18:42:49,397 - INFO - joeynmt.training - Epoch  17, Step:    62300, Batch Loss:     2.393838, Tokens per Sec:    16600, Lr: 0.000300\n",
      "2021-07-18 18:43:03,350 - INFO - joeynmt.training - Epoch  17, Step:    62400, Batch Loss:     2.559532, Tokens per Sec:    16768, Lr: 0.000300\n",
      "2021-07-18 18:43:17,007 - INFO - joeynmt.training - Epoch  17, Step:    62500, Batch Loss:     2.522063, Tokens per Sec:    16640, Lr: 0.000300\n",
      "2021-07-18 18:43:43,876 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:43:43,876 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:43:43,877 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:43:45,037 - INFO - joeynmt.training - Example #0\n",
      "2021-07-18 18:43:45,038 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-18 18:43:45,038 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-18 18:43:45,038 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like them , saying , “ You have come to the door , and I have come to you , and I have come to you . ”\n",
      "2021-07-18 18:43:45,038 - INFO - joeynmt.training - Example #1\n",
      "2021-07-18 18:43:45,039 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-18 18:43:45,039 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-18 18:43:45,040 - INFO - joeynmt.training - \tHypothesis: And when Martha was going to see Mary , Mary and Mary , and Mary , and Mary said to Him , “ Mary , and you are going to see the little . ”\n",
      "2021-07-18 18:43:45,040 - INFO - joeynmt.training - Example #2\n",
      "2021-07-18 18:43:45,040 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-18 18:43:45,041 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-18 18:43:45,041 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a day of day .\n",
      "2021-07-18 18:43:45,041 - INFO - joeynmt.training - Example #3\n",
      "2021-07-18 18:43:45,041 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-18 18:43:45,042 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-18 18:43:45,042 - INFO - joeynmt.training - \tHypothesis: Paul also looked forward to the body , and when he was raised up , he was raised up , and the lamp of the sea , and the lamp of the sea , and the bread of the sea .\n",
      "2021-07-18 18:43:45,042 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    62500: bleu:   6.36, loss: 91230.0547, ppl:  13.5193, duration: 28.0349s\n",
      "2021-07-18 18:43:59,096 - INFO - joeynmt.training - Epoch  17, Step:    62600, Batch Loss:     2.324243, Tokens per Sec:    16158, Lr: 0.000300\n",
      "2021-07-18 18:44:12,942 - INFO - joeynmt.training - Epoch  17, Step:    62700, Batch Loss:     2.548611, Tokens per Sec:    16563, Lr: 0.000300\n",
      "2021-07-18 18:44:20,276 - INFO - joeynmt.training - Epoch  17: total training loss 5926.62\n",
      "2021-07-18 18:44:20,276 - INFO - joeynmt.training - Training ended after  17 epochs.\n",
      "2021-07-18 18:44:20,276 - INFO - joeynmt.training - Best validation result (greedy) at step    55000:  13.37 ppl.\n",
      "2021-07-18 18:44:20,298 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-07-18 18:44:20,675 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-18 18:44:20,880 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-18 18:44:20,949 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe.en)...\n",
      "2021-07-18 18:44:55,658 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:44:55,659 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:44:55,659 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:44:55,996 - INFO - joeynmt.prediction -  dev bleu[13a]:   7.41 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-18 18:44:56,002 - INFO - joeynmt.prediction - Translations saved to: models/back_lhen_reverse_transformer_continued/00055000.hyps.dev\n",
      "2021-07-18 18:44:56,003 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe.en)...\n",
      "2021-07-18 18:45:30,811 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-18 18:45:30,812 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-18 18:45:30,812 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-18 18:45:31,145 - INFO - joeynmt.prediction - test bleu[13a]:   7.09 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-18 18:45:31,151 - INFO - joeynmt.prediction - Translations saved to: models/back_lhen_reverse_transformer_continued/00055000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/back_transformer_reverse_lhen_reload.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYXeiMih2BgB"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 62500\n",
    "#model_path = '/content/gdrive/Shared drives/NMT_for_African_Language/Luganda/joeynmt/models/{name}_reverse_transformer2'\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/models/lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_{name}_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/back_lhen_reverse_transformer\"', f'model_dir: \"models/back_lhen_reverse_transformer_continued2\"').replace(\n",
    "            f'validation_freq: 5000', f'validation_freq: 2500')\n",
    "with open(\"joeynmt/configs/back_transformer_reverse_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BIF6sAgSbaEC",
    "outputId": "77ba415d-5c05-4b30-e944-2e389489e229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued/62500.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/back_lhen_reverse_transformer_continued2\"\n",
      "    overwrite: True              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/back_transformer_reverse_lhen_reload2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGpI-9UTbhnS",
    "outputId": "fa5222f7-3cac-4a92-f45c-a33ea4bea1de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-19 06:34:19,360 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-19 06:34:19,477 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-19 06:34:25,415 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-19 06:34:26,608 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-19 06:34:27,539 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-19 06:34:29,217 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-19 06:34:29,218 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-19 06:34:29,748 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-19 06:34:30.003105: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-19 06:34:32,209 - INFO - joeynmt.training - Total params: 12138240\n",
      "2021-07-19 06:34:40,915 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued/62500.ckpt\n",
      "2021-07-19 06:34:41,505 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-19 06:34:41,505 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-19 06:34:41,505 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-19 06:34:41,506 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\n",
      "2021-07-19 06:34:41,506 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\n",
      "2021-07-19 06:34:41,506 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\n",
      "2021-07-19 06:34:41,506 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-19 06:34:41,507 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-19 06:34:41,507 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-19 06:34:41,507 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-19 06:34:41,507 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-19 06:34:41,508 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-19 06:34:41,508 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-19 06:34:41,509 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued/62500.ckpt\n",
      "2021-07-19 06:34:41,509 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-19 06:34:41,509 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-19 06:34:41,509 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-19 06:34:41,510 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-19 06:34:41,510 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-19 06:34:41,510 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-19 06:34:41,510 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-19 06:34:41,511 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-19 06:34:41,511 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-19 06:34:41,511 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-19 06:34:41,511 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-19 06:34:41,512 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-19 06:34:41,512 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-19 06:34:41,512 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-19 06:34:41,512 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-19 06:34:41,513 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-19 06:34:41,514 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
      "2021-07-19 06:34:41,514 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
      "2021-07-19 06:34:41,514 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-19 06:34:41,514 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-19 06:34:41,515 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/back_lhen_reverse_transformer_continued2\n",
      "2021-07-19 06:34:41,515 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
      "2021-07-19 06:34:41,515 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-19 06:34:41,515 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-19 06:34:41,516 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-19 06:34:41,517 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-19 06:34:41,517 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-19 06:34:41,517 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-19 06:34:41,517 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-19 06:34:41,518 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-19 06:34:41,518 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-19 06:34:41,518 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-19 06:34:41,518 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-19 06:34:41,519 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-19 06:34:41,519 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-19 06:34:41,519 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-19 06:34:41,519 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-19 06:34:41,520 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-19 06:34:41,520 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-19 06:34:41,520 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-19 06:34:41,520 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-19 06:34:41,521 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-19 06:34:41,521 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-19 06:34:41,521 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-19 06:34:41,521 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-19 06:34:41,522 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-19 06:34:41,522 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-19 06:34:41,522 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-19 06:34:41,522 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 207289,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-19 06:34:41,523 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] P@@ il@@ a@@ to nakal@@ ukha itookho , ne nal@@ anga Yesu , namureeba , ari , “ Iwe ni@@ we omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then P@@ il@@ ate ent@@ ered the P@@ ra@@ et@@ or@@ i@@ u@@ m again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-19 06:34:41,523 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-19 06:34:41,523 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-19 06:34:41,523 - INFO - joeynmt.helpers - Number of Src words (types): 4211\n",
      "2021-07-19 06:34:41,524 - INFO - joeynmt.helpers - Number of Trg words (types): 4211\n",
      "2021-07-19 06:34:41,524 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4211),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4211))\n",
      "2021-07-19 06:34:41,542 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-19 06:34:41,542 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-19 06:35:12,220 - INFO - joeynmt.training - Epoch   1, Step:    62600, Batch Loss:     2.346399, Tokens per Sec:     7402, Lr: 0.000300\n",
      "2021-07-19 06:35:41,937 - INFO - joeynmt.training - Epoch   1, Step:    62700, Batch Loss:     2.545072, Tokens per Sec:     7717, Lr: 0.000300\n",
      "2021-07-19 06:35:57,717 - INFO - joeynmt.training - Epoch   1: total training loss 646.74\n",
      "2021-07-19 06:35:57,718 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-19 06:36:12,046 - INFO - joeynmt.training - Epoch   2, Step:    62800, Batch Loss:     2.736987, Tokens per Sec:     7515, Lr: 0.000300\n",
      "2021-07-19 06:36:41,671 - INFO - joeynmt.training - Epoch   2, Step:    62900, Batch Loss:     2.440602, Tokens per Sec:     7804, Lr: 0.000300\n",
      "2021-07-19 06:37:11,165 - INFO - joeynmt.training - Epoch   2, Step:    63000, Batch Loss:     2.554001, Tokens per Sec:     7750, Lr: 0.000300\n",
      "2021-07-19 06:37:40,753 - INFO - joeynmt.training - Epoch   2, Step:    63100, Batch Loss:     2.708537, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 06:38:10,212 - INFO - joeynmt.training - Epoch   2, Step:    63200, Batch Loss:     2.634101, Tokens per Sec:     7593, Lr: 0.000300\n",
      "2021-07-19 06:38:39,464 - INFO - joeynmt.training - Epoch   2, Step:    63300, Batch Loss:     2.772285, Tokens per Sec:     7595, Lr: 0.000300\n",
      "2021-07-19 06:39:08,917 - INFO - joeynmt.training - Epoch   2, Step:    63400, Batch Loss:     2.618553, Tokens per Sec:     7693, Lr: 0.000300\n",
      "2021-07-19 06:39:38,748 - INFO - joeynmt.training - Epoch   2, Step:    63500, Batch Loss:     2.428201, Tokens per Sec:     7764, Lr: 0.000300\n",
      "2021-07-19 06:40:08,238 - INFO - joeynmt.training - Epoch   2, Step:    63600, Batch Loss:     2.755451, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 06:40:37,936 - INFO - joeynmt.training - Epoch   2, Step:    63700, Batch Loss:     2.286687, Tokens per Sec:     7721, Lr: 0.000300\n",
      "2021-07-19 06:41:07,045 - INFO - joeynmt.training - Epoch   2, Step:    63800, Batch Loss:     2.797885, Tokens per Sec:     7587, Lr: 0.000300\n",
      "2021-07-19 06:41:36,709 - INFO - joeynmt.training - Epoch   2, Step:    63900, Batch Loss:     2.845188, Tokens per Sec:     7792, Lr: 0.000300\n",
      "2021-07-19 06:42:06,384 - INFO - joeynmt.training - Epoch   2, Step:    64000, Batch Loss:     2.760698, Tokens per Sec:     7731, Lr: 0.000300\n",
      "2021-07-19 06:42:36,091 - INFO - joeynmt.training - Epoch   2, Step:    64100, Batch Loss:     2.663007, Tokens per Sec:     7784, Lr: 0.000300\n",
      "2021-07-19 06:43:05,565 - INFO - joeynmt.training - Epoch   2, Step:    64200, Batch Loss:     2.447559, Tokens per Sec:     7563, Lr: 0.000300\n",
      "2021-07-19 06:43:35,073 - INFO - joeynmt.training - Epoch   2, Step:    64300, Batch Loss:     1.854980, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 06:44:04,268 - INFO - joeynmt.training - Epoch   2, Step:    64400, Batch Loss:     1.873276, Tokens per Sec:     7631, Lr: 0.000300\n",
      "2021-07-19 06:44:34,021 - INFO - joeynmt.training - Epoch   2, Step:    64500, Batch Loss:     2.559284, Tokens per Sec:     7707, Lr: 0.000300\n",
      "2021-07-19 06:45:03,777 - INFO - joeynmt.training - Epoch   2, Step:    64600, Batch Loss:     2.497647, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 06:45:33,402 - INFO - joeynmt.training - Epoch   2, Step:    64700, Batch Loss:     2.295540, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 06:46:02,968 - INFO - joeynmt.training - Epoch   2, Step:    64800, Batch Loss:     2.707605, Tokens per Sec:     7724, Lr: 0.000300\n",
      "2021-07-19 06:46:32,506 - INFO - joeynmt.training - Epoch   2, Step:    64900, Batch Loss:     2.602335, Tokens per Sec:     7694, Lr: 0.000300\n",
      "2021-07-19 06:47:01,836 - INFO - joeynmt.training - Epoch   2, Step:    65000, Batch Loss:     2.541043, Tokens per Sec:     7598, Lr: 0.000300\n",
      "2021-07-19 06:48:01,459 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 06:48:01,460 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 06:48:01,460 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 06:48:01,889 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 06:48:01,890 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 06:48:02,755 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 06:48:02,757 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 06:48:02,757 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 06:48:02,757 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a man , and said to them , “ You have come to the door , and I have come to you . ”\n",
      "2021-07-19 06:48:02,757 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 06:48:02,758 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 06:48:02,758 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 06:48:02,759 - INFO - joeynmt.training - \tHypothesis: Now Martha and Martha , and the sister came to Mary , and said to Him , “ Teacher , and the Lord , and we are with Him . ”\n",
      "2021-07-19 06:48:02,759 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 06:48:02,760 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 06:48:02,760 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 06:48:02,760 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and the day of the day was near .\n",
      "2021-07-19 06:48:02,760 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 06:48:02,761 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 06:48:02,761 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 06:48:02,761 - INFO - joeynmt.training - \tHypothesis: And when he had come , He saw the body , and saw Him , and the fire of the body , and the fire of the fire , and the fire of the sea , and the right hand of the right hand .\n",
      "2021-07-19 06:48:02,762 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    65000: bleu:   6.62, loss: 90258.0391, ppl:  13.1493, duration: 60.9247s\n",
      "2021-07-19 06:48:25,778 - INFO - joeynmt.training - Epoch   2: total training loss 5908.06\n",
      "2021-07-19 06:48:25,779 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-19 06:48:32,585 - INFO - joeynmt.training - Epoch   3, Step:    65100, Batch Loss:     2.414836, Tokens per Sec:     6814, Lr: 0.000300\n",
      "2021-07-19 06:49:02,227 - INFO - joeynmt.training - Epoch   3, Step:    65200, Batch Loss:     2.787891, Tokens per Sec:     7788, Lr: 0.000300\n",
      "2021-07-19 06:49:31,924 - INFO - joeynmt.training - Epoch   3, Step:    65300, Batch Loss:     2.723508, Tokens per Sec:     7669, Lr: 0.000300\n",
      "2021-07-19 06:50:01,471 - INFO - joeynmt.training - Epoch   3, Step:    65400, Batch Loss:     2.725967, Tokens per Sec:     7685, Lr: 0.000300\n",
      "2021-07-19 06:50:31,090 - INFO - joeynmt.training - Epoch   3, Step:    65500, Batch Loss:     2.502720, Tokens per Sec:     7718, Lr: 0.000300\n",
      "2021-07-19 06:51:00,736 - INFO - joeynmt.training - Epoch   3, Step:    65600, Batch Loss:     2.525380, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 06:51:30,352 - INFO - joeynmt.training - Epoch   3, Step:    65700, Batch Loss:     2.577008, Tokens per Sec:     7679, Lr: 0.000300\n",
      "2021-07-19 06:51:59,963 - INFO - joeynmt.training - Epoch   3, Step:    65800, Batch Loss:     2.718289, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 06:52:29,607 - INFO - joeynmt.training - Epoch   3, Step:    65900, Batch Loss:     2.071284, Tokens per Sec:     7709, Lr: 0.000300\n",
      "2021-07-19 06:52:59,228 - INFO - joeynmt.training - Epoch   3, Step:    66000, Batch Loss:     2.519513, Tokens per Sec:     7697, Lr: 0.000300\n",
      "2021-07-19 06:53:28,862 - INFO - joeynmt.training - Epoch   3, Step:    66100, Batch Loss:     2.458498, Tokens per Sec:     7688, Lr: 0.000300\n",
      "2021-07-19 06:53:58,543 - INFO - joeynmt.training - Epoch   3, Step:    66200, Batch Loss:     2.184022, Tokens per Sec:     7692, Lr: 0.000300\n",
      "2021-07-19 06:54:28,103 - INFO - joeynmt.training - Epoch   3, Step:    66300, Batch Loss:     2.688070, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 06:54:57,480 - INFO - joeynmt.training - Epoch   3, Step:    66400, Batch Loss:     2.234920, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 06:55:27,219 - INFO - joeynmt.training - Epoch   3, Step:    66500, Batch Loss:     2.315522, Tokens per Sec:     7723, Lr: 0.000300\n",
      "2021-07-19 06:55:56,312 - INFO - joeynmt.training - Epoch   3, Step:    66600, Batch Loss:     2.282195, Tokens per Sec:     7579, Lr: 0.000300\n",
      "2021-07-19 06:56:25,984 - INFO - joeynmt.training - Epoch   3, Step:    66700, Batch Loss:     2.451663, Tokens per Sec:     7750, Lr: 0.000300\n",
      "2021-07-19 06:56:55,707 - INFO - joeynmt.training - Epoch   3, Step:    66800, Batch Loss:     2.601985, Tokens per Sec:     7713, Lr: 0.000300\n",
      "2021-07-19 06:57:25,465 - INFO - joeynmt.training - Epoch   3, Step:    66900, Batch Loss:     2.572580, Tokens per Sec:     7715, Lr: 0.000300\n",
      "2021-07-19 06:57:55,123 - INFO - joeynmt.training - Epoch   3, Step:    67000, Batch Loss:     2.611930, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 06:58:24,636 - INFO - joeynmt.training - Epoch   3, Step:    67100, Batch Loss:     2.572382, Tokens per Sec:     7730, Lr: 0.000300\n",
      "2021-07-19 06:58:54,293 - INFO - joeynmt.training - Epoch   3, Step:    67200, Batch Loss:     2.570858, Tokens per Sec:     7649, Lr: 0.000300\n",
      "2021-07-19 06:59:23,959 - INFO - joeynmt.training - Epoch   3, Step:    67300, Batch Loss:     2.707041, Tokens per Sec:     7891, Lr: 0.000300\n",
      "2021-07-19 06:59:51,786 - INFO - joeynmt.training - Epoch   3: total training loss 5872.46\n",
      "2021-07-19 06:59:51,786 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-19 06:59:53,917 - INFO - joeynmt.training - Epoch   4, Step:    67400, Batch Loss:     2.783792, Tokens per Sec:     6737, Lr: 0.000300\n",
      "2021-07-19 07:00:23,359 - INFO - joeynmt.training - Epoch   4, Step:    67500, Batch Loss:     2.699785, Tokens per Sec:     7682, Lr: 0.000300\n",
      "2021-07-19 07:01:14,579 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:01:14,579 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:01:14,579 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:01:15,843 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:01:15,844 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:01:15,844 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:01:15,844 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees , and He said to them , “ Where I am , I am going to see , and I am going to see . ”\n",
      "2021-07-19 07:01:15,844 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:01:15,845 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:01:15,845 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:01:15,846 - INFO - joeynmt.training - \tHypothesis: And Martha , who had heard the voice of Mary , and said to Mary , “ Teacher , and the Lord , and said to Him , “ Take Him , and do not know Him . ”\n",
      "2021-07-19 07:01:15,846 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:01:15,846 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:01:15,847 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:01:15,847 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a great day .\n",
      "2021-07-19 07:01:15,847 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:01:15,848 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:01:15,848 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:01:15,848 - INFO - joeynmt.training - \tHypothesis: And Paul looked for Him , and when He had been raised , and when He was raised up , He was raised up , and the fire , and the fire of the head .\n",
      "2021-07-19 07:01:15,849 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    67500: bleu:   6.42, loss: 90324.8984, ppl:  13.1745, duration: 52.4884s\n",
      "2021-07-19 07:01:45,942 - INFO - joeynmt.training - Epoch   4, Step:    67600, Batch Loss:     2.727720, Tokens per Sec:     7724, Lr: 0.000300\n",
      "2021-07-19 07:02:15,195 - INFO - joeynmt.training - Epoch   4, Step:    67700, Batch Loss:     2.616213, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 07:02:44,654 - INFO - joeynmt.training - Epoch   4, Step:    67800, Batch Loss:     2.429446, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 07:03:14,584 - INFO - joeynmt.training - Epoch   4, Step:    67900, Batch Loss:     2.511340, Tokens per Sec:     7714, Lr: 0.000300\n",
      "2021-07-19 07:03:44,242 - INFO - joeynmt.training - Epoch   4, Step:    68000, Batch Loss:     2.827675, Tokens per Sec:     7798, Lr: 0.000300\n",
      "2021-07-19 07:04:13,603 - INFO - joeynmt.training - Epoch   4, Step:    68100, Batch Loss:     2.527565, Tokens per Sec:     7614, Lr: 0.000300\n",
      "2021-07-19 07:04:42,962 - INFO - joeynmt.training - Epoch   4, Step:    68200, Batch Loss:     2.602662, Tokens per Sec:     7629, Lr: 0.000300\n",
      "2021-07-19 07:05:12,289 - INFO - joeynmt.training - Epoch   4, Step:    68300, Batch Loss:     2.716846, Tokens per Sec:     7640, Lr: 0.000300\n",
      "2021-07-19 07:05:41,962 - INFO - joeynmt.training - Epoch   4, Step:    68400, Batch Loss:     2.713373, Tokens per Sec:     7772, Lr: 0.000300\n",
      "2021-07-19 07:06:11,577 - INFO - joeynmt.training - Epoch   4, Step:    68500, Batch Loss:     2.316921, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 07:06:41,006 - INFO - joeynmt.training - Epoch   4, Step:    68600, Batch Loss:     2.707377, Tokens per Sec:     7554, Lr: 0.000300\n",
      "2021-07-19 07:07:10,835 - INFO - joeynmt.training - Epoch   4, Step:    68700, Batch Loss:     2.214130, Tokens per Sec:     7773, Lr: 0.000300\n",
      "2021-07-19 07:07:40,259 - INFO - joeynmt.training - Epoch   4, Step:    68800, Batch Loss:     2.592070, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-19 07:08:09,971 - INFO - joeynmt.training - Epoch   4, Step:    68900, Batch Loss:     2.511361, Tokens per Sec:     7728, Lr: 0.000300\n",
      "2021-07-19 07:08:39,435 - INFO - joeynmt.training - Epoch   4, Step:    69000, Batch Loss:     2.558335, Tokens per Sec:     7626, Lr: 0.000300\n",
      "2021-07-19 07:09:09,023 - INFO - joeynmt.training - Epoch   4, Step:    69100, Batch Loss:     2.652383, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 07:09:38,627 - INFO - joeynmt.training - Epoch   4, Step:    69200, Batch Loss:     2.504856, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 07:10:08,289 - INFO - joeynmt.training - Epoch   4, Step:    69300, Batch Loss:     2.429661, Tokens per Sec:     7778, Lr: 0.000300\n",
      "2021-07-19 07:10:37,723 - INFO - joeynmt.training - Epoch   4, Step:    69400, Batch Loss:     2.292341, Tokens per Sec:     7695, Lr: 0.000300\n",
      "2021-07-19 07:11:06,970 - INFO - joeynmt.training - Epoch   4, Step:    69500, Batch Loss:     2.741869, Tokens per Sec:     7604, Lr: 0.000300\n",
      "2021-07-19 07:11:36,377 - INFO - joeynmt.training - Epoch   4, Step:    69600, Batch Loss:     2.655686, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 07:12:05,858 - INFO - joeynmt.training - Epoch   4, Step:    69700, Batch Loss:     2.667983, Tokens per Sec:     7740, Lr: 0.000300\n",
      "2021-07-19 07:12:11,212 - INFO - joeynmt.training - Epoch   4: total training loss 5879.79\n",
      "2021-07-19 07:12:11,212 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-19 07:12:35,806 - INFO - joeynmt.training - Epoch   5, Step:    69800, Batch Loss:     2.424563, Tokens per Sec:     7654, Lr: 0.000300\n",
      "2021-07-19 07:13:05,314 - INFO - joeynmt.training - Epoch   5, Step:    69900, Batch Loss:     2.599828, Tokens per Sec:     7712, Lr: 0.000300\n",
      "2021-07-19 07:13:34,943 - INFO - joeynmt.training - Epoch   5, Step:    70000, Batch Loss:     2.418897, Tokens per Sec:     7737, Lr: 0.000300\n",
      "2021-07-19 07:14:33,189 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:14:33,189 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:14:33,190 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:14:33,626 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 07:14:33,627 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 07:14:34,558 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:14:34,560 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:14:34,560 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:14:34,560 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He who is in the midst of the scribes and said to them , “ I have come to you . ”\n",
      "2021-07-19 07:14:34,560 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:14:34,561 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:14:34,561 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:14:34,561 - INFO - joeynmt.training - \tHypothesis: When Martha was a man , she came to Mary , and said to her , “ Mary , and she said to her , “ Teacher , and she was here . ”\n",
      "2021-07-19 07:14:34,562 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:14:34,563 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:14:34,563 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:14:34,563 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a great day .\n",
      "2021-07-19 07:14:34,563 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:14:34,564 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:14:34,564 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:14:34,565 - INFO - joeynmt.training - \tHypothesis: And Paul did not have the body of the body , and when He was raised up , He was sleeping , and the sea of the sea , and the sea was set down .\n",
      "2021-07-19 07:14:34,565 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    70000: bleu:   6.69, loss: 90115.6875, ppl:  13.0960, duration: 59.6215s\n",
      "2021-07-19 07:15:04,460 - INFO - joeynmt.training - Epoch   5, Step:    70100, Batch Loss:     2.299713, Tokens per Sec:     7646, Lr: 0.000300\n",
      "2021-07-19 07:15:33,702 - INFO - joeynmt.training - Epoch   5, Step:    70200, Batch Loss:     2.013454, Tokens per Sec:     7669, Lr: 0.000300\n",
      "2021-07-19 07:16:02,721 - INFO - joeynmt.training - Epoch   5, Step:    70300, Batch Loss:     2.489102, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 07:16:32,326 - INFO - joeynmt.training - Epoch   5, Step:    70400, Batch Loss:     2.679864, Tokens per Sec:     7612, Lr: 0.000300\n",
      "2021-07-19 07:17:01,819 - INFO - joeynmt.training - Epoch   5, Step:    70500, Batch Loss:     2.736821, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 07:17:31,295 - INFO - joeynmt.training - Epoch   5, Step:    70600, Batch Loss:     2.654853, Tokens per Sec:     7663, Lr: 0.000300\n",
      "2021-07-19 07:18:00,914 - INFO - joeynmt.training - Epoch   5, Step:    70700, Batch Loss:     2.487808, Tokens per Sec:     7718, Lr: 0.000300\n",
      "2021-07-19 07:18:30,703 - INFO - joeynmt.training - Epoch   5, Step:    70800, Batch Loss:     2.596038, Tokens per Sec:     7706, Lr: 0.000300\n",
      "2021-07-19 07:19:00,328 - INFO - joeynmt.training - Epoch   5, Step:    70900, Batch Loss:     2.702940, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 07:19:29,767 - INFO - joeynmt.training - Epoch   5, Step:    71000, Batch Loss:     2.543967, Tokens per Sec:     7591, Lr: 0.000300\n",
      "2021-07-19 07:19:59,298 - INFO - joeynmt.training - Epoch   5, Step:    71100, Batch Loss:     2.449262, Tokens per Sec:     7717, Lr: 0.000300\n",
      "2021-07-19 07:20:28,935 - INFO - joeynmt.training - Epoch   5, Step:    71200, Batch Loss:     2.605769, Tokens per Sec:     7726, Lr: 0.000300\n",
      "2021-07-19 07:20:58,534 - INFO - joeynmt.training - Epoch   5, Step:    71300, Batch Loss:     2.602383, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 07:21:28,073 - INFO - joeynmt.training - Epoch   5, Step:    71400, Batch Loss:     2.833060, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-19 07:21:57,777 - INFO - joeynmt.training - Epoch   5, Step:    71500, Batch Loss:     1.846742, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 07:22:27,223 - INFO - joeynmt.training - Epoch   5, Step:    71600, Batch Loss:     2.568074, Tokens per Sec:     7581, Lr: 0.000300\n",
      "2021-07-19 07:22:56,512 - INFO - joeynmt.training - Epoch   5, Step:    71700, Batch Loss:     2.840994, Tokens per Sec:     7624, Lr: 0.000300\n",
      "2021-07-19 07:23:25,914 - INFO - joeynmt.training - Epoch   5, Step:    71800, Batch Loss:     2.323910, Tokens per Sec:     7593, Lr: 0.000300\n",
      "2021-07-19 07:23:55,611 - INFO - joeynmt.training - Epoch   5, Step:    71900, Batch Loss:     2.335999, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 07:24:24,961 - INFO - joeynmt.training - Epoch   5, Step:    72000, Batch Loss:     2.635591, Tokens per Sec:     7717, Lr: 0.000300\n",
      "2021-07-19 07:24:38,829 - INFO - joeynmt.training - Epoch   5: total training loss 5879.53\n",
      "2021-07-19 07:24:38,829 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-19 07:24:54,814 - INFO - joeynmt.training - Epoch   6, Step:    72100, Batch Loss:     2.426062, Tokens per Sec:     7572, Lr: 0.000300\n",
      "2021-07-19 07:25:24,484 - INFO - joeynmt.training - Epoch   6, Step:    72200, Batch Loss:     2.509454, Tokens per Sec:     7733, Lr: 0.000300\n",
      "2021-07-19 07:25:53,714 - INFO - joeynmt.training - Epoch   6, Step:    72300, Batch Loss:     2.302253, Tokens per Sec:     7634, Lr: 0.000300\n",
      "2021-07-19 07:26:23,235 - INFO - joeynmt.training - Epoch   6, Step:    72400, Batch Loss:     2.511296, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 07:26:52,982 - INFO - joeynmt.training - Epoch   6, Step:    72500, Batch Loss:     2.664346, Tokens per Sec:     7786, Lr: 0.000300\n",
      "2021-07-19 07:27:53,310 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:27:53,311 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:27:53,311 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:27:53,726 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 07:27:53,726 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 07:27:54,589 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:27:54,591 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:27:54,591 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:27:54,591 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ As I saw , I saw the scribes and the Pharisees , and said to them , “ I am going to see the seven times of my souls . ”\n",
      "2021-07-19 07:27:54,592 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:27:54,593 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:27:54,593 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:27:54,593 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary and Mary , and Mary , and Mary , and Mary said to Him , “ You are here . ”\n",
      "2021-07-19 07:27:54,593 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:27:54,594 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:27:54,594 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:27:54,594 - INFO - joeynmt.training - \tHypothesis: He was a day of time , and he was a day of judgment .\n",
      "2021-07-19 07:27:54,595 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:27:54,596 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:27:54,596 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:27:54,596 - INFO - joeynmt.training - \tHypothesis: And when he had sat down the sea , he saw the fire , and the fire of the sea , and the sea was cut down and the sea , and the sea was cut down .\n",
      "2021-07-19 07:27:54,596 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    72500: bleu:   6.75, loss: 89952.0469, ppl:  13.0350, duration: 61.6140s\n",
      "2021-07-19 07:28:24,662 - INFO - joeynmt.training - Epoch   6, Step:    72600, Batch Loss:     2.586247, Tokens per Sec:     7687, Lr: 0.000300\n",
      "2021-07-19 07:28:54,253 - INFO - joeynmt.training - Epoch   6, Step:    72700, Batch Loss:     2.817778, Tokens per Sec:     7620, Lr: 0.000300\n",
      "2021-07-19 07:29:23,624 - INFO - joeynmt.training - Epoch   6, Step:    72800, Batch Loss:     2.660525, Tokens per Sec:     7677, Lr: 0.000300\n",
      "2021-07-19 07:29:53,355 - INFO - joeynmt.training - Epoch   6, Step:    72900, Batch Loss:     2.583597, Tokens per Sec:     7792, Lr: 0.000300\n",
      "2021-07-19 07:30:22,997 - INFO - joeynmt.training - Epoch   6, Step:    73000, Batch Loss:     2.547949, Tokens per Sec:     7604, Lr: 0.000300\n",
      "2021-07-19 07:30:52,353 - INFO - joeynmt.training - Epoch   6, Step:    73100, Batch Loss:     2.299666, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 07:31:21,578 - INFO - joeynmt.training - Epoch   6, Step:    73200, Batch Loss:     1.925784, Tokens per Sec:     7729, Lr: 0.000300\n",
      "2021-07-19 07:31:51,104 - INFO - joeynmt.training - Epoch   6, Step:    73300, Batch Loss:     2.509909, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 07:32:20,568 - INFO - joeynmt.training - Epoch   6, Step:    73400, Batch Loss:     2.581945, Tokens per Sec:     7625, Lr: 0.000300\n",
      "2021-07-19 07:32:50,306 - INFO - joeynmt.training - Epoch   6, Step:    73500, Batch Loss:     2.230916, Tokens per Sec:     7769, Lr: 0.000300\n",
      "2021-07-19 07:33:20,101 - INFO - joeynmt.training - Epoch   6, Step:    73600, Batch Loss:     1.950913, Tokens per Sec:     7769, Lr: 0.000300\n",
      "2021-07-19 07:33:49,839 - INFO - joeynmt.training - Epoch   6, Step:    73700, Batch Loss:     2.683997, Tokens per Sec:     7724, Lr: 0.000300\n",
      "2021-07-19 07:34:19,446 - INFO - joeynmt.training - Epoch   6, Step:    73800, Batch Loss:     2.448956, Tokens per Sec:     7712, Lr: 0.000300\n",
      "2021-07-19 07:34:49,004 - INFO - joeynmt.training - Epoch   6, Step:    73900, Batch Loss:     2.616297, Tokens per Sec:     7584, Lr: 0.000300\n",
      "2021-07-19 07:35:18,557 - INFO - joeynmt.training - Epoch   6, Step:    74000, Batch Loss:     2.465466, Tokens per Sec:     7697, Lr: 0.000300\n",
      "2021-07-19 07:35:48,282 - INFO - joeynmt.training - Epoch   6, Step:    74100, Batch Loss:     2.633943, Tokens per Sec:     7747, Lr: 0.000300\n",
      "2021-07-19 07:36:17,472 - INFO - joeynmt.training - Epoch   6, Step:    74200, Batch Loss:     2.679731, Tokens per Sec:     7688, Lr: 0.000300\n",
      "2021-07-19 07:36:46,991 - INFO - joeynmt.training - Epoch   6, Step:    74300, Batch Loss:     2.502028, Tokens per Sec:     7747, Lr: 0.000300\n",
      "2021-07-19 07:37:07,018 - INFO - joeynmt.training - Epoch   6: total training loss 5844.39\n",
      "2021-07-19 07:37:07,019 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-19 07:37:16,873 - INFO - joeynmt.training - Epoch   7, Step:    74400, Batch Loss:     2.620971, Tokens per Sec:     7357, Lr: 0.000300\n",
      "2021-07-19 07:37:46,414 - INFO - joeynmt.training - Epoch   7, Step:    74500, Batch Loss:     2.674058, Tokens per Sec:     7690, Lr: 0.000300\n",
      "2021-07-19 07:38:15,807 - INFO - joeynmt.training - Epoch   7, Step:    74600, Batch Loss:     2.603203, Tokens per Sec:     7595, Lr: 0.000300\n",
      "2021-07-19 07:38:45,323 - INFO - joeynmt.training - Epoch   7, Step:    74700, Batch Loss:     2.723341, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 07:39:14,853 - INFO - joeynmt.training - Epoch   7, Step:    74800, Batch Loss:     2.397131, Tokens per Sec:     7619, Lr: 0.000300\n",
      "2021-07-19 07:39:44,886 - INFO - joeynmt.training - Epoch   7, Step:    74900, Batch Loss:     2.385890, Tokens per Sec:     7791, Lr: 0.000300\n",
      "2021-07-19 07:40:14,437 - INFO - joeynmt.training - Epoch   7, Step:    75000, Batch Loss:     2.569933, Tokens per Sec:     7596, Lr: 0.000300\n",
      "2021-07-19 07:41:08,935 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:41:08,936 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:41:08,936 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:41:09,347 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 07:41:09,348 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 07:41:10,655 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:41:10,656 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:41:10,656 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:41:10,656 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw Him , and He said to them , “ I have come to the seven times , and I have come to you . ”\n",
      "2021-07-19 07:41:10,657 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:41:10,657 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:41:10,658 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:41:10,658 - INFO - joeynmt.training - \tHypothesis: When Martha had heard the voice , Mary came to Mary , and said to her , “ Mary , and she was born , and she said to Him , “ You are going to see Him . ”\n",
      "2021-07-19 07:41:10,658 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:41:10,659 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:41:10,659 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:41:10,659 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was on the day of his day .\n",
      "2021-07-19 07:41:10,659 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:41:10,660 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:41:10,660 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:41:10,661 - INFO - joeynmt.training - \tHypothesis: Then Paul had sat down the sea , and when he had been sleeping , he was sleeping , and he was sleeping , and he was sleeping .\n",
      "2021-07-19 07:41:10,661 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    75000: bleu:   6.69, loss: 89738.5078, ppl:  12.9558, duration: 56.2233s\n",
      "2021-07-19 07:41:40,397 - INFO - joeynmt.training - Epoch   7, Step:    75100, Batch Loss:     2.510605, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 07:42:10,145 - INFO - joeynmt.training - Epoch   7, Step:    75200, Batch Loss:     2.726663, Tokens per Sec:     7609, Lr: 0.000300\n",
      "2021-07-19 07:42:40,159 - INFO - joeynmt.training - Epoch   7, Step:    75300, Batch Loss:     2.833705, Tokens per Sec:     7701, Lr: 0.000300\n",
      "2021-07-19 07:43:09,749 - INFO - joeynmt.training - Epoch   7, Step:    75400, Batch Loss:     2.228259, Tokens per Sec:     7540, Lr: 0.000300\n",
      "2021-07-19 07:43:39,443 - INFO - joeynmt.training - Epoch   7, Step:    75500, Batch Loss:     2.298226, Tokens per Sec:     7626, Lr: 0.000300\n",
      "2021-07-19 07:44:09,180 - INFO - joeynmt.training - Epoch   7, Step:    75600, Batch Loss:     2.632501, Tokens per Sec:     7617, Lr: 0.000300\n",
      "2021-07-19 07:44:39,057 - INFO - joeynmt.training - Epoch   7, Step:    75700, Batch Loss:     2.519464, Tokens per Sec:     7622, Lr: 0.000300\n",
      "2021-07-19 07:45:08,857 - INFO - joeynmt.training - Epoch   7, Step:    75800, Batch Loss:     2.524806, Tokens per Sec:     7589, Lr: 0.000300\n",
      "2021-07-19 07:45:38,718 - INFO - joeynmt.training - Epoch   7, Step:    75900, Batch Loss:     2.535977, Tokens per Sec:     7655, Lr: 0.000300\n",
      "2021-07-19 07:46:08,509 - INFO - joeynmt.training - Epoch   7, Step:    76000, Batch Loss:     2.686146, Tokens per Sec:     7568, Lr: 0.000300\n",
      "2021-07-19 07:46:38,250 - INFO - joeynmt.training - Epoch   7, Step:    76100, Batch Loss:     2.603976, Tokens per Sec:     7663, Lr: 0.000300\n",
      "2021-07-19 07:47:07,997 - INFO - joeynmt.training - Epoch   7, Step:    76200, Batch Loss:     2.653717, Tokens per Sec:     7699, Lr: 0.000300\n",
      "2021-07-19 07:47:37,818 - INFO - joeynmt.training - Epoch   7, Step:    76300, Batch Loss:     2.335499, Tokens per Sec:     7645, Lr: 0.000300\n",
      "2021-07-19 07:48:07,277 - INFO - joeynmt.training - Epoch   7, Step:    76400, Batch Loss:     2.561674, Tokens per Sec:     7627, Lr: 0.000300\n",
      "2021-07-19 07:48:36,919 - INFO - joeynmt.training - Epoch   7, Step:    76500, Batch Loss:     2.619548, Tokens per Sec:     7542, Lr: 0.000300\n",
      "2021-07-19 07:49:06,695 - INFO - joeynmt.training - Epoch   7, Step:    76600, Batch Loss:     2.364868, Tokens per Sec:     7739, Lr: 0.000300\n",
      "2021-07-19 07:49:34,372 - INFO - joeynmt.training - Epoch   7: total training loss 5847.28\n",
      "2021-07-19 07:49:34,372 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-19 07:49:36,452 - INFO - joeynmt.training - Epoch   8, Step:    76700, Batch Loss:     2.664941, Tokens per Sec:     6704, Lr: 0.000300\n",
      "2021-07-19 07:50:06,154 - INFO - joeynmt.training - Epoch   8, Step:    76800, Batch Loss:     2.294627, Tokens per Sec:     7653, Lr: 0.000300\n",
      "2021-07-19 07:50:35,952 - INFO - joeynmt.training - Epoch   8, Step:    76900, Batch Loss:     2.555516, Tokens per Sec:     7825, Lr: 0.000300\n",
      "2021-07-19 07:51:05,429 - INFO - joeynmt.training - Epoch   8, Step:    77000, Batch Loss:     2.537484, Tokens per Sec:     7719, Lr: 0.000300\n",
      "2021-07-19 07:51:34,920 - INFO - joeynmt.training - Epoch   8, Step:    77100, Batch Loss:     2.482327, Tokens per Sec:     7697, Lr: 0.000300\n",
      "2021-07-19 07:52:04,346 - INFO - joeynmt.training - Epoch   8, Step:    77200, Batch Loss:     2.259924, Tokens per Sec:     7695, Lr: 0.000300\n",
      "2021-07-19 07:52:34,023 - INFO - joeynmt.training - Epoch   8, Step:    77300, Batch Loss:     2.775788, Tokens per Sec:     7655, Lr: 0.000300\n",
      "2021-07-19 07:53:03,543 - INFO - joeynmt.training - Epoch   8, Step:    77400, Batch Loss:     2.601133, Tokens per Sec:     7765, Lr: 0.000300\n",
      "2021-07-19 07:53:32,912 - INFO - joeynmt.training - Epoch   8, Step:    77500, Batch Loss:     2.526324, Tokens per Sec:     7662, Lr: 0.000300\n",
      "2021-07-19 07:54:35,384 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 07:54:35,384 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 07:54:35,384 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 07:54:35,787 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 07:54:35,788 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 07:54:36,640 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 07:54:36,641 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 07:54:36,641 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 07:54:36,642 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ As a man , I saw Him , and said to them , “ I have come and come to the seven hundred and fell into the tomb . ”\n",
      "2021-07-19 07:54:36,642 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 07:54:36,643 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 07:54:36,643 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 07:54:36,644 - INFO - joeynmt.training - \tHypothesis: Now when Martha had heard this , Mary came to Mary and said to her , “ Mary , and the daughter of Mary , and the Lord said to Him , “ You are going to see . ”\n",
      "2021-07-19 07:54:36,644 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 07:54:36,644 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 07:54:36,645 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 07:54:36,645 - INFO - joeynmt.training - \tHypothesis: He was a long time to be a day of judgment on the Sabbath .\n",
      "2021-07-19 07:54:36,645 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 07:54:36,646 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 07:54:36,646 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 07:54:36,647 - INFO - joeynmt.training - \tHypothesis: Then Paul had spoken , and when He had been sat down , when He was sleeping , he was sleeping , and the sea of the sea , and the sea was raised up .\n",
      "2021-07-19 07:54:36,647 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    77500: bleu:   6.57, loss: 89585.5703, ppl:  12.8993, duration: 63.7340s\n",
      "2021-07-19 07:55:06,746 - INFO - joeynmt.training - Epoch   8, Step:    77600, Batch Loss:     2.567941, Tokens per Sec:     7696, Lr: 0.000300\n",
      "2021-07-19 07:55:36,057 - INFO - joeynmt.training - Epoch   8, Step:    77700, Batch Loss:     2.551858, Tokens per Sec:     7754, Lr: 0.000300\n",
      "2021-07-19 07:56:05,505 - INFO - joeynmt.training - Epoch   8, Step:    77800, Batch Loss:     2.744708, Tokens per Sec:     7645, Lr: 0.000300\n",
      "2021-07-19 07:56:34,850 - INFO - joeynmt.training - Epoch   8, Step:    77900, Batch Loss:     2.685618, Tokens per Sec:     7677, Lr: 0.000300\n",
      "2021-07-19 07:57:03,990 - INFO - joeynmt.training - Epoch   8, Step:    78000, Batch Loss:     2.612380, Tokens per Sec:     7661, Lr: 0.000300\n",
      "2021-07-19 07:57:33,158 - INFO - joeynmt.training - Epoch   8, Step:    78100, Batch Loss:     2.659914, Tokens per Sec:     7668, Lr: 0.000300\n",
      "2021-07-19 07:58:02,980 - INFO - joeynmt.training - Epoch   8, Step:    78200, Batch Loss:     2.671862, Tokens per Sec:     7682, Lr: 0.000300\n",
      "2021-07-19 07:58:32,260 - INFO - joeynmt.training - Epoch   8, Step:    78300, Batch Loss:     2.482863, Tokens per Sec:     7841, Lr: 0.000300\n",
      "2021-07-19 07:59:01,734 - INFO - joeynmt.training - Epoch   8, Step:    78400, Batch Loss:     2.651960, Tokens per Sec:     7726, Lr: 0.000300\n",
      "2021-07-19 07:59:31,198 - INFO - joeynmt.training - Epoch   8, Step:    78500, Batch Loss:     2.555839, Tokens per Sec:     7693, Lr: 0.000300\n",
      "2021-07-19 08:00:00,772 - INFO - joeynmt.training - Epoch   8, Step:    78600, Batch Loss:     2.397027, Tokens per Sec:     7664, Lr: 0.000300\n",
      "2021-07-19 08:00:30,231 - INFO - joeynmt.training - Epoch   8, Step:    78700, Batch Loss:     2.604766, Tokens per Sec:     7749, Lr: 0.000300\n",
      "2021-07-19 08:00:59,772 - INFO - joeynmt.training - Epoch   8, Step:    78800, Batch Loss:     2.448844, Tokens per Sec:     7751, Lr: 0.000300\n",
      "2021-07-19 08:01:29,319 - INFO - joeynmt.training - Epoch   8, Step:    78900, Batch Loss:     2.320485, Tokens per Sec:     7741, Lr: 0.000300\n",
      "2021-07-19 08:01:58,927 - INFO - joeynmt.training - Epoch   8, Step:    79000, Batch Loss:     2.636914, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 08:02:02,893 - INFO - joeynmt.training - Epoch   8: total training loss 5819.94\n",
      "2021-07-19 08:02:02,893 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-19 08:02:28,698 - INFO - joeynmt.training - Epoch   9, Step:    79100, Batch Loss:     2.443236, Tokens per Sec:     7665, Lr: 0.000300\n",
      "2021-07-19 08:02:57,898 - INFO - joeynmt.training - Epoch   9, Step:    79200, Batch Loss:     2.300488, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-19 08:03:27,400 - INFO - joeynmt.training - Epoch   9, Step:    79300, Batch Loss:     2.413076, Tokens per Sec:     7690, Lr: 0.000300\n",
      "2021-07-19 08:03:57,094 - INFO - joeynmt.training - Epoch   9, Step:    79400, Batch Loss:     2.494830, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 08:04:26,554 - INFO - joeynmt.training - Epoch   9, Step:    79500, Batch Loss:     2.828069, Tokens per Sec:     7723, Lr: 0.000300\n",
      "2021-07-19 08:04:56,205 - INFO - joeynmt.training - Epoch   9, Step:    79600, Batch Loss:     2.337211, Tokens per Sec:     7778, Lr: 0.000300\n",
      "2021-07-19 08:05:25,505 - INFO - joeynmt.training - Epoch   9, Step:    79700, Batch Loss:     2.472323, Tokens per Sec:     7744, Lr: 0.000300\n",
      "2021-07-19 08:05:54,748 - INFO - joeynmt.training - Epoch   9, Step:    79800, Batch Loss:     2.669052, Tokens per Sec:     7706, Lr: 0.000300\n",
      "2021-07-19 08:06:24,030 - INFO - joeynmt.training - Epoch   9, Step:    79900, Batch Loss:     2.742452, Tokens per Sec:     7699, Lr: 0.000300\n",
      "2021-07-19 08:06:53,701 - INFO - joeynmt.training - Epoch   9, Step:    80000, Batch Loss:     2.602110, Tokens per Sec:     7775, Lr: 0.000300\n",
      "2021-07-19 08:07:55,289 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 08:07:55,290 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 08:07:55,290 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 08:07:56,533 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 08:07:56,534 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 08:07:56,534 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 08:07:56,534 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees had a man , and he saw him , saying , “ I have come to the seven thousand and thousand years , and I have come to you . ”\n",
      "2021-07-19 08:07:56,534 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 08:07:56,535 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 08:07:56,535 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 08:07:56,535 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary and Mary , and Mary , and Mary , and Mary , said to him , “ Teacher , and you are going to know . ”\n",
      "2021-07-19 08:07:56,536 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 08:07:56,536 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 08:07:56,536 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 08:07:56,537 - INFO - joeynmt.training - \tHypothesis: He was a long time to come to the day of the Sabbath .\n",
      "2021-07-19 08:07:56,537 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 08:07:56,538 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 08:07:56,538 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 08:07:56,538 - INFO - joeynmt.training - \tHypothesis: Paul was looking for the sake of the sea , and when he was raised up , he was raised up and sat down with the fire , and the fire of the sea was set out .\n",
      "2021-07-19 08:07:56,539 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    80000: bleu:   6.47, loss: 89616.5391, ppl:  12.9108, duration: 62.8341s\n",
      "2021-07-19 08:08:26,149 - INFO - joeynmt.training - Epoch   9, Step:    80100, Batch Loss:     2.214161, Tokens per Sec:     7635, Lr: 0.000300\n",
      "2021-07-19 08:08:55,645 - INFO - joeynmt.training - Epoch   9, Step:    80200, Batch Loss:     2.246352, Tokens per Sec:     7698, Lr: 0.000300\n",
      "2021-07-19 08:09:25,269 - INFO - joeynmt.training - Epoch   9, Step:    80300, Batch Loss:     2.544671, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 08:09:54,586 - INFO - joeynmt.training - Epoch   9, Step:    80400, Batch Loss:     2.687201, Tokens per Sec:     7807, Lr: 0.000300\n",
      "2021-07-19 08:10:23,992 - INFO - joeynmt.training - Epoch   9, Step:    80500, Batch Loss:     2.606653, Tokens per Sec:     7692, Lr: 0.000300\n",
      "2021-07-19 08:10:53,229 - INFO - joeynmt.training - Epoch   9, Step:    80600, Batch Loss:     2.571099, Tokens per Sec:     7556, Lr: 0.000300\n",
      "2021-07-19 08:11:22,574 - INFO - joeynmt.training - Epoch   9, Step:    80700, Batch Loss:     2.440603, Tokens per Sec:     7662, Lr: 0.000300\n",
      "2021-07-19 08:11:51,866 - INFO - joeynmt.training - Epoch   9, Step:    80800, Batch Loss:     2.414575, Tokens per Sec:     7656, Lr: 0.000300\n",
      "2021-07-19 08:12:21,091 - INFO - joeynmt.training - Epoch   9, Step:    80900, Batch Loss:     2.664021, Tokens per Sec:     7680, Lr: 0.000300\n",
      "2021-07-19 08:12:50,064 - INFO - joeynmt.training - Epoch   9, Step:    81000, Batch Loss:     2.303547, Tokens per Sec:     7702, Lr: 0.000300\n",
      "2021-07-19 08:13:19,571 - INFO - joeynmt.training - Epoch   9, Step:    81100, Batch Loss:     2.481421, Tokens per Sec:     7643, Lr: 0.000300\n",
      "2021-07-19 08:13:48,900 - INFO - joeynmt.training - Epoch   9, Step:    81200, Batch Loss:     2.584865, Tokens per Sec:     7728, Lr: 0.000300\n",
      "2021-07-19 08:14:18,501 - INFO - joeynmt.training - Epoch   9, Step:    81300, Batch Loss:     2.471842, Tokens per Sec:     7671, Lr: 0.000300\n",
      "2021-07-19 08:14:31,988 - INFO - joeynmt.training - Epoch   9: total training loss 5838.05\n",
      "2021-07-19 08:14:31,988 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-19 08:14:48,369 - INFO - joeynmt.training - Epoch  10, Step:    81400, Batch Loss:     2.451086, Tokens per Sec:     7650, Lr: 0.000300\n",
      "2021-07-19 08:15:17,884 - INFO - joeynmt.training - Epoch  10, Step:    81500, Batch Loss:     2.269515, Tokens per Sec:     7683, Lr: 0.000300\n",
      "2021-07-19 08:15:47,623 - INFO - joeynmt.training - Epoch  10, Step:    81600, Batch Loss:     2.584017, Tokens per Sec:     7799, Lr: 0.000300\n",
      "2021-07-19 08:16:17,311 - INFO - joeynmt.training - Epoch  10, Step:    81700, Batch Loss:     2.119755, Tokens per Sec:     7766, Lr: 0.000300\n",
      "2021-07-19 08:16:46,489 - INFO - joeynmt.training - Epoch  10, Step:    81800, Batch Loss:     2.566749, Tokens per Sec:     7638, Lr: 0.000300\n",
      "2021-07-19 08:17:16,063 - INFO - joeynmt.training - Epoch  10, Step:    81900, Batch Loss:     2.566232, Tokens per Sec:     7715, Lr: 0.000300\n",
      "2021-07-19 08:17:45,514 - INFO - joeynmt.training - Epoch  10, Step:    82000, Batch Loss:     2.778504, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 08:18:14,993 - INFO - joeynmt.training - Epoch  10, Step:    82100, Batch Loss:     2.505421, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 08:18:44,471 - INFO - joeynmt.training - Epoch  10, Step:    82200, Batch Loss:     2.293391, Tokens per Sec:     7688, Lr: 0.000300\n",
      "2021-07-19 08:19:14,065 - INFO - joeynmt.training - Epoch  10, Step:    82300, Batch Loss:     2.750319, Tokens per Sec:     7752, Lr: 0.000300\n",
      "2021-07-19 08:19:43,637 - INFO - joeynmt.training - Epoch  10, Step:    82400, Batch Loss:     2.548778, Tokens per Sec:     7792, Lr: 0.000300\n",
      "2021-07-19 08:20:13,419 - INFO - joeynmt.training - Epoch  10, Step:    82500, Batch Loss:     2.631675, Tokens per Sec:     7847, Lr: 0.000300\n",
      "2021-07-19 08:21:07,627 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 08:21:07,627 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 08:21:07,628 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 08:21:08,970 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 08:21:08,971 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 08:21:08,971 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 08:21:08,971 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He saw the scribes and the Pharisees , and said to them , “ I have come and come to you . ”\n",
      "2021-07-19 08:21:08,972 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 08:21:08,973 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 08:21:08,974 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 08:21:08,974 - INFO - joeynmt.training - \tHypothesis: Now when Martha heard that her sister , Mary , and her mother , and her mother , and her mother , said to her , “ See , and you are going to see . ”\n",
      "2021-07-19 08:21:08,974 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 08:21:08,975 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 08:21:08,975 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 08:21:08,976 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and the day was near .\n",
      "2021-07-19 08:21:08,976 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 08:21:08,977 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 08:21:08,977 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 08:21:08,977 - INFO - joeynmt.training - \tHypothesis: Then Paul looked out to Him , and when He was sleeping , He was sleeping , and the sea of the sea , and the sea was sprout of the sea .\n",
      "2021-07-19 08:21:08,977 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    82500: bleu:   6.86, loss: 89637.4922, ppl:  12.9185, duration: 55.5574s\n",
      "2021-07-19 08:21:38,493 - INFO - joeynmt.training - Epoch  10, Step:    82600, Batch Loss:     2.391757, Tokens per Sec:     7687, Lr: 0.000300\n",
      "2021-07-19 08:22:08,055 - INFO - joeynmt.training - Epoch  10, Step:    82700, Batch Loss:     2.695880, Tokens per Sec:     7619, Lr: 0.000300\n",
      "2021-07-19 08:22:37,127 - INFO - joeynmt.training - Epoch  10, Step:    82800, Batch Loss:     2.809226, Tokens per Sec:     7627, Lr: 0.000300\n",
      "2021-07-19 08:23:06,371 - INFO - joeynmt.training - Epoch  10, Step:    82900, Batch Loss:     2.405377, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-19 08:23:35,646 - INFO - joeynmt.training - Epoch  10, Step:    83000, Batch Loss:     2.335890, Tokens per Sec:     7699, Lr: 0.000300\n",
      "2021-07-19 08:24:05,113 - INFO - joeynmt.training - Epoch  10, Step:    83100, Batch Loss:     2.778522, Tokens per Sec:     7758, Lr: 0.000300\n",
      "2021-07-19 08:24:34,565 - INFO - joeynmt.training - Epoch  10, Step:    83200, Batch Loss:     2.503555, Tokens per Sec:     7707, Lr: 0.000300\n",
      "2021-07-19 08:25:03,937 - INFO - joeynmt.training - Epoch  10, Step:    83300, Batch Loss:     2.703060, Tokens per Sec:     7613, Lr: 0.000300\n",
      "2021-07-19 08:25:33,346 - INFO - joeynmt.training - Epoch  10, Step:    83400, Batch Loss:     2.568027, Tokens per Sec:     7739, Lr: 0.000300\n",
      "2021-07-19 08:26:02,539 - INFO - joeynmt.training - Epoch  10, Step:    83500, Batch Loss:     2.432688, Tokens per Sec:     7636, Lr: 0.000300\n",
      "2021-07-19 08:26:32,215 - INFO - joeynmt.training - Epoch  10, Step:    83600, Batch Loss:     2.594275, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 08:26:53,082 - INFO - joeynmt.training - Epoch  10: total training loss 5809.66\n",
      "2021-07-19 08:26:53,083 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-19 08:27:02,065 - INFO - joeynmt.training - Epoch  11, Step:    83700, Batch Loss:     2.643567, Tokens per Sec:     7561, Lr: 0.000300\n",
      "2021-07-19 08:27:31,535 - INFO - joeynmt.training - Epoch  11, Step:    83800, Batch Loss:     2.475717, Tokens per Sec:     7559, Lr: 0.000300\n",
      "2021-07-19 08:28:01,178 - INFO - joeynmt.training - Epoch  11, Step:    83900, Batch Loss:     2.599801, Tokens per Sec:     7790, Lr: 0.000300\n",
      "2021-07-19 08:28:30,563 - INFO - joeynmt.training - Epoch  11, Step:    84000, Batch Loss:     2.459551, Tokens per Sec:     7728, Lr: 0.000300\n",
      "2021-07-19 08:28:59,962 - INFO - joeynmt.training - Epoch  11, Step:    84100, Batch Loss:     1.946769, Tokens per Sec:     7654, Lr: 0.000300\n",
      "2021-07-19 08:29:29,242 - INFO - joeynmt.training - Epoch  11, Step:    84200, Batch Loss:     2.666283, Tokens per Sec:     7709, Lr: 0.000300\n",
      "2021-07-19 08:29:58,833 - INFO - joeynmt.training - Epoch  11, Step:    84300, Batch Loss:     2.390704, Tokens per Sec:     7696, Lr: 0.000300\n",
      "2021-07-19 08:30:28,149 - INFO - joeynmt.training - Epoch  11, Step:    84400, Batch Loss:     2.483641, Tokens per Sec:     7726, Lr: 0.000300\n",
      "2021-07-19 08:30:57,685 - INFO - joeynmt.training - Epoch  11, Step:    84500, Batch Loss:     2.717983, Tokens per Sec:     7835, Lr: 0.000300\n",
      "2021-07-19 08:31:27,308 - INFO - joeynmt.training - Epoch  11, Step:    84600, Batch Loss:     2.771831, Tokens per Sec:     7758, Lr: 0.000300\n",
      "2021-07-19 08:31:56,644 - INFO - joeynmt.training - Epoch  11, Step:    84700, Batch Loss:     2.626935, Tokens per Sec:     7605, Lr: 0.000300\n",
      "2021-07-19 08:32:26,313 - INFO - joeynmt.training - Epoch  11, Step:    84800, Batch Loss:     2.483963, Tokens per Sec:     7680, Lr: 0.000300\n",
      "2021-07-19 08:32:55,622 - INFO - joeynmt.training - Epoch  11, Step:    84900, Batch Loss:     2.557310, Tokens per Sec:     7654, Lr: 0.000300\n",
      "2021-07-19 08:33:25,143 - INFO - joeynmt.training - Epoch  11, Step:    85000, Batch Loss:     2.529170, Tokens per Sec:     7722, Lr: 0.000300\n",
      "2021-07-19 08:34:27,555 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 08:34:27,555 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 08:34:27,555 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 08:34:27,984 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 08:34:27,985 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 08:34:28,861 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 08:34:28,862 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 08:34:28,862 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 08:34:28,862 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees , who had been a man , and said to them , “ I have come to the city , and I have come to the city and come to the twelve . ”\n",
      "2021-07-19 08:34:28,862 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 08:34:28,863 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 08:34:28,863 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 08:34:28,864 - INFO - joeynmt.training - \tHypothesis: When Martha had heard this , Mary , and Mary , and Mary , and Mary , and Mary , and Mary , and said to her , “ Teacher , and we are going to see Him . ”\n",
      "2021-07-19 08:34:28,864 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 08:34:28,864 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 08:34:28,865 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 08:34:28,865 - INFO - joeynmt.training - \tHypothesis: He was a long time for the day of his day , and he was in the Sabbath .\n",
      "2021-07-19 08:34:28,865 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 08:34:28,866 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 08:34:28,866 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 08:34:28,866 - INFO - joeynmt.training - \tHypothesis: Then Paul gave them a cup , and when He had sat down , he was cast out of the sea , and the fire of the sea , and the sea was a of the sea .\n",
      "2021-07-19 08:34:28,866 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    85000: bleu:   6.79, loss: 89185.6641, ppl:  12.7529, duration: 63.7230s\n",
      "2021-07-19 08:34:58,752 - INFO - joeynmt.training - Epoch  11, Step:    85100, Batch Loss:     2.616946, Tokens per Sec:     7722, Lr: 0.000300\n",
      "2021-07-19 08:35:28,489 - INFO - joeynmt.training - Epoch  11, Step:    85200, Batch Loss:     2.215174, Tokens per Sec:     7676, Lr: 0.000300\n",
      "2021-07-19 08:35:58,303 - INFO - joeynmt.training - Epoch  11, Step:    85300, Batch Loss:     2.190792, Tokens per Sec:     7762, Lr: 0.000300\n",
      "2021-07-19 08:36:27,863 - INFO - joeynmt.training - Epoch  11, Step:    85400, Batch Loss:     2.393881, Tokens per Sec:     7673, Lr: 0.000300\n",
      "2021-07-19 08:36:57,279 - INFO - joeynmt.training - Epoch  11, Step:    85500, Batch Loss:     2.299319, Tokens per Sec:     7607, Lr: 0.000300\n",
      "2021-07-19 08:37:26,909 - INFO - joeynmt.training - Epoch  11, Step:    85600, Batch Loss:     2.519327, Tokens per Sec:     7778, Lr: 0.000300\n",
      "2021-07-19 08:37:56,163 - INFO - joeynmt.training - Epoch  11, Step:    85700, Batch Loss:     2.682425, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 08:38:25,480 - INFO - joeynmt.training - Epoch  11, Step:    85800, Batch Loss:     2.603841, Tokens per Sec:     7582, Lr: 0.000300\n",
      "2021-07-19 08:38:54,955 - INFO - joeynmt.training - Epoch  11, Step:    85900, Batch Loss:     2.512598, Tokens per Sec:     7634, Lr: 0.000300\n",
      "2021-07-19 08:39:22,983 - INFO - joeynmt.training - Epoch  11: total training loss 5794.68\n",
      "2021-07-19 08:39:22,984 - INFO - joeynmt.training - EPOCH 12\n",
      "2021-07-19 08:39:24,730 - INFO - joeynmt.training - Epoch  12, Step:    86000, Batch Loss:     2.406390, Tokens per Sec:     6251, Lr: 0.000300\n",
      "2021-07-19 08:39:53,924 - INFO - joeynmt.training - Epoch  12, Step:    86100, Batch Loss:     2.342146, Tokens per Sec:     7611, Lr: 0.000300\n",
      "2021-07-19 08:40:23,330 - INFO - joeynmt.training - Epoch  12, Step:    86200, Batch Loss:     2.129266, Tokens per Sec:     7673, Lr: 0.000300\n",
      "2021-07-19 08:40:52,524 - INFO - joeynmt.training - Epoch  12, Step:    86300, Batch Loss:     2.348691, Tokens per Sec:     7638, Lr: 0.000300\n",
      "2021-07-19 08:41:22,064 - INFO - joeynmt.training - Epoch  12, Step:    86400, Batch Loss:     2.480468, Tokens per Sec:     7720, Lr: 0.000300\n",
      "2021-07-19 08:41:51,514 - INFO - joeynmt.training - Epoch  12, Step:    86500, Batch Loss:     2.648408, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 08:42:20,855 - INFO - joeynmt.training - Epoch  12, Step:    86600, Batch Loss:     2.695687, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-07-19 08:42:50,424 - INFO - joeynmt.training - Epoch  12, Step:    86700, Batch Loss:     2.329769, Tokens per Sec:     7712, Lr: 0.000300\n",
      "2021-07-19 08:43:19,996 - INFO - joeynmt.training - Epoch  12, Step:    86800, Batch Loss:     2.487303, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 08:43:49,576 - INFO - joeynmt.training - Epoch  12, Step:    86900, Batch Loss:     2.452820, Tokens per Sec:     7678, Lr: 0.000300\n",
      "2021-07-19 08:44:18,965 - INFO - joeynmt.training - Epoch  12, Step:    87000, Batch Loss:     2.402815, Tokens per Sec:     7819, Lr: 0.000300\n",
      "2021-07-19 08:44:48,676 - INFO - joeynmt.training - Epoch  12, Step:    87100, Batch Loss:     2.321086, Tokens per Sec:     7698, Lr: 0.000300\n",
      "2021-07-19 08:45:18,483 - INFO - joeynmt.training - Epoch  12, Step:    87200, Batch Loss:     2.671277, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 08:45:47,968 - INFO - joeynmt.training - Epoch  12, Step:    87300, Batch Loss:     2.220192, Tokens per Sec:     7775, Lr: 0.000300\n",
      "2021-07-19 08:46:17,417 - INFO - joeynmt.training - Epoch  12, Step:    87400, Batch Loss:     2.376643, Tokens per Sec:     7659, Lr: 0.000300\n",
      "2021-07-19 08:46:47,013 - INFO - joeynmt.training - Epoch  12, Step:    87500, Batch Loss:     2.263819, Tokens per Sec:     7863, Lr: 0.000300\n",
      "2021-07-19 08:47:39,847 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 08:47:39,847 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 08:47:39,848 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 08:47:40,272 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 08:47:40,273 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 08:47:41,102 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 08:47:41,104 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 08:47:41,104 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 08:47:41,104 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw him , and He said to them , “ I have come and come to the seven times and come and come to the sword of the sword . ”\n",
      "2021-07-19 08:47:41,104 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 08:47:41,105 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 08:47:41,105 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 08:47:41,106 - INFO - joeynmt.training - \tHypothesis: When Martha had heard the voice of Mary , Mary and Mary , and Mary said to her , “ See , and you are going to see Him . ”\n",
      "2021-07-19 08:47:41,106 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 08:47:41,106 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 08:47:41,107 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 08:47:41,107 - INFO - joeynmt.training - \tHypothesis: He was a day of service , and he was a day of service .\n",
      "2021-07-19 08:47:41,107 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 08:47:41,108 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 08:47:41,108 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 08:47:41,108 - INFO - joeynmt.training - \tHypothesis: Then Paul gave them a car , and when he was on the sea , he was raised up , and he was raised up and sat down and sat down , and the sea of the sea .\n",
      "2021-07-19 08:47:41,109 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    87500: bleu:   6.91, loss: 89144.7344, ppl:  12.7380, duration: 54.0947s\n",
      "2021-07-19 08:48:10,787 - INFO - joeynmt.training - Epoch  12, Step:    87600, Batch Loss:     2.279918, Tokens per Sec:     7579, Lr: 0.000300\n",
      "2021-07-19 08:48:40,251 - INFO - joeynmt.training - Epoch  12, Step:    87700, Batch Loss:     2.612780, Tokens per Sec:     7670, Lr: 0.000300\n",
      "2021-07-19 08:49:09,600 - INFO - joeynmt.training - Epoch  12, Step:    87800, Batch Loss:     2.600908, Tokens per Sec:     7743, Lr: 0.000300\n",
      "2021-07-19 08:49:39,046 - INFO - joeynmt.training - Epoch  12, Step:    87900, Batch Loss:     2.391170, Tokens per Sec:     7692, Lr: 0.000300\n",
      "2021-07-19 08:50:08,553 - INFO - joeynmt.training - Epoch  12, Step:    88000, Batch Loss:     2.545542, Tokens per Sec:     7626, Lr: 0.000300\n",
      "2021-07-19 08:50:38,343 - INFO - joeynmt.training - Epoch  12, Step:    88100, Batch Loss:     2.162135, Tokens per Sec:     7768, Lr: 0.000300\n",
      "2021-07-19 08:51:07,588 - INFO - joeynmt.training - Epoch  12, Step:    88200, Batch Loss:     2.646303, Tokens per Sec:     7618, Lr: 0.000300\n",
      "2021-07-19 08:51:37,056 - INFO - joeynmt.training - Epoch  12, Step:    88300, Batch Loss:     2.588193, Tokens per Sec:     7736, Lr: 0.000300\n",
      "2021-07-19 08:51:43,442 - INFO - joeynmt.training - Epoch  12: total training loss 5790.70\n",
      "2021-07-19 08:51:43,443 - INFO - joeynmt.training - EPOCH 13\n",
      "2021-07-19 08:52:06,946 - INFO - joeynmt.training - Epoch  13, Step:    88400, Batch Loss:     2.314396, Tokens per Sec:     7510, Lr: 0.000300\n",
      "2021-07-19 08:52:36,258 - INFO - joeynmt.training - Epoch  13, Step:    88500, Batch Loss:     2.217283, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-19 08:53:05,764 - INFO - joeynmt.training - Epoch  13, Step:    88600, Batch Loss:     2.363145, Tokens per Sec:     7715, Lr: 0.000300\n",
      "2021-07-19 08:53:34,957 - INFO - joeynmt.training - Epoch  13, Step:    88700, Batch Loss:     2.203974, Tokens per Sec:     7706, Lr: 0.000300\n",
      "2021-07-19 08:54:04,701 - INFO - joeynmt.training - Epoch  13, Step:    88800, Batch Loss:     2.476982, Tokens per Sec:     7701, Lr: 0.000300\n",
      "2021-07-19 08:54:34,580 - INFO - joeynmt.training - Epoch  13, Step:    88900, Batch Loss:     2.401724, Tokens per Sec:     7719, Lr: 0.000300\n",
      "2021-07-19 08:55:04,042 - INFO - joeynmt.training - Epoch  13, Step:    89000, Batch Loss:     2.458592, Tokens per Sec:     7754, Lr: 0.000300\n",
      "2021-07-19 08:55:33,624 - INFO - joeynmt.training - Epoch  13, Step:    89100, Batch Loss:     2.728800, Tokens per Sec:     7716, Lr: 0.000300\n",
      "2021-07-19 08:56:03,180 - INFO - joeynmt.training - Epoch  13, Step:    89200, Batch Loss:     2.568518, Tokens per Sec:     7646, Lr: 0.000300\n",
      "2021-07-19 08:56:32,716 - INFO - joeynmt.training - Epoch  13, Step:    89300, Batch Loss:     2.439526, Tokens per Sec:     7760, Lr: 0.000300\n",
      "2021-07-19 08:57:02,526 - INFO - joeynmt.training - Epoch  13, Step:    89400, Batch Loss:     2.143273, Tokens per Sec:     7767, Lr: 0.000300\n",
      "2021-07-19 08:57:32,008 - INFO - joeynmt.training - Epoch  13, Step:    89500, Batch Loss:     2.674571, Tokens per Sec:     7634, Lr: 0.000300\n",
      "2021-07-19 08:58:01,600 - INFO - joeynmt.training - Epoch  13, Step:    89600, Batch Loss:     2.487332, Tokens per Sec:     7735, Lr: 0.000300\n",
      "2021-07-19 08:58:30,921 - INFO - joeynmt.training - Epoch  13, Step:    89700, Batch Loss:     2.485613, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 08:59:00,259 - INFO - joeynmt.training - Epoch  13, Step:    89800, Batch Loss:     2.523307, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 08:59:29,799 - INFO - joeynmt.training - Epoch  13, Step:    89900, Batch Loss:     2.534072, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 08:59:59,466 - INFO - joeynmt.training - Epoch  13, Step:    90000, Batch Loss:     2.421108, Tokens per Sec:     7694, Lr: 0.000300\n",
      "2021-07-19 09:00:54,513 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:00:54,514 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:00:54,514 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:00:54,915 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 09:00:54,915 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 09:00:55,755 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:00:55,757 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:00:55,757 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:00:55,757 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a sword , and said , “ I have come to the seven times , and I have come to you . ”\n",
      "2021-07-19 09:00:55,758 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:00:55,759 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:00:55,759 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:00:55,759 - INFO - joeynmt.training - \tHypothesis: And Martha , who had heard the voice of Mary , and Mary , and said to her , “ Teacher , and we have heard it . ”\n",
      "2021-07-19 09:00:55,759 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:00:55,760 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:00:55,761 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:00:55,761 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the Sabbath , and he was a day .\n",
      "2021-07-19 09:00:55,761 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:00:55,762 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:00:55,762 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:00:55,762 - INFO - joeynmt.training - \tHypothesis: Then Paul gave them the water , and when he was cut off , he was raised up , and was raised up in the head of the sea , and the bread of the sea .\n",
      "2021-07-19 09:00:55,763 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    90000: bleu:   7.14, loss: 88830.9219, ppl:  12.6245, duration: 56.2963s\n",
      "2021-07-19 09:01:25,313 - INFO - joeynmt.training - Epoch  13, Step:    90100, Batch Loss:     2.616355, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-19 09:01:54,947 - INFO - joeynmt.training - Epoch  13, Step:    90200, Batch Loss:     2.246246, Tokens per Sec:     7643, Lr: 0.000300\n",
      "2021-07-19 09:02:24,375 - INFO - joeynmt.training - Epoch  13, Step:    90300, Batch Loss:     2.156032, Tokens per Sec:     7757, Lr: 0.000300\n",
      "2021-07-19 09:02:53,793 - INFO - joeynmt.training - Epoch  13, Step:    90400, Batch Loss:     2.754822, Tokens per Sec:     7727, Lr: 0.000300\n",
      "2021-07-19 09:03:23,562 - INFO - joeynmt.training - Epoch  13, Step:    90500, Batch Loss:     2.005337, Tokens per Sec:     7820, Lr: 0.000300\n",
      "2021-07-19 09:03:52,936 - INFO - joeynmt.training - Epoch  13, Step:    90600, Batch Loss:     2.570472, Tokens per Sec:     7612, Lr: 0.000300\n",
      "2021-07-19 09:04:05,944 - INFO - joeynmt.training - Epoch  13: total training loss 5778.03\n",
      "2021-07-19 09:04:05,944 - INFO - joeynmt.training - EPOCH 14\n",
      "2021-07-19 09:04:22,426 - INFO - joeynmt.training - Epoch  14, Step:    90700, Batch Loss:     1.827927, Tokens per Sec:     7504, Lr: 0.000300\n",
      "2021-07-19 09:04:52,030 - INFO - joeynmt.training - Epoch  14, Step:    90800, Batch Loss:     2.638566, Tokens per Sec:     7738, Lr: 0.000300\n",
      "2021-07-19 09:05:21,456 - INFO - joeynmt.training - Epoch  14, Step:    90900, Batch Loss:     2.480012, Tokens per Sec:     7670, Lr: 0.000300\n",
      "2021-07-19 09:05:51,174 - INFO - joeynmt.training - Epoch  14, Step:    91000, Batch Loss:     2.593673, Tokens per Sec:     7679, Lr: 0.000300\n",
      "2021-07-19 09:06:20,602 - INFO - joeynmt.training - Epoch  14, Step:    91100, Batch Loss:     2.684629, Tokens per Sec:     7600, Lr: 0.000300\n",
      "2021-07-19 09:06:50,306 - INFO - joeynmt.training - Epoch  14, Step:    91200, Batch Loss:     2.686171, Tokens per Sec:     7665, Lr: 0.000300\n",
      "2021-07-19 09:07:19,737 - INFO - joeynmt.training - Epoch  14, Step:    91300, Batch Loss:     2.501434, Tokens per Sec:     7739, Lr: 0.000300\n",
      "2021-07-19 09:07:49,198 - INFO - joeynmt.training - Epoch  14, Step:    91400, Batch Loss:     2.388778, Tokens per Sec:     7719, Lr: 0.000300\n",
      "2021-07-19 09:08:18,629 - INFO - joeynmt.training - Epoch  14, Step:    91500, Batch Loss:     2.783267, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 09:08:48,472 - INFO - joeynmt.training - Epoch  14, Step:    91600, Batch Loss:     2.309846, Tokens per Sec:     7794, Lr: 0.000300\n",
      "2021-07-19 09:09:18,254 - INFO - joeynmt.training - Epoch  14, Step:    91700, Batch Loss:     2.797705, Tokens per Sec:     7681, Lr: 0.000300\n",
      "2021-07-19 09:09:47,834 - INFO - joeynmt.training - Epoch  14, Step:    91800, Batch Loss:     2.598845, Tokens per Sec:     7723, Lr: 0.000300\n",
      "2021-07-19 09:10:17,203 - INFO - joeynmt.training - Epoch  14, Step:    91900, Batch Loss:     2.432075, Tokens per Sec:     7609, Lr: 0.000300\n",
      "2021-07-19 09:10:46,870 - INFO - joeynmt.training - Epoch  14, Step:    92000, Batch Loss:     2.473405, Tokens per Sec:     7701, Lr: 0.000300\n",
      "2021-07-19 09:11:16,605 - INFO - joeynmt.training - Epoch  14, Step:    92100, Batch Loss:     2.517340, Tokens per Sec:     7730, Lr: 0.000300\n",
      "2021-07-19 09:11:46,100 - INFO - joeynmt.training - Epoch  14, Step:    92200, Batch Loss:     2.201544, Tokens per Sec:     7679, Lr: 0.000300\n",
      "2021-07-19 09:12:15,551 - INFO - joeynmt.training - Epoch  14, Step:    92300, Batch Loss:     2.372685, Tokens per Sec:     7658, Lr: 0.000300\n",
      "2021-07-19 09:12:45,114 - INFO - joeynmt.training - Epoch  14, Step:    92400, Batch Loss:     2.734512, Tokens per Sec:     7671, Lr: 0.000300\n",
      "2021-07-19 09:13:14,666 - INFO - joeynmt.training - Epoch  14, Step:    92500, Batch Loss:     2.392317, Tokens per Sec:     7575, Lr: 0.000300\n",
      "2021-07-19 09:14:12,239 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:14:12,239 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:14:12,239 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:14:13,453 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:14:13,456 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:14:13,456 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:14:13,456 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees saw the man who had seen him , and said to them , “ I saw the scribes and the Pharisees and the scribes . ”\n",
      "2021-07-19 09:14:13,456 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:14:13,457 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:14:13,457 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:14:13,457 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary came to Mary , and said to her , “ Mary , and her mother , and her mother , and her mother . ”\n",
      "2021-07-19 09:14:13,458 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:14:13,458 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:14:13,459 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:14:13,459 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a day of distress .\n",
      "2021-07-19 09:14:13,459 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:14:13,460 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:14:13,460 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:14:13,460 - INFO - joeynmt.training - \tHypothesis: Paul also looked for the sower , and when he was sleeping , he was raised up , and he was raised up and sleep on the road .\n",
      "2021-07-19 09:14:13,460 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    92500: bleu:   6.74, loss: 89991.4688, ppl:  13.0497, duration: 58.7936s\n",
      "2021-07-19 09:14:43,079 - INFO - joeynmt.training - Epoch  14, Step:    92600, Batch Loss:     2.434109, Tokens per Sec:     7621, Lr: 0.000300\n",
      "2021-07-19 09:15:12,635 - INFO - joeynmt.training - Epoch  14, Step:    92700, Batch Loss:     2.209276, Tokens per Sec:     7537, Lr: 0.000300\n",
      "2021-07-19 09:15:42,209 - INFO - joeynmt.training - Epoch  14, Step:    92800, Batch Loss:     2.002571, Tokens per Sec:     7598, Lr: 0.000300\n",
      "2021-07-19 09:16:11,576 - INFO - joeynmt.training - Epoch  14, Step:    92900, Batch Loss:     2.627333, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 09:16:33,424 - INFO - joeynmt.training - Epoch  14: total training loss 5783.18\n",
      "2021-07-19 09:16:33,425 - INFO - joeynmt.training - EPOCH 15\n",
      "2021-07-19 09:16:41,515 - INFO - joeynmt.training - Epoch  15, Step:    93000, Batch Loss:     2.239525, Tokens per Sec:     7344, Lr: 0.000300\n",
      "2021-07-19 09:17:11,090 - INFO - joeynmt.training - Epoch  15, Step:    93100, Batch Loss:     2.610316, Tokens per Sec:     7575, Lr: 0.000300\n",
      "2021-07-19 09:17:40,847 - INFO - joeynmt.training - Epoch  15, Step:    93200, Batch Loss:     2.544096, Tokens per Sec:     7666, Lr: 0.000300\n",
      "2021-07-19 09:18:10,380 - INFO - joeynmt.training - Epoch  15, Step:    93300, Batch Loss:     2.695454, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-19 09:18:40,151 - INFO - joeynmt.training - Epoch  15, Step:    93400, Batch Loss:     2.518601, Tokens per Sec:     7708, Lr: 0.000300\n",
      "2021-07-19 09:19:09,951 - INFO - joeynmt.training - Epoch  15, Step:    93500, Batch Loss:     2.473676, Tokens per Sec:     7742, Lr: 0.000300\n",
      "2021-07-19 09:19:39,749 - INFO - joeynmt.training - Epoch  15, Step:    93600, Batch Loss:     2.721448, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 09:20:09,284 - INFO - joeynmt.training - Epoch  15, Step:    93700, Batch Loss:     2.521717, Tokens per Sec:     7629, Lr: 0.000300\n",
      "2021-07-19 09:20:39,013 - INFO - joeynmt.training - Epoch  15, Step:    93800, Batch Loss:     2.540029, Tokens per Sec:     7588, Lr: 0.000300\n",
      "2021-07-19 09:21:08,631 - INFO - joeynmt.training - Epoch  15, Step:    93900, Batch Loss:     2.674350, Tokens per Sec:     7709, Lr: 0.000300\n",
      "2021-07-19 09:21:38,498 - INFO - joeynmt.training - Epoch  15, Step:    94000, Batch Loss:     2.589412, Tokens per Sec:     7743, Lr: 0.000300\n",
      "2021-07-19 09:22:08,129 - INFO - joeynmt.training - Epoch  15, Step:    94100, Batch Loss:     2.448942, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-07-19 09:22:37,864 - INFO - joeynmt.training - Epoch  15, Step:    94200, Batch Loss:     2.557356, Tokens per Sec:     7671, Lr: 0.000300\n",
      "2021-07-19 09:23:07,457 - INFO - joeynmt.training - Epoch  15, Step:    94300, Batch Loss:     2.242952, Tokens per Sec:     7673, Lr: 0.000300\n",
      "2021-07-19 09:23:36,676 - INFO - joeynmt.training - Epoch  15, Step:    94400, Batch Loss:     2.513350, Tokens per Sec:     7685, Lr: 0.000300\n",
      "2021-07-19 09:24:06,469 - INFO - joeynmt.training - Epoch  15, Step:    94500, Batch Loss:     2.170829, Tokens per Sec:     7706, Lr: 0.000300\n",
      "2021-07-19 09:24:35,951 - INFO - joeynmt.training - Epoch  15, Step:    94600, Batch Loss:     2.481086, Tokens per Sec:     7587, Lr: 0.000300\n",
      "2021-07-19 09:25:05,115 - INFO - joeynmt.training - Epoch  15, Step:    94700, Batch Loss:     2.446076, Tokens per Sec:     7603, Lr: 0.000300\n",
      "2021-07-19 09:25:34,601 - INFO - joeynmt.training - Epoch  15, Step:    94800, Batch Loss:     2.369297, Tokens per Sec:     7625, Lr: 0.000300\n",
      "2021-07-19 09:26:04,223 - INFO - joeynmt.training - Epoch  15, Step:    94900, Batch Loss:     2.519997, Tokens per Sec:     7728, Lr: 0.000300\n",
      "2021-07-19 09:26:34,042 - INFO - joeynmt.training - Epoch  15, Step:    95000, Batch Loss:     2.376582, Tokens per Sec:     7789, Lr: 0.000300\n",
      "2021-07-19 09:27:28,938 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:27:28,939 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:27:28,939 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:27:29,373 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-19 09:27:29,374 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-19 09:27:30,690 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:27:30,691 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:27:30,691 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:27:30,692 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a sword , and said to them , “ I have come to the twelve and come to the tomb . ”\n",
      "2021-07-19 09:27:30,692 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:27:30,692 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:27:30,693 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:27:30,693 - INFO - joeynmt.training - \tHypothesis: Now when Martha had heard these things , Mary and Mary said to Mary , “ Mary , and the Lord , and the Lord , and He was coming to Him . ”\n",
      "2021-07-19 09:27:30,693 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:27:30,694 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:27:30,694 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:27:30,695 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the day of his day .\n",
      "2021-07-19 09:27:30,695 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:27:30,695 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:27:30,696 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:27:30,696 - INFO - joeynmt.training - \tHypothesis: Then he had sat down the sea , and when He was cast , he was cut off , and the sea of the sea , and the sea was cut down .\n",
      "2021-07-19 09:27:30,696 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    95000: bleu:   7.33, loss: 88346.5078, ppl:  12.4511, duration: 56.6532s\n",
      "2021-07-19 09:28:00,143 - INFO - joeynmt.training - Epoch  15, Step:    95100, Batch Loss:     2.509693, Tokens per Sec:     7554, Lr: 0.000300\n",
      "2021-07-19 09:28:29,504 - INFO - joeynmt.training - Epoch  15, Step:    95200, Batch Loss:     2.372368, Tokens per Sec:     7639, Lr: 0.000300\n",
      "2021-07-19 09:28:59,153 - INFO - joeynmt.training - Epoch  15, Step:    95300, Batch Loss:     2.414425, Tokens per Sec:     7632, Lr: 0.000300\n",
      "2021-07-19 09:28:59,492 - INFO - joeynmt.training - Epoch  15: total training loss 5767.28\n",
      "2021-07-19 09:28:59,492 - INFO - joeynmt.training - EPOCH 16\n",
      "2021-07-19 09:29:29,245 - INFO - joeynmt.training - Epoch  16, Step:    95400, Batch Loss:     2.400470, Tokens per Sec:     7576, Lr: 0.000300\n",
      "2021-07-19 09:29:58,834 - INFO - joeynmt.training - Epoch  16, Step:    95500, Batch Loss:     2.258091, Tokens per Sec:     7607, Lr: 0.000300\n",
      "2021-07-19 09:30:28,613 - INFO - joeynmt.training - Epoch  16, Step:    95600, Batch Loss:     2.302876, Tokens per Sec:     7749, Lr: 0.000300\n",
      "2021-07-19 09:30:58,501 - INFO - joeynmt.training - Epoch  16, Step:    95700, Batch Loss:     2.626441, Tokens per Sec:     7757, Lr: 0.000300\n",
      "2021-07-19 09:31:28,073 - INFO - joeynmt.training - Epoch  16, Step:    95800, Batch Loss:     2.365883, Tokens per Sec:     7721, Lr: 0.000300\n",
      "2021-07-19 09:31:57,986 - INFO - joeynmt.training - Epoch  16, Step:    95900, Batch Loss:     2.765775, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 09:32:27,567 - INFO - joeynmt.training - Epoch  16, Step:    96000, Batch Loss:     2.629493, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-19 09:32:57,113 - INFO - joeynmt.training - Epoch  16, Step:    96100, Batch Loss:     2.107132, Tokens per Sec:     7677, Lr: 0.000300\n",
      "2021-07-19 09:33:26,728 - INFO - joeynmt.training - Epoch  16, Step:    96200, Batch Loss:     2.085318, Tokens per Sec:     7668, Lr: 0.000300\n",
      "2021-07-19 09:33:56,124 - INFO - joeynmt.training - Epoch  16, Step:    96300, Batch Loss:     2.599802, Tokens per Sec:     7695, Lr: 0.000300\n",
      "2021-07-19 09:34:25,747 - INFO - joeynmt.training - Epoch  16, Step:    96400, Batch Loss:     2.388417, Tokens per Sec:     7599, Lr: 0.000300\n",
      "2021-07-19 09:34:55,352 - INFO - joeynmt.training - Epoch  16, Step:    96500, Batch Loss:     2.159601, Tokens per Sec:     7628, Lr: 0.000300\n",
      "2021-07-19 09:35:25,124 - INFO - joeynmt.training - Epoch  16, Step:    96600, Batch Loss:     2.593119, Tokens per Sec:     7646, Lr: 0.000300\n",
      "2021-07-19 09:35:54,750 - INFO - joeynmt.training - Epoch  16, Step:    96700, Batch Loss:     2.707765, Tokens per Sec:     7630, Lr: 0.000300\n",
      "2021-07-19 09:36:24,382 - INFO - joeynmt.training - Epoch  16, Step:    96800, Batch Loss:     2.530856, Tokens per Sec:     7617, Lr: 0.000300\n",
      "2021-07-19 09:36:54,142 - INFO - joeynmt.training - Epoch  16, Step:    96900, Batch Loss:     2.704400, Tokens per Sec:     7718, Lr: 0.000300\n",
      "2021-07-19 09:37:24,194 - INFO - joeynmt.training - Epoch  16, Step:    97000, Batch Loss:     2.550211, Tokens per Sec:     7569, Lr: 0.000300\n",
      "2021-07-19 09:37:53,879 - INFO - joeynmt.training - Epoch  16, Step:    97100, Batch Loss:     2.548666, Tokens per Sec:     7637, Lr: 0.000300\n",
      "2021-07-19 09:38:23,521 - INFO - joeynmt.training - Epoch  16, Step:    97200, Batch Loss:     2.396454, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 09:38:53,317 - INFO - joeynmt.training - Epoch  16, Step:    97300, Batch Loss:     2.479393, Tokens per Sec:     7769, Lr: 0.000300\n",
      "2021-07-19 09:39:22,806 - INFO - joeynmt.training - Epoch  16, Step:    97400, Batch Loss:     2.021215, Tokens per Sec:     7690, Lr: 0.000300\n",
      "2021-07-19 09:39:52,524 - INFO - joeynmt.training - Epoch  16, Step:    97500, Batch Loss:     2.605484, Tokens per Sec:     7616, Lr: 0.000300\n",
      "2021-07-19 09:40:49,447 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:40:49,447 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:40:49,447 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:40:50,635 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:40:50,636 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:40:50,636 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:40:50,636 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He who has seen me , and he has seen me , and he has seen me , and I have come to see my house . ”\n",
      "2021-07-19 09:40:50,636 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:40:50,637 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:40:50,637 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:40:50,637 - INFO - joeynmt.training - \tHypothesis: Then Martha had heard that she had heard her , and she said to her , “ See , and the Lord , and the Lord , and the Lord , and the Lord has come . ”\n",
      "2021-07-19 09:40:50,638 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:40:50,638 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:40:50,638 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:40:50,639 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the Sabbath .\n",
      "2021-07-19 09:40:50,639 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:40:50,640 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:40:50,640 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:40:50,640 - INFO - joeynmt.training - \tHypothesis: Paul also had a full full share in the field when he was on the road , and he was a brief car , and he was a brief car .\n",
      "2021-07-19 09:40:50,640 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    97500: bleu:   6.85, loss: 89012.3984, ppl:  12.6900, duration: 58.1158s\n",
      "2021-07-19 09:41:20,327 - INFO - joeynmt.training - Epoch  16, Step:    97600, Batch Loss:     2.474289, Tokens per Sec:     7569, Lr: 0.000300\n",
      "2021-07-19 09:41:26,988 - INFO - joeynmt.training - Epoch  16: total training loss 5745.07\n",
      "2021-07-19 09:41:26,989 - INFO - joeynmt.training - EPOCH 17\n",
      "2021-07-19 09:41:49,866 - INFO - joeynmt.training - Epoch  17, Step:    97700, Batch Loss:     2.487382, Tokens per Sec:     7561, Lr: 0.000300\n",
      "2021-07-19 09:42:19,359 - INFO - joeynmt.training - Epoch  17, Step:    97800, Batch Loss:     2.204356, Tokens per Sec:     7641, Lr: 0.000300\n",
      "2021-07-19 09:42:49,233 - INFO - joeynmt.training - Epoch  17, Step:    97900, Batch Loss:     2.458670, Tokens per Sec:     7830, Lr: 0.000300\n",
      "2021-07-19 09:43:18,850 - INFO - joeynmt.training - Epoch  17, Step:    98000, Batch Loss:     2.601521, Tokens per Sec:     7586, Lr: 0.000300\n",
      "2021-07-19 09:43:48,638 - INFO - joeynmt.training - Epoch  17, Step:    98100, Batch Loss:     2.450954, Tokens per Sec:     7652, Lr: 0.000300\n",
      "2021-07-19 09:44:18,371 - INFO - joeynmt.training - Epoch  17, Step:    98200, Batch Loss:     2.485296, Tokens per Sec:     7657, Lr: 0.000300\n",
      "2021-07-19 09:44:47,738 - INFO - joeynmt.training - Epoch  17, Step:    98300, Batch Loss:     2.495255, Tokens per Sec:     7549, Lr: 0.000300\n",
      "2021-07-19 09:45:17,489 - INFO - joeynmt.training - Epoch  17, Step:    98400, Batch Loss:     2.449237, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 09:45:47,362 - INFO - joeynmt.training - Epoch  17, Step:    98500, Batch Loss:     2.124894, Tokens per Sec:     7759, Lr: 0.000300\n",
      "2021-07-19 09:46:17,085 - INFO - joeynmt.training - Epoch  17, Step:    98600, Batch Loss:     2.476737, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 09:46:46,711 - INFO - joeynmt.training - Epoch  17, Step:    98700, Batch Loss:     1.896112, Tokens per Sec:     7676, Lr: 0.000300\n",
      "2021-07-19 09:47:16,478 - INFO - joeynmt.training - Epoch  17, Step:    98800, Batch Loss:     2.451104, Tokens per Sec:     7641, Lr: 0.000300\n",
      "2021-07-19 09:47:46,206 - INFO - joeynmt.training - Epoch  17, Step:    98900, Batch Loss:     2.654869, Tokens per Sec:     7653, Lr: 0.000300\n",
      "2021-07-19 09:48:15,994 - INFO - joeynmt.training - Epoch  17, Step:    99000, Batch Loss:     2.186166, Tokens per Sec:     7722, Lr: 0.000300\n",
      "2021-07-19 09:48:45,568 - INFO - joeynmt.training - Epoch  17, Step:    99100, Batch Loss:     2.512928, Tokens per Sec:     7688, Lr: 0.000300\n",
      "2021-07-19 09:49:15,211 - INFO - joeynmt.training - Epoch  17, Step:    99200, Batch Loss:     2.553317, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 09:49:44,787 - INFO - joeynmt.training - Epoch  17, Step:    99300, Batch Loss:     2.547517, Tokens per Sec:     7689, Lr: 0.000300\n",
      "2021-07-19 09:50:14,120 - INFO - joeynmt.training - Epoch  17, Step:    99400, Batch Loss:     2.581047, Tokens per Sec:     7441, Lr: 0.000300\n",
      "2021-07-19 09:50:43,964 - INFO - joeynmt.training - Epoch  17, Step:    99500, Batch Loss:     2.734441, Tokens per Sec:     7638, Lr: 0.000300\n",
      "2021-07-19 09:51:13,372 - INFO - joeynmt.training - Epoch  17, Step:    99600, Batch Loss:     2.587189, Tokens per Sec:     7710, Lr: 0.000300\n",
      "2021-07-19 09:51:43,164 - INFO - joeynmt.training - Epoch  17, Step:    99700, Batch Loss:     2.406415, Tokens per Sec:     7597, Lr: 0.000300\n",
      "2021-07-19 09:52:12,840 - INFO - joeynmt.training - Epoch  17, Step:    99800, Batch Loss:     2.209472, Tokens per Sec:     7642, Lr: 0.000300\n",
      "2021-07-19 09:52:42,323 - INFO - joeynmt.training - Epoch  17, Step:    99900, Batch Loss:     2.519387, Tokens per Sec:     7755, Lr: 0.000300\n",
      "2021-07-19 09:52:56,499 - INFO - joeynmt.training - Epoch  17: total training loss 5744.76\n",
      "2021-07-19 09:52:56,499 - INFO - joeynmt.training - EPOCH 18\n",
      "2021-07-19 09:53:12,262 - INFO - joeynmt.training - Epoch  18, Step:   100000, Batch Loss:     2.393476, Tokens per Sec:     7527, Lr: 0.000300\n",
      "2021-07-19 09:54:10,470 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 09:54:10,470 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 09:54:10,470 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 09:54:11,718 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 09:54:11,719 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 09:54:11,720 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 09:54:11,720 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a man , saying , “ I have come to the scribes and the scribes and the scribes and the scribes and the scribes . ”\n",
      "2021-07-19 09:54:11,720 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 09:54:11,721 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 09:54:11,721 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 09:54:11,721 - INFO - joeynmt.training - \tHypothesis: Now Martha , who had heard the voice of Mary , and Martha , said to Mary , “ See , and the Lord , and you are going to see him . ”\n",
      "2021-07-19 09:54:11,722 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 09:54:11,722 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 09:54:11,722 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 09:54:11,723 - INFO - joeynmt.training - \tHypothesis: He was a day of service in the day of the day of the end .\n",
      "2021-07-19 09:54:11,723 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 09:54:11,724 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 09:54:11,724 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 09:54:11,724 - INFO - joeynmt.training - \tHypothesis: Paul also had sound reasons for the sea , and when he was cast , he was cut off from the sea , and the fire was cut down .\n",
      "2021-07-19 09:54:11,724 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   100000: bleu:   6.88, loss: 88687.5234, ppl:  12.5729, duration: 59.4615s\n",
      "2021-07-19 09:54:41,490 - INFO - joeynmt.training - Epoch  18, Step:   100100, Batch Loss:     2.323984, Tokens per Sec:     7548, Lr: 0.000300\n",
      "2021-07-19 09:55:11,146 - INFO - joeynmt.training - Epoch  18, Step:   100200, Batch Loss:     2.231801, Tokens per Sec:     7691, Lr: 0.000300\n",
      "2021-07-19 09:55:40,915 - INFO - joeynmt.training - Epoch  18, Step:   100300, Batch Loss:     2.398235, Tokens per Sec:     7692, Lr: 0.000300\n",
      "2021-07-19 09:56:10,578 - INFO - joeynmt.training - Epoch  18, Step:   100400, Batch Loss:     2.799313, Tokens per Sec:     7627, Lr: 0.000300\n",
      "2021-07-19 09:56:40,021 - INFO - joeynmt.training - Epoch  18, Step:   100500, Batch Loss:     2.515724, Tokens per Sec:     7617, Lr: 0.000300\n",
      "2021-07-19 09:57:09,510 - INFO - joeynmt.training - Epoch  18, Step:   100600, Batch Loss:     2.649131, Tokens per Sec:     7588, Lr: 0.000300\n",
      "2021-07-19 09:57:38,953 - INFO - joeynmt.training - Epoch  18, Step:   100700, Batch Loss:     2.241764, Tokens per Sec:     7574, Lr: 0.000300\n",
      "2021-07-19 09:58:08,706 - INFO - joeynmt.training - Epoch  18, Step:   100800, Batch Loss:     2.530624, Tokens per Sec:     7672, Lr: 0.000300\n",
      "2021-07-19 09:58:38,928 - INFO - joeynmt.training - Epoch  18, Step:   100900, Batch Loss:     2.456237, Tokens per Sec:     7752, Lr: 0.000300\n",
      "2021-07-19 09:59:08,323 - INFO - joeynmt.training - Epoch  18, Step:   101000, Batch Loss:     2.249847, Tokens per Sec:     7663, Lr: 0.000300\n",
      "2021-07-19 09:59:38,082 - INFO - joeynmt.training - Epoch  18, Step:   101100, Batch Loss:     2.606418, Tokens per Sec:     7699, Lr: 0.000300\n",
      "2021-07-19 10:00:07,760 - INFO - joeynmt.training - Epoch  18, Step:   101200, Batch Loss:     2.375940, Tokens per Sec:     7601, Lr: 0.000300\n",
      "2021-07-19 10:00:37,124 - INFO - joeynmt.training - Epoch  18, Step:   101300, Batch Loss:     2.274651, Tokens per Sec:     7563, Lr: 0.000300\n",
      "2021-07-19 10:01:06,651 - INFO - joeynmt.training - Epoch  18, Step:   101400, Batch Loss:     2.506844, Tokens per Sec:     7744, Lr: 0.000300\n",
      "2021-07-19 10:01:36,430 - INFO - joeynmt.training - Epoch  18, Step:   101500, Batch Loss:     2.397336, Tokens per Sec:     7660, Lr: 0.000300\n",
      "2021-07-19 10:02:06,013 - INFO - joeynmt.training - Epoch  18, Step:   101600, Batch Loss:     2.388252, Tokens per Sec:     7647, Lr: 0.000300\n",
      "2021-07-19 10:02:35,637 - INFO - joeynmt.training - Epoch  18, Step:   101700, Batch Loss:     2.460960, Tokens per Sec:     7721, Lr: 0.000300\n",
      "2021-07-19 10:03:05,420 - INFO - joeynmt.training - Epoch  18, Step:   101800, Batch Loss:     2.351370, Tokens per Sec:     7746, Lr: 0.000300\n",
      "2021-07-19 10:03:34,965 - INFO - joeynmt.training - Epoch  18, Step:   101900, Batch Loss:     2.537027, Tokens per Sec:     7633, Lr: 0.000300\n",
      "2021-07-19 10:04:04,680 - INFO - joeynmt.training - Epoch  18, Step:   102000, Batch Loss:     2.779261, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 10:04:34,406 - INFO - joeynmt.training - Epoch  18, Step:   102100, Batch Loss:     2.597404, Tokens per Sec:     7655, Lr: 0.000300\n",
      "2021-07-19 10:05:04,108 - INFO - joeynmt.training - Epoch  18, Step:   102200, Batch Loss:     2.613183, Tokens per Sec:     7684, Lr: 0.000300\n",
      "2021-07-19 10:05:25,292 - INFO - joeynmt.training - Epoch  18: total training loss 5732.58\n",
      "2021-07-19 10:05:25,293 - INFO - joeynmt.training - EPOCH 19\n",
      "2021-07-19 10:05:33,895 - INFO - joeynmt.training - Epoch  19, Step:   102300, Batch Loss:     2.619388, Tokens per Sec:     7247, Lr: 0.000300\n",
      "2021-07-19 10:06:03,732 - INFO - joeynmt.training - Epoch  19, Step:   102400, Batch Loss:     2.434654, Tokens per Sec:     7686, Lr: 0.000300\n",
      "2021-07-19 10:06:33,734 - INFO - joeynmt.training - Epoch  19, Step:   102500, Batch Loss:     2.279320, Tokens per Sec:     7720, Lr: 0.000300\n",
      "2021-07-19 10:07:37,379 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-19 10:07:37,380 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-19 10:07:37,380 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-19 10:07:38,631 - INFO - joeynmt.training - Example #0\n",
      "2021-07-19 10:07:38,632 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-19 10:07:38,632 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-19 10:07:38,632 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw him as He said to them , “ I have come to the seven hundred and fell into the tomb , and I have come to you . ”\n",
      "2021-07-19 10:07:38,633 - INFO - joeynmt.training - Example #1\n",
      "2021-07-19 10:07:38,633 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-19 10:07:38,634 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-19 10:07:38,634 - INFO - joeynmt.training - \tHypothesis: When Martha was a man , Mary and Mary , and Mary , and Mary , and Mary said to her , “ See , and you are going to know where she was going . ”\n",
      "2021-07-19 10:07:38,634 - INFO - joeynmt.training - Example #2\n",
      "2021-07-19 10:07:38,635 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-19 10:07:38,635 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-19 10:07:38,635 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the Sabbath , and he was a day .\n",
      "2021-07-19 10:07:38,636 - INFO - joeynmt.training - Example #3\n",
      "2021-07-19 10:07:38,636 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-19 10:07:38,636 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-19 10:07:38,637 - INFO - joeynmt.training - \tHypothesis: Paul also looked for the sake of the sea , and when he was cut off , he was a fire of the sea , and the sea was cut off the road .\n",
      "2021-07-19 10:07:38,637 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   102500: bleu:   6.56, loss: 89562.3203, ppl:  12.8908, duration: 64.9026s\n",
      "2021-07-19 10:08:08,361 - INFO - joeynmt.training - Epoch  19, Step:   102600, Batch Loss:     2.602778, Tokens per Sec:     7550, Lr: 0.000300\n",
      "2021-07-19 10:08:37,791 - INFO - joeynmt.training - Epoch  19, Step:   102700, Batch Loss:     2.351069, Tokens per Sec:     7582, Lr: 0.000300\n",
      "2021-07-19 10:09:07,441 - INFO - joeynmt.training - Epoch  19, Step:   102800, Batch Loss:     2.479653, Tokens per Sec:     7694, Lr: 0.000300\n",
      "2021-07-19 10:09:37,309 - INFO - joeynmt.training - Epoch  19, Step:   102900, Batch Loss:     2.525683, Tokens per Sec:     7674, Lr: 0.000300\n",
      "2021-07-19 10:10:07,216 - INFO - joeynmt.training - Epoch  19, Step:   103000, Batch Loss:     2.477801, Tokens per Sec:     7783, Lr: 0.000300\n",
      "2021-07-19 10:10:36,563 - INFO - joeynmt.training - Epoch  19, Step:   103100, Batch Loss:     2.384779, Tokens per Sec:     7600, Lr: 0.000300\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/back_transformer_reverse_lhen_reload2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFztkp8YeODE"
   },
   "outputs": [],
   "source": [
    "# Reloading configuration file\n",
    "ckpt_number = 102500\n",
    "reload_config = config.replace(\n",
    "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/models/lhen_transformer/1.ckpt\"', \n",
    "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_{name}_reverse_transformer_continued2/{ckpt_number}.ckpt\"').replace(\n",
    "        f'model_dir: \"models/back_lhen_reverse_transformer\"', f'model_dir: \"models/back_lhen_reverse_transformer_continued3\"').replace(\n",
    "            f'validation_freq: 5000', f'validation_freq: 2500').replace(\n",
    "            f'epochs: 30', f'epochs: 11')\n",
    "with open(\"joeynmt/configs/back_transformer_reverse_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
    "    f.write(reload_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "aY_QjWww2lS0",
    "outputId": "8e43604c-b8b3-4a71-c611-9b0776a98ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: \"lhen_reverse_transformer\"\n",
      "\n",
      "data:\n",
      "    src: \"lh\"\n",
      "    trg: \"en\"\n",
      "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\"\n",
      "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\"\n",
      "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\"\n",
      "    level: \"bpe\"\n",
      "    lowercase: False\n",
      "    max_sent_length: 100\n",
      "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\"\n",
      "\n",
      "testing:\n",
      "    beam_size: 5\n",
      "    alpha: 1.0\n",
      "\n",
      "training:\n",
      "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued2/102500.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
      "    random_seed: 42\n",
      "    optimizer: \"adam\"\n",
      "    normalization: \"tokens\"\n",
      "    adam_betas: [0.9, 0.999] \n",
      "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
      "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
      "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
      "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
      "    decrease_factor: 0.7\n",
      "    loss: \"crossentropy\"\n",
      "    learning_rate: 0.0003\n",
      "    learning_rate_min: 0.00000001\n",
      "    weight_decay: 0.0\n",
      "    label_smoothing: 0.1\n",
      "    batch_size: 4096\n",
      "    batch_type: \"token\"\n",
      "    eval_batch_size: 1600\n",
      "    eval_batch_type: \"token\"\n",
      "    batch_multiplier: 1\n",
      "    early_stopping_metric: \"ppl\"\n",
      "    epochs: 11                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
      "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
      "    logging_freq: 100\n",
      "    eval_metric: \"bleu\"\n",
      "    model_dir: \"models/back_lhen_reverse_transformer_continued3\"\n",
      "    overwrite: False              # TODO: Set to True if you want to overwrite possibly existing models. \n",
      "    shuffle: True\n",
      "    use_cuda: True\n",
      "    max_output_length: 100\n",
      "    print_valid_sents: [0, 1, 2, 3]\n",
      "    keep_last_ckpts: 3\n",
      "\n",
      "model:\n",
      "    initializer: \"xavier\"\n",
      "    bias_initializer: \"zeros\"\n",
      "    init_gain: 1.0\n",
      "    embed_initializer: \"xavier\"\n",
      "    embed_init_gain: 1.0\n",
      "    tied_embeddings: True\n",
      "    tied_softmax: True\n",
      "    encoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n",
      "    decoder:\n",
      "        type: \"transformer\"\n",
      "        num_layers: 6\n",
      "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
      "        embeddings:\n",
      "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
      "            scale: True\n",
      "            dropout: 0.2\n",
      "        # typically ff_size = 4 x hidden_size\n",
      "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
      "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
      "        dropout: 0.3\n"
     ]
    }
   ],
   "source": [
    "!cat \"joeynmt/configs/back_transformer_reverse_lhen_reload3.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLFcAIfy2owA",
    "outputId": "18ff262d-5ec3-4f8f-cbe5-1ccace8b2d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-27 07:34:56,135 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
      "2021-07-27 07:34:56,204 - INFO - joeynmt.data - Loading training data...\n",
      "2021-07-27 07:35:00,825 - INFO - joeynmt.data - Building vocabulary...\n",
      "2021-07-27 07:35:01,381 - INFO - joeynmt.data - Loading dev data...\n",
      "2021-07-27 07:35:02,417 - INFO - joeynmt.data - Loading test data...\n",
      "2021-07-27 07:35:03,772 - INFO - joeynmt.data - Data loaded.\n",
      "2021-07-27 07:35:03,772 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-27 07:35:04,161 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-27 07:35:04.414349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-27 07:35:06,069 - INFO - joeynmt.training - Total params: 12138240\n",
      "2021-07-27 07:35:16,618 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued2/102500.ckpt\n",
      "2021-07-27 07:35:17,082 - INFO - joeynmt.helpers - cfg.name                           : lhen_reverse_transformer\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.src                       : lh\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back.bpe\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
      "2021-07-27 07:35:17,083 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/vocab2.txt\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/joeynmt/models/back_lhen_reverse_transformer_continued2/102500.ckpt\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
      "2021-07-27 07:35:17,084 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
      "2021-07-27 07:35:17,085 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1600\n",
      "2021-07-27 07:35:17,086 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.epochs                : 11\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
      "2021-07-27 07:35:17,087 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/back_lhen_reverse_transformer_continued3\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
      "2021-07-27 07:35:17,088 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
      "2021-07-27 07:35:17,089 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
      "2021-07-27 07:35:17,090 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
      "2021-07-27 07:35:17,091 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - Data set sizes: \n",
      "\ttrain 207289,\n",
      "\tvalid 1000,\n",
      "\ttest 1000\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - First training example:\n",
      "\t[SRC] P@@ il@@ a@@ to nakal@@ ukha itookho , ne nal@@ anga Yesu , namureeba , ari , “ Iwe ni@@ we omuruchi wa Abayahudi ? ”\n",
      "\t[TRG] Then P@@ il@@ ate ent@@ ered the P@@ ra@@ et@@ or@@ i@@ u@@ m again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
      "2021-07-27 07:35:17,092 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-27 07:35:17,093 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) “ (9) mbu\n",
      "2021-07-27 07:35:17,093 - INFO - joeynmt.helpers - Number of Src words (types): 4211\n",
      "2021-07-27 07:35:17,093 - INFO - joeynmt.helpers - Number of Trg words (types): 4211\n",
      "2021-07-27 07:35:17,093 - INFO - joeynmt.training - Model(\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
      "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4211),\n",
      "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4211))\n",
      "2021-07-27 07:35:17,120 - INFO - joeynmt.training - Train stats:\n",
      "\tdevice: cuda\n",
      "\tn_gpu: 1\n",
      "\t16-bits training: False\n",
      "\tgradient accumulation: 1\n",
      "\tbatch size per device: 4096\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\n",
      "2021-07-27 07:35:17,120 - INFO - joeynmt.training - EPOCH 1\n",
      "2021-07-27 07:35:30,485 - INFO - joeynmt.training - Epoch   1, Step:   102600, Batch Loss:     2.609651, Tokens per Sec:    16793, Lr: 0.000300\n",
      "2021-07-27 07:35:43,073 - INFO - joeynmt.training - Epoch   1, Step:   102700, Batch Loss:     2.340782, Tokens per Sec:    17726, Lr: 0.000300\n",
      "2021-07-27 07:35:55,822 - INFO - joeynmt.training - Epoch   1, Step:   102800, Batch Loss:     2.479663, Tokens per Sec:    17893, Lr: 0.000300\n",
      "2021-07-27 07:36:08,760 - INFO - joeynmt.training - Epoch   1, Step:   102900, Batch Loss:     2.559927, Tokens per Sec:    17716, Lr: 0.000300\n",
      "2021-07-27 07:36:21,929 - INFO - joeynmt.training - Epoch   1, Step:   103000, Batch Loss:     2.483108, Tokens per Sec:    17675, Lr: 0.000300\n",
      "2021-07-27 07:36:34,979 - INFO - joeynmt.training - Epoch   1, Step:   103100, Batch Loss:     2.374309, Tokens per Sec:    17093, Lr: 0.000300\n",
      "2021-07-27 07:36:48,146 - INFO - joeynmt.training - Epoch   1, Step:   103200, Batch Loss:     2.598075, Tokens per Sec:    17407, Lr: 0.000300\n",
      "2021-07-27 07:37:01,223 - INFO - joeynmt.training - Epoch   1, Step:   103300, Batch Loss:     2.453875, Tokens per Sec:    17314, Lr: 0.000300\n",
      "2021-07-27 07:37:14,243 - INFO - joeynmt.training - Epoch   1, Step:   103400, Batch Loss:     2.434888, Tokens per Sec:    17157, Lr: 0.000300\n",
      "2021-07-27 07:37:27,473 - INFO - joeynmt.training - Epoch   1, Step:   103500, Batch Loss:     2.692916, Tokens per Sec:    16893, Lr: 0.000300\n",
      "2021-07-27 07:37:40,924 - INFO - joeynmt.training - Epoch   1, Step:   103600, Batch Loss:     2.679719, Tokens per Sec:    17190, Lr: 0.000300\n",
      "2021-07-27 07:37:54,152 - INFO - joeynmt.training - Epoch   1, Step:   103700, Batch Loss:     2.170151, Tokens per Sec:    16982, Lr: 0.000300\n",
      "2021-07-27 07:38:07,514 - INFO - joeynmt.training - Epoch   1, Step:   103800, Batch Loss:     2.432665, Tokens per Sec:    17144, Lr: 0.000300\n",
      "2021-07-27 07:38:20,800 - INFO - joeynmt.training - Epoch   1, Step:   103900, Batch Loss:     2.363423, Tokens per Sec:    17083, Lr: 0.000300\n",
      "2021-07-27 07:38:34,285 - INFO - joeynmt.training - Epoch   1, Step:   104000, Batch Loss:     2.558351, Tokens per Sec:    16915, Lr: 0.000300\n",
      "2021-07-27 07:38:47,765 - INFO - joeynmt.training - Epoch   1, Step:   104100, Batch Loss:     2.478926, Tokens per Sec:    16722, Lr: 0.000300\n",
      "2021-07-27 07:39:01,192 - INFO - joeynmt.training - Epoch   1, Step:   104200, Batch Loss:     1.944168, Tokens per Sec:    16905, Lr: 0.000300\n",
      "2021-07-27 07:39:14,641 - INFO - joeynmt.training - Epoch   1, Step:   104300, Batch Loss:     2.703756, Tokens per Sec:    16789, Lr: 0.000300\n",
      "2021-07-27 07:39:28,042 - INFO - joeynmt.training - Epoch   1, Step:   104400, Batch Loss:     2.555315, Tokens per Sec:    16959, Lr: 0.000300\n",
      "2021-07-27 07:39:41,764 - INFO - joeynmt.training - Epoch   1, Step:   104500, Batch Loss:     2.394606, Tokens per Sec:    16850, Lr: 0.000300\n",
      "2021-07-27 07:39:54,797 - INFO - joeynmt.training - Epoch   1: total training loss 5166.13\n",
      "2021-07-27 07:39:54,798 - INFO - joeynmt.training - EPOCH 2\n",
      "2021-07-27 07:39:55,643 - INFO - joeynmt.training - Epoch   2, Step:   104600, Batch Loss:     2.137786, Tokens per Sec:    10041, Lr: 0.000300\n",
      "2021-07-27 07:40:09,400 - INFO - joeynmt.training - Epoch   2, Step:   104700, Batch Loss:     2.663298, Tokens per Sec:    16846, Lr: 0.000300\n",
      "2021-07-27 07:40:23,151 - INFO - joeynmt.training - Epoch   2, Step:   104800, Batch Loss:     2.565871, Tokens per Sec:    16572, Lr: 0.000300\n",
      "2021-07-27 07:40:36,687 - INFO - joeynmt.training - Epoch   2, Step:   104900, Batch Loss:     2.693704, Tokens per Sec:    16769, Lr: 0.000300\n",
      "2021-07-27 07:40:50,194 - INFO - joeynmt.training - Epoch   2, Step:   105000, Batch Loss:     2.621908, Tokens per Sec:    16621, Lr: 0.000300\n",
      "2021-07-27 07:41:18,340 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 07:41:18,340 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 07:41:18,340 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 07:41:19,464 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 07:41:19,465 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 07:41:19,465 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 07:41:19,465 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees said to them , “ He saw the scribes and said to them , “ I have come to the tomb and come to the tomb , and I have come to know where I have come . ”\n",
      "2021-07-27 07:41:19,465 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 07:41:19,466 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 07:41:19,466 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 07:41:19,466 - INFO - joeynmt.training - \tHypothesis: Now Martha , who had heard this , and said to Mary , “ Mary , and Mary , and Mary , and He said to Him , “ Teacher , and you are going to know Him . ”\n",
      "2021-07-27 07:41:19,466 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 07:41:19,467 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 07:41:19,467 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 07:41:19,467 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and the day was near .\n",
      "2021-07-27 07:41:19,467 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 07:41:19,468 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 07:41:19,468 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 07:41:19,468 - INFO - joeynmt.training - \tHypothesis: And when Paul had looked at the sea , He was cut off , and was cut off , and the lamp of the sea , and the lamb was cut down .\n",
      "2021-07-27 07:41:19,468 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   105000: bleu:   6.82, loss: 88786.8828, ppl:  12.6086, duration: 29.2742s\n",
      "2021-07-27 07:41:33,367 - INFO - joeynmt.training - Epoch   2, Step:   105100, Batch Loss:     2.434491, Tokens per Sec:    16282, Lr: 0.000300\n",
      "2021-07-27 07:41:46,948 - INFO - joeynmt.training - Epoch   2, Step:   105200, Batch Loss:     2.607318, Tokens per Sec:    16699, Lr: 0.000300\n",
      "2021-07-27 07:42:00,577 - INFO - joeynmt.training - Epoch   2, Step:   105300, Batch Loss:     2.078145, Tokens per Sec:    17058, Lr: 0.000300\n",
      "2021-07-27 07:42:14,159 - INFO - joeynmt.training - Epoch   2, Step:   105400, Batch Loss:     2.669848, Tokens per Sec:    16773, Lr: 0.000300\n",
      "2021-07-27 07:42:27,609 - INFO - joeynmt.training - Epoch   2, Step:   105500, Batch Loss:     2.589326, Tokens per Sec:    16701, Lr: 0.000300\n",
      "2021-07-27 07:42:41,223 - INFO - joeynmt.training - Epoch   2, Step:   105600, Batch Loss:     2.472005, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-27 07:42:54,833 - INFO - joeynmt.training - Epoch   2, Step:   105700, Batch Loss:     2.469747, Tokens per Sec:    17097, Lr: 0.000300\n",
      "2021-07-27 07:43:08,371 - INFO - joeynmt.training - Epoch   2, Step:   105800, Batch Loss:     2.285844, Tokens per Sec:    16956, Lr: 0.000300\n",
      "2021-07-27 07:43:21,750 - INFO - joeynmt.training - Epoch   2, Step:   105900, Batch Loss:     2.289291, Tokens per Sec:    16640, Lr: 0.000300\n",
      "2021-07-27 07:43:35,298 - INFO - joeynmt.training - Epoch   2, Step:   106000, Batch Loss:     2.290530, Tokens per Sec:    16402, Lr: 0.000300\n",
      "2021-07-27 07:43:48,786 - INFO - joeynmt.training - Epoch   2, Step:   106100, Batch Loss:     2.410551, Tokens per Sec:    16483, Lr: 0.000300\n",
      "2021-07-27 07:44:02,378 - INFO - joeynmt.training - Epoch   2, Step:   106200, Batch Loss:     2.623958, Tokens per Sec:    17034, Lr: 0.000300\n",
      "2021-07-27 07:44:15,896 - INFO - joeynmt.training - Epoch   2, Step:   106300, Batch Loss:     2.604288, Tokens per Sec:    17055, Lr: 0.000300\n",
      "2021-07-27 07:44:29,535 - INFO - joeynmt.training - Epoch   2, Step:   106400, Batch Loss:     2.652615, Tokens per Sec:    16843, Lr: 0.000300\n",
      "2021-07-27 07:44:43,296 - INFO - joeynmt.training - Epoch   2, Step:   106500, Batch Loss:     2.174897, Tokens per Sec:    16651, Lr: 0.000300\n",
      "2021-07-27 07:44:56,776 - INFO - joeynmt.training - Epoch   2, Step:   106600, Batch Loss:     2.476755, Tokens per Sec:    16465, Lr: 0.000300\n",
      "2021-07-27 07:45:10,526 - INFO - joeynmt.training - Epoch   2, Step:   106700, Batch Loss:     2.565251, Tokens per Sec:    16916, Lr: 0.000300\n",
      "2021-07-27 07:45:24,016 - INFO - joeynmt.training - Epoch   2, Step:   106800, Batch Loss:     2.618560, Tokens per Sec:    16957, Lr: 0.000300\n",
      "2021-07-27 07:45:37,675 - INFO - joeynmt.training - Epoch   2, Step:   106900, Batch Loss:     2.347380, Tokens per Sec:    16860, Lr: 0.000300\n",
      "2021-07-27 07:45:39,561 - INFO - joeynmt.training - Epoch   2: total training loss 5700.36\n",
      "2021-07-27 07:45:39,562 - INFO - joeynmt.training - EPOCH 3\n",
      "2021-07-27 07:45:51,576 - INFO - joeynmt.training - Epoch   3, Step:   107000, Batch Loss:     2.513559, Tokens per Sec:    16251, Lr: 0.000300\n",
      "2021-07-27 07:46:05,296 - INFO - joeynmt.training - Epoch   3, Step:   107100, Batch Loss:     2.559005, Tokens per Sec:    16770, Lr: 0.000300\n",
      "2021-07-27 07:46:18,994 - INFO - joeynmt.training - Epoch   3, Step:   107200, Batch Loss:     2.490681, Tokens per Sec:    16652, Lr: 0.000300\n",
      "2021-07-27 07:46:32,661 - INFO - joeynmt.training - Epoch   3, Step:   107300, Batch Loss:     2.635666, Tokens per Sec:    16760, Lr: 0.000300\n",
      "2021-07-27 07:46:46,004 - INFO - joeynmt.training - Epoch   3, Step:   107400, Batch Loss:     2.700786, Tokens per Sec:    16620, Lr: 0.000300\n",
      "2021-07-27 07:46:59,536 - INFO - joeynmt.training - Epoch   3, Step:   107500, Batch Loss:     2.513614, Tokens per Sec:    16878, Lr: 0.000300\n",
      "2021-07-27 07:47:30,002 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 07:47:30,002 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 07:47:30,003 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 07:47:31,567 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 07:47:31,568 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 07:47:31,568 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 07:47:31,568 - INFO - joeynmt.training - \tHypothesis: The Pharisees , who was like a man , and He saw it , saying , “ I have come to the throne , and I have come to see the treasure of my house . ”\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - \tHypothesis: When Martha was a great crowd , Mary came to Mary , and said to her , “ Lord , and we have heard Him , and He is coming . ”\n",
      "2021-07-27 07:47:31,569 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 07:47:31,570 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 07:47:31,570 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 07:47:31,570 - INFO - joeynmt.training - \tHypothesis: He was a great day of service to the Sabbath , and he was happy .\n",
      "2021-07-27 07:47:31,570 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 07:47:31,571 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 07:47:31,571 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 07:47:31,571 - INFO - joeynmt.training - \tHypothesis: Paul also looked for the sea , and when He was cut off , he was cut off , and was cut off , and the sea was cut down with the road .\n",
      "2021-07-27 07:47:31,571 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   107500: bleu:   6.55, loss: 89133.1953, ppl:  12.7338, duration: 32.0352s\n",
      "2021-07-27 07:47:45,491 - INFO - joeynmt.training - Epoch   3, Step:   107600, Batch Loss:     2.658170, Tokens per Sec:    16492, Lr: 0.000300\n",
      "2021-07-27 07:47:59,096 - INFO - joeynmt.training - Epoch   3, Step:   107700, Batch Loss:     2.448322, Tokens per Sec:    16848, Lr: 0.000300\n",
      "2021-07-27 07:48:12,719 - INFO - joeynmt.training - Epoch   3, Step:   107800, Batch Loss:     2.495867, Tokens per Sec:    16405, Lr: 0.000300\n",
      "2021-07-27 07:48:26,669 - INFO - joeynmt.training - Epoch   3, Step:   107900, Batch Loss:     2.340499, Tokens per Sec:    16275, Lr: 0.000300\n",
      "2021-07-27 07:48:40,284 - INFO - joeynmt.training - Epoch   3, Step:   108000, Batch Loss:     2.409793, Tokens per Sec:    16016, Lr: 0.000300\n",
      "2021-07-27 07:48:53,851 - INFO - joeynmt.training - Epoch   3, Step:   108100, Batch Loss:     2.406248, Tokens per Sec:    16458, Lr: 0.000300\n",
      "2021-07-27 07:49:07,540 - INFO - joeynmt.training - Epoch   3, Step:   108200, Batch Loss:     2.492194, Tokens per Sec:    16677, Lr: 0.000300\n",
      "2021-07-27 07:49:21,122 - INFO - joeynmt.training - Epoch   3, Step:   108300, Batch Loss:     2.565594, Tokens per Sec:    16606, Lr: 0.000300\n",
      "2021-07-27 07:49:34,890 - INFO - joeynmt.training - Epoch   3, Step:   108400, Batch Loss:     2.356618, Tokens per Sec:    16429, Lr: 0.000300\n",
      "2021-07-27 07:49:48,740 - INFO - joeynmt.training - Epoch   3, Step:   108500, Batch Loss:     2.406695, Tokens per Sec:    16456, Lr: 0.000300\n",
      "2021-07-27 07:50:02,461 - INFO - joeynmt.training - Epoch   3, Step:   108600, Batch Loss:     2.237736, Tokens per Sec:    16645, Lr: 0.000300\n",
      "2021-07-27 07:50:16,053 - INFO - joeynmt.training - Epoch   3, Step:   108700, Batch Loss:     2.425822, Tokens per Sec:    16387, Lr: 0.000300\n",
      "2021-07-27 07:50:29,848 - INFO - joeynmt.training - Epoch   3, Step:   108800, Batch Loss:     2.439195, Tokens per Sec:    16767, Lr: 0.000300\n",
      "2021-07-27 07:50:43,636 - INFO - joeynmt.training - Epoch   3, Step:   108900, Batch Loss:     2.118551, Tokens per Sec:    16605, Lr: 0.000300\n",
      "2021-07-27 07:50:57,351 - INFO - joeynmt.training - Epoch   3, Step:   109000, Batch Loss:     2.746655, Tokens per Sec:    16424, Lr: 0.000300\n",
      "2021-07-27 07:51:10,951 - INFO - joeynmt.training - Epoch   3, Step:   109100, Batch Loss:     2.127923, Tokens per Sec:    16432, Lr: 0.000300\n",
      "2021-07-27 07:51:24,736 - INFO - joeynmt.training - Epoch   3, Step:   109200, Batch Loss:     2.502550, Tokens per Sec:    16355, Lr: 0.000300\n",
      "2021-07-27 07:51:30,680 - INFO - joeynmt.training - Epoch   3: total training loss 5720.83\n",
      "2021-07-27 07:51:30,681 - INFO - joeynmt.training - EPOCH 4\n",
      "2021-07-27 07:51:38,635 - INFO - joeynmt.training - Epoch   4, Step:   109300, Batch Loss:     2.519888, Tokens per Sec:    16185, Lr: 0.000300\n",
      "2021-07-27 07:51:52,337 - INFO - joeynmt.training - Epoch   4, Step:   109400, Batch Loss:     2.403097, Tokens per Sec:    16786, Lr: 0.000300\n",
      "2021-07-27 07:52:05,762 - INFO - joeynmt.training - Epoch   4, Step:   109500, Batch Loss:     2.587027, Tokens per Sec:    16613, Lr: 0.000300\n",
      "2021-07-27 07:52:19,510 - INFO - joeynmt.training - Epoch   4, Step:   109600, Batch Loss:     2.309095, Tokens per Sec:    16324, Lr: 0.000300\n",
      "2021-07-27 07:52:33,271 - INFO - joeynmt.training - Epoch   4, Step:   109700, Batch Loss:     2.468905, Tokens per Sec:    16631, Lr: 0.000300\n",
      "2021-07-27 07:52:46,868 - INFO - joeynmt.training - Epoch   4, Step:   109800, Batch Loss:     2.196414, Tokens per Sec:    16583, Lr: 0.000300\n",
      "2021-07-27 07:53:00,410 - INFO - joeynmt.training - Epoch   4, Step:   109900, Batch Loss:     2.099122, Tokens per Sec:    16584, Lr: 0.000300\n",
      "2021-07-27 07:53:14,055 - INFO - joeynmt.training - Epoch   4, Step:   110000, Batch Loss:     2.599920, Tokens per Sec:    16618, Lr: 0.000300\n",
      "2021-07-27 07:53:41,054 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 07:53:41,054 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 07:53:41,054 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 07:53:42,124 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 07:53:42,125 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 07:53:42,125 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 07:53:42,125 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees , who was like a man , said to them , “ I saw the scribes and the scribes and the Pharisees , and I saw him . ”\n",
      "2021-07-27 07:53:42,125 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 07:53:42,126 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 07:53:42,126 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 07:53:42,126 - INFO - joeynmt.training - \tHypothesis: When Martha had heard this , Mary and Mary came to Mary , and said to her , “ Teacher , and she saw Him , and she was going to see Him . ”\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - \tHypothesis: Jehovah has provided the day for the day of his day .\n",
      "2021-07-27 07:53:42,127 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 07:53:42,128 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 07:53:42,128 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 07:53:42,128 - INFO - joeynmt.training - \tHypothesis: Then Paul had sat down the tree , and when He had sat down , He was cut down , and the fire was cut down , and the sea was cut down .\n",
      "2021-07-27 07:53:42,128 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   110000: bleu:   7.29, loss: 88425.1719, ppl:  12.4791, duration: 28.0729s\n",
      "2021-07-27 07:53:55,873 - INFO - joeynmt.training - Epoch   4, Step:   110100, Batch Loss:     2.473654, Tokens per Sec:    16326, Lr: 0.000210\n",
      "2021-07-27 07:54:09,407 - INFO - joeynmt.training - Epoch   4, Step:   110200, Batch Loss:     2.525152, Tokens per Sec:    16803, Lr: 0.000210\n",
      "2021-07-27 07:54:23,131 - INFO - joeynmt.training - Epoch   4, Step:   110300, Batch Loss:     2.483147, Tokens per Sec:    16741, Lr: 0.000210\n",
      "2021-07-27 07:54:36,900 - INFO - joeynmt.training - Epoch   4, Step:   110400, Batch Loss:     1.823732, Tokens per Sec:    16851, Lr: 0.000210\n",
      "2021-07-27 07:54:50,514 - INFO - joeynmt.training - Epoch   4, Step:   110500, Batch Loss:     2.579154, Tokens per Sec:    16658, Lr: 0.000210\n",
      "2021-07-27 07:55:04,158 - INFO - joeynmt.training - Epoch   4, Step:   110600, Batch Loss:     2.603118, Tokens per Sec:    16791, Lr: 0.000210\n",
      "2021-07-27 07:55:17,622 - INFO - joeynmt.training - Epoch   4, Step:   110700, Batch Loss:     2.265122, Tokens per Sec:    16810, Lr: 0.000210\n",
      "2021-07-27 07:55:31,280 - INFO - joeynmt.training - Epoch   4, Step:   110800, Batch Loss:     2.386464, Tokens per Sec:    16834, Lr: 0.000210\n",
      "2021-07-27 07:55:44,970 - INFO - joeynmt.training - Epoch   4, Step:   110900, Batch Loss:     2.455701, Tokens per Sec:    16401, Lr: 0.000210\n",
      "2021-07-27 07:55:58,534 - INFO - joeynmt.training - Epoch   4, Step:   111000, Batch Loss:     2.312092, Tokens per Sec:    16585, Lr: 0.000210\n",
      "2021-07-27 07:56:12,055 - INFO - joeynmt.training - Epoch   4, Step:   111100, Batch Loss:     2.401133, Tokens per Sec:    16595, Lr: 0.000210\n",
      "2021-07-27 07:56:25,583 - INFO - joeynmt.training - Epoch   4, Step:   111200, Batch Loss:     2.692233, Tokens per Sec:    16948, Lr: 0.000210\n",
      "2021-07-27 07:56:39,212 - INFO - joeynmt.training - Epoch   4, Step:   111300, Batch Loss:     2.238806, Tokens per Sec:    16576, Lr: 0.000210\n",
      "2021-07-27 07:56:52,864 - INFO - joeynmt.training - Epoch   4, Step:   111400, Batch Loss:     2.447489, Tokens per Sec:    16825, Lr: 0.000210\n",
      "2021-07-27 07:57:06,452 - INFO - joeynmt.training - Epoch   4, Step:   111500, Batch Loss:     2.434512, Tokens per Sec:    16524, Lr: 0.000210\n",
      "2021-07-27 07:57:15,966 - INFO - joeynmt.training - Epoch   4: total training loss 5691.36\n",
      "2021-07-27 07:57:15,966 - INFO - joeynmt.training - EPOCH 5\n",
      "2021-07-27 07:57:20,234 - INFO - joeynmt.training - Epoch   5, Step:   111600, Batch Loss:     2.483492, Tokens per Sec:    16158, Lr: 0.000210\n",
      "2021-07-27 07:57:33,961 - INFO - joeynmt.training - Epoch   5, Step:   111700, Batch Loss:     2.449212, Tokens per Sec:    16929, Lr: 0.000210\n",
      "2021-07-27 07:57:47,472 - INFO - joeynmt.training - Epoch   5, Step:   111800, Batch Loss:     2.350066, Tokens per Sec:    16872, Lr: 0.000210\n",
      "2021-07-27 07:58:00,988 - INFO - joeynmt.training - Epoch   5, Step:   111900, Batch Loss:     2.243186, Tokens per Sec:    16970, Lr: 0.000210\n",
      "2021-07-27 07:58:14,591 - INFO - joeynmt.training - Epoch   5, Step:   112000, Batch Loss:     2.161210, Tokens per Sec:    16874, Lr: 0.000210\n",
      "2021-07-27 07:58:28,202 - INFO - joeynmt.training - Epoch   5, Step:   112100, Batch Loss:     2.457512, Tokens per Sec:    16486, Lr: 0.000210\n",
      "2021-07-27 07:58:41,873 - INFO - joeynmt.training - Epoch   5, Step:   112200, Batch Loss:     2.348396, Tokens per Sec:    16590, Lr: 0.000210\n",
      "2021-07-27 07:58:55,313 - INFO - joeynmt.training - Epoch   5, Step:   112300, Batch Loss:     2.500723, Tokens per Sec:    16707, Lr: 0.000210\n",
      "2021-07-27 07:59:08,803 - INFO - joeynmt.training - Epoch   5, Step:   112400, Batch Loss:     2.626327, Tokens per Sec:    16823, Lr: 0.000210\n",
      "2021-07-27 07:59:22,413 - INFO - joeynmt.training - Epoch   5, Step:   112500, Batch Loss:     2.437505, Tokens per Sec:    16870, Lr: 0.000210\n",
      "2021-07-27 07:59:48,720 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 07:59:48,720 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 07:59:48,720 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 07:59:49,058 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 07:59:49,059 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 07:59:49,757 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 07:59:49,759 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 07:59:49,759 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 07:59:49,759 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a man , and he said to them , “ I have come to the scribes and see me . ”\n",
      "2021-07-27 07:59:49,759 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 07:59:49,760 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 07:59:49,760 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 07:59:49,760 - INFO - joeynmt.training - \tHypothesis: Now Martha and Martha had found the same way , and Mary said to Mary , “ Teacher , and He was going to see Him . ”\n",
      "2021-07-27 07:59:49,760 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - \tHypothesis: He was a day of day , and he was a day of distress .\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 07:59:49,761 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 07:59:49,762 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 07:59:49,762 - INFO - joeynmt.training - \tHypothesis: Paul had sat down the tree of the sea , and when he was cut off , he was cut off , and the fire was cut down .\n",
      "2021-07-27 07:59:49,762 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   112500: bleu:   7.13, loss: 88103.1953, ppl:  12.3649, duration: 27.3490s\n",
      "2021-07-27 08:00:03,376 - INFO - joeynmt.training - Epoch   5, Step:   112600, Batch Loss:     2.706132, Tokens per Sec:    16512, Lr: 0.000210\n",
      "2021-07-27 08:00:17,047 - INFO - joeynmt.training - Epoch   5, Step:   112700, Batch Loss:     2.584447, Tokens per Sec:    16981, Lr: 0.000210\n",
      "2021-07-27 08:00:30,765 - INFO - joeynmt.training - Epoch   5, Step:   112800, Batch Loss:     2.264445, Tokens per Sec:    16697, Lr: 0.000210\n",
      "2021-07-27 08:00:44,376 - INFO - joeynmt.training - Epoch   5, Step:   112900, Batch Loss:     2.601777, Tokens per Sec:    16595, Lr: 0.000210\n",
      "2021-07-27 08:00:57,962 - INFO - joeynmt.training - Epoch   5, Step:   113000, Batch Loss:     2.434226, Tokens per Sec:    16813, Lr: 0.000210\n",
      "2021-07-27 08:01:11,398 - INFO - joeynmt.training - Epoch   5, Step:   113100, Batch Loss:     2.222914, Tokens per Sec:    16669, Lr: 0.000210\n",
      "2021-07-27 08:01:24,828 - INFO - joeynmt.training - Epoch   5, Step:   113200, Batch Loss:     2.453332, Tokens per Sec:    16762, Lr: 0.000210\n",
      "2021-07-27 08:01:38,503 - INFO - joeynmt.training - Epoch   5, Step:   113300, Batch Loss:     2.122350, Tokens per Sec:    16731, Lr: 0.000210\n",
      "2021-07-27 08:01:52,225 - INFO - joeynmt.training - Epoch   5, Step:   113400, Batch Loss:     2.462603, Tokens per Sec:    16654, Lr: 0.000210\n",
      "2021-07-27 08:02:05,718 - INFO - joeynmt.training - Epoch   5, Step:   113500, Batch Loss:     2.619079, Tokens per Sec:    16430, Lr: 0.000210\n",
      "2021-07-27 08:02:19,325 - INFO - joeynmt.training - Epoch   5, Step:   113600, Batch Loss:     2.729548, Tokens per Sec:    16594, Lr: 0.000210\n",
      "2021-07-27 08:02:32,867 - INFO - joeynmt.training - Epoch   5, Step:   113700, Batch Loss:     2.547987, Tokens per Sec:    16573, Lr: 0.000210\n",
      "2021-07-27 08:02:46,474 - INFO - joeynmt.training - Epoch   5, Step:   113800, Batch Loss:     2.506671, Tokens per Sec:    17130, Lr: 0.000210\n",
      "2021-07-27 08:02:58,664 - INFO - joeynmt.training - Epoch   5: total training loss 5640.06\n",
      "2021-07-27 08:02:58,664 - INFO - joeynmt.training - EPOCH 6\n",
      "2021-07-27 08:03:00,157 - INFO - joeynmt.training - Epoch   6, Step:   113900, Batch Loss:     2.651522, Tokens per Sec:    14113, Lr: 0.000210\n",
      "2021-07-27 08:03:13,611 - INFO - joeynmt.training - Epoch   6, Step:   114000, Batch Loss:     2.423502, Tokens per Sec:    16609, Lr: 0.000210\n",
      "2021-07-27 08:03:27,130 - INFO - joeynmt.training - Epoch   6, Step:   114100, Batch Loss:     2.439190, Tokens per Sec:    16577, Lr: 0.000210\n",
      "2021-07-27 08:03:40,773 - INFO - joeynmt.training - Epoch   6, Step:   114200, Batch Loss:     2.556720, Tokens per Sec:    16652, Lr: 0.000210\n",
      "2021-07-27 08:03:54,253 - INFO - joeynmt.training - Epoch   6, Step:   114300, Batch Loss:     1.940488, Tokens per Sec:    16779, Lr: 0.000210\n",
      "2021-07-27 08:04:07,643 - INFO - joeynmt.training - Epoch   6, Step:   114400, Batch Loss:     2.539602, Tokens per Sec:    16727, Lr: 0.000210\n",
      "2021-07-27 08:04:21,309 - INFO - joeynmt.training - Epoch   6, Step:   114500, Batch Loss:     2.601215, Tokens per Sec:    16643, Lr: 0.000210\n",
      "2021-07-27 08:04:34,946 - INFO - joeynmt.training - Epoch   6, Step:   114600, Batch Loss:     2.398460, Tokens per Sec:    16722, Lr: 0.000210\n",
      "2021-07-27 08:04:48,523 - INFO - joeynmt.training - Epoch   6, Step:   114700, Batch Loss:     2.207838, Tokens per Sec:    16505, Lr: 0.000210\n",
      "2021-07-27 08:05:02,316 - INFO - joeynmt.training - Epoch   6, Step:   114800, Batch Loss:     2.243703, Tokens per Sec:    16925, Lr: 0.000210\n",
      "2021-07-27 08:05:15,952 - INFO - joeynmt.training - Epoch   6, Step:   114900, Batch Loss:     2.383183, Tokens per Sec:    16501, Lr: 0.000210\n",
      "2021-07-27 08:05:29,679 - INFO - joeynmt.training - Epoch   6, Step:   115000, Batch Loss:     2.407759, Tokens per Sec:    16727, Lr: 0.000210\n",
      "2021-07-27 08:05:57,321 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:05:57,321 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:05:57,321 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:05:57,654 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 08:05:57,655 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 08:05:58,734 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:05:58,736 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:05:58,736 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:05:58,736 - INFO - joeynmt.training - \tHypothesis: The Pharisees , who was like a man , saw him , and said to them , “ I have come to the seven times , and I have come to know where I have come . ”\n",
      "2021-07-27 08:05:58,736 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:05:58,737 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:05:58,737 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:05:58,737 - INFO - joeynmt.training - \tHypothesis: When Martha and Mary had found the same , Mary and Mary said to her , “ Teacher , and Mary , and He was with Him , and He was with Him . ”\n",
      "2021-07-27 08:05:58,737 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:05:58,738 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:05:58,738 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:05:58,738 - INFO - joeynmt.training - \tHypothesis: He was a day of service today , and he was a day of service .\n",
      "2021-07-27 08:05:58,738 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:05:58,739 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:05:58,739 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:05:58,739 - INFO - joeynmt.training - \tHypothesis: Paul had sat down and looked for the sea , and when He was cut off , He was cut off from the sea , and the fire was cut down .\n",
      "2021-07-27 08:05:58,739 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   115000: bleu:   7.32, loss: 87862.8984, ppl:  12.2804, duration: 29.0597s\n",
      "2021-07-27 08:06:12,570 - INFO - joeynmt.training - Epoch   6, Step:   115100, Batch Loss:     2.355669, Tokens per Sec:    16643, Lr: 0.000210\n",
      "2021-07-27 08:06:26,215 - INFO - joeynmt.training - Epoch   6, Step:   115200, Batch Loss:     2.565229, Tokens per Sec:    17035, Lr: 0.000210\n",
      "2021-07-27 08:06:39,917 - INFO - joeynmt.training - Epoch   6, Step:   115300, Batch Loss:     2.217152, Tokens per Sec:    16383, Lr: 0.000210\n",
      "2021-07-27 08:06:53,423 - INFO - joeynmt.training - Epoch   6, Step:   115400, Batch Loss:     2.571432, Tokens per Sec:    16232, Lr: 0.000210\n",
      "2021-07-27 08:07:07,141 - INFO - joeynmt.training - Epoch   6, Step:   115500, Batch Loss:     2.283047, Tokens per Sec:    17002, Lr: 0.000210\n",
      "2021-07-27 08:07:20,657 - INFO - joeynmt.training - Epoch   6, Step:   115600, Batch Loss:     2.495604, Tokens per Sec:    16737, Lr: 0.000210\n",
      "2021-07-27 08:07:34,254 - INFO - joeynmt.training - Epoch   6, Step:   115700, Batch Loss:     2.626188, Tokens per Sec:    16711, Lr: 0.000210\n",
      "2021-07-27 08:07:47,921 - INFO - joeynmt.training - Epoch   6, Step:   115800, Batch Loss:     2.436184, Tokens per Sec:    16511, Lr: 0.000210\n",
      "2021-07-27 08:08:01,706 - INFO - joeynmt.training - Epoch   6, Step:   115900, Batch Loss:     2.395348, Tokens per Sec:    16773, Lr: 0.000210\n",
      "2021-07-27 08:08:15,433 - INFO - joeynmt.training - Epoch   6, Step:   116000, Batch Loss:     2.482064, Tokens per Sec:    16577, Lr: 0.000210\n",
      "2021-07-27 08:08:29,114 - INFO - joeynmt.training - Epoch   6, Step:   116100, Batch Loss:     2.438924, Tokens per Sec:    16217, Lr: 0.000210\n",
      "2021-07-27 08:08:42,812 - INFO - joeynmt.training - Epoch   6, Step:   116200, Batch Loss:     2.477787, Tokens per Sec:    16635, Lr: 0.000210\n",
      "2021-07-27 08:08:45,131 - INFO - joeynmt.training - Epoch   6: total training loss 5643.48\n",
      "2021-07-27 08:08:45,132 - INFO - joeynmt.training - EPOCH 7\n",
      "2021-07-27 08:08:56,662 - INFO - joeynmt.training - Epoch   7, Step:   116300, Batch Loss:     2.611559, Tokens per Sec:    16210, Lr: 0.000210\n",
      "2021-07-27 08:09:10,367 - INFO - joeynmt.training - Epoch   7, Step:   116400, Batch Loss:     2.461609, Tokens per Sec:    16986, Lr: 0.000210\n",
      "2021-07-27 08:09:24,106 - INFO - joeynmt.training - Epoch   7, Step:   116500, Batch Loss:     2.614424, Tokens per Sec:    16553, Lr: 0.000210\n",
      "2021-07-27 08:09:37,807 - INFO - joeynmt.training - Epoch   7, Step:   116600, Batch Loss:     2.489360, Tokens per Sec:    16566, Lr: 0.000210\n",
      "2021-07-27 08:09:51,407 - INFO - joeynmt.training - Epoch   7, Step:   116700, Batch Loss:     2.501258, Tokens per Sec:    16745, Lr: 0.000210\n",
      "2021-07-27 08:10:05,053 - INFO - joeynmt.training - Epoch   7, Step:   116800, Batch Loss:     2.428561, Tokens per Sec:    16752, Lr: 0.000210\n",
      "2021-07-27 08:10:18,699 - INFO - joeynmt.training - Epoch   7, Step:   116900, Batch Loss:     2.476547, Tokens per Sec:    16690, Lr: 0.000210\n",
      "2021-07-27 08:10:32,379 - INFO - joeynmt.training - Epoch   7, Step:   117000, Batch Loss:     2.505526, Tokens per Sec:    16575, Lr: 0.000210\n",
      "2021-07-27 08:10:46,105 - INFO - joeynmt.training - Epoch   7, Step:   117100, Batch Loss:     2.418744, Tokens per Sec:    16725, Lr: 0.000210\n",
      "2021-07-27 08:10:59,622 - INFO - joeynmt.training - Epoch   7, Step:   117200, Batch Loss:     2.407301, Tokens per Sec:    16647, Lr: 0.000210\n",
      "2021-07-27 08:11:13,227 - INFO - joeynmt.training - Epoch   7, Step:   117300, Batch Loss:     2.544755, Tokens per Sec:    16986, Lr: 0.000210\n",
      "2021-07-27 08:11:26,794 - INFO - joeynmt.training - Epoch   7, Step:   117400, Batch Loss:     2.222024, Tokens per Sec:    16735, Lr: 0.000210\n",
      "2021-07-27 08:11:40,460 - INFO - joeynmt.training - Epoch   7, Step:   117500, Batch Loss:     2.431499, Tokens per Sec:    16459, Lr: 0.000210\n",
      "2021-07-27 08:12:07,893 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:12:07,894 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:12:07,894 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:12:09,018 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:12:09,018 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:12:09,018 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:12:09,019 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a tree , and he said to them , “ I have come and come down and come down and come down . ”\n",
      "2021-07-27 08:12:09,019 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:12:09,019 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - \tHypothesis: Then Martha and Mary came to her , and her mother came to Mary , and said to her , “ Teacher , and her daughter was coming . ”\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:12:09,020 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - \tHypothesis: He was a day of judgment on the Passover .\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:12:09,021 - INFO - joeynmt.training - \tHypothesis: Then Paul gave them the treasure of the sea , and when He was cast , He was cut off the sea , and the fire was cut down and the sea .\n",
      "2021-07-27 08:12:09,022 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   117500: bleu:   6.91, loss: 88369.0234, ppl:  12.4591, duration: 28.5610s\n",
      "2021-07-27 08:12:22,654 - INFO - joeynmt.training - Epoch   7, Step:   117600, Batch Loss:     2.411600, Tokens per Sec:    16470, Lr: 0.000210\n",
      "2021-07-27 08:12:36,168 - INFO - joeynmt.training - Epoch   7, Step:   117700, Batch Loss:     2.633862, Tokens per Sec:    16355, Lr: 0.000210\n",
      "2021-07-27 08:12:49,887 - INFO - joeynmt.training - Epoch   7, Step:   117800, Batch Loss:     2.490059, Tokens per Sec:    16641, Lr: 0.000210\n",
      "2021-07-27 08:13:03,435 - INFO - joeynmt.training - Epoch   7, Step:   117900, Batch Loss:     2.416721, Tokens per Sec:    16681, Lr: 0.000210\n",
      "2021-07-27 08:13:17,022 - INFO - joeynmt.training - Epoch   7, Step:   118000, Batch Loss:     2.696039, Tokens per Sec:    16514, Lr: 0.000210\n",
      "2021-07-27 08:13:30,683 - INFO - joeynmt.training - Epoch   7, Step:   118100, Batch Loss:     2.574065, Tokens per Sec:    16420, Lr: 0.000210\n",
      "2021-07-27 08:13:44,009 - INFO - joeynmt.training - Epoch   7, Step:   118200, Batch Loss:     2.632088, Tokens per Sec:    16838, Lr: 0.000210\n",
      "2021-07-27 08:13:57,501 - INFO - joeynmt.training - Epoch   7, Step:   118300, Batch Loss:     2.396899, Tokens per Sec:    16663, Lr: 0.000210\n",
      "2021-07-27 08:14:11,004 - INFO - joeynmt.training - Epoch   7, Step:   118400, Batch Loss:     2.500988, Tokens per Sec:    16643, Lr: 0.000210\n",
      "2021-07-27 08:14:24,723 - INFO - joeynmt.training - Epoch   7, Step:   118500, Batch Loss:     2.380573, Tokens per Sec:    16788, Lr: 0.000210\n",
      "2021-07-27 08:14:30,863 - INFO - joeynmt.training - Epoch   7: total training loss 5636.37\n",
      "2021-07-27 08:14:30,864 - INFO - joeynmt.training - EPOCH 8\n",
      "2021-07-27 08:14:38,611 - INFO - joeynmt.training - Epoch   8, Step:   118600, Batch Loss:     2.548654, Tokens per Sec:    16010, Lr: 0.000210\n",
      "2021-07-27 08:14:52,162 - INFO - joeynmt.training - Epoch   8, Step:   118700, Batch Loss:     2.211355, Tokens per Sec:    16813, Lr: 0.000210\n",
      "2021-07-27 08:15:05,738 - INFO - joeynmt.training - Epoch   8, Step:   118800, Batch Loss:     2.538246, Tokens per Sec:    16971, Lr: 0.000210\n",
      "2021-07-27 08:15:19,273 - INFO - joeynmt.training - Epoch   8, Step:   118900, Batch Loss:     2.161235, Tokens per Sec:    16699, Lr: 0.000210\n",
      "2021-07-27 08:15:32,781 - INFO - joeynmt.training - Epoch   8, Step:   119000, Batch Loss:     2.567760, Tokens per Sec:    16266, Lr: 0.000210\n",
      "2021-07-27 08:15:46,487 - INFO - joeynmt.training - Epoch   8, Step:   119100, Batch Loss:     2.308064, Tokens per Sec:    16984, Lr: 0.000210\n",
      "2021-07-27 08:16:00,001 - INFO - joeynmt.training - Epoch   8, Step:   119200, Batch Loss:     2.367049, Tokens per Sec:    16709, Lr: 0.000210\n",
      "2021-07-27 08:16:13,588 - INFO - joeynmt.training - Epoch   8, Step:   119300, Batch Loss:     2.265653, Tokens per Sec:    16689, Lr: 0.000210\n",
      "2021-07-27 08:16:27,288 - INFO - joeynmt.training - Epoch   8, Step:   119400, Batch Loss:     2.365632, Tokens per Sec:    16780, Lr: 0.000210\n",
      "2021-07-27 08:16:40,888 - INFO - joeynmt.training - Epoch   8, Step:   119500, Batch Loss:     2.591372, Tokens per Sec:    16465, Lr: 0.000210\n",
      "2021-07-27 08:16:54,435 - INFO - joeynmt.training - Epoch   8, Step:   119600, Batch Loss:     2.183113, Tokens per Sec:    16600, Lr: 0.000210\n",
      "2021-07-27 08:17:07,954 - INFO - joeynmt.training - Epoch   8, Step:   119700, Batch Loss:     2.603106, Tokens per Sec:    16782, Lr: 0.000210\n",
      "2021-07-27 08:17:21,415 - INFO - joeynmt.training - Epoch   8, Step:   119800, Batch Loss:     2.385073, Tokens per Sec:    16726, Lr: 0.000210\n",
      "2021-07-27 08:17:35,122 - INFO - joeynmt.training - Epoch   8, Step:   119900, Batch Loss:     2.329810, Tokens per Sec:    16796, Lr: 0.000210\n",
      "2021-07-27 08:17:48,798 - INFO - joeynmt.training - Epoch   8, Step:   120000, Batch Loss:     2.380733, Tokens per Sec:    16619, Lr: 0.000210\n",
      "2021-07-27 08:18:15,915 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:18:15,915 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:18:15,916 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:18:16,241 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 08:18:16,242 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 08:18:17,013 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:18:17,014 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:18:17,014 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:18:17,015 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a tree , and He said to them , “ I have come and come to the tomb and come to me . ”\n",
      "2021-07-27 08:18:17,016 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:18:17,016 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:18:17,016 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:18:17,017 - INFO - joeynmt.training - \tHypothesis: Now Martha and Mary went to the house , and Mary said to Mary , “ Teacher , and Mary , and Mary , and Mary , and Mary . ”\n",
      "2021-07-27 08:18:17,017 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:18:17,017 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:18:17,017 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:18:17,018 - INFO - joeynmt.training - \tHypothesis: He was a day of seventh day , and he was in the days of Noah .\n",
      "2021-07-27 08:18:17,018 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:18:17,018 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:18:17,018 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:18:17,019 - INFO - joeynmt.training - \tHypothesis: And Paul had sound in the sea , and when He had sat on the sea , He was cast , and the sea was cut down and the sea .\n",
      "2021-07-27 08:18:17,019 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   120000: bleu:   7.34, loss: 87600.4766, ppl:  12.1887, duration: 28.2203s\n",
      "2021-07-27 08:18:30,579 - INFO - joeynmt.training - Epoch   8, Step:   120100, Batch Loss:     2.445420, Tokens per Sec:    16505, Lr: 0.000210\n",
      "2021-07-27 08:18:44,255 - INFO - joeynmt.training - Epoch   8, Step:   120200, Batch Loss:     2.420484, Tokens per Sec:    16652, Lr: 0.000210\n",
      "2021-07-27 08:18:57,886 - INFO - joeynmt.training - Epoch   8, Step:   120300, Batch Loss:     2.527857, Tokens per Sec:    16552, Lr: 0.000210\n",
      "2021-07-27 08:19:11,555 - INFO - joeynmt.training - Epoch   8, Step:   120400, Batch Loss:     2.190420, Tokens per Sec:    16978, Lr: 0.000210\n",
      "2021-07-27 08:19:25,252 - INFO - joeynmt.training - Epoch   8, Step:   120500, Batch Loss:     2.366493, Tokens per Sec:    16611, Lr: 0.000210\n",
      "2021-07-27 08:19:38,835 - INFO - joeynmt.training - Epoch   8, Step:   120600, Batch Loss:     2.591762, Tokens per Sec:    16779, Lr: 0.000210\n",
      "2021-07-27 08:19:52,435 - INFO - joeynmt.training - Epoch   8, Step:   120700, Batch Loss:     2.445704, Tokens per Sec:    16870, Lr: 0.000210\n",
      "2021-07-27 08:20:05,998 - INFO - joeynmt.training - Epoch   8, Step:   120800, Batch Loss:     2.305495, Tokens per Sec:    16938, Lr: 0.000210\n",
      "2021-07-27 08:20:15,127 - INFO - joeynmt.training - Epoch   8: total training loss 5620.24\n",
      "2021-07-27 08:20:15,127 - INFO - joeynmt.training - EPOCH 9\n",
      "2021-07-27 08:20:19,758 - INFO - joeynmt.training - Epoch   9, Step:   120900, Batch Loss:     2.309544, Tokens per Sec:    15494, Lr: 0.000210\n",
      "2021-07-27 08:20:33,489 - INFO - joeynmt.training - Epoch   9, Step:   121000, Batch Loss:     2.126650, Tokens per Sec:    16648, Lr: 0.000210\n",
      "2021-07-27 08:20:47,061 - INFO - joeynmt.training - Epoch   9, Step:   121100, Batch Loss:     2.491347, Tokens per Sec:    16627, Lr: 0.000210\n",
      "2021-07-27 08:21:00,655 - INFO - joeynmt.training - Epoch   9, Step:   121200, Batch Loss:     2.481128, Tokens per Sec:    16904, Lr: 0.000210\n",
      "2021-07-27 08:21:14,088 - INFO - joeynmt.training - Epoch   9, Step:   121300, Batch Loss:     2.425834, Tokens per Sec:    16782, Lr: 0.000210\n",
      "2021-07-27 08:21:27,724 - INFO - joeynmt.training - Epoch   9, Step:   121400, Batch Loss:     2.337860, Tokens per Sec:    16655, Lr: 0.000210\n",
      "2021-07-27 08:21:41,385 - INFO - joeynmt.training - Epoch   9, Step:   121500, Batch Loss:     2.451119, Tokens per Sec:    16768, Lr: 0.000210\n",
      "2021-07-27 08:21:55,012 - INFO - joeynmt.training - Epoch   9, Step:   121600, Batch Loss:     2.487977, Tokens per Sec:    16488, Lr: 0.000210\n",
      "2021-07-27 08:22:08,679 - INFO - joeynmt.training - Epoch   9, Step:   121700, Batch Loss:     2.691375, Tokens per Sec:    16727, Lr: 0.000210\n",
      "2021-07-27 08:22:22,273 - INFO - joeynmt.training - Epoch   9, Step:   121800, Batch Loss:     2.534315, Tokens per Sec:    16699, Lr: 0.000210\n",
      "2021-07-27 08:22:36,020 - INFO - joeynmt.training - Epoch   9, Step:   121900, Batch Loss:     2.209948, Tokens per Sec:    16633, Lr: 0.000210\n",
      "2021-07-27 08:22:49,621 - INFO - joeynmt.training - Epoch   9, Step:   122000, Batch Loss:     2.460074, Tokens per Sec:    16538, Lr: 0.000210\n",
      "2021-07-27 08:23:03,218 - INFO - joeynmt.training - Epoch   9, Step:   122100, Batch Loss:     2.584368, Tokens per Sec:    16928, Lr: 0.000210\n",
      "2021-07-27 08:23:16,803 - INFO - joeynmt.training - Epoch   9, Step:   122200, Batch Loss:     2.641792, Tokens per Sec:    16838, Lr: 0.000210\n",
      "2021-07-27 08:23:30,177 - INFO - joeynmt.training - Epoch   9, Step:   122300, Batch Loss:     2.108025, Tokens per Sec:    16773, Lr: 0.000210\n",
      "2021-07-27 08:23:43,914 - INFO - joeynmt.training - Epoch   9, Step:   122400, Batch Loss:     2.078990, Tokens per Sec:    16466, Lr: 0.000210\n",
      "2021-07-27 08:23:57,493 - INFO - joeynmt.training - Epoch   9, Step:   122500, Batch Loss:     2.023412, Tokens per Sec:    16645, Lr: 0.000210\n",
      "2021-07-27 08:24:23,010 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:24:23,010 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:24:23,011 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:24:24,087 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:24:24,087 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:24:24,087 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:24:24,088 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees saw him , and said to them , “ I have seen the scribes and the scribes and the scribes and the Pharisees . ”\n",
      "2021-07-27 08:24:24,088 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:24:24,088 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:24:24,088 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:24:24,089 - INFO - joeynmt.training - \tHypothesis: When Martha had heard that Mary was a great crowd , Mary said to him , “ Teacher , and Mary , and Mary , and Mary , and Mary . ”\n",
      "2021-07-27 08:24:24,089 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:24:24,089 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - \tHypothesis: He was a day of service to the Sabbath , and he was very happy .\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:24:24,090 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:24:24,091 - INFO - joeynmt.training - \tHypothesis: Paul also gave them the treasure of the sea , and when He was cut off , he was cut off and sat down , and the sea was cut down .\n",
      "2021-07-27 08:24:24,091 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   122500: bleu:   7.38, loss: 87918.3750, ppl:  12.2999, duration: 26.5970s\n",
      "2021-07-27 08:24:37,690 - INFO - joeynmt.training - Epoch   9, Step:   122600, Batch Loss:     2.432909, Tokens per Sec:    16495, Lr: 0.000210\n",
      "2021-07-27 08:24:51,171 - INFO - joeynmt.training - Epoch   9, Step:   122700, Batch Loss:     2.292904, Tokens per Sec:    17072, Lr: 0.000210\n",
      "2021-07-27 08:25:04,608 - INFO - joeynmt.training - Epoch   9, Step:   122800, Batch Loss:     2.648040, Tokens per Sec:    16834, Lr: 0.000210\n",
      "2021-07-27 08:25:18,323 - INFO - joeynmt.training - Epoch   9, Step:   122900, Batch Loss:     2.308712, Tokens per Sec:    16956, Lr: 0.000210\n",
      "2021-07-27 08:25:32,011 - INFO - joeynmt.training - Epoch   9, Step:   123000, Batch Loss:     2.517625, Tokens per Sec:    16600, Lr: 0.000210\n",
      "2021-07-27 08:25:45,451 - INFO - joeynmt.training - Epoch   9, Step:   123100, Batch Loss:     2.562794, Tokens per Sec:    16819, Lr: 0.000210\n",
      "2021-07-27 08:25:57,692 - INFO - joeynmt.training - Epoch   9: total training loss 5617.43\n",
      "2021-07-27 08:25:57,692 - INFO - joeynmt.training - EPOCH 10\n",
      "2021-07-27 08:25:59,019 - INFO - joeynmt.training - Epoch  10, Step:   123200, Batch Loss:     2.192390, Tokens per Sec:    13432, Lr: 0.000210\n",
      "2021-07-27 08:26:12,409 - INFO - joeynmt.training - Epoch  10, Step:   123300, Batch Loss:     2.659591, Tokens per Sec:    16782, Lr: 0.000210\n",
      "2021-07-27 08:26:25,903 - INFO - joeynmt.training - Epoch  10, Step:   123400, Batch Loss:     2.452177, Tokens per Sec:    16595, Lr: 0.000210\n",
      "2021-07-27 08:26:39,526 - INFO - joeynmt.training - Epoch  10, Step:   123500, Batch Loss:     2.554398, Tokens per Sec:    16582, Lr: 0.000210\n",
      "2021-07-27 08:26:53,092 - INFO - joeynmt.training - Epoch  10, Step:   123600, Batch Loss:     2.577075, Tokens per Sec:    16976, Lr: 0.000210\n",
      "2021-07-27 08:27:06,496 - INFO - joeynmt.training - Epoch  10, Step:   123700, Batch Loss:     2.480223, Tokens per Sec:    16791, Lr: 0.000210\n",
      "2021-07-27 08:27:19,844 - INFO - joeynmt.training - Epoch  10, Step:   123800, Batch Loss:     2.359327, Tokens per Sec:    16911, Lr: 0.000210\n",
      "2021-07-27 08:27:33,666 - INFO - joeynmt.training - Epoch  10, Step:   123900, Batch Loss:     2.401583, Tokens per Sec:    16853, Lr: 0.000210\n",
      "2021-07-27 08:27:47,389 - INFO - joeynmt.training - Epoch  10, Step:   124000, Batch Loss:     2.644904, Tokens per Sec:    16507, Lr: 0.000210\n",
      "2021-07-27 08:28:00,987 - INFO - joeynmt.training - Epoch  10, Step:   124100, Batch Loss:     2.537358, Tokens per Sec:    16527, Lr: 0.000210\n",
      "2021-07-27 08:28:14,619 - INFO - joeynmt.training - Epoch  10, Step:   124200, Batch Loss:     2.410045, Tokens per Sec:    16877, Lr: 0.000210\n",
      "2021-07-27 08:28:28,049 - INFO - joeynmt.training - Epoch  10, Step:   124300, Batch Loss:     2.405060, Tokens per Sec:    16891, Lr: 0.000210\n",
      "2021-07-27 08:28:41,624 - INFO - joeynmt.training - Epoch  10, Step:   124400, Batch Loss:     2.538678, Tokens per Sec:    16650, Lr: 0.000210\n",
      "2021-07-27 08:28:55,182 - INFO - joeynmt.training - Epoch  10, Step:   124500, Batch Loss:     1.863895, Tokens per Sec:    16570, Lr: 0.000210\n",
      "2021-07-27 08:29:08,606 - INFO - joeynmt.training - Epoch  10, Step:   124600, Batch Loss:     2.493046, Tokens per Sec:    16761, Lr: 0.000210\n",
      "2021-07-27 08:29:22,199 - INFO - joeynmt.training - Epoch  10, Step:   124700, Batch Loss:     2.522714, Tokens per Sec:    16814, Lr: 0.000210\n",
      "2021-07-27 08:29:35,695 - INFO - joeynmt.training - Epoch  10, Step:   124800, Batch Loss:     2.441285, Tokens per Sec:    16548, Lr: 0.000210\n",
      "2021-07-27 08:29:49,347 - INFO - joeynmt.training - Epoch  10, Step:   124900, Batch Loss:     2.609269, Tokens per Sec:    16375, Lr: 0.000210\n",
      "2021-07-27 08:30:02,988 - INFO - joeynmt.training - Epoch  10, Step:   125000, Batch Loss:     2.325733, Tokens per Sec:    16931, Lr: 0.000210\n",
      "2021-07-27 08:30:29,835 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:30:29,835 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:30:29,835 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:30:30,195 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
      "2021-07-27 08:30:30,196 - INFO - joeynmt.training - Saving new checkpoint.\n",
      "2021-07-27 08:30:30,993 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:30:30,994 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:30:30,994 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:30:30,994 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees , as he had seen , He said to them , “ I have come and come to the scribes and said to them , “ I have come and see me . ”\n",
      "2021-07-27 08:30:30,994 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:30:30,995 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:30:30,995 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:30:30,995 - INFO - joeynmt.training - \tHypothesis: When Martha had heard that Mary was a sister , Mary , and Mary , and Mary said to her , “ Teacher , and Mary , and she was born . ”\n",
      "2021-07-27 08:30:30,995 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:30:30,996 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:30:30,996 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:30:30,996 - INFO - joeynmt.training - \tHypothesis: He was a day of service to the Sabbath , and he was very happy .\n",
      "2021-07-27 08:30:30,996 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:30:30,997 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:30:30,997 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:30:30,997 - INFO - joeynmt.training - \tHypothesis: And when Paul had spoken , He had spoken of the sea , and was cut out of the sea , and the fire was cut down .\n",
      "2021-07-27 08:30:30,997 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   125000: bleu:   7.51, loss: 87552.7031, ppl:  12.1721, duration: 28.0084s\n",
      "2021-07-27 08:30:44,710 - INFO - joeynmt.training - Epoch  10, Step:   125100, Batch Loss:     2.710145, Tokens per Sec:    16778, Lr: 0.000210\n",
      "2021-07-27 08:30:58,232 - INFO - joeynmt.training - Epoch  10, Step:   125200, Batch Loss:     2.449889, Tokens per Sec:    16846, Lr: 0.000210\n",
      "2021-07-27 08:31:11,681 - INFO - joeynmt.training - Epoch  10, Step:   125300, Batch Loss:     2.597331, Tokens per Sec:    16847, Lr: 0.000210\n",
      "2021-07-27 08:31:25,227 - INFO - joeynmt.training - Epoch  10, Step:   125400, Batch Loss:     2.706754, Tokens per Sec:    16584, Lr: 0.000210\n",
      "2021-07-27 08:31:38,862 - INFO - joeynmt.training - Epoch  10, Step:   125500, Batch Loss:     2.571770, Tokens per Sec:    16478, Lr: 0.000210\n",
      "2021-07-27 08:31:41,723 - INFO - joeynmt.training - Epoch  10: total training loss 5622.03\n",
      "2021-07-27 08:31:41,723 - INFO - joeynmt.training - EPOCH 11\n",
      "2021-07-27 08:31:52,470 - INFO - joeynmt.training - Epoch  11, Step:   125600, Batch Loss:     2.555787, Tokens per Sec:    16341, Lr: 0.000210\n",
      "2021-07-27 08:32:06,097 - INFO - joeynmt.training - Epoch  11, Step:   125700, Batch Loss:     2.130382, Tokens per Sec:    16967, Lr: 0.000210\n",
      "2021-07-27 08:32:19,576 - INFO - joeynmt.training - Epoch  11, Step:   125800, Batch Loss:     2.539688, Tokens per Sec:    16935, Lr: 0.000210\n",
      "2021-07-27 08:32:33,135 - INFO - joeynmt.training - Epoch  11, Step:   125900, Batch Loss:     2.427249, Tokens per Sec:    16572, Lr: 0.000210\n",
      "2021-07-27 08:32:46,716 - INFO - joeynmt.training - Epoch  11, Step:   126000, Batch Loss:     2.383571, Tokens per Sec:    16651, Lr: 0.000210\n",
      "2021-07-27 08:33:00,056 - INFO - joeynmt.training - Epoch  11, Step:   126100, Batch Loss:     2.441225, Tokens per Sec:    16599, Lr: 0.000210\n",
      "2021-07-27 08:33:13,359 - INFO - joeynmt.training - Epoch  11, Step:   126200, Batch Loss:     2.131477, Tokens per Sec:    16773, Lr: 0.000210\n",
      "2021-07-27 08:33:26,876 - INFO - joeynmt.training - Epoch  11, Step:   126300, Batch Loss:     2.453427, Tokens per Sec:    16910, Lr: 0.000210\n",
      "2021-07-27 08:33:40,418 - INFO - joeynmt.training - Epoch  11, Step:   126400, Batch Loss:     2.393158, Tokens per Sec:    16814, Lr: 0.000210\n",
      "2021-07-27 08:33:53,900 - INFO - joeynmt.training - Epoch  11, Step:   126500, Batch Loss:     2.584401, Tokens per Sec:    16647, Lr: 0.000210\n",
      "2021-07-27 08:34:07,373 - INFO - joeynmt.training - Epoch  11, Step:   126600, Batch Loss:     2.633683, Tokens per Sec:    16755, Lr: 0.000210\n",
      "2021-07-27 08:34:20,770 - INFO - joeynmt.training - Epoch  11, Step:   126700, Batch Loss:     2.383806, Tokens per Sec:    16784, Lr: 0.000210\n",
      "2021-07-27 08:34:34,352 - INFO - joeynmt.training - Epoch  11, Step:   126800, Batch Loss:     2.627386, Tokens per Sec:    16742, Lr: 0.000210\n",
      "2021-07-27 08:34:48,312 - INFO - joeynmt.training - Epoch  11, Step:   126900, Batch Loss:     2.508087, Tokens per Sec:    17028, Lr: 0.000210\n",
      "2021-07-27 08:35:01,768 - INFO - joeynmt.training - Epoch  11, Step:   127000, Batch Loss:     2.437658, Tokens per Sec:    16597, Lr: 0.000210\n",
      "2021-07-27 08:35:15,354 - INFO - joeynmt.training - Epoch  11, Step:   127100, Batch Loss:     2.445096, Tokens per Sec:    16674, Lr: 0.000210\n",
      "2021-07-27 08:35:28,869 - INFO - joeynmt.training - Epoch  11, Step:   127200, Batch Loss:     2.352874, Tokens per Sec:    16501, Lr: 0.000210\n",
      "2021-07-27 08:35:42,302 - INFO - joeynmt.training - Epoch  11, Step:   127300, Batch Loss:     2.553585, Tokens per Sec:    16950, Lr: 0.000210\n",
      "2021-07-27 08:35:55,767 - INFO - joeynmt.training - Epoch  11, Step:   127400, Batch Loss:     2.436206, Tokens per Sec:    16913, Lr: 0.000210\n",
      "2021-07-27 08:36:09,258 - INFO - joeynmt.training - Epoch  11, Step:   127500, Batch Loss:     2.336831, Tokens per Sec:    16862, Lr: 0.000210\n",
      "2021-07-27 08:36:37,381 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:36:37,381 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:36:37,381 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:36:38,777 - INFO - joeynmt.training - Example #0\n",
      "2021-07-27 08:36:38,779 - INFO - joeynmt.training - \tSource:     Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
      "2021-07-27 08:36:38,779 - INFO - joeynmt.training - \tReference:  Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
      "2021-07-27 08:36:38,779 - INFO - joeynmt.training - \tHypothesis: Then the Pharisees and the Pharisees were like a man , and He said to them , “ I have come and come to the tomb , and I have come to know . ”\n",
      "2021-07-27 08:36:38,779 - INFO - joeynmt.training - Example #1\n",
      "2021-07-27 08:36:38,780 - INFO - joeynmt.training - \tSource:     Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
      "2021-07-27 08:36:38,780 - INFO - joeynmt.training - \tReference:  And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
      "2021-07-27 08:36:38,780 - INFO - joeynmt.training - \tHypothesis: When Martha had heard that she had been called Mary , Mary , and Mary , and Mary said to Him , “ Teacher , and we have come to Him . ”\n",
      "2021-07-27 08:36:38,780 - INFO - joeynmt.training - Example #2\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - \tSource:     Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - \tReference:  That day was the Preparation , and the Sabbath drew near .\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - \tHypothesis: He was a day of service , and he was very happy to be happy .\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - Example #3\n",
      "2021-07-27 08:36:38,781 - INFO - joeynmt.training - \tSource:     Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
      "2021-07-27 08:36:38,782 - INFO - joeynmt.training - \tReference:  But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
      "2021-07-27 08:36:38,782 - INFO - joeynmt.training - \tHypothesis: And Paul had sat out the sea , and when He was cut out of the sea , He was cut off the sea , and the sea was cut down and the sea .\n",
      "2021-07-27 08:36:38,782 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   127500: bleu:   7.36, loss: 87687.9141, ppl:  12.2192, duration: 29.5232s\n",
      "2021-07-27 08:36:52,515 - INFO - joeynmt.training - Epoch  11, Step:   127600, Batch Loss:     2.523544, Tokens per Sec:    16663, Lr: 0.000210\n",
      "2021-07-27 08:37:05,826 - INFO - joeynmt.training - Epoch  11, Step:   127700, Batch Loss:     2.365040, Tokens per Sec:    16722, Lr: 0.000210\n",
      "2021-07-27 08:37:19,352 - INFO - joeynmt.training - Epoch  11, Step:   127800, Batch Loss:     1.654168, Tokens per Sec:    16762, Lr: 0.000210\n",
      "2021-07-27 08:37:26,458 - INFO - joeynmt.training - Epoch  11: total training loss 5617.86\n",
      "2021-07-27 08:37:26,459 - INFO - joeynmt.training - Training ended after  11 epochs.\n",
      "2021-07-27 08:37:26,459 - INFO - joeynmt.training - Best validation result (greedy) at step   125000:  12.17 ppl.\n",
      "2021-07-27 08:37:26,481 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 8000 (with beam_size)\n",
      "2021-07-27 08:37:26,843 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
      "2021-07-27 08:37:27,043 - INFO - joeynmt.model - Enc-dec model built.\n",
      "2021-07-27 08:37:27,115 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_dev.bpe.en)...\n",
      "2021-07-27 08:38:01,728 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:38:01,728 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:38:01,728 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:38:02,031 - INFO - joeynmt.prediction -  dev bleu[13a]:   8.11 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-27 08:38:02,035 - INFO - joeynmt.prediction - Translations saved to: models/back_lhen_reverse_transformer_continued3/00125000.hyps.dev\n",
      "2021-07-27 08:38:02,036 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Luhyia/back_test.bpe.en)...\n",
      "2021-07-27 08:38:35,790 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
      "2021-07-27 08:38:35,790 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "2021-07-27 08:38:35,790 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
      "2021-07-27 08:38:36,121 - INFO - joeynmt.prediction - test bleu[13a]:   8.13 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2021-07-27 08:38:36,126 - INFO - joeynmt.prediction - Translations saved to: models/back_lhen_reverse_transformer_continued3/00125000.hyps.test\n"
     ]
    }
   ],
   "source": [
    "# Training continued\n",
    "!cd joeynmt; python3 -m joeynmt train configs/back_transformer_reverse_lhen_reload3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4lBkqSz2wRS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rHeLcKzLJyvx"
   ],
   "include_colab_link": true,
   "name": "Baseline_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
