{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LugandaLuhya_Multilingual_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yZHwRtRiK-mL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EverlynAsiko/Neural_Machine_Translation_for_African_Languages/blob/main/LugandaLuhya_Multilingual_NMT_results1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6cJZzBRP1-N"
      },
      "source": [
        "# Multilingual neural machine translation.\n",
        "\n",
        "For this case, we shall to a many-to-one translation:\n",
        "{Luganda, Luhya} to English. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4V-O3nJPsAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2d34d4-04d2-4e01-ed4c-0592f41af7bd"
      },
      "source": [
        "# Linking to drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcRP_CqbRQzj"
      },
      "source": [
        "# Importing needed libraries for preprocessing and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "collapsed": true,
        "id": "grB3V9FhReiZ",
        "outputId": "0f59bff7-ca74-43f6-e638-fcbb702b95ef"
      },
      "source": [
        "#@title Default title text\n",
        "# Install Pytorch with GPU support v1.8.0.\n",
        "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 763.5 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7jAsiRLRlMs"
      },
      "source": [
        "# Filtering warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaH6F-u3RrAb"
      },
      "source": [
        "# Loading the drive\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH_IYggrTJKa"
      },
      "source": [
        "# Setting source and target languages\n",
        "source_language = \"en\"\n",
        "target_language = \"lg_lh\"\n",
        "\n",
        "os.environ[\"src\"] = source_language \n",
        "os.environ[\"tgt\"] = target_language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G0mZmUETh-A"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VI9NWxchThje",
        "outputId": "fbf4e6b5-7f29-4e6b-cac7-83d82556a456"
      },
      "source": [
        "! head Luganda/train.*\n",
        "! head Luganda/dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Luganda/train.bpe.en <==\n",
            "Ev@@ en@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
            "At last , I have the st@@ able family life that I always cr@@ av@@ ed , and I have the loving Father that I always wanted .\n",
            "I was a new husband , only 25 years old and very in@@ experienced , but off we went with confidence in Jehovah .\n",
            "What can you do to show these de@@ a@@ f brothers personal attention ?\n",
            "R@@ ef@@ er@@ r@@ ing to what the rul@@ er@@ ship of God’s Son will accompl@@ ish , Isaiah 9 : 7 says : “ The very z@@ eal of Jehovah of arm@@ ies will do this . ”\n",
            "Jesus is the m@@ igh@@ ti@@ est of all of Jehovah’s spirit sons .\n",
            "The ste@@ ad@@ f@@ ast example set by J@@ ac@@ o@@ b and R@@ ac@@ he@@ l no doubt had a powerful effect on their son Joseph , influ@@ enc@@ ing how he would hand@@ le t@@ ests of his own faith .\n",
            "When s@@ ent@@ enc@@ ing “ the orig@@ in@@ al ser@@ p@@ ent , ” Satan the Devil , God said : “ I shall put en@@ m@@ ity between you and the woman and between your se@@ ed and her se@@ ed . He will br@@ u@@ ise you in the head and you will br@@ u@@ ise him in the h@@ ee@@ l . ”\n",
            "Will this or@@ de@@ al bring David down to S@@ he@@ ol in g@@ ri@@ ef and dis@@ gr@@ ace ?\n",
            "How can Christian love help to strengthen the marriage b@@ ond ?\n",
            "\n",
            "==> Luganda/train.bpe.lg <==\n",
            "Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
            "N@@ ze ne mukyala wange tuli basanyufu , era nk@@ imanyi nti Katonda anj@@ agala nnyo .\n",
            "Mu kiseera ekyo , nnali nn@@ aak@@ aw@@ asa , nga ndi wa myaka 25 gy@@ okka , era nga s@@ ir@@ ina b@@ um@@ any@@ irivu . Naye nnali muk@@ akafu nti Yakuwa yali ajja ku@@ nn@@ yamba .\n",
            "Mu kibiina k@@ yo bwe mu@@ ba nga mulimu bak@@ igg@@ ala , oyinza kukola ki okulaga nti o@@ faayo ku b’@@ oluganda abo ?\n",
            "Isaaya 9 : 7 wal@@ aga nti Omwana wa Katonda y@@ andibadde Kabaka era nti yand@@ ikol@@ edde abantu ebintu ebirungi bingi .\n",
            "Yesu y’@@ as@@ inga obuyinza mu ba@@ ana ba Yakuwa bonna ab’@@ omwoyo .\n",
            "Eky@@ okulabirako ekirungi Yakobo ne L@@ aak@@ e@@ eri kye baat@@ ek@@ awo mu kw@@ oleka obug@@ umiikiriza ky@@ akwata nnyo ku mutabani waabwe Yusufu , era ekyo ky@@ amu@@ yamba nnyo bwe yay@@ olekagana n’@@ embeera ez@@ aag@@ ez@@ esa okukkiriza kwe .\n",
            "Bwe yali as@@ alira omusango “ omus@@ ot@@ a ogw’@@ edda , ” Setaani Omulyolyomi , Katonda yagamba : “ O@@ bul@@ abe n’@@ abu@@ teek@@ anga wakati wo n’@@ omukazi , era ne wakati w’@@ ez@@ zadde l@@ yo n’@@ ez@@ zadde ly’@@ omukazi : ( ez@@ zadde ly’@@ omukazi ) l@@ iri@@ ku@@ be@@ t@@ ent@@ a omutwe , naawe ol@@ ir@@ ib@@ et@@ ent@@ a ekis@@ inzi@@ iro . ”\n",
            "Em@@ beera eno en@@ zibu en@@ e@@ er@@ eetera Dawudi okuk@@ ka em@@ ag@@ om@@ be nga mun@@ aku@@ w@@ avu ?\n",
            "Okwagala kw’@@ Ekikristaayo ku@@ yinza kutya okuny@@ weza obufumbo ?\n",
            "\n",
            "==> Luganda/train.en <==\n",
            "Eventually , however , the truths I learned from the Bible began to sink deeper into my heart . I realized that if I wanted to serve Jehovah , I had to change my political viewpoints and associations .\n",
            "At last , I have the stable family life that I always craved , and I have the loving Father that I always wanted .\n",
            "I was a new husband , only 25 years old and very inexperienced , but off we went with confidence in Jehovah .\n",
            "What can you do to show these deaf brothers personal attention ?\n",
            "Referring to what the rulership of God’s Son will accomplish , Isaiah 9 : 7 says : “ The very zeal of Jehovah of armies will do this . ”\n",
            "Jesus is the mightiest of all of Jehovah’s spirit sons .\n",
            "The steadfast example set by Jacob and Rachel no doubt had a powerful effect on their son Joseph , influencing how he would handle tests of his own faith .\n",
            "When sentencing “ the original serpent , ” Satan the Devil , God said : “ I shall put enmity between you and the woman and between your seed and her seed . He will bruise you in the head and you will bruise him in the heel . ”\n",
            "Will this ordeal bring David down to Sheol in grief and disgrace ?\n",
            "How can Christian love help to strengthen the marriage bond ?\n",
            "\n",
            "==> Luganda/train.lg <==\n",
            "Naye oluvannyuma lw’ekiseera , nnatandika okukolera ku mazima ge nnali njiga , era nnakiraba nti okusobola okuweereza Yakuwa nnalina okuva mu by’obufuzi n’okuleka emikwano emibi gye nnalina .\n",
            "Nze ne mukyala wange tuli basanyufu , era nkimanyi nti Katonda anjagala nnyo .\n",
            "Mu kiseera ekyo , nnali nnaakawasa , nga ndi wa myaka 25 gyokka , era nga sirina bumanyirivu . Naye nnali mukakafu nti Yakuwa yali ajja kunnyamba .\n",
            "Mu kibiina kyo bwe muba nga mulimu bakiggala , oyinza kukola ki okulaga nti ofaayo ku b’oluganda abo ?\n",
            "Isaaya 9 : 7 walaga nti Omwana wa Katonda yandibadde Kabaka era nti yandikoledde abantu ebintu ebirungi bingi .\n",
            "Yesu y’asinga obuyinza mu baana ba Yakuwa bonna ab’omwoyo .\n",
            "Ekyokulabirako ekirungi Yakobo ne Laakeeri kye baatekawo mu kwoleka obugumiikiriza kyakwata nnyo ku mutabani waabwe Yusufu , era ekyo kyamuyamba nnyo bwe yayolekagana n’embeera ezaagezesa okukkiriza kwe .\n",
            "Bwe yali asalira omusango “ omusota ogw’edda , ” Setaani Omulyolyomi , Katonda yagamba : “ Obulabe n’abuteekanga wakati wo n’omukazi , era ne wakati w’ezzadde lyo n’ezzadde ly’omukazi : ( ezzadde ly’omukazi ) lirikubetenta omutwe , naawe oliribetenta ekisinziiro . ”\n",
            "Embeera eno enzibu eneereetera Dawudi okukka emagombe nga munakuwavu ?\n",
            "Okwagala kw’Ekikristaayo kuyinza kutya okunyweza obufumbo ?\n",
            "==> Luganda/dev.bpe.en <==\n",
            "But if , when you are doing good and you su@@ ff@@ er , you end@@ ure it , this is a thing ag@@ re@@ e@@ able with God . ”\n",
            "At his bap@@ tism , Jesus heard a vo@@ ice from heaven say : “ This is my Son , the bel@@ ov@@ ed , whom I have ap@@ proved . ”\n",
            "( b ) How did Jehovah answer Jesus ’ personal requ@@ est about his future ?\n",
            "But Ab@@ ig@@ a@@ il took action to s@@ ave her hou@@ se@@ hold .\n",
            "While anger is not one of God’s d@@ om@@ in@@ ant qualities , he is prov@@ ok@@ ed to righteous ind@@ ign@@ ation by del@@ ib@@ er@@ ate ac@@ ts of in@@ justice , especially when the v@@ ic@@ tim@@ s are v@@ ul@@ n@@ er@@ able ones . ​ — Psalm 10@@ 3 : 6 .\n",
            "TH@@ E TH@@ R@@ E@@ A@@ T : Mic@@ r@@ ob@@ es that live har@@ m@@ less@@ ly in@@ side an an@@ im@@ al can th@@ reat@@ en your health .\n",
            "What does Jehovah exp@@ ect of us in our service to him ?\n",
            "Jesus said that ‘ the Father in heaven gives holy spirit to those as@@ king him . ’\n",
            "9 , 10 . ( a ) What did Jehovah allow the Babyl@@ on@@ ians to do ?\n",
            "( b ) In harmon@@ y with Phili@@ pp@@ ians 1 : 7 , how have Jehovah’s people re@@ ac@@ ted to Satan’s anger ?\n",
            "\n",
            "==> Luganda/dev.bpe.lg <==\n",
            "[ N ] aye bwe muk@@ ola obulungi ne mu@@ b@@ on@@ ya@@ abon@@ y@@ ez@@ ebwa bwe mul@@ ig@@ umiikiriza , ekyo kye kis@@ iimibwa eri Katonda . ”\n",
            "Yesu bwe yali ya@@ ak@@ amala okub@@ atizibwa , y@@ awulira edd@@ oboozi okuva mu ggulu nga l@@ ig@@ amba nti : “ O@@ no ye M@@ wana wange omw@@ agal@@ wa gwe n@@ si@@ ima . ”\n",
            "( b ) Yakuwa y@@ addamu atya ekyo Yesu kye ye@@ es@@ abira ?\n",
            "Naye Ab@@ big@@ ay@@ iri alina kye yak@@ ol@@ awo okusobola okuw@@ onya ab’omu maka ge .\n",
            "Wadde ng’@@ obus@@ ungu si ngeri ya Yakuwa en@@ kulu , as@@ ung@@ u@@ wala singa ab@@ an@@ aku n’@@ abat@@ alina bu@@ yambi bay@@ is@@ ibwa mu ngeri et@@ ali ya bw@@ enkanya . ​ — Zabbuli 10@@ 3 : 6 .\n",
            "O@@ B@@ UL@@ AB@@ E : Obu@@ w@@ uka obumu bus@@ obola okubeera mu bis@@ olo , ne bwe bib@@ a bya w@@ aka , ne bit@@ afuna bu@@ zibu bwonna , naye ate nga bwe bu@@ ku@@ y@@ ingiramu bu@@ kul@@ wa@@ za .\n",
            "Kiki Yakuwa ky’@@ at@@ us@@ uubir@@ amu nga tu@@ mu@@ weereza ?\n",
            "Yesu yagamba nti ‘ Kitaffe ali mu ggulu awa omwoyo omutukuvu abo ab@@ amus@@ aba . ’\n",
            "9 , 10 . ( a ) Kiki Yakuwa kye yak@@ kiriza Ab@@ abab@@ ulooni okukola ?\n",
            "( b ) Nga kitu@@ uk@@ agana ne Aba@@ f@@ iri@@ pi 1 : 7 , abantu ba Yakuwa bak@@ oze ki nga Setaani ab@@ o@@ olek@@ ezza obus@@ ungu bwe ?\n",
            "\n",
            "==> Luganda/dev.en <==\n",
            "But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "But Abigail took action to save her household .\n",
            "While anger is not one of God’s dominant qualities , he is provoked to righteous indignation by deliberate acts of injustice , especially when the victims are vulnerable ones . ​ — Psalm 103 : 6 .\n",
            "THE THREAT : Microbes that live harmlessly inside an animal can threaten your health .\n",
            "What does Jehovah expect of us in our service to him ?\n",
            "Jesus said that ‘ the Father in heaven gives holy spirit to those asking him . ’\n",
            "9 , 10 . ( a ) What did Jehovah allow the Babylonians to do ?\n",
            "( b ) In harmony with Philippians 1 : 7 , how have Jehovah’s people reacted to Satan’s anger ?\n",
            "\n",
            "==> Luganda/dev.lg <==\n",
            "[ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "Wadde ng’obusungu si ngeri ya Yakuwa enkulu , asunguwala singa abanaku n’abatalina buyambi bayisibwa mu ngeri etali ya bwenkanya . ​ — Zabbuli 103 : 6 .\n",
            "OBULABE : Obuwuka obumu busobola okubeera mu bisolo , ne bwe biba bya waka , ne bitafuna buzibu bwonna , naye ate nga bwe bukuyingiramu bukulwaza .\n",
            "Kiki Yakuwa ky’atusuubiramu nga tumuweereza ?\n",
            "Yesu yagamba nti ‘ Kitaffe ali mu ggulu awa omwoyo omutukuvu abo abamusaba . ’\n",
            "9 , 10 . ( a ) Kiki Yakuwa kye yakkiriza Abababulooni okukola ?\n",
            "( b ) Nga kituukagana ne Abafiripi 1 : 7 , abantu ba Yakuwa bakoze ki nga Setaani aboolekezza obusungu bwe ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IP5nJ822UGJM",
        "outputId": "0c13f8a7-ccea-4a57-d855-ff866c26bb5b"
      },
      "source": [
        "! head Luhyia/train.*\n",
        "! head Luhyia/dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Luhyia/train.bpe.en <==\n",
            "Then Pilate entered the P@@ ra@@ et@@ or@@ i@@ um again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
            "If anyone th@@ in@@ ks himself to be a prophet or spirit@@ ual , let him ac@@ knowledge that the things which I write to you are the commandments of the Lord .\n",
            "E@@ very br@@ an@@ ch in Me that does not bear fruit He tak@@ es away ; and every br@@ an@@ ch that be@@ ars fruit He pr@@ un@@ es , that it may bear more fruit .\n",
            "D@@ em@@ et@@ ri@@ us has a good testimony from all , and from the truth its@@ el@@ f . And we also bear witness , and you know that our testimony is true .\n",
            "And supp@@ er being ended , the devil having already put it into the heart of Judas Is@@ c@@ ari@@ ot , Simon ’ s son , to betr@@ ay Him ,\n",
            "im@@ pl@@ or@@ ing us with much ur@@ gen@@ c@@ y that we would receive the gift and the fel@@ low@@ ship of the minis@@ ter@@ ing to the saints .\n",
            "It is written in the prophets , ‘ And they shall all be taught by G@@ od@@ . ’ Therefore everyone who has heard and lear@@ ned from the Father comes to Me .\n",
            "For those who are such do not serve our Lord Jesus Christ , but their own bel@@ ly , and by sm@@ oo@@ th words and fl@@ at@@ ter@@ ing spe@@ e@@ ch dece@@ ive the hearts of the s@@ im@@ ple .\n",
            "So when he had received food , he was streng@@ th@@ ened . Then Saul sp@@ ent some days with the disciples at Damas@@ cus .\n",
            "Therefore if you have not been faithful in the un@@ righteous m@@ am@@ m@@ on , who will comm@@ it to your tr@@ ust the true riches ?\n",
            "\n",
            "==> Luhyia/train.bpe.lh <==\n",
            "Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
            "Omundu yesi ow@@ il@@ olanga mbu , nomu@@ r@@ umwa , wa Nyasaye noho mbu ali neshi@@ haanwa eshia Roho okhuula amany@@ e khandi afuchil@@ ile mbu , aka , emu@@ handi@@ chilanga kano nel@@ ilako elia Omwami .\n",
            "A@@ rem@@ anga buli lis@@ aka , mwisie el@@ il@@ am@@ anga ebiamo ta , ne akh@@ alilanga buli lis@@ aka , eli@@ am@@ anga ebiamo kho mbu , li@@ be lil@@ ayi nil@@ im@@ e@@ eta okhw@@ ama , ebiamo ebinji .\n",
            "Buli mundu amw@@ itsoom@@ injia D@@ em@@ eter@@ io ; ne obw@@ atieli , bwene bu@@ mw@@ itsoom@@ injia . Ne nasi em@@ e@@ et@@ akhwo obuloli , bwanje , ne mumanyile mbu ak@@ emb@@ oola n@@ akatoto . Am@@ ash@@ esio K@@ okhumalil@@ isia ,\n",
            "Yesu nende abeechibe bali nib@@ ali@@ itsanga eshiokhulia , eshia ha@@ muk@@ ol@@ oba . Setani yali namalile okhur@@ a mu Yuda omwana wa Simoni Is@@ ik@@ ari@@ o@@ ti amapaaro k@@ okhukh@@ oba Yes@@ u. ,\n",
            "bakhu@@ s@@ aba nib@@ akhu@@ saaya okhu@@ fuchil@@ ilwa okhus@@ anga , mukhu@@ khon@@ ya abakristo bashi@@ abwe aba Yudea .\n",
            "Ab@@ al@@ akusi , b@@ ahandika mbu , ‘ Buli mundu ali@@ eches@@ ibwa nende , Nyasaye. ’ Kho oyo yesi ou@@ hulilanga aka Papa nende , okhw@@ eka okhurula khuye , yetsa khwisie .\n",
            "Okhuba , abakholanga amakhuwa kario shi@@ bakh@@ alaban@@ ilanga Kristo , Omwami wefwe ta , habula bakh@@ alaban@@ ilanga ts@@ inda , tsiabwe abeene . B@@ ekh@@ oonyelanga amakhuwa kabwe , k@@ okhu@@ ka@@ at@@ ilisia nende ak@@ okhul@@ aha khulwa okhu@@ ka@@ atia , amapaaro k@@ abat@@ eshele .\n",
            "Ne olwa yamala , okhulia eshiokhulia , omubil@@ ikwe kw@@ anyoola amaani . Saulo ayaala Injiili Damas@@ iko Saulo y@@ amenya Damas@@ iko halala nab@@ asuubili khulwa , tsinyanga tsind@@ i@@ iti .\n",
            "Kho , nimul@@ aba ab@@ esi@@ ikwa mubu@@ yinda , bw@@ omushialo shino ta , mwakh@@ aba murie ab@@ esi@@ ikwa , mubu@@ yinda bwatoto ?\n",
            "\n",
            "==> Luhyia/train.en <==\n",
            "Then Pilate entered the Praetorium again , called Jesus , and said to Him , “ Are You the King of the Jews ? ”\n",
            "If anyone thinks himself to be a prophet or spiritual , let him acknowledge that the things which I write to you are the commandments of the Lord .\n",
            "Every branch in Me that does not bear fruit He takes away ; and every branch that bears fruit He prunes , that it may bear more fruit .\n",
            "Demetrius has a good testimony from all , and from the truth itself . And we also bear witness , and you know that our testimony is true .\n",
            "And supper being ended , the devil having already put it into the heart of Judas Iscariot , Simon ’ s son , to betray Him ,\n",
            "imploring us with much urgency that we would receive the gift and the fellowship of the ministering to the saints .\n",
            "It is written in the prophets , ‘ And they shall all be taught by God. ’ Therefore everyone who has heard and learned from the Father comes to Me .\n",
            "For those who are such do not serve our Lord Jesus Christ , but their own belly , and by smooth words and flattering speech deceive the hearts of the simple .\n",
            "So when he had received food , he was strengthened . Then Saul spent some days with the disciples at Damascus .\n",
            "Therefore if you have not been faithful in the unrighteous mammon , who will commit to your trust the true riches ?\n",
            "\n",
            "==> Luhyia/train.lh <==\n",
            "Pilato nakalukha itookho , ne nalanga Yesu , namureeba , ari , “ Iwe niwe omuruchi wa Abayahudi ? ”\n",
            "Omundu yesi owilolanga mbu , nomurumwa , wa Nyasaye noho mbu ali neshihaanwa eshia Roho okhuula amanye khandi afuchilile mbu , aka , emuhandichilanga kano nelilako elia Omwami .\n",
            "Aremanga buli lisaka , mwisie elilamanga ebiamo ta , ne akhalilanga buli lisaka , eliamanga ebiamo kho mbu , libe lilayi nilimeeta okhwama , ebiamo ebinji .\n",
            "Buli mundu amwitsoominjia Demeterio ; ne obwatieli , bwene bumwitsoominjia . Ne nasi emeetakhwo obuloli , bwanje , ne mumanyile mbu akemboola nakatoto . Amashesio Kokhumalilisia ,\n",
            "Yesu nende abeechibe bali nibaliitsanga eshiokhulia , eshia hamukoloba . Setani yali namalile okhura mu Yuda omwana wa Simoni Isikarioti amapaaro kokhukhoba Yesu. ,\n",
            "bakhusaba nibakhusaaya okhufuchililwa okhusanga , mukhukhonya abakristo bashiabwe aba Yudea .\n",
            "Abalakusi , bahandika mbu , ‘ Buli mundu aliechesibwa nende , Nyasaye. ’ Kho oyo yesi ouhulilanga aka Papa nende , okhweka okhurula khuye , yetsa khwisie .\n",
            "Okhuba , abakholanga amakhuwa kario shibakhalabanilanga Kristo , Omwami wefwe ta , habula bakhalabanilanga tsinda , tsiabwe abeene . Bekhoonyelanga amakhuwa kabwe , kokhukaatilisia nende akokhulaha khulwa okhukaatia , amapaaro kabateshele .\n",
            "Ne olwa yamala , okhulia eshiokhulia , omubilikwe kwanyoola amaani . Saulo ayaala Injiili Damasiko Saulo yamenya Damasiko halala nabasuubili khulwa , tsinyanga tsindiiti .\n",
            "Kho , nimulaba abesiikwa mubuyinda , bwomushialo shino ta , mwakhaba murie abesiikwa , mubuyinda bwatoto ?\n",
            "==> Luhyia/dev.bpe.en <==\n",
            "Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put cl@@ ay on my eyes , and I w@@ ash@@ ed , and I see . ”\n",
            "And when she had said these things , she went her way and secre@@ tly called Mary her sis@@ ter , saying , “ The Teacher has come and is cal@@ ling for you . ”\n",
            "T@@ hat day was the P@@ re@@ par@@ ation , and the Sabbath drew near .\n",
            "But when Paul had gathered a bu@@ nd@@ le of st@@ ic@@ ks and laid them on the fire , a vi@@ p@@ er came out because of the he@@ at , and f@@ ast@@ ened on his hand .\n",
            "not given to wine , not vi@@ ol@@ ent , not gre@@ ed@@ y for money , but g@@ ent@@ le , not qu@@ ar@@ rel@@ some , not cov@@ et@@ ous ;\n",
            "and this woman was a wi@@ do@@ w of about ei@@ gh@@ t@@ y-@@ four years , who did not depart from the temple , but serv@@ ed God with f@@ ast@@ ings and pray@@ ers night and day .\n",
            "And not being weak in faith , he did not consi@@ der his own body , already dead ( since he was about a hundred years old ) , and the de@@ ad@@ ness of S@@ ar@@ ah ’ s wom@@ b .\n",
            "In these lay a great multitude of sick people , blind , l@@ ame , par@@ al@@ y@@ zed , wa@@ it@@ ing for the m@@ ov@@ ing of the water .\n",
            "Then he go@@ es and tak@@ es with him seven other spirits more wi@@ cked than himself , and they enter and dwell there ; and the last st@@ ate of that man is wor@@ se than the first . So shall it also be with this wi@@ cked generation . ”\n",
            "So He got into a boat , cr@@ os@@ sed over , and came to His own city .\n",
            "\n",
            "==> Luhyia/dev.bpe.lh <==\n",
            "Abafarisayo nabo nib@@ areeba omundu oyo shinga olwa , yali n@@ any@@ ali@@ ilwe okhulola khandi . Naye nababoolela ari “ Ab@@ ashi@@ le lit@@ oy@@ i khum@@ oni tsi@@ anje , ne nind@@ iy@@ osia mumoni mana bulano enyala okhulola . ”\n",
            "Olwa Mar@@ itsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe m@@ as@@ ili@@ isi , ne , namuboolela ari , “ Omwechesia yets@@ ile ali hano , ne , a@@ khul@@ anganga . ”\n",
            "Yali , inyanga yo@@ khw@@ ire@@ chekha khulwa inyanga eya Isabato e@@ yali niy@@ ili ahambi okhu@@ chaaka .\n",
            "Paulo yab@@ unj@@ elesia olu@@ kh@@ any@@ a , lwe@@ tsi@@ khw@@ i , ne olwa yali n@@ ats@@ ir@@ er@@ anga khu@@ mul@@ ilo , khulwa , olu@@ u@@ ya lw@@ omulilo okwo , inz@@ okha yar@@ ulamwo niy@@ ik@@ any@@ ila , khumu@@ khonokwe .\n",
            "alaba , omum@@ esi kata ow@@ obus@@ olo ta , habula omu@@ h@@ olo khandi ow@@ omulembe , kata ow@@ ala@@ heela amapesa tawe .\n",
            "Khandi y@@ amenya nali omule@@ khwa khulw@@ emiyika , amakhumi mun@@ ane na@@ chi@@ ne . Nebutswa emiyika echio , chiosi , y@@ amenyanga butswa muhekalu . Y@@ enam@@ ilanga , OMWAMI Nyasaye eshilo neshi@@ teere , n@@ ahon@@ ga inzala , nende okhusaaya .\n",
            "Yali ahambi ow@@ emiyika , eshi@@ khumi shilala , nebutswa obusuub@@ ilibwe , shi@@ bw@@ at@@ it@@ iy@@ akhwo kata olwa y@@ apaara khu@@ bul@@ amu , bw@@ omubil@@ ikwe okw@@ ali nik@@ w@@ ah@@ w@@ amwo amaani ta , noho , kata olwa y@@ amanya mbu , S@@ ara nomu@@ ko@@ fu shianyala , okhw@@ ibula tawe .\n",
            "Omuk@@ anda , omukhongo kwabandu , abalwale , abab@@ ofu , abal@@ ema nende , abak@@ wa amak@@ ara , bali nib@@ ak@@ on@@ ile mub@@ ir@@ o@@ ok@@ oola ebio .\n",
            "nishi@@ kal@@ ukh@@ ayo shi@@ tsia okhul@@ anga ebishieno b@@ indi , musafu ebib@@ i muno , nibi@@ ch@@ elela okhum@@ eny@@ amwo . Ne , olunyuma lw@@ okhumw@@ injil@@ amwo , omundu tsana aba obubi , okhushilakhwo shinga olwa yali olw@@ ambeli . Ak@@ o niko , ak@@ atsia okhwikholekha khubandu b@@ olwibulo ol@@ um@@ ayanu , lwa bul@@ an@@ o. ” N@@ y@@ ina Yesu nende abaana babwe ,\n",
            "Yesu yenjila muliaro niy@@ am@@ bu@@ kha ni@@ yoola mwitaala , li@@ ew@@ abwe elia K@@ aper@@ in@@ a@@ umu .\n",
            "\n",
            "==> Luhyia/dev.en <==\n",
            "Then the Pharisees also asked him again how he had received his sight . He said to them , “ He put clay on my eyes , and I washed , and I see . ”\n",
            "And when she had said these things , she went her way and secretly called Mary her sister , saying , “ The Teacher has come and is calling for you . ”\n",
            "That day was the Preparation , and the Sabbath drew near .\n",
            "But when Paul had gathered a bundle of sticks and laid them on the fire , a viper came out because of the heat , and fastened on his hand .\n",
            "not given to wine , not violent , not greedy for money , but gentle , not quarrelsome , not covetous ;\n",
            "and this woman was a widow of about eighty-four years , who did not depart from the temple , but served God with fastings and prayers night and day .\n",
            "And not being weak in faith , he did not consider his own body , already dead ( since he was about a hundred years old ) , and the deadness of Sarah ’ s womb .\n",
            "In these lay a great multitude of sick people , blind , lame , paralyzed , waiting for the moving of the water .\n",
            "Then he goes and takes with him seven other spirits more wicked than himself , and they enter and dwell there ; and the last state of that man is worse than the first . So shall it also be with this wicked generation . ”\n",
            "So He got into a boat , crossed over , and came to His own city .\n",
            "\n",
            "==> Luhyia/dev.lh <==\n",
            "Abafarisayo nabo nibareeba omundu oyo shinga olwa , yali nanyaliilwe okhulola khandi . Naye nababoolela ari “ Abashile litoyi khumoni tsianje , ne nindiyosia mumoni mana bulano enyala okhulola . ”\n",
            "Olwa Maritsa yamala okhuboola amakhuwa ako , yatsia , nalanga Mariamu omukhaana wabwe masiliisi , ne , namuboolela ari , “ Omwechesia yetsile ali hano , ne , akhulanganga . ”\n",
            "Yali , inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka .\n",
            "Paulo yabunjelesia olukhanya , lwetsikhwi , ne olwa yali natsireranga khumulilo , khulwa , oluuya lwomulilo okwo , inzokha yarulamwo niyikanyila , khumukhonokwe .\n",
            "alaba , omumesi kata owobusolo ta , habula omuholo khandi owomulembe , kata owalaheela amapesa tawe .\n",
            "Khandi yamenya nali omulekhwa khulwemiyika , amakhumi munane nachine . Nebutswa emiyika echio , chiosi , yamenyanga butswa muhekalu . Yenamilanga , OMWAMI Nyasaye eshilo neshiteere , nahonga inzala , nende okhusaaya .\n",
            "Yali ahambi owemiyika , eshikhumi shilala , nebutswa obusuubilibwe , shibwatitiyakhwo kata olwa yapaara khubulamu , bwomubilikwe okwali nikwahwamwo amaani ta , noho , kata olwa yamanya mbu , Sara nomukofu shianyala , okhwibula tawe .\n",
            "Omukanda , omukhongo kwabandu , abalwale , ababofu , abalema nende , abakwa amakara , bali nibakonile mubirookoola ebio .\n",
            "nishikalukhayo shitsia okhulanga ebishieno bindi , musafu ebibi muno , nibichelela okhumenyamwo . Ne , olunyuma lwokhumwinjilamwo , omundu tsana aba obubi , okhushilakhwo shinga olwa yali olwambeli . Ako niko , akatsia okhwikholekha khubandu bolwibulo olumayanu , lwa bulano. ” Nyina Yesu nende abaana babwe ,\n",
            "Yesu yenjila muliaro niyambukha niyoola mwitaala , liewabwe elia Kaperinaumu .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_D0BqDaUOF_"
      },
      "source": [
        "pre = '/content/gdrive/Shared drives/NMT_for_African_Language/'\n",
        "# Train data source\n",
        "filenames = [pre+'Luganda/train.en',pre+'Luhyia/train.en']\n",
        "\n",
        "# Train data target\n",
        "filenames2 = [pre+'Luganda/train.lg',pre+'Luhyia/train.lh']\n",
        "\n",
        "# Dev data source\n",
        "file1 = [pre+'Luganda/dev.en',pre+'Luhyia/dev.en']\n",
        "\n",
        "# Dev data target\n",
        "file2 = [pre+'Luganda/dev.lg',pre+'Luhyia/dev.lh']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EtJJpkCH9ka"
      },
      "source": [
        "pre2 = '/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual/'\n",
        "trun_file1 = [pre2+'train_trunc.en1',pre+'Luhyia/train.en']\n",
        "trun_file2 = [pre2+'train.lg',pre+'Luhyia/train.lh']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icPaGn2GlpM-"
      },
      "source": [
        "# Changing to Multilingual2 directory\n",
        "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5zhE2IimjPs"
      },
      "source": [
        "# Procedure to create concatenated files\n",
        "def create_file(x,filename):\n",
        "  # Open filename in write mode\n",
        "  with open(filename, 'w') as outfile:\n",
        "      for names in x:\n",
        "          # Open each file in read mode\n",
        "          with open(names) as infile:\n",
        "              # read the data and write it in file3\n",
        "              outfile.write(infile.read())\n",
        "          outfile.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay8047V3oyeG"
      },
      "source": [
        "# Creating multilingual files\n",
        "create_file(filenames,'train.en')\n",
        "create_file(filenames2,'train.lg_lh')\n",
        "create_file(file1,'dev.en')\n",
        "create_file(file2,'dev.lg_lh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VouMO0lishSQ"
      },
      "source": [
        "### BPE codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5lK1btbKswMN",
        "outputId": "57475bd0-0d14-455a-febd-154f767abf64"
      },
      "source": [
        "#! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual2/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
            "Collecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 92 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 21.5 MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 60.2 MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "  Downloading pylint-2.9.5-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 62.5 MB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting astroid<2.7,>=2.6.5\n",
            "  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
            "Collecting isort<6,>=4.2.5\n",
            "  Downloading isort-5.9.2-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 72.7 MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 58.7 MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
            "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
            "Building wheels for collected packages: joeynmt, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=ae58d71a3855292c914018e9a129dd7f404e00807ccc39d5089f4641c0a3a3cf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e8z7r0w3/wheels/b2/63/79/00b1ca041c00d851cc67df0e726b6636bbb52e38c09a484bbe\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68444 sha256=e3b1014c8f57b72eff0d0df57350e0d6553e396006b0e11af93361959bb704d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
            "Successfully built joeynmt wrapt\n",
            "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
            "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed astroid-2.6.5 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.5 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOYk3OK4rw6d"
      },
      "source": [
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt --output_path vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ-Xn-Y1x50D"
      },
      "source": [
        "# Applying BPE to tests\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test1.$src > test.bpe.en1\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test1.lh > test.bpe.lh\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test2.$src > test.bpe.en2\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test2.lg > test.bpe.lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj0HdPZ6t5i3",
        "outputId": "96f80147-07ba-4bde-aafd-2b53ce11392e"
      },
      "source": [
        "# Some output\n",
        "! echo \"BPE Sentences\"\n",
        "! tail -n 5 test.bpe.lh\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 vocab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BPE Sentences\n",
            "N@@ asi , n@@ ir@@ e@@ eba end@@ i , ‘ N@@ i@@ we w@@ ina , Omwami ? ’ Omwoyo okwo , n@@ i@@ kum@@ bo@@ ol@@ ela ku@@ r@@ i , ‘ N@@ is@@ ie Yesu owa N@@ az@@ ar@@ et@@ i ow@@ os@@ aan@@ d@@ inj@@ ia . ’\n",
            "sh@@ ich@@ ila , omuk@@ h@@ aan@@ a@@ we omut@@ el@@ wa , ow@@ em@@ iy@@ ika ek@@ hum@@ i n@@ ach@@ ib@@ ili yali n@@ any@@ iranga . Ne olwa yali nat@@ s@@ it@@ s@@ anga , abandu , bam@@ w@@ ib@@ um@@ bak@@ h@@ wo okhur@@ ula mut@@ sim@@ b@@ eka t@@ si@@ osi .\n",
            "Ne olwa k@@ ab@@ is@@ ibwa mbu kh@@ uk@@ ho@@ y@@ ile okh@@ ut@@ si@@ ila , mum@@ e@@ el@@ i okh@@ uula I@@ t@@ ali@@ a , ba@@ h@@ aana Pa@@ ulo nende abab@@ o@@ he , b@@ andi kh@@ um@@ us@@ inj@@ il@@ ili w@@ el@@ i@@ he J@@ ul@@ i@@ asi o@@ we@@ ing@@ '@@ anda eya , esh@@ ir@@ oma ey@@ il@@ ang@@ wa mbu , “ I@@ ng@@ '@@ anda ey@@ il@@ ind@@ anga , Om@@ ur@@ uc@@ h@@ i . ”\n",
            "Ol@@ uny@@ um@@ akh@@ wo , abaku@@ uka be@@ f@@ we , ab@@ abu@@ kula li@@ he@@ ema el@@ o okhur@@ ula kh@@ ub@@ as@@ abwe , bal@@ ich@@ inga , okh@@ uula mut@@ sin@@ y@@ anga t@@ sia Y@@ os@@ h@@ wa nib@@ abu@@ kula eshi@@ alo , eshi@@ a ama@@ h@@ anga aka Nyasaye yal@@ ond@@ anga n@@ ik@@ ar@@ ula im@@ bel@@ i , w@@ abwe . Ne li@@ am@@ eny@@ ayo okh@@ uula mut@@ sin@@ y@@ anga t@@ sia , om@@ ur@@ uc@@ h@@ i D@@ a@@ udi .\n",
            "Ne olwa , y@@ enj@@ il@@ am@@ wo , yab@@ ar@@ e@@ eba ari , “ M@@ w@@ ik@@ hu@@ ul@@ anga n@@ im@@ ukh@@ up@@ a , t@@ sim@@ b@@ ungu mb@@ us@@ hi@@ ina ? Omwana uno shi@@ af@@ w@@ ile ta , h@@ abula , ak@@ on@@ anga but@@ swa t@@ sin@@ do@@ olo . ”\n",
            "Combined BPE Vocab\n",
            "ereber@@\n",
            "ö\n",
            "Egy@@\n",
            "oseewo\n",
            "ʺ\n",
            "erefore\n",
            "taayo\n",
            "\\\n",
            "ŋ\n",
            "(@@\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gSynCGijJUlV",
        "outputId": "53106433-7318-4f5f-ea10-5c8406c26b47"
      },
      "source": [
        "! tail train.*\n",
        "! tail dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.bpe.en <==\n",
            "But if anyone lo@@ ves God , this one is known by Him .\n",
            "And the second is like it : ‘ You shall love your neigh@@ b@@ or as yourself . ’\n",
            "until the day in which He was taken up , after He through the H@@ ol@@ y Sp@@ irit had given command@@ ments to the apostles whom He had cho@@ s@@ en ,\n",
            "For what if some did not believe ? W@@ ill their un@@ belie@@ f make the faith@@ ful@@ ness of God without effect ?\n",
            "And when you go into a hou@@ se@@ hold , gre@@ et it .\n",
            "And a very great mul@@ t@@ itude sp@@ read their clo@@ th@@ es on the ro@@ ad ; others c@@ ut down br@@ an@@ ch@@ es from the tre@@ es and sp@@ read them on the ro@@ ad .\n",
            "And we heard this vo@@ ice which came from heaven when we were with Him on the holy mo@@ un@@ tain .\n",
            "O@@ r those e@@ igh@@ t@@ een on whom the tower in S@@ il@@ o@@ am f@@ ell and kill@@ ed them , do you think that they were wor@@ se sinn@@ ers than all other men who dw@@ el@@ t in Jerusalem ?\n",
            "For I be@@ ar him witness that he has a great z@@ eal for you , and those who are in La@@ o@@ dic@@ ea , and those in H@@ i@@ er@@ ap@@ ol@@ is .\n",
            "\n",
            "\n",
            "==> train.bpe.lg_lh <==\n",
            "N@@ ebutswa , om@@ undu y@@ esi ou@@ he@@ ela Nyasaye , omw@@ en@@ oyo Nyasaye , yam@@ um@@ anya .\n",
            "Eli@@ akh@@ ab@@ ili el@@ ili shinga el@@ o , n@@ di@@ el@@ ino mbu , ‘ O@@ he@@ el@@ e ow@@ as@@ hi@@ o shinga olwa wi@@ he@@ ela , omw@@ ene . ’\n",
            "okh@@ uula , kh@@ uny@@ anga eya y@@ abu@@ kul@@ il@@ wak@@ h@@ wo n@@ ay@@ il@@ wa mw@@ i@@ kulu . Ne , n@@ as@@ hil@@ i okhu@@ y@@ il@@ wa mw@@ i@@ kulu , y@@ ec@@ h@@ es@@ ia aba yali n@@ i@@ ya@@ ah@@ ula , okh@@ uba ab@@ ar@@ um@@ eb@@ e mub@@ un@@ yali bwa Ro@@ ho Omut@@ ak@@ ati@@ fu@@ . ,\n",
            "N@@ ebutswa ab@@ andi kh@@ ub@@ o sh@@ ib@@ ali , abas@@ uub@@ il@@ wa ta . K@@ ho k@@ ano k@@ akh@@ am@@ any@@ is@@ ia mbu , Nyasaye , shi@@ ali omus@@ uub@@ il@@ wa ta no@@ ho ?\n",
            "Ne olwa mw@@ inj@@ ila mun@@ z@@ u mub@@ as@@ hi@@ es@@ ie , omul@@ embe .\n",
            "Ab@@ andu ab@@ anj@@ i nib@@ aala , eb@@ if@@ wal@@ o bi@@ abwe kh@@ um@@ u@@ h@@ anda , ne ab@@ andi nib@@ ar@@ ema am@@ as@@ aka k@@ emis@@ aala nib@@ aala kh@@ um@@ u@@ h@@ anda okwo .\n",
            "K@@ h@@ wali n@@ ikh@@ uli , n@@ in@@ aye kh@@ us@@ hi@@ kulu esh@@ it@@ ak@@ ati@@ fu olwa kh@@ wa@@ h@@ ul@@ ila omwoyo , n@@ i@@ ku@@ r@@ ula mw@@ i@@ kulu e@@ wa Nyasaye .\n",
            "No@@ ho , mu@@ p@@ a@@ ar@@ anga mbu , abandu ek@@ hum@@ i nam@@ un@@ an@@ e b@@ omun@@ a@@ ar@@ a , k@@ wak@@ w@@ ila nib@@ af@@ wa bo@@ osi mul@@ uk@@ ongo lwa S@@ il@@ o@@ amu , bali , abon@@ ooni okh@@ us@@ h@@ ila abandu bo@@ osi abam@@ eny@@ anga mu , Yerus@@ al@@ emu ?\n",
            "E@@ si@@ e omw@@ ene end@@ i omut@@ er@@ er@@ eri kh@@ ub@@ uk@@ hal@@ ab@@ an@@ ibwe , obut@@ iny@@ u kh@@ ul@@ weny@@ we nende kh@@ ulwa abandu abali mu , La@@ od@@ ik@@ ia nende kh@@ ulwa abo abali H@@ i@@ er@@ ap@@ oli .\n",
            "\n",
            "\n",
            "==> train.en <==\n",
            "But if anyone loves God , this one is known by Him .\n",
            "And the second is like it : ‘ You shall love your neighbor as yourself . ’\n",
            "until the day in which He was taken up , after He through the Holy Spirit had given commandments to the apostles whom He had chosen ,\n",
            "For what if some did not believe ? Will their unbelief make the faithfulness of God without effect ?\n",
            "And when you go into a household , greet it .\n",
            "And a very great multitude spread their clothes on the road ; others cut down branches from the trees and spread them on the road .\n",
            "And we heard this voice which came from heaven when we were with Him on the holy mountain .\n",
            "Or those eighteen on whom the tower in Siloam fell and killed them , do you think that they were worse sinners than all other men who dwelt in Jerusalem ?\n",
            "For I bear him witness that he has a great zeal for you , and those who are in Laodicea , and those in Hierapolis .\n",
            "\n",
            "\n",
            "==> train.lg_lh <==\n",
            "Nebutswa , omundu yesi ouheela Nyasaye , omwenoyo Nyasaye , yamumanya .\n",
            "Eliakhabili elili shinga elo , ndielino mbu , ‘ Oheele owashio shinga olwa wiheela , omwene . ’\n",
            "okhuula , khunyanga eya yabukulilwakhwo nayilwa mwikulu . Ne , nashili okhuyilwa mwikulu , yechesia aba yali niyaahula , okhuba abarumebe mubunyali bwa Roho Omutakatifu. ,\n",
            "Nebutswa abandi khubo shibali , abasuubilwa ta . Kho kano kakhamanyisia mbu , Nyasaye , shiali omusuubilwa ta noho ?\n",
            "Ne olwa mwinjila munzu mubashiesie , omulembe .\n",
            "Abandu abanji nibaala , ebifwalo biabwe khumuhanda , ne abandi nibarema amasaka kemisaala nibaala khumuhanda okwo .\n",
            "Khwali nikhuli , ninaye khushikulu eshitakatifu olwa khwahulila omwoyo , nikurula mwikulu ewa Nyasaye .\n",
            "Noho , mupaaranga mbu , abandu ekhumi namunane bomunaara , kwakwila nibafwa boosi mulukongo lwa Siloamu , bali , abonooni okhushila abandu boosi abamenyanga mu , Yerusalemu ?\n",
            "Esie omwene endi omuterereri khubukhalabanibwe , obutinyu khulwenywe nende khulwa abandu abali mu , Laodikia nende khulwa abo abali Hierapoli .\n",
            "\n",
            "==> dev.bpe.en <==\n",
            "I do not say this to con@@ dem@@ n ; for I have said before that you are in our hearts , to di@@ e together and to live together .\n",
            "So when they were fill@@ ed , He said to His disciples , “ G@@ ather up the fr@@ ag@@ ments that remain , so that nothing is lost . ”\n",
            "When the D@@ ay of P@@ ent@@ ec@@ ost had fully come , they were all with one acc@@ ord in one place .\n",
            "For it has been declar@@ ed to me concer@@ ning you , my bre@@ th@@ ren , by those of Ch@@ lo@@ e ’ s hou@@ se@@ hold , that there are cont@@ en@@ tions among you .\n",
            "For “ who has known the mind of the Lord that he may instruc@@ t Him ? ” But we have the mind of Christ .\n",
            "And do not become id@@ ol@@ at@@ ers as were some of them . As it is written , “ The people s@@ at down to eat and dr@@ ink , and ro@@ se up to pl@@ ay . ”\n",
            "Now f@@ ive of them were wise , and f@@ ive were fo@@ ol@@ ish .\n",
            "When He op@@ ened the second se@@ al , I heard the second living cre@@ ature saying , “ C@@ ome and see . ”\n",
            "But let n@@ one of you suff@@ er as a m@@ ur@@ der@@ er , a th@@ ie@@ f , an ev@@ il@@ do@@ er , or as a bus@@ y@@ body in other people ’ s matters .\n",
            "\n",
            "\n",
            "==> dev.bpe.lg_lh <==\n",
            "S@@ hi@@ emb@@ ool@@ anga k@@ ano , kh@@ ulwa okh@@ um@@ uk@@ hal@@ ach@@ ila eshi@@ ina ta , okh@@ uba , shinga , nd@@ am@@ ub@@ oolela kh@@ ale , muli aba@@ he@@ el@@ wa mun@@ o kh@@ w@@ if@@ we , ne , kh@@ u@@ be@@ t@@ s@@ anga hal@@ ala buli lw@@ osi , k@@ ata n@@ ikh@@ uba abal@@ amu no@@ ho , n@@ ik@@ hu@@ f@@ wa .\n",
            "Ne olwa bo@@ osi bali nib@@ e@@ ku@@ r@@ e yab@@ oolela abee@@ ch@@ ib@@ e ari , “ Muk@@ h@@ ung@@ '@@ as@@ ie ebit@@ on@@ ye bit@@ ony@@ ile , bi@@ osi , kh@@ o mbu kh@@ ul@@ es@@ he okh@@ us@@ as@@ i@@ akh@@ wo esh@@ ind@@ u shi@@ osi shi@@ osi tawe . ”\n",
            "Ne olwa iny@@ anga ya P@@ end@@ ek@@ o@@ te y@@ ola , abas@@ uub@@ ili bo@@ osi , bak@@ h@@ ung@@ '@@ ana hab@@ undu hal@@ ala .\n",
            "O@@ kh@@ uba abaana be@@ f@@ we abandu b@@ andi ab@@ omun@@ z@@ u eya K@@ ul@@ o@@ e b@@ amb@@ ool@@ el@@ e but@@ swa , hab@@ ul@@ afu mbu , obus@@ ool@@ o buli h@@ ak@@ ari mw@@ iny@@ we@@ . ,\n",
            "Sh@@ inga A@@ ma@@ h@@ andik@@ o k@@ ab@@ ool@@ anga mbu “ N@@ i@@ w@@ iina ou@@ m@@ any@@ ile am@@ ap@@ a@@ ar@@ o aka Omwami ? , N@@ i@@ w@@ iina o@@ uny@@ ala okh@@ um@@ uc@@ hel@@ ela ? ” , N@@ ebutswa if@@ we kh@@ uli nam@@ ay@@ il@@ il@@ is@@ io aka Kristo .\n",
            "no@@ ho k@@ ata , okh@@ w@@ in@@ am@@ ila eb@@ if@@ wan@@ ani , shinga balala kh@@ ub@@ o bak@@ h@@ ola , tawe . Sh@@ inga olwa A@@ ma@@ h@@ andik@@ o k@@ ab@@ ool@@ anga mbu , “ Ab@@ andu , b@@ ek@@ h@@ ala h@@ asi okh@@ ul@@ ia lis@@ abo el@@ i@@ amala lik@@ al@@ ukh@@ an@@ e okh@@ uba , eshi@@ f@@ w@@ ab@@ w@@ i eshi@@ obum@@ e@@ esi nende obu@@ y@@ il@@ ani . ”\n",
            "B@@ ar@@ ano kh@@ ub@@ o , bali abay@@ ing@@ wa , ne b@@ ar@@ ano b@@ andi bali ab@@ ach@@ esi .\n",
            "M@@ ana E@@ sh@@ im@@ eme shi@@ el@@ ik@@ ond@@ i n@@ is@@ hi@@ i@@ kula esh@@ ib@@ al@@ ik@@ ho , shi@@ akh@@ ab@@ ili ne n@@ im@@ bul@@ ila es@@ hil@@ on@@ j@@ e eshi@@ akh@@ ab@@ ili es@@ hil@@ im@@ woyo n@@ ish@@ ib@@ oola sh@@ iri , “ Y@@ it@@ sa ! ”\n",
            "N@@ ebutswa om@@ undu y@@ esi kh@@ w@@ iny@@ we al@@ any@@ as@@ ibwa shinga , omu@@ y@@ iri , no@@ ho omw@@ if@@ i , no@@ ho omuk@@ hol@@ i w@@ am@@ akh@@ uwa am@@ abi , no@@ ho om@@ undu we@@ ind@@ ob@@ oyo tawe .\n",
            "\n",
            "\n",
            "==> dev.en <==\n",
            "I do not say this to condemn ; for I have said before that you are in our hearts , to die together and to live together .\n",
            "So when they were filled , He said to His disciples , “ Gather up the fragments that remain , so that nothing is lost . ”\n",
            "When the Day of Pentecost had fully come , they were all with one accord in one place .\n",
            "For it has been declared to me concerning you , my brethren , by those of Chloe ’ s household , that there are contentions among you .\n",
            "For “ who has known the mind of the Lord that he may instruct Him ? ” But we have the mind of Christ .\n",
            "And do not become idolaters as were some of them . As it is written , “ The people sat down to eat and drink , and rose up to play . ”\n",
            "Now five of them were wise , and five were foolish .\n",
            "When He opened the second seal , I heard the second living creature saying , “ Come and see . ”\n",
            "But let none of you suffer as a murderer , a thief , an evildoer , or as a busybody in other people ’ s matters .\n",
            "\n",
            "\n",
            "==> dev.lg_lh <==\n",
            "Shiemboolanga kano , khulwa okhumukhalachila eshiina ta , okhuba , shinga , ndamuboolela khale , muli abaheelwa muno khwifwe , ne , khubetsanga halala buli lwosi , kata nikhuba abalamu noho , nikhufwa .\n",
            "Ne olwa boosi bali nibekure yaboolela abeechibe ari , “ Mukhung'asie ebitonye bitonyile , biosi , kho mbu khuleshe okhusasiakhwo eshindu shiosi shiosi tawe . ”\n",
            "Ne olwa inyanga ya Pendekote yola , abasuubili boosi , bakhung'ana habundu halala .\n",
            "Okhuba abaana befwe abandu bandi abomunzu eya Kuloe bamboolele butswa , habulafu mbu , obusoolo buli hakari mwinywe. ,\n",
            "Shinga Amahandiko kaboolanga mbu “ Niwiina oumanyile amapaaro aka Omwami ? , Niwiina ounyala okhumuchelela ? ” , Nebutswa ifwe khuli namayililisio aka Kristo .\n",
            "noho kata , okhwinamila ebifwanani , shinga balala khubo bakhola , tawe . Shinga olwa Amahandiko kaboolanga mbu , “ Abandu , bekhala hasi okhulia lisabo eliamala likalukhane okhuba , eshifwabwi eshiobumeesi nende obuyilani . ”\n",
            "Barano khubo , bali abayingwa , ne barano bandi bali abachesi .\n",
            "Mana Eshimeme shielikondi nishiikula eshibalikho , shiakhabili ne nimbulila eshilonje eshiakhabili eshilimwoyo nishiboola shiri , “ Yitsa ! ”\n",
            "Nebutswa omundu yesi khwinywe alanyasibwa shinga , omuyiri , noho omwifi , noho omukholi wamakhuwa amabi , noho omundu weindoboyo tawe .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abtMrZzIK3QL"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBArdOdXK6Bd"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Smh87evI9G",
        "cellView": "code"
      },
      "source": [
        "#@title\n",
        "name = '%s%s' % (target_language, source_language)\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{target_language}{source_language}_reverse_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{target_language}\"\n",
        "    trg: \"{source_language}\"\n",
        "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\"\n",
        "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\"\n",
        "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
        "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 1000\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 200\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_reverse_transformer\"\n",
        "    overwrite: True \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2\", source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej5aVkIwwHu7",
        "outputId": "d8ec56c5-93b2-45ca-844c-973080885b64"
      },
      "source": [
        "# Train the model\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt$src.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-13 10:38:45,835 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-13 10:38:45,865 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-13 10:38:50,769 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-13 10:38:51,077 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-13 10:38:51,123 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-13 10:38:51,144 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-13 10:38:51,145 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-13 10:38:51,394 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-13 10:38:51.575863: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-13 10:38:53,789 - INFO - joeynmt.training - Total params: 12152320\n",
            "2021-07-13 10:38:55,963 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
            "2021-07-13 10:38:55,964 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
            "2021-07-13 10:38:55,964 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-13 10:38:55,964 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
            "2021-07-13 10:38:55,964 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
            "2021-07-13 10:38:55,965 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
            "2021-07-13 10:38:55,965 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-13 10:38:55,965 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-13 10:38:55,965 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-13 10:38:55,966 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
            "2021-07-13 10:38:55,966 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
            "2021-07-13 10:38:55,966 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-13 10:38:55,966 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-13 10:38:55,967 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-13 10:38:55,967 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-13 10:38:55,967 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-13 10:38:55,967 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-13 10:38:55,968 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-13 10:38:55,968 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-13 10:38:55,968 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-13 10:38:55,968 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-13 10:38:55,969 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-13 10:38:55,969 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-13 10:38:55,969 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-13 10:38:55,969 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-13 10:38:55,970 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-13 10:38:55,970 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-13 10:38:55,970 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-13 10:38:55,970 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-13 10:38:55,970 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-13 10:38:55,971 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-13 10:38:55,971 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-13 10:38:55,971 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-13 10:38:55,971 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-13 10:38:55,972 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
            "2021-07-13 10:38:55,972 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-13 10:38:55,972 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-13 10:38:55,972 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer\n",
            "2021-07-13 10:38:55,973 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-13 10:38:55,973 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-13 10:38:55,973 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-13 10:38:55,973 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-13 10:38:55,974 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-13 10:38:55,974 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-13 10:38:55,974 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-13 10:38:55,974 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-13 10:38:55,975 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-13 10:38:55,975 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-13 10:38:55,975 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-13 10:38:55,976 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-13 10:38:55,976 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-13 10:38:55,976 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-13 10:38:55,976 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-13 10:38:55,977 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-13 10:38:55,977 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 10:38:55,977 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-13 10:38:55,977 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-13 10:38:55,978 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-13 10:38:55,978 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-13 10:38:55,978 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-13 10:38:55,978 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-13 10:38:55,978 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-13 10:38:55,979 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-13 10:38:55,979 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 10:38:55,979 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-13 10:38:55,979 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-13 10:38:55,980 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-13 10:38:55,980 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-13 10:38:55,980 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-13 10:38:55,980 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 231302,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-13 10:38:55,981 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
            "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
            "2021-07-13 10:38:55,981 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 10:38:55,981 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 10:38:55,982 - INFO - joeynmt.helpers - Number of Src words (types): 4266\n",
            "2021-07-13 10:38:55,982 - INFO - joeynmt.helpers - Number of Trg words (types): 4266\n",
            "2021-07-13 10:38:55,982 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4266),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4266))\n",
            "2021-07-13 10:38:55,995 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-13 10:38:55,995 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-13 10:39:55,215 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.482846, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-13 10:40:53,188 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.040339, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-13 10:41:51,301 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.558581, Tokens per Sec:     7297, Lr: 0.000300\n",
            "2021-07-13 10:42:49,614 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.316972, Tokens per Sec:     7489, Lr: 0.000300\n",
            "2021-07-13 10:43:47,779 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.438428, Tokens per Sec:     7321, Lr: 0.000300\n",
            "2021-07-13 10:44:46,010 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.095161, Tokens per Sec:     7363, Lr: 0.000300\n",
            "2021-07-13 10:45:44,595 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.318918, Tokens per Sec:     7476, Lr: 0.000300\n",
            "2021-07-13 10:46:42,949 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.019433, Tokens per Sec:     7399, Lr: 0.000300\n",
            "2021-07-13 10:47:40,937 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.815217, Tokens per Sec:     7333, Lr: 0.000300\n",
            "2021-07-13 10:48:39,205 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.795290, Tokens per Sec:     7371, Lr: 0.000300\n",
            "2021-07-13 10:49:36,644 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.655495, Tokens per Sec:     7275, Lr: 0.000300\n",
            "2021-07-13 10:50:34,593 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.758233, Tokens per Sec:     7372, Lr: 0.000300\n",
            "2021-07-13 10:53:08,656 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 10:53:08,656 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 10:53:08,657 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 10:53:09,441 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 10:53:09,442 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 10:53:10,304 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 10:53:10,305 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 10:53:10,305 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 10:53:10,306 - INFO - joeynmt.training - \tHypothesis: * He was not a man who have been a man who are a man who are not to be a good news . ”\n",
            "2021-07-13 10:53:10,306 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 10:53:10,306 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 10:53:10,307 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 10:53:10,307 - INFO - joeynmt.training - \tHypothesis: When Jesus was a man , he was a man who was a man who was a man who had been said : “ I am I am not not not know that I am I am . ”\n",
            "2021-07-13 10:53:10,307 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 10:53:10,308 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 10:53:10,308 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 10:53:10,309 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah give his disciples ?\n",
            "2021-07-13 10:53:10,309 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 10:53:10,310 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 10:53:10,310 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 10:53:10,310 - INFO - joeynmt.training - \tHypothesis: But he was a few - time time time to be a few - time time .\n",
            "2021-07-13 10:53:10,311 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     2500: bleu:   2.57, loss: 228267.9844, ppl:  41.1629, duration: 126.9375s\n",
            "2021-07-13 10:53:39,705 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.773490, Tokens per Sec:     7409, Lr: 0.000300\n",
            "2021-07-13 10:54:37,486 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.636611, Tokens per Sec:     7318, Lr: 0.000300\n",
            "2021-07-13 10:55:01,235 - INFO - joeynmt.training - Epoch   1: total training loss 12311.97\n",
            "2021-07-13 10:55:01,236 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-13 10:55:35,960 - INFO - joeynmt.training - Epoch   2, Step:     3000, Batch Loss:     3.477988, Tokens per Sec:     7245, Lr: 0.000300\n",
            "2021-07-13 10:56:34,257 - INFO - joeynmt.training - Epoch   2, Step:     3200, Batch Loss:     3.333899, Tokens per Sec:     7391, Lr: 0.000300\n",
            "2021-07-13 10:57:32,333 - INFO - joeynmt.training - Epoch   2, Step:     3400, Batch Loss:     3.324048, Tokens per Sec:     7327, Lr: 0.000300\n",
            "2021-07-13 10:58:30,370 - INFO - joeynmt.training - Epoch   2, Step:     3600, Batch Loss:     3.337537, Tokens per Sec:     7317, Lr: 0.000300\n",
            "2021-07-13 10:59:28,047 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     3.475656, Tokens per Sec:     7349, Lr: 0.000300\n",
            "2021-07-13 11:00:26,055 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     3.412441, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-13 11:01:24,344 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     2.821386, Tokens per Sec:     7464, Lr: 0.000300\n",
            "2021-07-13 11:02:22,407 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     3.178905, Tokens per Sec:     7488, Lr: 0.000300\n",
            "2021-07-13 11:03:20,502 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     2.962477, Tokens per Sec:     7326, Lr: 0.000300\n",
            "2021-07-13 11:04:18,462 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     3.229018, Tokens per Sec:     7359, Lr: 0.000300\n",
            "2021-07-13 11:05:16,934 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     3.299410, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-13 11:07:11,772 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:07:11,773 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:07:11,773 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:07:12,541 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:07:12,541 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:07:13,409 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:07:13,410 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 11:07:13,410 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 11:07:13,410 - INFO - joeynmt.training - \tHypothesis: [ He ] will be a good and his heart , and he will be a loving way of God . ”\n",
            "2021-07-13 11:07:13,411 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:07:13,411 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 11:07:13,411 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 11:07:13,412 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he was baptized in heaven , he said : “ I am my Father , I am my Father . ”\n",
            "2021-07-13 11:07:13,412 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:07:13,413 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 11:07:13,413 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 11:07:13,413 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah give him to him ?\n",
            "2021-07-13 11:07:13,413 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:07:13,414 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 11:07:13,414 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 11:07:13,414 - INFO - joeynmt.training - \tHypothesis: But the Saul was a family , he was a family .\n",
            "2021-07-13 11:07:13,415 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     5000: bleu:   5.09, loss: 199037.6562, ppl:  25.5720, duration: 116.4799s\n",
            "2021-07-13 11:08:11,256 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.268315, Tokens per Sec:     7364, Lr: 0.000300\n",
            "2021-07-13 11:09:09,508 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     2.780973, Tokens per Sec:     7343, Lr: 0.000300\n",
            "2021-07-13 11:10:07,712 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.531078, Tokens per Sec:     7420, Lr: 0.000300\n",
            "2021-07-13 11:10:56,364 - INFO - joeynmt.training - Epoch   2: total training loss 9408.46\n",
            "2021-07-13 11:10:56,365 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-13 11:11:06,067 - INFO - joeynmt.training - Epoch   3, Step:     5800, Batch Loss:     3.202988, Tokens per Sec:     7231, Lr: 0.000300\n",
            "2021-07-13 11:12:04,009 - INFO - joeynmt.training - Epoch   3, Step:     6000, Batch Loss:     2.914789, Tokens per Sec:     7374, Lr: 0.000300\n",
            "2021-07-13 11:13:01,840 - INFO - joeynmt.training - Epoch   3, Step:     6200, Batch Loss:     3.190946, Tokens per Sec:     7408, Lr: 0.000300\n",
            "2021-07-13 11:13:59,987 - INFO - joeynmt.training - Epoch   3, Step:     6400, Batch Loss:     2.862369, Tokens per Sec:     7429, Lr: 0.000300\n",
            "2021-07-13 11:14:57,799 - INFO - joeynmt.training - Epoch   3, Step:     6600, Batch Loss:     2.890129, Tokens per Sec:     7286, Lr: 0.000300\n",
            "2021-07-13 11:15:56,692 - INFO - joeynmt.training - Epoch   3, Step:     6800, Batch Loss:     2.779277, Tokens per Sec:     7447, Lr: 0.000300\n",
            "2021-07-13 11:16:54,913 - INFO - joeynmt.training - Epoch   3, Step:     7000, Batch Loss:     3.187752, Tokens per Sec:     7383, Lr: 0.000300\n",
            "2021-07-13 11:17:52,660 - INFO - joeynmt.training - Epoch   3, Step:     7200, Batch Loss:     2.708927, Tokens per Sec:     7272, Lr: 0.000300\n",
            "2021-07-13 11:18:50,776 - INFO - joeynmt.training - Epoch   3, Step:     7400, Batch Loss:     3.032754, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-13 11:21:24,518 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:21:24,518 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:21:24,519 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:21:25,288 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:21:25,289 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:21:26,158 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:21:26,159 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 11:21:26,160 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 11:21:26,160 - INFO - joeynmt.training - \tHypothesis: [ Picture ] and the good and the flesh and the strength of the right , and he has made his purpose . ”\n",
            "2021-07-13 11:21:26,160 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:21:26,161 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 11:21:26,161 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 11:21:26,161 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he said to his brother , “ You have heard the Father of my Father . ”\n",
            "2021-07-13 11:21:26,161 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:21:26,162 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 11:21:26,162 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 11:21:26,162 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayer ?\n",
            "2021-07-13 11:21:26,163 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:21:26,164 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 11:21:26,164 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 11:21:26,164 - INFO - joeynmt.training - \tHypothesis: But the Philistines was a suggestions of his family .\n",
            "2021-07-13 11:21:26,164 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     7500: bleu:   6.37, loss: 185500.9531, ppl:  20.5126, duration: 126.6981s\n",
            "2021-07-13 11:21:55,555 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     2.702511, Tokens per Sec:     7369, Lr: 0.000300\n",
            "2021-07-13 11:22:53,205 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     3.011460, Tokens per Sec:     7318, Lr: 0.000300\n",
            "2021-07-13 11:23:51,316 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     2.815609, Tokens per Sec:     7406, Lr: 0.000300\n",
            "2021-07-13 11:24:49,732 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     3.038997, Tokens per Sec:     7491, Lr: 0.000300\n",
            "2021-07-13 11:25:47,103 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     2.944175, Tokens per Sec:     7193, Lr: 0.000300\n",
            "2021-07-13 11:26:45,423 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     3.188998, Tokens per Sec:     7414, Lr: 0.000300\n",
            "2021-07-13 11:27:02,149 - INFO - joeynmt.training - Epoch   3: total training loss 8457.30\n",
            "2021-07-13 11:27:02,149 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-13 11:27:43,535 - INFO - joeynmt.training - Epoch   4, Step:     8800, Batch Loss:     2.843766, Tokens per Sec:     7282, Lr: 0.000300\n",
            "2021-07-13 11:28:41,864 - INFO - joeynmt.training - Epoch   4, Step:     9000, Batch Loss:     2.727510, Tokens per Sec:     7473, Lr: 0.000300\n",
            "2021-07-13 11:29:39,179 - INFO - joeynmt.training - Epoch   4, Step:     9200, Batch Loss:     2.663144, Tokens per Sec:     7293, Lr: 0.000300\n",
            "2021-07-13 11:30:37,520 - INFO - joeynmt.training - Epoch   4, Step:     9400, Batch Loss:     2.624633, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-13 11:31:35,511 - INFO - joeynmt.training - Epoch   4, Step:     9600, Batch Loss:     2.421069, Tokens per Sec:     7392, Lr: 0.000300\n",
            "2021-07-13 11:32:33,415 - INFO - joeynmt.training - Epoch   4, Step:     9800, Batch Loss:     2.796311, Tokens per Sec:     7377, Lr: 0.000300\n",
            "2021-07-13 11:33:31,211 - INFO - joeynmt.training - Epoch   4, Step:    10000, Batch Loss:     2.929321, Tokens per Sec:     7328, Lr: 0.000300\n",
            "2021-07-13 11:35:35,118 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:35:35,118 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:35:35,118 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:35:35,850 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:35:35,850 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:35:36,755 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:35:36,756 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 11:35:36,756 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 11:35:36,756 - INFO - joeynmt.training - \tHypothesis: [ Sam ] and the fruit of the flesh and the strength of the patience , that is God’s will . ”\n",
            "2021-07-13 11:35:36,757 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:35:36,757 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 11:35:36,758 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 11:35:36,758 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt listening to heaven , saying : “ You are my son . ”\n",
            "2021-07-13 11:35:36,758 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:35:36,759 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 11:35:36,759 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 11:35:36,759 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ?\n",
            "2021-07-13 11:35:36,759 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:35:36,760 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 11:35:36,760 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 11:35:36,760 - INFO - joeynmt.training - \tHypothesis: But the Gospel was not to make a family .\n",
            "2021-07-13 11:35:36,761 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    10000: bleu:   8.15, loss: 176041.4688, ppl:  17.5839, duration: 125.5487s\n",
            "2021-07-13 11:36:35,167 - INFO - joeynmt.training - Epoch   4, Step:    10200, Batch Loss:     2.593507, Tokens per Sec:     7376, Lr: 0.000300\n",
            "2021-07-13 11:37:33,961 - INFO - joeynmt.training - Epoch   4, Step:    10400, Batch Loss:     2.801595, Tokens per Sec:     7491, Lr: 0.000300\n",
            "2021-07-13 11:38:32,210 - INFO - joeynmt.training - Epoch   4, Step:    10600, Batch Loss:     3.015158, Tokens per Sec:     7493, Lr: 0.000300\n",
            "2021-07-13 11:39:30,369 - INFO - joeynmt.training - Epoch   4, Step:    10800, Batch Loss:     3.022585, Tokens per Sec:     7428, Lr: 0.000300\n",
            "2021-07-13 11:40:28,713 - INFO - joeynmt.training - Epoch   4, Step:    11000, Batch Loss:     2.745151, Tokens per Sec:     7480, Lr: 0.000300\n",
            "2021-07-13 11:41:26,268 - INFO - joeynmt.training - Epoch   4, Step:    11200, Batch Loss:     2.607893, Tokens per Sec:     7279, Lr: 0.000300\n",
            "2021-07-13 11:42:24,060 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     2.744026, Tokens per Sec:     7458, Lr: 0.000300\n",
            "2021-07-13 11:43:03,408 - INFO - joeynmt.training - Epoch   4: total training loss 7855.02\n",
            "2021-07-13 11:43:03,409 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-13 11:43:22,221 - INFO - joeynmt.training - Epoch   5, Step:    11600, Batch Loss:     2.830557, Tokens per Sec:     7222, Lr: 0.000300\n",
            "2021-07-13 11:44:20,040 - INFO - joeynmt.training - Epoch   5, Step:    11800, Batch Loss:     2.358741, Tokens per Sec:     7321, Lr: 0.000300\n",
            "2021-07-13 11:45:17,960 - INFO - joeynmt.training - Epoch   5, Step:    12000, Batch Loss:     2.363652, Tokens per Sec:     7351, Lr: 0.000300\n",
            "2021-07-13 11:46:15,812 - INFO - joeynmt.training - Epoch   5, Step:    12200, Batch Loss:     2.892723, Tokens per Sec:     7398, Lr: 0.000300\n",
            "2021-07-13 11:47:13,847 - INFO - joeynmt.training - Epoch   5, Step:    12400, Batch Loss:     2.798207, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-13 11:49:59,720 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:49:59,720 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:49:59,720 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:50:00,505 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:50:00,506 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:50:01,371 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:50:01,372 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 11:50:01,372 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 11:50:01,372 - INFO - joeynmt.training - \tHypothesis: [ Just as he did not give up his eyes and will be saved to be patient , he is God’s approval . ”\n",
            "2021-07-13 11:50:01,373 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:50:01,373 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 11:50:01,373 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 11:50:01,374 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt about heaven from heaven , saying : “ You are my son . ”\n",
            "2021-07-13 11:50:01,374 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:50:01,375 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 11:50:01,375 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 11:50:01,375 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-13 11:50:01,375 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:50:01,376 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 11:50:01,376 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 11:50:01,377 - INFO - joeynmt.training - \tHypothesis: But the Gospel had to make his family .\n",
            "2021-07-13 11:50:01,377 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    12500: bleu:   8.56, loss: 170617.8438, ppl:  16.0974, duration: 138.4996s\n",
            "2021-07-13 11:50:30,922 - INFO - joeynmt.training - Epoch   5, Step:    12600, Batch Loss:     2.504455, Tokens per Sec:     7351, Lr: 0.000300\n",
            "2021-07-13 11:51:29,351 - INFO - joeynmt.training - Epoch   5, Step:    12800, Batch Loss:     2.643275, Tokens per Sec:     7460, Lr: 0.000300\n",
            "2021-07-13 11:52:27,788 - INFO - joeynmt.training - Epoch   5, Step:    13000, Batch Loss:     2.571552, Tokens per Sec:     7462, Lr: 0.000300\n",
            "2021-07-13 11:53:25,584 - INFO - joeynmt.training - Epoch   5, Step:    13200, Batch Loss:     2.619803, Tokens per Sec:     7363, Lr: 0.000300\n",
            "2021-07-13 11:54:23,389 - INFO - joeynmt.training - Epoch   5, Step:    13400, Batch Loss:     2.627335, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-13 11:55:21,238 - INFO - joeynmt.training - Epoch   5, Step:    13600, Batch Loss:     2.569509, Tokens per Sec:     7313, Lr: 0.000300\n",
            "2021-07-13 11:56:19,792 - INFO - joeynmt.training - Epoch   5, Step:    13800, Batch Loss:     2.148726, Tokens per Sec:     7470, Lr: 0.000300\n",
            "2021-07-13 11:57:17,882 - INFO - joeynmt.training - Epoch   5, Step:    14000, Batch Loss:     2.681810, Tokens per Sec:     7330, Lr: 0.000300\n",
            "2021-07-13 11:58:15,938 - INFO - joeynmt.training - Epoch   5, Step:    14200, Batch Loss:     2.476826, Tokens per Sec:     7429, Lr: 0.000300\n",
            "2021-07-13 11:59:13,910 - INFO - joeynmt.training - Epoch   5, Step:    14400, Batch Loss:     2.461026, Tokens per Sec:     7416, Lr: 0.000300\n",
            "2021-07-13 11:59:19,232 - INFO - joeynmt.training - Epoch   5: total training loss 7475.34\n",
            "2021-07-13 11:59:19,233 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-13 12:00:12,114 - INFO - joeynmt.training - Epoch   6, Step:    14600, Batch Loss:     2.431282, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-13 12:01:09,715 - INFO - joeynmt.training - Epoch   6, Step:    14800, Batch Loss:     2.860508, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-13 12:02:07,551 - INFO - joeynmt.training - Epoch   6, Step:    15000, Batch Loss:     2.583388, Tokens per Sec:     7330, Lr: 0.000300\n",
            "2021-07-13 12:04:12,469 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:04:12,470 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:04:12,470 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:04:13,248 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:04:13,248 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:04:14,494 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:04:14,495 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:04:14,496 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:04:14,496 - INFO - joeynmt.training - \tHypothesis: [ Picture ] and his own works , and he will be saved , that is the approval of God . ”\n",
            "2021-07-13 12:04:14,497 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:04:14,497 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:04:14,498 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:04:14,498 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt that he heard from heaven , saying : “ You have a son of my beloved father . ”\n",
            "2021-07-13 12:04:14,498 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:04:14,499 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:04:14,499 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:04:14,500 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer that Jesus did not forget ?\n",
            "2021-07-13 12:04:14,500 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:04:14,501 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:04:14,501 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:04:14,501 - INFO - joeynmt.training - \tHypothesis: But Abiga’s experience was a challenge to make his family .\n",
            "2021-07-13 12:04:14,501 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    15000: bleu:   9.22, loss: 165838.3125, ppl:  14.8919, duration: 126.9492s\n",
            "2021-07-13 12:05:13,186 - INFO - joeynmt.training - Epoch   6, Step:    15200, Batch Loss:     2.691442, Tokens per Sec:     7492, Lr: 0.000300\n",
            "2021-07-13 12:06:11,391 - INFO - joeynmt.training - Epoch   6, Step:    15400, Batch Loss:     1.866497, Tokens per Sec:     7348, Lr: 0.000300\n",
            "2021-07-13 12:07:09,451 - INFO - joeynmt.training - Epoch   6, Step:    15600, Batch Loss:     2.488444, Tokens per Sec:     7467, Lr: 0.000300\n",
            "2021-07-13 12:08:07,254 - INFO - joeynmt.training - Epoch   6, Step:    15800, Batch Loss:     2.540210, Tokens per Sec:     7315, Lr: 0.000300\n",
            "2021-07-13 12:09:06,386 - INFO - joeynmt.training - Epoch   6, Step:    16000, Batch Loss:     2.644926, Tokens per Sec:     7210, Lr: 0.000300\n",
            "2021-07-13 12:10:06,580 - INFO - joeynmt.training - Epoch   6, Step:    16200, Batch Loss:     2.140376, Tokens per Sec:     7140, Lr: 0.000300\n",
            "2021-07-13 12:11:05,789 - INFO - joeynmt.training - Epoch   6, Step:    16400, Batch Loss:     2.554359, Tokens per Sec:     7219, Lr: 0.000300\n",
            "2021-07-13 12:12:03,589 - INFO - joeynmt.training - Epoch   6, Step:    16600, Batch Loss:     2.550662, Tokens per Sec:     7344, Lr: 0.000300\n",
            "2021-07-13 12:13:01,914 - INFO - joeynmt.training - Epoch   6, Step:    16800, Batch Loss:     2.627572, Tokens per Sec:     7318, Lr: 0.000300\n",
            "2021-07-13 12:13:59,885 - INFO - joeynmt.training - Epoch   6, Step:    17000, Batch Loss:     2.547328, Tokens per Sec:     7422, Lr: 0.000300\n",
            "2021-07-13 12:15:00,369 - INFO - joeynmt.training - Epoch   6, Step:    17200, Batch Loss:     2.504341, Tokens per Sec:     7168, Lr: 0.000300\n",
            "2021-07-13 12:15:31,089 - INFO - joeynmt.training - Epoch   6: total training loss 7191.53\n",
            "2021-07-13 12:15:31,089 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-13 12:15:58,472 - INFO - joeynmt.training - Epoch   7, Step:    17400, Batch Loss:     2.378485, Tokens per Sec:     7251, Lr: 0.000300\n",
            "2021-07-13 12:18:23,129 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:18:23,130 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:18:23,130 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:18:23,867 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:18:23,868 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:18:25,095 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:18:25,096 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:18:25,097 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:18:25,097 - INFO - joeynmt.training - \tHypothesis: [ N ] He will do good and suffer when he suffered his endurance , that is the right thing to God . ”\n",
            "2021-07-13 12:18:25,097 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:18:25,098 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:18:25,098 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:18:25,098 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt that he was baptized from heaven , saying : “ You are my Son , my beloved Son . ”\n",
            "2021-07-13 12:18:25,099 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:18:25,099 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:18:25,099 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:18:25,100 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ?\n",
            "2021-07-13 12:18:25,100 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:18:25,100 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:18:25,101 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:18:25,101 - INFO - joeynmt.training - \tHypothesis: But Abigail had to stop his family .\n",
            "2021-07-13 12:18:25,101 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    17500: bleu:  10.79, loss: 160958.6250, ppl:  13.7542, duration: 117.6157s\n",
            "2021-07-13 12:18:54,097 - INFO - joeynmt.training - Epoch   7, Step:    17600, Batch Loss:     2.771304, Tokens per Sec:     7316, Lr: 0.000300\n",
            "2021-07-13 12:19:52,468 - INFO - joeynmt.training - Epoch   7, Step:    17800, Batch Loss:     2.619062, Tokens per Sec:     7219, Lr: 0.000300\n",
            "2021-07-13 12:20:50,185 - INFO - joeynmt.training - Epoch   7, Step:    18000, Batch Loss:     2.688880, Tokens per Sec:     7311, Lr: 0.000300\n",
            "2021-07-13 12:21:47,603 - INFO - joeynmt.training - Epoch   7, Step:    18200, Batch Loss:     2.624473, Tokens per Sec:     7337, Lr: 0.000300\n",
            "2021-07-13 12:22:46,649 - INFO - joeynmt.training - Epoch   7, Step:    18400, Batch Loss:     2.436187, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-13 12:23:44,639 - INFO - joeynmt.training - Epoch   7, Step:    18600, Batch Loss:     2.297176, Tokens per Sec:     7359, Lr: 0.000300\n",
            "2021-07-13 12:24:43,194 - INFO - joeynmt.training - Epoch   7, Step:    18800, Batch Loss:     2.446168, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-13 12:25:42,001 - INFO - joeynmt.training - Epoch   7, Step:    19000, Batch Loss:     2.368961, Tokens per Sec:     7373, Lr: 0.000300\n",
            "2021-07-13 12:26:40,120 - INFO - joeynmt.training - Epoch   7, Step:    19200, Batch Loss:     2.366360, Tokens per Sec:     7370, Lr: 0.000300\n",
            "2021-07-13 12:27:38,103 - INFO - joeynmt.training - Epoch   7, Step:    19400, Batch Loss:     1.637915, Tokens per Sec:     7425, Lr: 0.000300\n",
            "2021-07-13 12:28:36,425 - INFO - joeynmt.training - Epoch   7, Step:    19600, Batch Loss:     2.339921, Tokens per Sec:     7333, Lr: 0.000300\n",
            "2021-07-13 12:29:34,994 - INFO - joeynmt.training - Epoch   7, Step:    19800, Batch Loss:     2.520588, Tokens per Sec:     7402, Lr: 0.000300\n",
            "2021-07-13 12:30:33,597 - INFO - joeynmt.training - Epoch   7, Step:    20000, Batch Loss:     2.190453, Tokens per Sec:     7465, Lr: 0.000300\n",
            "2021-07-13 12:32:37,446 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:32:37,446 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:32:37,447 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:32:38,174 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:32:38,174 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:32:39,450 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:32:39,452 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:32:39,452 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:32:39,452 - INFO - joeynmt.training - \tHypothesis: “ When he has done good and suffered the endurance of endurance , he is the approval of God . ”\n",
            "2021-07-13 12:32:39,453 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:32:39,453 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:32:39,453 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:32:39,454 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt that he heard from heaven , saying : “ This is my Son of my beloved Son . ”\n",
            "2021-07-13 12:32:39,454 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:32:39,455 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:32:39,455 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:32:39,455 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus did ?\n",
            "2021-07-13 12:32:39,455 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:32:39,456 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:32:39,456 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:32:39,456 - INFO - joeynmt.training - \tHypothesis: But Abigail had to remove his family .\n",
            "2021-07-13 12:32:39,457 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    20000: bleu:  11.50, loss: 157274.6875, ppl:  12.9533, duration: 125.8589s\n",
            "2021-07-13 12:33:35,334 - INFO - joeynmt.training - Epoch   7: total training loss 6965.64\n",
            "2021-07-13 12:33:35,334 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-13 12:33:38,000 - INFO - joeynmt.training - Epoch   8, Step:    20200, Batch Loss:     2.430514, Tokens per Sec:     6255, Lr: 0.000300\n",
            "2021-07-13 12:34:36,019 - INFO - joeynmt.training - Epoch   8, Step:    20400, Batch Loss:     2.633343, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-13 12:35:33,859 - INFO - joeynmt.training - Epoch   8, Step:    20600, Batch Loss:     2.372613, Tokens per Sec:     7277, Lr: 0.000300\n",
            "2021-07-13 12:36:32,516 - INFO - joeynmt.training - Epoch   8, Step:    20800, Batch Loss:     2.072747, Tokens per Sec:     7321, Lr: 0.000300\n",
            "2021-07-13 12:37:30,493 - INFO - joeynmt.training - Epoch   8, Step:    21000, Batch Loss:     2.790609, Tokens per Sec:     7332, Lr: 0.000300\n",
            "2021-07-13 12:38:29,214 - INFO - joeynmt.training - Epoch   8, Step:    21200, Batch Loss:     2.282213, Tokens per Sec:     7390, Lr: 0.000300\n",
            "2021-07-13 12:39:27,458 - INFO - joeynmt.training - Epoch   8, Step:    21400, Batch Loss:     1.626438, Tokens per Sec:     7468, Lr: 0.000300\n",
            "2021-07-13 12:40:25,598 - INFO - joeynmt.training - Epoch   8, Step:    21600, Batch Loss:     2.369209, Tokens per Sec:     7227, Lr: 0.000300\n",
            "2021-07-13 12:41:23,623 - INFO - joeynmt.training - Epoch   8, Step:    21800, Batch Loss:     2.113739, Tokens per Sec:     7467, Lr: 0.000300\n",
            "2021-07-13 12:42:21,481 - INFO - joeynmt.training - Epoch   8, Step:    22000, Batch Loss:     2.437642, Tokens per Sec:     7434, Lr: 0.000300\n",
            "2021-07-13 12:43:19,195 - INFO - joeynmt.training - Epoch   8, Step:    22200, Batch Loss:     2.227558, Tokens per Sec:     7358, Lr: 0.000300\n",
            "2021-07-13 12:44:17,417 - INFO - joeynmt.training - Epoch   8, Step:    22400, Batch Loss:     2.587390, Tokens per Sec:     7310, Lr: 0.000300\n",
            "2021-07-13 12:46:34,523 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:46:34,523 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:46:34,524 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:46:35,229 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:46:35,230 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:46:36,075 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:46:36,077 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 12:46:36,077 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 12:46:36,077 - INFO - joeynmt.training - \tHypothesis: “ When he went on to work with the good and suffering he suffered , he is the best way to be patient , what is God’s approval . ”\n",
            "2021-07-13 12:46:36,077 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:46:36,078 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 12:46:36,078 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 12:46:36,078 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt that he heard from heaven , saying : “ This is my Son of my beloved Son . ”\n",
            "2021-07-13 12:46:36,079 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:46:36,080 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 12:46:36,080 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 12:46:36,080 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-13 12:46:36,080 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:46:36,081 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 12:46:36,081 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 12:46:36,082 - INFO - joeynmt.training - \tHypothesis: But Abigail had to remove his family .\n",
            "2021-07-13 12:46:36,082 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    22500: bleu:  12.13, loss: 154986.7031, ppl:  12.4795, duration: 109.7942s\n",
            "2021-07-13 12:47:05,309 - INFO - joeynmt.training - Epoch   8, Step:    22600, Batch Loss:     2.416340, Tokens per Sec:     7346, Lr: 0.000300\n",
            "2021-07-13 12:48:03,771 - INFO - joeynmt.training - Epoch   8, Step:    22800, Batch Loss:     2.121096, Tokens per Sec:     7523, Lr: 0.000300\n",
            "2021-07-13 12:49:02,109 - INFO - joeynmt.training - Epoch   8, Step:    23000, Batch Loss:     2.266371, Tokens per Sec:     7229, Lr: 0.000300\n",
            "2021-07-13 12:49:25,162 - INFO - joeynmt.training - Epoch   8: total training loss 6781.92\n",
            "2021-07-13 12:49:25,163 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-13 12:50:01,285 - INFO - joeynmt.training - Epoch   9, Step:    23200, Batch Loss:     2.296813, Tokens per Sec:     7028, Lr: 0.000300\n",
            "2021-07-13 12:50:59,513 - INFO - joeynmt.training - Epoch   9, Step:    23400, Batch Loss:     2.120721, Tokens per Sec:     7450, Lr: 0.000300\n",
            "2021-07-13 12:51:57,685 - INFO - joeynmt.training - Epoch   9, Step:    23600, Batch Loss:     2.666935, Tokens per Sec:     7170, Lr: 0.000300\n",
            "2021-07-13 12:52:55,955 - INFO - joeynmt.training - Epoch   9, Step:    23800, Batch Loss:     2.480580, Tokens per Sec:     7244, Lr: 0.000300\n",
            "2021-07-13 12:53:54,228 - INFO - joeynmt.training - Epoch   9, Step:    24000, Batch Loss:     2.279624, Tokens per Sec:     7399, Lr: 0.000300\n",
            "2021-07-13 12:54:51,575 - INFO - joeynmt.training - Epoch   9, Step:    24200, Batch Loss:     2.231338, Tokens per Sec:     7277, Lr: 0.000300\n",
            "2021-07-13 12:55:49,385 - INFO - joeynmt.training - Epoch   9, Step:    24400, Batch Loss:     2.377175, Tokens per Sec:     7364, Lr: 0.000300\n",
            "2021-07-13 12:56:47,503 - INFO - joeynmt.training - Epoch   9, Step:    24600, Batch Loss:     2.363206, Tokens per Sec:     7526, Lr: 0.000300\n",
            "2021-07-13 12:57:45,394 - INFO - joeynmt.training - Epoch   9, Step:    24800, Batch Loss:     2.290188, Tokens per Sec:     7446, Lr: 0.000300\n",
            "2021-07-13 12:58:44,326 - INFO - joeynmt.training - Epoch   9, Step:    25000, Batch Loss:     2.437701, Tokens per Sec:     7255, Lr: 0.000300\n",
            "2021-07-13 13:00:51,703 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:00:51,704 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:00:51,704 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:00:53,315 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:00:53,316 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 13:00:53,316 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 13:00:53,316 - INFO - joeynmt.training - \tHypothesis: I have made good decisions and suffered when you endure , that is the approval of God . ”\n",
            "2021-07-13 13:00:53,316 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:00:53,317 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 13:00:53,317 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 13:00:53,317 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt a voice from heaven , saying : “ This is my beloved Son . ”\n",
            "2021-07-13 13:00:53,318 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:00:53,318 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 13:00:53,319 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 13:00:53,319 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to what Jesus prayed ?\n",
            "2021-07-13 13:00:53,319 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:00:53,320 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 13:00:53,320 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 13:00:53,321 - INFO - joeynmt.training - \tHypothesis: But Abigail had to save his family .\n",
            "2021-07-13 13:00:53,321 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    25000: bleu:  11.13, loss: 155826.4531, ppl:  12.6514, duration: 128.9948s\n",
            "2021-07-13 13:01:50,948 - INFO - joeynmt.training - Epoch   9, Step:    25200, Batch Loss:     2.075818, Tokens per Sec:     7306, Lr: 0.000300\n",
            "2021-07-13 13:02:49,154 - INFO - joeynmt.training - Epoch   9, Step:    25400, Batch Loss:     2.607106, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-13 13:03:46,995 - INFO - joeynmt.training - Epoch   9, Step:    25600, Batch Loss:     2.126274, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-13 13:04:45,268 - INFO - joeynmt.training - Epoch   9, Step:    25800, Batch Loss:     1.979178, Tokens per Sec:     7487, Lr: 0.000300\n",
            "2021-07-13 13:05:35,180 - INFO - joeynmt.training - Epoch   9: total training loss 6644.53\n",
            "2021-07-13 13:05:35,181 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-13 13:05:43,547 - INFO - joeynmt.training - Epoch  10, Step:    26000, Batch Loss:     2.385340, Tokens per Sec:     7031, Lr: 0.000300\n",
            "2021-07-13 13:06:41,737 - INFO - joeynmt.training - Epoch  10, Step:    26200, Batch Loss:     2.228610, Tokens per Sec:     7396, Lr: 0.000300\n",
            "2021-07-13 13:07:39,617 - INFO - joeynmt.training - Epoch  10, Step:    26400, Batch Loss:     2.159047, Tokens per Sec:     7404, Lr: 0.000300\n",
            "2021-07-13 13:08:38,287 - INFO - joeynmt.training - Epoch  10, Step:    26600, Batch Loss:     2.053046, Tokens per Sec:     7408, Lr: 0.000300\n",
            "2021-07-13 13:09:35,853 - INFO - joeynmt.training - Epoch  10, Step:    26800, Batch Loss:     1.372318, Tokens per Sec:     7420, Lr: 0.000300\n",
            "2021-07-13 13:10:34,303 - INFO - joeynmt.training - Epoch  10, Step:    27000, Batch Loss:     2.110693, Tokens per Sec:     7395, Lr: 0.000300\n",
            "2021-07-13 13:11:32,831 - INFO - joeynmt.training - Epoch  10, Step:    27200, Batch Loss:     2.244521, Tokens per Sec:     7320, Lr: 0.000300\n",
            "2021-07-13 13:12:30,803 - INFO - joeynmt.training - Epoch  10, Step:    27400, Batch Loss:     2.419393, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-13 13:15:02,606 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:15:02,606 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:15:02,606 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:15:03,374 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:15:03,374 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:15:04,310 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:15:04,310 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 13:15:04,311 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 13:15:04,311 - INFO - joeynmt.training - \tHypothesis: I have shown the good and suffer when you endure , that is approved to God . ”\n",
            "2021-07-13 13:15:04,311 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:15:04,312 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 13:15:04,312 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 13:15:04,312 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , my beloved Son . ”\n",
            "2021-07-13 13:15:04,313 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:15:04,313 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 13:15:04,313 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 13:15:04,314 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ approval ?\n",
            "2021-07-13 13:15:04,314 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:15:04,315 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 13:15:04,315 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 13:15:04,315 - INFO - joeynmt.training - \tHypothesis: But Abigail had to save his family .\n",
            "2021-07-13 13:15:04,315 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    27500: bleu:  12.49, loss: 150788.4062, ppl:  11.6548, duration: 124.0182s\n",
            "2021-07-13 13:15:33,970 - INFO - joeynmt.training - Epoch  10, Step:    27600, Batch Loss:     2.094541, Tokens per Sec:     7206, Lr: 0.000300\n",
            "2021-07-13 13:16:31,756 - INFO - joeynmt.training - Epoch  10, Step:    27800, Batch Loss:     2.435700, Tokens per Sec:     7380, Lr: 0.000300\n",
            "2021-07-13 13:17:30,181 - INFO - joeynmt.training - Epoch  10, Step:    28000, Batch Loss:     2.189076, Tokens per Sec:     7378, Lr: 0.000300\n",
            "2021-07-13 13:18:28,366 - INFO - joeynmt.training - Epoch  10, Step:    28200, Batch Loss:     2.037922, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-13 13:19:26,316 - INFO - joeynmt.training - Epoch  10, Step:    28400, Batch Loss:     2.075635, Tokens per Sec:     7393, Lr: 0.000300\n",
            "2021-07-13 13:20:24,431 - INFO - joeynmt.training - Epoch  10, Step:    28600, Batch Loss:     2.243607, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-13 13:21:22,169 - INFO - joeynmt.training - Epoch  10, Step:    28800, Batch Loss:     1.983697, Tokens per Sec:     7294, Lr: 0.000300\n",
            "2021-07-13 13:21:38,255 - INFO - joeynmt.training - Epoch  10: total training loss 6481.21\n",
            "2021-07-13 13:21:38,256 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-13 13:22:21,295 - INFO - joeynmt.training - Epoch  11, Step:    29000, Batch Loss:     2.085943, Tokens per Sec:     7325, Lr: 0.000300\n",
            "2021-07-13 13:23:19,349 - INFO - joeynmt.training - Epoch  11, Step:    29200, Batch Loss:     2.050257, Tokens per Sec:     7272, Lr: 0.000300\n",
            "2021-07-13 13:24:17,335 - INFO - joeynmt.training - Epoch  11, Step:    29400, Batch Loss:     2.517100, Tokens per Sec:     7431, Lr: 0.000300\n",
            "2021-07-13 13:25:14,958 - INFO - joeynmt.training - Epoch  11, Step:    29600, Batch Loss:     2.168416, Tokens per Sec:     7235, Lr: 0.000300\n",
            "2021-07-13 13:26:13,604 - INFO - joeynmt.training - Epoch  11, Step:    29800, Batch Loss:     2.953834, Tokens per Sec:     7421, Lr: 0.000300\n",
            "2021-07-13 13:27:12,246 - INFO - joeynmt.training - Epoch  11, Step:    30000, Batch Loss:     1.986822, Tokens per Sec:     7459, Lr: 0.000300\n",
            "2021-07-13 13:29:25,575 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:29:25,576 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:29:25,576 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:29:26,360 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:29:26,360 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:29:27,192 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:29:27,193 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 13:29:27,193 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 13:29:27,193 - INFO - joeynmt.training - \tHypothesis: And when he was good and suffered when you endure patience , what is good for God . ”\n",
            "2021-07-13 13:29:27,193 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:29:27,194 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 13:29:27,194 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 13:29:27,194 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt a voice from heaven , saying : “ This is my beloved Son , whom I am my beloved Son . ”\n",
            "2021-07-13 13:29:27,195 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:29:27,195 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 13:29:27,196 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 13:29:27,196 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 13:29:27,196 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:29:27,197 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 13:29:27,197 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 13:29:27,197 - INFO - joeynmt.training - \tHypothesis: But Abigail did not have to save his family .\n",
            "2021-07-13 13:29:27,198 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    30000: bleu:  12.29, loss: 148554.7500, ppl:  11.2384, duration: 134.9514s\n",
            "2021-07-13 13:30:25,533 - INFO - joeynmt.training - Epoch  11, Step:    30200, Batch Loss:     2.332680, Tokens per Sec:     7358, Lr: 0.000300\n",
            "2021-07-13 13:31:23,600 - INFO - joeynmt.training - Epoch  11, Step:    30400, Batch Loss:     2.177344, Tokens per Sec:     7431, Lr: 0.000300\n",
            "2021-07-13 13:32:21,949 - INFO - joeynmt.training - Epoch  11, Step:    30600, Batch Loss:     2.507879, Tokens per Sec:     7449, Lr: 0.000300\n",
            "2021-07-13 13:33:20,258 - INFO - joeynmt.training - Epoch  11, Step:    30800, Batch Loss:     1.879647, Tokens per Sec:     7245, Lr: 0.000300\n",
            "2021-07-13 13:34:18,255 - INFO - joeynmt.training - Epoch  11, Step:    31000, Batch Loss:     2.242538, Tokens per Sec:     7301, Lr: 0.000300\n",
            "2021-07-13 13:35:16,424 - INFO - joeynmt.training - Epoch  11, Step:    31200, Batch Loss:     2.414242, Tokens per Sec:     7489, Lr: 0.000300\n",
            "2021-07-13 13:36:15,494 - INFO - joeynmt.training - Epoch  11, Step:    31400, Batch Loss:     2.289873, Tokens per Sec:     7340, Lr: 0.000300\n",
            "2021-07-13 13:37:13,301 - INFO - joeynmt.training - Epoch  11, Step:    31600, Batch Loss:     2.224284, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-13 13:37:52,604 - INFO - joeynmt.training - Epoch  11: total training loss 6369.16\n",
            "2021-07-13 13:37:52,604 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-13 13:38:11,474 - INFO - joeynmt.training - Epoch  12, Step:    31800, Batch Loss:     2.195853, Tokens per Sec:     7223, Lr: 0.000300\n",
            "2021-07-13 13:39:09,394 - INFO - joeynmt.training - Epoch  12, Step:    32000, Batch Loss:     1.979524, Tokens per Sec:     7445, Lr: 0.000300\n",
            "2021-07-13 13:40:07,060 - INFO - joeynmt.training - Epoch  12, Step:    32200, Batch Loss:     2.520462, Tokens per Sec:     7350, Lr: 0.000300\n",
            "2021-07-13 13:41:05,405 - INFO - joeynmt.training - Epoch  12, Step:    32400, Batch Loss:     2.611221, Tokens per Sec:     7305, Lr: 0.000300\n",
            "2021-07-13 13:43:40,812 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:43:40,813 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:43:40,813 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:43:41,597 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:43:41,598 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:43:42,423 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:43:42,424 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 13:43:42,424 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 13:43:42,425 - INFO - joeynmt.training - \tHypothesis: [ I ] hated him with good works and will be tormented when you endure , that is what is approved to God . ”\n",
            "2021-07-13 13:43:42,425 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:43:42,425 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 13:43:42,426 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 13:43:42,426 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he felt a voice from heaven , saying : “ This is my beloved Son , whom I am my beloved Son . ”\n",
            "2021-07-13 13:43:42,426 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:43:42,427 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 13:43:42,427 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 13:43:42,427 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus forgotten ?\n",
            "2021-07-13 13:43:42,427 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:43:42,428 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 13:43:42,428 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 13:43:42,429 - INFO - joeynmt.training - \tHypothesis: But Abigail did not have to save his family .\n",
            "2021-07-13 13:43:42,429 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    32500: bleu:  12.92, loss: 146919.8281, ppl:  10.9432, duration: 127.6937s\n",
            "2021-07-13 13:44:11,465 - INFO - joeynmt.training - Epoch  12, Step:    32600, Batch Loss:     2.249601, Tokens per Sec:     7442, Lr: 0.000300\n",
            "2021-07-13 13:45:09,300 - INFO - joeynmt.training - Epoch  12, Step:    32800, Batch Loss:     2.370827, Tokens per Sec:     7312, Lr: 0.000300\n",
            "2021-07-13 13:46:07,196 - INFO - joeynmt.training - Epoch  12, Step:    33000, Batch Loss:     2.334886, Tokens per Sec:     7360, Lr: 0.000300\n",
            "2021-07-13 13:47:05,283 - INFO - joeynmt.training - Epoch  12, Step:    33200, Batch Loss:     1.417963, Tokens per Sec:     7388, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEV2zvqhBunO",
        "outputId": "5f423df6-049b-4caa-c0d1-3fa002bd59b7"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"joeynmt/models/lg_lhen_reverse_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 2500\tLoss: 228267.98438\tPPL: 41.16292\tbleu: 2.56686\tLR: 0.00030000\t*\n",
            "Steps: 5000\tLoss: 199037.65625\tPPL: 25.57197\tbleu: 5.08604\tLR: 0.00030000\t*\n",
            "Steps: 7500\tLoss: 185500.95312\tPPL: 20.51262\tbleu: 6.36613\tLR: 0.00030000\t*\n",
            "Steps: 10000\tLoss: 176041.46875\tPPL: 17.58392\tbleu: 8.14781\tLR: 0.00030000\t*\n",
            "Steps: 12500\tLoss: 170617.84375\tPPL: 16.09738\tbleu: 8.55822\tLR: 0.00030000\t*\n",
            "Steps: 15000\tLoss: 165838.31250\tPPL: 14.89190\tbleu: 9.21606\tLR: 0.00030000\t*\n",
            "Steps: 17500\tLoss: 160958.62500\tPPL: 13.75425\tbleu: 10.78737\tLR: 0.00030000\t*\n",
            "Steps: 20000\tLoss: 157274.68750\tPPL: 12.95332\tbleu: 11.50072\tLR: 0.00030000\t*\n",
            "Steps: 22500\tLoss: 154986.70312\tPPL: 12.47953\tbleu: 12.13082\tLR: 0.00030000\t*\n",
            "Steps: 25000\tLoss: 155826.45312\tPPL: 12.65138\tbleu: 11.13397\tLR: 0.00030000\t\n",
            "Steps: 27500\tLoss: 150788.40625\tPPL: 11.65479\tbleu: 12.49335\tLR: 0.00030000\t*\n",
            "Steps: 30000\tLoss: 148554.75000\tPPL: 11.23844\tbleu: 12.29347\tLR: 0.00030000\t*\n",
            "Steps: 32500\tLoss: 146919.82812\tPPL: 10.94315\tbleu: 12.92199\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJWjURXjwSnG"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 32500\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/models/lg_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_lhen_reverse_transformer\"', f'model_dir: \"models/lg_lhen_reverse_transformer_continued\"').replace(\n",
        "        f'epochs: 30', f'epochs: 18')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY-9k-G7VMd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "25056953-9172-4f24-fc3f-571133e168ea"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_lg_lhen_reload.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"lg_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"lg_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer/32500.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 18                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/lg_lhen_reverse_transformer_continued\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG7hbW9vuvT2",
        "outputId": "405a85f5-534e-419d-85be-e0ef0f8ca25b"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_lhen_reload.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-13 14:24:21,277 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-13 14:24:21,349 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-13 14:24:26,459 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-13 14:24:27,122 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-13 14:24:27,925 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-13 14:24:29,027 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-13 14:24:29,027 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-13 14:24:29,224 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-13 14:24:29.469826: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-13 14:24:32,282 - INFO - joeynmt.training - Total params: 12152320\n",
            "2021-07-13 14:24:40,984 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer/32500.ckpt\n",
            "2021-07-13 14:24:41,420 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
            "2021-07-13 14:24:41,420 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
            "2021-07-13 14:24:41,420 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-13 14:24:41,420 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
            "2021-07-13 14:24:41,421 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
            "2021-07-13 14:24:41,421 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
            "2021-07-13 14:24:41,421 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-13 14:24:41,421 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-13 14:24:41,421 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-13 14:24:41,421 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
            "2021-07-13 14:24:41,421 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
            "2021-07-13 14:24:41,421 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-13 14:24:41,422 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-13 14:24:41,422 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer/32500.ckpt\n",
            "2021-07-13 14:24:41,422 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-13 14:24:41,422 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-13 14:24:41,422 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-13 14:24:41,422 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-13 14:24:41,422 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-13 14:24:41,423 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-13 14:24:41,423 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-13 14:24:41,423 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-13 14:24:41,423 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-13 14:24:41,423 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-13 14:24:41,423 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-13 14:24:41,423 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-13 14:24:41,423 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-13 14:24:41,424 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-13 14:24:41,424 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-13 14:24:41,424 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-13 14:24:41,424 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-13 14:24:41,424 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-13 14:24:41,424 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-13 14:24:41,424 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-13 14:24:41,424 - INFO - joeynmt.helpers - cfg.training.epochs                : 18\n",
            "2021-07-13 14:24:41,425 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
            "2021-07-13 14:24:41,425 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-13 14:24:41,425 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-13 14:24:41,425 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer_continued\n",
            "2021-07-13 14:24:41,425 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-13 14:24:41,425 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-13 14:24:41,425 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-13 14:24:41,426 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-13 14:24:41,426 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-13 14:24:41,426 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-13 14:24:41,426 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-13 14:24:41,426 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-13 14:24:41,426 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-13 14:24:41,427 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-13 14:24:41,427 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-13 14:24:41,427 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-13 14:24:41,427 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-13 14:24:41,427 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-13 14:24:41,427 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-13 14:24:41,428 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-13 14:24:41,428 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 14:24:41,428 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-13 14:24:41,428 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-13 14:24:41,428 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-13 14:24:41,428 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-13 14:24:41,428 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-13 14:24:41,428 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-13 14:24:41,429 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-13 14:24:41,429 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-13 14:24:41,429 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 14:24:41,429 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-13 14:24:41,429 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-13 14:24:41,429 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-13 14:24:41,429 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-13 14:24:41,429 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-13 14:24:41,430 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 231302,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-13 14:24:41,430 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
            "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
            "2021-07-13 14:24:41,430 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 14:24:41,430 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 14:24:41,430 - INFO - joeynmt.helpers - Number of Src words (types): 4266\n",
            "2021-07-13 14:24:41,430 - INFO - joeynmt.helpers - Number of Trg words (types): 4266\n",
            "2021-07-13 14:24:41,431 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4266),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4266))\n",
            "2021-07-13 14:24:41,458 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-13 14:24:41,458 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-13 14:24:54,769 - INFO - joeynmt.training - Epoch   1, Step:    32600, Batch Loss:     2.238633, Tokens per Sec:    16216, Lr: 0.000300\n",
            "2021-07-13 14:25:20,001 - INFO - joeynmt.training - Epoch   1, Step:    32800, Batch Loss:     2.362046, Tokens per Sec:    16761, Lr: 0.000300\n",
            "2021-07-13 14:25:45,639 - INFO - joeynmt.training - Epoch   1, Step:    33000, Batch Loss:     2.321169, Tokens per Sec:    16621, Lr: 0.000300\n",
            "2021-07-13 14:26:11,893 - INFO - joeynmt.training - Epoch   1, Step:    33200, Batch Loss:     1.408575, Tokens per Sec:    16345, Lr: 0.000300\n",
            "2021-07-13 14:26:38,956 - INFO - joeynmt.training - Epoch   1, Step:    33400, Batch Loss:     2.120732, Tokens per Sec:    16280, Lr: 0.000300\n",
            "2021-07-13 14:27:06,116 - INFO - joeynmt.training - Epoch   1, Step:    33600, Batch Loss:     2.048376, Tokens per Sec:    15751, Lr: 0.000300\n",
            "2021-07-13 14:27:33,008 - INFO - joeynmt.training - Epoch   1, Step:    33800, Batch Loss:     2.190071, Tokens per Sec:    16015, Lr: 0.000300\n",
            "2021-07-13 14:27:59,895 - INFO - joeynmt.training - Epoch   1, Step:    34000, Batch Loss:     2.197989, Tokens per Sec:    16085, Lr: 0.000300\n",
            "2021-07-13 14:28:27,056 - INFO - joeynmt.training - Epoch   1, Step:    34200, Batch Loss:     2.280892, Tokens per Sec:    15985, Lr: 0.000300\n",
            "2021-07-13 14:28:53,856 - INFO - joeynmt.training - Epoch   1, Step:    34400, Batch Loss:     2.259953, Tokens per Sec:    15808, Lr: 0.000300\n",
            "2021-07-13 14:29:20,498 - INFO - joeynmt.training - Epoch   1, Step:    34600, Batch Loss:     2.128389, Tokens per Sec:    15722, Lr: 0.000300\n",
            "2021-07-13 14:29:23,261 - INFO - joeynmt.training - Epoch   1: total training loss 4619.88\n",
            "2021-07-13 14:29:23,262 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-13 14:29:47,853 - INFO - joeynmt.training - Epoch   2, Step:    34800, Batch Loss:     1.826713, Tokens per Sec:    15811, Lr: 0.000300\n",
            "2021-07-13 14:30:14,843 - INFO - joeynmt.training - Epoch   2, Step:    35000, Batch Loss:     2.348040, Tokens per Sec:    15875, Lr: 0.000300\n",
            "2021-07-13 14:31:20,105 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:31:20,106 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:31:20,106 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:31:20,705 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:31:20,706 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:31:21,418 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:31:21,419 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:31:21,419 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:31:21,419 - INFO - joeynmt.training - \tHypothesis: I have made a good speech and suffer when you endure , what is good to God . ”\n",
            "2021-07-13 14:31:21,419 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:31:21,420 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:31:21,420 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:31:21,421 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am my beloved Son . ”\n",
            "2021-07-13 14:31:21,421 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:31:21,422 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:31:21,422 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:31:21,422 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus forget ?\n",
            "2021-07-13 14:31:21,422 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:31:21,423 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:31:21,423 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:31:21,423 - INFO - joeynmt.training - \tHypothesis: But Abigail had to save his family .\n",
            "2021-07-13 14:31:21,423 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    35000: bleu:  13.83, loss: 145461.1562, ppl:  10.6863, duration: 66.5798s\n",
            "2021-07-13 14:31:48,461 - INFO - joeynmt.training - Epoch   2, Step:    35200, Batch Loss:     2.225792, Tokens per Sec:    15746, Lr: 0.000300\n",
            "2021-07-13 14:32:15,475 - INFO - joeynmt.training - Epoch   2, Step:    35400, Batch Loss:     2.040188, Tokens per Sec:    15859, Lr: 0.000300\n",
            "2021-07-13 14:32:42,663 - INFO - joeynmt.training - Epoch   2, Step:    35600, Batch Loss:     2.486684, Tokens per Sec:    16038, Lr: 0.000300\n",
            "2021-07-13 14:33:09,683 - INFO - joeynmt.training - Epoch   2, Step:    35800, Batch Loss:     2.185693, Tokens per Sec:    15773, Lr: 0.000300\n",
            "2021-07-13 14:33:36,597 - INFO - joeynmt.training - Epoch   2, Step:    36000, Batch Loss:     2.073160, Tokens per Sec:    15955, Lr: 0.000300\n",
            "2021-07-13 14:34:03,805 - INFO - joeynmt.training - Epoch   2, Step:    36200, Batch Loss:     2.166534, Tokens per Sec:    16068, Lr: 0.000300\n",
            "2021-07-13 14:34:30,954 - INFO - joeynmt.training - Epoch   2, Step:    36400, Batch Loss:     2.259595, Tokens per Sec:    15889, Lr: 0.000300\n",
            "2021-07-13 14:34:57,942 - INFO - joeynmt.training - Epoch   2, Step:    36600, Batch Loss:     2.307683, Tokens per Sec:    15900, Lr: 0.000300\n",
            "2021-07-13 14:35:25,004 - INFO - joeynmt.training - Epoch   2, Step:    36800, Batch Loss:     2.360542, Tokens per Sec:    15873, Lr: 0.000300\n",
            "2021-07-13 14:35:51,847 - INFO - joeynmt.training - Epoch   2, Step:    37000, Batch Loss:     2.076383, Tokens per Sec:    15790, Lr: 0.000300\n",
            "2021-07-13 14:36:18,726 - INFO - joeynmt.training - Epoch   2, Step:    37200, Batch Loss:     2.170236, Tokens per Sec:    15922, Lr: 0.000300\n",
            "2021-07-13 14:36:45,779 - INFO - joeynmt.training - Epoch   2, Step:    37400, Batch Loss:     1.841984, Tokens per Sec:    15973, Lr: 0.000300\n",
            "2021-07-13 14:36:58,703 - INFO - joeynmt.training - Epoch   2: total training loss 6179.71\n",
            "2021-07-13 14:36:58,703 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-13 14:38:00,913 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:38:00,913 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:38:00,914 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:38:01,507 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:38:01,507 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:38:02,233 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:38:02,234 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:38:02,235 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:38:02,235 - INFO - joeynmt.training - \tHypothesis: When you are doing good and suffer when you endure , what is good to God . ”\n",
            "2021-07-13 14:38:02,235 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:38:02,235 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:38:02,236 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:38:02,236 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am . ”\n",
            "2021-07-13 14:38:02,236 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:38:02,236 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:38:02,237 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:38:02,237 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 14:38:02,237 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:38:02,237 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:38:02,238 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:38:02,238 - INFO - joeynmt.training - \tHypothesis: But Abigail did not have to save his family .\n",
            "2021-07-13 14:38:02,238 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    37500: bleu:  13.95, loss: 144813.4375, ppl:  10.5741, duration: 62.5170s\n",
            "2021-07-13 14:38:15,740 - INFO - joeynmt.training - Epoch   3, Step:    37600, Batch Loss:     2.176021, Tokens per Sec:    15315, Lr: 0.000300\n",
            "2021-07-13 14:38:42,718 - INFO - joeynmt.training - Epoch   3, Step:    37800, Batch Loss:     1.973176, Tokens per Sec:    15909, Lr: 0.000300\n",
            "2021-07-13 14:39:09,702 - INFO - joeynmt.training - Epoch   3, Step:    38000, Batch Loss:     2.181074, Tokens per Sec:    15773, Lr: 0.000300\n",
            "2021-07-13 14:39:36,490 - INFO - joeynmt.training - Epoch   3, Step:    38200, Batch Loss:     2.501165, Tokens per Sec:    15803, Lr: 0.000300\n",
            "2021-07-13 14:40:03,341 - INFO - joeynmt.training - Epoch   3, Step:    38400, Batch Loss:     1.978907, Tokens per Sec:    15881, Lr: 0.000300\n",
            "2021-07-13 14:40:30,370 - INFO - joeynmt.training - Epoch   3, Step:    38600, Batch Loss:     1.928595, Tokens per Sec:    15869, Lr: 0.000300\n",
            "2021-07-13 14:40:57,426 - INFO - joeynmt.training - Epoch   3, Step:    38800, Batch Loss:     2.166934, Tokens per Sec:    15726, Lr: 0.000300\n",
            "2021-07-13 14:41:24,792 - INFO - joeynmt.training - Epoch   3, Step:    39000, Batch Loss:     2.142913, Tokens per Sec:    16040, Lr: 0.000300\n",
            "2021-07-13 14:41:51,568 - INFO - joeynmt.training - Epoch   3, Step:    39200, Batch Loss:     2.157439, Tokens per Sec:    15831, Lr: 0.000300\n",
            "2021-07-13 14:42:18,271 - INFO - joeynmt.training - Epoch   3, Step:    39400, Batch Loss:     2.266984, Tokens per Sec:    15893, Lr: 0.000300\n",
            "2021-07-13 14:42:45,415 - INFO - joeynmt.training - Epoch   3, Step:    39600, Batch Loss:     2.240040, Tokens per Sec:    16031, Lr: 0.000300\n",
            "2021-07-13 14:43:12,248 - INFO - joeynmt.training - Epoch   3, Step:    39800, Batch Loss:     2.142384, Tokens per Sec:    16048, Lr: 0.000300\n",
            "2021-07-13 14:43:39,293 - INFO - joeynmt.training - Epoch   3, Step:    40000, Batch Loss:     2.269577, Tokens per Sec:    16110, Lr: 0.000300\n",
            "2021-07-13 14:44:45,385 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:44:45,386 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:44:45,386 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:44:45,980 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:44:45,980 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:44:46,730 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:44:46,731 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:44:46,731 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:44:46,731 - INFO - joeynmt.training - \tHypothesis: I have hated him with good works and suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-13 14:44:46,731 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:44:46,732 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:44:46,732 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:44:46,732 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he felt a voice from heaven : “ This is my beloved Son . ”\n",
            "2021-07-13 14:44:46,732 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:44:46,732 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:44:46,733 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:44:46,733 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 14:44:46,733 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:44:46,733 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:44:46,733 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:44:46,734 - INFO - joeynmt.training - \tHypothesis: But Abigail had to make his family health .\n",
            "2021-07-13 14:44:46,734 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    40000: bleu:  14.04, loss: 142993.6875, ppl:  10.2653, duration: 67.4406s\n",
            "2021-07-13 14:45:13,871 - INFO - joeynmt.training - Epoch   3, Step:    40200, Batch Loss:     2.115135, Tokens per Sec:    15592, Lr: 0.000300\n",
            "2021-07-13 14:45:38,002 - INFO - joeynmt.training - Epoch   3: total training loss 6129.12\n",
            "2021-07-13 14:45:38,002 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-13 14:45:40,891 - INFO - joeynmt.training - Epoch   4, Step:    40400, Batch Loss:     1.894094, Tokens per Sec:    14636, Lr: 0.000300\n",
            "2021-07-13 14:46:07,676 - INFO - joeynmt.training - Epoch   4, Step:    40600, Batch Loss:     2.144558, Tokens per Sec:    15889, Lr: 0.000300\n",
            "2021-07-13 14:46:34,308 - INFO - joeynmt.training - Epoch   4, Step:    40800, Batch Loss:     2.222411, Tokens per Sec:    15921, Lr: 0.000300\n",
            "2021-07-13 14:47:01,021 - INFO - joeynmt.training - Epoch   4, Step:    41000, Batch Loss:     2.181857, Tokens per Sec:    16117, Lr: 0.000300\n",
            "2021-07-13 14:47:27,893 - INFO - joeynmt.training - Epoch   4, Step:    41200, Batch Loss:     2.182415, Tokens per Sec:    15757, Lr: 0.000300\n",
            "2021-07-13 14:47:55,006 - INFO - joeynmt.training - Epoch   4, Step:    41400, Batch Loss:     1.906959, Tokens per Sec:    15822, Lr: 0.000300\n",
            "2021-07-13 14:48:21,931 - INFO - joeynmt.training - Epoch   4, Step:    41600, Batch Loss:     1.181870, Tokens per Sec:    15924, Lr: 0.000300\n",
            "2021-07-13 14:48:48,884 - INFO - joeynmt.training - Epoch   4, Step:    41800, Batch Loss:     1.912336, Tokens per Sec:    15942, Lr: 0.000300\n",
            "2021-07-13 14:49:15,916 - INFO - joeynmt.training - Epoch   4, Step:    42000, Batch Loss:     2.418119, Tokens per Sec:    16065, Lr: 0.000300\n",
            "2021-07-13 14:49:43,096 - INFO - joeynmt.training - Epoch   4, Step:    42200, Batch Loss:     1.817100, Tokens per Sec:    16063, Lr: 0.000300\n",
            "2021-07-13 14:50:09,746 - INFO - joeynmt.training - Epoch   4, Step:    42400, Batch Loss:     1.888585, Tokens per Sec:    15657, Lr: 0.000300\n",
            "2021-07-13 14:51:25,300 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:51:25,300 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:51:25,301 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:51:25,898 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:51:25,899 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:51:26,646 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:51:26,646 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:51:26,646 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:51:26,646 - INFO - joeynmt.training - \tHypothesis: I will be able to do good and to suffer when you endure , that is acceptable to God . ”\n",
            "2021-07-13 14:51:26,647 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:51:26,647 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:51:26,647 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:51:26,647 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am not grateful . ”\n",
            "2021-07-13 14:51:26,648 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:51:26,648 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:51:26,648 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:51:26,648 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ forgiveness ?\n",
            "2021-07-13 14:51:26,648 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:51:26,649 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:51:26,649 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:51:26,649 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 14:51:26,649 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    42500: bleu:  14.44, loss: 141620.4219, ppl:  10.0383, duration: 63.3378s\n",
            "2021-07-13 14:51:40,102 - INFO - joeynmt.training - Epoch   4, Step:    42600, Batch Loss:     1.657137, Tokens per Sec:    15841, Lr: 0.000300\n",
            "2021-07-13 14:52:07,149 - INFO - joeynmt.training - Epoch   4, Step:    42800, Batch Loss:     2.184105, Tokens per Sec:    15992, Lr: 0.000300\n",
            "2021-07-13 14:52:33,815 - INFO - joeynmt.training - Epoch   4, Step:    43000, Batch Loss:     1.796998, Tokens per Sec:    15975, Lr: 0.000300\n",
            "2021-07-13 14:53:00,647 - INFO - joeynmt.training - Epoch   4, Step:    43200, Batch Loss:     2.088075, Tokens per Sec:    15933, Lr: 0.000300\n",
            "2021-07-13 14:53:10,043 - INFO - joeynmt.training - Epoch   4: total training loss 6081.01\n",
            "2021-07-13 14:53:10,044 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-13 14:53:27,798 - INFO - joeynmt.training - Epoch   5, Step:    43400, Batch Loss:     2.166337, Tokens per Sec:    15640, Lr: 0.000300\n",
            "2021-07-13 14:53:54,689 - INFO - joeynmt.training - Epoch   5, Step:    43600, Batch Loss:     2.218195, Tokens per Sec:    16017, Lr: 0.000300\n",
            "2021-07-13 14:54:21,766 - INFO - joeynmt.training - Epoch   5, Step:    43800, Batch Loss:     1.961032, Tokens per Sec:    15854, Lr: 0.000300\n",
            "2021-07-13 14:54:48,817 - INFO - joeynmt.training - Epoch   5, Step:    44000, Batch Loss:     1.718747, Tokens per Sec:    15859, Lr: 0.000300\n",
            "2021-07-13 14:55:15,652 - INFO - joeynmt.training - Epoch   5, Step:    44200, Batch Loss:     2.021991, Tokens per Sec:    15932, Lr: 0.000300\n",
            "2021-07-13 14:55:42,669 - INFO - joeynmt.training - Epoch   5, Step:    44400, Batch Loss:     1.953772, Tokens per Sec:    16006, Lr: 0.000300\n",
            "2021-07-13 14:56:09,565 - INFO - joeynmt.training - Epoch   5, Step:    44600, Batch Loss:     2.259624, Tokens per Sec:    15850, Lr: 0.000300\n",
            "2021-07-13 14:56:36,505 - INFO - joeynmt.training - Epoch   5, Step:    44800, Batch Loss:     2.211805, Tokens per Sec:    16018, Lr: 0.000300\n",
            "2021-07-13 14:57:03,561 - INFO - joeynmt.training - Epoch   5, Step:    45000, Batch Loss:     2.129933, Tokens per Sec:    16133, Lr: 0.000300\n",
            "2021-07-13 14:58:03,689 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:58:03,689 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:58:03,689 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:58:04,281 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:58:04,281 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:58:04,964 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:58:04,965 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 14:58:04,965 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 14:58:04,965 - INFO - joeynmt.training - \tHypothesis: And when you are doing good and will suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-13 14:58:04,965 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:58:04,966 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 14:58:04,966 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 14:58:04,967 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard the voice from heaven : “ This is my beloved Son , whom I am not grateful . ”\n",
            "2021-07-13 14:58:04,967 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:58:04,967 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 14:58:04,968 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 14:58:04,968 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-13 14:58:04,968 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:58:04,968 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 14:58:04,969 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 14:58:04,969 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 14:58:04,969 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    45000: bleu:  14.59, loss: 141061.1875, ppl:   9.9473, duration: 61.4076s\n",
            "2021-07-13 14:58:31,980 - INFO - joeynmt.training - Epoch   5, Step:    45200, Batch Loss:     2.371204, Tokens per Sec:    15667, Lr: 0.000300\n",
            "2021-07-13 14:58:58,901 - INFO - joeynmt.training - Epoch   5, Step:    45400, Batch Loss:     1.506232, Tokens per Sec:    15882, Lr: 0.000300\n",
            "2021-07-13 14:59:25,706 - INFO - joeynmt.training - Epoch   5, Step:    45600, Batch Loss:     2.092286, Tokens per Sec:    15755, Lr: 0.000300\n",
            "2021-07-13 14:59:52,412 - INFO - joeynmt.training - Epoch   5, Step:    45800, Batch Loss:     1.959700, Tokens per Sec:    15818, Lr: 0.000300\n",
            "2021-07-13 15:00:19,049 - INFO - joeynmt.training - Epoch   5, Step:    46000, Batch Loss:     2.151821, Tokens per Sec:    15878, Lr: 0.000300\n",
            "2021-07-13 15:00:40,235 - INFO - joeynmt.training - Epoch   5: total training loss 6003.23\n",
            "2021-07-13 15:00:40,235 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-13 15:00:46,379 - INFO - joeynmt.training - Epoch   6, Step:    46200, Batch Loss:     1.912706, Tokens per Sec:    15336, Lr: 0.000300\n",
            "2021-07-13 15:01:13,374 - INFO - joeynmt.training - Epoch   6, Step:    46400, Batch Loss:     2.246910, Tokens per Sec:    16188, Lr: 0.000300\n",
            "2021-07-13 15:01:40,058 - INFO - joeynmt.training - Epoch   6, Step:    46600, Batch Loss:     1.969785, Tokens per Sec:    15877, Lr: 0.000300\n",
            "2021-07-13 15:02:06,963 - INFO - joeynmt.training - Epoch   6, Step:    46800, Batch Loss:     1.950958, Tokens per Sec:    15961, Lr: 0.000300\n",
            "2021-07-13 15:02:33,700 - INFO - joeynmt.training - Epoch   6, Step:    47000, Batch Loss:     2.162778, Tokens per Sec:    16025, Lr: 0.000300\n",
            "2021-07-13 15:03:00,542 - INFO - joeynmt.training - Epoch   6, Step:    47200, Batch Loss:     1.619108, Tokens per Sec:    15973, Lr: 0.000300\n",
            "2021-07-13 15:03:26,950 - INFO - joeynmt.training - Epoch   6, Step:    47400, Batch Loss:     1.807608, Tokens per Sec:    15602, Lr: 0.000300\n",
            "2021-07-13 15:04:45,025 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:04:45,025 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:04:45,025 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:04:45,633 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:04:45,633 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:04:46,683 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:04:46,684 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 15:04:46,684 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 15:04:46,684 - INFO - joeynmt.training - \tHypothesis: When you have done good and suffer when you endure , that is what is approved to God . ”\n",
            "2021-07-13 15:04:46,685 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:04:46,685 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 15:04:46,685 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 15:04:46,685 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am the beloved Son . ”\n",
            "2021-07-13 15:04:46,686 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:04:46,686 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 15:04:46,686 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 15:04:46,686 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ forgiveness ?\n",
            "2021-07-13 15:04:46,686 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:04:46,687 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 15:04:46,687 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 15:04:46,687 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 15:04:46,687 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    47500: bleu:  14.24, loss: 140957.2344, ppl:   9.9305, duration: 66.4172s\n",
            "2021-07-13 15:05:00,324 - INFO - joeynmt.training - Epoch   6, Step:    47600, Batch Loss:     2.028957, Tokens per Sec:    15971, Lr: 0.000300\n",
            "2021-07-13 15:05:27,042 - INFO - joeynmt.training - Epoch   6, Step:    47800, Batch Loss:     1.973670, Tokens per Sec:    16015, Lr: 0.000300\n",
            "2021-07-13 15:05:53,773 - INFO - joeynmt.training - Epoch   6, Step:    48000, Batch Loss:     2.213484, Tokens per Sec:    16116, Lr: 0.000300\n",
            "2021-07-13 15:06:20,296 - INFO - joeynmt.training - Epoch   6, Step:    48200, Batch Loss:     1.429922, Tokens per Sec:    16044, Lr: 0.000300\n",
            "2021-07-13 15:06:47,121 - INFO - joeynmt.training - Epoch   6, Step:    48400, Batch Loss:     2.158389, Tokens per Sec:    16151, Lr: 0.000300\n",
            "2021-07-13 15:07:13,978 - INFO - joeynmt.training - Epoch   6, Step:    48600, Batch Loss:     2.118782, Tokens per Sec:    16145, Lr: 0.000300\n",
            "2021-07-13 15:07:40,668 - INFO - joeynmt.training - Epoch   6, Step:    48800, Batch Loss:     2.058663, Tokens per Sec:    15849, Lr: 0.000300\n",
            "2021-07-13 15:08:07,302 - INFO - joeynmt.training - Epoch   6, Step:    49000, Batch Loss:     2.135326, Tokens per Sec:    15732, Lr: 0.000300\n",
            "2021-07-13 15:08:13,563 - INFO - joeynmt.training - Epoch   6: total training loss 5956.41\n",
            "2021-07-13 15:08:13,563 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-13 15:08:34,250 - INFO - joeynmt.training - Epoch   7, Step:    49200, Batch Loss:     2.044683, Tokens per Sec:    15532, Lr: 0.000300\n",
            "2021-07-13 15:09:00,928 - INFO - joeynmt.training - Epoch   7, Step:    49400, Batch Loss:     2.318341, Tokens per Sec:    15961, Lr: 0.000300\n",
            "2021-07-13 15:09:27,626 - INFO - joeynmt.training - Epoch   7, Step:    49600, Batch Loss:     1.983683, Tokens per Sec:    16167, Lr: 0.000300\n",
            "2021-07-13 15:09:54,267 - INFO - joeynmt.training - Epoch   7, Step:    49800, Batch Loss:     2.019709, Tokens per Sec:    16134, Lr: 0.000300\n",
            "2021-07-13 15:10:21,137 - INFO - joeynmt.training - Epoch   7, Step:    50000, Batch Loss:     1.878168, Tokens per Sec:    15985, Lr: 0.000300\n",
            "2021-07-13 15:11:26,739 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:11:26,739 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:11:26,740 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:11:27,346 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:11:27,346 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:11:28,031 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:11:28,032 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 15:11:28,033 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 15:11:28,033 - INFO - joeynmt.training - \tHypothesis: If you have a good friend and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-13 15:11:28,033 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:11:28,034 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 15:11:28,034 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 15:11:28,034 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard the voice from heaven : “ This is my Son , whom I am the beloved Son . ”\n",
            "2021-07-13 15:11:28,034 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:11:28,035 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 15:11:28,035 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 15:11:28,035 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-13 15:11:28,035 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:11:28,035 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 15:11:28,036 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 15:11:28,036 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 15:11:28,036 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    50000: bleu:  14.15, loss: 140635.3594, ppl:   9.8786, duration: 66.8986s\n",
            "2021-07-13 15:11:54,813 - INFO - joeynmt.training - Epoch   7, Step:    50200, Batch Loss:     2.149264, Tokens per Sec:    15940, Lr: 0.000300\n",
            "2021-07-13 15:12:21,802 - INFO - joeynmt.training - Epoch   7, Step:    50400, Batch Loss:     1.476766, Tokens per Sec:    16079, Lr: 0.000300\n",
            "2021-07-13 15:12:48,744 - INFO - joeynmt.training - Epoch   7, Step:    50600, Batch Loss:     1.991872, Tokens per Sec:    16155, Lr: 0.000300\n",
            "2021-07-13 15:13:15,458 - INFO - joeynmt.training - Epoch   7, Step:    50800, Batch Loss:     2.103597, Tokens per Sec:    15932, Lr: 0.000300\n",
            "2021-07-13 15:13:42,203 - INFO - joeynmt.training - Epoch   7, Step:    51000, Batch Loss:     1.703778, Tokens per Sec:    16021, Lr: 0.000300\n",
            "2021-07-13 15:14:08,957 - INFO - joeynmt.training - Epoch   7, Step:    51200, Batch Loss:     2.088673, Tokens per Sec:    15978, Lr: 0.000300\n",
            "2021-07-13 15:14:35,593 - INFO - joeynmt.training - Epoch   7, Step:    51400, Batch Loss:     2.162482, Tokens per Sec:    15824, Lr: 0.000300\n",
            "2021-07-13 15:15:02,183 - INFO - joeynmt.training - Epoch   7, Step:    51600, Batch Loss:     2.284104, Tokens per Sec:    16046, Lr: 0.000300\n",
            "2021-07-13 15:15:29,054 - INFO - joeynmt.training - Epoch   7, Step:    51800, Batch Loss:     2.154071, Tokens per Sec:    16142, Lr: 0.000300\n",
            "2021-07-13 15:15:46,374 - INFO - joeynmt.training - Epoch   7: total training loss 5896.79\n",
            "2021-07-13 15:15:46,375 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-13 15:15:56,184 - INFO - joeynmt.training - Epoch   8, Step:    52000, Batch Loss:     2.190580, Tokens per Sec:    15850, Lr: 0.000300\n",
            "2021-07-13 15:16:22,864 - INFO - joeynmt.training - Epoch   8, Step:    52200, Batch Loss:     1.630378, Tokens per Sec:    16102, Lr: 0.000300\n",
            "2021-07-13 15:16:49,558 - INFO - joeynmt.training - Epoch   8, Step:    52400, Batch Loss:     2.193340, Tokens per Sec:    15991, Lr: 0.000300\n",
            "2021-07-13 15:18:07,413 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:18:07,413 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:18:07,414 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:18:08,015 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:18:08,015 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:18:08,683 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:18:08,686 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 15:18:08,687 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 15:18:08,687 - INFO - joeynmt.training - \tHypothesis: If you are doing good and punishment , you will be tormented when you endure , what is acceptable to God . ”\n",
            "2021-07-13 15:18:08,687 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:18:08,688 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 15:18:08,689 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 15:18:08,689 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , whom I am the beloved . ”\n",
            "2021-07-13 15:18:08,689 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:18:08,690 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 15:18:08,690 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 15:18:08,690 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-13 15:18:08,691 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:18:08,695 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 15:18:08,695 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 15:18:08,695 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 15:18:08,695 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    52500: bleu:  15.13, loss: 138489.6875, ppl:   9.5393, duration: 65.9748s\n",
            "2021-07-13 15:18:22,256 - INFO - joeynmt.training - Epoch   8, Step:    52600, Batch Loss:     2.012659, Tokens per Sec:    15870, Lr: 0.000300\n",
            "2021-07-13 15:18:49,207 - INFO - joeynmt.training - Epoch   8, Step:    52800, Batch Loss:     1.830319, Tokens per Sec:    16207, Lr: 0.000300\n",
            "2021-07-13 15:19:15,910 - INFO - joeynmt.training - Epoch   8, Step:    53000, Batch Loss:     2.111525, Tokens per Sec:    16080, Lr: 0.000300\n",
            "2021-07-13 15:19:42,512 - INFO - joeynmt.training - Epoch   8, Step:    53200, Batch Loss:     2.065151, Tokens per Sec:    15818, Lr: 0.000300\n",
            "2021-07-13 15:20:09,311 - INFO - joeynmt.training - Epoch   8, Step:    53400, Batch Loss:     2.111068, Tokens per Sec:    16115, Lr: 0.000300\n",
            "2021-07-13 15:20:36,053 - INFO - joeynmt.training - Epoch   8, Step:    53600, Batch Loss:     2.025404, Tokens per Sec:    16021, Lr: 0.000300\n",
            "2021-07-13 15:21:02,767 - INFO - joeynmt.training - Epoch   8, Step:    53800, Batch Loss:     1.992142, Tokens per Sec:    16105, Lr: 0.000300\n",
            "2021-07-13 15:21:29,394 - INFO - joeynmt.training - Epoch   8, Step:    54000, Batch Loss:     1.982752, Tokens per Sec:    16055, Lr: 0.000300\n",
            "2021-07-13 15:21:56,142 - INFO - joeynmt.training - Epoch   8, Step:    54200, Batch Loss:     2.306502, Tokens per Sec:    15843, Lr: 0.000300\n",
            "2021-07-13 15:22:23,240 - INFO - joeynmt.training - Epoch   8, Step:    54400, Batch Loss:     1.973540, Tokens per Sec:    16127, Lr: 0.000300\n",
            "2021-07-13 15:22:50,023 - INFO - joeynmt.training - Epoch   8, Step:    54600, Batch Loss:     1.947513, Tokens per Sec:    16068, Lr: 0.000300\n",
            "2021-07-13 15:23:16,877 - INFO - joeynmt.training - Epoch   8, Step:    54800, Batch Loss:     1.972823, Tokens per Sec:    15889, Lr: 0.000300\n",
            "2021-07-13 15:23:18,252 - INFO - joeynmt.training - Epoch   8: total training loss 5846.50\n",
            "2021-07-13 15:23:18,253 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-13 15:23:43,789 - INFO - joeynmt.training - Epoch   9, Step:    55000, Batch Loss:     2.243845, Tokens per Sec:    15730, Lr: 0.000300\n",
            "2021-07-13 15:24:43,679 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:24:43,679 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:24:43,680 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:24:44,271 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:24:44,272 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:24:45,327 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:24:45,338 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 15:24:45,338 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 15:24:45,339 - INFO - joeynmt.training - \tHypothesis: When you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-13 15:24:45,339 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:24:45,339 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 15:24:45,339 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 15:24:45,340 - INFO - joeynmt.training - \tHypothesis: When Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 15:24:45,340 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:24:45,340 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 15:24:45,340 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 15:24:45,340 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-13 15:24:45,340 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:24:45,341 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 15:24:45,341 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 15:24:45,341 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 15:24:45,341 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    55000: bleu:  15.60, loss: 137841.7656, ppl:   9.4392, duration: 61.5515s\n",
            "2021-07-13 15:25:12,207 - INFO - joeynmt.training - Epoch   9, Step:    55200, Batch Loss:     2.077842, Tokens per Sec:    15909, Lr: 0.000300\n",
            "2021-07-13 15:25:39,197 - INFO - joeynmt.training - Epoch   9, Step:    55400, Batch Loss:     2.308788, Tokens per Sec:    15943, Lr: 0.000300\n",
            "2021-07-13 15:26:06,061 - INFO - joeynmt.training - Epoch   9, Step:    55600, Batch Loss:     1.982602, Tokens per Sec:    16114, Lr: 0.000300\n",
            "2021-07-13 15:26:33,076 - INFO - joeynmt.training - Epoch   9, Step:    55800, Batch Loss:     2.006014, Tokens per Sec:    16144, Lr: 0.000300\n",
            "2021-07-13 15:27:00,136 - INFO - joeynmt.training - Epoch   9, Step:    56000, Batch Loss:     1.864727, Tokens per Sec:    15860, Lr: 0.000300\n",
            "2021-07-13 15:27:26,815 - INFO - joeynmt.training - Epoch   9, Step:    56200, Batch Loss:     2.135244, Tokens per Sec:    15783, Lr: 0.000300\n",
            "2021-07-13 15:27:53,597 - INFO - joeynmt.training - Epoch   9, Step:    56400, Batch Loss:     1.966208, Tokens per Sec:    15957, Lr: 0.000300\n",
            "2021-07-13 15:28:20,603 - INFO - joeynmt.training - Epoch   9, Step:    56600, Batch Loss:     1.938389, Tokens per Sec:    16101, Lr: 0.000300\n",
            "2021-07-13 15:28:47,405 - INFO - joeynmt.training - Epoch   9, Step:    56800, Batch Loss:     2.024562, Tokens per Sec:    15782, Lr: 0.000300\n",
            "2021-07-13 15:29:14,440 - INFO - joeynmt.training - Epoch   9, Step:    57000, Batch Loss:     2.171356, Tokens per Sec:    16078, Lr: 0.000300\n",
            "2021-07-13 15:29:41,054 - INFO - joeynmt.training - Epoch   9, Step:    57200, Batch Loss:     1.968767, Tokens per Sec:    15782, Lr: 0.000300\n",
            "2021-07-13 15:30:07,994 - INFO - joeynmt.training - Epoch   9, Step:    57400, Batch Loss:     2.271033, Tokens per Sec:    16028, Lr: 0.000300\n",
            "2021-07-13 15:31:22,013 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:31:22,013 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:31:22,014 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:31:22,616 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:31:22,616 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:31:23,301 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:31:23,302 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 15:31:23,303 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 15:31:23,303 - INFO - joeynmt.training - \tHypothesis: [ I ] have hated you with good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-13 15:31:23,303 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:31:23,303 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 15:31:23,303 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 15:31:23,304 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 15:31:23,304 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:31:23,304 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 15:31:23,304 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 15:31:23,305 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-13 15:31:23,305 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:31:23,305 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 15:31:23,305 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 15:31:23,306 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to do to save his family .\n",
            "2021-07-13 15:31:23,306 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    57500: bleu:  15.36, loss: 137239.7188, ppl:   9.3471, duration: 61.9812s\n",
            "2021-07-13 15:31:36,933 - INFO - joeynmt.training - Epoch   9, Step:    57600, Batch Loss:     1.535096, Tokens per Sec:    15808, Lr: 0.000300\n",
            "2021-07-13 15:31:49,507 - INFO - joeynmt.training - Epoch   9: total training loss 5805.99\n",
            "2021-07-13 15:31:49,507 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-13 15:32:03,962 - INFO - joeynmt.training - Epoch  10, Step:    57800, Batch Loss:     2.081113, Tokens per Sec:    15368, Lr: 0.000300\n",
            "2021-07-13 15:32:30,783 - INFO - joeynmt.training - Epoch  10, Step:    58000, Batch Loss:     2.006075, Tokens per Sec:    16048, Lr: 0.000300\n",
            "2021-07-13 15:32:57,408 - INFO - joeynmt.training - Epoch  10, Step:    58200, Batch Loss:     1.950063, Tokens per Sec:    15940, Lr: 0.000300\n",
            "2021-07-13 15:33:24,165 - INFO - joeynmt.training - Epoch  10, Step:    58400, Batch Loss:     2.073067, Tokens per Sec:    16116, Lr: 0.000300\n",
            "2021-07-13 15:33:51,020 - INFO - joeynmt.training - Epoch  10, Step:    58600, Batch Loss:     2.038408, Tokens per Sec:    16012, Lr: 0.000300\n",
            "2021-07-13 15:34:17,874 - INFO - joeynmt.training - Epoch  10, Step:    58800, Batch Loss:     1.988990, Tokens per Sec:    16056, Lr: 0.000300\n",
            "2021-07-13 15:34:44,697 - INFO - joeynmt.training - Epoch  10, Step:    59000, Batch Loss:     1.954651, Tokens per Sec:    16091, Lr: 0.000300\n",
            "2021-07-13 15:35:11,578 - INFO - joeynmt.training - Epoch  10, Step:    59200, Batch Loss:     2.387050, Tokens per Sec:    15760, Lr: 0.000300\n",
            "2021-07-13 15:35:38,443 - INFO - joeynmt.training - Epoch  10, Step:    59400, Batch Loss:     2.192607, Tokens per Sec:    16060, Lr: 0.000300\n",
            "2021-07-13 15:36:05,215 - INFO - joeynmt.training - Epoch  10, Step:    59600, Batch Loss:     2.197866, Tokens per Sec:    15914, Lr: 0.000300\n",
            "2021-07-13 15:36:31,944 - INFO - joeynmt.training - Epoch  10, Step:    59800, Batch Loss:     2.078279, Tokens per Sec:    15800, Lr: 0.000300\n",
            "2021-07-13 15:36:58,704 - INFO - joeynmt.training - Epoch  10, Step:    60000, Batch Loss:     1.971877, Tokens per Sec:    16038, Lr: 0.000300\n",
            "2021-07-13 15:38:06,595 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:38:06,595 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:38:06,595 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:38:07,228 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:38:07,229 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:38:07,906 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:38:07,907 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 15:38:07,907 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 15:38:07,907 - INFO - joeynmt.training - \tHypothesis: When you have a good friend and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-13 15:38:07,907 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:38:07,908 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 15:38:07,908 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 15:38:07,908 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-13 15:38:07,908 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:38:07,909 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 15:38:07,909 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 15:38:07,909 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-13 15:38:07,909 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:38:07,910 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 15:38:07,910 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 15:38:07,910 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to do to save his family .\n",
            "2021-07-13 15:38:07,910 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    60000: bleu:  14.62, loss: 136941.4375, ppl:   9.3018, duration: 69.2062s\n",
            "2021-07-13 15:38:34,859 - INFO - joeynmt.training - Epoch  10, Step:    60200, Batch Loss:     2.023072, Tokens per Sec:    15684, Lr: 0.000300\n",
            "2021-07-13 15:39:01,807 - INFO - joeynmt.training - Epoch  10, Step:    60400, Batch Loss:     2.082151, Tokens per Sec:    15985, Lr: 0.000300\n",
            "2021-07-13 15:39:26,231 - INFO - joeynmt.training - Epoch  10: total training loss 5781.29\n",
            "2021-07-13 15:39:26,231 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-13 15:39:28,856 - INFO - joeynmt.training - Epoch  11, Step:    60600, Batch Loss:     1.554266, Tokens per Sec:    14473, Lr: 0.000300\n",
            "2021-07-13 15:39:55,567 - INFO - joeynmt.training - Epoch  11, Step:    60800, Batch Loss:     1.695426, Tokens per Sec:    15922, Lr: 0.000300\n",
            "2021-07-13 15:40:22,638 - INFO - joeynmt.training - Epoch  11, Step:    61000, Batch Loss:     1.533256, Tokens per Sec:    16134, Lr: 0.000300\n",
            "2021-07-13 15:40:49,440 - INFO - joeynmt.training - Epoch  11, Step:    61200, Batch Loss:     2.139320, Tokens per Sec:    15878, Lr: 0.000300\n",
            "2021-07-13 15:41:16,229 - INFO - joeynmt.training - Epoch  11, Step:    61400, Batch Loss:     2.043695, Tokens per Sec:    16080, Lr: 0.000300\n",
            "2021-07-13 15:41:42,861 - INFO - joeynmt.training - Epoch  11, Step:    61600, Batch Loss:     1.812197, Tokens per Sec:    15980, Lr: 0.000300\n",
            "2021-07-13 15:42:09,474 - INFO - joeynmt.training - Epoch  11, Step:    61800, Batch Loss:     2.570075, Tokens per Sec:    15883, Lr: 0.000300\n",
            "2021-07-13 15:42:36,198 - INFO - joeynmt.training - Epoch  11, Step:    62000, Batch Loss:     1.545416, Tokens per Sec:    15894, Lr: 0.000300\n",
            "2021-07-13 15:43:02,818 - INFO - joeynmt.training - Epoch  11, Step:    62200, Batch Loss:     2.283978, Tokens per Sec:    15889, Lr: 0.000300\n",
            "2021-07-13 15:43:29,415 - INFO - joeynmt.training - Epoch  11, Step:    62400, Batch Loss:     1.945134, Tokens per Sec:    16054, Lr: 0.000300\n",
            "2021-07-13 15:44:45,540 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:44:45,541 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:44:45,541 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:44:46,148 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:44:46,148 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:44:46,822 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:44:46,823 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-13 15:44:46,823 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-13 15:44:46,823 - INFO - joeynmt.training - \tHypothesis: When you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-13 15:44:46,823 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:44:46,824 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-13 15:44:46,824 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-13 15:44:46,824 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , my beloved Son , whom I am . ”\n",
            "2021-07-13 15:44:46,824 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:44:46,825 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-13 15:44:46,825 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-13 15:44:46,825 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-13 15:44:46,825 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:44:46,825 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-13 15:44:46,826 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-13 15:44:46,826 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-13 15:44:46,826 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    62500: bleu:  15.43, loss: 135767.0000, ppl:   9.1256, duration: 64.0173s\n",
            "2021-07-13 15:45:00,279 - INFO - joeynmt.training - Epoch  11, Step:    62600, Batch Loss:     2.029629, Tokens per Sec:    16088, Lr: 0.000300\n",
            "2021-07-13 15:45:27,105 - INFO - joeynmt.training - Epoch  11, Step:    62800, Batch Loss:     1.930335, Tokens per Sec:    15920, Lr: 0.000300\n",
            "2021-07-13 15:45:53,907 - INFO - joeynmt.training - Epoch  11, Step:    63000, Batch Loss:     1.822715, Tokens per Sec:    15848, Lr: 0.000300\n",
            "2021-07-13 15:46:20,493 - INFO - joeynmt.training - Epoch  11, Step:    63200, Batch Loss:     1.890908, Tokens per Sec:    16105, Lr: 0.000300\n",
            "2021-07-13 15:46:47,090 - INFO - joeynmt.training - Epoch  11, Step:    63400, Batch Loss:     1.641337, Tokens per Sec:    16025, Lr: 0.000300\n",
            "2021-07-13 15:46:57,074 - INFO - joeynmt.training - Epoch  11: total training loss 5757.16\n",
            "2021-07-13 15:46:57,075 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-13 15:47:13,912 - INFO - joeynmt.training - Epoch  12, Step:    63600, Batch Loss:     1.861826, Tokens per Sec:    15559, Lr: 0.000300\n",
            "2021-07-13 15:47:40,671 - INFO - joeynmt.training - Epoch  12, Step:    63800, Batch Loss:     1.987771, Tokens per Sec:    15985, Lr: 0.000300\n",
            "2021-07-13 15:48:07,450 - INFO - joeynmt.training - Epoch  12, Step:    64000, Batch Loss:     1.890187, Tokens per Sec:    16001, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlJ8i_1H3afn",
        "outputId": "99227b40-e260-4bcb-be0e-894fd12a1064"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"joeynmt/models/lg_lhen_reverse_transformer_continued/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 35000\tLoss: 145461.15625\tPPL: 10.68626\tbleu: 13.82629\tLR: 0.00030000\t*\n",
            "Steps: 37500\tLoss: 144813.43750\tPPL: 10.57412\tbleu: 13.95177\tLR: 0.00030000\t*\n",
            "Steps: 40000\tLoss: 142993.68750\tPPL: 10.26534\tbleu: 14.04079\tLR: 0.00030000\t*\n",
            "Steps: 42500\tLoss: 141620.42188\tPPL: 10.03831\tbleu: 14.43683\tLR: 0.00030000\t*\n",
            "Steps: 45000\tLoss: 141061.18750\tPPL: 9.94730\tbleu: 14.58504\tLR: 0.00030000\t*\n",
            "Steps: 47500\tLoss: 140957.23438\tPPL: 9.93047\tbleu: 14.24453\tLR: 0.00030000\t*\n",
            "Steps: 50000\tLoss: 140635.35938\tPPL: 9.87855\tbleu: 14.14640\tLR: 0.00030000\t*\n",
            "Steps: 52500\tLoss: 138489.68750\tPPL: 9.53932\tbleu: 15.12613\tLR: 0.00030000\t*\n",
            "Steps: 55000\tLoss: 137841.76562\tPPL: 9.43919\tbleu: 15.60457\tLR: 0.00030000\t*\n",
            "Steps: 57500\tLoss: 137239.71875\tPPL: 9.34709\tbleu: 15.36421\tLR: 0.00030000\t*\n",
            "Steps: 60000\tLoss: 136941.43750\tPPL: 9.30180\tbleu: 14.61966\tLR: 0.00030000\t*\n",
            "Steps: 62500\tLoss: 135767.00000\tPPL: 9.12557\tbleu: 15.43062\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNBKJDdVEuEY"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 62500\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/models/lg_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_lhen_reverse_transformer\"', f'model_dir: \"models/lg_lhen_reverse_transformer_continued2\"').replace(\n",
        "        f'epochs: 30', f'epochs: 6')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GeNTAuUpFUAn",
        "outputId": "b927b455-6b1f-41cc-8753-d7b5ae86d723"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_lg_lhen_reload2.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"lg_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"lg_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued/62500.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 6                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/lg_lhen_reverse_transformer_continued2\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wal6iATCFX9V",
        "outputId": "912ae08c-f0ca-46af-f3bb-1aa24036102a"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_lhen_reload2.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 12:56:02,417 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 12:56:02,506 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-14 12:56:09,114 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-14 12:56:09,935 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-14 12:56:10,976 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-14 12:56:12,352 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-14 12:56:12,352 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 12:56:12,597 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-14 12:56:12.855030: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 12:56:15,938 - INFO - joeynmt.training - Total params: 12152320\n",
            "2021-07-14 12:56:24,564 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued/62500.ckpt\n",
            "2021-07-14 12:56:25,121 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
            "2021-07-14 12:56:25,121 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
            "2021-07-14 12:56:25,121 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-14 12:56:25,122 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
            "2021-07-14 12:56:25,122 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
            "2021-07-14 12:56:25,122 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
            "2021-07-14 12:56:25,122 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-14 12:56:25,123 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-14 12:56:25,123 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-14 12:56:25,123 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
            "2021-07-14 12:56:25,123 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
            "2021-07-14 12:56:25,123 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-14 12:56:25,124 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-14 12:56:25,124 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued/62500.ckpt\n",
            "2021-07-14 12:56:25,124 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-14 12:56:25,124 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-14 12:56:25,125 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-14 12:56:25,125 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-14 12:56:25,125 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-14 12:56:25,125 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-14 12:56:25,126 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-14 12:56:25,126 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-14 12:56:25,126 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-14 12:56:25,126 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-14 12:56:25,127 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-14 12:56:25,127 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-14 12:56:25,127 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-14 12:56:25,127 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-14 12:56:25,127 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-14 12:56:25,128 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-14 12:56:25,128 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-14 12:56:25,128 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-14 12:56:25,128 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-14 12:56:25,129 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-14 12:56:25,129 - INFO - joeynmt.helpers - cfg.training.epochs                : 6\n",
            "2021-07-14 12:56:25,129 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
            "2021-07-14 12:56:25,129 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-14 12:56:25,130 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-14 12:56:25,130 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer_continued2\n",
            "2021-07-14 12:56:25,130 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-14 12:56:25,130 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-14 12:56:25,131 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-14 12:56:25,131 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-14 12:56:25,131 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-14 12:56:25,131 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-14 12:56:25,131 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-14 12:56:25,132 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-14 12:56:25,132 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-14 12:56:25,132 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-14 12:56:25,133 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-14 12:56:25,133 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-14 12:56:25,133 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-14 12:56:25,133 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-14 12:56:25,134 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-14 12:56:25,134 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-14 12:56:25,134 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-14 12:56:25,134 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-14 12:56:25,134 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-14 12:56:25,135 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-14 12:56:25,135 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-14 12:56:25,135 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-14 12:56:25,135 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-14 12:56:25,136 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-14 12:56:25,136 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-14 12:56:25,136 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-14 12:56:25,136 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-14 12:56:25,137 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-14 12:56:25,137 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-14 12:56:25,137 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-14 12:56:25,137 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-14 12:56:25,137 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 231302,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-14 12:56:25,138 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
            "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
            "2021-07-14 12:56:25,138 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
            "2021-07-14 12:56:25,138 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
            "2021-07-14 12:56:25,139 - INFO - joeynmt.helpers - Number of Src words (types): 4266\n",
            "2021-07-14 12:56:25,139 - INFO - joeynmt.helpers - Number of Trg words (types): 4266\n",
            "2021-07-14 12:56:25,139 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4266),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4266))\n",
            "2021-07-14 12:56:25,152 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-14 12:56:25,153 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-14 12:56:55,860 - INFO - joeynmt.training - Epoch   1, Step:    62600, Batch Loss:     2.030535, Tokens per Sec:     6951, Lr: 0.000300\n",
            "2021-07-14 12:57:55,494 - INFO - joeynmt.training - Epoch   1, Step:    62800, Batch Loss:     1.898705, Tokens per Sec:     7161, Lr: 0.000300\n",
            "2021-07-14 12:58:55,189 - INFO - joeynmt.training - Epoch   1, Step:    63000, Batch Loss:     1.835428, Tokens per Sec:     7116, Lr: 0.000300\n",
            "2021-07-14 12:59:54,540 - INFO - joeynmt.training - Epoch   1, Step:    63200, Batch Loss:     1.867559, Tokens per Sec:     7214, Lr: 0.000300\n",
            "2021-07-14 13:00:54,003 - INFO - joeynmt.training - Epoch   1, Step:    63400, Batch Loss:     1.618923, Tokens per Sec:     7168, Lr: 0.000300\n",
            "2021-07-14 13:01:16,345 - INFO - joeynmt.training - Epoch   1: total training loss 1947.49\n",
            "2021-07-14 13:01:16,346 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-14 13:01:53,667 - INFO - joeynmt.training - Epoch   2, Step:    63600, Batch Loss:     1.860639, Tokens per Sec:     7020, Lr: 0.000300\n",
            "2021-07-14 13:02:53,203 - INFO - joeynmt.training - Epoch   2, Step:    63800, Batch Loss:     2.012113, Tokens per Sec:     7184, Lr: 0.000300\n",
            "2021-07-14 13:03:52,847 - INFO - joeynmt.training - Epoch   2, Step:    64000, Batch Loss:     1.889649, Tokens per Sec:     7184, Lr: 0.000300\n",
            "2021-07-14 13:04:52,447 - INFO - joeynmt.training - Epoch   2, Step:    64200, Batch Loss:     2.033569, Tokens per Sec:     7215, Lr: 0.000300\n",
            "2021-07-14 13:05:52,035 - INFO - joeynmt.training - Epoch   2, Step:    64400, Batch Loss:     2.179035, Tokens per Sec:     7162, Lr: 0.000300\n",
            "2021-07-14 13:06:51,337 - INFO - joeynmt.training - Epoch   2, Step:    64600, Batch Loss:     1.963310, Tokens per Sec:     7183, Lr: 0.000300\n",
            "2021-07-14 13:07:51,131 - INFO - joeynmt.training - Epoch   2, Step:    64800, Batch Loss:     1.669863, Tokens per Sec:     7248, Lr: 0.000300\n",
            "2021-07-14 13:08:50,929 - INFO - joeynmt.training - Epoch   2, Step:    65000, Batch Loss:     1.841821, Tokens per Sec:     7178, Lr: 0.000300\n",
            "2021-07-14 13:10:53,906 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:10:53,907 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:10:53,907 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:10:54,700 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 13:10:54,701 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 13:10:55,608 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:10:55,610 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 13:10:55,610 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 13:10:55,610 - INFO - joeynmt.training - \tHypothesis: I will be able to be patient , and you will be punished when you endure , that is what is acceptable to God . ”\n",
            "2021-07-14 13:10:55,611 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:10:55,612 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 13:10:55,612 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 13:10:55,612 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , whom I am grateful . ”\n",
            "2021-07-14 13:10:55,613 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:10:55,614 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 13:10:55,614 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 13:10:55,615 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-14 13:10:55,615 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:10:55,615 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 13:10:55,616 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 13:10:55,616 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-14 13:10:55,616 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    65000: bleu:  15.85, loss: 134558.1094, ppl:   8.9477, duration: 124.6870s\n",
            "2021-07-14 13:11:55,596 - INFO - joeynmt.training - Epoch   2, Step:    65200, Batch Loss:     2.348196, Tokens per Sec:     7251, Lr: 0.000300\n",
            "2021-07-14 13:12:55,299 - INFO - joeynmt.training - Epoch   2, Step:    65400, Batch Loss:     1.643145, Tokens per Sec:     7164, Lr: 0.000300\n",
            "2021-07-14 13:13:54,472 - INFO - joeynmt.training - Epoch   2, Step:    65600, Batch Loss:     2.053719, Tokens per Sec:     7166, Lr: 0.000300\n",
            "2021-07-14 13:14:53,682 - INFO - joeynmt.training - Epoch   2, Step:    65800, Batch Loss:     2.019640, Tokens per Sec:     7139, Lr: 0.000300\n",
            "2021-07-14 13:15:53,222 - INFO - joeynmt.training - Epoch   2, Step:    66000, Batch Loss:     1.818984, Tokens per Sec:     7124, Lr: 0.000300\n",
            "2021-07-14 13:16:53,298 - INFO - joeynmt.training - Epoch   2, Step:    66200, Batch Loss:     1.995557, Tokens per Sec:     7169, Lr: 0.000300\n",
            "2021-07-14 13:17:42,240 - INFO - joeynmt.training - Epoch   2: total training loss 5718.37\n",
            "2021-07-14 13:17:42,241 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-14 13:17:53,315 - INFO - joeynmt.training - Epoch   3, Step:    66400, Batch Loss:     1.811290, Tokens per Sec:     6838, Lr: 0.000300\n",
            "2021-07-14 13:18:53,287 - INFO - joeynmt.training - Epoch   3, Step:    66600, Batch Loss:     2.140575, Tokens per Sec:     7252, Lr: 0.000300\n",
            "2021-07-14 13:19:52,674 - INFO - joeynmt.training - Epoch   3, Step:    66800, Batch Loss:     1.774339, Tokens per Sec:     7205, Lr: 0.000300\n",
            "2021-07-14 13:20:52,023 - INFO - joeynmt.training - Epoch   3, Step:    67000, Batch Loss:     2.142756, Tokens per Sec:     7156, Lr: 0.000300\n",
            "2021-07-14 13:21:51,419 - INFO - joeynmt.training - Epoch   3, Step:    67200, Batch Loss:     1.741992, Tokens per Sec:     7153, Lr: 0.000300\n",
            "2021-07-14 13:22:51,112 - INFO - joeynmt.training - Epoch   3, Step:    67400, Batch Loss:     1.874300, Tokens per Sec:     7214, Lr: 0.000300\n",
            "2021-07-14 13:25:30,305 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:25:30,306 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:25:30,306 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:25:31,967 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:25:31,968 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 13:25:31,968 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 13:25:31,968 - INFO - joeynmt.training - \tHypothesis: When you have been doing good and suffered , you will be tormented by your endurance , that is what is acceptable to God . ”\n",
            "2021-07-14 13:25:31,968 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:25:31,969 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 13:25:31,969 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 13:25:31,969 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven , saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-14 13:25:31,970 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:25:31,970 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 13:25:31,970 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 13:25:31,971 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-14 13:25:31,971 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:25:31,972 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 13:25:31,972 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 13:25:31,972 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-14 13:25:31,972 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    67500: bleu:  15.42, loss: 135527.7031, ppl:   9.0901, duration: 131.4506s\n",
            "2021-07-14 13:26:02,169 - INFO - joeynmt.training - Epoch   3, Step:    67600, Batch Loss:     2.076301, Tokens per Sec:     7174, Lr: 0.000300\n",
            "2021-07-14 13:27:02,033 - INFO - joeynmt.training - Epoch   3, Step:    67800, Batch Loss:     2.042751, Tokens per Sec:     7178, Lr: 0.000300\n",
            "2021-07-14 13:28:02,310 - INFO - joeynmt.training - Epoch   3, Step:    68000, Batch Loss:     1.865932, Tokens per Sec:     7206, Lr: 0.000300\n",
            "2021-07-14 13:29:02,125 - INFO - joeynmt.training - Epoch   3, Step:    68200, Batch Loss:     1.895920, Tokens per Sec:     7136, Lr: 0.000300\n",
            "2021-07-14 13:30:01,777 - INFO - joeynmt.training - Epoch   3, Step:    68400, Batch Loss:     1.868003, Tokens per Sec:     7228, Lr: 0.000300\n",
            "2021-07-14 13:31:01,560 - INFO - joeynmt.training - Epoch   3, Step:    68600, Batch Loss:     2.223895, Tokens per Sec:     7220, Lr: 0.000300\n",
            "2021-07-14 13:32:01,444 - INFO - joeynmt.training - Epoch   3, Step:    68800, Batch Loss:     1.949568, Tokens per Sec:     7267, Lr: 0.000300\n",
            "2021-07-14 13:33:00,898 - INFO - joeynmt.training - Epoch   3, Step:    69000, Batch Loss:     2.016216, Tokens per Sec:     7143, Lr: 0.000300\n",
            "2021-07-14 13:34:00,187 - INFO - joeynmt.training - Epoch   3, Step:    69200, Batch Loss:     2.006118, Tokens per Sec:     7042, Lr: 0.000300\n",
            "2021-07-14 13:34:14,505 - INFO - joeynmt.training - Epoch   3: total training loss 5677.68\n",
            "2021-07-14 13:34:14,505 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-14 13:35:00,352 - INFO - joeynmt.training - Epoch   4, Step:    69400, Batch Loss:     1.718817, Tokens per Sec:     7005, Lr: 0.000300\n",
            "2021-07-14 13:35:59,536 - INFO - joeynmt.training - Epoch   4, Step:    69600, Batch Loss:     2.117076, Tokens per Sec:     7197, Lr: 0.000300\n",
            "2021-07-14 13:36:59,361 - INFO - joeynmt.training - Epoch   4, Step:    69800, Batch Loss:     1.705801, Tokens per Sec:     7184, Lr: 0.000300\n",
            "2021-07-14 13:37:58,897 - INFO - joeynmt.training - Epoch   4, Step:    70000, Batch Loss:     2.122095, Tokens per Sec:     7070, Lr: 0.000300\n",
            "2021-07-14 13:39:56,276 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:39:56,277 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:39:56,277 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:39:57,050 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 13:39:57,050 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 13:39:58,436 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:39:58,437 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 13:39:58,437 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 13:39:58,437 - INFO - joeynmt.training - \tHypothesis: If you have a good friend , you will suffer when you endure , what is approved to God . ”\n",
            "2021-07-14 13:39:58,438 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:39:58,438 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 13:39:58,439 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 13:39:58,439 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son . ”\n",
            "2021-07-14 13:39:58,439 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:39:58,440 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 13:39:58,440 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 13:39:58,440 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-14 13:39:58,441 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:39:58,441 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 13:39:58,442 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 13:39:58,442 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-14 13:39:58,442 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    70000: bleu:  16.02, loss: 134097.1094, ppl:   8.8807, duration: 119.5445s\n",
            "2021-07-14 13:40:58,366 - INFO - joeynmt.training - Epoch   4, Step:    70200, Batch Loss:     1.742387, Tokens per Sec:     7176, Lr: 0.000300\n",
            "2021-07-14 13:41:58,163 - INFO - joeynmt.training - Epoch   4, Step:    70400, Batch Loss:     2.123537, Tokens per Sec:     7150, Lr: 0.000300\n",
            "2021-07-14 13:42:57,247 - INFO - joeynmt.training - Epoch   4, Step:    70600, Batch Loss:     2.074317, Tokens per Sec:     7137, Lr: 0.000300\n",
            "2021-07-14 13:43:57,002 - INFO - joeynmt.training - Epoch   4, Step:    70800, Batch Loss:     1.961870, Tokens per Sec:     7225, Lr: 0.000300\n",
            "2021-07-14 13:44:57,131 - INFO - joeynmt.training - Epoch   4, Step:    71000, Batch Loss:     2.032303, Tokens per Sec:     7214, Lr: 0.000300\n",
            "2021-07-14 13:45:57,048 - INFO - joeynmt.training - Epoch   4, Step:    71200, Batch Loss:     2.116080, Tokens per Sec:     7145, Lr: 0.000300\n",
            "2021-07-14 13:46:57,132 - INFO - joeynmt.training - Epoch   4, Step:    71400, Batch Loss:     2.241623, Tokens per Sec:     7216, Lr: 0.000300\n",
            "2021-07-14 13:47:56,413 - INFO - joeynmt.training - Epoch   4, Step:    71600, Batch Loss:     1.622877, Tokens per Sec:     7223, Lr: 0.000300\n",
            "2021-07-14 13:48:56,150 - INFO - joeynmt.training - Epoch   4, Step:    71800, Batch Loss:     2.220649, Tokens per Sec:     7144, Lr: 0.000300\n",
            "2021-07-14 13:49:55,652 - INFO - joeynmt.training - Epoch   4, Step:    72000, Batch Loss:     2.004412, Tokens per Sec:     7184, Lr: 0.000300\n",
            "2021-07-14 13:50:36,108 - INFO - joeynmt.training - Epoch   4: total training loss 5650.27\n",
            "2021-07-14 13:50:36,109 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-14 13:50:55,644 - INFO - joeynmt.training - Epoch   5, Step:    72200, Batch Loss:     1.916708, Tokens per Sec:     6987, Lr: 0.000300\n",
            "2021-07-14 13:51:55,546 - INFO - joeynmt.training - Epoch   5, Step:    72400, Batch Loss:     1.802712, Tokens per Sec:     7174, Lr: 0.000300\n",
            "2021-07-14 13:54:19,557 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:54:19,558 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:54:19,558 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:54:20,314 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 13:54:20,314 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 13:54:21,582 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:54:21,583 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 13:54:21,584 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 13:54:21,584 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , what is approved to God . ”\n",
            "2021-07-14 13:54:21,584 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:54:21,585 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 13:54:21,585 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 13:54:21,585 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son of whom I am giving thanksgiving . ”\n",
            "2021-07-14 13:54:21,586 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:54:21,586 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 13:54:21,587 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 13:54:21,587 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-14 13:54:21,587 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:54:21,588 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 13:54:21,588 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 13:54:21,588 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to do to save his family .\n",
            "2021-07-14 13:54:21,588 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    72500: bleu:  16.29, loss: 133619.3125, ppl:   8.8119, duration: 116.1504s\n",
            "2021-07-14 13:54:51,691 - INFO - joeynmt.training - Epoch   5, Step:    72600, Batch Loss:     2.098821, Tokens per Sec:     7194, Lr: 0.000300\n",
            "2021-07-14 13:55:51,260 - INFO - joeynmt.training - Epoch   5, Step:    72800, Batch Loss:     1.918632, Tokens per Sec:     7224, Lr: 0.000300\n",
            "2021-07-14 13:56:51,279 - INFO - joeynmt.training - Epoch   5, Step:    73000, Batch Loss:     1.819686, Tokens per Sec:     7269, Lr: 0.000300\n",
            "2021-07-14 13:57:51,135 - INFO - joeynmt.training - Epoch   5, Step:    73200, Batch Loss:     1.872079, Tokens per Sec:     7233, Lr: 0.000300\n",
            "2021-07-14 13:58:50,901 - INFO - joeynmt.training - Epoch   5, Step:    73400, Batch Loss:     1.942369, Tokens per Sec:     7207, Lr: 0.000300\n",
            "2021-07-14 13:59:49,832 - INFO - joeynmt.training - Epoch   5, Step:    73600, Batch Loss:     2.017744, Tokens per Sec:     7104, Lr: 0.000300\n",
            "2021-07-14 14:00:49,913 - INFO - joeynmt.training - Epoch   5, Step:    73800, Batch Loss:     1.767118, Tokens per Sec:     7086, Lr: 0.000300\n",
            "2021-07-14 14:01:49,898 - INFO - joeynmt.training - Epoch   5, Step:    74000, Batch Loss:     1.880497, Tokens per Sec:     7183, Lr: 0.000300\n",
            "2021-07-14 14:02:49,804 - INFO - joeynmt.training - Epoch   5, Step:    74200, Batch Loss:     1.858565, Tokens per Sec:     7178, Lr: 0.000300\n",
            "2021-07-14 14:03:49,397 - INFO - joeynmt.training - Epoch   5, Step:    74400, Batch Loss:     1.473223, Tokens per Sec:     7118, Lr: 0.000300\n",
            "2021-07-14 14:04:49,017 - INFO - joeynmt.training - Epoch   5, Step:    74600, Batch Loss:     2.034116, Tokens per Sec:     7180, Lr: 0.000300\n",
            "2021-07-14 14:05:49,100 - INFO - joeynmt.training - Epoch   5, Step:    74800, Batch Loss:     1.950121, Tokens per Sec:     7190, Lr: 0.000300\n",
            "2021-07-14 14:06:48,818 - INFO - joeynmt.training - Epoch   5, Step:    75000, Batch Loss:     2.194497, Tokens per Sec:     7091, Lr: 0.000300\n",
            "2021-07-14 14:08:32,590 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:08:32,591 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:08:32,591 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:08:33,367 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 14:08:33,368 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 14:08:34,705 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:08:34,706 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 14:08:34,706 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 14:08:34,707 - INFO - joeynmt.training - \tHypothesis: I have made your mouth working well and suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-14 14:08:34,707 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:08:34,708 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 14:08:34,708 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 14:08:34,708 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son . ”\n",
            "2021-07-14 14:08:34,709 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:08:34,709 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 14:08:34,710 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 14:08:34,710 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-14 14:08:34,710 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:08:34,711 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 14:08:34,711 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 14:08:34,712 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-14 14:08:34,712 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    75000: bleu:  16.64, loss: 132864.8438, ppl:   8.7043, duration: 105.8935s\n",
            "2021-07-14 14:08:40,150 - INFO - joeynmt.training - Epoch   5: total training loss 5610.22\n",
            "2021-07-14 14:08:40,150 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-14 14:09:35,203 - INFO - joeynmt.training - Epoch   6, Step:    75200, Batch Loss:     2.031440, Tokens per Sec:     7133, Lr: 0.000300\n",
            "2021-07-14 14:10:35,121 - INFO - joeynmt.training - Epoch   6, Step:    75400, Batch Loss:     1.784257, Tokens per Sec:     7097, Lr: 0.000300\n",
            "2021-07-14 14:11:34,335 - INFO - joeynmt.training - Epoch   6, Step:    75600, Batch Loss:     1.751221, Tokens per Sec:     7097, Lr: 0.000300\n",
            "2021-07-14 14:12:34,166 - INFO - joeynmt.training - Epoch   6, Step:    75800, Batch Loss:     1.792912, Tokens per Sec:     7197, Lr: 0.000300\n",
            "2021-07-14 14:13:33,729 - INFO - joeynmt.training - Epoch   6, Step:    76000, Batch Loss:     2.001759, Tokens per Sec:     7173, Lr: 0.000300\n",
            "2021-07-14 14:14:33,179 - INFO - joeynmt.training - Epoch   6, Step:    76200, Batch Loss:     1.905588, Tokens per Sec:     7170, Lr: 0.000300\n",
            "2021-07-14 14:15:32,972 - INFO - joeynmt.training - Epoch   6, Step:    76400, Batch Loss:     1.544792, Tokens per Sec:     7137, Lr: 0.000300\n",
            "2021-07-14 14:16:32,690 - INFO - joeynmt.training - Epoch   6, Step:    76600, Batch Loss:     1.938333, Tokens per Sec:     7206, Lr: 0.000300\n",
            "2021-07-14 14:17:32,393 - INFO - joeynmt.training - Epoch   6, Step:    76800, Batch Loss:     2.064925, Tokens per Sec:     7168, Lr: 0.000300\n",
            "2021-07-14 14:18:31,984 - INFO - joeynmt.training - Epoch   6, Step:    77000, Batch Loss:     1.353596, Tokens per Sec:     7135, Lr: 0.000300\n",
            "2021-07-14 14:19:31,409 - INFO - joeynmt.training - Epoch   6, Step:    77200, Batch Loss:     1.739295, Tokens per Sec:     7156, Lr: 0.000300\n",
            "2021-07-14 14:20:31,173 - INFO - joeynmt.training - Epoch   6, Step:    77400, Batch Loss:     2.184101, Tokens per Sec:     7161, Lr: 0.000300\n",
            "2021-07-14 14:22:58,205 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:22:58,205 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:22:58,206 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:22:58,988 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 14:22:58,989 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 14:23:00,232 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:23:00,233 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-14 14:23:00,233 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-14 14:23:00,234 - INFO - joeynmt.training - \tHypothesis: When you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-14 14:23:00,234 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:23:00,235 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-14 14:23:00,235 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-14 14:23:00,235 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-14 14:23:00,235 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:23:00,236 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-14 14:23:00,236 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-14 14:23:00,237 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-14 14:23:00,237 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:23:00,238 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-14 14:23:00,238 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-14 14:23:00,238 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-14 14:23:00,238 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    77500: bleu:  16.45, loss: 132711.6562, ppl:   8.6826, duration: 118.9093s\n",
            "2021-07-14 14:23:30,478 - INFO - joeynmt.training - Epoch   6, Step:    77600, Batch Loss:     1.998454, Tokens per Sec:     7212, Lr: 0.000300\n",
            "2021-07-14 14:24:30,059 - INFO - joeynmt.training - Epoch   6, Step:    77800, Batch Loss:     1.838620, Tokens per Sec:     7044, Lr: 0.000300\n",
            "2021-07-14 14:25:03,560 - INFO - joeynmt.training - Epoch   6: total training loss 5623.46\n",
            "2021-07-14 14:25:03,561 - INFO - joeynmt.training - Training ended after   6 epochs.\n",
            "2021-07-14 14:25:03,561 - INFO - joeynmt.training - Best validation result (greedy) at step    77500:   8.68 ppl.\n",
            "2021-07-14 14:25:03,589 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-14 14:25:04,033 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 14:25:04,266 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-14 14:25:04,336 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe.en)...\n",
            "2021-07-14 14:27:44,110 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:27:44,111 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:27:44,111 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:27:44,859 - INFO - joeynmt.prediction -  dev bleu[13a]:  17.35 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-14 14:27:44,866 - INFO - joeynmt.prediction - Translations saved to: models/lg_lhen_reverse_transformer_continued2/00077500.hyps.dev\n",
            "2021-07-14 14:27:44,867 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.en)...\n",
            "2021-07-14 14:29:26,742 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:29:26,742 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:29:26,743 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:29:27,148 - INFO - joeynmt.prediction - test bleu[13a]:  10.12 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-14 14:29:27,155 - INFO - joeynmt.prediction - Translations saved to: models/lg_lhen_reverse_transformer_continued2/00077500.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z5r5_ZceaEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "642f4125-94d7-4dc2-d881-6580b54dc01a"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt test 'models/lg_lhen_reverse_transformer_continued2/config.yaml'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 14:35:39,152 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 14:35:39,157 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-14 14:35:39,457 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-14 14:35:39,491 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-14 14:35:39,510 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-14 14:35:39,534 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-14 14:35:42,202 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 14:35:42,451 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-14 14:35:42,540 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe.en)...\n",
            "2021-07-14 14:38:21,830 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:38:21,831 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:38:21,831 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:38:22,587 - INFO - joeynmt.prediction -  dev bleu[13a]:  17.35 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-14 14:38:22,587 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.en)...\n",
            "2021-07-14 14:40:03,893 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:40:03,894 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:40:03,894 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:40:04,279 - INFO - joeynmt.prediction - test bleu[13a]:  10.12 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pFGz01l0931",
        "outputId": "92bfeac6-a160-4857-b5e9-c50ef27423bd"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_lhen_reverse_transformer_continued2/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/translation.bpe.lh_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 14:40:45,978 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 14:40:48,854 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 14:40:49,116 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuR4Oadk2cJ6",
        "outputId": "18452113-9def-475e-8569-0fee7161dfc4"
      },
      "source": [
        "!cat \"translation.bpe.lh_en\" | sacrebleu \"test1.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 10.1 41.5/15.1/6.3/3.1 (BP = 0.961 ratio = 0.962 hyp_len = 25045 ref_len = 26044)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0NaoiGM2yQL",
        "outputId": "8d13e1bb-9e6b-4ec6-ae18-73daeef4666b"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_lhen_reverse_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.lg\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/translation.bpe.lg_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 14:42:54,924 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 14:42:57,838 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 14:42:58,080 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BYn6P-3v3yz5"
      },
      "source": [
        "#@title\n",
        "def empty_counter(x):\n",
        "  # Opening a file\n",
        "  infile = open(x,\"r\")\n",
        "  empty = []\n",
        "  \n",
        "  for i,line in enumerate(infile):\n",
        "    if not line.strip(): \n",
        "      empty.append(i)\n",
        "\n",
        "  return empty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "3JYGO4zG35A8"
      },
      "source": [
        "#@title\n",
        "# Reference: https://thispointer.com/python-how-to-delete-specific-lines-in-a-file-in-a-memory-efficient-way/\n",
        "def delete_multiple_lines(original_file, line_numbers):\n",
        "    \"\"\"In a file, delete the lines at line number in given list\"\"\"\n",
        "    is_skipped = False\n",
        "    counter = 0\n",
        "    # Create name of dummy / temporary file\n",
        "    dummy_file = original_file + '.bak'\n",
        "    # Open original file in read only mode and dummy file in write mode\n",
        "    with open(original_file, 'r') as read_obj, open(dummy_file, 'w') as write_obj:\n",
        "        # Line by line copy data from original file to dummy file\n",
        "        for line in read_obj:\n",
        "            # If current line number exist in list then skip copying that line\n",
        "            if counter not in line_numbers:\n",
        "                write_obj.write(line)\n",
        "            else:\n",
        "                is_skipped = True\n",
        "            counter += 1\n",
        "    # If any line is skipped then rename dummy file as original file\n",
        "    if is_skipped:\n",
        "        os.remove(original_file)\n",
        "        os.rename(dummy_file, original_file)\n",
        "    else:\n",
        "        os.remove(dummy_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrAOZxF44E2u"
      },
      "source": [
        "delete_multiple_lines(\"test2.en\",empty_counter(\"test2.lg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ60czHj3NUK",
        "outputId": "22be5f86-47bd-4c08-9489-a284bdd86fb7"
      },
      "source": [
        "!cat \"translation.bpe.lg_en\" | sacrebleu \"test2.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 35.1 65.0/42.8/31.8/24.7 (BP = 0.911 ratio = 0.915 hyp_len = 39454 ref_len = 43116)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp5f07uBi8wo"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 77500\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/models/lg_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued2/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/lg_lhen_reverse_transformer\"', f'model_dir: \"models/lg_lhen_reverse_transformer_continued3\"')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ihmkNdjhj_lO",
        "outputId": "fe2e7d14-a0c4-44cf-de5a-01f95f961c28"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_lg_lhen_reload3.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"lg_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"lg_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued2/77500.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 2500         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/lg_lhen_reverse_transformer_continued3\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgr2ksxOkLiW",
        "outputId": "18134d68-abb1-4ad7-de46-0c7938d8dd75"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_lg_lhen_reload3.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-25 07:29:08,461 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-25 07:29:08,526 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-25 07:29:13,759 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-25 07:29:14,503 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-25 07:29:15,213 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-25 07:29:15,979 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-25 07:29:15,979 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-25 07:29:16,346 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-25 07:29:16.590924: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-25 07:29:18,892 - INFO - joeynmt.training - Total params: 12152320\n",
            "2021-07-25 07:29:29,521 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued2/77500.ckpt\n",
            "2021-07-25 07:29:29,969 - INFO - joeynmt.helpers - cfg.name                           : lg_lhen_reverse_transformer\n",
            "2021-07-25 07:29:29,969 - INFO - joeynmt.helpers - cfg.data.src                       : lg_lh\n",
            "2021-07-25 07:29:29,969 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-25 07:29:29,970 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/train.bpe\n",
            "2021-07-25 07:29:29,970 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe\n",
            "2021-07-25 07:29:29,970 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe\n",
            "2021-07-25 07:29:29,970 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-25 07:29:29,970 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-25 07:29:29,970 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-25 07:29:29,970 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
            "2021-07-25 07:29:29,970 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/vocab.txt\n",
            "2021-07-25 07:29:29,971 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-25 07:29:29,971 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-25 07:29:29,971 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/joeynmt/models/lg_lhen_reverse_transformer_continued2/77500.ckpt\n",
            "2021-07-25 07:29:29,971 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-25 07:29:29,971 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-25 07:29:29,971 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-25 07:29:29,971 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-25 07:29:29,971 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-25 07:29:29,972 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-25 07:29:29,972 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-25 07:29:29,972 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-25 07:29:29,972 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-25 07:29:29,972 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-25 07:29:29,972 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-25 07:29:29,972 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-25 07:29:29,972 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-25 07:29:29,973 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-25 07:29:29,973 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-25 07:29:29,973 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-25 07:29:29,973 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-25 07:29:29,973 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-25 07:29:29,973 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-25 07:29:29,973 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-25 07:29:29,974 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-25 07:29:29,974 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 2500\n",
            "2021-07-25 07:29:29,974 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-25 07:29:29,974 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-25 07:29:29,974 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/lg_lhen_reverse_transformer_continued3\n",
            "2021-07-25 07:29:29,974 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-25 07:29:29,974 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-25 07:29:29,974 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-25 07:29:29,975 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-25 07:29:29,975 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-25 07:29:29,975 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-25 07:29:29,975 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-25 07:29:29,975 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-25 07:29:29,975 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-25 07:29:29,975 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-25 07:29:29,975 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-25 07:29:29,976 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-25 07:29:29,976 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-25 07:29:29,976 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-25 07:29:29,976 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-25 07:29:29,976 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-25 07:29:29,976 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-25 07:29:29,976 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-25 07:29:29,976 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-25 07:29:29,977 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-25 07:29:29,977 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-25 07:29:29,977 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-25 07:29:29,977 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-25 07:29:29,977 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-25 07:29:29,977 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-25 07:29:29,977 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-25 07:29:29,978 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-25 07:29:29,978 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-25 07:29:29,978 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-25 07:29:29,978 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-25 07:29:29,978 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-25 07:29:29,978 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 231302,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-25 07:29:29,978 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Naye oluvannyuma lw’@@ ekiseera , nn@@ atandika okukolera ku mazima ge nnali nj@@ iga , era nn@@ ak@@ iraba nti okusobola okuweereza Yakuwa nn@@ alina okuva mu by’@@ obufuzi n’@@ okul@@ eka emik@@ wano em@@ ib@@ i gye nn@@ alina .\n",
            "\t[TRG] E@@ ven@@ tually , however , the tru@@ ths I learned from the Bible began to sin@@ k deep@@ er into my heart . I real@@ ized that if I wanted to serve Jehovah , I had to change my pol@@ it@@ ical view@@ poin@@ ts and associ@@ ations .\n",
            "2021-07-25 07:29:29,979 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
            "2021-07-25 07:29:29,979 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) the (7) to (8) of (9) mu\n",
            "2021-07-25 07:29:29,979 - INFO - joeynmt.helpers - Number of Src words (types): 4266\n",
            "2021-07-25 07:29:29,979 - INFO - joeynmt.helpers - Number of Trg words (types): 4266\n",
            "2021-07-25 07:29:29,979 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4266),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4266))\n",
            "2021-07-25 07:29:29,988 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-25 07:29:29,989 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-25 07:29:43,755 - INFO - joeynmt.training - Epoch   1, Step:    77600, Batch Loss:     1.983827, Tokens per Sec:    15761, Lr: 0.000300\n",
            "2021-07-25 07:30:08,917 - INFO - joeynmt.training - Epoch   1, Step:    77800, Batch Loss:     1.813932, Tokens per Sec:    16681, Lr: 0.000300\n",
            "2021-07-25 07:30:23,386 - INFO - joeynmt.training - Epoch   1: total training loss 806.46\n",
            "2021-07-25 07:30:23,386 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-25 07:30:34,732 - INFO - joeynmt.training - Epoch   2, Step:    78000, Batch Loss:     2.077531, Tokens per Sec:    16204, Lr: 0.000300\n",
            "2021-07-25 07:31:00,578 - INFO - joeynmt.training - Epoch   2, Step:    78200, Batch Loss:     1.709428, Tokens per Sec:    16368, Lr: 0.000300\n",
            "2021-07-25 07:31:27,122 - INFO - joeynmt.training - Epoch   2, Step:    78400, Batch Loss:     1.843272, Tokens per Sec:    16136, Lr: 0.000300\n",
            "2021-07-25 07:31:53,575 - INFO - joeynmt.training - Epoch   2, Step:    78600, Batch Loss:     2.244375, Tokens per Sec:    16060, Lr: 0.000300\n",
            "2021-07-25 07:32:19,810 - INFO - joeynmt.training - Epoch   2, Step:    78800, Batch Loss:     2.054668, Tokens per Sec:    16134, Lr: 0.000300\n",
            "2021-07-25 07:32:46,102 - INFO - joeynmt.training - Epoch   2, Step:    79000, Batch Loss:     1.954990, Tokens per Sec:    16439, Lr: 0.000300\n",
            "2021-07-25 07:33:12,918 - INFO - joeynmt.training - Epoch   2, Step:    79200, Batch Loss:     1.741471, Tokens per Sec:    16310, Lr: 0.000300\n",
            "2021-07-25 07:33:39,087 - INFO - joeynmt.training - Epoch   2, Step:    79400, Batch Loss:     1.851443, Tokens per Sec:    16304, Lr: 0.000300\n",
            "2021-07-25 07:34:05,467 - INFO - joeynmt.training - Epoch   2, Step:    79600, Batch Loss:     2.056974, Tokens per Sec:    16217, Lr: 0.000300\n",
            "2021-07-25 07:34:32,083 - INFO - joeynmt.training - Epoch   2, Step:    79800, Batch Loss:     1.994552, Tokens per Sec:    16399, Lr: 0.000300\n",
            "2021-07-25 07:34:58,425 - INFO - joeynmt.training - Epoch   2, Step:    80000, Batch Loss:     2.026582, Tokens per Sec:    16343, Lr: 0.000300\n",
            "2021-07-25 07:36:03,035 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 07:36:03,036 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 07:36:03,036 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 07:36:03,657 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 07:36:03,657 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 07:36:04,369 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 07:36:04,369 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 07:36:04,370 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 07:36:04,370 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is what is approved to God . ”\n",
            "2021-07-25 07:36:04,370 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 07:36:04,370 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 07:36:04,371 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 07:36:04,371 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , whom I am grateful . ”\n",
            "2021-07-25 07:36:04,371 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 07:36:04,371 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 07:36:04,371 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 07:36:04,371 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 07:36:04,372 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 07:36:04,372 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 07:36:04,372 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 07:36:04,372 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 07:36:04,372 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    80000: bleu:  16.66, loss: 132312.2969, ppl:   8.6263, duration: 65.9469s\n",
            "2021-07-25 07:36:31,242 - INFO - joeynmt.training - Epoch   2, Step:    80200, Batch Loss:     2.042300, Tokens per Sec:    15769, Lr: 0.000300\n",
            "2021-07-25 07:36:57,992 - INFO - joeynmt.training - Epoch   2, Step:    80400, Batch Loss:     2.025255, Tokens per Sec:    16142, Lr: 0.000300\n",
            "2021-07-25 07:37:24,584 - INFO - joeynmt.training - Epoch   2, Step:    80600, Batch Loss:     2.039941, Tokens per Sec:    16104, Lr: 0.000300\n",
            "2021-07-25 07:37:50,693 - INFO - joeynmt.training - Epoch   2: total training loss 5568.51\n",
            "2021-07-25 07:37:50,693 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-25 07:37:51,534 - INFO - joeynmt.training - Epoch   3, Step:    80800, Batch Loss:     2.422934, Tokens per Sec:    10485, Lr: 0.000300\n",
            "2021-07-25 07:38:18,019 - INFO - joeynmt.training - Epoch   3, Step:    81000, Batch Loss:     1.904354, Tokens per Sec:    16190, Lr: 0.000300\n",
            "2021-07-25 07:38:44,151 - INFO - joeynmt.training - Epoch   3, Step:    81200, Batch Loss:     1.948882, Tokens per Sec:    16196, Lr: 0.000300\n",
            "2021-07-25 07:39:10,412 - INFO - joeynmt.training - Epoch   3, Step:    81400, Batch Loss:     1.819397, Tokens per Sec:    15972, Lr: 0.000300\n",
            "2021-07-25 07:39:36,769 - INFO - joeynmt.training - Epoch   3, Step:    81600, Batch Loss:     1.932583, Tokens per Sec:    16293, Lr: 0.000300\n",
            "2021-07-25 07:40:03,104 - INFO - joeynmt.training - Epoch   3, Step:    81800, Batch Loss:     2.066569, Tokens per Sec:    16315, Lr: 0.000300\n",
            "2021-07-25 07:40:29,651 - INFO - joeynmt.training - Epoch   3, Step:    82000, Batch Loss:     1.968839, Tokens per Sec:    16104, Lr: 0.000300\n",
            "2021-07-25 07:40:55,827 - INFO - joeynmt.training - Epoch   3, Step:    82200, Batch Loss:     2.049047, Tokens per Sec:    16171, Lr: 0.000300\n",
            "2021-07-25 07:41:22,379 - INFO - joeynmt.training - Epoch   3, Step:    82400, Batch Loss:     1.945058, Tokens per Sec:    16351, Lr: 0.000300\n",
            "2021-07-25 07:42:37,864 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 07:42:37,865 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 07:42:37,865 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 07:42:38,471 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 07:42:38,472 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 07:42:39,577 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 07:42:39,578 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 07:42:39,578 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 07:42:39,579 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is what is approved to God . ”\n",
            "2021-07-25 07:42:39,579 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 07:42:39,579 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 07:42:39,579 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 07:42:39,580 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son . ”\n",
            "2021-07-25 07:42:39,580 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 07:42:39,580 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 07:42:39,580 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 07:42:39,581 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 07:42:39,581 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 07:42:39,581 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 07:42:39,581 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 07:42:39,581 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 07:42:39,582 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    82500: bleu:  16.94, loss: 131681.2344, ppl:   8.5381, duration: 64.1068s\n",
            "2021-07-25 07:42:53,197 - INFO - joeynmt.training - Epoch   3, Step:    82600, Batch Loss:     2.110677, Tokens per Sec:    16008, Lr: 0.000300\n",
            "2021-07-25 07:43:19,560 - INFO - joeynmt.training - Epoch   3, Step:    82800, Batch Loss:     2.155807, Tokens per Sec:    16181, Lr: 0.000300\n",
            "2021-07-25 07:43:45,889 - INFO - joeynmt.training - Epoch   3, Step:    83000, Batch Loss:     2.021662, Tokens per Sec:    16488, Lr: 0.000300\n",
            "2021-07-25 07:44:12,514 - INFO - joeynmt.training - Epoch   3, Step:    83200, Batch Loss:     1.657645, Tokens per Sec:    16229, Lr: 0.000300\n",
            "2021-07-25 07:44:38,742 - INFO - joeynmt.training - Epoch   3, Step:    83400, Batch Loss:     2.149078, Tokens per Sec:    16392, Lr: 0.000300\n",
            "2021-07-25 07:45:04,997 - INFO - joeynmt.training - Epoch   3, Step:    83600, Batch Loss:     1.764372, Tokens per Sec:    16086, Lr: 0.000300\n",
            "2021-07-25 07:45:16,357 - INFO - joeynmt.training - Epoch   3: total training loss 5552.52\n",
            "2021-07-25 07:45:16,357 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-25 07:45:31,770 - INFO - joeynmt.training - Epoch   4, Step:    83800, Batch Loss:     2.073828, Tokens per Sec:    15745, Lr: 0.000300\n",
            "2021-07-25 07:45:58,091 - INFO - joeynmt.training - Epoch   4, Step:    84000, Batch Loss:     1.919142, Tokens per Sec:    16376, Lr: 0.000300\n",
            "2021-07-25 07:46:24,503 - INFO - joeynmt.training - Epoch   4, Step:    84200, Batch Loss:     1.939371, Tokens per Sec:    16125, Lr: 0.000300\n",
            "2021-07-25 07:46:50,634 - INFO - joeynmt.training - Epoch   4, Step:    84400, Batch Loss:     2.082556, Tokens per Sec:    16475, Lr: 0.000300\n",
            "2021-07-25 07:47:16,917 - INFO - joeynmt.training - Epoch   4, Step:    84600, Batch Loss:     1.958132, Tokens per Sec:    16077, Lr: 0.000300\n",
            "2021-07-25 07:47:43,220 - INFO - joeynmt.training - Epoch   4, Step:    84800, Batch Loss:     1.925531, Tokens per Sec:    16075, Lr: 0.000300\n",
            "2021-07-25 07:48:09,522 - INFO - joeynmt.training - Epoch   4, Step:    85000, Batch Loss:     2.113686, Tokens per Sec:    16205, Lr: 0.000300\n",
            "2021-07-25 07:49:17,049 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 07:49:17,049 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 07:49:17,050 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 07:49:18,357 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 07:49:18,358 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 07:49:18,358 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 07:49:18,358 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , that is what is approved to God . ”\n",
            "2021-07-25 07:49:18,359 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 07:49:18,359 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 07:49:18,359 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 07:49:18,359 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-25 07:49:18,359 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 07:49:18,360 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 07:49:18,360 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 07:49:18,360 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 07:49:18,361 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 07:49:18,362 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 07:49:18,362 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 07:49:18,362 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 07:49:18,362 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    85000: bleu:  16.23, loss: 132562.3594, ppl:   8.6615, duration: 68.8395s\n",
            "2021-07-25 07:49:44,863 - INFO - joeynmt.training - Epoch   4, Step:    85200, Batch Loss:     1.743462, Tokens per Sec:    16333, Lr: 0.000300\n",
            "2021-07-25 07:50:11,209 - INFO - joeynmt.training - Epoch   4, Step:    85400, Batch Loss:     1.862387, Tokens per Sec:    16106, Lr: 0.000300\n",
            "2021-07-25 07:50:37,230 - INFO - joeynmt.training - Epoch   4, Step:    85600, Batch Loss:     2.154722, Tokens per Sec:    16349, Lr: 0.000300\n",
            "2021-07-25 07:51:03,731 - INFO - joeynmt.training - Epoch   4, Step:    85800, Batch Loss:     1.931834, Tokens per Sec:    16444, Lr: 0.000300\n",
            "2021-07-25 07:51:30,231 - INFO - joeynmt.training - Epoch   4, Step:    86000, Batch Loss:     2.309177, Tokens per Sec:    16387, Lr: 0.000300\n",
            "2021-07-25 07:51:56,440 - INFO - joeynmt.training - Epoch   4, Step:    86200, Batch Loss:     1.720852, Tokens per Sec:    16295, Lr: 0.000300\n",
            "2021-07-25 07:52:22,879 - INFO - joeynmt.training - Epoch   4, Step:    86400, Batch Loss:     2.088709, Tokens per Sec:    16225, Lr: 0.000300\n",
            "2021-07-25 07:52:45,379 - INFO - joeynmt.training - Epoch   4: total training loss 5528.39\n",
            "2021-07-25 07:52:45,379 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-25 07:52:49,285 - INFO - joeynmt.training - Epoch   5, Step:    86600, Batch Loss:     2.154875, Tokens per Sec:    15340, Lr: 0.000300\n",
            "2021-07-25 07:53:15,753 - INFO - joeynmt.training - Epoch   5, Step:    86800, Batch Loss:     1.957486, Tokens per Sec:    16420, Lr: 0.000300\n",
            "2021-07-25 07:53:42,288 - INFO - joeynmt.training - Epoch   5, Step:    87000, Batch Loss:     2.142058, Tokens per Sec:    16141, Lr: 0.000300\n",
            "2021-07-25 07:54:08,842 - INFO - joeynmt.training - Epoch   5, Step:    87200, Batch Loss:     2.078249, Tokens per Sec:    16196, Lr: 0.000300\n",
            "2021-07-25 07:54:35,027 - INFO - joeynmt.training - Epoch   5, Step:    87400, Batch Loss:     1.868787, Tokens per Sec:    16259, Lr: 0.000300\n",
            "2021-07-25 07:55:52,671 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 07:55:52,671 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 07:55:52,671 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 07:55:53,285 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 07:55:53,285 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 07:55:54,043 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 07:55:54,044 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 07:55:54,044 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 07:55:54,044 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is what is pleasing to God . ”\n",
            "2021-07-25 07:55:54,044 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 07:55:54,045 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 07:55:54,045 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 07:55:54,045 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I am grateful . ”\n",
            "2021-07-25 07:55:54,045 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 07:55:54,046 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 07:55:54,046 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 07:55:54,046 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-25 07:55:54,047 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 07:55:54,047 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 07:55:54,047 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 07:55:54,047 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 07:55:54,047 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    87500: bleu:  16.86, loss: 131202.1719, ppl:   8.4718, duration: 66.1522s\n",
            "2021-07-25 07:56:07,583 - INFO - joeynmt.training - Epoch   5, Step:    87600, Batch Loss:     1.940330, Tokens per Sec:    16084, Lr: 0.000300\n",
            "2021-07-25 07:56:33,925 - INFO - joeynmt.training - Epoch   5, Step:    87800, Batch Loss:     1.974289, Tokens per Sec:    16245, Lr: 0.000300\n",
            "2021-07-25 07:57:00,013 - INFO - joeynmt.training - Epoch   5, Step:    88000, Batch Loss:     1.914285, Tokens per Sec:    16296, Lr: 0.000300\n",
            "2021-07-25 07:57:26,495 - INFO - joeynmt.training - Epoch   5, Step:    88200, Batch Loss:     2.036110, Tokens per Sec:    16264, Lr: 0.000300\n",
            "2021-07-25 07:57:52,769 - INFO - joeynmt.training - Epoch   5, Step:    88400, Batch Loss:     1.959165, Tokens per Sec:    16472, Lr: 0.000300\n",
            "2021-07-25 07:58:19,232 - INFO - joeynmt.training - Epoch   5, Step:    88600, Batch Loss:     2.025503, Tokens per Sec:    16362, Lr: 0.000300\n",
            "2021-07-25 07:58:45,467 - INFO - joeynmt.training - Epoch   5, Step:    88800, Batch Loss:     1.995811, Tokens per Sec:    16238, Lr: 0.000300\n",
            "2021-07-25 07:59:11,705 - INFO - joeynmt.training - Epoch   5, Step:    89000, Batch Loss:     1.542668, Tokens per Sec:    16276, Lr: 0.000300\n",
            "2021-07-25 07:59:38,268 - INFO - joeynmt.training - Epoch   5, Step:    89200, Batch Loss:     2.044317, Tokens per Sec:    16084, Lr: 0.000300\n",
            "2021-07-25 08:00:04,783 - INFO - joeynmt.training - Epoch   5, Step:    89400, Batch Loss:     1.912257, Tokens per Sec:    16101, Lr: 0.000300\n",
            "2021-07-25 08:00:11,990 - INFO - joeynmt.training - Epoch   5: total training loss 5496.63\n",
            "2021-07-25 08:00:11,991 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-25 08:00:31,379 - INFO - joeynmt.training - Epoch   6, Step:    89600, Batch Loss:     1.917936, Tokens per Sec:    16068, Lr: 0.000300\n",
            "2021-07-25 08:00:57,489 - INFO - joeynmt.training - Epoch   6, Step:    89800, Batch Loss:     2.000267, Tokens per Sec:    16244, Lr: 0.000300\n",
            "2021-07-25 08:01:23,988 - INFO - joeynmt.training - Epoch   6, Step:    90000, Batch Loss:     1.965584, Tokens per Sec:    16395, Lr: 0.000300\n",
            "2021-07-25 08:02:29,142 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:02:29,143 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:02:29,143 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:02:29,754 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 08:02:29,755 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 08:02:30,480 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:02:30,480 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:02:30,481 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:02:30,481 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 08:02:30,481 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:02:30,481 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:02:30,481 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:02:30,482 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 08:02:30,482 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:02:30,482 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:02:30,482 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:02:30,482 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 08:02:30,483 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:02:30,483 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:02:30,483 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:02:30,483 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 08:02:30,483 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    90000: bleu:  16.88, loss: 130763.4062, ppl:   8.4114, duration: 66.4947s\n",
            "2021-07-25 08:02:56,873 - INFO - joeynmt.training - Epoch   6, Step:    90200, Batch Loss:     1.829728, Tokens per Sec:    16341, Lr: 0.000300\n",
            "2021-07-25 08:03:23,268 - INFO - joeynmt.training - Epoch   6, Step:    90400, Batch Loss:     1.809986, Tokens per Sec:    16197, Lr: 0.000300\n",
            "2021-07-25 08:03:49,323 - INFO - joeynmt.training - Epoch   6, Step:    90600, Batch Loss:     2.012995, Tokens per Sec:    16148, Lr: 0.000300\n",
            "2021-07-25 08:04:15,766 - INFO - joeynmt.training - Epoch   6, Step:    90800, Batch Loss:     2.026474, Tokens per Sec:    16452, Lr: 0.000300\n",
            "2021-07-25 08:04:42,160 - INFO - joeynmt.training - Epoch   6, Step:    91000, Batch Loss:     1.813699, Tokens per Sec:    16140, Lr: 0.000300\n",
            "2021-07-25 08:05:08,962 - INFO - joeynmt.training - Epoch   6, Step:    91200, Batch Loss:     1.054348, Tokens per Sec:    16088, Lr: 0.000300\n",
            "2021-07-25 08:05:35,536 - INFO - joeynmt.training - Epoch   6, Step:    91400, Batch Loss:     2.019993, Tokens per Sec:    16207, Lr: 0.000300\n",
            "2021-07-25 08:06:01,767 - INFO - joeynmt.training - Epoch   6, Step:    91600, Batch Loss:     1.430730, Tokens per Sec:    16285, Lr: 0.000300\n",
            "2021-07-25 08:06:28,179 - INFO - joeynmt.training - Epoch   6, Step:    91800, Batch Loss:     1.915616, Tokens per Sec:    16273, Lr: 0.000300\n",
            "2021-07-25 08:06:54,393 - INFO - joeynmt.training - Epoch   6, Step:    92000, Batch Loss:     1.967947, Tokens per Sec:    16279, Lr: 0.000300\n",
            "2021-07-25 08:07:20,837 - INFO - joeynmt.training - Epoch   6, Step:    92200, Batch Loss:     1.743539, Tokens per Sec:    15979, Lr: 0.000300\n",
            "2021-07-25 08:07:39,096 - INFO - joeynmt.training - Epoch   6: total training loss 5481.73\n",
            "2021-07-25 08:07:39,097 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-25 08:07:47,369 - INFO - joeynmt.training - Epoch   7, Step:    92400, Batch Loss:     1.981006, Tokens per Sec:    15559, Lr: 0.000300\n",
            "2021-07-25 08:09:05,635 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:09:05,635 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:09:05,636 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:09:06,262 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 08:09:06,262 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 08:09:07,290 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:09:07,291 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:09:07,291 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:09:07,291 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-25 08:09:07,291 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:09:07,292 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:09:07,292 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:09:07,292 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , whom I am loved . ”\n",
            "2021-07-25 08:09:07,292 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:09:07,292 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:09:07,292 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:09:07,293 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 08:09:07,293 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:09:07,293 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:09:07,293 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:09:07,293 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 08:09:07,293 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    92500: bleu:  17.46, loss: 130271.1016, ppl:   8.3443, duration: 66.8629s\n",
            "2021-07-25 08:09:20,842 - INFO - joeynmt.training - Epoch   7, Step:    92600, Batch Loss:     1.462470, Tokens per Sec:    16119, Lr: 0.000300\n",
            "2021-07-25 08:09:47,020 - INFO - joeynmt.training - Epoch   7, Step:    92800, Batch Loss:     1.761168, Tokens per Sec:    16262, Lr: 0.000300\n",
            "2021-07-25 08:10:13,310 - INFO - joeynmt.training - Epoch   7, Step:    93000, Batch Loss:     2.015376, Tokens per Sec:    16297, Lr: 0.000300\n",
            "2021-07-25 08:10:39,830 - INFO - joeynmt.training - Epoch   7, Step:    93200, Batch Loss:     1.926548, Tokens per Sec:    16008, Lr: 0.000300\n",
            "2021-07-25 08:11:06,359 - INFO - joeynmt.training - Epoch   7, Step:    93400, Batch Loss:     1.784886, Tokens per Sec:    16124, Lr: 0.000300\n",
            "2021-07-25 08:11:32,766 - INFO - joeynmt.training - Epoch   7, Step:    93600, Batch Loss:     2.044145, Tokens per Sec:    16408, Lr: 0.000300\n",
            "2021-07-25 08:11:59,104 - INFO - joeynmt.training - Epoch   7, Step:    93800, Batch Loss:     1.574395, Tokens per Sec:    16354, Lr: 0.000300\n",
            "2021-07-25 08:12:25,253 - INFO - joeynmt.training - Epoch   7, Step:    94000, Batch Loss:     1.900875, Tokens per Sec:    16101, Lr: 0.000300\n",
            "2021-07-25 08:12:51,647 - INFO - joeynmt.training - Epoch   7, Step:    94200, Batch Loss:     2.015982, Tokens per Sec:    16235, Lr: 0.000300\n",
            "2021-07-25 08:13:18,004 - INFO - joeynmt.training - Epoch   7, Step:    94400, Batch Loss:     1.985376, Tokens per Sec:    16213, Lr: 0.000300\n",
            "2021-07-25 08:13:44,319 - INFO - joeynmt.training - Epoch   7, Step:    94600, Batch Loss:     2.075169, Tokens per Sec:    16547, Lr: 0.000300\n",
            "2021-07-25 08:14:10,424 - INFO - joeynmt.training - Epoch   7, Step:    94800, Batch Loss:     1.555374, Tokens per Sec:    16157, Lr: 0.000300\n",
            "2021-07-25 08:14:36,965 - INFO - joeynmt.training - Epoch   7, Step:    95000, Batch Loss:     1.888291, Tokens per Sec:    16343, Lr: 0.000300\n",
            "2021-07-25 08:15:45,259 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:15:45,259 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:15:45,260 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:15:45,943 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 08:15:45,943 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 08:15:46,694 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:15:46,695 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:15:46,695 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:15:46,695 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 08:15:46,695 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:15:46,696 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:15:46,696 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:15:46,696 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 08:15:46,696 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:15:46,696 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:15:46,697 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:15:46,697 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 08:15:46,697 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:15:46,698 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:15:46,699 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:15:46,699 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 08:15:46,699 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    95000: bleu:  17.00, loss: 130243.0703, ppl:   8.3405, duration: 69.7336s\n",
            "2021-07-25 08:16:13,683 - INFO - joeynmt.training - Epoch   7, Step:    95200, Batch Loss:     1.990982, Tokens per Sec:    15928, Lr: 0.000300\n",
            "2021-07-25 08:16:17,039 - INFO - joeynmt.training - Epoch   7: total training loss 5473.54\n",
            "2021-07-25 08:16:17,040 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-25 08:16:40,027 - INFO - joeynmt.training - Epoch   8, Step:    95400, Batch Loss:     2.104489, Tokens per Sec:    15800, Lr: 0.000300\n",
            "2021-07-25 08:17:06,419 - INFO - joeynmt.training - Epoch   8, Step:    95600, Batch Loss:     1.615018, Tokens per Sec:    16290, Lr: 0.000300\n",
            "2021-07-25 08:17:32,799 - INFO - joeynmt.training - Epoch   8, Step:    95800, Batch Loss:     1.954648, Tokens per Sec:    16438, Lr: 0.000300\n",
            "2021-07-25 08:17:59,192 - INFO - joeynmt.training - Epoch   8, Step:    96000, Batch Loss:     1.857181, Tokens per Sec:    16364, Lr: 0.000300\n",
            "2021-07-25 08:18:25,663 - INFO - joeynmt.training - Epoch   8, Step:    96200, Batch Loss:     2.105503, Tokens per Sec:    16239, Lr: 0.000300\n",
            "2021-07-25 08:18:51,859 - INFO - joeynmt.training - Epoch   8, Step:    96400, Batch Loss:     1.888409, Tokens per Sec:    16330, Lr: 0.000300\n",
            "2021-07-25 08:19:18,122 - INFO - joeynmt.training - Epoch   8, Step:    96600, Batch Loss:     1.982138, Tokens per Sec:    16020, Lr: 0.000300\n",
            "2021-07-25 08:19:44,514 - INFO - joeynmt.training - Epoch   8, Step:    96800, Batch Loss:     1.333111, Tokens per Sec:    16108, Lr: 0.000300\n",
            "2021-07-25 08:20:10,908 - INFO - joeynmt.training - Epoch   8, Step:    97000, Batch Loss:     1.909199, Tokens per Sec:    16360, Lr: 0.000300\n",
            "2021-07-25 08:20:37,263 - INFO - joeynmt.training - Epoch   8, Step:    97200, Batch Loss:     1.990848, Tokens per Sec:    16196, Lr: 0.000300\n",
            "2021-07-25 08:21:03,730 - INFO - joeynmt.training - Epoch   8, Step:    97400, Batch Loss:     1.726471, Tokens per Sec:    16536, Lr: 0.000300\n",
            "2021-07-25 08:22:19,763 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:22:19,764 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:22:19,764 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:22:20,367 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 08:22:20,367 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 08:22:21,033 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:22:21,034 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:22:21,034 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:22:21,034 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 08:22:21,035 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:22:21,035 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:22:21,035 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:22:21,035 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 08:22:21,035 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:22:21,036 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:22:21,036 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:22:21,036 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-25 08:22:21,036 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:22:21,037 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:22:21,037 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:22:21,037 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 08:22:21,037 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    97500: bleu:  17.51, loss: 129610.9453, ppl:   8.2550, duration: 64.2864s\n",
            "2021-07-25 08:22:34,255 - INFO - joeynmt.training - Epoch   8, Step:    97600, Batch Loss:     1.969213, Tokens per Sec:    16187, Lr: 0.000300\n",
            "2021-07-25 08:23:00,563 - INFO - joeynmt.training - Epoch   8, Step:    97800, Batch Loss:     2.061930, Tokens per Sec:    16188, Lr: 0.000300\n",
            "2021-07-25 08:23:26,895 - INFO - joeynmt.training - Epoch   8, Step:    98000, Batch Loss:     1.988707, Tokens per Sec:    16122, Lr: 0.000300\n",
            "2021-07-25 08:23:41,836 - INFO - joeynmt.training - Epoch   8: total training loss 5456.85\n",
            "2021-07-25 08:23:41,836 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-25 08:23:53,246 - INFO - joeynmt.training - Epoch   9, Step:    98200, Batch Loss:     1.812322, Tokens per Sec:    15814, Lr: 0.000300\n",
            "2021-07-25 08:24:19,447 - INFO - joeynmt.training - Epoch   9, Step:    98400, Batch Loss:     2.089979, Tokens per Sec:    16082, Lr: 0.000300\n",
            "2021-07-25 08:24:45,807 - INFO - joeynmt.training - Epoch   9, Step:    98600, Batch Loss:     1.849135, Tokens per Sec:    16355, Lr: 0.000300\n",
            "2021-07-25 08:25:12,253 - INFO - joeynmt.training - Epoch   9, Step:    98800, Batch Loss:     1.948316, Tokens per Sec:    16447, Lr: 0.000300\n",
            "2021-07-25 08:25:38,660 - INFO - joeynmt.training - Epoch   9, Step:    99000, Batch Loss:     2.032316, Tokens per Sec:    16245, Lr: 0.000300\n",
            "2021-07-25 08:26:04,859 - INFO - joeynmt.training - Epoch   9, Step:    99200, Batch Loss:     1.851983, Tokens per Sec:    16346, Lr: 0.000300\n",
            "2021-07-25 08:26:30,990 - INFO - joeynmt.training - Epoch   9, Step:    99400, Batch Loss:     2.088154, Tokens per Sec:    15904, Lr: 0.000300\n",
            "2021-07-25 08:26:57,663 - INFO - joeynmt.training - Epoch   9, Step:    99600, Batch Loss:     1.845574, Tokens per Sec:    16275, Lr: 0.000300\n",
            "2021-07-25 08:27:24,096 - INFO - joeynmt.training - Epoch   9, Step:    99800, Batch Loss:     1.559925, Tokens per Sec:    16109, Lr: 0.000300\n",
            "2021-07-25 08:27:50,400 - INFO - joeynmt.training - Epoch   9, Step:   100000, Batch Loss:     2.037718, Tokens per Sec:    16042, Lr: 0.000300\n",
            "2021-07-25 08:28:57,001 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:28:57,002 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:28:57,002 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:28:58,427 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:28:58,428 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:28:58,428 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:28:58,428 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 08:28:58,428 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:28:58,429 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:28:58,429 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:28:58,430 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 08:28:58,430 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:28:58,431 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:28:58,431 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:28:58,432 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-25 08:28:58,432 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:28:58,432 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:28:58,433 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:28:58,433 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to do to save his family .\n",
            "2021-07-25 08:28:58,433 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   100000: bleu:  17.74, loss: 129796.6172, ppl:   8.2800, duration: 68.0324s\n",
            "2021-07-25 08:29:25,487 - INFO - joeynmt.training - Epoch   9, Step:   100200, Batch Loss:     1.429662, Tokens per Sec:    16001, Lr: 0.000300\n",
            "2021-07-25 08:29:51,919 - INFO - joeynmt.training - Epoch   9, Step:   100400, Batch Loss:     1.883961, Tokens per Sec:    16470, Lr: 0.000300\n",
            "2021-07-25 08:30:18,485 - INFO - joeynmt.training - Epoch   9, Step:   100600, Batch Loss:     2.112966, Tokens per Sec:    16250, Lr: 0.000300\n",
            "2021-07-25 08:30:45,089 - INFO - joeynmt.training - Epoch   9, Step:   100800, Batch Loss:     1.881939, Tokens per Sec:    16275, Lr: 0.000300\n",
            "2021-07-25 08:31:11,447 - INFO - joeynmt.training - Epoch   9, Step:   101000, Batch Loss:     1.654520, Tokens per Sec:    16077, Lr: 0.000300\n",
            "2021-07-25 08:31:11,461 - INFO - joeynmt.training - Epoch   9: total training loss 5430.68\n",
            "2021-07-25 08:31:11,461 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-25 08:31:38,468 - INFO - joeynmt.training - Epoch  10, Step:   101200, Batch Loss:     1.433179, Tokens per Sec:    16204, Lr: 0.000300\n",
            "2021-07-25 08:32:05,371 - INFO - joeynmt.training - Epoch  10, Step:   101400, Batch Loss:     2.066865, Tokens per Sec:    16283, Lr: 0.000300\n",
            "2021-07-25 08:32:31,411 - INFO - joeynmt.training - Epoch  10, Step:   101600, Batch Loss:     2.037160, Tokens per Sec:    16125, Lr: 0.000300\n",
            "2021-07-25 08:32:57,898 - INFO - joeynmt.training - Epoch  10, Step:   101800, Batch Loss:     1.967239, Tokens per Sec:    16343, Lr: 0.000300\n",
            "2021-07-25 08:33:24,360 - INFO - joeynmt.training - Epoch  10, Step:   102000, Batch Loss:     1.449423, Tokens per Sec:    16078, Lr: 0.000300\n",
            "2021-07-25 08:33:50,564 - INFO - joeynmt.training - Epoch  10, Step:   102200, Batch Loss:     1.858356, Tokens per Sec:    16477, Lr: 0.000300\n",
            "2021-07-25 08:34:16,876 - INFO - joeynmt.training - Epoch  10, Step:   102400, Batch Loss:     1.854949, Tokens per Sec:    16087, Lr: 0.000300\n",
            "2021-07-25 08:35:34,327 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:35:34,328 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:35:34,328 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:35:34,939 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 08:35:34,939 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 08:35:35,723 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:35:35,724 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:35:35,725 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:35:35,725 - INFO - joeynmt.training - \tHypothesis: If you do good , you will suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 08:35:35,725 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:35:35,725 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:35:35,726 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:35:35,726 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven saying : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 08:35:35,726 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:35:35,726 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:35:35,726 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:35:35,727 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayer ?\n",
            "2021-07-25 08:35:35,727 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:35:35,727 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:35:35,727 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:35:35,728 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 08:35:35,728 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   102500: bleu:  17.62, loss: 128857.9141, ppl:   8.1544, duration: 65.8247s\n",
            "2021-07-25 08:35:49,087 - INFO - joeynmt.training - Epoch  10, Step:   102600, Batch Loss:     1.771595, Tokens per Sec:    15963, Lr: 0.000300\n",
            "2021-07-25 08:36:15,738 - INFO - joeynmt.training - Epoch  10, Step:   102800, Batch Loss:     2.057064, Tokens per Sec:    15945, Lr: 0.000300\n",
            "2021-07-25 08:36:42,639 - INFO - joeynmt.training - Epoch  10, Step:   103000, Batch Loss:     1.863504, Tokens per Sec:    15981, Lr: 0.000300\n",
            "2021-07-25 08:37:09,186 - INFO - joeynmt.training - Epoch  10, Step:   103200, Batch Loss:     1.920487, Tokens per Sec:    16123, Lr: 0.000300\n",
            "2021-07-25 08:37:35,845 - INFO - joeynmt.training - Epoch  10, Step:   103400, Batch Loss:     1.981879, Tokens per Sec:    16046, Lr: 0.000300\n",
            "2021-07-25 08:38:02,187 - INFO - joeynmt.training - Epoch  10, Step:   103600, Batch Loss:     1.675862, Tokens per Sec:    16186, Lr: 0.000300\n",
            "2021-07-25 08:38:28,622 - INFO - joeynmt.training - Epoch  10, Step:   103800, Batch Loss:     2.216549, Tokens per Sec:    16353, Lr: 0.000300\n",
            "2021-07-25 08:38:39,839 - INFO - joeynmt.training - Epoch  10: total training loss 5416.93\n",
            "2021-07-25 08:38:39,839 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-25 08:38:55,195 - INFO - joeynmt.training - Epoch  11, Step:   104000, Batch Loss:     1.720355, Tokens per Sec:    15929, Lr: 0.000300\n",
            "2021-07-25 08:39:21,544 - INFO - joeynmt.training - Epoch  11, Step:   104200, Batch Loss:     2.256385, Tokens per Sec:    16207, Lr: 0.000300\n",
            "2021-07-25 08:39:47,897 - INFO - joeynmt.training - Epoch  11, Step:   104400, Batch Loss:     1.899694, Tokens per Sec:    16418, Lr: 0.000300\n",
            "2021-07-25 08:40:14,183 - INFO - joeynmt.training - Epoch  11, Step:   104600, Batch Loss:     1.943183, Tokens per Sec:    16068, Lr: 0.000300\n",
            "2021-07-25 08:40:40,724 - INFO - joeynmt.training - Epoch  11, Step:   104800, Batch Loss:     1.869296, Tokens per Sec:    16246, Lr: 0.000300\n",
            "2021-07-25 08:41:06,915 - INFO - joeynmt.training - Epoch  11, Step:   105000, Batch Loss:     1.923335, Tokens per Sec:    16400, Lr: 0.000300\n",
            "2021-07-25 08:42:13,887 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:42:13,888 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:42:13,888 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:42:15,271 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:42:15,272 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:42:15,272 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:42:15,272 - INFO - joeynmt.training - \tHypothesis: If you do good , you will suffer when you endure , what is acceptable to God . ”\n",
            "2021-07-25 08:42:15,272 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:42:15,273 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:42:15,273 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:42:15,273 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven saying : “ This is my beloved Son , whom I am grateful . ”\n",
            "2021-07-25 08:42:15,274 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:42:15,274 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:42:15,274 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:42:15,274 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-25 08:42:15,275 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:42:15,275 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:42:15,275 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:42:15,275 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 08:42:15,276 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   105000: bleu:  16.92, loss: 129928.9375, ppl:   8.2979, duration: 68.3599s\n",
            "2021-07-25 08:42:42,407 - INFO - joeynmt.training - Epoch  11, Step:   105200, Batch Loss:     1.395508, Tokens per Sec:    15885, Lr: 0.000300\n",
            "2021-07-25 08:43:08,740 - INFO - joeynmt.training - Epoch  11, Step:   105400, Batch Loss:     1.660827, Tokens per Sec:    16178, Lr: 0.000300\n",
            "2021-07-25 08:43:34,830 - INFO - joeynmt.training - Epoch  11, Step:   105600, Batch Loss:     1.995145, Tokens per Sec:    16262, Lr: 0.000300\n",
            "2021-07-25 08:44:01,017 - INFO - joeynmt.training - Epoch  11, Step:   105800, Batch Loss:     2.063204, Tokens per Sec:    16100, Lr: 0.000300\n",
            "2021-07-25 08:44:27,512 - INFO - joeynmt.training - Epoch  11, Step:   106000, Batch Loss:     1.948503, Tokens per Sec:    16595, Lr: 0.000300\n",
            "2021-07-25 08:44:53,526 - INFO - joeynmt.training - Epoch  11, Step:   106200, Batch Loss:     1.976991, Tokens per Sec:    16125, Lr: 0.000300\n",
            "2021-07-25 08:45:20,006 - INFO - joeynmt.training - Epoch  11, Step:   106400, Batch Loss:     1.939235, Tokens per Sec:    16226, Lr: 0.000300\n",
            "2021-07-25 08:45:46,292 - INFO - joeynmt.training - Epoch  11, Step:   106600, Batch Loss:     1.934935, Tokens per Sec:    16409, Lr: 0.000300\n",
            "2021-07-25 08:46:09,106 - INFO - joeynmt.training - Epoch  11: total training loss 5400.81\n",
            "2021-07-25 08:46:09,106 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-25 08:46:12,901 - INFO - joeynmt.training - Epoch  12, Step:   106800, Batch Loss:     1.919760, Tokens per Sec:    15061, Lr: 0.000300\n",
            "2021-07-25 08:46:38,949 - INFO - joeynmt.training - Epoch  12, Step:   107000, Batch Loss:     2.005176, Tokens per Sec:    16166, Lr: 0.000300\n",
            "2021-07-25 08:47:05,128 - INFO - joeynmt.training - Epoch  12, Step:   107200, Batch Loss:     1.962967, Tokens per Sec:    16542, Lr: 0.000300\n",
            "2021-07-25 08:47:31,648 - INFO - joeynmt.training - Epoch  12, Step:   107400, Batch Loss:     1.876811, Tokens per Sec:    16194, Lr: 0.000300\n",
            "2021-07-25 08:48:47,328 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:48:47,329 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:48:47,329 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:48:47,992 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 08:48:47,993 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 08:48:48,744 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:48:48,746 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:48:48,746 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:48:48,746 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 08:48:48,746 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:48:48,747 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:48:48,747 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:48:48,747 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I have approved . ”\n",
            "2021-07-25 08:48:48,747 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:48:48,748 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:48:48,748 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:48:48,748 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-25 08:48:48,748 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:48:48,749 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:48:48,749 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:48:48,749 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 08:48:48,749 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   107500: bleu:  17.88, loss: 128348.5859, ppl:   8.0871, duration: 63.9768s\n",
            "2021-07-25 08:49:02,237 - INFO - joeynmt.training - Epoch  12, Step:   107600, Batch Loss:     1.794726, Tokens per Sec:    16130, Lr: 0.000300\n",
            "2021-07-25 08:49:28,629 - INFO - joeynmt.training - Epoch  12, Step:   107800, Batch Loss:     1.947066, Tokens per Sec:    16328, Lr: 0.000300\n",
            "2021-07-25 08:49:54,806 - INFO - joeynmt.training - Epoch  12, Step:   108000, Batch Loss:     1.610254, Tokens per Sec:    16308, Lr: 0.000300\n",
            "2021-07-25 08:50:21,035 - INFO - joeynmt.training - Epoch  12, Step:   108200, Batch Loss:     2.057719, Tokens per Sec:    16151, Lr: 0.000300\n",
            "2021-07-25 08:50:47,334 - INFO - joeynmt.training - Epoch  12, Step:   108400, Batch Loss:     2.284182, Tokens per Sec:    16325, Lr: 0.000300\n",
            "2021-07-25 08:51:13,814 - INFO - joeynmt.training - Epoch  12, Step:   108600, Batch Loss:     1.648036, Tokens per Sec:    16202, Lr: 0.000300\n",
            "2021-07-25 08:51:40,199 - INFO - joeynmt.training - Epoch  12, Step:   108800, Batch Loss:     1.973083, Tokens per Sec:    16401, Lr: 0.000300\n",
            "2021-07-25 08:52:06,313 - INFO - joeynmt.training - Epoch  12, Step:   109000, Batch Loss:     2.193346, Tokens per Sec:    16198, Lr: 0.000300\n",
            "2021-07-25 08:52:32,839 - INFO - joeynmt.training - Epoch  12, Step:   109200, Batch Loss:     1.842894, Tokens per Sec:    16376, Lr: 0.000300\n",
            "2021-07-25 08:52:58,893 - INFO - joeynmt.training - Epoch  12, Step:   109400, Batch Loss:     1.818984, Tokens per Sec:    16271, Lr: 0.000300\n",
            "2021-07-25 08:53:25,206 - INFO - joeynmt.training - Epoch  12, Step:   109600, Batch Loss:     1.714667, Tokens per Sec:    16231, Lr: 0.000300\n",
            "2021-07-25 08:53:33,162 - INFO - joeynmt.training - Epoch  12: total training loss 5390.05\n",
            "2021-07-25 08:53:33,163 - INFO - joeynmt.training - EPOCH 13\n",
            "2021-07-25 08:53:52,010 - INFO - joeynmt.training - Epoch  13, Step:   109800, Batch Loss:     1.980442, Tokens per Sec:    15799, Lr: 0.000300\n",
            "2021-07-25 08:54:18,428 - INFO - joeynmt.training - Epoch  13, Step:   110000, Batch Loss:     1.864255, Tokens per Sec:    16355, Lr: 0.000300\n",
            "2021-07-25 08:55:19,988 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 08:55:19,988 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 08:55:19,988 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 08:55:21,248 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 08:55:21,249 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 08:55:21,249 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 08:55:21,249 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is acceptable to God . ”\n",
            "2021-07-25 08:55:21,250 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 08:55:21,250 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 08:55:21,250 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 08:55:21,250 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 08:55:21,250 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 08:55:21,251 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 08:55:21,251 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 08:55:21,251 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ prayers ?\n",
            "2021-07-25 08:55:21,251 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 08:55:21,252 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 08:55:21,252 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 08:55:21,252 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
            "2021-07-25 08:55:21,252 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   110000: bleu:  17.79, loss: 128648.7188, ppl:   8.1267, duration: 62.8234s\n",
            "2021-07-25 08:55:47,913 - INFO - joeynmt.training - Epoch  13, Step:   110200, Batch Loss:     1.796096, Tokens per Sec:    16028, Lr: 0.000300\n",
            "2021-07-25 08:56:14,337 - INFO - joeynmt.training - Epoch  13, Step:   110400, Batch Loss:     1.820615, Tokens per Sec:    16190, Lr: 0.000300\n",
            "2021-07-25 08:56:40,513 - INFO - joeynmt.training - Epoch  13, Step:   110600, Batch Loss:     1.844661, Tokens per Sec:    16353, Lr: 0.000300\n",
            "2021-07-25 08:57:06,675 - INFO - joeynmt.training - Epoch  13, Step:   110800, Batch Loss:     1.762389, Tokens per Sec:    16385, Lr: 0.000300\n",
            "2021-07-25 08:57:33,148 - INFO - joeynmt.training - Epoch  13, Step:   111000, Batch Loss:     1.835995, Tokens per Sec:    16341, Lr: 0.000300\n",
            "2021-07-25 08:57:59,537 - INFO - joeynmt.training - Epoch  13, Step:   111200, Batch Loss:     1.985680, Tokens per Sec:    16333, Lr: 0.000300\n",
            "2021-07-25 08:58:25,735 - INFO - joeynmt.training - Epoch  13, Step:   111400, Batch Loss:     1.917003, Tokens per Sec:    16023, Lr: 0.000300\n",
            "2021-07-25 08:58:51,935 - INFO - joeynmt.training - Epoch  13, Step:   111600, Batch Loss:     1.917585, Tokens per Sec:    16356, Lr: 0.000300\n",
            "2021-07-25 08:59:18,365 - INFO - joeynmt.training - Epoch  13, Step:   111800, Batch Loss:     1.418425, Tokens per Sec:    16408, Lr: 0.000300\n",
            "2021-07-25 08:59:44,596 - INFO - joeynmt.training - Epoch  13, Step:   112000, Batch Loss:     1.989367, Tokens per Sec:    16086, Lr: 0.000300\n",
            "2021-07-25 09:00:11,011 - INFO - joeynmt.training - Epoch  13, Step:   112200, Batch Loss:     1.754028, Tokens per Sec:    16275, Lr: 0.000300\n",
            "2021-07-25 09:00:37,059 - INFO - joeynmt.training - Epoch  13, Step:   112400, Batch Loss:     1.896638, Tokens per Sec:    16250, Lr: 0.000300\n",
            "2021-07-25 09:01:54,358 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:01:54,358 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:01:54,359 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:01:54,963 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 09:01:54,964 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 09:01:56,058 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:01:56,059 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:01:56,059 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:01:56,059 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 09:01:56,059 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:01:56,060 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:01:56,060 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:01:56,060 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I am approved . ”\n",
            "2021-07-25 09:01:56,060 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:01:56,060 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:01:56,061 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:01:56,061 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-25 09:01:56,061 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:01:56,061 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:01:56,061 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:01:56,062 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 09:01:56,062 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   112500: bleu:  17.65, loss: 127820.8672, ppl:   8.0179, duration: 66.0340s\n",
            "2021-07-25 09:02:02,731 - INFO - joeynmt.training - Epoch  13: total training loss 5380.77\n",
            "2021-07-25 09:02:02,731 - INFO - joeynmt.training - EPOCH 14\n",
            "2021-07-25 09:02:09,807 - INFO - joeynmt.training - Epoch  14, Step:   112600, Batch Loss:     1.933141, Tokens per Sec:    15170, Lr: 0.000300\n",
            "2021-07-25 09:02:36,041 - INFO - joeynmt.training - Epoch  14, Step:   112800, Batch Loss:     1.809585, Tokens per Sec:    15975, Lr: 0.000300\n",
            "2021-07-25 09:03:02,388 - INFO - joeynmt.training - Epoch  14, Step:   113000, Batch Loss:     1.926017, Tokens per Sec:    16411, Lr: 0.000300\n",
            "2021-07-25 09:03:28,911 - INFO - joeynmt.training - Epoch  14, Step:   113200, Batch Loss:     1.694583, Tokens per Sec:    15979, Lr: 0.000300\n",
            "2021-07-25 09:03:55,431 - INFO - joeynmt.training - Epoch  14, Step:   113400, Batch Loss:     2.158957, Tokens per Sec:    16339, Lr: 0.000300\n",
            "2021-07-25 09:04:21,759 - INFO - joeynmt.training - Epoch  14, Step:   113600, Batch Loss:     1.641001, Tokens per Sec:    16282, Lr: 0.000300\n",
            "2021-07-25 09:04:48,306 - INFO - joeynmt.training - Epoch  14, Step:   113800, Batch Loss:     1.911677, Tokens per Sec:    16625, Lr: 0.000300\n",
            "2021-07-25 09:05:14,442 - INFO - joeynmt.training - Epoch  14, Step:   114000, Batch Loss:     1.464596, Tokens per Sec:    16126, Lr: 0.000300\n",
            "2021-07-25 09:05:40,777 - INFO - joeynmt.training - Epoch  14, Step:   114200, Batch Loss:     2.084225, Tokens per Sec:    16644, Lr: 0.000300\n",
            "2021-07-25 09:06:06,833 - INFO - joeynmt.training - Epoch  14, Step:   114400, Batch Loss:     1.775280, Tokens per Sec:    16001, Lr: 0.000300\n",
            "2021-07-25 09:06:33,141 - INFO - joeynmt.training - Epoch  14, Step:   114600, Batch Loss:     1.797305, Tokens per Sec:    16121, Lr: 0.000300\n",
            "2021-07-25 09:06:59,261 - INFO - joeynmt.training - Epoch  14, Step:   114800, Batch Loss:     1.722999, Tokens per Sec:    16107, Lr: 0.000300\n",
            "2021-07-25 09:07:25,693 - INFO - joeynmt.training - Epoch  14, Step:   115000, Batch Loss:     1.879547, Tokens per Sec:    16384, Lr: 0.000300\n",
            "2021-07-25 09:08:29,242 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:08:29,243 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:08:29,243 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:08:29,871 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 09:08:29,871 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 09:08:30,566 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:08:30,568 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:08:30,568 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:08:30,568 - INFO - joeynmt.training - \tHypothesis: “ When you do good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 09:08:30,568 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:08:30,569 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:08:30,569 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:08:30,569 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 09:08:30,569 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:08:30,570 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:08:30,570 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:08:30,570 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 09:08:30,571 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:08:30,571 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:08:30,571 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:08:30,571 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 09:08:30,571 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   115000: bleu:  18.13, loss: 127402.3672, ppl:   7.9634, duration: 64.8780s\n",
            "2021-07-25 09:08:56,961 - INFO - joeynmt.training - Epoch  14, Step:   115200, Batch Loss:     1.952864, Tokens per Sec:    15803, Lr: 0.000300\n",
            "2021-07-25 09:09:23,868 - INFO - joeynmt.training - Epoch  14, Step:   115400, Batch Loss:     1.839863, Tokens per Sec:    16119, Lr: 0.000300\n",
            "2021-07-25 09:09:29,545 - INFO - joeynmt.training - Epoch  14: total training loss 5371.64\n",
            "2021-07-25 09:09:29,546 - INFO - joeynmt.training - EPOCH 15\n",
            "2021-07-25 09:09:50,472 - INFO - joeynmt.training - Epoch  15, Step:   115600, Batch Loss:     1.683027, Tokens per Sec:    15704, Lr: 0.000300\n",
            "2021-07-25 09:10:16,888 - INFO - joeynmt.training - Epoch  15, Step:   115800, Batch Loss:     1.835563, Tokens per Sec:    16462, Lr: 0.000300\n",
            "2021-07-25 09:10:43,421 - INFO - joeynmt.training - Epoch  15, Step:   116000, Batch Loss:     1.835834, Tokens per Sec:    16092, Lr: 0.000300\n",
            "2021-07-25 09:11:10,166 - INFO - joeynmt.training - Epoch  15, Step:   116200, Batch Loss:     2.250904, Tokens per Sec:    16018, Lr: 0.000300\n",
            "2021-07-25 09:11:36,548 - INFO - joeynmt.training - Epoch  15, Step:   116400, Batch Loss:     1.788851, Tokens per Sec:    16532, Lr: 0.000300\n",
            "2021-07-25 09:12:02,741 - INFO - joeynmt.training - Epoch  15, Step:   116600, Batch Loss:     1.969051, Tokens per Sec:    16292, Lr: 0.000300\n",
            "2021-07-25 09:12:29,114 - INFO - joeynmt.training - Epoch  15, Step:   116800, Batch Loss:     1.881706, Tokens per Sec:    16211, Lr: 0.000300\n",
            "2021-07-25 09:12:55,241 - INFO - joeynmt.training - Epoch  15, Step:   117000, Batch Loss:     1.265499, Tokens per Sec:    16365, Lr: 0.000300\n",
            "2021-07-25 09:13:21,568 - INFO - joeynmt.training - Epoch  15, Step:   117200, Batch Loss:     1.763478, Tokens per Sec:    16269, Lr: 0.000300\n",
            "2021-07-25 09:13:47,762 - INFO - joeynmt.training - Epoch  15, Step:   117400, Batch Loss:     1.958412, Tokens per Sec:    16202, Lr: 0.000300\n",
            "2021-07-25 09:15:05,907 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:15:05,908 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:15:05,908 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:15:06,525 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 09:15:06,525 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 09:15:07,231 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:15:07,232 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:15:07,233 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:15:07,233 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , that is what is approved to God . ”\n",
            "2021-07-25 09:15:07,234 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:15:07,235 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:15:07,235 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:15:07,235 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 09:15:07,236 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:15:07,236 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:15:07,236 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:15:07,236 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 09:15:07,237 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:15:07,237 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:15:07,237 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:15:07,237 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
            "2021-07-25 09:15:07,238 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   117500: bleu:  17.78, loss: 127369.1250, ppl:   7.9591, duration: 66.2630s\n",
            "2021-07-25 09:15:20,853 - INFO - joeynmt.training - Epoch  15, Step:   117600, Batch Loss:     1.601251, Tokens per Sec:    16249, Lr: 0.000300\n",
            "2021-07-25 09:15:47,409 - INFO - joeynmt.training - Epoch  15, Step:   117800, Batch Loss:     1.821565, Tokens per Sec:    16270, Lr: 0.000300\n",
            "2021-07-25 09:16:13,789 - INFO - joeynmt.training - Epoch  15, Step:   118000, Batch Loss:     1.806474, Tokens per Sec:    16050, Lr: 0.000300\n",
            "2021-07-25 09:16:40,192 - INFO - joeynmt.training - Epoch  15, Step:   118200, Batch Loss:     1.955940, Tokens per Sec:    16397, Lr: 0.000300\n",
            "2021-07-25 09:16:56,496 - INFO - joeynmt.training - Epoch  15: total training loss 5331.56\n",
            "2021-07-25 09:16:56,497 - INFO - joeynmt.training - EPOCH 16\n",
            "2021-07-25 09:17:07,051 - INFO - joeynmt.training - Epoch  16, Step:   118400, Batch Loss:     1.809401, Tokens per Sec:    15881, Lr: 0.000300\n",
            "2021-07-25 09:17:33,313 - INFO - joeynmt.training - Epoch  16, Step:   118600, Batch Loss:     1.971984, Tokens per Sec:    16332, Lr: 0.000300\n",
            "2021-07-25 09:17:59,722 - INFO - joeynmt.training - Epoch  16, Step:   118800, Batch Loss:     1.953009, Tokens per Sec:    16155, Lr: 0.000300\n",
            "2021-07-25 09:18:26,087 - INFO - joeynmt.training - Epoch  16, Step:   119000, Batch Loss:     1.831474, Tokens per Sec:    16023, Lr: 0.000300\n",
            "2021-07-25 09:18:52,649 - INFO - joeynmt.training - Epoch  16, Step:   119200, Batch Loss:     1.757609, Tokens per Sec:    16576, Lr: 0.000300\n",
            "2021-07-25 09:19:18,808 - INFO - joeynmt.training - Epoch  16, Step:   119400, Batch Loss:     1.519196, Tokens per Sec:    15661, Lr: 0.000300\n",
            "2021-07-25 09:19:45,047 - INFO - joeynmt.training - Epoch  16, Step:   119600, Batch Loss:     1.974456, Tokens per Sec:    16187, Lr: 0.000300\n",
            "2021-07-25 09:20:11,213 - INFO - joeynmt.training - Epoch  16, Step:   119800, Batch Loss:     1.894360, Tokens per Sec:    16183, Lr: 0.000300\n",
            "2021-07-25 09:20:37,882 - INFO - joeynmt.training - Epoch  16, Step:   120000, Batch Loss:     2.073003, Tokens per Sec:    16272, Lr: 0.000300\n",
            "2021-07-25 09:21:42,742 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:21:42,742 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:21:42,742 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:21:44,084 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:21:44,084 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:21:44,085 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:21:44,085 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 09:21:44,085 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:21:44,088 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:21:44,089 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:21:44,089 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , whom I have approved . ”\n",
            "2021-07-25 09:21:44,089 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:21:44,089 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:21:44,090 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:21:44,090 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayer ?\n",
            "2021-07-25 09:21:44,090 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:21:44,090 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:21:44,091 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:21:44,091 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 09:21:44,091 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   120000: bleu:  18.00, loss: 127494.6719, ppl:   7.9754, duration: 66.2090s\n",
            "2021-07-25 09:22:11,136 - INFO - joeynmt.training - Epoch  16, Step:   120200, Batch Loss:     1.671220, Tokens per Sec:    16044, Lr: 0.000300\n",
            "2021-07-25 09:22:37,475 - INFO - joeynmt.training - Epoch  16, Step:   120400, Batch Loss:     1.798890, Tokens per Sec:    16146, Lr: 0.000300\n",
            "2021-07-25 09:23:04,207 - INFO - joeynmt.training - Epoch  16, Step:   120600, Batch Loss:     1.948081, Tokens per Sec:    16360, Lr: 0.000300\n",
            "2021-07-25 09:23:30,922 - INFO - joeynmt.training - Epoch  16, Step:   120800, Batch Loss:     1.731644, Tokens per Sec:    15850, Lr: 0.000300\n",
            "2021-07-25 09:23:57,510 - INFO - joeynmt.training - Epoch  16, Step:   121000, Batch Loss:     1.885458, Tokens per Sec:    16182, Lr: 0.000300\n",
            "2021-07-25 09:24:24,406 - INFO - joeynmt.training - Epoch  16, Step:   121200, Batch Loss:     1.841260, Tokens per Sec:    16044, Lr: 0.000300\n",
            "2021-07-25 09:24:25,601 - INFO - joeynmt.training - Epoch  16: total training loss 5328.69\n",
            "2021-07-25 09:24:25,601 - INFO - joeynmt.training - EPOCH 17\n",
            "2021-07-25 09:24:50,998 - INFO - joeynmt.training - Epoch  17, Step:   121400, Batch Loss:     1.707529, Tokens per Sec:    16091, Lr: 0.000300\n",
            "2021-07-25 09:25:17,460 - INFO - joeynmt.training - Epoch  17, Step:   121600, Batch Loss:     2.026881, Tokens per Sec:    16055, Lr: 0.000300\n",
            "2021-07-25 09:25:44,090 - INFO - joeynmt.training - Epoch  17, Step:   121800, Batch Loss:     1.843445, Tokens per Sec:    16078, Lr: 0.000300\n",
            "2021-07-25 09:26:10,294 - INFO - joeynmt.training - Epoch  17, Step:   122000, Batch Loss:     1.968642, Tokens per Sec:    16181, Lr: 0.000300\n",
            "2021-07-25 09:26:36,881 - INFO - joeynmt.training - Epoch  17, Step:   122200, Batch Loss:     1.813546, Tokens per Sec:    16170, Lr: 0.000300\n",
            "2021-07-25 09:27:03,424 - INFO - joeynmt.training - Epoch  17, Step:   122400, Batch Loss:     2.024344, Tokens per Sec:    16153, Lr: 0.000300\n",
            "2021-07-25 09:28:21,482 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:28:21,483 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:28:21,483 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:28:22,119 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 09:28:22,119 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 09:28:22,857 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:28:22,858 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:28:22,858 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:28:22,858 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 09:28:22,858 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:28:22,859 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:28:22,859 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:28:22,859 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I am approved . ”\n",
            "2021-07-25 09:28:22,859 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:28:22,860 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:28:22,860 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:28:22,860 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 09:28:22,860 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:28:22,861 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:28:22,861 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:28:22,861 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 09:28:22,861 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   122500: bleu:  18.16, loss: 126936.6953, ppl:   7.9032, duration: 66.1880s\n",
            "2021-07-25 09:28:36,127 - INFO - joeynmt.training - Epoch  17, Step:   122600, Batch Loss:     1.875098, Tokens per Sec:    16191, Lr: 0.000300\n",
            "2021-07-25 09:29:02,654 - INFO - joeynmt.training - Epoch  17, Step:   122800, Batch Loss:     2.346616, Tokens per Sec:    16221, Lr: 0.000300\n",
            "2021-07-25 09:29:29,223 - INFO - joeynmt.training - Epoch  17, Step:   123000, Batch Loss:     1.963510, Tokens per Sec:    16107, Lr: 0.000300\n",
            "2021-07-25 09:29:55,458 - INFO - joeynmt.training - Epoch  17, Step:   123200, Batch Loss:     1.733216, Tokens per Sec:    16118, Lr: 0.000300\n",
            "2021-07-25 09:30:21,996 - INFO - joeynmt.training - Epoch  17, Step:   123400, Batch Loss:     1.998625, Tokens per Sec:    16257, Lr: 0.000300\n",
            "2021-07-25 09:30:48,481 - INFO - joeynmt.training - Epoch  17, Step:   123600, Batch Loss:     1.940512, Tokens per Sec:    16231, Lr: 0.000300\n",
            "2021-07-25 09:31:14,999 - INFO - joeynmt.training - Epoch  17, Step:   123800, Batch Loss:     1.902115, Tokens per Sec:    16101, Lr: 0.000300\n",
            "2021-07-25 09:31:41,526 - INFO - joeynmt.training - Epoch  17, Step:   124000, Batch Loss:     1.792546, Tokens per Sec:    16229, Lr: 0.000300\n",
            "2021-07-25 09:31:54,188 - INFO - joeynmt.training - Epoch  17: total training loss 5316.85\n",
            "2021-07-25 09:31:54,189 - INFO - joeynmt.training - EPOCH 18\n",
            "2021-07-25 09:32:08,664 - INFO - joeynmt.training - Epoch  18, Step:   124200, Batch Loss:     1.721871, Tokens per Sec:    15814, Lr: 0.000300\n",
            "2021-07-25 09:32:35,207 - INFO - joeynmt.training - Epoch  18, Step:   124400, Batch Loss:     1.709259, Tokens per Sec:    16419, Lr: 0.000300\n",
            "2021-07-25 09:33:01,670 - INFO - joeynmt.training - Epoch  18, Step:   124600, Batch Loss:     1.910643, Tokens per Sec:    16199, Lr: 0.000300\n",
            "2021-07-25 09:33:27,968 - INFO - joeynmt.training - Epoch  18, Step:   124800, Batch Loss:     1.850824, Tokens per Sec:    16084, Lr: 0.000300\n",
            "2021-07-25 09:33:54,393 - INFO - joeynmt.training - Epoch  18, Step:   125000, Batch Loss:     1.873699, Tokens per Sec:    16205, Lr: 0.000300\n",
            "2021-07-25 09:34:57,772 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:34:57,772 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:34:57,772 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:34:58,395 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 09:34:58,395 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 09:34:59,117 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:34:59,118 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:34:59,119 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:34:59,119 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is acceptable to God . ”\n",
            "2021-07-25 09:34:59,119 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:34:59,120 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:34:59,120 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:34:59,120 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son whom I have approved . ”\n",
            "2021-07-25 09:34:59,120 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:34:59,121 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:34:59,121 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:34:59,121 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ prayer ?\n",
            "2021-07-25 09:34:59,121 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:34:59,121 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:34:59,122 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:34:59,122 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 09:34:59,122 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   125000: bleu:  18.17, loss: 126641.1719, ppl:   7.8653, duration: 64.7288s\n",
            "2021-07-25 09:35:25,760 - INFO - joeynmt.training - Epoch  18, Step:   125200, Batch Loss:     1.910403, Tokens per Sec:    15836, Lr: 0.000300\n",
            "2021-07-25 09:35:52,134 - INFO - joeynmt.training - Epoch  18, Step:   125400, Batch Loss:     1.670843, Tokens per Sec:    16356, Lr: 0.000300\n",
            "2021-07-25 09:36:18,479 - INFO - joeynmt.training - Epoch  18, Step:   125600, Batch Loss:     1.909249, Tokens per Sec:    16133, Lr: 0.000300\n",
            "2021-07-25 09:36:44,754 - INFO - joeynmt.training - Epoch  18, Step:   125800, Batch Loss:     1.931303, Tokens per Sec:    16147, Lr: 0.000300\n",
            "2021-07-25 09:37:11,142 - INFO - joeynmt.training - Epoch  18, Step:   126000, Batch Loss:     1.798415, Tokens per Sec:    16400, Lr: 0.000300\n",
            "2021-07-25 09:37:37,687 - INFO - joeynmt.training - Epoch  18, Step:   126200, Batch Loss:     1.440847, Tokens per Sec:    15958, Lr: 0.000300\n",
            "2021-07-25 09:38:04,239 - INFO - joeynmt.training - Epoch  18, Step:   126400, Batch Loss:     1.879335, Tokens per Sec:    16047, Lr: 0.000300\n",
            "2021-07-25 09:38:30,880 - INFO - joeynmt.training - Epoch  18, Step:   126600, Batch Loss:     1.957710, Tokens per Sec:    16471, Lr: 0.000300\n",
            "2021-07-25 09:38:57,218 - INFO - joeynmt.training - Epoch  18, Step:   126800, Batch Loss:     1.901411, Tokens per Sec:    16183, Lr: 0.000300\n",
            "2021-07-25 09:39:21,237 - INFO - joeynmt.training - Epoch  18: total training loss 5299.08\n",
            "2021-07-25 09:39:21,237 - INFO - joeynmt.training - EPOCH 19\n",
            "2021-07-25 09:39:24,205 - INFO - joeynmt.training - Epoch  19, Step:   127000, Batch Loss:     1.886474, Tokens per Sec:    14861, Lr: 0.000300\n",
            "2021-07-25 09:39:50,413 - INFO - joeynmt.training - Epoch  19, Step:   127200, Batch Loss:     1.890863, Tokens per Sec:    16153, Lr: 0.000300\n",
            "2021-07-25 09:40:16,838 - INFO - joeynmt.training - Epoch  19, Step:   127400, Batch Loss:     2.065299, Tokens per Sec:    16144, Lr: 0.000300\n",
            "2021-07-25 09:41:31,410 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:41:31,410 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:41:31,410 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:41:32,685 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:41:32,686 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:41:32,686 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:41:32,686 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 09:41:32,686 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:41:32,687 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:41:32,687 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:41:32,687 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , whom I am approved . ”\n",
            "2021-07-25 09:41:32,687 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:41:32,688 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:41:32,688 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:41:32,688 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 09:41:32,688 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:41:32,689 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:41:32,689 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:41:32,689 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 09:41:32,689 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   127500: bleu:  18.12, loss: 126855.0391, ppl:   7.8927, duration: 62.5894s\n",
            "2021-07-25 09:41:46,027 - INFO - joeynmt.training - Epoch  19, Step:   127600, Batch Loss:     1.729486, Tokens per Sec:    16004, Lr: 0.000300\n",
            "2021-07-25 09:42:12,345 - INFO - joeynmt.training - Epoch  19, Step:   127800, Batch Loss:     1.902093, Tokens per Sec:    16194, Lr: 0.000300\n",
            "2021-07-25 09:42:38,924 - INFO - joeynmt.training - Epoch  19, Step:   128000, Batch Loss:     1.479226, Tokens per Sec:    16038, Lr: 0.000300\n",
            "2021-07-25 09:43:05,748 - INFO - joeynmt.training - Epoch  19, Step:   128200, Batch Loss:     1.795903, Tokens per Sec:    16299, Lr: 0.000300\n",
            "2021-07-25 09:43:32,110 - INFO - joeynmt.training - Epoch  19, Step:   128400, Batch Loss:     1.851240, Tokens per Sec:    16425, Lr: 0.000300\n",
            "2021-07-25 09:43:58,362 - INFO - joeynmt.training - Epoch  19, Step:   128600, Batch Loss:     1.901347, Tokens per Sec:    16182, Lr: 0.000300\n",
            "2021-07-25 09:44:24,845 - INFO - joeynmt.training - Epoch  19, Step:   128800, Batch Loss:     2.065041, Tokens per Sec:    16166, Lr: 0.000300\n",
            "2021-07-25 09:44:51,366 - INFO - joeynmt.training - Epoch  19, Step:   129000, Batch Loss:     1.814999, Tokens per Sec:    16283, Lr: 0.000300\n",
            "2021-07-25 09:45:17,994 - INFO - joeynmt.training - Epoch  19, Step:   129200, Batch Loss:     1.882584, Tokens per Sec:    15989, Lr: 0.000300\n",
            "2021-07-25 09:45:44,545 - INFO - joeynmt.training - Epoch  19, Step:   129400, Batch Loss:     1.986441, Tokens per Sec:    16030, Lr: 0.000300\n",
            "2021-07-25 09:46:11,398 - INFO - joeynmt.training - Epoch  19, Step:   129600, Batch Loss:     1.899171, Tokens per Sec:    16046, Lr: 0.000300\n",
            "2021-07-25 09:46:38,244 - INFO - joeynmt.training - Epoch  19, Step:   129800, Batch Loss:     1.465287, Tokens per Sec:    16177, Lr: 0.000300\n",
            "2021-07-25 09:46:46,810 - INFO - joeynmt.training - Epoch  19: total training loss 5294.57\n",
            "2021-07-25 09:46:46,810 - INFO - joeynmt.training - EPOCH 20\n",
            "2021-07-25 09:47:04,824 - INFO - joeynmt.training - Epoch  20, Step:   130000, Batch Loss:     1.842516, Tokens per Sec:    16021, Lr: 0.000300\n",
            "2021-07-25 09:48:11,913 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:48:11,914 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:48:11,914 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:48:13,307 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:48:13,307 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:48:13,308 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:48:13,308 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 09:48:13,308 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:48:13,308 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:48:13,309 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:48:13,309 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 09:48:13,309 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:48:13,309 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:48:13,309 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:48:13,310 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 09:48:13,310 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:48:13,310 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:48:13,310 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:48:13,311 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 09:48:13,311 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   130000: bleu:  17.84, loss: 127121.6094, ppl:   7.9271, duration: 68.4867s\n",
            "2021-07-25 09:48:40,260 - INFO - joeynmt.training - Epoch  20, Step:   130200, Batch Loss:     1.795171, Tokens per Sec:    16106, Lr: 0.000300\n",
            "2021-07-25 09:49:06,748 - INFO - joeynmt.training - Epoch  20, Step:   130400, Batch Loss:     1.798635, Tokens per Sec:    16022, Lr: 0.000300\n",
            "2021-07-25 09:49:32,780 - INFO - joeynmt.training - Epoch  20, Step:   130600, Batch Loss:     1.772271, Tokens per Sec:    16239, Lr: 0.000300\n",
            "2021-07-25 09:49:59,155 - INFO - joeynmt.training - Epoch  20, Step:   130800, Batch Loss:     1.748285, Tokens per Sec:    16155, Lr: 0.000300\n",
            "2021-07-25 09:50:25,529 - INFO - joeynmt.training - Epoch  20, Step:   131000, Batch Loss:     1.873950, Tokens per Sec:    16253, Lr: 0.000300\n",
            "2021-07-25 09:50:51,578 - INFO - joeynmt.training - Epoch  20, Step:   131200, Batch Loss:     1.738420, Tokens per Sec:    16353, Lr: 0.000300\n",
            "2021-07-25 09:51:18,056 - INFO - joeynmt.training - Epoch  20, Step:   131400, Batch Loss:     1.588508, Tokens per Sec:    16081, Lr: 0.000300\n",
            "2021-07-25 09:51:44,366 - INFO - joeynmt.training - Epoch  20, Step:   131600, Batch Loss:     1.919115, Tokens per Sec:    16468, Lr: 0.000300\n",
            "2021-07-25 09:52:10,594 - INFO - joeynmt.training - Epoch  20, Step:   131800, Batch Loss:     1.893097, Tokens per Sec:    16364, Lr: 0.000300\n",
            "2021-07-25 09:52:36,925 - INFO - joeynmt.training - Epoch  20, Step:   132000, Batch Loss:     1.968800, Tokens per Sec:    16335, Lr: 0.000300\n",
            "2021-07-25 09:53:03,051 - INFO - joeynmt.training - Epoch  20, Step:   132200, Batch Loss:     1.756312, Tokens per Sec:    16271, Lr: 0.000300\n",
            "2021-07-25 09:53:29,567 - INFO - joeynmt.training - Epoch  20, Step:   132400, Batch Loss:     1.613979, Tokens per Sec:    16361, Lr: 0.000300\n",
            "2021-07-25 09:54:47,370 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 09:54:47,371 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 09:54:47,371 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 09:54:47,973 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 09:54:47,973 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 09:54:48,701 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 09:54:48,702 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 09:54:48,702 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 09:54:48,702 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 09:54:48,703 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 09:54:48,703 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 09:54:48,703 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 09:54:48,703 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 09:54:48,703 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 09:54:48,704 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 09:54:48,704 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 09:54:48,704 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 09:54:48,705 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 09:54:48,705 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 09:54:48,705 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 09:54:48,705 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
            "2021-07-25 09:54:48,706 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   132500: bleu:  18.19, loss: 126321.8672, ppl:   7.8245, duration: 66.1101s\n",
            "2021-07-25 09:55:02,070 - INFO - joeynmt.training - Epoch  20, Step:   132600, Batch Loss:     1.770517, Tokens per Sec:    15973, Lr: 0.000300\n",
            "2021-07-25 09:55:22,259 - INFO - joeynmt.training - Epoch  20: total training loss 5283.36\n",
            "2021-07-25 09:55:22,260 - INFO - joeynmt.training - EPOCH 21\n",
            "2021-07-25 09:55:28,649 - INFO - joeynmt.training - Epoch  21, Step:   132800, Batch Loss:     1.715463, Tokens per Sec:    15570, Lr: 0.000300\n",
            "2021-07-25 09:55:54,888 - INFO - joeynmt.training - Epoch  21, Step:   133000, Batch Loss:     1.482329, Tokens per Sec:    16271, Lr: 0.000300\n",
            "2021-07-25 09:56:21,331 - INFO - joeynmt.training - Epoch  21, Step:   133200, Batch Loss:     2.101025, Tokens per Sec:    16320, Lr: 0.000300\n",
            "2021-07-25 09:56:47,579 - INFO - joeynmt.training - Epoch  21, Step:   133400, Batch Loss:     1.584865, Tokens per Sec:    16257, Lr: 0.000300\n",
            "2021-07-25 09:57:13,892 - INFO - joeynmt.training - Epoch  21, Step:   133600, Batch Loss:     1.845647, Tokens per Sec:    16187, Lr: 0.000300\n",
            "2021-07-25 09:57:40,314 - INFO - joeynmt.training - Epoch  21, Step:   133800, Batch Loss:     1.942966, Tokens per Sec:    16404, Lr: 0.000300\n",
            "2021-07-25 09:58:06,474 - INFO - joeynmt.training - Epoch  21, Step:   134000, Batch Loss:     1.651947, Tokens per Sec:    16311, Lr: 0.000300\n",
            "2021-07-25 09:58:33,089 - INFO - joeynmt.training - Epoch  21, Step:   134200, Batch Loss:     1.913178, Tokens per Sec:    16204, Lr: 0.000300\n",
            "2021-07-25 09:58:59,216 - INFO - joeynmt.training - Epoch  21, Step:   134400, Batch Loss:     1.811029, Tokens per Sec:    16173, Lr: 0.000300\n",
            "2021-07-25 09:59:25,643 - INFO - joeynmt.training - Epoch  21, Step:   134600, Batch Loss:     1.596295, Tokens per Sec:    16186, Lr: 0.000300\n",
            "2021-07-25 09:59:52,114 - INFO - joeynmt.training - Epoch  21, Step:   134800, Batch Loss:     1.990122, Tokens per Sec:    16215, Lr: 0.000300\n",
            "2021-07-25 10:00:18,319 - INFO - joeynmt.training - Epoch  21, Step:   135000, Batch Loss:     1.761478, Tokens per Sec:    16170, Lr: 0.000300\n",
            "2021-07-25 10:01:22,979 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:01:22,979 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:01:22,980 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:01:23,592 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 10:01:23,592 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 10:01:24,326 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:01:24,327 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:01:24,327 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:01:24,327 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 10:01:24,327 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:01:24,328 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:01:24,328 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:01:24,328 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , whom I am approved . ”\n",
            "2021-07-25 10:01:24,328 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:01:24,329 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:01:24,329 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:01:24,329 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-25 10:01:24,329 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:01:24,330 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:01:24,330 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:01:24,330 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
            "2021-07-25 10:01:24,330 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step   135000: bleu:  18.09, loss: 126280.7969, ppl:   7.8193, duration: 66.0108s\n",
            "2021-07-25 10:01:50,549 - INFO - joeynmt.training - Epoch  21, Step:   135200, Batch Loss:     1.767909, Tokens per Sec:    16185, Lr: 0.000300\n",
            "2021-07-25 10:02:17,022 - INFO - joeynmt.training - Epoch  21, Step:   135400, Batch Loss:     1.894096, Tokens per Sec:    16307, Lr: 0.000300\n",
            "2021-07-25 10:02:43,294 - INFO - joeynmt.training - Epoch  21, Step:   135600, Batch Loss:     1.293566, Tokens per Sec:    16393, Lr: 0.000300\n",
            "2021-07-25 10:02:48,751 - INFO - joeynmt.training - Epoch  21: total training loss 5273.44\n",
            "2021-07-25 10:02:48,752 - INFO - joeynmt.training - EPOCH 22\n",
            "2021-07-25 10:03:09,652 - INFO - joeynmt.training - Epoch  22, Step:   135800, Batch Loss:     1.742491, Tokens per Sec:    15801, Lr: 0.000300\n",
            "2021-07-25 10:03:36,015 - INFO - joeynmt.training - Epoch  22, Step:   136000, Batch Loss:     1.823626, Tokens per Sec:    16156, Lr: 0.000300\n",
            "2021-07-25 10:04:02,220 - INFO - joeynmt.training - Epoch  22, Step:   136200, Batch Loss:     1.658058, Tokens per Sec:    16320, Lr: 0.000300\n",
            "2021-07-25 10:04:28,654 - INFO - joeynmt.training - Epoch  22, Step:   136400, Batch Loss:     1.883511, Tokens per Sec:    16267, Lr: 0.000300\n",
            "2021-07-25 10:04:55,453 - INFO - joeynmt.training - Epoch  22, Step:   136600, Batch Loss:     2.090934, Tokens per Sec:    16332, Lr: 0.000300\n",
            "2021-07-25 10:05:21,825 - INFO - joeynmt.training - Epoch  22, Step:   136800, Batch Loss:     1.954671, Tokens per Sec:    16176, Lr: 0.000300\n",
            "2021-07-25 10:05:48,092 - INFO - joeynmt.training - Epoch  22, Step:   137000, Batch Loss:     1.950885, Tokens per Sec:    16463, Lr: 0.000300\n",
            "2021-07-25 10:06:14,751 - INFO - joeynmt.training - Epoch  22, Step:   137200, Batch Loss:     1.327721, Tokens per Sec:    16317, Lr: 0.000300\n",
            "2021-07-25 10:06:40,908 - INFO - joeynmt.training - Epoch  22, Step:   137400, Batch Loss:     2.002554, Tokens per Sec:    16331, Lr: 0.000300\n",
            "2021-07-25 10:08:06,331 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:08:06,331 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:08:06,332 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:08:08,110 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:08:08,111 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:08:08,111 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:08:08,111 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 10:08:08,111 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:08:08,112 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:08:08,112 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:08:08,112 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , the beloved Son , whom I am approved . ”\n",
            "2021-07-25 10:08:08,112 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:08:08,113 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:08:08,113 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:08:08,113 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer what Jesus prayed ?\n",
            "2021-07-25 10:08:08,113 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:08:08,113 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:08:08,114 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:08:08,114 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 10:08:08,114 - INFO - joeynmt.training - Validation result (greedy) at epoch  22, step   137500: bleu:  16.77, loss: 129125.9219, ppl:   8.1901, duration: 74.3559s\n",
            "2021-07-25 10:08:21,699 - INFO - joeynmt.training - Epoch  22, Step:   137600, Batch Loss:     1.961074, Tokens per Sec:    16023, Lr: 0.000300\n",
            "2021-07-25 10:08:47,895 - INFO - joeynmt.training - Epoch  22, Step:   137800, Batch Loss:     1.809639, Tokens per Sec:    16267, Lr: 0.000300\n",
            "2021-07-25 10:09:13,922 - INFO - joeynmt.training - Epoch  22, Step:   138000, Batch Loss:     1.740990, Tokens per Sec:    16246, Lr: 0.000300\n",
            "2021-07-25 10:09:40,497 - INFO - joeynmt.training - Epoch  22, Step:   138200, Batch Loss:     1.571149, Tokens per Sec:    16370, Lr: 0.000300\n",
            "2021-07-25 10:10:06,756 - INFO - joeynmt.training - Epoch  22, Step:   138400, Batch Loss:     2.088475, Tokens per Sec:    16272, Lr: 0.000300\n",
            "2021-07-25 10:10:23,331 - INFO - joeynmt.training - Epoch  22: total training loss 5252.18\n",
            "2021-07-25 10:10:23,331 - INFO - joeynmt.training - EPOCH 23\n",
            "2021-07-25 10:10:33,785 - INFO - joeynmt.training - Epoch  23, Step:   138600, Batch Loss:     1.818512, Tokens per Sec:    15821, Lr: 0.000300\n",
            "2021-07-25 10:11:00,162 - INFO - joeynmt.training - Epoch  23, Step:   138800, Batch Loss:     2.000964, Tokens per Sec:    16401, Lr: 0.000300\n",
            "2021-07-25 10:11:26,296 - INFO - joeynmt.training - Epoch  23, Step:   139000, Batch Loss:     1.649897, Tokens per Sec:    16146, Lr: 0.000300\n",
            "2021-07-25 10:11:52,364 - INFO - joeynmt.training - Epoch  23, Step:   139200, Batch Loss:     1.929137, Tokens per Sec:    16244, Lr: 0.000300\n",
            "2021-07-25 10:12:18,766 - INFO - joeynmt.training - Epoch  23, Step:   139400, Batch Loss:     1.895236, Tokens per Sec:    16034, Lr: 0.000300\n",
            "2021-07-25 10:12:44,765 - INFO - joeynmt.training - Epoch  23, Step:   139600, Batch Loss:     1.773274, Tokens per Sec:    16251, Lr: 0.000300\n",
            "2021-07-25 10:13:11,162 - INFO - joeynmt.training - Epoch  23, Step:   139800, Batch Loss:     1.854708, Tokens per Sec:    16220, Lr: 0.000300\n",
            "2021-07-25 10:13:37,521 - INFO - joeynmt.training - Epoch  23, Step:   140000, Batch Loss:     1.668513, Tokens per Sec:    16283, Lr: 0.000300\n",
            "2021-07-25 10:14:42,270 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:14:42,271 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:14:42,271 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:14:42,877 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 10:14:42,877 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 10:14:43,629 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:14:43,630 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:14:43,630 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:14:43,631 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 10:14:43,631 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:14:43,632 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:14:43,632 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:14:43,632 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven saying : “ This is my Son , whom I am approved . ”\n",
            "2021-07-25 10:14:43,632 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:14:43,633 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:14:43,633 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:14:43,633 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 10:14:43,633 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:14:43,635 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:14:43,635 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:14:43,635 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 10:14:43,635 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step   140000: bleu:  18.50, loss: 125790.2422, ppl:   7.7570, duration: 66.1144s\n",
            "2021-07-25 10:15:10,020 - INFO - joeynmt.training - Epoch  23, Step:   140200, Batch Loss:     1.798202, Tokens per Sec:    16395, Lr: 0.000300\n",
            "2021-07-25 10:15:36,604 - INFO - joeynmt.training - Epoch  23, Step:   140400, Batch Loss:     1.927561, Tokens per Sec:    16314, Lr: 0.000300\n",
            "2021-07-25 10:16:02,974 - INFO - joeynmt.training - Epoch  23, Step:   140600, Batch Loss:     1.997305, Tokens per Sec:    16268, Lr: 0.000300\n",
            "2021-07-25 10:16:29,194 - INFO - joeynmt.training - Epoch  23, Step:   140800, Batch Loss:     1.777151, Tokens per Sec:    16276, Lr: 0.000300\n",
            "2021-07-25 10:16:55,610 - INFO - joeynmt.training - Epoch  23, Step:   141000, Batch Loss:     2.220490, Tokens per Sec:    16571, Lr: 0.000300\n",
            "2021-07-25 10:17:21,988 - INFO - joeynmt.training - Epoch  23, Step:   141200, Batch Loss:     1.990651, Tokens per Sec:    16253, Lr: 0.000300\n",
            "2021-07-25 10:17:48,063 - INFO - joeynmt.training - Epoch  23, Step:   141400, Batch Loss:     1.834205, Tokens per Sec:    16434, Lr: 0.000300\n",
            "2021-07-25 10:17:49,005 - INFO - joeynmt.training - Epoch  23: total training loss 5243.49\n",
            "2021-07-25 10:17:49,005 - INFO - joeynmt.training - EPOCH 24\n",
            "2021-07-25 10:18:14,618 - INFO - joeynmt.training - Epoch  24, Step:   141600, Batch Loss:     1.815633, Tokens per Sec:    15973, Lr: 0.000300\n",
            "2021-07-25 10:18:40,702 - INFO - joeynmt.training - Epoch  24, Step:   141800, Batch Loss:     2.257616, Tokens per Sec:    16301, Lr: 0.000300\n",
            "2021-07-25 10:19:06,877 - INFO - joeynmt.training - Epoch  24, Step:   142000, Batch Loss:     1.960044, Tokens per Sec:    16124, Lr: 0.000300\n",
            "2021-07-25 10:19:33,379 - INFO - joeynmt.training - Epoch  24, Step:   142200, Batch Loss:     1.945796, Tokens per Sec:    16222, Lr: 0.000300\n",
            "2021-07-25 10:19:59,488 - INFO - joeynmt.training - Epoch  24, Step:   142400, Batch Loss:     1.795331, Tokens per Sec:    16322, Lr: 0.000300\n",
            "2021-07-25 10:21:15,172 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:21:15,173 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:21:15,173 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:21:15,834 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 10:21:15,834 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 10:21:16,543 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:21:16,544 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:21:16,544 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:21:16,544 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 10:21:16,544 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:21:16,545 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:21:16,545 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:21:16,545 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , the beloved , whom I am approved . ”\n",
            "2021-07-25 10:21:16,545 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:21:16,545 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:21:16,546 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:21:16,546 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 10:21:16,546 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:21:16,547 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:21:16,547 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:21:16,547 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 10:21:16,547 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step   142500: bleu:  18.43, loss: 125430.4609, ppl:   7.7117, duration: 63.7569s\n",
            "2021-07-25 10:21:29,994 - INFO - joeynmt.training - Epoch  24, Step:   142600, Batch Loss:     1.878845, Tokens per Sec:    16148, Lr: 0.000300\n",
            "2021-07-25 10:21:56,515 - INFO - joeynmt.training - Epoch  24, Step:   142800, Batch Loss:     1.965806, Tokens per Sec:    16144, Lr: 0.000300\n",
            "2021-07-25 10:22:22,763 - INFO - joeynmt.training - Epoch  24, Step:   143000, Batch Loss:     1.917843, Tokens per Sec:    16210, Lr: 0.000300\n",
            "2021-07-25 10:22:48,625 - INFO - joeynmt.training - Epoch  24, Step:   143200, Batch Loss:     2.182695, Tokens per Sec:    16310, Lr: 0.000300\n",
            "2021-07-25 10:23:14,912 - INFO - joeynmt.training - Epoch  24, Step:   143400, Batch Loss:     1.886829, Tokens per Sec:    16336, Lr: 0.000300\n",
            "2021-07-25 10:23:41,098 - INFO - joeynmt.training - Epoch  24, Step:   143600, Batch Loss:     1.824303, Tokens per Sec:    16223, Lr: 0.000300\n",
            "2021-07-25 10:24:07,250 - INFO - joeynmt.training - Epoch  24, Step:   143800, Batch Loss:     1.571238, Tokens per Sec:    16355, Lr: 0.000300\n",
            "2021-07-25 10:24:33,705 - INFO - joeynmt.training - Epoch  24, Step:   144000, Batch Loss:     1.793606, Tokens per Sec:    16309, Lr: 0.000300\n",
            "2021-07-25 10:24:59,865 - INFO - joeynmt.training - Epoch  24, Step:   144200, Batch Loss:     1.795713, Tokens per Sec:    16366, Lr: 0.000300\n",
            "2021-07-25 10:25:13,335 - INFO - joeynmt.training - Epoch  24: total training loss 5258.19\n",
            "2021-07-25 10:25:13,335 - INFO - joeynmt.training - EPOCH 25\n",
            "2021-07-25 10:25:26,501 - INFO - joeynmt.training - Epoch  25, Step:   144400, Batch Loss:     2.258106, Tokens per Sec:    16122, Lr: 0.000300\n",
            "2021-07-25 10:25:52,741 - INFO - joeynmt.training - Epoch  25, Step:   144600, Batch Loss:     1.859167, Tokens per Sec:    16430, Lr: 0.000300\n",
            "2021-07-25 10:26:18,882 - INFO - joeynmt.training - Epoch  25, Step:   144800, Batch Loss:     1.570572, Tokens per Sec:    16191, Lr: 0.000300\n",
            "2021-07-25 10:26:45,145 - INFO - joeynmt.training - Epoch  25, Step:   145000, Batch Loss:     1.911706, Tokens per Sec:    16152, Lr: 0.000300\n",
            "2021-07-25 10:27:48,480 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:27:48,480 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:27:48,481 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:27:49,085 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 10:27:49,085 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 10:27:50,199 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:27:50,200 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:27:50,200 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:27:50,200 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , that is what is acceptable to God . ”\n",
            "2021-07-25 10:27:50,200 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:27:50,201 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:27:50,201 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:27:50,201 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , the beloved , whom I am approved . ”\n",
            "2021-07-25 10:27:50,201 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:27:50,202 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:27:50,202 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:27:50,202 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to what Jesus prayed ?\n",
            "2021-07-25 10:27:50,202 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:27:50,202 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:27:50,203 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:27:50,203 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 10:27:50,203 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step   145000: bleu:  18.62, loss: 125195.2344, ppl:   7.6822, duration: 65.0575s\n",
            "2021-07-25 10:28:17,091 - INFO - joeynmt.training - Epoch  25, Step:   145200, Batch Loss:     2.197307, Tokens per Sec:    16072, Lr: 0.000300\n",
            "2021-07-25 10:28:43,112 - INFO - joeynmt.training - Epoch  25, Step:   145400, Batch Loss:     1.844157, Tokens per Sec:    16219, Lr: 0.000300\n",
            "2021-07-25 10:29:09,303 - INFO - joeynmt.training - Epoch  25, Step:   145600, Batch Loss:     1.711595, Tokens per Sec:    15990, Lr: 0.000300\n",
            "2021-07-25 10:29:35,656 - INFO - joeynmt.training - Epoch  25, Step:   145800, Batch Loss:     1.759515, Tokens per Sec:    16310, Lr: 0.000300\n",
            "2021-07-25 10:30:01,798 - INFO - joeynmt.training - Epoch  25, Step:   146000, Batch Loss:     1.868912, Tokens per Sec:    16299, Lr: 0.000300\n",
            "2021-07-25 10:30:28,251 - INFO - joeynmt.training - Epoch  25, Step:   146200, Batch Loss:     2.036358, Tokens per Sec:    16103, Lr: 0.000300\n",
            "2021-07-25 10:30:54,667 - INFO - joeynmt.training - Epoch  25, Step:   146400, Batch Loss:     1.624764, Tokens per Sec:    16490, Lr: 0.000300\n",
            "2021-07-25 10:31:21,061 - INFO - joeynmt.training - Epoch  25, Step:   146600, Batch Loss:     1.932182, Tokens per Sec:    16381, Lr: 0.000300\n",
            "2021-07-25 10:31:47,484 - INFO - joeynmt.training - Epoch  25, Step:   146800, Batch Loss:     1.676093, Tokens per Sec:    16244, Lr: 0.000300\n",
            "2021-07-25 10:32:13,753 - INFO - joeynmt.training - Epoch  25, Step:   147000, Batch Loss:     1.814088, Tokens per Sec:    16188, Lr: 0.000300\n",
            "2021-07-25 10:32:38,938 - INFO - joeynmt.training - Epoch  25: total training loss 5238.60\n",
            "2021-07-25 10:32:38,938 - INFO - joeynmt.training - EPOCH 26\n",
            "2021-07-25 10:32:40,304 - INFO - joeynmt.training - Epoch  26, Step:   147200, Batch Loss:     1.781903, Tokens per Sec:    12474, Lr: 0.000300\n",
            "2021-07-25 10:33:06,831 - INFO - joeynmt.training - Epoch  26, Step:   147400, Batch Loss:     1.694866, Tokens per Sec:    16065, Lr: 0.000300\n",
            "2021-07-25 10:34:22,162 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:34:22,162 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:34:22,162 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:34:22,776 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 10:34:22,776 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 10:34:23,484 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:34:23,485 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:34:23,485 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:34:23,485 - INFO - joeynmt.training - \tHypothesis: “ If you are doing good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 10:34:23,485 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:34:23,486 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:34:23,486 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:34:23,486 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven saying : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 10:34:23,486 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:34:23,487 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:34:23,487 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:34:23,487 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 10:34:23,487 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:34:23,487 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:34:23,488 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:34:23,488 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
            "2021-07-25 10:34:23,488 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step   147500: bleu:  18.82, loss: 124894.7500, ppl:   7.6447, duration: 63.4595s\n",
            "2021-07-25 10:34:36,817 - INFO - joeynmt.training - Epoch  26, Step:   147600, Batch Loss:     2.097782, Tokens per Sec:    16218, Lr: 0.000300\n",
            "2021-07-25 10:35:03,261 - INFO - joeynmt.training - Epoch  26, Step:   147800, Batch Loss:     1.878576, Tokens per Sec:    16347, Lr: 0.000300\n",
            "2021-07-25 10:35:29,546 - INFO - joeynmt.training - Epoch  26, Step:   148000, Batch Loss:     1.609474, Tokens per Sec:    16114, Lr: 0.000300\n",
            "2021-07-25 10:35:55,730 - INFO - joeynmt.training - Epoch  26, Step:   148200, Batch Loss:     1.788905, Tokens per Sec:    16445, Lr: 0.000300\n",
            "2021-07-25 10:36:21,879 - INFO - joeynmt.training - Epoch  26, Step:   148400, Batch Loss:     1.986697, Tokens per Sec:    16158, Lr: 0.000300\n",
            "2021-07-25 10:36:48,252 - INFO - joeynmt.training - Epoch  26, Step:   148600, Batch Loss:     1.813309, Tokens per Sec:    16504, Lr: 0.000300\n",
            "2021-07-25 10:37:14,510 - INFO - joeynmt.training - Epoch  26, Step:   148800, Batch Loss:     1.768535, Tokens per Sec:    16261, Lr: 0.000300\n",
            "2021-07-25 10:37:41,177 - INFO - joeynmt.training - Epoch  26, Step:   149000, Batch Loss:     1.964503, Tokens per Sec:    16432, Lr: 0.000300\n",
            "2021-07-25 10:38:07,333 - INFO - joeynmt.training - Epoch  26, Step:   149200, Batch Loss:     1.864113, Tokens per Sec:    16126, Lr: 0.000300\n",
            "2021-07-25 10:38:33,492 - INFO - joeynmt.training - Epoch  26, Step:   149400, Batch Loss:     2.013803, Tokens per Sec:    16296, Lr: 0.000300\n",
            "2021-07-25 10:38:59,782 - INFO - joeynmt.training - Epoch  26, Step:   149600, Batch Loss:     1.935716, Tokens per Sec:    16439, Lr: 0.000300\n",
            "2021-07-25 10:39:26,163 - INFO - joeynmt.training - Epoch  26, Step:   149800, Batch Loss:     1.466567, Tokens per Sec:    16235, Lr: 0.000300\n",
            "2021-07-25 10:39:52,274 - INFO - joeynmt.training - Epoch  26, Step:   150000, Batch Loss:     1.773406, Tokens per Sec:    16472, Lr: 0.000300\n",
            "2021-07-25 10:40:56,411 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:40:56,411 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:40:56,412 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:40:57,016 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 10:40:57,016 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 10:40:57,710 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:40:57,711 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:40:57,711 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:40:57,711 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 10:40:57,711 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:40:57,712 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:40:57,712 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:40:57,712 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , whom I am approved . ”\n",
            "2021-07-25 10:40:57,712 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:40:57,712 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:40:57,713 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:40:57,713 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 10:40:57,713 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:40:57,713 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:40:57,713 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:40:57,715 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 10:40:57,715 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step   150000: bleu:  18.69, loss: 124563.8906, ppl:   7.6037, duration: 65.4401s\n",
            "2021-07-25 10:41:07,715 - INFO - joeynmt.training - Epoch  26: total training loss 5215.53\n",
            "2021-07-25 10:41:07,715 - INFO - joeynmt.training - EPOCH 27\n",
            "2021-07-25 10:41:24,797 - INFO - joeynmt.training - Epoch  27, Step:   150200, Batch Loss:     1.855901, Tokens per Sec:    16081, Lr: 0.000300\n",
            "2021-07-25 10:41:50,825 - INFO - joeynmt.training - Epoch  27, Step:   150400, Batch Loss:     1.894772, Tokens per Sec:    16296, Lr: 0.000300\n",
            "2021-07-25 10:42:17,085 - INFO - joeynmt.training - Epoch  27, Step:   150600, Batch Loss:     1.523725, Tokens per Sec:    16370, Lr: 0.000300\n",
            "2021-07-25 10:42:43,595 - INFO - joeynmt.training - Epoch  27, Step:   150800, Batch Loss:     1.790340, Tokens per Sec:    16352, Lr: 0.000300\n",
            "2021-07-25 10:43:10,027 - INFO - joeynmt.training - Epoch  27, Step:   151000, Batch Loss:     1.781056, Tokens per Sec:    16136, Lr: 0.000300\n",
            "2021-07-25 10:43:36,128 - INFO - joeynmt.training - Epoch  27, Step:   151200, Batch Loss:     2.134633, Tokens per Sec:    16231, Lr: 0.000300\n",
            "2021-07-25 10:44:02,435 - INFO - joeynmt.training - Epoch  27, Step:   151400, Batch Loss:     1.720435, Tokens per Sec:    16405, Lr: 0.000300\n",
            "2021-07-25 10:44:28,457 - INFO - joeynmt.training - Epoch  27, Step:   151600, Batch Loss:     1.974329, Tokens per Sec:    15944, Lr: 0.000300\n",
            "2021-07-25 10:44:54,825 - INFO - joeynmt.training - Epoch  27, Step:   151800, Batch Loss:     1.819200, Tokens per Sec:    16454, Lr: 0.000300\n",
            "2021-07-25 10:45:21,483 - INFO - joeynmt.training - Epoch  27, Step:   152000, Batch Loss:     1.789693, Tokens per Sec:    16281, Lr: 0.000300\n",
            "2021-07-25 10:45:47,807 - INFO - joeynmt.training - Epoch  27, Step:   152200, Batch Loss:     1.878596, Tokens per Sec:    16390, Lr: 0.000300\n",
            "2021-07-25 10:46:14,234 - INFO - joeynmt.training - Epoch  27, Step:   152400, Batch Loss:     1.965835, Tokens per Sec:    16391, Lr: 0.000300\n",
            "2021-07-25 10:47:32,085 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:47:32,085 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:47:32,085 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:47:33,389 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:47:33,390 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:47:33,390 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:47:33,390 - INFO - joeynmt.training - \tHypothesis: If you are doing good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 10:47:33,390 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:47:33,391 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:47:33,391 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:47:33,391 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven : “ This is my Son , the beloved , whom I am approved . ”\n",
            "2021-07-25 10:47:33,391 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:47:33,391 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:47:33,392 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:47:33,392 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah respond to Jesus ’ request ?\n",
            "2021-07-25 10:47:33,392 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:47:33,392 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:47:33,393 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:47:33,393 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
            "2021-07-25 10:47:33,393 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step   152500: bleu:  18.25, loss: 127660.9531, ppl:   7.9970, duration: 66.0093s\n",
            "2021-07-25 10:47:46,574 - INFO - joeynmt.training - Epoch  27, Step:   152600, Batch Loss:     1.706877, Tokens per Sec:    15956, Lr: 0.000300\n",
            "2021-07-25 10:48:12,776 - INFO - joeynmt.training - Epoch  27, Step:   152800, Batch Loss:     1.804171, Tokens per Sec:    16358, Lr: 0.000300\n",
            "2021-07-25 10:48:33,705 - INFO - joeynmt.training - Epoch  27: total training loss 5205.45\n",
            "2021-07-25 10:48:33,706 - INFO - joeynmt.training - EPOCH 28\n",
            "2021-07-25 10:48:39,710 - INFO - joeynmt.training - Epoch  28, Step:   153000, Batch Loss:     1.707893, Tokens per Sec:    15769, Lr: 0.000300\n",
            "2021-07-25 10:49:06,329 - INFO - joeynmt.training - Epoch  28, Step:   153200, Batch Loss:     2.006401, Tokens per Sec:    16302, Lr: 0.000300\n",
            "2021-07-25 10:49:32,595 - INFO - joeynmt.training - Epoch  28, Step:   153400, Batch Loss:     1.797016, Tokens per Sec:    16388, Lr: 0.000300\n",
            "2021-07-25 10:49:58,862 - INFO - joeynmt.training - Epoch  28, Step:   153600, Batch Loss:     1.886773, Tokens per Sec:    16115, Lr: 0.000300\n",
            "2021-07-25 10:50:25,332 - INFO - joeynmt.training - Epoch  28, Step:   153800, Batch Loss:     1.705175, Tokens per Sec:    16366, Lr: 0.000300\n",
            "2021-07-25 10:50:51,396 - INFO - joeynmt.training - Epoch  28, Step:   154000, Batch Loss:     1.893720, Tokens per Sec:    16332, Lr: 0.000300\n",
            "2021-07-25 10:51:17,840 - INFO - joeynmt.training - Epoch  28, Step:   154200, Batch Loss:     1.749640, Tokens per Sec:    16275, Lr: 0.000300\n",
            "2021-07-25 10:51:44,069 - INFO - joeynmt.training - Epoch  28, Step:   154400, Batch Loss:     1.917596, Tokens per Sec:    16516, Lr: 0.000300\n",
            "2021-07-25 10:52:10,105 - INFO - joeynmt.training - Epoch  28, Step:   154600, Batch Loss:     1.760701, Tokens per Sec:    16211, Lr: 0.000300\n",
            "2021-07-25 10:52:36,339 - INFO - joeynmt.training - Epoch  28, Step:   154800, Batch Loss:     1.774302, Tokens per Sec:    16221, Lr: 0.000300\n",
            "2021-07-25 10:53:02,665 - INFO - joeynmt.training - Epoch  28, Step:   155000, Batch Loss:     1.835142, Tokens per Sec:    16533, Lr: 0.000300\n",
            "2021-07-25 10:54:05,226 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 10:54:05,226 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 10:54:05,226 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 10:54:05,880 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-25 10:54:05,880 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-25 10:54:07,001 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 10:54:07,002 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 10:54:07,002 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 10:54:07,002 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer tribulation when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 10:54:07,002 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 10:54:07,003 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 10:54:07,003 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 10:54:07,003 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven , saying : “ This is my Son , whom I am approved . ”\n",
            "2021-07-25 10:54:07,003 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 10:54:07,003 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 10:54:07,004 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 10:54:07,004 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 10:54:07,004 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 10:54:07,004 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 10:54:07,004 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 10:54:07,005 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to save his family .\n",
            "2021-07-25 10:54:07,005 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step   155000: bleu:  18.72, loss: 124441.7812, ppl:   7.5885, duration: 64.3397s\n",
            "2021-07-25 10:54:33,530 - INFO - joeynmt.training - Epoch  28, Step:   155200, Batch Loss:     1.464822, Tokens per Sec:    16032, Lr: 0.000300\n",
            "2021-07-25 10:54:59,694 - INFO - joeynmt.training - Epoch  28, Step:   155400, Batch Loss:     1.603970, Tokens per Sec:    16033, Lr: 0.000300\n",
            "2021-07-25 10:55:25,982 - INFO - joeynmt.training - Epoch  28, Step:   155600, Batch Loss:     1.782960, Tokens per Sec:    16356, Lr: 0.000300\n",
            "2021-07-25 10:55:51,923 - INFO - joeynmt.training - Epoch  28, Step:   155800, Batch Loss:     1.939744, Tokens per Sec:    16209, Lr: 0.000300\n",
            "2021-07-25 10:55:57,990 - INFO - joeynmt.training - Epoch  28: total training loss 5211.34\n",
            "2021-07-25 10:55:57,991 - INFO - joeynmt.training - EPOCH 29\n",
            "2021-07-25 10:56:18,672 - INFO - joeynmt.training - Epoch  29, Step:   156000, Batch Loss:     2.003399, Tokens per Sec:    15942, Lr: 0.000300\n",
            "2021-07-25 10:56:44,760 - INFO - joeynmt.training - Epoch  29, Step:   156200, Batch Loss:     1.140093, Tokens per Sec:    16222, Lr: 0.000300\n",
            "2021-07-25 10:57:10,970 - INFO - joeynmt.training - Epoch  29, Step:   156400, Batch Loss:     1.598546, Tokens per Sec:    16252, Lr: 0.000300\n",
            "2021-07-25 10:57:37,483 - INFO - joeynmt.training - Epoch  29, Step:   156600, Batch Loss:     1.864306, Tokens per Sec:    16393, Lr: 0.000300\n",
            "2021-07-25 10:58:03,563 - INFO - joeynmt.training - Epoch  29, Step:   156800, Batch Loss:     1.776600, Tokens per Sec:    16377, Lr: 0.000300\n",
            "2021-07-25 10:58:30,062 - INFO - joeynmt.training - Epoch  29, Step:   157000, Batch Loss:     1.572510, Tokens per Sec:    16131, Lr: 0.000300\n",
            "2021-07-25 10:58:56,276 - INFO - joeynmt.training - Epoch  29, Step:   157200, Batch Loss:     1.902926, Tokens per Sec:    16225, Lr: 0.000300\n",
            "2021-07-25 10:59:22,481 - INFO - joeynmt.training - Epoch  29, Step:   157400, Batch Loss:     1.943073, Tokens per Sec:    16240, Lr: 0.000300\n",
            "2021-07-25 11:00:41,467 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 11:00:41,467 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 11:00:41,468 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 11:00:42,727 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 11:00:42,728 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 11:00:42,728 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 11:00:42,728 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer when you endure , this is what is acceptable to God . ”\n",
            "2021-07-25 11:00:42,728 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 11:00:42,729 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 11:00:42,729 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 11:00:42,729 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven saying : “ This is my beloved Son , whom I am approved . ”\n",
            "2021-07-25 11:00:42,729 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 11:00:42,729 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 11:00:42,730 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 11:00:42,730 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 11:00:42,730 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 11:00:42,730 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 11:00:42,731 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 11:00:42,731 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
            "2021-07-25 11:00:42,731 - INFO - joeynmt.training - Validation result (greedy) at epoch  29, step   157500: bleu:  18.63, loss: 125464.5078, ppl:   7.7160, duration: 67.0509s\n",
            "2021-07-25 11:00:56,233 - INFO - joeynmt.training - Epoch  29, Step:   157600, Batch Loss:     1.623632, Tokens per Sec:    16106, Lr: 0.000300\n",
            "2021-07-25 11:01:22,728 - INFO - joeynmt.training - Epoch  29, Step:   157800, Batch Loss:     1.781736, Tokens per Sec:    16298, Lr: 0.000300\n",
            "2021-07-25 11:01:48,852 - INFO - joeynmt.training - Epoch  29, Step:   158000, Batch Loss:     1.589799, Tokens per Sec:    16380, Lr: 0.000300\n",
            "2021-07-25 11:02:15,162 - INFO - joeynmt.training - Epoch  29, Step:   158200, Batch Loss:     1.911999, Tokens per Sec:    16196, Lr: 0.000300\n",
            "2021-07-25 11:02:41,596 - INFO - joeynmt.training - Epoch  29, Step:   158400, Batch Loss:     1.816706, Tokens per Sec:    16355, Lr: 0.000300\n",
            "2021-07-25 11:03:08,029 - INFO - joeynmt.training - Epoch  29, Step:   158600, Batch Loss:     1.657784, Tokens per Sec:    16289, Lr: 0.000300\n",
            "2021-07-25 11:03:25,561 - INFO - joeynmt.training - Epoch  29: total training loss 5190.13\n",
            "2021-07-25 11:03:25,562 - INFO - joeynmt.training - EPOCH 30\n",
            "2021-07-25 11:03:34,790 - INFO - joeynmt.training - Epoch  30, Step:   158800, Batch Loss:     1.643161, Tokens per Sec:    15650, Lr: 0.000300\n",
            "2021-07-25 11:04:00,986 - INFO - joeynmt.training - Epoch  30, Step:   159000, Batch Loss:     1.835043, Tokens per Sec:    16551, Lr: 0.000300\n",
            "2021-07-25 11:04:27,498 - INFO - joeynmt.training - Epoch  30, Step:   159200, Batch Loss:     2.019570, Tokens per Sec:    16297, Lr: 0.000300\n",
            "2021-07-25 11:04:53,898 - INFO - joeynmt.training - Epoch  30, Step:   159400, Batch Loss:     1.961187, Tokens per Sec:    16239, Lr: 0.000300\n",
            "2021-07-25 11:05:20,204 - INFO - joeynmt.training - Epoch  30, Step:   159600, Batch Loss:     1.810663, Tokens per Sec:    16221, Lr: 0.000300\n",
            "2021-07-25 11:05:46,271 - INFO - joeynmt.training - Epoch  30, Step:   159800, Batch Loss:     1.871025, Tokens per Sec:    16395, Lr: 0.000300\n",
            "2021-07-25 11:06:12,700 - INFO - joeynmt.training - Epoch  30, Step:   160000, Batch Loss:     1.699046, Tokens per Sec:    16153, Lr: 0.000300\n",
            "2021-07-25 11:07:17,043 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 11:07:17,043 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 11:07:17,043 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 11:07:18,432 - INFO - joeynmt.training - Example #0\n",
            "2021-07-25 11:07:18,433 - INFO - joeynmt.training - \tSource:     [ N ] aye bwe mukola obulungi ne mubonyaabonyezebwa bwe muligumiikiriza , ekyo kye kisiimibwa eri Katonda . ”\n",
            "2021-07-25 11:07:18,433 - INFO - joeynmt.training - \tReference:  But if , when you are doing good and you suffer , you endure it , this is a thing agreeable with God . ”\n",
            "2021-07-25 11:07:18,433 - INFO - joeynmt.training - \tHypothesis: If you do good and suffer tribulation , this is what is acceptable to God . ”\n",
            "2021-07-25 11:07:18,433 - INFO - joeynmt.training - Example #1\n",
            "2021-07-25 11:07:18,434 - INFO - joeynmt.training - \tSource:     Yesu bwe yali yaakamala okubatizibwa , yawulira eddoboozi okuva mu ggulu nga ligamba nti : “ Ono ye Mwana wange omwagalwa gwe nsiima . ”\n",
            "2021-07-25 11:07:18,434 - INFO - joeynmt.training - \tReference:  At his baptism , Jesus heard a voice from heaven say : “ This is my Son , the beloved , whom I have approved . ”\n",
            "2021-07-25 11:07:18,434 - INFO - joeynmt.training - \tHypothesis: After Jesus was baptized , he heard a voice from heaven , saying : “ This is my Son , whom I am approved . ”\n",
            "2021-07-25 11:07:18,434 - INFO - joeynmt.training - Example #2\n",
            "2021-07-25 11:07:18,435 - INFO - joeynmt.training - \tSource:     ( b ) Yakuwa yaddamu atya ekyo Yesu kye yeesabira ?\n",
            "2021-07-25 11:07:18,435 - INFO - joeynmt.training - \tReference:  ( b ) How did Jehovah answer Jesus ’ personal request about his future ?\n",
            "2021-07-25 11:07:18,435 - INFO - joeynmt.training - \tHypothesis: ( b ) How did Jehovah answer Jesus ’ request ?\n",
            "2021-07-25 11:07:18,435 - INFO - joeynmt.training - Example #3\n",
            "2021-07-25 11:07:18,436 - INFO - joeynmt.training - \tSource:     Naye Abbigayiri alina kye yakolawo okusobola okuwonya ab’omu maka ge .\n",
            "2021-07-25 11:07:18,436 - INFO - joeynmt.training - \tReference:  But Abigail took action to save her household .\n",
            "2021-07-25 11:07:18,437 - INFO - joeynmt.training - \tHypothesis: But Abigail did what he did to heal his family .\n",
            "2021-07-25 11:07:18,437 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step   160000: bleu:  18.62, loss: 124954.7422, ppl:   7.6522, duration: 65.7362s\n",
            "2021-07-25 11:07:44,925 - INFO - joeynmt.training - Epoch  30, Step:   160200, Batch Loss:     1.561813, Tokens per Sec:    16147, Lr: 0.000300\n",
            "2021-07-25 11:08:11,456 - INFO - joeynmt.training - Epoch  30, Step:   160400, Batch Loss:     1.961448, Tokens per Sec:    16301, Lr: 0.000300\n",
            "2021-07-25 11:08:37,656 - INFO - joeynmt.training - Epoch  30, Step:   160600, Batch Loss:     1.436814, Tokens per Sec:    16147, Lr: 0.000300\n",
            "2021-07-25 11:09:03,705 - INFO - joeynmt.training - Epoch  30, Step:   160800, Batch Loss:     1.902132, Tokens per Sec:    16540, Lr: 0.000300\n",
            "2021-07-25 11:09:30,117 - INFO - joeynmt.training - Epoch  30, Step:   161000, Batch Loss:     2.016936, Tokens per Sec:    16208, Lr: 0.000300\n",
            "2021-07-25 11:09:56,304 - INFO - joeynmt.training - Epoch  30, Step:   161200, Batch Loss:     1.893069, Tokens per Sec:    16217, Lr: 0.000300\n",
            "2021-07-25 11:10:22,574 - INFO - joeynmt.training - Epoch  30, Step:   161400, Batch Loss:     1.255039, Tokens per Sec:    16416, Lr: 0.000300\n",
            "2021-07-25 11:10:48,778 - INFO - joeynmt.training - Epoch  30, Step:   161600, Batch Loss:     1.864959, Tokens per Sec:    16038, Lr: 0.000300\n",
            "2021-07-25 11:10:51,368 - INFO - joeynmt.training - Epoch  30: total training loss 5187.56\n",
            "2021-07-25 11:10:51,368 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
            "2021-07-25 11:10:51,368 - INFO - joeynmt.training - Best validation result (greedy) at step   155000:   7.59 ppl.\n",
            "2021-07-25 11:10:51,392 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-25 11:10:51,768 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-25 11:10:51,965 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-25 11:10:52,041 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/dev.bpe.en)...\n",
            "2021-07-25 11:11:58,841 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 11:11:58,841 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 11:11:58,841 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 11:11:59,466 - INFO - joeynmt.prediction -  dev bleu[13a]:  19.22 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-25 11:11:59,471 - INFO - joeynmt.prediction - Translations saved to: models/lg_lhen_reverse_transformer_continued3/00155000.hyps.dev\n",
            "2021-07-25 11:11:59,471 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.en)...\n",
            "2021-07-25 11:12:41,036 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-25 11:12:41,036 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-25 11:12:41,037 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-25 11:12:41,344 - INFO - joeynmt.prediction - test bleu[13a]:  12.32 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-25 11:12:41,348 - INFO - joeynmt.prediction - Translations saved to: models/lg_lhen_reverse_transformer_continued3/00155000.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL9G7wtudqIC",
        "outputId": "4322a76f-afd7-4ef0-9bca-ab16fec55e6c"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_lhen_reverse_transformer_continued3/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/translation.bpe.lh_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-25 11:47:45,081 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-25 11:47:49,094 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-25 11:47:49,291 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIw66TiSfuYs",
        "outputId": "2dc78d2c-847a-4a59-cb7c-976eedd4daf1"
      },
      "source": [
        "!cat \"translation.bpe.lh_en\" | sacrebleu \"test1.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 12.6 44.2/17.7/8.3/4.4 (BP = 0.969 ratio = 0.969 hyp_len = 25240 ref_len = 26044)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz6Le20zfNI-",
        "outputId": "4620ff87-0bfe-43db-a6ab-3b15c018600c"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/lg_lhen_reverse_transformer_continued3/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/test.bpe.lg\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual2/translation2.bpe.lg_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-25 11:52:49,017 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-25 11:52:53,224 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-25 11:52:53,426 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5hJip2Rg7VL",
        "outputId": "c33fc5e7-e905-4175-c549-03acd2e67236"
      },
      "source": [
        "!cat \"translation2.bpe.lg_en\" | sacrebleu \"test2.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 37.3 66.5/45.0/34.0/26.8 (BP = 0.919 ratio = 0.922 hyp_len = 39750 ref_len = 43116)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}