{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KinyarwandaLuhya_Multilingual_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LyNOs7gmOVGL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EverlynAsiko/Neural_Machine_Translation_for_African_Languages/blob/main/KinyarwandaLuhya_Multilingual_NMT_results1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6cJZzBRP1-N"
      },
      "source": [
        "# Multilingual neural machine translation.\n",
        "\n",
        "For this case, we shall to a many-to-one translation:\n",
        "{Kinyarwanda, Luhya} to English. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4V-O3nJPsAA",
        "outputId": "84ad0316-158d-40bb-af96-8434ed609136"
      },
      "source": [
        "# Linking to drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcRP_CqbRQzj"
      },
      "source": [
        "# Importing needed libraries for preprocessing and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "collapsed": true,
        "id": "grB3V9FhReiZ",
        "outputId": "36201de7-9583-4eec-9dc9-bcdb33048a49"
      },
      "source": [
        "#@title Default title text\n",
        "# Install Pytorch with GPU support v1.8.0.\n",
        "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 763.5 MB 15 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7jAsiRLRlMs"
      },
      "source": [
        "# Filtering warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaH6F-u3RrAb"
      },
      "source": [
        "# Loading the drive\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH_IYggrTJKa"
      },
      "source": [
        "# Setting source and target languages\n",
        "source_language = \"en\"\n",
        "target_language = \"rw_lh\"\n",
        "\n",
        "os.environ[\"src\"] = source_language \n",
        "os.environ[\"tgt\"] = target_language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G0mZmUETh-A"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uMD6Hic-TSCa",
        "outputId": "1d6f4fc0-d51b-4c0f-870f-b23c5a73de93"
      },
      "source": [
        "! head Kinyarwanda/train.*\n",
        "! head Kinyarwanda/dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Kinyarwanda/train.bpe.en <==\n",
            "R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ul@@ a that was conduc@@ ive to med@@ it@@ ation .\n",
            "You will see the time when God br@@ ings righteous rule to all the earth , und@@ o@@ ing the d@@ am@@ age and inj@@ ust@@ ice brought by human rul@@ er@@ ship .\n",
            "Let us consider f@@ ive reas@@ ons why we should want to follow the Christ .\n",
            "Even in the Bible , the id@@ ea of pers@@ u@@ as@@ ion som@@ et@@ imes has n@@ eg@@ ative con@@ no@@ t@@ ations , den@@ ot@@ ing a cor@@ rup@@ ting or a lead@@ ing as@@ tr@@ ay .\n",
            "For God’s servants to be deliv@@ ered , Satan and his ent@@ ire world@@ wide system of things need to be rem@@ ov@@ ed .\n",
            "I had never heard that name used in my ch@@ urch .\n",
            "S@@ imp@@ ly having authority or a wid@@ er name recogn@@ ition is not the important thing .\n",
            "M@@ ost people do not believe in the spir@@ its .\n",
            "And others are encourag@@ ed to be merc@@ if@@ ul , for merc@@ y beg@@ ets merc@@ y . ​ — Luke 6 : 38 .\n",
            "Like such ro@@ o@@ ts in earth@@ ’s no@@ ur@@ ish@@ ing so@@ il , our m@@ inds and hearts need to del@@ v@@ e exp@@ ans@@ ively into God’s Word and d@@ raw from its life - giving wat@@ ers .\n",
            "\n",
            "==> Kinyarwanda/train.bpe.rw <==\n",
            "A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
            "U@@ z@@ aba uh@@ ari igihe Imana iz@@ ashy@@ iraho ubutegetsi buk@@ iranuka ku isi hose , ik@@ av@@ an@@ aho ibibi n’@@ akar@@ eng@@ ane byat@@ ewe n’@@ ubutegetsi bw’@@ abantu .\n",
            "Nim@@ ucyo dusuzume impamvu esh@@ anu z@@ agombye gutuma tw@@ ifuza guk@@ urikira Kristo .\n",
            "Nd@@ etse no muri Bibiliya , igit@@ ekerezo cyo kw@@ emeza umuntu ikintu , rimwe na rimwe cy@@ umvikana mu buryo bub@@ i , kig@@ as@@ obanura k@@ osh@@ ya , cyangwa kuy@@ ob@@ ya .\n",
            "Kugira ngo abagaragu b’Imana baz@@ ac@@ ung@@ ur@@ we , Satani na gahunda ye y’@@ ibintu yose yo ku isi hose big@@ omba kuv@@ an@@ waho .\n",
            "Mu idini n@@ abag@@ amo sin@@ ari nar@@ igeze n@@ umva bak@@ oresha iryo z@@ ina .\n",
            "G@@ uh@@ abwa ubut@@ ware gusa cyangwa kugira umw@@ anya ukomeye si cyo kintu cy’@@ ingenzi .\n",
            "Abantu benshi ntib@@ emera imy@@ uka .\n",
            "Iyo tug@@ iriye abantu imbabazi na bo bib@@ at@@ era kugira imbabazi , kuko imbabazi zit@@ urwa izindi . — Luka 6 : 38 .\n",
            "Nk’uko iyo m@@ izi ig@@ ab@@ urira ig@@ iti ib@@ iv@@ uye mu but@@ aka buk@@ ung@@ ah@@ aye , ubwenge n’@@ umutima byacu big@@ omba guc@@ eng@@ era mu Ijambo ry’Imana maze bik@@ av@@ om@@ amo amazi atanga ubuzima .\n",
            "\n",
            "==> Kinyarwanda/train.en <==\n",
            "Right after his baptism , he “ went off into Arabia ” ​ — either the Syrian Desert or possibly some quiet place on the Arabian Peninsula that was conducive to meditation .\n",
            "You will see the time when God brings righteous rule to all the earth , undoing the damage and injustice brought by human rulership .\n",
            "Let us consider five reasons why we should want to follow the Christ .\n",
            "Even in the Bible , the idea of persuasion sometimes has negative connotations , denoting a corrupting or a leading astray .\n",
            "For God’s servants to be delivered , Satan and his entire worldwide system of things need to be removed .\n",
            "I had never heard that name used in my church .\n",
            "Simply having authority or a wider name recognition is not the important thing .\n",
            "Most people do not believe in the spirits .\n",
            "And others are encouraged to be merciful , for mercy begets mercy . ​ — Luke 6 : 38 .\n",
            "Like such roots in earth’s nourishing soil , our minds and hearts need to delve expansively into God’s Word and draw from its life - giving waters .\n",
            "\n",
            "==> Kinyarwanda/train.rw <==\n",
            "Ashobora kuba yaragiye ahantu hatuje mu Butayu bwa Siriya cyangwa se wenda ku Mwigimbakirwa wa Arabiya , uri mu burasirazuba bw’Inyanja Itukura , kugira ngo hamufashe gutekereza .\n",
            "Uzaba uhari igihe Imana izashyiraho ubutegetsi bukiranuka ku isi hose , ikavanaho ibibi n’akarengane byatewe n’ubutegetsi bw’abantu .\n",
            "Nimucyo dusuzume impamvu eshanu zagombye gutuma twifuza gukurikira Kristo .\n",
            "Ndetse no muri Bibiliya , igitekerezo cyo kwemeza umuntu ikintu , rimwe na rimwe cyumvikana mu buryo bubi , kigasobanura koshya , cyangwa kuyobya .\n",
            "Kugira ngo abagaragu b’Imana bazacungurwe , Satani na gahunda ye y’ibintu yose yo ku isi hose bigomba kuvanwaho .\n",
            "Mu idini nabagamo sinari narigeze numva bakoresha iryo zina .\n",
            "Guhabwa ubutware gusa cyangwa kugira umwanya ukomeye si cyo kintu cy’ingenzi .\n",
            "Abantu benshi ntibemera imyuka .\n",
            "Iyo tugiriye abantu imbabazi na bo bibatera kugira imbabazi , kuko imbabazi ziturwa izindi . — Luka 6 : 38 .\n",
            "Nk’uko iyo mizi igaburira igiti ibivuye mu butaka bukungahaye , ubwenge n’umutima byacu bigomba gucengera mu Ijambo ry’Imana maze bikavomamo amazi atanga ubuzima .\n",
            "==> Kinyarwanda/dev.bpe.en <==\n",
            "My heart was t@@ ou@@ ched .\n",
            "Consider , however , what was involved in reading a s@@ cro@@ l@@ l .\n",
            "Rather than succ@@ um@@ b to p@@ an@@ ic or des@@ pa@@ ir , we should b@@ ol@@ st@@ er our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "S@@ ad@@ ly , some within the Christian congregation give evidence that , at le@@ ast to a d@@ eg@@ ree , they feel com@@ for@@ table in Satan’s world .\n",
            "Jehovah took into account El@@ ij@@ ah’s l@@ imit@@ ations and dis@@ pat@@ ched an ang@@ el .\n",
            "T@@ ho@@ ugh in@@ her@@ iting imper@@ f@@ ection from Adam , mill@@ ions of other God - f@@ earing humans have follow@@ ed in Jesus ’ f@@ oo@@ t@@ st@@ ep@@ s by keep@@ ing integr@@ ity in the face of sat@@ an@@ ic att@@ ac@@ ks . — 1 Peter 1 : 18 , 19 ; 2 : 19 , 21 .\n",
            "C@@ ould those who later me@@ et the son righ@@ t@@ fully con@@ clud@@ e that he had a bad father or even that he had no father at all ?\n",
            "A@@ ud@@ i@@ ences were moved by the dr@@ ama “ R@@ esp@@ ect Jehovah’s A@@ uth@@ or@@ ity ”\n",
            "Satan has destro@@ y@@ ed coun@@ t@@ less who@@ l@@ es@@ ome , tr@@ us@@ ting relation@@ ship@@ s through in@@ sid@@ ious doub@@ ts pl@@ an@@ ted in that way . ​ — Gal@@ at@@ ians 5 : 7 - 9 .\n",
            "T@@ om , a m@@ ember of the Be@@ the@@ l family in E@@ st@@ on@@ ia , says : “ J@@ ust a b@@ loc@@ k away from Be@@ the@@ l is the se@@ a , and n@@ ear@@ by there is a be@@ aut@@ if@@ ul for@@ est where my wife and I enjoy going for sh@@ ort wal@@ ks .\n",
            "\n",
            "==> Kinyarwanda/dev.bpe.rw <==\n",
            "By@@ ank@@ oze ku mutima .\n",
            "Umw@@ andiko w@@ abaga w@@ anditse mu nk@@ ing@@ i ku ruhande rw’@@ imbere rw’@@ uwo muz@@ ingo .\n",
            "Aho kugira ngo duh@@ ang@@ ay@@ ike cyangwa tw@@ ih@@ ebe , twagombye gukomeza ukwizera duf@@ itiye Imana binyuriye mu gusoma Ijambo ry@@ ayo . — Abaroma 8 : 35 - 39 .\n",
            "Ik@@ ibab@@ aje ni uko hari bamwe mu bagize itorero rya gikristo bag@@ aragaza , mu rugero runaka , ko bag@@ uwe neza muri iyi si ya Satani .\n",
            "Yehova y@@ az@@ irik@@ anye intege nk@@ e za El@@ iya maze amw@@ oh@@ erer@@ eza umum@@ ar@@ ayika .\n",
            "N’ubwo bar@@ az@@ we uk@@ ud@@ at@@ ung@@ ana bitewe n’@@ icyaha cya Adamu , abandi bantu bat@@ inya Imana babarirwa muri za miriyoni , bag@@ eze ikir@@ enge mu cya Yesu bak@@ omeza gush@@ ik@@ ama mu gihe bari bah@@ anganye n’@@ ibit@@ ero bya Satani . ​ —⁠ 1 Petero 1 : 18 , 19 ; 2 : ​ 19 , 21 .\n",
            "Ese byaba bik@@ wiriye kuvuga ko uwo mub@@ yeyi yar@@ eze nabi , tuk@@ aba tw@@ an@@ avuga ko umwana at@@ agira se ?\n",
            "Ab@@ ari mu ikor@@ aniro bak@@ ozwe ku mutima na d@@ ar@@ ame yari ifite umutwe uvuga ngo “ Jya W@@ ubaha Ubut@@ ware bwa Yehova ”\n",
            "Satani yash@@ enye imishyikirano myiza yar@@ ang@@ waga no kw@@ iz@@ er@@ ana abantu bat@@ abar@@ ika bari b@@ af@@ itanye , binyuriye mu bit@@ ekerezo b@@ if@@ if@@ itse byo gushidikanya yagiye ab@@ iba muri ubwo buryo . — Abag@@ al@@ at@@ iya 5 : 7 - 9 .\n",
            "T@@ om , umwe mu bagize umuryango wa Bet@@ eli yo muri Es@@ it@@ on@@ iya , yagize ati “ iyo ur@@ enze inzu imwe gusa uv@@ uye kuri Bet@@ eli uh@@ ita ug@@ era ku ny@@ anja , kandi hafi aho hari ag@@ ashy@@ amba k@@ eza aho jye n’umugore wanjye duk@@ unda kujya gut@@ emb@@ er@@ era ak@@ anya gato .\n",
            "\n",
            "==> Kinyarwanda/dev.en <==\n",
            "My heart was touched .\n",
            "Consider , however , what was involved in reading a scroll .\n",
            "Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "Jehovah took into account Elijah’s limitations and dispatched an angel .\n",
            "Though inheriting imperfection from Adam , millions of other God - fearing humans have followed in Jesus ’ footsteps by keeping integrity in the face of satanic attacks . — 1 Peter 1 : 18 , 19 ; 2 : 19 , 21 .\n",
            "Could those who later meet the son rightfully conclude that he had a bad father or even that he had no father at all ?\n",
            "Audiences were moved by the drama “ Respect Jehovah’s Authority ”\n",
            "Satan has destroyed countless wholesome , trusting relationships through insidious doubts planted in that way . ​ — Galatians 5 : 7 - 9 .\n",
            "Tom , a member of the Bethel family in Estonia , says : “ Just a block away from Bethel is the sea , and nearby there is a beautiful forest where my wife and I enjoy going for short walks .\n",
            "\n",
            "==> Kinyarwanda/dev.rw <==\n",
            "Byankoze ku mutima .\n",
            "Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "Yehova yazirikanye intege nke za Eliya maze amwoherereza umumarayika .\n",
            "N’ubwo barazwe ukudatungana bitewe n’icyaha cya Adamu , abandi bantu batinya Imana babarirwa muri za miriyoni , bageze ikirenge mu cya Yesu bakomeza gushikama mu gihe bari bahanganye n’ibitero bya Satani . ​ —⁠ 1 Petero 1 : 18 , 19 ; 2 : ​ 19 , 21 .\n",
            "Ese byaba bikwiriye kuvuga ko uwo mubyeyi yareze nabi , tukaba twanavuga ko umwana atagira se ?\n",
            "Abari mu ikoraniro bakozwe ku mutima na darame yari ifite umutwe uvuga ngo “ Jya Wubaha Ubutware bwa Yehova ”\n",
            "Satani yashenye imishyikirano myiza yarangwaga no kwizerana abantu batabarika bari bafitanye , binyuriye mu bitekerezo bififitse byo gushidikanya yagiye abiba muri ubwo buryo . — Abagalatiya 5 : 7 - 9 .\n",
            "Tom , umwe mu bagize umuryango wa Beteli yo muri Esitoniya , yagize ati “ iyo urenze inzu imwe gusa uvuye kuri Beteli uhita ugera ku nyanja , kandi hafi aho hari agashyamba keza aho jye n’umugore wanjye dukunda kujya gutemberera akanya gato .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IP5nJ822UGJM",
        "outputId": "73cdbd2b-5230-47d1-c5b5-741c04f9b832"
      },
      "source": [
        "! head Luhyia/train.*\n",
        "! head Luhyia/dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Luhyia/train.bpe.en <==\n",
            " T@@ hat day was the P@@ re@@ par@@ ation, and the Sab@@ b@@ ath drew ne@@ ar@@ .\n",
            " Behold, I am coming qui@@ ck@@ l@@ y@@ ! H@@ old fast what you ha@@ ve, that no one may take your crow@@ n.\n",
            " The next day, because he wan@@ ted to know for certain why he was acc@@ us@@ ed by the Jews, he r@@ ele@@ as@@ ed him from his bond@@ s, and commanded the chief priests and all their coun@@ ci@@ l to appear@@ , and brought Paul down and set him before them. \n",
            "\n",
            " This He said, sig@@ ni@@ f@@ ying by what death He would di@@ e.\n",
            " Then they said to the wom@@ an, “@@ Now we believ@@ e, not because of what you said, for we our@@ selves have heard Him and we know that this is indeed the Christ, the Sa@@ vi@@ or of the worl@@ d.”\n",
            " But rej@@ ect prof@@ ane and old wi@@ ves@@ ’ f@@ ab@@ les, and ex@@ er@@ c@@ ise your@@ self toward god@@ lin@@ es@@ s.\n",
            " It is written in the prophe@@ ts, ‘@@ And they shall all be taught by God@@ .’ Therefore everyone who has heard and lear@@ ned from the Father comes to Me.\n",
            " Then out of the sm@@ oke lo@@ c@@ us@@ ts came upon the ear@@ th. And to them was given pow@@ er, as the s@@ cor@@ pi@@ ons of the earth have pow@@ er.\n",
            " Jesus said to him, “R@@ is@@ e, take up your b@@ ed and wal@@ k@@ .”\n",
            "\n",
            "==> Luhyia/train.bpe.lh <==\n",
            " Y@@ ali,@@ inyanga yo@@ khwi@@ re@@ chekha khulwa inyanga eya Is@@ aba@@ to ey@@ ali niy@@ ili ahambi okhu@@ chaak@@ a. \n",
            " N@@ di@@ itsanga bwangu o@@ hand@@ e khu aka oli nin@@ ako, kho mbu, omundu yesi yesi,@@ alab@@ uk@@ ul@@ akhwo olu@@ si@@ mb@@ il@@ wo tawe. \n",
            " Omu@@ s@@ injilili wab@@ elihe oyo y@@ enya okhumanya eshi@@ chila,@@ Abayahudi nib@@ enj@@ ililanga Paulo itookh@@ o. Kho iny@@ anga,@@ yal@@ ondakhwo yab@@ ol@@ ola em@@ iny@@ olol@@ o echia bali nibab@@ oy@@ ile,@@ Paulo mana nal@@ aka abesaaliisi aba@@ khongo nende ab@@ eshiin@@ a,@@ boosi okhw@@ aka@@ ana. Mana nay@@ ila Paulo namu@@ s@@ injisia imbeli,@@ w@@ abwe.\n",
            "\n",
            " Y@@ aboola ako khulw@@ okhumany@@ ia,@@ shinga lw@@ okhuf@@ wakh@@ we khw@@ itsa okhuba@@ . \n",
            " Kho nibaboolela omukhasi oyo bari, “I@@ fwe,@@ shikhu@@ suu@@ bile khulwa okhubela aka iwe okhubool@@ ile ta habula khu@@ suubi@@ ile shichila mbu, abeene khw@@ its@@ ile,@@ nikhu@@ hulila na@@ ya@@ ala, ne bulano khumanyile mbu, niye,@@ omu@@ honia w@@ abandu boosi@@ .” Yesu ahonia omwana w@@ omus@@ esi@@ a,\n",
            " Nebutswa,@@ wi@@ h@@ any@@ e okhurula khu@@ tsing@@ ano tsi@@ ab@@ u@@ ts@@ wa ets@@ il@@ akhoy@@ e@@ ele,@@ ta, w@@ ina@@ sie okhwe@@ ka amakhuwa amalayi k@@ obulamu obwa,@@ eshik@@ rist@@ o. \n",
            " Ab@@ al@@ akusi@@ ,@@ ba@@ hand@@ ika mbu, ‘@@ Buli mundu ali@@ e@@ chesi@@ bwa nende,@@ Nyasaye@@ .’ Kho oyo yesi ou@@ hulilanga aka Papa nende,@@ okhwe@@ ka okhurula khuy@@ e, yetsa khw@@ isie. \n",
            " Ne tsi@@ si@@ che nitsi@@ rula mu@@ mw@@ osi nitsi@@ ba khushialo nitsi@@ helesi@@ bwe obunyali obu@@ fwana shinga obwa amak@@ ati@@ a,@@ k@@ eshial@@ o. \n",
            " Yesu namuboolela ari, “S@@ injila wit@@ u@@ ushe omuk@@ e@@ kw@@ o,@@ mana o@@ chend@@ e.” \n",
            "\n",
            "==> Luhyia/train.en <==\n",
            " That day was the Preparation, and the Sabbath drew near.\n",
            " Behold, I am coming quickly! Hold fast what you have, that no one may take your crown.\n",
            " The next day, because he wanted to know for certain why he was accused by the Jews, he released him from his bonds, and commanded the chief priests and all their council to appear, and brought Paul down and set him before them. \n",
            "\n",
            " This He said, signifying by what death He would die.\n",
            " Then they said to the woman, “Now we believe, not because of what you said, for we ourselves have heard Him and we know that this is indeed the Christ, the Savior of the world.”\n",
            " But reject profane and old wives’ fables, and exercise yourself toward godliness.\n",
            " It is written in the prophets, ‘And they shall all be taught by God.’ Therefore everyone who has heard and learned from the Father comes to Me.\n",
            " Then out of the smoke locusts came upon the earth. And to them was given power, as the scorpions of the earth have power.\n",
            " Jesus said to him, “Rise, take up your bed and walk.”\n",
            "\n",
            "==> Luhyia/train.lh <==\n",
            " Yali,inyanga yokhwirechekha khulwa inyanga eya Isabato eyali niyili ahambi okhuchaaka. \n",
            " Ndiitsanga bwangu ohande khu aka oli ninako, kho mbu, omundu yesi yesi,alabukulakhwo olusimbilwo tawe. \n",
            " Omusinjilili wabelihe oyo yenya okhumanya eshichila,Abayahudi nibenjililanga Paulo itookho. Kho inyanga,yalondakhwo yabolola eminyololo echia bali nibaboyile,Paulo mana nalaka abesaaliisi abakhongo nende abeshiina,boosi okhwakaana. Mana nayila Paulo namusinjisia imbeli,wabwe.\n",
            "\n",
            " Yaboola ako khulwokhumanyia,shinga lwokhufwakhwe khwitsa okhuba. \n",
            " Kho nibaboolela omukhasi oyo bari, “Ifwe,shikhusuubile khulwa okhubela aka iwe okhuboolile ta habula khusuubiile shichila mbu, abeene khwitsile,nikhuhulila nayaala, ne bulano khumanyile mbu, niye,omuhonia wabandu boosi.” Yesu ahonia omwana womusesia,\n",
            " Nebutswa,wihanye okhurula khutsingano tsiabutswa etsilakhoyeele,ta, winasie okhweka amakhuwa amalayi kobulamu obwa,eshikristo. \n",
            " Abalakusi,bahandika mbu, ‘Buli mundu aliechesibwa nende,Nyasaye.’ Kho oyo yesi ouhulilanga aka Papa nende,okhweka okhurula khuye, yetsa khwisie. \n",
            " Ne tsisiche nitsirula mumwosi nitsiba khushialo nitsihelesibwe obunyali obufwana shinga obwa amakatia,keshialo. \n",
            " Yesu namuboolela ari, “Sinjila wituushe omukekwo,mana ochende.” \n",
            "==> Luhyia/dev.bpe.en <==\n",
            " They brought him who for@@ mer@@ ly was blind to the Pharise@@ es.\n",
            " And whoever li@@ ves and believ@@ es in Me shall never di@@ e. Do you believe this@@ ?”\n",
            " Then he took it dow@@ n, w@@ ra@@ pp@@ ed it in lin@@ en, and laid it in a tom@@ b that was he@@ w@@ n out of the ro@@ ck@@ , where no one had ever la@@ in be@@ for@@ e.\n",
            " Now when they had es@@ ca@@ pe@@ d, they then found out that the is@@ land was called M@@ al@@ ta@@ .\n",
            " Nevertheles@@ s she will be sa@@ ved in chil@@ d@@ be@@ ar@@ ing if they continu@@ e in faith, love, and hol@@ in@@ es@@ s, with self@@ -@@ con@@ tr@@ ol@@ . \n",
            "\n",
            " and this woman was a wi@@ do@@ w of about ei@@ ght@@ y@@ -@@ four year@@ s, who did not depar@@ t from the temple, but ser@@ ved God with f@@ ast@@ ings and pray@@ ers night and day@@ .\n",
            " (@@ as it is writt@@ en, “I have made you a father of many nati@@ on@@ s@@ ”@@ ) in the presence of Him whom he believ@@ ed@@ —@@ God, who gives life to the dead and call@@ s those things which do not ex@@ is@@ t as though they di@@ d;\n",
            " Now there is in Jerusalem by the S@@ he@@ ep G@@ ate a p@@ ool@@ , which is called in H@@ eb@@ re@@ w@@ , Beth@@ es@@ d@@ a, having five p@@ or@@ ch@@ es.\n",
            " Then he go@@ es and tak@@ es with him seven other spir@@ its more wi@@ cked than himself, and they enter and dwell ther@@ e; and the last st@@ ate of that man is wor@@ se than the first@@ . So shall it also be with this wi@@ cked gener@@ ation@@ .”\n",
            "\n",
            "==> Luhyia/dev.bpe.lh <==\n",
            " Kho niba@@ yila omundu owali omubo@@ fu oyo khu,@@ Abafari@@ sa@@ yo@@ . \n",
            " Ne yesi yesi,@@ ou@@ m@@ enya ne nas@@ uu@@ bila mw@@ isie, shi@@ ali@@ fwa tawe. O@@ suubil@@ a,@@ ako@@ ?” \n",
            " Mana nak@@ u@@ ru@@ sia,@@ khumusal@@ aba, nak@@ u@@ f@@ im@@ ba@@ khwo is@@ anda ye@@ ik@@ it@@ ani, ne,@@ nak@@ u@@ yab@@ ila mu@@ ng'@@ ani, ey@@ ali niya@@ yab@@ wa mul@@ w@@ anda y@@ omundu yesi yali nashili okhu@@ yab@@ il@@ w@@ amwo tawe. \n",
            " Olwa khwali khul@@ uk@@ uku ni@@ khwi@@ hon@@ oko@@ os@@ he khw@@ amala ni@@ khumany@@ a mbu, eshi@@ khala@@ chinga eshi@@ o,@@ shil@@ angwa mbu M@@ al@@ it@@ a. \n",
            " Nebutswa omukhasi al@@ ah@@ oni@@ bwa khulw@@ okhwi@@ bul@@ a,@@ aba@@ ana, naba natsi@@ ililila okhw@@ if@@ w@@ ila mubusuubili mubu@@ heel@@ i, nobu@@ takati@@ fu nende obw@@ it@@ e@@ mb@@ elesi@@ .\n",
            "\n",
            " Khandi yam@@ enya nali omule@@ khwa khulwemi@@ y@@ ik@@ a,@@ amakhumi mu@@ n@@ ane na@@ chin@@ e. Nebutswa emi@@ yika echi@@ o,@@ chi@@ osi, yam@@ enyanga butswa muhekalu@@ . Y@@ enam@@ ilanga,@@ OMWAMI Nyasaye eshilo neshi@@ te@@ er@@ e, nah@@ onga inz@@ ala,@@ nende okhusa@@ aya@@ . \n",
            " shinga olwa Amahandik@@ o,@@ kaboolanga mbu, “E@@ khu@@ kholile iwe okhuba s@@ amw@@ ana,@@ Am@@ ahanga am@@ anj@@ i.” Kho obusuubisie buno nobul@@ ayi imbeli,@@ wa Nyasaye, owa Aburahamu ya@@ su@@ bil@@ amwo oul@@ amu@@ s@@ injia,@@ aba@@ fu, khandi owa li@@ khuw@@ ali@@ e li@@ kholanga ebil@@ aliho t@@ a,okhu@@ ba@@ ho. \n",
            " N@@ ali e@@ bwen@@ eyo, li@@ ali@@ yo l@@ iti@@ ba,@@ el@@ il@@ angwa mulu@@ he@@ bur@@ ania mbu, B@@ etsi@@ z@@ at@@ sa, aham@@ bi,@@ khushi@@ li@@ bwa shil@@ angwa mbu, Eshi@@ amak@@ on@@ di@@ . L@@ iti@@ ba,@@ li@@ amaatsi elo liali nebi@@ ro@@ ok@@ oola bir@@ ano. \n",
            " nishi@@ kalu@@ kh@@ ayo shi@@ tsia okhul@@ anga ebishieno b@@ ind@@ i,@@ musafu ebi@@ bi muno, nibi@@ chel@@ ela okhum@@ eny@@ amw@@ o. Ne,@@ olunyuma lw@@ okhumw@@ injil@@ amw@@ o, omundu tsana aba obu@@ bi,@@ okhushila@@ khwo shinga olwa yali olw@@ amb@@ eli. A@@ ko nik@@ o,@@ ak@@ atsia okhwi@@ kholekha khubandu b@@ olwibulo ol@@ um@@ ayan@@ u,@@ lwa bul@@ ano@@ .” N@@ y@@ ina Yesu nende abaana bab@@ we, \n",
            "\n",
            "==> Luhyia/dev.en <==\n",
            " They brought him who formerly was blind to the Pharisees.\n",
            " And whoever lives and believes in Me shall never die. Do you believe this?”\n",
            " Then he took it down, wrapped it in linen, and laid it in a tomb that was hewn out of the rock, where no one had ever lain before.\n",
            " Now when they had escaped, they then found out that the island was called Malta.\n",
            " Nevertheless she will be saved in childbearing if they continue in faith, love, and holiness, with self-control. \n",
            "\n",
            " and this woman was a widow of about eighty-four years, who did not depart from the temple, but served God with fastings and prayers night and day.\n",
            " (as it is written, “I have made you a father of many nations”) in the presence of Him whom he believed—God, who gives life to the dead and calls those things which do not exist as though they did;\n",
            " Now there is in Jerusalem by the Sheep Gate a pool, which is called in Hebrew, Bethesda, having five porches.\n",
            " Then he goes and takes with him seven other spirits more wicked than himself, and they enter and dwell there; and the last state of that man is worse than the first. So shall it also be with this wicked generation.”\n",
            "\n",
            "==> Luhyia/dev.lh <==\n",
            " Kho nibayila omundu owali omubofu oyo khu,Abafarisayo. \n",
            " Ne yesi yesi,oumenya ne nasuubila mwisie, shialifwa tawe. Osuubila,ako?” \n",
            " Mana nakurusia,khumusalaba, nakufimbakhwo isanda yeikitani, ne,nakuyabila mung'ani, eyali niyayabwa mulwanda yomundu yesi yali nashili okhuyabilwamwo tawe. \n",
            " Olwa khwali khulukuku nikhwihonokooshe khwamala nikhumanya mbu, eshikhalachinga eshio,shilangwa mbu Malita. \n",
            " Nebutswa omukhasi alahonibwa khulwokhwibula,abaana, naba natsiililila okhwifwila mubusuubili mubuheeli, nobutakatifu nende obwitembelesi.\n",
            "\n",
            " Khandi yamenya nali omulekhwa khulwemiyika,amakhumi munane nachine. Nebutswa emiyika echio,chiosi, yamenyanga butswa muhekalu. Yenamilanga,OMWAMI Nyasaye eshilo neshiteere, nahonga inzala,nende okhusaaya. \n",
            " shinga olwa Amahandiko,kaboolanga mbu, “Ekhukholile iwe okhuba samwana,Amahanga amanji.” Kho obusuubisie buno nobulayi imbeli,wa Nyasaye, owa Aburahamu yasubilamwo oulamusinjia,abafu, khandi owa likhuwalie likholanga ebilaliho ta,okhubaho. \n",
            " Nali ebweneyo, lialiyo litiba,elilangwa muluheburania mbu, Betsizatsa, ahambi,khushilibwa shilangwa mbu, Eshiamakondi. Litiba,liamaatsi elo liali nebirookoola birano. \n",
            " nishikalukhayo shitsia okhulanga ebishieno bindi,musafu ebibi muno, nibichelela okhumenyamwo. Ne,olunyuma lwokhumwinjilamwo, omundu tsana aba obubi,okhushilakhwo shinga olwa yali olwambeli. Ako niko,akatsia okhwikholekha khubandu bolwibulo olumayanu,lwa bulano.” Nyina Yesu nende abaana babwe, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_D0BqDaUOF_"
      },
      "source": [
        "pre = '/content/gdrive/Shared drives/NMT_for_African_Language/'\n",
        "# Train data source\n",
        "filenames = [pre+'Kinyarwanda/train.en',pre+'Luhyia/train.en']\n",
        "\n",
        "# Train data target\n",
        "filenames2 = [pre+'Kinyarwanda/train.rw',pre+'Luhyia/train.lh']\n",
        "\n",
        "# Dev data source\n",
        "file1 = [pre+'Kinyarwanda/dev.en',pre+'Luhyia/dev.en']\n",
        "\n",
        "# Dev data target\n",
        "file2 = [pre+'Kinyarwanda/dev.rw',pre+'Luhyia/dev.lh']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icPaGn2GlpM-"
      },
      "source": [
        "# Changing to Multilingual directory\n",
        "os.chdir(\"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5zhE2IimjPs"
      },
      "source": [
        "# Procedure to create concatenated files\n",
        "def create_file(x,filename):\n",
        "  # Open filename in write mode\n",
        "  with open(filename, 'w') as outfile:\n",
        "      for names in x:\n",
        "          # Open each file in read mode\n",
        "          with open(names) as infile:\n",
        "              # read the data and write it in file3\n",
        "              outfile.write(infile.read())\n",
        "          outfile.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay8047V3oyeG"
      },
      "source": [
        "create_file(filenames,'train.en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCp4K92o-9_"
      },
      "source": [
        "create_file(filenames2,'train.rw_lh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnEREgvqp2SS"
      },
      "source": [
        "create_file(file1,'dev.en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdwjw7VvqD4C"
      },
      "source": [
        "create_file(file2,'dev.rw_lh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VouMO0lishSQ"
      },
      "source": [
        "### BPE codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5lK1btbKswMN",
        "outputId": "14a517a0-6814-440f-aec6-f8e671c0926a"
      },
      "source": [
        "#! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual3/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
            "Collecting numpy==1.20.1\n",
            "  Downloading numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 94 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.2.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.8.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.5.0)\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 27.6 MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 40.5 MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "  Downloading pylint-2.9.5-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 23.3 MB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading wrapt-1.11.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.34.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt==1.3) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
            "Collecting mccabe<0.7,>=0.6\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Collecting isort<6,>=4.2.5\n",
            "  Downloading isort-5.9.2-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 50.3 MB/s \n",
            "\u001b[?25hCollecting astroid<2.7,>=2.6.5\n",
            "  Downloading astroid-2.6.5-py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 48.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
            "Collecting typed-ast<1.5,>=1.4.0\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 35.3 MB/s \n",
            "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
            "  Downloading lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
            "Building wheels for collected packages: joeynmt, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=85116 sha256=0536e028f20db5cbee96ecdad3c97d863aa869bcd4a2f5512a62a074c0a64659\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cl9bkyd3/wheels/2e/a9/b9/84da571c44d33793bbb649c7f11a265dabc39b91f2462a1333\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68452 sha256=267d7dbc6d4261d4a23d1cb4e6f3d52341ad7fe1d5061398693eea5752bd6078\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/58/9d/da8bad4545585ca52311498ff677647c95c7b690b3040171f8\n",
            "Successfully built joeynmt wrapt\n",
            "Installing collected packages: six, wrapt, typed-ast, numpy, lazy-object-proxy, portalocker, mccabe, isort, astroid, torchtext, subword-nmt, sacrebleu, pyyaml, pylint, joeynmt\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.0+cu101 which is incompatible.\n",
            "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
            "tensorflow 2.5.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "tensorflow 2.5.0 requires wrapt~=1.12.1, but you have wrapt 1.11.1 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.8 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.26.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed astroid-2.6.5 isort-5.9.2 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.9.5 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOYk3OK4rw6d"
      },
      "source": [
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py train.bpe.$src train.bpe.$tgt --output_path vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGHVoKL-3a8x"
      },
      "source": [
        "# Applying BPE to tests\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test1.$src > test.bpe.en1\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test1.lh > test.bpe.lh\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test2.$src > test.bpe.en2\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test2.rw > test.bpe.rw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj0HdPZ6t5i3",
        "outputId": "3030174b-d0ae-4abc-e8ec-5edeaca3bdbc"
      },
      "source": [
        "# Some output\n",
        "! echo \"BPE Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 vocab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BPE Sentences\n",
            "N@@ asi , n@@ ir@@ ee@@ ba end@@ i , ‘ N@@ iwe w@@ ina , O@@ mw@@ ami ? ’ O@@ mw@@ o@@ yo ok@@ wo , n@@ ik@@ umb@@ ool@@ ela kuri , ‘ N@@ is@@ ie Yesu o@@ wa N@@ azar@@ eti ow@@ os@@ a@@ and@@ inj@@ ia . ’\n",
            "sh@@ ich@@ ila , om@@ uk@@ ha@@ an@@ awe om@@ ut@@ el@@ wa , ow@@ em@@ iy@@ ika ek@@ hum@@ i n@@ ach@@ ib@@ ili y@@ ali n@@ any@@ ir@@ anga . N@@ e ol@@ wa y@@ ali nat@@ s@@ it@@ s@@ anga , aband@@ u , bam@@ wib@@ um@@ bak@@ h@@ wo okh@@ ur@@ ula mut@@ s@@ imb@@ eka t@@ si@@ o@@ si .\n",
            "N@@ e ol@@ wa kab@@ is@@ ib@@ wa mbu kh@@ uk@@ ho@@ y@@ ile okh@@ uts@@ i@@ ila , m@@ um@@ e@@ eli okh@@ u@@ ula It@@ al@@ ia , bah@@ a@@ ana Paul@@ o nende abab@@ o@@ he , bandi k@@ hum@@ us@@ inj@@ il@@ ili w@@ el@@ ihe J@@ ul@@ i@@ asi ow@@ e@@ ing@@ '@@ anda e@@ ya , esh@@ ir@@ oma ey@@ il@@ angwa mbu , “ I@@ ng@@ '@@ anda ey@@ il@@ ind@@ anga , O@@ mur@@ uc@@ h@@ i . ”\n",
            "O@@ l@@ uny@@ um@@ akh@@ wo , abak@@ u@@ uka b@@ ef@@ we , abab@@ uk@@ ula l@@ ih@@ e@@ ema el@@ o okh@@ ur@@ ula kh@@ ub@@ as@@ ab@@ we , bal@@ ich@@ inga , okh@@ u@@ ula mut@@ s@@ iny@@ anga t@@ s@@ ia Yo@@ sh@@ wa nib@@ ab@@ uk@@ ula esh@@ i@@ al@@ o , esh@@ ia amahanga aka Nyasaye y@@ al@@ ond@@ anga n@@ ik@@ ar@@ ula imb@@ eli , wab@@ we . N@@ e li@@ am@@ eny@@ ayo okh@@ u@@ ula mut@@ s@@ iny@@ anga t@@ s@@ ia , om@@ ur@@ uc@@ h@@ i D@@ a@@ udi .\n",
            "N@@ e ol@@ wa , y@@ en@@ j@@ il@@ am@@ wo , yab@@ ar@@ ee@@ ba ari , “ Mw@@ ik@@ h@@ u@@ ul@@ anga nim@@ uk@@ h@@ up@@ a , t@@ s@@ imb@@ ungu mb@@ ush@@ i@@ ina ? O@@ mw@@ ana un@@ o shi@@ af@@ w@@ ile ta , hab@@ ula , ak@@ on@@ anga but@@ s@@ wa t@@ sind@@ ool@@ o . ”\n",
            "Combined BPE Vocab\n",
            "Ê@@\n",
            ";@@\n",
            "ʺ\n",
            "Ă@@\n",
            "!@@\n",
            "ϊ@@\n",
            "ointed\n",
            "ḥ\n",
            "̆\n",
            "⁄\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gSynCGijJUlV",
        "outputId": "c318898b-74af-4cb6-b6a4-941a84cb25da"
      },
      "source": [
        "! tail train.*\n",
        "! tail dev.*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.bpe.en <==\n",
            "But if anyone lov@@ es God , this one is known by Him .\n",
            "And the second is like it : ‘ You shall love your neigh@@ b@@ or as yourself . ’\n",
            "until the day in which He was taken up , after He through the H@@ o@@ ly Sp@@ ir@@ it had given command@@ ments to the apostles whom He had cho@@ s@@ en ,\n",
            "For what if some did not believe ? W@@ ill their un@@ beli@@ ef make the faith@@ ful@@ ness of God without ef@@ fect ?\n",
            "And when you go into a hous@@ eho@@ ld , gre@@ et it .\n",
            "And a very great mul@@ t@@ itude sp@@ read their clo@@ th@@ es on the ro@@ ad ; others c@@ ut down br@@ an@@ ch@@ es from the tre@@ es and sp@@ read them on the ro@@ ad .\n",
            "And we heard this vo@@ ice which came from heaven when we were with Him on the holy m@@ ount@@ ain .\n",
            "O@@ r those e@@ igh@@ te@@ en on whom the tower in S@@ il@@ o@@ am f@@ ell and k@@ ill@@ ed them , do you think that they were wor@@ se sin@@ n@@ ers than all other men who dw@@ el@@ t in Jerusalem ?\n",
            "For I be@@ ar him witness that he has a great z@@ eal for you , and those who are in L@@ a@@ od@@ ic@@ ea , and those in H@@ i@@ er@@ ap@@ ol@@ is .\n",
            "\n",
            "\n",
            "==> train.bpe.rw_lh <==\n",
            "N@@ eb@@ uts@@ wa , om@@ undu y@@ esi ou@@ he@@ ela Nyasaye , om@@ w@@ en@@ o@@ yo Nyasaye , yam@@ um@@ anya .\n",
            "El@@ i@@ ak@@ hab@@ ili el@@ ili shing@@ a el@@ o , nd@@ iel@@ ino mbu , ‘ O@@ he@@ el@@ e ow@@ ash@@ io shing@@ a ol@@ wa w@@ ih@@ e@@ ela , om@@ w@@ ene . ’\n",
            "okh@@ u@@ ula , kh@@ uny@@ anga e@@ ya yab@@ uk@@ ul@@ il@@ wak@@ h@@ wo n@@ ay@@ il@@ wa mw@@ ik@@ ul@@ u . N@@ e , n@@ ash@@ ili okh@@ uy@@ il@@ wa mw@@ ik@@ ul@@ u , y@@ ech@@ es@@ ia aba y@@ ali n@@ iy@@ a@@ ah@@ ula , okh@@ uba abar@@ um@@ ebe mub@@ uny@@ ali bwa R@@ o@@ ho O@@ mut@@ ak@@ at@@ if@@ u@@ . ,\n",
            "N@@ eb@@ uts@@ wa abandi kh@@ ub@@ o sh@@ ib@@ ali , abas@@ u@@ ub@@ il@@ wa ta . K@@ ho k@@ ano k@@ akh@@ am@@ any@@ is@@ ia mbu , Nyasaye , shi@@ ali om@@ us@@ u@@ ub@@ il@@ wa ta no@@ ho ?\n",
            "N@@ e ol@@ wa mw@@ inj@@ ila mun@@ z@@ u mub@@ ash@@ i@@ es@@ ie , om@@ ul@@ em@@ be .\n",
            "Ab@@ and@@ u ab@@ anj@@ i nib@@ a@@ ala , eb@@ if@@ wal@@ o bi@@ ab@@ we k@@ hum@@ uh@@ anda , ne abandi nib@@ ar@@ ema amas@@ aka k@@ em@@ is@@ a@@ ala nib@@ a@@ ala k@@ hum@@ uh@@ anda ok@@ wo .\n",
            "K@@ h@@ w@@ ali n@@ ik@@ h@@ ul@@ i , n@@ in@@ aye kh@@ ush@@ ik@@ ul@@ u esh@@ it@@ ak@@ at@@ if@@ u ol@@ wa kh@@ wah@@ ul@@ ila om@@ w@@ o@@ yo , n@@ ik@@ ur@@ ula mw@@ ik@@ ul@@ u e@@ wa Nyasaye .\n",
            "No@@ ho , m@@ up@@ a@@ ar@@ anga mbu , aband@@ u ek@@ hum@@ i nam@@ un@@ ane b@@ om@@ un@@ a@@ ara , kw@@ akw@@ ila nib@@ af@@ wa boo@@ si mul@@ uk@@ ongo l@@ wa S@@ il@@ o@@ amu , b@@ ali , abon@@ o@@ oni okh@@ ush@@ ila aband@@ u boo@@ si abam@@ eny@@ anga mu , Yerusalemu ?\n",
            "Es@@ ie om@@ w@@ ene end@@ i om@@ ut@@ erer@@ eri kh@@ ubuk@@ hal@@ aban@@ ib@@ we , ob@@ ut@@ iny@@ u kh@@ ul@@ w@@ eny@@ we nende kh@@ ul@@ wa aband@@ u ab@@ ali mu , L@@ a@@ od@@ ik@@ ia nende kh@@ ul@@ wa abo ab@@ ali H@@ i@@ er@@ ap@@ ol@@ i .\n",
            "\n",
            "\n",
            "==> train.en <==\n",
            "But if anyone loves God , this one is known by Him .\n",
            "And the second is like it : ‘ You shall love your neighbor as yourself . ’\n",
            "until the day in which He was taken up , after He through the Holy Spirit had given commandments to the apostles whom He had chosen ,\n",
            "For what if some did not believe ? Will their unbelief make the faithfulness of God without effect ?\n",
            "And when you go into a household , greet it .\n",
            "And a very great multitude spread their clothes on the road ; others cut down branches from the trees and spread them on the road .\n",
            "And we heard this voice which came from heaven when we were with Him on the holy mountain .\n",
            "Or those eighteen on whom the tower in Siloam fell and killed them , do you think that they were worse sinners than all other men who dwelt in Jerusalem ?\n",
            "For I bear him witness that he has a great zeal for you , and those who are in Laodicea , and those in Hierapolis .\n",
            "\n",
            "\n",
            "==> train.rw_lh <==\n",
            "Nebutswa , omundu yesi ouheela Nyasaye , omwenoyo Nyasaye , yamumanya .\n",
            "Eliakhabili elili shinga elo , ndielino mbu , ‘ Oheele owashio shinga olwa wiheela , omwene . ’\n",
            "okhuula , khunyanga eya yabukulilwakhwo nayilwa mwikulu . Ne , nashili okhuyilwa mwikulu , yechesia aba yali niyaahula , okhuba abarumebe mubunyali bwa Roho Omutakatifu. ,\n",
            "Nebutswa abandi khubo shibali , abasuubilwa ta . Kho kano kakhamanyisia mbu , Nyasaye , shiali omusuubilwa ta noho ?\n",
            "Ne olwa mwinjila munzu mubashiesie , omulembe .\n",
            "Abandu abanji nibaala , ebifwalo biabwe khumuhanda , ne abandi nibarema amasaka kemisaala nibaala khumuhanda okwo .\n",
            "Khwali nikhuli , ninaye khushikulu eshitakatifu olwa khwahulila omwoyo , nikurula mwikulu ewa Nyasaye .\n",
            "Noho , mupaaranga mbu , abandu ekhumi namunane bomunaara , kwakwila nibafwa boosi mulukongo lwa Siloamu , bali , abonooni okhushila abandu boosi abamenyanga mu , Yerusalemu ?\n",
            "Esie omwene endi omuterereri khubukhalabanibwe , obutinyu khulwenywe nende khulwa abandu abali mu , Laodikia nende khulwa abo abali Hierapoli .\n",
            "\n",
            "==> dev.bpe.en <==\n",
            "I do not say this to con@@ dem@@ n ; for I have said before that you are in our hearts , to d@@ ie together and to live together .\n",
            "So when they were fill@@ ed , He said to His disciples , “ G@@ ather up the f@@ r@@ ag@@ ments that remain , so that nothing is lost . ”\n",
            "When the D@@ ay of P@@ ent@@ ec@@ ost had fully come , they were all with one acc@@ ord in one place .\n",
            "For it has been declar@@ ed to me concer@@ ning you , my bre@@ th@@ ren , by those of Ch@@ lo@@ e ’ s hous@@ eho@@ ld , that there are cont@@ ent@@ ions among you .\n",
            "For “ who has known the mind of the Lord that he may instruc@@ t Him ? ” But we have the mind of Christ .\n",
            "And do not become id@@ ol@@ at@@ ers as were some of them . As it is written , “ The people s@@ at down to eat and dr@@ ink , and ro@@ se up to pl@@ ay . ”\n",
            "Now f@@ ive of them were wise , and f@@ ive were f@@ ool@@ ish .\n",
            "When He op@@ ened the second se@@ al , I heard the second living cre@@ ature saying , “ C@@ ome and see . ”\n",
            "But let none of you suf@@ fer as a mur@@ d@@ er@@ er , a th@@ ief , an ev@@ il@@ do@@ er , or as a bus@@ y@@ body in other people ’ s matters .\n",
            "\n",
            "\n",
            "==> dev.bpe.rw_lh <==\n",
            "Sh@@ i@@ emb@@ ool@@ anga k@@ ano , kh@@ ul@@ wa ok@@ hum@@ uk@@ hal@@ ach@@ ila esh@@ i@@ ina ta , okh@@ uba , shing@@ a , nd@@ amub@@ ool@@ ela kh@@ al@@ e , mul@@ i abah@@ e@@ el@@ wa mun@@ o kh@@ w@@ if@@ we , ne , kh@@ ub@@ et@@ s@@ anga hal@@ ala b@@ ul@@ i l@@ w@@ o@@ si , k@@ ata n@@ ik@@ h@@ uba ab@@ al@@ amu no@@ ho , n@@ ik@@ h@@ uf@@ wa .\n",
            "N@@ e ol@@ wa boo@@ si b@@ ali nib@@ ek@@ ure yab@@ ool@@ ela ab@@ e@@ ech@@ ib@@ e ari , “ Muk@@ h@@ ung@@ '@@ as@@ ie e@@ bit@@ onye bit@@ ony@@ ile , bi@@ o@@ si , k@@ ho mbu kh@@ ul@@ esh@@ e okh@@ us@@ as@@ i@@ akh@@ wo esh@@ ind@@ u shi@@ o@@ si shi@@ o@@ si t@@ awe . ”\n",
            "N@@ e ol@@ wa iny@@ anga ya P@@ end@@ ek@@ ote yo@@ l@@ a , abas@@ u@@ ub@@ ili boo@@ si , bak@@ h@@ ung@@ '@@ ana hab@@ undu hal@@ ala .\n",
            "O@@ kh@@ uba ab@@ a@@ ana b@@ ef@@ we aband@@ u bandi ab@@ om@@ un@@ z@@ u e@@ ya K@@ u@@ lo@@ e b@@ amb@@ ool@@ el@@ e but@@ s@@ wa , hab@@ ul@@ af@@ u mbu , ob@@ us@@ ool@@ o b@@ ul@@ i hak@@ ari mw@@ iny@@ we@@ . ,\n",
            "Sh@@ inga Amah@@ andiko kab@@ ool@@ anga mbu “ N@@ i@@ w@@ i@@ ina o@@ um@@ any@@ ile am@@ ap@@ a@@ aro aka O@@ mw@@ ami ? , N@@ i@@ w@@ i@@ ina oun@@ y@@ ala ok@@ hum@@ uc@@ hel@@ ela ? ” , N@@ eb@@ uts@@ wa if@@ we kh@@ ul@@ i nam@@ ay@@ il@@ il@@ is@@ io aka Kristo .\n",
            "no@@ ho k@@ ata , okh@@ w@@ in@@ am@@ ila eb@@ if@@ wan@@ ani , shing@@ a bal@@ ala kh@@ ub@@ o bak@@ hol@@ a , t@@ awe . Sh@@ inga ol@@ wa Amah@@ andiko kab@@ ool@@ anga mbu , “ Ab@@ and@@ u , b@@ ek@@ hal@@ a hasi okh@@ ul@@ ia l@@ is@@ abo el@@ i@@ am@@ ala lik@@ al@@ uk@@ h@@ ane okh@@ uba , esh@@ if@@ wab@@ wi esh@@ i@@ ob@@ um@@ e@@ esi nende ob@@ uy@@ il@@ ani . ”\n",
            "Bar@@ ano kh@@ ub@@ o , b@@ ali abay@@ ing@@ wa , ne bar@@ ano bandi b@@ ali ab@@ ach@@ esi .\n",
            "Mana E@@ sh@@ im@@ em@@ e shi@@ el@@ ik@@ ond@@ i n@@ ish@@ i@@ ik@@ ula esh@@ ib@@ al@@ ik@@ ho , shi@@ ak@@ hab@@ ili ne n@@ imb@@ ul@@ ila esh@@ il@@ on@@ je esh@@ i@@ ak@@ hab@@ ili esh@@ il@@ im@@ w@@ o@@ yo n@@ ish@@ ib@@ ool@@ a sh@@ iri , “ Y@@ it@@ sa ! ”\n",
            "N@@ eb@@ uts@@ wa om@@ undu y@@ esi kh@@ w@@ iny@@ we al@@ any@@ as@@ ib@@ wa shing@@ a , om@@ uy@@ iri , no@@ ho om@@ w@@ if@@ i , no@@ ho om@@ uk@@ hol@@ i w@@ amak@@ h@@ uwa amab@@ i , no@@ ho om@@ undu we@@ ind@@ ob@@ o@@ yo t@@ awe .\n",
            "\n",
            "\n",
            "==> dev.en <==\n",
            "I do not say this to condemn ; for I have said before that you are in our hearts , to die together and to live together .\n",
            "So when they were filled , He said to His disciples , “ Gather up the fragments that remain , so that nothing is lost . ”\n",
            "When the Day of Pentecost had fully come , they were all with one accord in one place .\n",
            "For it has been declared to me concerning you , my brethren , by those of Chloe ’ s household , that there are contentions among you .\n",
            "For “ who has known the mind of the Lord that he may instruct Him ? ” But we have the mind of Christ .\n",
            "And do not become idolaters as were some of them . As it is written , “ The people sat down to eat and drink , and rose up to play . ”\n",
            "Now five of them were wise , and five were foolish .\n",
            "When He opened the second seal , I heard the second living creature saying , “ Come and see . ”\n",
            "But let none of you suffer as a murderer , a thief , an evildoer , or as a busybody in other people ’ s matters .\n",
            "\n",
            "\n",
            "==> dev.rw_lh <==\n",
            "Shiemboolanga kano , khulwa okhumukhalachila eshiina ta , okhuba , shinga , ndamuboolela khale , muli abaheelwa muno khwifwe , ne , khubetsanga halala buli lwosi , kata nikhuba abalamu noho , nikhufwa .\n",
            "Ne olwa boosi bali nibekure yaboolela abeechibe ari , “ Mukhung'asie ebitonye bitonyile , biosi , kho mbu khuleshe okhusasiakhwo eshindu shiosi shiosi tawe . ”\n",
            "Ne olwa inyanga ya Pendekote yola , abasuubili boosi , bakhung'ana habundu halala .\n",
            "Okhuba abaana befwe abandu bandi abomunzu eya Kuloe bamboolele butswa , habulafu mbu , obusoolo buli hakari mwinywe. ,\n",
            "Shinga Amahandiko kaboolanga mbu “ Niwiina oumanyile amapaaro aka Omwami ? , Niwiina ounyala okhumuchelela ? ” , Nebutswa ifwe khuli namayililisio aka Kristo .\n",
            "noho kata , okhwinamila ebifwanani , shinga balala khubo bakhola , tawe . Shinga olwa Amahandiko kaboolanga mbu , “ Abandu , bekhala hasi okhulia lisabo eliamala likalukhane okhuba , eshifwabwi eshiobumeesi nende obuyilani . ”\n",
            "Barano khubo , bali abayingwa , ne barano bandi bali abachesi .\n",
            "Mana Eshimeme shielikondi nishiikula eshibalikho , shiakhabili ne nimbulila eshilonje eshiakhabili eshilimwoyo nishiboola shiri , “ Yitsa ! ”\n",
            "Nebutswa omundu yesi khwinywe alanyasibwa shinga , omuyiri , noho omwifi , noho omukholi wamakhuwa amabi , noho omundu weindoboyo tawe .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9-N3k9-OO9-"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plc04U6hOQ9u"
      },
      "source": [
        "### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8Smh87evI9G",
        "cellView": "code"
      },
      "source": [
        "#@title\n",
        "name = '%s%s' % (target_language, source_language)\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{target_language}{source_language}_reverse_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{target_language}\"\n",
        "    trg: \"{source_language}\"\n",
        "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
        "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
        "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
        "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 1000\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 200\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_reverse_transformer\"\n",
        "    overwrite: True \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=\"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3\", source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_reverse_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej5aVkIwwHu7",
        "outputId": "ce0d9dd3-9403-4672-ecd7-d5d1cab55ca4"
      },
      "source": [
        "# Train the model\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_reverse_$tgt$src.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-13 10:50:31,579 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-13 10:50:31,657 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-13 10:50:42,167 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-13 10:50:42,507 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-13 10:50:42,605 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-13 10:50:43,790 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-13 10:50:43,790 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-13 10:50:44,196 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-13 10:50:44.460578: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-13 10:50:46,651 - INFO - joeynmt.training - Total params: 12177920\n",
            "2021-07-13 10:50:55,312 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
            "2021-07-13 10:50:55,312 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
            "2021-07-13 10:50:55,312 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-13 10:50:55,312 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
            "2021-07-13 10:50:55,313 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
            "2021-07-13 10:50:55,313 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
            "2021-07-13 10:50:55,313 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-13 10:50:55,313 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-13 10:50:55,314 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-13 10:50:55,314 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-13 10:50:55,314 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-13 10:50:55,315 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-13 10:50:55,315 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-13 10:50:55,315 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-13 10:50:55,316 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-13 10:50:55,316 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-13 10:50:55,316 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-13 10:50:55,316 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-13 10:50:55,317 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-13 10:50:55,317 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-13 10:50:55,317 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-13 10:50:55,317 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-13 10:50:55,317 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-13 10:50:55,318 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-13 10:50:55,318 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-13 10:50:55,318 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-13 10:50:55,318 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-13 10:50:55,319 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-13 10:50:55,319 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-13 10:50:55,319 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-13 10:50:55,319 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-13 10:50:55,320 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-13 10:50:55,320 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-13 10:50:55,320 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-13 10:50:55,320 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-13 10:50:55,321 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-13 10:50:55,321 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-13 10:50:55,321 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer\n",
            "2021-07-13 10:50:55,321 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-13 10:50:55,322 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-13 10:50:55,322 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-13 10:50:55,322 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-13 10:50:55,322 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-13 10:50:55,322 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-13 10:50:55,323 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-13 10:50:55,323 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-13 10:50:55,323 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-13 10:50:55,323 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-13 10:50:55,324 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-13 10:50:55,324 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-13 10:50:55,324 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-13 10:50:55,324 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-13 10:50:55,325 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-13 10:50:55,325 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-13 10:50:55,325 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 10:50:55,325 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-13 10:50:55,326 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-13 10:50:55,326 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-13 10:50:55,326 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-13 10:50:55,326 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-13 10:50:55,327 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-13 10:50:55,327 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-13 10:50:55,327 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-13 10:50:55,327 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-13 10:50:55,328 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-13 10:50:55,328 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-13 10:50:55,328 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-13 10:50:55,328 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-13 10:50:55,329 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-13 10:50:55,329 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 439996,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-13 10:50:55,329 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
            "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
            "2021-07-13 10:50:55,329 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 10:50:55,330 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-13 10:50:55,330 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
            "2021-07-13 10:50:55,330 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
            "2021-07-13 10:50:55,331 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
            "2021-07-13 10:50:55,344 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-13 10:50:55,345 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-13 10:51:55,294 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.422735, Tokens per Sec:     7286, Lr: 0.000300\n",
            "2021-07-13 10:52:54,697 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     5.018930, Tokens per Sec:     7441, Lr: 0.000300\n",
            "2021-07-13 10:53:53,988 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.486201, Tokens per Sec:     7342, Lr: 0.000300\n",
            "2021-07-13 10:54:53,341 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.294386, Tokens per Sec:     7276, Lr: 0.000300\n",
            "2021-07-13 10:55:52,910 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.275492, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-13 10:56:52,013 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.381277, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-13 10:57:51,898 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.254271, Tokens per Sec:     7325, Lr: 0.000300\n",
            "2021-07-13 10:58:51,309 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.373857, Tokens per Sec:     7299, Lr: 0.000300\n",
            "2021-07-13 10:59:51,074 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.181458, Tokens per Sec:     7253, Lr: 0.000300\n",
            "2021-07-13 11:00:50,879 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.985414, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-13 11:01:50,267 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.792715, Tokens per Sec:     7395, Lr: 0.000300\n",
            "2021-07-13 11:02:50,007 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     4.026135, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-13 11:03:49,456 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.693944, Tokens per Sec:     7288, Lr: 0.000300\n",
            "2021-07-13 11:04:48,753 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.653280, Tokens per Sec:     7327, Lr: 0.000300\n",
            "2021-07-13 11:05:48,397 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.868564, Tokens per Sec:     7301, Lr: 0.000300\n",
            "2021-07-13 11:06:47,797 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.271598, Tokens per Sec:     7365, Lr: 0.000300\n",
            "2021-07-13 11:07:48,104 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.366094, Tokens per Sec:     7334, Lr: 0.000300\n",
            "2021-07-13 11:08:47,920 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.531315, Tokens per Sec:     7344, Lr: 0.000300\n",
            "2021-07-13 11:09:47,321 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     3.233364, Tokens per Sec:     7311, Lr: 0.000300\n",
            "2021-07-13 11:10:46,983 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     3.456711, Tokens per Sec:     7295, Lr: 0.000300\n",
            "2021-07-13 11:11:46,828 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     3.466134, Tokens per Sec:     7382, Lr: 0.000300\n",
            "2021-07-13 11:12:47,094 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     3.356965, Tokens per Sec:     7419, Lr: 0.000300\n",
            "2021-07-13 11:13:46,147 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     3.471561, Tokens per Sec:     7310, Lr: 0.000300\n",
            "2021-07-13 11:14:45,296 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     3.222565, Tokens per Sec:     7363, Lr: 0.000300\n",
            "2021-07-13 11:15:44,669 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     3.324484, Tokens per Sec:     7392, Lr: 0.000300\n",
            "2021-07-13 11:18:00,861 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:18:00,862 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:18:00,862 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:18:01,658 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:18:01,658 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:18:02,564 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:18:02,566 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 11:18:02,566 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 11:18:02,566 - INFO - joeynmt.training - \tHypothesis: I was a heart .\n",
            "2021-07-13 11:18:02,567 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:18:02,567 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 11:18:02,567 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 11:18:02,568 - INFO - joeynmt.training - \tHypothesis: The Greek writers were written in the text of the weeds of the weeds .\n",
            "2021-07-13 11:18:02,568 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:18:02,569 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 11:18:02,569 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 11:18:02,569 - INFO - joeynmt.training - \tHypothesis: Instead , we must be able to be a faith or to God , we should be able to imitate the Word of the Bible . ​ — Romans 8 : 28 - 30 .\n",
            "2021-07-13 11:18:02,569 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:18:02,570 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 11:18:02,570 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 11:18:02,571 - INFO - joeynmt.training - \tHypothesis: The same is not a Christian congregation , some of the congregation , some of the world , that Satan is the world .\n",
            "2021-07-13 11:18:02,571 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     5000: bleu:   3.85, loss: 209669.9219, ppl:  28.5287, duration: 137.9015s\n",
            "2021-07-13 11:19:01,642 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     3.150763, Tokens per Sec:     7346, Lr: 0.000300\n",
            "2021-07-13 11:20:01,070 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     3.127223, Tokens per Sec:     7347, Lr: 0.000300\n",
            "2021-07-13 11:20:40,877 - INFO - joeynmt.training - Epoch   1: total training loss 21413.36\n",
            "2021-07-13 11:20:40,877 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-13 11:21:01,427 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     3.006624, Tokens per Sec:     6955, Lr: 0.000300\n",
            "2021-07-13 11:22:00,925 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     3.032935, Tokens per Sec:     7403, Lr: 0.000300\n",
            "2021-07-13 11:23:00,299 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     3.360975, Tokens per Sec:     7409, Lr: 0.000300\n",
            "2021-07-13 11:23:59,519 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     2.878675, Tokens per Sec:     7357, Lr: 0.000300\n",
            "2021-07-13 11:24:59,380 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     2.766648, Tokens per Sec:     7480, Lr: 0.000300\n",
            "2021-07-13 11:25:58,966 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     2.697726, Tokens per Sec:     7437, Lr: 0.000300\n",
            "2021-07-13 11:26:57,790 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     3.077035, Tokens per Sec:     7406, Lr: 0.000300\n",
            "2021-07-13 11:27:57,177 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     2.800082, Tokens per Sec:     7366, Lr: 0.000300\n",
            "2021-07-13 11:28:56,556 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     2.947928, Tokens per Sec:     7403, Lr: 0.000300\n",
            "2021-07-13 11:29:55,416 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     2.969863, Tokens per Sec:     7239, Lr: 0.000300\n",
            "2021-07-13 11:30:54,801 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     3.377084, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-13 11:31:54,550 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     2.951606, Tokens per Sec:     7452, Lr: 0.000300\n",
            "2021-07-13 11:32:53,626 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     2.644552, Tokens per Sec:     7319, Lr: 0.000300\n",
            "2021-07-13 11:33:52,943 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     3.016939, Tokens per Sec:     7394, Lr: 0.000300\n",
            "2021-07-13 11:34:52,303 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     2.640210, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-13 11:35:51,348 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     2.906324, Tokens per Sec:     7342, Lr: 0.000300\n",
            "2021-07-13 11:36:50,597 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     3.275393, Tokens per Sec:     7405, Lr: 0.000300\n",
            "2021-07-13 11:37:50,562 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     2.655473, Tokens per Sec:     7480, Lr: 0.000300\n",
            "2021-07-13 11:38:49,860 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     2.928154, Tokens per Sec:     7301, Lr: 0.000300\n",
            "2021-07-13 11:39:49,584 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     2.873697, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-13 11:40:49,395 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     2.761093, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-13 11:41:48,890 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     2.807665, Tokens per Sec:     7329, Lr: 0.000300\n",
            "2021-07-13 11:42:48,419 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     2.845018, Tokens per Sec:     7340, Lr: 0.000300\n",
            "2021-07-13 11:44:47,240 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 11:44:47,240 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 11:44:47,241 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 11:44:48,025 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 11:44:48,025 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 11:44:49,301 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 11:44:49,303 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 11:44:49,303 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 11:44:49,303 - INFO - joeynmt.training - \tHypothesis: He was a heart .\n",
            "2021-07-13 11:44:49,304 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 11:44:49,304 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 11:44:49,305 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 11:44:49,305 - INFO - joeynmt.training - \tHypothesis: The letter was written in the flock of the week .\n",
            "2021-07-13 11:44:49,305 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 11:44:49,306 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 11:44:49,306 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 11:44:49,306 - INFO - joeynmt.training - \tHypothesis: Instead of being silent or to see , we should continue to continue faith in God through his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 11:44:49,307 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 11:44:49,307 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 11:44:49,308 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 11:44:49,308 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show that some of the Christian congregation are not a different example of Satan’s world .\n",
            "2021-07-13 11:44:49,308 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    10000: bleu:   6.93, loss: 186578.7344, ppl:  19.7247, duration: 120.8882s\n",
            "2021-07-13 11:45:48,630 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     2.543143, Tokens per Sec:     7279, Lr: 0.000300\n",
            "2021-07-13 11:46:47,997 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     2.674425, Tokens per Sec:     7404, Lr: 0.000300\n",
            "2021-07-13 11:47:46,825 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     3.109112, Tokens per Sec:     7304, Lr: 0.000300\n",
            "2021-07-13 11:48:46,666 - INFO - joeynmt.training - Epoch   2, Step:    10800, Batch Loss:     2.641872, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-13 11:49:45,918 - INFO - joeynmt.training - Epoch   2, Step:    11000, Batch Loss:     3.276810, Tokens per Sec:     7420, Lr: 0.000300\n",
            "2021-07-13 11:50:03,834 - INFO - joeynmt.training - Epoch   2: total training loss 16177.72\n",
            "2021-07-13 11:50:03,834 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-13 11:50:45,767 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     2.673029, Tokens per Sec:     7092, Lr: 0.000300\n",
            "2021-07-13 11:51:45,102 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     2.771029, Tokens per Sec:     7411, Lr: 0.000300\n",
            "2021-07-13 11:52:44,879 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     2.517902, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-13 11:53:43,652 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     3.042062, Tokens per Sec:     7311, Lr: 0.000300\n",
            "2021-07-13 11:54:43,397 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     2.521004, Tokens per Sec:     7491, Lr: 0.000300\n",
            "2021-07-13 11:55:42,626 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     2.992766, Tokens per Sec:     7351, Lr: 0.000300\n",
            "2021-07-13 11:56:42,203 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     2.484534, Tokens per Sec:     7487, Lr: 0.000300\n",
            "2021-07-13 11:57:41,804 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     2.354923, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-13 11:58:40,754 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     2.652259, Tokens per Sec:     7327, Lr: 0.000300\n",
            "2021-07-13 11:59:40,208 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     3.034445, Tokens per Sec:     7445, Lr: 0.000300\n",
            "2021-07-13 12:00:39,596 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     2.698354, Tokens per Sec:     7336, Lr: 0.000300\n",
            "2021-07-13 12:01:38,702 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     2.818572, Tokens per Sec:     7265, Lr: 0.000300\n",
            "2021-07-13 12:02:37,733 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     2.843916, Tokens per Sec:     7368, Lr: 0.000300\n",
            "2021-07-13 12:03:37,328 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     2.301717, Tokens per Sec:     7371, Lr: 0.000300\n",
            "2021-07-13 12:04:36,764 - INFO - joeynmt.training - Epoch   3, Step:    14000, Batch Loss:     2.284377, Tokens per Sec:     7381, Lr: 0.000300\n",
            "2021-07-13 12:05:35,976 - INFO - joeynmt.training - Epoch   3, Step:    14200, Batch Loss:     2.633990, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-13 12:06:35,360 - INFO - joeynmt.training - Epoch   3, Step:    14400, Batch Loss:     2.649392, Tokens per Sec:     7392, Lr: 0.000300\n",
            "2021-07-13 12:07:34,580 - INFO - joeynmt.training - Epoch   3, Step:    14600, Batch Loss:     2.828890, Tokens per Sec:     7436, Lr: 0.000300\n",
            "2021-07-13 12:08:33,878 - INFO - joeynmt.training - Epoch   3, Step:    14800, Batch Loss:     2.332023, Tokens per Sec:     7402, Lr: 0.000300\n",
            "2021-07-13 12:09:33,058 - INFO - joeynmt.training - Epoch   3, Step:    15000, Batch Loss:     2.611481, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-13 12:11:53,467 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:11:53,467 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:11:53,467 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:11:54,254 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:11:54,254 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:11:55,165 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:11:55,166 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 12:11:55,167 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 12:11:55,167 - INFO - joeynmt.training - \tHypothesis: I was my heart .\n",
            "2021-07-13 12:11:55,167 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:11:55,168 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 12:11:55,168 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 12:11:55,168 - INFO - joeynmt.training - \tHypothesis: The writer wrote in the wilderness of the wine .\n",
            "2021-07-13 12:11:55,168 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:11:55,169 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 12:11:55,169 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 12:11:55,170 - INFO - joeynmt.training - \tHypothesis: Rather than being sick or see , we should continue to remain in God’s Word through his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 12:11:55,170 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:11:55,171 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 12:11:55,171 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 12:11:55,171 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show that some of the example of the world is not a good part of Satan’s world .\n",
            "2021-07-13 12:11:55,171 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    15000: bleu:   8.03, loss: 176237.1250, ppl:  16.7198, duration: 142.1129s\n",
            "2021-07-13 12:12:54,657 - INFO - joeynmt.training - Epoch   3, Step:    15200, Batch Loss:     2.646403, Tokens per Sec:     7411, Lr: 0.000300\n",
            "2021-07-13 12:13:54,233 - INFO - joeynmt.training - Epoch   3, Step:    15400, Batch Loss:     2.595217, Tokens per Sec:     7483, Lr: 0.000300\n",
            "2021-07-13 12:14:53,450 - INFO - joeynmt.training - Epoch   3, Step:    15600, Batch Loss:     2.478041, Tokens per Sec:     7369, Lr: 0.000300\n",
            "2021-07-13 12:15:52,976 - INFO - joeynmt.training - Epoch   3, Step:    15800, Batch Loss:     2.854590, Tokens per Sec:     7406, Lr: 0.000300\n",
            "2021-07-13 12:16:52,842 - INFO - joeynmt.training - Epoch   3, Step:    16000, Batch Loss:     3.063638, Tokens per Sec:     7485, Lr: 0.000300\n",
            "2021-07-13 12:17:52,496 - INFO - joeynmt.training - Epoch   3, Step:    16200, Batch Loss:     2.483223, Tokens per Sec:     7418, Lr: 0.000300\n",
            "2021-07-13 12:18:51,195 - INFO - joeynmt.training - Epoch   3, Step:    16400, Batch Loss:     2.246221, Tokens per Sec:     7318, Lr: 0.000300\n",
            "2021-07-13 12:19:44,430 - INFO - joeynmt.training - Epoch   3: total training loss 14467.10\n",
            "2021-07-13 12:19:44,430 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-13 12:19:51,072 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     2.398542, Tokens per Sec:     6845, Lr: 0.000300\n",
            "2021-07-13 12:20:50,203 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     2.923926, Tokens per Sec:     7391, Lr: 0.000300\n",
            "2021-07-13 12:21:49,684 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     2.296506, Tokens per Sec:     7456, Lr: 0.000300\n",
            "2021-07-13 12:22:49,031 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     2.504104, Tokens per Sec:     7421, Lr: 0.000300\n",
            "2021-07-13 12:23:48,380 - INFO - joeynmt.training - Epoch   4, Step:    17400, Batch Loss:     2.426107, Tokens per Sec:     7365, Lr: 0.000300\n",
            "2021-07-13 12:24:47,600 - INFO - joeynmt.training - Epoch   4, Step:    17600, Batch Loss:     2.595611, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-13 12:25:46,970 - INFO - joeynmt.training - Epoch   4, Step:    17800, Batch Loss:     2.586021, Tokens per Sec:     7435, Lr: 0.000300\n",
            "2021-07-13 12:26:45,990 - INFO - joeynmt.training - Epoch   4, Step:    18000, Batch Loss:     2.532304, Tokens per Sec:     7339, Lr: 0.000300\n",
            "2021-07-13 12:27:45,267 - INFO - joeynmt.training - Epoch   4, Step:    18200, Batch Loss:     2.423755, Tokens per Sec:     7417, Lr: 0.000300\n",
            "2021-07-13 12:28:44,436 - INFO - joeynmt.training - Epoch   4, Step:    18400, Batch Loss:     2.121600, Tokens per Sec:     7323, Lr: 0.000300\n",
            "2021-07-13 12:29:43,157 - INFO - joeynmt.training - Epoch   4, Step:    18600, Batch Loss:     2.527172, Tokens per Sec:     7395, Lr: 0.000300\n",
            "2021-07-13 12:30:42,211 - INFO - joeynmt.training - Epoch   4, Step:    18800, Batch Loss:     2.508675, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-13 12:31:41,479 - INFO - joeynmt.training - Epoch   4, Step:    19000, Batch Loss:     2.276875, Tokens per Sec:     7479, Lr: 0.000300\n",
            "2021-07-13 12:32:40,152 - INFO - joeynmt.training - Epoch   4, Step:    19200, Batch Loss:     2.346715, Tokens per Sec:     7336, Lr: 0.000300\n",
            "2021-07-13 12:33:39,394 - INFO - joeynmt.training - Epoch   4, Step:    19400, Batch Loss:     2.365159, Tokens per Sec:     7493, Lr: 0.000300\n",
            "2021-07-13 12:34:38,082 - INFO - joeynmt.training - Epoch   4, Step:    19600, Batch Loss:     2.548311, Tokens per Sec:     7421, Lr: 0.000300\n",
            "2021-07-13 12:35:37,474 - INFO - joeynmt.training - Epoch   4, Step:    19800, Batch Loss:     2.324321, Tokens per Sec:     7404, Lr: 0.000300\n",
            "2021-07-13 12:36:36,698 - INFO - joeynmt.training - Epoch   4, Step:    20000, Batch Loss:     2.421255, Tokens per Sec:     7370, Lr: 0.000300\n",
            "2021-07-13 12:39:04,487 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 12:39:04,488 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 12:39:04,488 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 12:39:05,300 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 12:39:05,301 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 12:39:06,229 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 12:39:06,231 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 12:39:06,231 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 12:39:06,231 - INFO - joeynmt.training - \tHypothesis: I did so .\n",
            "2021-07-13 12:39:06,232 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 12:39:06,232 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 12:39:06,233 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 12:39:06,233 - INFO - joeynmt.training - \tHypothesis: The writer wrote in the wilderness of the scroll .\n",
            "2021-07-13 12:39:06,233 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 12:39:06,234 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 12:39:06,234 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 12:39:06,234 - INFO - joeynmt.training - \tHypothesis: Instead of being sick or see , we should keep our faith in God through his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 12:39:06,235 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 12:39:06,235 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 12:39:06,236 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 12:39:06,236 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a certain example that they are made in Satan’s world .\n",
            "2021-07-13 12:39:06,236 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    20000: bleu:   9.33, loss: 167712.2500, ppl:  14.5902, duration: 149.5374s\n",
            "2021-07-13 12:40:06,416 - INFO - joeynmt.training - Epoch   4, Step:    20200, Batch Loss:     2.438175, Tokens per Sec:     7326, Lr: 0.000300\n",
            "2021-07-13 12:41:05,927 - INFO - joeynmt.training - Epoch   4, Step:    20400, Batch Loss:     2.575054, Tokens per Sec:     7280, Lr: 0.000300\n",
            "2021-07-13 12:42:05,768 - INFO - joeynmt.training - Epoch   4, Step:    20600, Batch Loss:     2.415640, Tokens per Sec:     7347, Lr: 0.000300\n",
            "2021-07-13 12:43:05,693 - INFO - joeynmt.training - Epoch   4, Step:    20800, Batch Loss:     2.372867, Tokens per Sec:     7419, Lr: 0.000300\n",
            "2021-07-13 12:44:05,614 - INFO - joeynmt.training - Epoch   4, Step:    21000, Batch Loss:     2.876717, Tokens per Sec:     7369, Lr: 0.000300\n",
            "2021-07-13 12:45:05,140 - INFO - joeynmt.training - Epoch   4, Step:    21200, Batch Loss:     2.099952, Tokens per Sec:     7334, Lr: 0.000300\n",
            "2021-07-13 12:46:04,658 - INFO - joeynmt.training - Epoch   4, Step:    21400, Batch Loss:     2.349805, Tokens per Sec:     7283, Lr: 0.000300\n",
            "2021-07-13 12:47:04,511 - INFO - joeynmt.training - Epoch   4, Step:    21600, Batch Loss:     2.176558, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-13 12:48:03,704 - INFO - joeynmt.training - Epoch   4, Step:    21800, Batch Loss:     2.393855, Tokens per Sec:     7314, Lr: 0.000300\n",
            "2021-07-13 12:49:03,445 - INFO - joeynmt.training - Epoch   4, Step:    22000, Batch Loss:     2.523527, Tokens per Sec:     7368, Lr: 0.000300\n",
            "2021-07-13 12:49:35,772 - INFO - joeynmt.training - Epoch   4: total training loss 13489.15\n",
            "2021-07-13 12:49:35,772 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-13 12:50:03,907 - INFO - joeynmt.training - Epoch   5, Step:    22200, Batch Loss:     2.853983, Tokens per Sec:     7159, Lr: 0.000300\n",
            "2021-07-13 12:51:03,077 - INFO - joeynmt.training - Epoch   5, Step:    22400, Batch Loss:     2.098232, Tokens per Sec:     7275, Lr: 0.000300\n",
            "2021-07-13 12:52:02,474 - INFO - joeynmt.training - Epoch   5, Step:    22600, Batch Loss:     2.227836, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-13 12:53:02,003 - INFO - joeynmt.training - Epoch   5, Step:    22800, Batch Loss:     2.218535, Tokens per Sec:     7369, Lr: 0.000300\n",
            "2021-07-13 12:54:00,994 - INFO - joeynmt.training - Epoch   5, Step:    23000, Batch Loss:     2.562284, Tokens per Sec:     7370, Lr: 0.000300\n",
            "2021-07-13 12:55:00,185 - INFO - joeynmt.training - Epoch   5, Step:    23200, Batch Loss:     2.174851, Tokens per Sec:     7378, Lr: 0.000300\n",
            "2021-07-13 12:55:59,320 - INFO - joeynmt.training - Epoch   5, Step:    23400, Batch Loss:     2.351145, Tokens per Sec:     7349, Lr: 0.000300\n",
            "2021-07-13 12:56:58,617 - INFO - joeynmt.training - Epoch   5, Step:    23600, Batch Loss:     2.512364, Tokens per Sec:     7440, Lr: 0.000300\n",
            "2021-07-13 12:57:57,621 - INFO - joeynmt.training - Epoch   5, Step:    23800, Batch Loss:     2.430239, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-13 12:58:56,319 - INFO - joeynmt.training - Epoch   5, Step:    24000, Batch Loss:     2.612895, Tokens per Sec:     7394, Lr: 0.000300\n",
            "2021-07-13 12:59:55,487 - INFO - joeynmt.training - Epoch   5, Step:    24200, Batch Loss:     2.300022, Tokens per Sec:     7355, Lr: 0.000300\n",
            "2021-07-13 13:00:54,370 - INFO - joeynmt.training - Epoch   5, Step:    24400, Batch Loss:     2.465397, Tokens per Sec:     7466, Lr: 0.000300\n",
            "2021-07-13 13:01:53,233 - INFO - joeynmt.training - Epoch   5, Step:    24600, Batch Loss:     2.244101, Tokens per Sec:     7328, Lr: 0.000300\n",
            "2021-07-13 13:02:52,063 - INFO - joeynmt.training - Epoch   5, Step:    24800, Batch Loss:     2.342327, Tokens per Sec:     7421, Lr: 0.000300\n",
            "2021-07-13 13:03:51,287 - INFO - joeynmt.training - Epoch   5, Step:    25000, Batch Loss:     2.452401, Tokens per Sec:     7448, Lr: 0.000300\n",
            "2021-07-13 13:06:14,689 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:06:14,690 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:06:14,690 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:06:15,460 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:06:15,461 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:06:16,316 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:06:16,317 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 13:06:16,317 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 13:06:16,317 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 13:06:16,317 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:06:16,318 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 13:06:16,318 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 13:06:16,318 - INFO - joeynmt.training - \tHypothesis: The writer wrote in the week on the next scroll .\n",
            "2021-07-13 13:06:16,319 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:06:16,319 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 13:06:16,320 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 13:06:16,320 - INFO - joeynmt.training - \tHypothesis: Rather than being sick or see , we should keep our faith in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 13:06:16,320 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:06:16,321 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 13:06:16,322 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 13:06:16,322 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a certain example of the world of Satan’s world .\n",
            "2021-07-13 13:06:16,322 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    25000: bleu:   9.93, loss: 162152.3281, ppl:  13.3497, duration: 145.0350s\n",
            "2021-07-13 13:07:15,454 - INFO - joeynmt.training - Epoch   5, Step:    25200, Batch Loss:     2.386118, Tokens per Sec:     7432, Lr: 0.000300\n",
            "2021-07-13 13:08:14,372 - INFO - joeynmt.training - Epoch   5, Step:    25400, Batch Loss:     2.461610, Tokens per Sec:     7407, Lr: 0.000300\n",
            "2021-07-13 13:09:13,169 - INFO - joeynmt.training - Epoch   5, Step:    25600, Batch Loss:     2.330096, Tokens per Sec:     7502, Lr: 0.000300\n",
            "2021-07-13 13:10:11,730 - INFO - joeynmt.training - Epoch   5, Step:    25800, Batch Loss:     2.205730, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-13 13:11:10,364 - INFO - joeynmt.training - Epoch   5, Step:    26000, Batch Loss:     2.448589, Tokens per Sec:     7474, Lr: 0.000300\n",
            "2021-07-13 13:12:09,257 - INFO - joeynmt.training - Epoch   5, Step:    26200, Batch Loss:     2.043302, Tokens per Sec:     7430, Lr: 0.000300\n",
            "2021-07-13 13:13:08,166 - INFO - joeynmt.training - Epoch   5, Step:    26400, Batch Loss:     2.278671, Tokens per Sec:     7484, Lr: 0.000300\n",
            "2021-07-13 13:14:06,669 - INFO - joeynmt.training - Epoch   5, Step:    26600, Batch Loss:     2.252606, Tokens per Sec:     7448, Lr: 0.000300\n",
            "2021-07-13 13:15:05,992 - INFO - joeynmt.training - Epoch   5, Step:    26800, Batch Loss:     2.254345, Tokens per Sec:     7513, Lr: 0.000300\n",
            "2021-07-13 13:16:05,014 - INFO - joeynmt.training - Epoch   5, Step:    27000, Batch Loss:     2.425315, Tokens per Sec:     7491, Lr: 0.000300\n",
            "2021-07-13 13:17:03,835 - INFO - joeynmt.training - Epoch   5, Step:    27200, Batch Loss:     2.518034, Tokens per Sec:     7408, Lr: 0.000300\n",
            "2021-07-13 13:18:02,295 - INFO - joeynmt.training - Epoch   5, Step:    27400, Batch Loss:     2.324431, Tokens per Sec:     7316, Lr: 0.000300\n",
            "2021-07-13 13:19:00,972 - INFO - joeynmt.training - Epoch   5, Step:    27600, Batch Loss:     2.393028, Tokens per Sec:     7547, Lr: 0.000300\n",
            "2021-07-13 13:19:14,255 - INFO - joeynmt.training - Epoch   5: total training loss 12848.20\n",
            "2021-07-13 13:19:14,256 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-13 13:20:00,109 - INFO - joeynmt.training - Epoch   6, Step:    27800, Batch Loss:     2.055386, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-13 13:20:59,147 - INFO - joeynmt.training - Epoch   6, Step:    28000, Batch Loss:     2.309330, Tokens per Sec:     7509, Lr: 0.000300\n",
            "2021-07-13 13:21:57,881 - INFO - joeynmt.training - Epoch   6, Step:    28200, Batch Loss:     2.528515, Tokens per Sec:     7455, Lr: 0.000300\n",
            "2021-07-13 13:22:57,102 - INFO - joeynmt.training - Epoch   6, Step:    28400, Batch Loss:     2.202049, Tokens per Sec:     7507, Lr: 0.000300\n",
            "2021-07-13 13:23:55,557 - INFO - joeynmt.training - Epoch   6, Step:    28600, Batch Loss:     2.076091, Tokens per Sec:     7371, Lr: 0.000300\n",
            "2021-07-13 13:24:54,966 - INFO - joeynmt.training - Epoch   6, Step:    28800, Batch Loss:     2.139910, Tokens per Sec:     7597, Lr: 0.000300\n",
            "2021-07-13 13:25:53,870 - INFO - joeynmt.training - Epoch   6, Step:    29000, Batch Loss:     2.647105, Tokens per Sec:     7474, Lr: 0.000300\n",
            "2021-07-13 13:26:51,994 - INFO - joeynmt.training - Epoch   6, Step:    29200, Batch Loss:     1.969639, Tokens per Sec:     7382, Lr: 0.000300\n",
            "2021-07-13 13:27:50,885 - INFO - joeynmt.training - Epoch   6, Step:    29400, Batch Loss:     2.080487, Tokens per Sec:     7478, Lr: 0.000300\n",
            "2021-07-13 13:28:49,666 - INFO - joeynmt.training - Epoch   6, Step:    29600, Batch Loss:     2.362475, Tokens per Sec:     7500, Lr: 0.000300\n",
            "2021-07-13 13:29:48,586 - INFO - joeynmt.training - Epoch   6, Step:    29800, Batch Loss:     1.944575, Tokens per Sec:     7505, Lr: 0.000300\n",
            "2021-07-13 13:30:47,055 - INFO - joeynmt.training - Epoch   6, Step:    30000, Batch Loss:     2.073451, Tokens per Sec:     7448, Lr: 0.000300\n",
            "2021-07-13 13:32:46,027 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:32:46,027 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:32:46,027 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:32:46,829 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:32:46,829 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:32:48,069 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:32:48,070 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 13:32:48,071 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 13:32:48,071 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 13:32:48,071 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:32:48,072 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 13:32:48,072 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 13:32:48,072 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-13 13:32:48,073 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:32:48,073 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 13:32:48,074 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 13:32:48,074 - INFO - joeynmt.training - \tHypothesis: Rather than being tempted or see , we should continue to remain in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 13:32:48,074 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:32:48,075 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 13:32:48,075 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 13:32:48,076 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a certain example of being made in Satan’s world .\n",
            "2021-07-13 13:32:48,076 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step    30000: bleu:  11.15, loss: 157637.4219, ppl:  12.4204, duration: 121.0203s\n",
            "2021-07-13 13:33:47,275 - INFO - joeynmt.training - Epoch   6, Step:    30200, Batch Loss:     2.211873, Tokens per Sec:     7421, Lr: 0.000300\n",
            "2021-07-13 13:34:45,591 - INFO - joeynmt.training - Epoch   6, Step:    30400, Batch Loss:     2.040225, Tokens per Sec:     7255, Lr: 0.000300\n",
            "2021-07-13 13:35:45,132 - INFO - joeynmt.training - Epoch   6, Step:    30600, Batch Loss:     2.154772, Tokens per Sec:     7507, Lr: 0.000300\n",
            "2021-07-13 13:36:43,941 - INFO - joeynmt.training - Epoch   6, Step:    30800, Batch Loss:     2.103831, Tokens per Sec:     7306, Lr: 0.000300\n",
            "2021-07-13 13:37:43,085 - INFO - joeynmt.training - Epoch   6, Step:    31000, Batch Loss:     2.303462, Tokens per Sec:     7465, Lr: 0.000300\n",
            "2021-07-13 13:38:41,832 - INFO - joeynmt.training - Epoch   6, Step:    31200, Batch Loss:     2.431363, Tokens per Sec:     7443, Lr: 0.000300\n",
            "2021-07-13 13:39:41,026 - INFO - joeynmt.training - Epoch   6, Step:    31400, Batch Loss:     2.174099, Tokens per Sec:     7491, Lr: 0.000300\n",
            "2021-07-13 13:40:40,326 - INFO - joeynmt.training - Epoch   6, Step:    31600, Batch Loss:     2.231938, Tokens per Sec:     7523, Lr: 0.000300\n",
            "2021-07-13 13:41:38,942 - INFO - joeynmt.training - Epoch   6, Step:    31800, Batch Loss:     2.204005, Tokens per Sec:     7443, Lr: 0.000300\n",
            "2021-07-13 13:42:37,457 - INFO - joeynmt.training - Epoch   6, Step:    32000, Batch Loss:     2.032291, Tokens per Sec:     7381, Lr: 0.000300\n",
            "2021-07-13 13:43:36,506 - INFO - joeynmt.training - Epoch   6, Step:    32200, Batch Loss:     2.305456, Tokens per Sec:     7465, Lr: 0.000300\n",
            "2021-07-13 13:44:36,061 - INFO - joeynmt.training - Epoch   6, Step:    32400, Batch Loss:     2.122708, Tokens per Sec:     7495, Lr: 0.000300\n",
            "2021-07-13 13:45:34,869 - INFO - joeynmt.training - Epoch   6, Step:    32600, Batch Loss:     1.949971, Tokens per Sec:     7380, Lr: 0.000300\n",
            "2021-07-13 13:46:33,805 - INFO - joeynmt.training - Epoch   6, Step:    32800, Batch Loss:     2.528174, Tokens per Sec:     7467, Lr: 0.000300\n",
            "2021-07-13 13:47:32,692 - INFO - joeynmt.training - Epoch   6, Step:    33000, Batch Loss:     1.998030, Tokens per Sec:     7383, Lr: 0.000300\n",
            "2021-07-13 13:48:20,607 - INFO - joeynmt.training - Epoch   6: total training loss 12330.17\n",
            "2021-07-13 13:48:20,617 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-13 13:48:32,666 - INFO - joeynmt.training - Epoch   7, Step:    33200, Batch Loss:     2.019690, Tokens per Sec:     7112, Lr: 0.000300\n",
            "2021-07-13 13:49:32,004 - INFO - joeynmt.training - Epoch   7, Step:    33400, Batch Loss:     2.054083, Tokens per Sec:     7564, Lr: 0.000300\n",
            "2021-07-13 13:50:30,796 - INFO - joeynmt.training - Epoch   7, Step:    33600, Batch Loss:     2.063499, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-13 13:51:29,561 - INFO - joeynmt.training - Epoch   7, Step:    33800, Batch Loss:     2.291883, Tokens per Sec:     7411, Lr: 0.000300\n",
            "2021-07-13 13:52:28,260 - INFO - joeynmt.training - Epoch   7, Step:    34000, Batch Loss:     2.474647, Tokens per Sec:     7350, Lr: 0.000300\n",
            "2021-07-13 13:53:27,313 - INFO - joeynmt.training - Epoch   7, Step:    34200, Batch Loss:     2.623296, Tokens per Sec:     7460, Lr: 0.000300\n",
            "2021-07-13 13:54:26,330 - INFO - joeynmt.training - Epoch   7, Step:    34400, Batch Loss:     2.171820, Tokens per Sec:     7367, Lr: 0.000300\n",
            "2021-07-13 13:55:25,254 - INFO - joeynmt.training - Epoch   7, Step:    34600, Batch Loss:     2.343518, Tokens per Sec:     7459, Lr: 0.000300\n",
            "2021-07-13 13:56:23,889 - INFO - joeynmt.training - Epoch   7, Step:    34800, Batch Loss:     2.080966, Tokens per Sec:     7364, Lr: 0.000300\n",
            "2021-07-13 13:57:22,818 - INFO - joeynmt.training - Epoch   7, Step:    35000, Batch Loss:     2.080535, Tokens per Sec:     7413, Lr: 0.000300\n",
            "2021-07-13 13:59:32,877 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 13:59:32,877 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 13:59:32,877 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 13:59:33,641 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 13:59:33,643 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 13:59:34,487 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 13:59:34,488 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 13:59:34,488 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 13:59:34,488 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 13:59:34,489 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 13:59:34,489 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 13:59:34,490 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 13:59:34,490 - INFO - joeynmt.training - \tHypothesis: The writer was written in the front of the scroll .\n",
            "2021-07-13 13:59:34,490 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 13:59:34,491 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 13:59:34,491 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 13:59:34,491 - INFO - joeynmt.training - \tHypothesis: Rather than being tempted or see , we should keep our faith by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 13:59:34,492 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 13:59:34,492 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 13:59:34,494 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 13:59:34,494 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of example , that they have been made clear in Satan’s world .\n",
            "2021-07-13 13:59:34,495 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step    35000: bleu:  11.75, loss: 155305.0781, ppl:  11.9659, duration: 131.6766s\n",
            "2021-07-13 14:00:33,248 - INFO - joeynmt.training - Epoch   7, Step:    35200, Batch Loss:     2.237087, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-13 14:01:32,606 - INFO - joeynmt.training - Epoch   7, Step:    35400, Batch Loss:     2.119400, Tokens per Sec:     7501, Lr: 0.000300\n",
            "2021-07-13 14:02:31,876 - INFO - joeynmt.training - Epoch   7, Step:    35600, Batch Loss:     2.354309, Tokens per Sec:     7541, Lr: 0.000300\n",
            "2021-07-13 14:03:30,863 - INFO - joeynmt.training - Epoch   7, Step:    35800, Batch Loss:     2.468952, Tokens per Sec:     7450, Lr: 0.000300\n",
            "2021-07-13 14:04:29,961 - INFO - joeynmt.training - Epoch   7, Step:    36000, Batch Loss:     2.276540, Tokens per Sec:     7437, Lr: 0.000300\n",
            "2021-07-13 14:05:28,321 - INFO - joeynmt.training - Epoch   7, Step:    36200, Batch Loss:     2.033282, Tokens per Sec:     7376, Lr: 0.000300\n",
            "2021-07-13 14:06:27,239 - INFO - joeynmt.training - Epoch   7, Step:    36400, Batch Loss:     2.029487, Tokens per Sec:     7474, Lr: 0.000300\n",
            "2021-07-13 14:07:26,043 - INFO - joeynmt.training - Epoch   7, Step:    36600, Batch Loss:     2.222265, Tokens per Sec:     7342, Lr: 0.000300\n",
            "2021-07-13 14:08:25,074 - INFO - joeynmt.training - Epoch   7, Step:    36800, Batch Loss:     2.306391, Tokens per Sec:     7493, Lr: 0.000300\n",
            "2021-07-13 14:09:24,032 - INFO - joeynmt.training - Epoch   7, Step:    37000, Batch Loss:     2.030288, Tokens per Sec:     7450, Lr: 0.000300\n",
            "2021-07-13 14:10:23,281 - INFO - joeynmt.training - Epoch   7, Step:    37200, Batch Loss:     2.289641, Tokens per Sec:     7441, Lr: 0.000300\n",
            "2021-07-13 14:11:22,493 - INFO - joeynmt.training - Epoch   7, Step:    37400, Batch Loss:     2.179243, Tokens per Sec:     7429, Lr: 0.000300\n",
            "2021-07-13 14:12:22,166 - INFO - joeynmt.training - Epoch   7, Step:    37600, Batch Loss:     1.968992, Tokens per Sec:     7529, Lr: 0.000300\n",
            "2021-07-13 14:13:21,328 - INFO - joeynmt.training - Epoch   7, Step:    37800, Batch Loss:     2.191652, Tokens per Sec:     7358, Lr: 0.000300\n",
            "2021-07-13 14:14:20,639 - INFO - joeynmt.training - Epoch   7, Step:    38000, Batch Loss:     2.078921, Tokens per Sec:     7354, Lr: 0.000300\n",
            "2021-07-13 14:15:19,855 - INFO - joeynmt.training - Epoch   7, Step:    38200, Batch Loss:     1.886166, Tokens per Sec:     7279, Lr: 0.000300\n",
            "2021-07-13 14:16:18,866 - INFO - joeynmt.training - Epoch   7, Step:    38400, Batch Loss:     2.018742, Tokens per Sec:     7303, Lr: 0.000300\n",
            "2021-07-13 14:17:18,022 - INFO - joeynmt.training - Epoch   7, Step:    38600, Batch Loss:     2.002061, Tokens per Sec:     7361, Lr: 0.000300\n",
            "2021-07-13 14:17:44,524 - INFO - joeynmt.training - Epoch   7: total training loss 12011.89\n",
            "2021-07-13 14:17:44,524 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-13 14:18:18,121 - INFO - joeynmt.training - Epoch   8, Step:    38800, Batch Loss:     1.976375, Tokens per Sec:     7281, Lr: 0.000300\n",
            "2021-07-13 14:19:17,135 - INFO - joeynmt.training - Epoch   8, Step:    39000, Batch Loss:     2.220231, Tokens per Sec:     7301, Lr: 0.000300\n",
            "2021-07-13 14:20:16,337 - INFO - joeynmt.training - Epoch   8, Step:    39200, Batch Loss:     2.089898, Tokens per Sec:     7443, Lr: 0.000300\n",
            "2021-07-13 14:21:15,503 - INFO - joeynmt.training - Epoch   8, Step:    39400, Batch Loss:     2.160846, Tokens per Sec:     7313, Lr: 0.000300\n",
            "2021-07-13 14:22:15,086 - INFO - joeynmt.training - Epoch   8, Step:    39600, Batch Loss:     2.161353, Tokens per Sec:     7393, Lr: 0.000300\n",
            "2021-07-13 14:23:14,788 - INFO - joeynmt.training - Epoch   8, Step:    39800, Batch Loss:     2.286709, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-13 14:24:14,342 - INFO - joeynmt.training - Epoch   8, Step:    40000, Batch Loss:     2.007583, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-13 14:26:24,219 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:26:24,220 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:26:24,220 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:26:25,000 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:26:25,000 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:26:25,870 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:26:25,872 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 14:26:25,872 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 14:26:25,872 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 14:26:25,873 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:26:25,873 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 14:26:25,874 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 14:26:25,874 - INFO - joeynmt.training - \tHypothesis: The writer was written in the billion on the side of the scroll .\n",
            "2021-07-13 14:26:25,874 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:26:25,875 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 14:26:25,875 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 14:26:25,876 - INFO - joeynmt.training - \tHypothesis: Rather than being tempted or see , we should continue to strengthen our faith in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 14:26:25,876 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:26:25,878 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 14:26:25,878 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 14:26:25,878 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a similar example , that they are getting up in Satan’s world .\n",
            "2021-07-13 14:26:25,878 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step    40000: bleu:  11.44, loss: 152631.0000, ppl:  11.4653, duration: 131.5353s\n",
            "2021-07-13 14:27:25,304 - INFO - joeynmt.training - Epoch   8, Step:    40200, Batch Loss:     2.198380, Tokens per Sec:     7485, Lr: 0.000300\n",
            "2021-07-13 14:28:25,045 - INFO - joeynmt.training - Epoch   8, Step:    40400, Batch Loss:     2.121310, Tokens per Sec:     7301, Lr: 0.000300\n",
            "2021-07-13 14:29:24,097 - INFO - joeynmt.training - Epoch   8, Step:    40600, Batch Loss:     2.210069, Tokens per Sec:     7460, Lr: 0.000300\n",
            "2021-07-13 14:30:23,870 - INFO - joeynmt.training - Epoch   8, Step:    40800, Batch Loss:     2.734935, Tokens per Sec:     7437, Lr: 0.000300\n",
            "2021-07-13 14:31:23,487 - INFO - joeynmt.training - Epoch   8, Step:    41000, Batch Loss:     2.162668, Tokens per Sec:     7453, Lr: 0.000300\n",
            "2021-07-13 14:32:22,280 - INFO - joeynmt.training - Epoch   8, Step:    41200, Batch Loss:     2.075005, Tokens per Sec:     7245, Lr: 0.000300\n",
            "2021-07-13 14:33:20,820 - INFO - joeynmt.training - Epoch   8, Step:    41400, Batch Loss:     2.003020, Tokens per Sec:     7368, Lr: 0.000300\n",
            "2021-07-13 14:34:20,019 - INFO - joeynmt.training - Epoch   8, Step:    41600, Batch Loss:     2.189381, Tokens per Sec:     7407, Lr: 0.000300\n",
            "2021-07-13 14:35:18,870 - INFO - joeynmt.training - Epoch   8, Step:    41800, Batch Loss:     2.027132, Tokens per Sec:     7366, Lr: 0.000300\n",
            "2021-07-13 14:36:18,192 - INFO - joeynmt.training - Epoch   8, Step:    42000, Batch Loss:     1.739811, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-13 14:37:17,905 - INFO - joeynmt.training - Epoch   8, Step:    42200, Batch Loss:     1.933836, Tokens per Sec:     7573, Lr: 0.000300\n",
            "2021-07-13 14:38:17,212 - INFO - joeynmt.training - Epoch   8, Step:    42400, Batch Loss:     2.187680, Tokens per Sec:     7295, Lr: 0.000300\n",
            "2021-07-13 14:39:16,413 - INFO - joeynmt.training - Epoch   8, Step:    42600, Batch Loss:     2.078966, Tokens per Sec:     7314, Lr: 0.000300\n",
            "2021-07-13 14:40:15,643 - INFO - joeynmt.training - Epoch   8, Step:    42800, Batch Loss:     2.224083, Tokens per Sec:     7273, Lr: 0.000300\n",
            "2021-07-13 14:41:15,109 - INFO - joeynmt.training - Epoch   8, Step:    43000, Batch Loss:     2.170304, Tokens per Sec:     7453, Lr: 0.000300\n",
            "2021-07-13 14:42:14,566 - INFO - joeynmt.training - Epoch   8, Step:    43200, Batch Loss:     2.227552, Tokens per Sec:     7333, Lr: 0.000300\n",
            "2021-07-13 14:43:13,625 - INFO - joeynmt.training - Epoch   8, Step:    43400, Batch Loss:     2.054286, Tokens per Sec:     7464, Lr: 0.000300\n",
            "2021-07-13 14:44:13,427 - INFO - joeynmt.training - Epoch   8, Step:    43600, Batch Loss:     2.289517, Tokens per Sec:     7399, Lr: 0.000300\n",
            "2021-07-13 14:45:12,400 - INFO - joeynmt.training - Epoch   8, Step:    43800, Batch Loss:     2.101588, Tokens per Sec:     7440, Lr: 0.000300\n",
            "2021-07-13 14:46:10,831 - INFO - joeynmt.training - Epoch   8, Step:    44000, Batch Loss:     2.157770, Tokens per Sec:     7499, Lr: 0.000300\n",
            "2021-07-13 14:47:08,991 - INFO - joeynmt.training - Epoch   8, Step:    44200, Batch Loss:     1.919103, Tokens per Sec:     7462, Lr: 0.000300\n",
            "2021-07-13 14:47:13,075 - INFO - joeynmt.training - Epoch   8: total training loss 11729.65\n",
            "2021-07-13 14:47:13,075 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-13 14:48:08,442 - INFO - joeynmt.training - Epoch   9, Step:    44400, Batch Loss:     2.304124, Tokens per Sec:     7478, Lr: 0.000300\n",
            "2021-07-13 14:49:06,672 - INFO - joeynmt.training - Epoch   9, Step:    44600, Batch Loss:     2.031351, Tokens per Sec:     7487, Lr: 0.000300\n",
            "2021-07-13 14:50:04,679 - INFO - joeynmt.training - Epoch   9, Step:    44800, Batch Loss:     1.952445, Tokens per Sec:     7479, Lr: 0.000300\n",
            "2021-07-13 14:51:03,085 - INFO - joeynmt.training - Epoch   9, Step:    45000, Batch Loss:     2.272469, Tokens per Sec:     7479, Lr: 0.000300\n",
            "2021-07-13 14:53:05,291 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 14:53:05,292 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 14:53:05,292 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 14:53:06,009 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 14:53:06,010 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 14:53:06,816 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 14:53:06,818 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 14:53:06,818 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 14:53:06,818 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 14:53:06,819 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 14:53:06,819 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 14:53:06,820 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 14:53:06,820 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-13 14:53:06,820 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 14:53:06,821 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 14:53:06,821 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 14:53:06,821 - INFO - joeynmt.training - \tHypothesis: Instead of being tempted or objects , we should keep our faith in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 14:53:06,821 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 14:53:06,822 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 14:53:06,822 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 14:53:06,823 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-13 14:53:06,823 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step    45000: bleu:  12.76, loss: 150888.7188, ppl:  11.1505, duration: 123.7376s\n",
            "2021-07-13 14:54:05,062 - INFO - joeynmt.training - Epoch   9, Step:    45200, Batch Loss:     1.990451, Tokens per Sec:     7382, Lr: 0.000300\n",
            "2021-07-13 14:55:03,873 - INFO - joeynmt.training - Epoch   9, Step:    45400, Batch Loss:     1.993416, Tokens per Sec:     7521, Lr: 0.000300\n",
            "2021-07-13 14:56:02,700 - INFO - joeynmt.training - Epoch   9, Step:    45600, Batch Loss:     1.935526, Tokens per Sec:     7464, Lr: 0.000300\n",
            "2021-07-13 14:57:01,314 - INFO - joeynmt.training - Epoch   9, Step:    45800, Batch Loss:     2.255942, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-13 14:57:59,826 - INFO - joeynmt.training - Epoch   9, Step:    46000, Batch Loss:     2.185687, Tokens per Sec:     7508, Lr: 0.000300\n",
            "2021-07-13 14:58:57,913 - INFO - joeynmt.training - Epoch   9, Step:    46200, Batch Loss:     1.988583, Tokens per Sec:     7373, Lr: 0.000300\n",
            "2021-07-13 14:59:56,218 - INFO - joeynmt.training - Epoch   9, Step:    46400, Batch Loss:     2.125129, Tokens per Sec:     7434, Lr: 0.000300\n",
            "2021-07-13 15:00:54,935 - INFO - joeynmt.training - Epoch   9, Step:    46600, Batch Loss:     2.168051, Tokens per Sec:     7515, Lr: 0.000300\n",
            "2021-07-13 15:01:53,878 - INFO - joeynmt.training - Epoch   9, Step:    46800, Batch Loss:     1.958253, Tokens per Sec:     7504, Lr: 0.000300\n",
            "2021-07-13 15:02:52,274 - INFO - joeynmt.training - Epoch   9, Step:    47000, Batch Loss:     2.024166, Tokens per Sec:     7491, Lr: 0.000300\n",
            "2021-07-13 15:03:50,745 - INFO - joeynmt.training - Epoch   9, Step:    47200, Batch Loss:     1.998724, Tokens per Sec:     7530, Lr: 0.000300\n",
            "2021-07-13 15:04:49,247 - INFO - joeynmt.training - Epoch   9, Step:    47400, Batch Loss:     2.068645, Tokens per Sec:     7496, Lr: 0.000300\n",
            "2021-07-13 15:05:48,014 - INFO - joeynmt.training - Epoch   9, Step:    47600, Batch Loss:     1.908441, Tokens per Sec:     7616, Lr: 0.000300\n",
            "2021-07-13 15:06:46,230 - INFO - joeynmt.training - Epoch   9, Step:    47800, Batch Loss:     2.103785, Tokens per Sec:     7454, Lr: 0.000300\n",
            "2021-07-13 15:07:44,418 - INFO - joeynmt.training - Epoch   9, Step:    48000, Batch Loss:     2.332268, Tokens per Sec:     7400, Lr: 0.000300\n",
            "2021-07-13 15:08:43,040 - INFO - joeynmt.training - Epoch   9, Step:    48200, Batch Loss:     2.251101, Tokens per Sec:     7566, Lr: 0.000300\n",
            "2021-07-13 15:09:41,485 - INFO - joeynmt.training - Epoch   9, Step:    48400, Batch Loss:     2.105929, Tokens per Sec:     7523, Lr: 0.000300\n",
            "2021-07-13 15:10:40,046 - INFO - joeynmt.training - Epoch   9, Step:    48600, Batch Loss:     2.300836, Tokens per Sec:     7522, Lr: 0.000300\n",
            "2021-07-13 15:11:38,653 - INFO - joeynmt.training - Epoch   9, Step:    48800, Batch Loss:     2.025387, Tokens per Sec:     7521, Lr: 0.000300\n",
            "2021-07-13 15:12:36,957 - INFO - joeynmt.training - Epoch   9, Step:    49000, Batch Loss:     2.041000, Tokens per Sec:     7593, Lr: 0.000300\n",
            "2021-07-13 15:13:35,007 - INFO - joeynmt.training - Epoch   9, Step:    49200, Batch Loss:     2.644220, Tokens per Sec:     7395, Lr: 0.000300\n",
            "2021-07-13 15:14:33,620 - INFO - joeynmt.training - Epoch   9, Step:    49400, Batch Loss:     2.286180, Tokens per Sec:     7541, Lr: 0.000300\n",
            "2021-07-13 15:15:31,744 - INFO - joeynmt.training - Epoch   9, Step:    49600, Batch Loss:     2.385859, Tokens per Sec:     7471, Lr: 0.000300\n",
            "2021-07-13 15:16:13,981 - INFO - joeynmt.training - Epoch   9: total training loss 11509.43\n",
            "2021-07-13 15:16:13,982 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-13 15:16:30,725 - INFO - joeynmt.training - Epoch  10, Step:    49800, Batch Loss:     2.099829, Tokens per Sec:     7363, Lr: 0.000300\n",
            "2021-07-13 15:17:28,842 - INFO - joeynmt.training - Epoch  10, Step:    50000, Batch Loss:     2.042659, Tokens per Sec:     7474, Lr: 0.000300\n",
            "2021-07-13 15:19:35,257 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:19:35,257 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:19:35,257 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:19:35,956 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:19:35,957 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:19:37,148 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:19:37,149 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 15:19:37,149 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 15:19:37,150 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 15:19:37,150 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:19:37,151 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 15:19:37,151 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 15:19:37,151 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-13 15:19:37,151 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:19:37,152 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 15:19:37,152 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 15:19:37,152 - INFO - joeynmt.training - \tHypothesis: Rather than being tempted or objects , we should maintain our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 15:19:37,153 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:19:37,153 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 15:19:37,154 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 15:19:37,154 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in some way , are well - released in Satan’s world .\n",
            "2021-07-13 15:19:37,154 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    50000: bleu:  12.33, loss: 149148.7812, ppl:  10.8447, duration: 128.3114s\n",
            "2021-07-13 15:20:35,872 - INFO - joeynmt.training - Epoch  10, Step:    50200, Batch Loss:     1.774250, Tokens per Sec:     7535, Lr: 0.000300\n",
            "2021-07-13 15:21:34,688 - INFO - joeynmt.training - Epoch  10, Step:    50400, Batch Loss:     2.198027, Tokens per Sec:     7534, Lr: 0.000300\n",
            "2021-07-13 15:22:33,430 - INFO - joeynmt.training - Epoch  10, Step:    50600, Batch Loss:     1.895511, Tokens per Sec:     7549, Lr: 0.000300\n",
            "2021-07-13 15:23:31,779 - INFO - joeynmt.training - Epoch  10, Step:    50800, Batch Loss:     1.955900, Tokens per Sec:     7504, Lr: 0.000300\n",
            "2021-07-13 15:24:30,710 - INFO - joeynmt.training - Epoch  10, Step:    51000, Batch Loss:     1.883989, Tokens per Sec:     7581, Lr: 0.000300\n",
            "2021-07-13 15:25:28,946 - INFO - joeynmt.training - Epoch  10, Step:    51200, Batch Loss:     2.144911, Tokens per Sec:     7575, Lr: 0.000300\n",
            "2021-07-13 15:26:27,081 - INFO - joeynmt.training - Epoch  10, Step:    51400, Batch Loss:     2.016639, Tokens per Sec:     7510, Lr: 0.000300\n",
            "2021-07-13 15:27:25,025 - INFO - joeynmt.training - Epoch  10, Step:    51600, Batch Loss:     2.175156, Tokens per Sec:     7496, Lr: 0.000300\n",
            "2021-07-13 15:28:23,417 - INFO - joeynmt.training - Epoch  10, Step:    51800, Batch Loss:     2.086144, Tokens per Sec:     7501, Lr: 0.000300\n",
            "2021-07-13 15:29:21,589 - INFO - joeynmt.training - Epoch  10, Step:    52000, Batch Loss:     1.907387, Tokens per Sec:     7554, Lr: 0.000300\n",
            "2021-07-13 15:30:19,808 - INFO - joeynmt.training - Epoch  10, Step:    52200, Batch Loss:     1.991418, Tokens per Sec:     7551, Lr: 0.000300\n",
            "2021-07-13 15:31:18,220 - INFO - joeynmt.training - Epoch  10, Step:    52400, Batch Loss:     2.125854, Tokens per Sec:     7507, Lr: 0.000300\n",
            "2021-07-13 15:32:16,271 - INFO - joeynmt.training - Epoch  10, Step:    52600, Batch Loss:     2.020127, Tokens per Sec:     7508, Lr: 0.000300\n",
            "2021-07-13 15:33:14,549 - INFO - joeynmt.training - Epoch  10, Step:    52800, Batch Loss:     2.241585, Tokens per Sec:     7496, Lr: 0.000300\n",
            "2021-07-13 15:34:13,200 - INFO - joeynmt.training - Epoch  10, Step:    53000, Batch Loss:     1.984221, Tokens per Sec:     7539, Lr: 0.000300\n",
            "2021-07-13 15:35:11,201 - INFO - joeynmt.training - Epoch  10, Step:    53200, Batch Loss:     2.130802, Tokens per Sec:     7434, Lr: 0.000300\n",
            "2021-07-13 15:36:09,657 - INFO - joeynmt.training - Epoch  10, Step:    53400, Batch Loss:     1.989472, Tokens per Sec:     7480, Lr: 0.000300\n",
            "2021-07-13 15:37:07,869 - INFO - joeynmt.training - Epoch  10, Step:    53600, Batch Loss:     2.042331, Tokens per Sec:     7500, Lr: 0.000300\n",
            "2021-07-13 15:38:06,234 - INFO - joeynmt.training - Epoch  10, Step:    53800, Batch Loss:     1.948657, Tokens per Sec:     7458, Lr: 0.000300\n",
            "2021-07-13 15:39:04,620 - INFO - joeynmt.training - Epoch  10, Step:    54000, Batch Loss:     1.999362, Tokens per Sec:     7488, Lr: 0.000300\n",
            "2021-07-13 15:40:02,512 - INFO - joeynmt.training - Epoch  10, Step:    54200, Batch Loss:     1.834669, Tokens per Sec:     7452, Lr: 0.000300\n",
            "2021-07-13 15:41:01,141 - INFO - joeynmt.training - Epoch  10, Step:    54400, Batch Loss:     1.923211, Tokens per Sec:     7598, Lr: 0.000300\n",
            "2021-07-13 15:41:59,662 - INFO - joeynmt.training - Epoch  10, Step:    54600, Batch Loss:     1.914049, Tokens per Sec:     7525, Lr: 0.000300\n",
            "2021-07-13 15:42:57,877 - INFO - joeynmt.training - Epoch  10, Step:    54800, Batch Loss:     2.081892, Tokens per Sec:     7468, Lr: 0.000300\n",
            "2021-07-13 15:43:55,702 - INFO - joeynmt.training - Epoch  10, Step:    55000, Batch Loss:     1.987230, Tokens per Sec:     7358, Lr: 0.000300\n",
            "2021-07-13 15:45:54,251 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 15:45:54,251 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 15:45:54,252 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 15:45:54,984 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 15:45:54,984 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 15:45:56,122 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 15:45:56,123 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 15:45:56,123 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 15:45:56,124 - INFO - joeynmt.training - \tHypothesis: It was deeply moved .\n",
            "2021-07-13 15:45:56,124 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 15:45:56,125 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 15:45:56,125 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 15:45:56,125 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-13 15:45:56,125 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 15:45:56,126 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 15:45:56,126 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 15:45:56,126 - INFO - joeynmt.training - \tHypothesis: Instead of being tempted or observers , we should maintain our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 15:45:56,127 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 15:45:56,127 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 15:45:56,127 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 15:45:56,128 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - established in Satan’s world .\n",
            "2021-07-13 15:45:56,128 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    55000: bleu:  13.28, loss: 147339.1875, ppl:  10.5355, duration: 120.4252s\n",
            "2021-07-13 15:46:54,941 - INFO - joeynmt.training - Epoch  10, Step:    55200, Batch Loss:     2.578811, Tokens per Sec:     7551, Lr: 0.000300\n",
            "2021-07-13 15:47:14,791 - INFO - joeynmt.training - Epoch  10: total training loss 11304.52\n",
            "2021-07-13 15:47:14,791 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-13 15:47:54,094 - INFO - joeynmt.training - Epoch  11, Step:    55400, Batch Loss:     1.985468, Tokens per Sec:     7443, Lr: 0.000300\n",
            "2021-07-13 15:48:52,353 - INFO - joeynmt.training - Epoch  11, Step:    55600, Batch Loss:     1.651554, Tokens per Sec:     7450, Lr: 0.000300\n",
            "2021-07-13 15:49:50,675 - INFO - joeynmt.training - Epoch  11, Step:    55800, Batch Loss:     1.823055, Tokens per Sec:     7559, Lr: 0.000300\n",
            "2021-07-13 15:50:49,962 - INFO - joeynmt.training - Epoch  11, Step:    56000, Batch Loss:     2.070092, Tokens per Sec:     7499, Lr: 0.000300\n",
            "2021-07-13 15:51:48,280 - INFO - joeynmt.training - Epoch  11, Step:    56200, Batch Loss:     2.005097, Tokens per Sec:     7468, Lr: 0.000300\n",
            "2021-07-13 15:52:46,276 - INFO - joeynmt.training - Epoch  11, Step:    56400, Batch Loss:     1.734573, Tokens per Sec:     7470, Lr: 0.000300\n",
            "2021-07-13 15:53:44,723 - INFO - joeynmt.training - Epoch  11, Step:    56600, Batch Loss:     1.989186, Tokens per Sec:     7558, Lr: 0.000300\n",
            "2021-07-13 15:54:43,001 - INFO - joeynmt.training - Epoch  11, Step:    56800, Batch Loss:     2.077529, Tokens per Sec:     7487, Lr: 0.000300\n",
            "2021-07-13 15:55:41,368 - INFO - joeynmt.training - Epoch  11, Step:    57000, Batch Loss:     2.172663, Tokens per Sec:     7495, Lr: 0.000300\n",
            "2021-07-13 15:56:39,809 - INFO - joeynmt.training - Epoch  11, Step:    57200, Batch Loss:     2.099817, Tokens per Sec:     7488, Lr: 0.000300\n",
            "2021-07-13 15:57:38,094 - INFO - joeynmt.training - Epoch  11, Step:    57400, Batch Loss:     1.998448, Tokens per Sec:     7421, Lr: 0.000300\n",
            "2021-07-13 15:58:36,744 - INFO - joeynmt.training - Epoch  11, Step:    57600, Batch Loss:     1.940312, Tokens per Sec:     7515, Lr: 0.000300\n",
            "2021-07-13 15:59:35,508 - INFO - joeynmt.training - Epoch  11, Step:    57800, Batch Loss:     2.123287, Tokens per Sec:     7491, Lr: 0.000300\n",
            "2021-07-13 16:00:33,746 - INFO - joeynmt.training - Epoch  11, Step:    58000, Batch Loss:     2.062338, Tokens per Sec:     7453, Lr: 0.000300\n",
            "2021-07-13 16:01:31,849 - INFO - joeynmt.training - Epoch  11, Step:    58200, Batch Loss:     1.875822, Tokens per Sec:     7585, Lr: 0.000300\n",
            "2021-07-13 16:02:30,242 - INFO - joeynmt.training - Epoch  11, Step:    58400, Batch Loss:     2.054487, Tokens per Sec:     7541, Lr: 0.000300\n",
            "2021-07-13 16:03:28,751 - INFO - joeynmt.training - Epoch  11, Step:    58600, Batch Loss:     1.963067, Tokens per Sec:     7604, Lr: 0.000300\n",
            "2021-07-13 16:04:26,391 - INFO - joeynmt.training - Epoch  11, Step:    58800, Batch Loss:     1.953487, Tokens per Sec:     7390, Lr: 0.000300\n",
            "2021-07-13 16:05:24,780 - INFO - joeynmt.training - Epoch  11, Step:    59000, Batch Loss:     1.840820, Tokens per Sec:     7552, Lr: 0.000300\n",
            "2021-07-13 16:06:22,934 - INFO - joeynmt.training - Epoch  11, Step:    59200, Batch Loss:     2.071565, Tokens per Sec:     7445, Lr: 0.000300\n",
            "2021-07-13 16:07:21,653 - INFO - joeynmt.training - Epoch  11, Step:    59400, Batch Loss:     1.920345, Tokens per Sec:     7672, Lr: 0.000300\n",
            "2021-07-13 16:08:19,809 - INFO - joeynmt.training - Epoch  11, Step:    59600, Batch Loss:     1.927878, Tokens per Sec:     7497, Lr: 0.000300\n",
            "2021-07-13 16:09:18,009 - INFO - joeynmt.training - Epoch  11, Step:    59800, Batch Loss:     2.050627, Tokens per Sec:     7501, Lr: 0.000300\n",
            "2021-07-13 16:10:16,218 - INFO - joeynmt.training - Epoch  11, Step:    60000, Batch Loss:     1.918518, Tokens per Sec:     7514, Lr: 0.000300\n",
            "2021-07-13 16:12:26,942 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 16:12:26,942 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 16:12:26,943 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 16:12:27,662 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 16:12:27,662 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 16:12:28,498 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 16:12:28,501 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 16:12:28,501 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 16:12:28,502 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 16:12:28,502 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 16:12:28,502 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 16:12:28,503 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 16:12:28,503 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-13 16:12:28,503 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 16:12:28,504 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 16:12:28,504 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 16:12:28,505 - INFO - joeynmt.training - \tHypothesis: Rather than being discouraged or discerned , we should continue to strengthen our faith by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 16:12:28,505 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 16:12:28,507 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 16:12:28,507 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 16:12:28,508 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in some way , are well - being restored in Satan’s world .\n",
            "2021-07-13 16:12:28,508 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    60000: bleu:  12.85, loss: 145776.7188, ppl:  10.2757, duration: 132.2892s\n",
            "2021-07-13 16:13:26,937 - INFO - joeynmt.training - Epoch  11, Step:    60200, Batch Loss:     1.984662, Tokens per Sec:     7479, Lr: 0.000300\n",
            "2021-07-13 16:14:25,593 - INFO - joeynmt.training - Epoch  11, Step:    60400, Batch Loss:     1.961541, Tokens per Sec:     7556, Lr: 0.000300\n",
            "2021-07-13 16:15:24,178 - INFO - joeynmt.training - Epoch  11, Step:    60600, Batch Loss:     1.964082, Tokens per Sec:     7605, Lr: 0.000300\n",
            "2021-07-13 16:16:18,595 - INFO - joeynmt.training - Epoch  11: total training loss 11128.11\n",
            "2021-07-13 16:16:18,595 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-13 16:16:23,211 - INFO - joeynmt.training - Epoch  12, Step:    60800, Batch Loss:     1.792690, Tokens per Sec:     6140, Lr: 0.000300\n",
            "2021-07-13 16:17:20,970 - INFO - joeynmt.training - Epoch  12, Step:    61000, Batch Loss:     1.966733, Tokens per Sec:     7364, Lr: 0.000300\n",
            "2021-07-13 16:18:19,445 - INFO - joeynmt.training - Epoch  12, Step:    61200, Batch Loss:     1.973896, Tokens per Sec:     7538, Lr: 0.000300\n",
            "2021-07-13 16:19:17,495 - INFO - joeynmt.training - Epoch  12, Step:    61400, Batch Loss:     1.825082, Tokens per Sec:     7550, Lr: 0.000300\n",
            "2021-07-13 16:20:15,848 - INFO - joeynmt.training - Epoch  12, Step:    61600, Batch Loss:     1.853587, Tokens per Sec:     7561, Lr: 0.000300\n",
            "2021-07-13 16:21:14,698 - INFO - joeynmt.training - Epoch  12, Step:    61800, Batch Loss:     1.951689, Tokens per Sec:     7676, Lr: 0.000300\n",
            "2021-07-13 16:22:13,029 - INFO - joeynmt.training - Epoch  12, Step:    62000, Batch Loss:     1.762990, Tokens per Sec:     7485, Lr: 0.000300\n",
            "2021-07-13 16:23:11,538 - INFO - joeynmt.training - Epoch  12, Step:    62200, Batch Loss:     1.929895, Tokens per Sec:     7417, Lr: 0.000300\n",
            "2021-07-13 16:24:09,122 - INFO - joeynmt.training - Epoch  12, Step:    62400, Batch Loss:     1.935637, Tokens per Sec:     7448, Lr: 0.000300\n",
            "2021-07-13 16:25:07,370 - INFO - joeynmt.training - Epoch  12, Step:    62600, Batch Loss:     1.848925, Tokens per Sec:     7466, Lr: 0.000300\n",
            "2021-07-13 16:26:05,501 - INFO - joeynmt.training - Epoch  12, Step:    62800, Batch Loss:     1.752730, Tokens per Sec:     7526, Lr: 0.000300\n",
            "2021-07-13 16:27:03,818 - INFO - joeynmt.training - Epoch  12, Step:    63000, Batch Loss:     2.052345, Tokens per Sec:     7456, Lr: 0.000300\n",
            "2021-07-13 16:28:02,214 - INFO - joeynmt.training - Epoch  12, Step:    63200, Batch Loss:     2.165529, Tokens per Sec:     7558, Lr: 0.000300\n",
            "2021-07-13 16:29:00,840 - INFO - joeynmt.training - Epoch  12, Step:    63400, Batch Loss:     1.811451, Tokens per Sec:     7520, Lr: 0.000300\n",
            "2021-07-13 16:29:59,068 - INFO - joeynmt.training - Epoch  12, Step:    63600, Batch Loss:     2.353893, Tokens per Sec:     7505, Lr: 0.000300\n",
            "2021-07-13 16:30:57,027 - INFO - joeynmt.training - Epoch  12, Step:    63800, Batch Loss:     2.024256, Tokens per Sec:     7415, Lr: 0.000300\n",
            "2021-07-13 16:31:55,702 - INFO - joeynmt.training - Epoch  12, Step:    64000, Batch Loss:     2.121868, Tokens per Sec:     7564, Lr: 0.000300\n",
            "2021-07-13 16:32:53,821 - INFO - joeynmt.training - Epoch  12, Step:    64200, Batch Loss:     1.971426, Tokens per Sec:     7495, Lr: 0.000300\n",
            "2021-07-13 16:33:52,078 - INFO - joeynmt.training - Epoch  12, Step:    64400, Batch Loss:     1.715864, Tokens per Sec:     7581, Lr: 0.000300\n",
            "2021-07-13 16:34:50,519 - INFO - joeynmt.training - Epoch  12, Step:    64600, Batch Loss:     2.202614, Tokens per Sec:     7538, Lr: 0.000300\n",
            "2021-07-13 16:35:48,835 - INFO - joeynmt.training - Epoch  12, Step:    64800, Batch Loss:     2.031796, Tokens per Sec:     7495, Lr: 0.000300\n",
            "2021-07-13 16:36:47,303 - INFO - joeynmt.training - Epoch  12, Step:    65000, Batch Loss:     2.126581, Tokens per Sec:     7555, Lr: 0.000300\n",
            "2021-07-13 16:38:50,487 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 16:38:50,488 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 16:38:50,488 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 16:38:51,202 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 16:38:51,202 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 16:38:52,016 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 16:38:52,018 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 16:38:52,018 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 16:38:52,018 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 16:38:52,018 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 16:38:52,019 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 16:38:52,019 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 16:38:52,020 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-13 16:38:52,020 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 16:38:52,021 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 16:38:52,021 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 16:38:52,021 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or discerned , we should maintain our faith in God through reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 16:38:52,023 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 16:38:52,023 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 16:38:52,023 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 16:38:52,025 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of a measure of being released in Satan’s world .\n",
            "2021-07-13 16:38:52,025 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    65000: bleu:  13.63, loss: 144349.9531, ppl:  10.0440, duration: 124.7215s\n",
            "2021-07-13 16:39:50,936 - INFO - joeynmt.training - Epoch  12, Step:    65200, Batch Loss:     1.934529, Tokens per Sec:     7517, Lr: 0.000300\n",
            "2021-07-13 16:40:49,251 - INFO - joeynmt.training - Epoch  12, Step:    65400, Batch Loss:     1.925130, Tokens per Sec:     7428, Lr: 0.000300\n",
            "2021-07-13 16:41:47,797 - INFO - joeynmt.training - Epoch  12, Step:    65600, Batch Loss:     1.920379, Tokens per Sec:     7569, Lr: 0.000300\n",
            "2021-07-13 16:42:46,237 - INFO - joeynmt.training - Epoch  12, Step:    65800, Batch Loss:     2.309163, Tokens per Sec:     7460, Lr: 0.000300\n",
            "2021-07-13 16:43:45,169 - INFO - joeynmt.training - Epoch  12, Step:    66000, Batch Loss:     2.115569, Tokens per Sec:     7594, Lr: 0.000300\n",
            "2021-07-13 16:44:43,044 - INFO - joeynmt.training - Epoch  12, Step:    66200, Batch Loss:     2.188116, Tokens per Sec:     7478, Lr: 0.000300\n",
            "2021-07-13 16:45:15,947 - INFO - joeynmt.training - Epoch  12: total training loss 11011.58\n",
            "2021-07-13 16:45:15,948 - INFO - joeynmt.training - EPOCH 13\n",
            "2021-07-13 16:45:41,726 - INFO - joeynmt.training - Epoch  13, Step:    66400, Batch Loss:     2.303421, Tokens per Sec:     7337, Lr: 0.000300\n",
            "2021-07-13 16:46:40,706 - INFO - joeynmt.training - Epoch  13, Step:    66600, Batch Loss:     1.738779, Tokens per Sec:     7621, Lr: 0.000300\n",
            "2021-07-13 16:47:39,155 - INFO - joeynmt.training - Epoch  13, Step:    66800, Batch Loss:     2.042178, Tokens per Sec:     7392, Lr: 0.000300\n",
            "2021-07-13 16:48:37,310 - INFO - joeynmt.training - Epoch  13, Step:    67000, Batch Loss:     2.143698, Tokens per Sec:     7518, Lr: 0.000300\n",
            "2021-07-13 16:49:35,544 - INFO - joeynmt.training - Epoch  13, Step:    67200, Batch Loss:     1.833607, Tokens per Sec:     7446, Lr: 0.000300\n",
            "2021-07-13 16:50:33,778 - INFO - joeynmt.training - Epoch  13, Step:    67400, Batch Loss:     2.048861, Tokens per Sec:     7578, Lr: 0.000300\n",
            "2021-07-13 16:51:32,154 - INFO - joeynmt.training - Epoch  13, Step:    67600, Batch Loss:     2.002945, Tokens per Sec:     7420, Lr: 0.000300\n",
            "2021-07-13 16:52:30,742 - INFO - joeynmt.training - Epoch  13, Step:    67800, Batch Loss:     1.962777, Tokens per Sec:     7492, Lr: 0.000300\n",
            "2021-07-13 16:53:29,366 - INFO - joeynmt.training - Epoch  13, Step:    68000, Batch Loss:     1.798516, Tokens per Sec:     7457, Lr: 0.000300\n",
            "2021-07-13 16:54:27,740 - INFO - joeynmt.training - Epoch  13, Step:    68200, Batch Loss:     1.947686, Tokens per Sec:     7489, Lr: 0.000300\n",
            "2021-07-13 16:55:25,945 - INFO - joeynmt.training - Epoch  13, Step:    68400, Batch Loss:     1.861409, Tokens per Sec:     7503, Lr: 0.000300\n",
            "2021-07-13 16:56:23,960 - INFO - joeynmt.training - Epoch  13, Step:    68600, Batch Loss:     2.156104, Tokens per Sec:     7456, Lr: 0.000300\n",
            "2021-07-13 16:57:22,464 - INFO - joeynmt.training - Epoch  13, Step:    68800, Batch Loss:     2.061885, Tokens per Sec:     7493, Lr: 0.000300\n",
            "2021-07-13 16:58:20,647 - INFO - joeynmt.training - Epoch  13, Step:    69000, Batch Loss:     1.730417, Tokens per Sec:     7415, Lr: 0.000300\n",
            "2021-07-13 16:59:19,168 - INFO - joeynmt.training - Epoch  13, Step:    69200, Batch Loss:     2.036818, Tokens per Sec:     7440, Lr: 0.000300\n",
            "2021-07-13 17:00:18,009 - INFO - joeynmt.training - Epoch  13, Step:    69400, Batch Loss:     1.800813, Tokens per Sec:     7386, Lr: 0.000300\n",
            "2021-07-13 17:01:16,624 - INFO - joeynmt.training - Epoch  13, Step:    69600, Batch Loss:     2.062272, Tokens per Sec:     7501, Lr: 0.000300\n",
            "2021-07-13 17:02:14,597 - INFO - joeynmt.training - Epoch  13, Step:    69800, Batch Loss:     1.941891, Tokens per Sec:     7415, Lr: 0.000300\n",
            "2021-07-13 17:03:13,504 - INFO - joeynmt.training - Epoch  13, Step:    70000, Batch Loss:     1.787522, Tokens per Sec:     7566, Lr: 0.000300\n",
            "2021-07-13 17:05:18,589 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 17:05:18,590 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 17:05:18,590 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 17:05:19,285 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 17:05:19,285 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 17:05:20,117 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 17:05:20,118 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 17:05:20,118 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 17:05:20,119 - INFO - joeynmt.training - \tHypothesis: It was very touched .\n",
            "2021-07-13 17:05:20,119 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 17:05:20,120 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 17:05:20,120 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 17:05:20,120 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-13 17:05:20,121 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 17:05:20,121 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 17:05:20,122 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 17:05:20,122 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or discerned , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 17:05:20,122 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 17:05:20,123 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 17:05:20,123 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 17:05:20,123 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - fed in Satan’s world .\n",
            "2021-07-13 17:05:20,124 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    70000: bleu:  13.92, loss: 143590.7500, ppl:   9.9229, duration: 126.6197s\n",
            "2021-07-13 17:06:18,596 - INFO - joeynmt.training - Epoch  13, Step:    70200, Batch Loss:     2.019572, Tokens per Sec:     7461, Lr: 0.000300\n",
            "2021-07-13 17:07:16,673 - INFO - joeynmt.training - Epoch  13, Step:    70400, Batch Loss:     1.989264, Tokens per Sec:     7507, Lr: 0.000300\n",
            "2021-07-13 17:08:14,930 - INFO - joeynmt.training - Epoch  13, Step:    70600, Batch Loss:     1.886770, Tokens per Sec:     7561, Lr: 0.000300\n",
            "2021-07-13 17:09:13,208 - INFO - joeynmt.training - Epoch  13, Step:    70800, Batch Loss:     1.915995, Tokens per Sec:     7434, Lr: 0.000300\n",
            "2021-07-13 17:10:11,532 - INFO - joeynmt.training - Epoch  13, Step:    71000, Batch Loss:     1.838542, Tokens per Sec:     7522, Lr: 0.000300\n",
            "2021-07-13 17:11:10,079 - INFO - joeynmt.training - Epoch  13, Step:    71200, Batch Loss:     2.072811, Tokens per Sec:     7548, Lr: 0.000300\n",
            "2021-07-13 17:12:07,973 - INFO - joeynmt.training - Epoch  13, Step:    71400, Batch Loss:     1.915889, Tokens per Sec:     7497, Lr: 0.000300\n",
            "2021-07-13 17:13:06,637 - INFO - joeynmt.training - Epoch  13, Step:    71600, Batch Loss:     1.916653, Tokens per Sec:     7535, Lr: 0.000300\n",
            "2021-07-13 17:14:04,718 - INFO - joeynmt.training - Epoch  13, Step:    71800, Batch Loss:     1.557299, Tokens per Sec:     7399, Lr: 0.000300\n",
            "2021-07-13 17:14:20,224 - INFO - joeynmt.training - Epoch  13: total training loss 10913.61\n",
            "2021-07-13 17:14:20,224 - INFO - joeynmt.training - EPOCH 14\n",
            "2021-07-13 17:15:03,557 - INFO - joeynmt.training - Epoch  14, Step:    72000, Batch Loss:     2.000690, Tokens per Sec:     7408, Lr: 0.000300\n",
            "2021-07-13 17:16:02,088 - INFO - joeynmt.training - Epoch  14, Step:    72200, Batch Loss:     2.011120, Tokens per Sec:     7539, Lr: 0.000300\n",
            "2021-07-13 17:17:00,229 - INFO - joeynmt.training - Epoch  14, Step:    72400, Batch Loss:     1.814448, Tokens per Sec:     7509, Lr: 0.000300\n",
            "2021-07-13 17:17:58,523 - INFO - joeynmt.training - Epoch  14, Step:    72600, Batch Loss:     2.085888, Tokens per Sec:     7485, Lr: 0.000300\n",
            "2021-07-13 17:18:56,729 - INFO - joeynmt.training - Epoch  14, Step:    72800, Batch Loss:     1.711956, Tokens per Sec:     7422, Lr: 0.000300\n",
            "2021-07-13 17:19:55,438 - INFO - joeynmt.training - Epoch  14, Step:    73000, Batch Loss:     1.846574, Tokens per Sec:     7549, Lr: 0.000300\n",
            "2021-07-13 17:20:54,334 - INFO - joeynmt.training - Epoch  14, Step:    73200, Batch Loss:     1.790135, Tokens per Sec:     7500, Lr: 0.000300\n",
            "2021-07-13 17:21:52,802 - INFO - joeynmt.training - Epoch  14, Step:    73400, Batch Loss:     1.991323, Tokens per Sec:     7541, Lr: 0.000300\n",
            "2021-07-13 17:22:51,332 - INFO - joeynmt.training - Epoch  14, Step:    73600, Batch Loss:     1.797866, Tokens per Sec:     7492, Lr: 0.000300\n",
            "2021-07-13 17:23:49,791 - INFO - joeynmt.training - Epoch  14, Step:    73800, Batch Loss:     1.783954, Tokens per Sec:     7489, Lr: 0.000300\n",
            "2021-07-13 17:24:48,087 - INFO - joeynmt.training - Epoch  14, Step:    74000, Batch Loss:     2.077509, Tokens per Sec:     7502, Lr: 0.000300\n",
            "2021-07-13 17:25:46,362 - INFO - joeynmt.training - Epoch  14, Step:    74200, Batch Loss:     2.085146, Tokens per Sec:     7567, Lr: 0.000300\n",
            "2021-07-13 17:26:44,487 - INFO - joeynmt.training - Epoch  14, Step:    74400, Batch Loss:     1.914436, Tokens per Sec:     7462, Lr: 0.000300\n",
            "2021-07-13 17:27:42,717 - INFO - joeynmt.training - Epoch  14, Step:    74600, Batch Loss:     1.864811, Tokens per Sec:     7476, Lr: 0.000300\n",
            "2021-07-13 17:28:41,109 - INFO - joeynmt.training - Epoch  14, Step:    74800, Batch Loss:     1.934399, Tokens per Sec:     7475, Lr: 0.000300\n",
            "2021-07-13 17:29:39,349 - INFO - joeynmt.training - Epoch  14, Step:    75000, Batch Loss:     2.022531, Tokens per Sec:     7484, Lr: 0.000300\n",
            "2021-07-13 17:31:46,024 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 17:31:46,024 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 17:31:46,025 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 17:31:46,755 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 17:31:46,755 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 17:31:47,560 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 17:31:47,561 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 17:31:47,561 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 17:31:47,562 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-13 17:31:47,562 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 17:31:47,562 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 17:31:47,563 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 17:31:47,563 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-13 17:31:47,563 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 17:31:47,564 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 17:31:47,564 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 17:31:47,564 - INFO - joeynmt.training - \tHypothesis: Rather than mistake or discern , we should continue our faith by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 17:31:47,565 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 17:31:47,565 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 17:31:47,566 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 17:31:47,566 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show , in a sense , have been restored in Satan’s world .\n",
            "2021-07-13 17:31:47,566 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    75000: bleu:  13.94, loss: 143132.4375, ppl:   9.8505, duration: 128.2164s\n",
            "2021-07-13 17:32:46,042 - INFO - joeynmt.training - Epoch  14, Step:    75200, Batch Loss:     2.056211, Tokens per Sec:     7393, Lr: 0.000300\n",
            "2021-07-13 17:33:44,841 - INFO - joeynmt.training - Epoch  14, Step:    75400, Batch Loss:     1.685734, Tokens per Sec:     7444, Lr: 0.000300\n",
            "2021-07-13 17:34:42,797 - INFO - joeynmt.training - Epoch  14, Step:    75600, Batch Loss:     1.941723, Tokens per Sec:     7462, Lr: 0.000300\n",
            "2021-07-13 17:35:41,468 - INFO - joeynmt.training - Epoch  14, Step:    75800, Batch Loss:     1.854175, Tokens per Sec:     7542, Lr: 0.000300\n",
            "2021-07-13 17:36:40,198 - INFO - joeynmt.training - Epoch  14, Step:    76000, Batch Loss:     1.887380, Tokens per Sec:     7480, Lr: 0.000300\n",
            "2021-07-13 17:37:38,493 - INFO - joeynmt.training - Epoch  14, Step:    76200, Batch Loss:     1.746780, Tokens per Sec:     7538, Lr: 0.000300\n",
            "2021-07-13 17:38:37,161 - INFO - joeynmt.training - Epoch  14, Step:    76400, Batch Loss:     1.936836, Tokens per Sec:     7458, Lr: 0.000300\n",
            "2021-07-13 17:39:35,706 - INFO - joeynmt.training - Epoch  14, Step:    76600, Batch Loss:     2.023333, Tokens per Sec:     7580, Lr: 0.000300\n",
            "2021-07-13 17:40:33,894 - INFO - joeynmt.training - Epoch  14, Step:    76800, Batch Loss:     1.837384, Tokens per Sec:     7436, Lr: 0.000300\n",
            "2021-07-13 17:41:31,655 - INFO - joeynmt.training - Epoch  14, Step:    77000, Batch Loss:     1.738008, Tokens per Sec:     7477, Lr: 0.000300\n",
            "2021-07-13 17:42:29,984 - INFO - joeynmt.training - Epoch  14, Step:    77200, Batch Loss:     1.673677, Tokens per Sec:     7492, Lr: 0.000300\n",
            "2021-07-13 17:43:23,431 - INFO - joeynmt.training - Epoch  14: total training loss 10785.31\n",
            "2021-07-13 17:43:23,431 - INFO - joeynmt.training - EPOCH 15\n",
            "2021-07-13 17:43:28,855 - INFO - joeynmt.training - Epoch  15, Step:    77400, Batch Loss:     2.490561, Tokens per Sec:     6363, Lr: 0.000300\n",
            "2021-07-13 17:44:27,574 - INFO - joeynmt.training - Epoch  15, Step:    77600, Batch Loss:     1.996720, Tokens per Sec:     7486, Lr: 0.000300\n",
            "2021-07-13 17:45:25,437 - INFO - joeynmt.training - Epoch  15, Step:    77800, Batch Loss:     1.775003, Tokens per Sec:     7449, Lr: 0.000300\n",
            "2021-07-13 17:46:23,954 - INFO - joeynmt.training - Epoch  15, Step:    78000, Batch Loss:     1.771748, Tokens per Sec:     7488, Lr: 0.000300\n",
            "2021-07-13 17:47:22,457 - INFO - joeynmt.training - Epoch  15, Step:    78200, Batch Loss:     1.835771, Tokens per Sec:     7519, Lr: 0.000300\n",
            "2021-07-13 17:48:20,583 - INFO - joeynmt.training - Epoch  15, Step:    78400, Batch Loss:     1.690623, Tokens per Sec:     7436, Lr: 0.000300\n",
            "2021-07-13 17:49:18,827 - INFO - joeynmt.training - Epoch  15, Step:    78600, Batch Loss:     1.875375, Tokens per Sec:     7557, Lr: 0.000300\n",
            "2021-07-13 17:50:17,067 - INFO - joeynmt.training - Epoch  15, Step:    78800, Batch Loss:     2.026448, Tokens per Sec:     7501, Lr: 0.000300\n",
            "2021-07-13 17:51:15,898 - INFO - joeynmt.training - Epoch  15, Step:    79000, Batch Loss:     1.899524, Tokens per Sec:     7506, Lr: 0.000300\n",
            "2021-07-13 17:52:14,790 - INFO - joeynmt.training - Epoch  15, Step:    79200, Batch Loss:     1.897406, Tokens per Sec:     7526, Lr: 0.000300\n",
            "2021-07-13 17:53:13,270 - INFO - joeynmt.training - Epoch  15, Step:    79400, Batch Loss:     2.454070, Tokens per Sec:     7465, Lr: 0.000300\n",
            "2021-07-13 17:54:11,893 - INFO - joeynmt.training - Epoch  15, Step:    79600, Batch Loss:     1.984342, Tokens per Sec:     7553, Lr: 0.000300\n",
            "2021-07-13 17:55:10,034 - INFO - joeynmt.training - Epoch  15, Step:    79800, Batch Loss:     2.028225, Tokens per Sec:     7420, Lr: 0.000300\n",
            "2021-07-13 17:56:08,814 - INFO - joeynmt.training - Epoch  15, Step:    80000, Batch Loss:     2.011921, Tokens per Sec:     7489, Lr: 0.000300\n",
            "2021-07-13 17:58:26,352 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 17:58:26,352 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 17:58:26,352 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 17:58:27,099 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 17:58:27,099 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 17:58:27,944 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 17:58:27,945 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 17:58:27,945 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 17:58:27,945 - INFO - joeynmt.training - \tHypothesis: It was deeply moved .\n",
            "2021-07-13 17:58:27,946 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 17:58:27,946 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 17:58:27,947 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 17:58:27,947 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the side of the scroll .\n",
            "2021-07-13 17:58:27,948 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 17:58:27,949 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 17:58:27,949 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 17:58:27,950 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or discerned , we should continue to strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 17:58:27,950 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 17:58:27,951 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 17:58:27,951 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 17:58:27,951 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of goodness in Satan’s world .\n",
            "2021-07-13 17:58:27,951 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    80000: bleu:  13.81, loss: 142473.8594, ppl:   9.7473, duration: 139.1367s\n",
            "2021-07-13 17:59:26,631 - INFO - joeynmt.training - Epoch  15, Step:    80200, Batch Loss:     1.989659, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-13 18:00:25,458 - INFO - joeynmt.training - Epoch  15, Step:    80400, Batch Loss:     1.784497, Tokens per Sec:     7479, Lr: 0.000300\n",
            "2021-07-13 18:01:23,691 - INFO - joeynmt.training - Epoch  15, Step:    80600, Batch Loss:     1.898345, Tokens per Sec:     7359, Lr: 0.000300\n",
            "2021-07-13 18:02:22,157 - INFO - joeynmt.training - Epoch  15, Step:    80800, Batch Loss:     2.460619, Tokens per Sec:     7499, Lr: 0.000300\n",
            "2021-07-13 18:03:21,068 - INFO - joeynmt.training - Epoch  15, Step:    81000, Batch Loss:     1.593694, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-13 18:04:19,610 - INFO - joeynmt.training - Epoch  15, Step:    81200, Batch Loss:     2.017682, Tokens per Sec:     7480, Lr: 0.000300\n",
            "2021-07-13 18:05:18,446 - INFO - joeynmt.training - Epoch  15, Step:    81400, Batch Loss:     1.687585, Tokens per Sec:     7568, Lr: 0.000300\n",
            "2021-07-13 18:06:16,633 - INFO - joeynmt.training - Epoch  15, Step:    81600, Batch Loss:     1.693099, Tokens per Sec:     7429, Lr: 0.000300\n",
            "2021-07-13 18:07:15,288 - INFO - joeynmt.training - Epoch  15, Step:    81800, Batch Loss:     1.973889, Tokens per Sec:     7450, Lr: 0.000300\n",
            "2021-07-13 18:08:14,052 - INFO - joeynmt.training - Epoch  15, Step:    82000, Batch Loss:     1.618494, Tokens per Sec:     7469, Lr: 0.000300\n",
            "2021-07-13 18:09:12,755 - INFO - joeynmt.training - Epoch  15, Step:    82200, Batch Loss:     1.939155, Tokens per Sec:     7621, Lr: 0.000300\n",
            "2021-07-13 18:10:11,010 - INFO - joeynmt.training - Epoch  15, Step:    82400, Batch Loss:     1.828044, Tokens per Sec:     7479, Lr: 0.000300\n",
            "2021-07-13 18:11:10,204 - INFO - joeynmt.training - Epoch  15, Step:    82600, Batch Loss:     1.895709, Tokens per Sec:     7643, Lr: 0.000300\n",
            "2021-07-13 18:12:08,852 - INFO - joeynmt.training - Epoch  15, Step:    82800, Batch Loss:     1.865366, Tokens per Sec:     7497, Lr: 0.000300\n",
            "2021-07-13 18:12:39,566 - INFO - joeynmt.training - Epoch  15: total training loss 10673.96\n",
            "2021-07-13 18:12:39,567 - INFO - joeynmt.training - EPOCH 16\n",
            "2021-07-13 18:13:08,402 - INFO - joeynmt.training - Epoch  16, Step:    83000, Batch Loss:     2.012892, Tokens per Sec:     7415, Lr: 0.000300\n",
            "2021-07-13 18:14:07,413 - INFO - joeynmt.training - Epoch  16, Step:    83200, Batch Loss:     1.989468, Tokens per Sec:     7504, Lr: 0.000300\n",
            "2021-07-13 18:15:06,060 - INFO - joeynmt.training - Epoch  16, Step:    83400, Batch Loss:     1.778499, Tokens per Sec:     7494, Lr: 0.000300\n",
            "2021-07-13 18:16:04,912 - INFO - joeynmt.training - Epoch  16, Step:    83600, Batch Loss:     1.836245, Tokens per Sec:     7571, Lr: 0.000300\n",
            "2021-07-13 18:17:03,304 - INFO - joeynmt.training - Epoch  16, Step:    83800, Batch Loss:     1.910011, Tokens per Sec:     7528, Lr: 0.000300\n",
            "2021-07-13 18:18:01,619 - INFO - joeynmt.training - Epoch  16, Step:    84000, Batch Loss:     1.938823, Tokens per Sec:     7366, Lr: 0.000300\n",
            "2021-07-13 18:19:00,616 - INFO - joeynmt.training - Epoch  16, Step:    84200, Batch Loss:     1.824548, Tokens per Sec:     7612, Lr: 0.000300\n",
            "2021-07-13 18:19:59,477 - INFO - joeynmt.training - Epoch  16, Step:    84400, Batch Loss:     2.032460, Tokens per Sec:     7497, Lr: 0.000300\n",
            "2021-07-13 18:20:57,832 - INFO - joeynmt.training - Epoch  16, Step:    84600, Batch Loss:     2.031781, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-13 18:21:55,999 - INFO - joeynmt.training - Epoch  16, Step:    84800, Batch Loss:     1.935990, Tokens per Sec:     7471, Lr: 0.000300\n",
            "2021-07-13 18:22:54,659 - INFO - joeynmt.training - Epoch  16, Step:    85000, Batch Loss:     1.776747, Tokens per Sec:     7402, Lr: 0.000300\n",
            "2021-07-13 18:24:56,058 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-13 18:24:56,059 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-13 18:24:56,059 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-13 18:24:56,825 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-13 18:24:56,826 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-13 18:24:57,644 - INFO - joeynmt.training - Example #0\n",
            "2021-07-13 18:24:57,644 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-13 18:24:57,645 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-13 18:24:57,645 - INFO - joeynmt.training - \tHypothesis: It was heartfelt .\n",
            "2021-07-13 18:24:57,645 - INFO - joeynmt.training - Example #1\n",
            "2021-07-13 18:24:57,646 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-13 18:24:57,646 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-13 18:24:57,646 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-13 18:24:57,647 - INFO - joeynmt.training - Example #2\n",
            "2021-07-13 18:24:57,647 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-13 18:24:57,648 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 18:24:57,648 - INFO - joeynmt.training - \tHypothesis: Instead of being tempted or discerned , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-13 18:24:57,648 - INFO - joeynmt.training - Example #3\n",
            "2021-07-13 18:24:57,649 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-13 18:24:57,649 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-13 18:24:57,649 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - restored in Satan’s world .\n",
            "2021-07-13 18:24:57,650 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    85000: bleu:  14.33, loss: 140273.0156, ppl:   9.4105, duration: 122.9903s\n",
            "2021-07-13 18:25:56,521 - INFO - joeynmt.training - Epoch  16, Step:    85200, Batch Loss:     1.919367, Tokens per Sec:     7478, Lr: 0.000300\n",
            "2021-07-13 18:26:55,252 - INFO - joeynmt.training - Epoch  16, Step:    85400, Batch Loss:     1.895384, Tokens per Sec:     7451, Lr: 0.000300\n",
            "2021-07-13 18:27:54,311 - INFO - joeynmt.training - Epoch  16, Step:    85600, Batch Loss:     2.082689, Tokens per Sec:     7502, Lr: 0.000300\n",
            "2021-07-13 18:28:52,882 - INFO - joeynmt.training - Epoch  16, Step:    85800, Batch Loss:     1.768718, Tokens per Sec:     7358, Lr: 0.000300\n",
            "2021-07-13 18:29:51,659 - INFO - joeynmt.training - Epoch  16, Step:    86000, Batch Loss:     1.814171, Tokens per Sec:     7615, Lr: 0.000300\n",
            "2021-07-13 18:30:50,135 - INFO - joeynmt.training - Epoch  16, Step:    86200, Batch Loss:     1.958698, Tokens per Sec:     7464, Lr: 0.000300\n",
            "2021-07-13 18:31:48,590 - INFO - joeynmt.training - Epoch  16, Step:    86400, Batch Loss:     2.063689, Tokens per Sec:     7429, Lr: 0.000300\n",
            "2021-07-13 18:32:47,325 - INFO - joeynmt.training - Epoch  16, Step:    86600, Batch Loss:     1.962523, Tokens per Sec:     7566, Lr: 0.000300\n",
            "2021-07-13 18:33:45,776 - INFO - joeynmt.training - Epoch  16, Step:    86800, Batch Loss:     1.755999, Tokens per Sec:     7349, Lr: 0.000300\n",
            "2021-07-13 18:34:44,806 - INFO - joeynmt.training - Epoch  16, Step:    87000, Batch Loss:     1.942234, Tokens per Sec:     7523, Lr: 0.000300\n",
            "2021-07-13 18:35:43,075 - INFO - joeynmt.training - Epoch  16, Step:    87200, Batch Loss:     1.898593, Tokens per Sec:     7507, Lr: 0.000300\n",
            "2021-07-13 18:36:41,376 - INFO - joeynmt.training - Epoch  16, Step:    87400, Batch Loss:     1.971812, Tokens per Sec:     7396, Lr: 0.000300\n",
            "2021-07-13 18:37:40,270 - INFO - joeynmt.training - Epoch  16, Step:    87600, Batch Loss:     1.783937, Tokens per Sec:     7559, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzRdhzzPgrU9"
      },
      "source": [
        "16 epochs ran"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJWjURXjwSnG"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 85000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued\"').replace(\n",
        "        f'epochs: 30', f'epochs: 14')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY-9k-G7VMd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "2ef02da6-8d75-4fb2-c4a0-9ca31fb6c4bf"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_rw_lhen_reload.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"rw_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"rw_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer/85000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 14                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/rw_lhen_reverse_transformer_continued\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG7hbW9vuvT2",
        "outputId": "37474214-b9c0-476b-be91-298b1fe44a15"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 13:08:58,914 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 13:08:58,982 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-14 13:09:09,121 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-14 13:09:09,726 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-14 13:09:10,601 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-14 13:09:11,368 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-14 13:09:11,368 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 13:09:11,587 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-14 13:09:11.842929: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-14 13:09:13,491 - INFO - joeynmt.training - Total params: 12177920\n",
            "2021-07-14 13:09:23,401 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer/85000.ckpt\n",
            "2021-07-14 13:09:23,858 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
            "2021-07-14 13:09:23,858 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
            "2021-07-14 13:09:23,858 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-14 13:09:23,859 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
            "2021-07-14 13:09:23,859 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
            "2021-07-14 13:09:23,859 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
            "2021-07-14 13:09:23,859 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-14 13:09:23,859 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-14 13:09:23,859 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-14 13:09:23,860 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-14 13:09:23,860 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-14 13:09:23,860 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-14 13:09:23,860 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-14 13:09:23,860 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer/85000.ckpt\n",
            "2021-07-14 13:09:23,860 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-14 13:09:23,860 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-14 13:09:23,860 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-14 13:09:23,861 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-14 13:09:23,861 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-14 13:09:23,861 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-14 13:09:23,861 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-14 13:09:23,861 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-14 13:09:23,861 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-14 13:09:23,861 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-14 13:09:23,862 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-14 13:09:23,862 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-14 13:09:23,862 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-14 13:09:23,862 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-14 13:09:23,862 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-14 13:09:23,862 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-14 13:09:23,862 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-14 13:09:23,862 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-14 13:09:23,863 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-14 13:09:23,863 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-14 13:09:23,863 - INFO - joeynmt.helpers - cfg.training.epochs                : 14\n",
            "2021-07-14 13:09:23,863 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-14 13:09:23,863 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-14 13:09:23,863 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-14 13:09:23,863 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued\n",
            "2021-07-14 13:09:23,863 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-14 13:09:23,864 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-14 13:09:23,864 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-14 13:09:23,864 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-14 13:09:23,864 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-14 13:09:23,864 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-14 13:09:23,864 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-14 13:09:23,864 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-14 13:09:23,865 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-14 13:09:23,865 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-14 13:09:23,865 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-14 13:09:23,865 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-14 13:09:23,865 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-14 13:09:23,865 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-14 13:09:23,865 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-14 13:09:23,865 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-14 13:09:23,866 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-14 13:09:23,866 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-14 13:09:23,866 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-14 13:09:23,867 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-14 13:09:23,868 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-14 13:09:23,869 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-14 13:09:23,869 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-14 13:09:23,869 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-14 13:09:23,869 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-14 13:09:23,870 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-14 13:09:23,870 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-14 13:09:23,870 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-14 13:09:23,870 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-14 13:09:23,870 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-14 13:09:23,870 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-14 13:09:23,870 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 439996,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-14 13:09:23,871 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
            "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
            "2021-07-14 13:09:23,871 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-14 13:09:23,871 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-14 13:09:23,871 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
            "2021-07-14 13:09:23,871 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
            "2021-07-14 13:09:23,871 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
            "2021-07-14 13:09:23,883 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-14 13:09:23,883 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-14 13:09:51,457 - INFO - joeynmt.training - Epoch   1, Step:    85200, Batch Loss:     1.926026, Tokens per Sec:    15967, Lr: 0.000300\n",
            "2021-07-14 13:10:17,838 - INFO - joeynmt.training - Epoch   1, Step:    85400, Batch Loss:     1.895668, Tokens per Sec:    16588, Lr: 0.000300\n",
            "2021-07-14 13:10:45,322 - INFO - joeynmt.training - Epoch   1, Step:    85600, Batch Loss:     2.091363, Tokens per Sec:    16120, Lr: 0.000300\n",
            "2021-07-14 13:11:13,036 - INFO - joeynmt.training - Epoch   1, Step:    85800, Batch Loss:     1.750774, Tokens per Sec:    15551, Lr: 0.000300\n",
            "2021-07-14 13:11:41,263 - INFO - joeynmt.training - Epoch   1, Step:    86000, Batch Loss:     1.822441, Tokens per Sec:    15857, Lr: 0.000300\n",
            "2021-07-14 13:12:09,103 - INFO - joeynmt.training - Epoch   1, Step:    86200, Batch Loss:     1.949649, Tokens per Sec:    15676, Lr: 0.000300\n",
            "2021-07-14 13:12:36,869 - INFO - joeynmt.training - Epoch   1, Step:    86400, Batch Loss:     2.091254, Tokens per Sec:    15641, Lr: 0.000300\n",
            "2021-07-14 13:13:04,990 - INFO - joeynmt.training - Epoch   1, Step:    86600, Batch Loss:     1.994396, Tokens per Sec:    15803, Lr: 0.000300\n",
            "2021-07-14 13:13:32,900 - INFO - joeynmt.training - Epoch   1, Step:    86800, Batch Loss:     1.818950, Tokens per Sec:    15390, Lr: 0.000300\n",
            "2021-07-14 13:14:00,859 - INFO - joeynmt.training - Epoch   1, Step:    87000, Batch Loss:     1.944963, Tokens per Sec:    15884, Lr: 0.000300\n",
            "2021-07-14 13:14:28,549 - INFO - joeynmt.training - Epoch   1, Step:    87200, Batch Loss:     1.889658, Tokens per Sec:    15798, Lr: 0.000300\n",
            "2021-07-14 13:14:56,116 - INFO - joeynmt.training - Epoch   1, Step:    87400, Batch Loss:     1.984530, Tokens per Sec:    15642, Lr: 0.000300\n",
            "2021-07-14 13:15:24,006 - INFO - joeynmt.training - Epoch   1, Step:    87600, Batch Loss:     1.786676, Tokens per Sec:    15962, Lr: 0.000300\n",
            "2021-07-14 13:15:52,069 - INFO - joeynmt.training - Epoch   1, Step:    87800, Batch Loss:     1.868486, Tokens per Sec:    15884, Lr: 0.000300\n",
            "2021-07-14 13:16:19,580 - INFO - joeynmt.training - Epoch   1, Step:    88000, Batch Loss:     2.013900, Tokens per Sec:    15902, Lr: 0.000300\n",
            "2021-07-14 13:16:47,444 - INFO - joeynmt.training - Epoch   1, Step:    88200, Batch Loss:     2.113773, Tokens per Sec:    15486, Lr: 0.000300\n",
            "2021-07-14 13:17:15,013 - INFO - joeynmt.training - Epoch   1, Step:    88400, Batch Loss:     1.749532, Tokens per Sec:    15762, Lr: 0.000300\n",
            "2021-07-14 13:17:18,140 - INFO - joeynmt.training - Epoch   1: total training loss 6557.29\n",
            "2021-07-14 13:17:18,141 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-14 13:17:43,331 - INFO - joeynmt.training - Epoch   2, Step:    88600, Batch Loss:     1.901768, Tokens per Sec:    15267, Lr: 0.000300\n",
            "2021-07-14 13:18:10,947 - INFO - joeynmt.training - Epoch   2, Step:    88800, Batch Loss:     2.108777, Tokens per Sec:    15827, Lr: 0.000300\n",
            "2021-07-14 13:18:38,497 - INFO - joeynmt.training - Epoch   2, Step:    89000, Batch Loss:     1.892419, Tokens per Sec:    15918, Lr: 0.000300\n",
            "2021-07-14 13:19:06,007 - INFO - joeynmt.training - Epoch   2, Step:    89200, Batch Loss:     1.657407, Tokens per Sec:    15553, Lr: 0.000300\n",
            "2021-07-14 13:19:33,926 - INFO - joeynmt.training - Epoch   2, Step:    89400, Batch Loss:     1.872788, Tokens per Sec:    15789, Lr: 0.000300\n",
            "2021-07-14 13:20:01,667 - INFO - joeynmt.training - Epoch   2, Step:    89600, Batch Loss:     1.644232, Tokens per Sec:    16016, Lr: 0.000300\n",
            "2021-07-14 13:20:29,655 - INFO - joeynmt.training - Epoch   2, Step:    89800, Batch Loss:     1.934064, Tokens per Sec:    15921, Lr: 0.000300\n",
            "2021-07-14 13:20:57,465 - INFO - joeynmt.training - Epoch   2, Step:    90000, Batch Loss:     1.863869, Tokens per Sec:    15760, Lr: 0.000300\n",
            "2021-07-14 13:22:10,116 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:22:10,116 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:22:10,117 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:22:10,750 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 13:22:10,750 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 13:22:11,473 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:22:11,474 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 13:22:11,474 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 13:22:11,475 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-14 13:22:11,475 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:22:11,475 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 13:22:11,475 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 13:22:11,476 - INFO - joeynmt.training - \tHypothesis: The text written in the front of the scroll .\n",
            "2021-07-14 13:22:11,476 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:22:11,476 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 13:22:11,476 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 13:22:11,476 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 13:22:11,477 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:22:11,477 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 13:22:11,477 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 13:22:11,477 - INFO - joeynmt.training - \tHypothesis: Sadly , some of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-14 13:22:11,477 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    90000: bleu:  14.27, loss: 139744.1406, ppl:   9.3313, duration: 74.0117s\n",
            "2021-07-14 13:22:39,324 - INFO - joeynmt.training - Epoch   2, Step:    90200, Batch Loss:     1.831403, Tokens per Sec:    15762, Lr: 0.000300\n",
            "2021-07-14 13:23:07,085 - INFO - joeynmt.training - Epoch   2, Step:    90400, Batch Loss:     1.915932, Tokens per Sec:    15920, Lr: 0.000300\n",
            "2021-07-14 13:23:34,786 - INFO - joeynmt.training - Epoch   2, Step:    90600, Batch Loss:     2.073616, Tokens per Sec:    16005, Lr: 0.000300\n",
            "2021-07-14 13:24:02,454 - INFO - joeynmt.training - Epoch   2, Step:    90800, Batch Loss:     1.447480, Tokens per Sec:    15707, Lr: 0.000300\n",
            "2021-07-14 13:24:30,366 - INFO - joeynmt.training - Epoch   2, Step:    91000, Batch Loss:     1.908431, Tokens per Sec:    15932, Lr: 0.000300\n",
            "2021-07-14 13:24:58,120 - INFO - joeynmt.training - Epoch   2, Step:    91200, Batch Loss:     1.924360, Tokens per Sec:    15942, Lr: 0.000300\n",
            "2021-07-14 13:25:25,746 - INFO - joeynmt.training - Epoch   2, Step:    91400, Batch Loss:     1.936751, Tokens per Sec:    15823, Lr: 0.000300\n",
            "2021-07-14 13:25:53,314 - INFO - joeynmt.training - Epoch   2, Step:    91600, Batch Loss:     1.864744, Tokens per Sec:    15731, Lr: 0.000300\n",
            "2021-07-14 13:26:20,771 - INFO - joeynmt.training - Epoch   2, Step:    91800, Batch Loss:     1.892571, Tokens per Sec:    15865, Lr: 0.000300\n",
            "2021-07-14 13:26:48,472 - INFO - joeynmt.training - Epoch   2, Step:    92000, Batch Loss:     1.844113, Tokens per Sec:    15812, Lr: 0.000300\n",
            "2021-07-14 13:27:15,793 - INFO - joeynmt.training - Epoch   2, Step:    92200, Batch Loss:     1.971834, Tokens per Sec:    15684, Lr: 0.000300\n",
            "2021-07-14 13:27:43,615 - INFO - joeynmt.training - Epoch   2, Step:    92400, Batch Loss:     1.991147, Tokens per Sec:    15670, Lr: 0.000300\n",
            "2021-07-14 13:28:11,247 - INFO - joeynmt.training - Epoch   2, Step:    92600, Batch Loss:     1.709475, Tokens per Sec:    15823, Lr: 0.000300\n",
            "2021-07-14 13:28:38,920 - INFO - joeynmt.training - Epoch   2, Step:    92800, Batch Loss:     1.838915, Tokens per Sec:    15850, Lr: 0.000300\n",
            "2021-07-14 13:29:06,690 - INFO - joeynmt.training - Epoch   2, Step:    93000, Batch Loss:     1.853809, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-14 13:29:34,342 - INFO - joeynmt.training - Epoch   2, Step:    93200, Batch Loss:     1.982193, Tokens per Sec:    15861, Lr: 0.000300\n",
            "2021-07-14 13:30:02,181 - INFO - joeynmt.training - Epoch   2, Step:    93400, Batch Loss:     1.833032, Tokens per Sec:    15818, Lr: 0.000300\n",
            "2021-07-14 13:30:30,285 - INFO - joeynmt.training - Epoch   2, Step:    93600, Batch Loss:     1.965435, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-14 13:30:57,977 - INFO - joeynmt.training - Epoch   2, Step:    93800, Batch Loss:     1.789813, Tokens per Sec:    15999, Lr: 0.000300\n",
            "2021-07-14 13:31:17,644 - INFO - joeynmt.training - Epoch   2: total training loss 10494.00\n",
            "2021-07-14 13:31:17,645 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-14 13:31:26,281 - INFO - joeynmt.training - Epoch   3, Step:    94000, Batch Loss:     1.939229, Tokens per Sec:    14867, Lr: 0.000300\n",
            "2021-07-14 13:31:54,018 - INFO - joeynmt.training - Epoch   3, Step:    94200, Batch Loss:     1.923055, Tokens per Sec:    15902, Lr: 0.000300\n",
            "2021-07-14 13:32:21,653 - INFO - joeynmt.training - Epoch   3, Step:    94400, Batch Loss:     1.768378, Tokens per Sec:    15939, Lr: 0.000300\n",
            "2021-07-14 13:32:49,632 - INFO - joeynmt.training - Epoch   3, Step:    94600, Batch Loss:     2.032071, Tokens per Sec:    15824, Lr: 0.000300\n",
            "2021-07-14 13:33:17,270 - INFO - joeynmt.training - Epoch   3, Step:    94800, Batch Loss:     1.835091, Tokens per Sec:    15945, Lr: 0.000300\n",
            "2021-07-14 13:33:45,040 - INFO - joeynmt.training - Epoch   3, Step:    95000, Batch Loss:     1.836119, Tokens per Sec:    15585, Lr: 0.000300\n",
            "2021-07-14 13:35:00,717 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:35:00,717 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:35:00,717 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:35:01,351 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 13:35:01,351 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 13:35:02,074 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:35:02,075 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 13:35:02,075 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 13:35:02,075 - INFO - joeynmt.training - \tHypothesis: It was touched .\n",
            "2021-07-14 13:35:02,077 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:35:02,077 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 13:35:02,078 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 13:35:02,078 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-14 13:35:02,078 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:35:02,079 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 13:35:02,079 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 13:35:02,079 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 13:35:02,079 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:35:02,080 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 13:35:02,080 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 13:35:02,080 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of goodness in Satan’s world .\n",
            "2021-07-14 13:35:02,080 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    95000: bleu:  14.60, loss: 139600.4531, ppl:   9.3098, duration: 77.0395s\n",
            "2021-07-14 13:35:30,132 - INFO - joeynmt.training - Epoch   3, Step:    95200, Batch Loss:     1.912509, Tokens per Sec:    15624, Lr: 0.000300\n",
            "2021-07-14 13:35:57,739 - INFO - joeynmt.training - Epoch   3, Step:    95400, Batch Loss:     2.202932, Tokens per Sec:    15977, Lr: 0.000300\n",
            "2021-07-14 13:36:25,518 - INFO - joeynmt.training - Epoch   3, Step:    95600, Batch Loss:     1.721986, Tokens per Sec:    15829, Lr: 0.000300\n",
            "2021-07-14 13:36:52,981 - INFO - joeynmt.training - Epoch   3, Step:    95800, Batch Loss:     2.173738, Tokens per Sec:    15838, Lr: 0.000300\n",
            "2021-07-14 13:37:20,439 - INFO - joeynmt.training - Epoch   3, Step:    96000, Batch Loss:     1.770327, Tokens per Sec:    16037, Lr: 0.000300\n",
            "2021-07-14 13:37:48,125 - INFO - joeynmt.training - Epoch   3, Step:    96200, Batch Loss:     1.749487, Tokens per Sec:    15805, Lr: 0.000300\n",
            "2021-07-14 13:38:15,741 - INFO - joeynmt.training - Epoch   3, Step:    96400, Batch Loss:     1.968379, Tokens per Sec:    15816, Lr: 0.000300\n",
            "2021-07-14 13:38:43,590 - INFO - joeynmt.training - Epoch   3, Step:    96600, Batch Loss:     1.837223, Tokens per Sec:    15925, Lr: 0.000300\n",
            "2021-07-14 13:39:11,037 - INFO - joeynmt.training - Epoch   3, Step:    96800, Batch Loss:     1.867717, Tokens per Sec:    15803, Lr: 0.000300\n",
            "2021-07-14 13:39:38,572 - INFO - joeynmt.training - Epoch   3, Step:    97000, Batch Loss:     1.711019, Tokens per Sec:    15727, Lr: 0.000300\n",
            "2021-07-14 13:40:06,455 - INFO - joeynmt.training - Epoch   3, Step:    97200, Batch Loss:     1.932143, Tokens per Sec:    15724, Lr: 0.000300\n",
            "2021-07-14 13:40:34,128 - INFO - joeynmt.training - Epoch   3, Step:    97400, Batch Loss:     1.819742, Tokens per Sec:    15816, Lr: 0.000300\n",
            "2021-07-14 13:41:01,726 - INFO - joeynmt.training - Epoch   3, Step:    97600, Batch Loss:     2.883319, Tokens per Sec:    15860, Lr: 0.000300\n",
            "2021-07-14 13:41:29,539 - INFO - joeynmt.training - Epoch   3, Step:    97800, Batch Loss:     1.799924, Tokens per Sec:    15877, Lr: 0.000300\n",
            "2021-07-14 13:41:57,396 - INFO - joeynmt.training - Epoch   3, Step:    98000, Batch Loss:     1.668525, Tokens per Sec:    16014, Lr: 0.000300\n",
            "2021-07-14 13:42:25,186 - INFO - joeynmt.training - Epoch   3, Step:    98200, Batch Loss:     1.843223, Tokens per Sec:    15961, Lr: 0.000300\n",
            "2021-07-14 13:42:52,920 - INFO - joeynmt.training - Epoch   3, Step:    98400, Batch Loss:     2.263586, Tokens per Sec:    15611, Lr: 0.000300\n",
            "2021-07-14 13:43:20,553 - INFO - joeynmt.training - Epoch   3, Step:    98600, Batch Loss:     1.991052, Tokens per Sec:    15875, Lr: 0.000300\n",
            "2021-07-14 13:43:48,541 - INFO - joeynmt.training - Epoch   3, Step:    98800, Batch Loss:     2.113751, Tokens per Sec:    15578, Lr: 0.000300\n",
            "2021-07-14 13:44:16,275 - INFO - joeynmt.training - Epoch   3, Step:    99000, Batch Loss:     1.695482, Tokens per Sec:    15640, Lr: 0.000300\n",
            "2021-07-14 13:44:43,800 - INFO - joeynmt.training - Epoch   3, Step:    99200, Batch Loss:     1.939767, Tokens per Sec:    15561, Lr: 0.000300\n",
            "2021-07-14 13:45:11,593 - INFO - joeynmt.training - Epoch   3, Step:    99400, Batch Loss:     1.994806, Tokens per Sec:    15844, Lr: 0.000300\n",
            "2021-07-14 13:45:20,699 - INFO - joeynmt.training - Epoch   3: total training loss 10436.00\n",
            "2021-07-14 13:45:20,699 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-14 13:45:40,070 - INFO - joeynmt.training - Epoch   4, Step:    99600, Batch Loss:     1.701522, Tokens per Sec:    15232, Lr: 0.000300\n",
            "2021-07-14 13:46:07,498 - INFO - joeynmt.training - Epoch   4, Step:    99800, Batch Loss:     2.003929, Tokens per Sec:    15707, Lr: 0.000300\n",
            "2021-07-14 13:46:35,269 - INFO - joeynmt.training - Epoch   4, Step:   100000, Batch Loss:     1.791966, Tokens per Sec:    15750, Lr: 0.000300\n",
            "2021-07-14 13:47:52,429 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 13:47:52,429 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 13:47:52,429 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 13:47:53,070 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 13:47:53,071 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 13:47:53,767 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 13:47:53,768 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 13:47:53,768 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 13:47:53,768 - INFO - joeynmt.training - \tHypothesis: I was touched .\n",
            "2021-07-14 13:47:53,769 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 13:47:53,769 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 13:47:53,769 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 13:47:53,769 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-14 13:47:53,769 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 13:47:53,770 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 13:47:53,770 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 13:47:53,770 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 13:47:53,770 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 13:47:53,771 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 13:47:53,771 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 13:47:53,771 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - fed in Satan’s world .\n",
            "2021-07-14 13:47:53,771 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   100000: bleu:  14.39, loss: 138584.5781, ppl:   9.1599, duration: 78.5020s\n",
            "2021-07-14 13:48:21,590 - INFO - joeynmt.training - Epoch   4, Step:   100200, Batch Loss:     1.747388, Tokens per Sec:    15695, Lr: 0.000300\n",
            "2021-07-14 13:48:49,446 - INFO - joeynmt.training - Epoch   4, Step:   100400, Batch Loss:     1.794414, Tokens per Sec:    15890, Lr: 0.000300\n",
            "2021-07-14 13:49:17,076 - INFO - joeynmt.training - Epoch   4, Step:   100600, Batch Loss:     1.791275, Tokens per Sec:    15855, Lr: 0.000300\n",
            "2021-07-14 13:49:45,090 - INFO - joeynmt.training - Epoch   4, Step:   100800, Batch Loss:     1.905620, Tokens per Sec:    15627, Lr: 0.000300\n",
            "2021-07-14 13:50:13,255 - INFO - joeynmt.training - Epoch   4, Step:   101000, Batch Loss:     1.754429, Tokens per Sec:    15813, Lr: 0.000300\n",
            "2021-07-14 13:50:41,245 - INFO - joeynmt.training - Epoch   4, Step:   101200, Batch Loss:     1.766581, Tokens per Sec:    15707, Lr: 0.000300\n",
            "2021-07-14 13:51:08,966 - INFO - joeynmt.training - Epoch   4, Step:   101400, Batch Loss:     1.850955, Tokens per Sec:    15661, Lr: 0.000300\n",
            "2021-07-14 13:51:36,977 - INFO - joeynmt.training - Epoch   4, Step:   101600, Batch Loss:     1.970292, Tokens per Sec:    16023, Lr: 0.000300\n",
            "2021-07-14 13:52:04,782 - INFO - joeynmt.training - Epoch   4, Step:   101800, Batch Loss:     1.761408, Tokens per Sec:    16073, Lr: 0.000300\n",
            "2021-07-14 13:52:32,652 - INFO - joeynmt.training - Epoch   4, Step:   102000, Batch Loss:     1.748767, Tokens per Sec:    15541, Lr: 0.000300\n",
            "2021-07-14 13:53:00,299 - INFO - joeynmt.training - Epoch   4, Step:   102200, Batch Loss:     2.012413, Tokens per Sec:    15852, Lr: 0.000300\n",
            "2021-07-14 13:53:28,262 - INFO - joeynmt.training - Epoch   4, Step:   102400, Batch Loss:     2.023119, Tokens per Sec:    15842, Lr: 0.000300\n",
            "2021-07-14 13:53:55,768 - INFO - joeynmt.training - Epoch   4, Step:   102600, Batch Loss:     1.739619, Tokens per Sec:    15654, Lr: 0.000300\n",
            "2021-07-14 13:54:23,163 - INFO - joeynmt.training - Epoch   4, Step:   102800, Batch Loss:     1.969777, Tokens per Sec:    15832, Lr: 0.000300\n",
            "2021-07-14 13:54:50,887 - INFO - joeynmt.training - Epoch   4, Step:   103000, Batch Loss:     2.117726, Tokens per Sec:    15628, Lr: 0.000300\n",
            "2021-07-14 13:55:18,589 - INFO - joeynmt.training - Epoch   4, Step:   103200, Batch Loss:     2.000731, Tokens per Sec:    15852, Lr: 0.000300\n",
            "2021-07-14 13:55:46,365 - INFO - joeynmt.training - Epoch   4, Step:   103400, Batch Loss:     1.940063, Tokens per Sec:    15693, Lr: 0.000300\n",
            "2021-07-14 13:56:14,331 - INFO - joeynmt.training - Epoch   4, Step:   103600, Batch Loss:     1.620845, Tokens per Sec:    15913, Lr: 0.000300\n",
            "2021-07-14 13:56:42,078 - INFO - joeynmt.training - Epoch   4, Step:   103800, Batch Loss:     2.022204, Tokens per Sec:    15814, Lr: 0.000300\n",
            "2021-07-14 13:57:09,698 - INFO - joeynmt.training - Epoch   4, Step:   104000, Batch Loss:     1.993470, Tokens per Sec:    15543, Lr: 0.000300\n",
            "2021-07-14 13:57:37,535 - INFO - joeynmt.training - Epoch   4, Step:   104200, Batch Loss:     1.900037, Tokens per Sec:    15801, Lr: 0.000300\n",
            "2021-07-14 13:58:05,343 - INFO - joeynmt.training - Epoch   4, Step:   104400, Batch Loss:     1.785151, Tokens per Sec:    15982, Lr: 0.000300\n",
            "2021-07-14 13:58:33,418 - INFO - joeynmt.training - Epoch   4, Step:   104600, Batch Loss:     2.375848, Tokens per Sec:    15816, Lr: 0.000300\n",
            "2021-07-14 13:59:00,939 - INFO - joeynmt.training - Epoch   4, Step:   104800, Batch Loss:     1.997808, Tokens per Sec:    15605, Lr: 0.000300\n",
            "2021-07-14 13:59:27,279 - INFO - joeynmt.training - Epoch   4: total training loss 10371.65\n",
            "2021-07-14 13:59:27,279 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-14 13:59:29,340 - INFO - joeynmt.training - Epoch   5, Step:   105000, Batch Loss:     2.067222, Tokens per Sec:    10163, Lr: 0.000300\n",
            "2021-07-14 14:00:40,112 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:00:40,112 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:00:40,112 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:00:41,476 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:00:41,477 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 14:00:41,477 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 14:00:41,477 - INFO - joeynmt.training - \tHypothesis: It was a touch .\n",
            "2021-07-14 14:00:41,478 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:00:41,478 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 14:00:41,478 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 14:00:41,478 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-14 14:00:41,479 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:00:41,479 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 14:00:41,479 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:00:41,480 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:00:41,480 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:00:41,480 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 14:00:41,480 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 14:00:41,481 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - reached in Satan’s world .\n",
            "2021-07-14 14:00:41,481 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   105000: bleu:  15.11, loss: 139485.2188, ppl:   9.2927, duration: 72.1401s\n",
            "2021-07-14 14:01:09,432 - INFO - joeynmt.training - Epoch   5, Step:   105200, Batch Loss:     1.504996, Tokens per Sec:    15846, Lr: 0.000300\n",
            "2021-07-14 14:01:37,130 - INFO - joeynmt.training - Epoch   5, Step:   105400, Batch Loss:     1.886054, Tokens per Sec:    15771, Lr: 0.000300\n",
            "2021-07-14 14:02:04,953 - INFO - joeynmt.training - Epoch   5, Step:   105600, Batch Loss:     1.842597, Tokens per Sec:    15870, Lr: 0.000300\n",
            "2021-07-14 14:02:32,907 - INFO - joeynmt.training - Epoch   5, Step:   105800, Batch Loss:     1.930877, Tokens per Sec:    15877, Lr: 0.000300\n",
            "2021-07-14 14:03:00,678 - INFO - joeynmt.training - Epoch   5, Step:   106000, Batch Loss:     1.828114, Tokens per Sec:    15899, Lr: 0.000300\n",
            "2021-07-14 14:03:28,379 - INFO - joeynmt.training - Epoch   5, Step:   106200, Batch Loss:     1.910062, Tokens per Sec:    15545, Lr: 0.000300\n",
            "2021-07-14 14:03:55,857 - INFO - joeynmt.training - Epoch   5, Step:   106400, Batch Loss:     1.805649, Tokens per Sec:    15725, Lr: 0.000300\n",
            "2021-07-14 14:04:23,811 - INFO - joeynmt.training - Epoch   5, Step:   106600, Batch Loss:     1.830690, Tokens per Sec:    15840, Lr: 0.000300\n",
            "2021-07-14 14:04:51,649 - INFO - joeynmt.training - Epoch   5, Step:   106800, Batch Loss:     1.901598, Tokens per Sec:    15810, Lr: 0.000300\n",
            "2021-07-14 14:05:19,358 - INFO - joeynmt.training - Epoch   5, Step:   107000, Batch Loss:     2.189436, Tokens per Sec:    15932, Lr: 0.000300\n",
            "2021-07-14 14:05:47,173 - INFO - joeynmt.training - Epoch   5, Step:   107200, Batch Loss:     1.987432, Tokens per Sec:    15562, Lr: 0.000300\n",
            "2021-07-14 14:06:14,949 - INFO - joeynmt.training - Epoch   5, Step:   107400, Batch Loss:     1.844111, Tokens per Sec:    15825, Lr: 0.000300\n",
            "2021-07-14 14:06:42,551 - INFO - joeynmt.training - Epoch   5, Step:   107600, Batch Loss:     1.851351, Tokens per Sec:    15560, Lr: 0.000300\n",
            "2021-07-14 14:07:10,188 - INFO - joeynmt.training - Epoch   5, Step:   107800, Batch Loss:     1.822861, Tokens per Sec:    15795, Lr: 0.000300\n",
            "2021-07-14 14:07:37,814 - INFO - joeynmt.training - Epoch   5, Step:   108000, Batch Loss:     1.869002, Tokens per Sec:    15699, Lr: 0.000300\n",
            "2021-07-14 14:08:05,525 - INFO - joeynmt.training - Epoch   5, Step:   108200, Batch Loss:     1.946024, Tokens per Sec:    15548, Lr: 0.000300\n",
            "2021-07-14 14:08:33,407 - INFO - joeynmt.training - Epoch   5, Step:   108400, Batch Loss:     1.869615, Tokens per Sec:    15682, Lr: 0.000300\n",
            "2021-07-14 14:09:01,227 - INFO - joeynmt.training - Epoch   5, Step:   108600, Batch Loss:     1.863462, Tokens per Sec:    16063, Lr: 0.000300\n",
            "2021-07-14 14:09:29,113 - INFO - joeynmt.training - Epoch   5, Step:   108800, Batch Loss:     1.958820, Tokens per Sec:    15664, Lr: 0.000300\n",
            "2021-07-14 14:09:56,784 - INFO - joeynmt.training - Epoch   5, Step:   109000, Batch Loss:     1.528260, Tokens per Sec:    15839, Lr: 0.000300\n",
            "2021-07-14 14:10:24,638 - INFO - joeynmt.training - Epoch   5, Step:   109200, Batch Loss:     2.248158, Tokens per Sec:    16030, Lr: 0.000300\n",
            "2021-07-14 14:10:52,572 - INFO - joeynmt.training - Epoch   5, Step:   109400, Batch Loss:     1.839573, Tokens per Sec:    15883, Lr: 0.000300\n",
            "2021-07-14 14:11:20,229 - INFO - joeynmt.training - Epoch   5, Step:   109600, Batch Loss:     1.812538, Tokens per Sec:    15808, Lr: 0.000300\n",
            "2021-07-14 14:11:48,149 - INFO - joeynmt.training - Epoch   5, Step:   109800, Batch Loss:     1.828060, Tokens per Sec:    15872, Lr: 0.000300\n",
            "2021-07-14 14:12:15,823 - INFO - joeynmt.training - Epoch   5, Step:   110000, Batch Loss:     1.609885, Tokens per Sec:    15943, Lr: 0.000300\n",
            "2021-07-14 14:13:27,096 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:13:27,096 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:13:27,097 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:13:27,711 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 14:13:27,711 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 14:13:28,487 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:13:28,488 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 14:13:28,488 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 14:13:28,488 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-14 14:13:28,488 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:13:28,488 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 14:13:28,489 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 14:13:28,489 - INFO - joeynmt.training - \tHypothesis: The text was written in the front of the scroll .\n",
            "2021-07-14 14:13:28,489 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:13:28,489 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 14:13:28,490 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:13:28,490 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:13:28,490 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:13:28,490 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 14:13:28,490 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 14:13:28,491 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-14 14:13:28,491 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   110000: bleu:  15.45, loss: 137181.0625, ppl:   8.9567, duration: 72.6674s\n",
            "2021-07-14 14:13:56,026 - INFO - joeynmt.training - Epoch   5, Step:   110200, Batch Loss:     1.533219, Tokens per Sec:    15426, Lr: 0.000300\n",
            "2021-07-14 14:14:23,912 - INFO - joeynmt.training - Epoch   5, Step:   110400, Batch Loss:     1.750777, Tokens per Sec:    15813, Lr: 0.000300\n",
            "2021-07-14 14:14:39,696 - INFO - joeynmt.training - Epoch   5: total training loss 10309.06\n",
            "2021-07-14 14:14:39,697 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-14 14:14:52,295 - INFO - joeynmt.training - Epoch   6, Step:   110600, Batch Loss:     1.818014, Tokens per Sec:    15413, Lr: 0.000300\n",
            "2021-07-14 14:15:19,781 - INFO - joeynmt.training - Epoch   6, Step:   110800, Batch Loss:     1.709054, Tokens per Sec:    15714, Lr: 0.000300\n",
            "2021-07-14 14:15:47,290 - INFO - joeynmt.training - Epoch   6, Step:   111000, Batch Loss:     1.922979, Tokens per Sec:    15692, Lr: 0.000300\n",
            "2021-07-14 14:16:14,807 - INFO - joeynmt.training - Epoch   6, Step:   111200, Batch Loss:     1.923857, Tokens per Sec:    15861, Lr: 0.000300\n",
            "2021-07-14 14:16:42,364 - INFO - joeynmt.training - Epoch   6, Step:   111400, Batch Loss:     1.977822, Tokens per Sec:    15569, Lr: 0.000300\n",
            "2021-07-14 14:17:10,022 - INFO - joeynmt.training - Epoch   6, Step:   111600, Batch Loss:     1.815624, Tokens per Sec:    15820, Lr: 0.000300\n",
            "2021-07-14 14:17:37,495 - INFO - joeynmt.training - Epoch   6, Step:   111800, Batch Loss:     1.678168, Tokens per Sec:    15951, Lr: 0.000300\n",
            "2021-07-14 14:18:05,370 - INFO - joeynmt.training - Epoch   6, Step:   112000, Batch Loss:     1.839334, Tokens per Sec:    15888, Lr: 0.000300\n",
            "2021-07-14 14:18:33,086 - INFO - joeynmt.training - Epoch   6, Step:   112200, Batch Loss:     1.819330, Tokens per Sec:    16064, Lr: 0.000300\n",
            "2021-07-14 14:19:01,118 - INFO - joeynmt.training - Epoch   6, Step:   112400, Batch Loss:     1.681005, Tokens per Sec:    15631, Lr: 0.000300\n",
            "2021-07-14 14:19:29,274 - INFO - joeynmt.training - Epoch   6, Step:   112600, Batch Loss:     1.869967, Tokens per Sec:    15655, Lr: 0.000300\n",
            "2021-07-14 14:19:56,870 - INFO - joeynmt.training - Epoch   6, Step:   112800, Batch Loss:     1.659826, Tokens per Sec:    15782, Lr: 0.000300\n",
            "2021-07-14 14:20:24,622 - INFO - joeynmt.training - Epoch   6, Step:   113000, Batch Loss:     1.639655, Tokens per Sec:    15655, Lr: 0.000300\n",
            "2021-07-14 14:20:52,438 - INFO - joeynmt.training - Epoch   6, Step:   113200, Batch Loss:     2.181663, Tokens per Sec:    15770, Lr: 0.000300\n",
            "2021-07-14 14:21:20,306 - INFO - joeynmt.training - Epoch   6, Step:   113400, Batch Loss:     1.715755, Tokens per Sec:    15756, Lr: 0.000300\n",
            "2021-07-14 14:21:48,067 - INFO - joeynmt.training - Epoch   6, Step:   113600, Batch Loss:     1.861120, Tokens per Sec:    15540, Lr: 0.000300\n",
            "2021-07-14 14:22:15,947 - INFO - joeynmt.training - Epoch   6, Step:   113800, Batch Loss:     1.605816, Tokens per Sec:    15821, Lr: 0.000300\n",
            "2021-07-14 14:22:43,807 - INFO - joeynmt.training - Epoch   6, Step:   114000, Batch Loss:     2.144216, Tokens per Sec:    15810, Lr: 0.000300\n",
            "2021-07-14 14:23:11,653 - INFO - joeynmt.training - Epoch   6, Step:   114200, Batch Loss:     1.980442, Tokens per Sec:    15937, Lr: 0.000300\n",
            "2021-07-14 14:23:39,335 - INFO - joeynmt.training - Epoch   6, Step:   114400, Batch Loss:     1.616833, Tokens per Sec:    15844, Lr: 0.000300\n",
            "2021-07-14 14:24:07,259 - INFO - joeynmt.training - Epoch   6, Step:   114600, Batch Loss:     1.839722, Tokens per Sec:    15515, Lr: 0.000300\n",
            "2021-07-14 14:24:35,445 - INFO - joeynmt.training - Epoch   6, Step:   114800, Batch Loss:     1.788059, Tokens per Sec:    15783, Lr: 0.000300\n",
            "2021-07-14 14:25:03,099 - INFO - joeynmt.training - Epoch   6, Step:   115000, Batch Loss:     1.825332, Tokens per Sec:    15704, Lr: 0.000300\n",
            "2021-07-14 14:26:15,042 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:26:15,042 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:26:15,042 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:26:16,395 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:26:16,395 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 14:26:16,395 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 14:26:16,395 - INFO - joeynmt.training - \tHypothesis: I was moved to heart .\n",
            "2021-07-14 14:26:16,396 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:26:16,396 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 14:26:16,396 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 14:26:16,396 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-14 14:26:16,396 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:26:16,397 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 14:26:16,397 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:26:16,397 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:26:16,397 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:26:16,398 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 14:26:16,398 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 14:26:16,398 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of goodness in Satan’s world .\n",
            "2021-07-14 14:26:16,398 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   115000: bleu:  15.49, loss: 138259.0938, ppl:   9.1124, duration: 73.2992s\n",
            "2021-07-14 14:26:44,717 - INFO - joeynmt.training - Epoch   6, Step:   115200, Batch Loss:     1.995516, Tokens per Sec:    15498, Lr: 0.000300\n",
            "2021-07-14 14:27:12,468 - INFO - joeynmt.training - Epoch   6, Step:   115400, Batch Loss:     1.881984, Tokens per Sec:    15742, Lr: 0.000300\n",
            "2021-07-14 14:27:40,576 - INFO - joeynmt.training - Epoch   6, Step:   115600, Batch Loss:     1.891060, Tokens per Sec:    15682, Lr: 0.000300\n",
            "2021-07-14 14:28:08,216 - INFO - joeynmt.training - Epoch   6, Step:   115800, Batch Loss:     1.896735, Tokens per Sec:    15620, Lr: 0.000300\n",
            "2021-07-14 14:28:36,054 - INFO - joeynmt.training - Epoch   6, Step:   116000, Batch Loss:     1.889672, Tokens per Sec:    15891, Lr: 0.000300\n",
            "2021-07-14 14:28:41,917 - INFO - joeynmt.training - Epoch   6: total training loss 10265.06\n",
            "2021-07-14 14:28:41,917 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-14 14:29:04,562 - INFO - joeynmt.training - Epoch   7, Step:   116200, Batch Loss:     1.735105, Tokens per Sec:    15360, Lr: 0.000300\n",
            "2021-07-14 14:29:32,083 - INFO - joeynmt.training - Epoch   7, Step:   116400, Batch Loss:     1.735385, Tokens per Sec:    15806, Lr: 0.000300\n",
            "2021-07-14 14:30:00,188 - INFO - joeynmt.training - Epoch   7, Step:   116600, Batch Loss:     2.300202, Tokens per Sec:    15588, Lr: 0.000300\n",
            "2021-07-14 14:30:28,270 - INFO - joeynmt.training - Epoch   7, Step:   116800, Batch Loss:     1.946833, Tokens per Sec:    15588, Lr: 0.000300\n",
            "2021-07-14 14:30:56,107 - INFO - joeynmt.training - Epoch   7, Step:   117000, Batch Loss:     1.969188, Tokens per Sec:    15733, Lr: 0.000300\n",
            "2021-07-14 14:31:24,127 - INFO - joeynmt.training - Epoch   7, Step:   117200, Batch Loss:     1.739979, Tokens per Sec:    15715, Lr: 0.000300\n",
            "2021-07-14 14:31:51,678 - INFO - joeynmt.training - Epoch   7, Step:   117400, Batch Loss:     1.817573, Tokens per Sec:    15488, Lr: 0.000300\n",
            "2021-07-14 14:32:19,636 - INFO - joeynmt.training - Epoch   7, Step:   117600, Batch Loss:     1.764677, Tokens per Sec:    15864, Lr: 0.000300\n",
            "2021-07-14 14:32:47,671 - INFO - joeynmt.training - Epoch   7, Step:   117800, Batch Loss:     1.685816, Tokens per Sec:    15822, Lr: 0.000300\n",
            "2021-07-14 14:33:15,506 - INFO - joeynmt.training - Epoch   7, Step:   118000, Batch Loss:     1.766518, Tokens per Sec:    15889, Lr: 0.000300\n",
            "2021-07-14 14:33:43,733 - INFO - joeynmt.training - Epoch   7, Step:   118200, Batch Loss:     1.869693, Tokens per Sec:    15661, Lr: 0.000300\n",
            "2021-07-14 14:34:11,446 - INFO - joeynmt.training - Epoch   7, Step:   118400, Batch Loss:     1.935569, Tokens per Sec:    15589, Lr: 0.000300\n",
            "2021-07-14 14:34:39,347 - INFO - joeynmt.training - Epoch   7, Step:   118600, Batch Loss:     1.525676, Tokens per Sec:    15788, Lr: 0.000300\n",
            "2021-07-14 14:35:07,321 - INFO - joeynmt.training - Epoch   7, Step:   118800, Batch Loss:     2.101152, Tokens per Sec:    15564, Lr: 0.000300\n",
            "2021-07-14 14:35:35,422 - INFO - joeynmt.training - Epoch   7, Step:   119000, Batch Loss:     1.915470, Tokens per Sec:    15632, Lr: 0.000300\n",
            "2021-07-14 14:36:03,065 - INFO - joeynmt.training - Epoch   7, Step:   119200, Batch Loss:     1.683448, Tokens per Sec:    15786, Lr: 0.000300\n",
            "2021-07-14 14:36:31,288 - INFO - joeynmt.training - Epoch   7, Step:   119400, Batch Loss:     2.153136, Tokens per Sec:    15697, Lr: 0.000300\n",
            "2021-07-14 14:36:59,314 - INFO - joeynmt.training - Epoch   7, Step:   119600, Batch Loss:     1.724776, Tokens per Sec:    15808, Lr: 0.000300\n",
            "2021-07-14 14:37:27,417 - INFO - joeynmt.training - Epoch   7, Step:   119800, Batch Loss:     1.849840, Tokens per Sec:    15778, Lr: 0.000300\n",
            "2021-07-14 14:37:55,371 - INFO - joeynmt.training - Epoch   7, Step:   120000, Batch Loss:     1.889447, Tokens per Sec:    15793, Lr: 0.000300\n",
            "2021-07-14 14:39:04,712 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:39:04,713 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:39:04,713 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:39:05,325 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 14:39:05,326 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 14:39:06,061 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:39:06,061 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 14:39:06,061 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 14:39:06,062 - INFO - joeynmt.training - \tHypothesis: It was a touch of heart .\n",
            "2021-07-14 14:39:06,062 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:39:06,063 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 14:39:06,063 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 14:39:06,063 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the side of the scroll .\n",
            "2021-07-14 14:39:06,063 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:39:06,064 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 14:39:06,064 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:39:06,064 - INFO - joeynmt.training - \tHypothesis: Rather than anxiety or depression , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:39:06,064 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:39:06,064 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 14:39:06,065 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 14:39:06,065 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show some of the extent that they have been successful in Satan’s world .\n",
            "2021-07-14 14:39:06,065 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   120000: bleu:  15.67, loss: 136506.1094, ppl:   8.8606, duration: 70.6931s\n",
            "2021-07-14 14:39:33,686 - INFO - joeynmt.training - Epoch   7, Step:   120200, Batch Loss:     1.850174, Tokens per Sec:    15351, Lr: 0.000300\n",
            "2021-07-14 14:40:01,594 - INFO - joeynmt.training - Epoch   7, Step:   120400, Batch Loss:     1.907175, Tokens per Sec:    15711, Lr: 0.000300\n",
            "2021-07-14 14:40:29,319 - INFO - joeynmt.training - Epoch   7, Step:   120600, Batch Loss:     1.915244, Tokens per Sec:    15895, Lr: 0.000300\n",
            "2021-07-14 14:40:57,249 - INFO - joeynmt.training - Epoch   7, Step:   120800, Batch Loss:     1.628445, Tokens per Sec:    15547, Lr: 0.000300\n",
            "2021-07-14 14:41:24,931 - INFO - joeynmt.training - Epoch   7, Step:   121000, Batch Loss:     1.919976, Tokens per Sec:    15770, Lr: 0.000300\n",
            "2021-07-14 14:41:52,852 - INFO - joeynmt.training - Epoch   7, Step:   121200, Batch Loss:     1.568592, Tokens per Sec:    15723, Lr: 0.000300\n",
            "2021-07-14 14:42:21,141 - INFO - joeynmt.training - Epoch   7, Step:   121400, Batch Loss:     1.849473, Tokens per Sec:    15953, Lr: 0.000300\n",
            "2021-07-14 14:42:43,154 - INFO - joeynmt.training - Epoch   7: total training loss 10191.37\n",
            "2021-07-14 14:42:43,154 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-14 14:42:49,602 - INFO - joeynmt.training - Epoch   8, Step:   121600, Batch Loss:     1.826452, Tokens per Sec:    14314, Lr: 0.000300\n",
            "2021-07-14 14:43:17,071 - INFO - joeynmt.training - Epoch   8, Step:   121800, Batch Loss:     2.055963, Tokens per Sec:    15726, Lr: 0.000300\n",
            "2021-07-14 14:43:44,929 - INFO - joeynmt.training - Epoch   8, Step:   122000, Batch Loss:     1.818022, Tokens per Sec:    15816, Lr: 0.000300\n",
            "2021-07-14 14:44:12,544 - INFO - joeynmt.training - Epoch   8, Step:   122200, Batch Loss:     1.568148, Tokens per Sec:    15846, Lr: 0.000300\n",
            "2021-07-14 14:44:40,370 - INFO - joeynmt.training - Epoch   8, Step:   122400, Batch Loss:     1.950512, Tokens per Sec:    15870, Lr: 0.000300\n",
            "2021-07-14 14:45:07,879 - INFO - joeynmt.training - Epoch   8, Step:   122600, Batch Loss:     1.681261, Tokens per Sec:    15598, Lr: 0.000300\n",
            "2021-07-14 14:45:35,838 - INFO - joeynmt.training - Epoch   8, Step:   122800, Batch Loss:     1.812610, Tokens per Sec:    15945, Lr: 0.000300\n",
            "2021-07-14 14:46:03,530 - INFO - joeynmt.training - Epoch   8, Step:   123000, Batch Loss:     1.772542, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-14 14:46:31,452 - INFO - joeynmt.training - Epoch   8, Step:   123200, Batch Loss:     2.118108, Tokens per Sec:    15855, Lr: 0.000300\n",
            "2021-07-14 14:46:59,081 - INFO - joeynmt.training - Epoch   8, Step:   123400, Batch Loss:     1.730805, Tokens per Sec:    16007, Lr: 0.000300\n",
            "2021-07-14 14:47:26,712 - INFO - joeynmt.training - Epoch   8, Step:   123600, Batch Loss:     1.782927, Tokens per Sec:    15677, Lr: 0.000300\n",
            "2021-07-14 14:47:54,391 - INFO - joeynmt.training - Epoch   8, Step:   123800, Batch Loss:     1.969998, Tokens per Sec:    15903, Lr: 0.000300\n",
            "2021-07-14 14:48:21,871 - INFO - joeynmt.training - Epoch   8, Step:   124000, Batch Loss:     1.927000, Tokens per Sec:    15675, Lr: 0.000300\n",
            "2021-07-14 14:48:49,663 - INFO - joeynmt.training - Epoch   8, Step:   124200, Batch Loss:     1.854849, Tokens per Sec:    15726, Lr: 0.000300\n",
            "2021-07-14 14:49:17,192 - INFO - joeynmt.training - Epoch   8, Step:   124400, Batch Loss:     1.884728, Tokens per Sec:    16020, Lr: 0.000300\n",
            "2021-07-14 14:49:45,144 - INFO - joeynmt.training - Epoch   8, Step:   124600, Batch Loss:     1.901682, Tokens per Sec:    15889, Lr: 0.000300\n",
            "2021-07-14 14:50:12,804 - INFO - joeynmt.training - Epoch   8, Step:   124800, Batch Loss:     1.960574, Tokens per Sec:    15874, Lr: 0.000300\n",
            "2021-07-14 14:50:40,360 - INFO - joeynmt.training - Epoch   8, Step:   125000, Batch Loss:     1.987956, Tokens per Sec:    15844, Lr: 0.000300\n",
            "2021-07-14 14:51:55,126 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 14:51:55,126 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 14:51:55,126 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 14:51:56,510 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 14:51:56,512 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 14:51:56,512 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 14:51:56,512 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-14 14:51:56,512 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 14:51:56,513 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 14:51:56,513 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 14:51:56,513 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the side of the scroll .\n",
            "2021-07-14 14:51:56,513 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 14:51:56,514 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 14:51:56,514 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:51:56,514 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 14:51:56,515 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 14:51:56,515 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 14:51:56,515 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 14:51:56,516 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-14 14:51:56,516 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   125000: bleu:  15.49, loss: 136562.4375, ppl:   8.8686, duration: 76.1554s\n",
            "2021-07-14 14:52:24,717 - INFO - joeynmt.training - Epoch   8, Step:   125200, Batch Loss:     2.252695, Tokens per Sec:    15587, Lr: 0.000300\n",
            "2021-07-14 14:52:52,691 - INFO - joeynmt.training - Epoch   8, Step:   125400, Batch Loss:     1.967525, Tokens per Sec:    15906, Lr: 0.000300\n",
            "2021-07-14 14:53:20,448 - INFO - joeynmt.training - Epoch   8, Step:   125600, Batch Loss:     1.806883, Tokens per Sec:    15794, Lr: 0.000300\n",
            "2021-07-14 14:53:48,406 - INFO - joeynmt.training - Epoch   8, Step:   125800, Batch Loss:     1.807191, Tokens per Sec:    15710, Lr: 0.000300\n",
            "2021-07-14 14:54:16,034 - INFO - joeynmt.training - Epoch   8, Step:   126000, Batch Loss:     1.858602, Tokens per Sec:    15802, Lr: 0.000300\n",
            "2021-07-14 14:54:43,959 - INFO - joeynmt.training - Epoch   8, Step:   126200, Batch Loss:     1.632776, Tokens per Sec:    15784, Lr: 0.000300\n",
            "2021-07-14 14:55:11,936 - INFO - joeynmt.training - Epoch   8, Step:   126400, Batch Loss:     1.899934, Tokens per Sec:    16019, Lr: 0.000300\n",
            "2021-07-14 14:55:40,117 - INFO - joeynmt.training - Epoch   8, Step:   126600, Batch Loss:     1.937876, Tokens per Sec:    15752, Lr: 0.000300\n",
            "2021-07-14 14:56:08,123 - INFO - joeynmt.training - Epoch   8, Step:   126800, Batch Loss:     1.790748, Tokens per Sec:    15508, Lr: 0.000300\n",
            "2021-07-14 14:56:36,217 - INFO - joeynmt.training - Epoch   8, Step:   127000, Batch Loss:     1.792162, Tokens per Sec:    15758, Lr: 0.000300\n",
            "2021-07-14 14:56:46,226 - INFO - joeynmt.training - Epoch   8: total training loss 10140.79\n",
            "2021-07-14 14:56:46,227 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-14 14:57:05,336 - INFO - joeynmt.training - Epoch   9, Step:   127200, Batch Loss:     1.885556, Tokens per Sec:    15192, Lr: 0.000300\n",
            "2021-07-14 14:57:33,284 - INFO - joeynmt.training - Epoch   9, Step:   127400, Batch Loss:     2.017339, Tokens per Sec:    15431, Lr: 0.000300\n",
            "2021-07-14 14:58:00,889 - INFO - joeynmt.training - Epoch   9, Step:   127600, Batch Loss:     2.638165, Tokens per Sec:    15632, Lr: 0.000300\n",
            "2021-07-14 14:58:28,730 - INFO - joeynmt.training - Epoch   9, Step:   127800, Batch Loss:     1.589227, Tokens per Sec:    15515, Lr: 0.000300\n",
            "2021-07-14 14:58:56,482 - INFO - joeynmt.training - Epoch   9, Step:   128000, Batch Loss:     2.437955, Tokens per Sec:    15705, Lr: 0.000300\n",
            "2021-07-14 14:59:24,510 - INFO - joeynmt.training - Epoch   9, Step:   128200, Batch Loss:     1.883506, Tokens per Sec:    15890, Lr: 0.000300\n",
            "2021-07-14 14:59:52,582 - INFO - joeynmt.training - Epoch   9, Step:   128400, Batch Loss:     1.941667, Tokens per Sec:    15774, Lr: 0.000300\n",
            "2021-07-14 15:00:20,634 - INFO - joeynmt.training - Epoch   9, Step:   128600, Batch Loss:     1.784120, Tokens per Sec:    15908, Lr: 0.000300\n",
            "2021-07-14 15:00:48,345 - INFO - joeynmt.training - Epoch   9, Step:   128800, Batch Loss:     1.834907, Tokens per Sec:    15592, Lr: 0.000300\n",
            "2021-07-14 15:01:16,196 - INFO - joeynmt.training - Epoch   9, Step:   129000, Batch Loss:     1.722788, Tokens per Sec:    15612, Lr: 0.000300\n",
            "2021-07-14 15:01:44,060 - INFO - joeynmt.training - Epoch   9, Step:   129200, Batch Loss:     1.836332, Tokens per Sec:    15720, Lr: 0.000300\n",
            "2021-07-14 15:02:12,078 - INFO - joeynmt.training - Epoch   9, Step:   129400, Batch Loss:     1.788657, Tokens per Sec:    15818, Lr: 0.000300\n",
            "2021-07-14 15:02:39,599 - INFO - joeynmt.training - Epoch   9, Step:   129600, Batch Loss:     2.316935, Tokens per Sec:    15752, Lr: 0.000300\n",
            "2021-07-14 15:03:07,636 - INFO - joeynmt.training - Epoch   9, Step:   129800, Batch Loss:     1.719221, Tokens per Sec:    15437, Lr: 0.000300\n",
            "2021-07-14 15:03:35,448 - INFO - joeynmt.training - Epoch   9, Step:   130000, Batch Loss:     1.675867, Tokens per Sec:    15385, Lr: 0.000300\n",
            "2021-07-14 15:04:49,646 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 15:04:49,646 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 15:04:49,647 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 15:04:50,286 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 15:04:50,286 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 15:04:51,058 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 15:04:51,058 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 15:04:51,058 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 15:04:51,059 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-14 15:04:51,059 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 15:04:51,059 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 15:04:51,059 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 15:04:51,060 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-14 15:04:51,060 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 15:04:51,060 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 15:04:51,060 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:04:51,060 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should continue our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:04:51,061 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 15:04:51,061 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 15:04:51,061 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 15:04:51,061 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-14 15:04:51,061 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   130000: bleu:  15.73, loss: 136139.1875, ppl:   8.8088, duration: 75.6129s\n",
            "2021-07-14 15:05:19,207 - INFO - joeynmt.training - Epoch   9, Step:   130200, Batch Loss:     1.878584, Tokens per Sec:    15563, Lr: 0.000300\n",
            "2021-07-14 15:05:47,313 - INFO - joeynmt.training - Epoch   9, Step:   130400, Batch Loss:     1.814399, Tokens per Sec:    15545, Lr: 0.000300\n",
            "2021-07-14 15:06:15,213 - INFO - joeynmt.training - Epoch   9, Step:   130600, Batch Loss:     1.807510, Tokens per Sec:    15804, Lr: 0.000300\n",
            "2021-07-14 15:06:43,337 - INFO - joeynmt.training - Epoch   9, Step:   130800, Batch Loss:     2.032802, Tokens per Sec:    15627, Lr: 0.000300\n",
            "2021-07-14 15:07:11,433 - INFO - joeynmt.training - Epoch   9, Step:   131000, Batch Loss:     1.868724, Tokens per Sec:    15890, Lr: 0.000300\n",
            "2021-07-14 15:07:39,088 - INFO - joeynmt.training - Epoch   9, Step:   131200, Batch Loss:     1.758479, Tokens per Sec:    15796, Lr: 0.000300\n",
            "2021-07-14 15:08:07,178 - INFO - joeynmt.training - Epoch   9, Step:   131400, Batch Loss:     1.720775, Tokens per Sec:    15622, Lr: 0.000300\n",
            "2021-07-14 15:08:35,174 - INFO - joeynmt.training - Epoch   9, Step:   131600, Batch Loss:     1.752280, Tokens per Sec:    15793, Lr: 0.000300\n",
            "2021-07-14 15:09:02,895 - INFO - joeynmt.training - Epoch   9, Step:   131800, Batch Loss:     1.672926, Tokens per Sec:    15783, Lr: 0.000300\n",
            "2021-07-14 15:09:30,719 - INFO - joeynmt.training - Epoch   9, Step:   132000, Batch Loss:     1.964465, Tokens per Sec:    15602, Lr: 0.000300\n",
            "2021-07-14 15:09:58,497 - INFO - joeynmt.training - Epoch   9, Step:   132200, Batch Loss:     1.965758, Tokens per Sec:    15726, Lr: 0.000300\n",
            "2021-07-14 15:10:26,655 - INFO - joeynmt.training - Epoch   9, Step:   132400, Batch Loss:     1.934544, Tokens per Sec:    15512, Lr: 0.000300\n",
            "2021-07-14 15:10:54,580 - INFO - joeynmt.training - Epoch   9, Step:   132600, Batch Loss:     1.813857, Tokens per Sec:    15645, Lr: 0.000300\n",
            "2021-07-14 15:10:54,857 - INFO - joeynmt.training - Epoch   9: total training loss 10120.28\n",
            "2021-07-14 15:10:54,858 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-14 15:11:22,951 - INFO - joeynmt.training - Epoch  10, Step:   132800, Batch Loss:     1.757213, Tokens per Sec:    15292, Lr: 0.000300\n",
            "2021-07-14 15:11:50,897 - INFO - joeynmt.training - Epoch  10, Step:   133000, Batch Loss:     2.028235, Tokens per Sec:    15184, Lr: 0.000300\n",
            "2021-07-14 15:12:18,831 - INFO - joeynmt.training - Epoch  10, Step:   133200, Batch Loss:     1.958326, Tokens per Sec:    15689, Lr: 0.000300\n",
            "2021-07-14 15:12:46,701 - INFO - joeynmt.training - Epoch  10, Step:   133400, Batch Loss:     1.976657, Tokens per Sec:    15570, Lr: 0.000300\n",
            "2021-07-14 15:13:15,041 - INFO - joeynmt.training - Epoch  10, Step:   133600, Batch Loss:     1.714355, Tokens per Sec:    15545, Lr: 0.000300\n",
            "2021-07-14 15:13:43,111 - INFO - joeynmt.training - Epoch  10, Step:   133800, Batch Loss:     1.986488, Tokens per Sec:    15610, Lr: 0.000300\n",
            "2021-07-14 15:14:10,862 - INFO - joeynmt.training - Epoch  10, Step:   134000, Batch Loss:     1.856480, Tokens per Sec:    15861, Lr: 0.000300\n",
            "2021-07-14 15:14:38,846 - INFO - joeynmt.training - Epoch  10, Step:   134200, Batch Loss:     1.692654, Tokens per Sec:    15803, Lr: 0.000300\n",
            "2021-07-14 15:15:06,774 - INFO - joeynmt.training - Epoch  10, Step:   134400, Batch Loss:     1.861225, Tokens per Sec:    15811, Lr: 0.000300\n",
            "2021-07-14 15:15:34,304 - INFO - joeynmt.training - Epoch  10, Step:   134600, Batch Loss:     1.857651, Tokens per Sec:    15426, Lr: 0.000300\n",
            "2021-07-14 15:16:02,115 - INFO - joeynmt.training - Epoch  10, Step:   134800, Batch Loss:     1.704780, Tokens per Sec:    15586, Lr: 0.000300\n",
            "2021-07-14 15:16:30,254 - INFO - joeynmt.training - Epoch  10, Step:   135000, Batch Loss:     1.607082, Tokens per Sec:    15770, Lr: 0.000300\n",
            "2021-07-14 15:17:43,985 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 15:17:43,986 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 15:17:43,986 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 15:17:44,658 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 15:17:44,658 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 15:17:45,412 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 15:17:45,412 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 15:17:45,413 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 15:17:45,413 - INFO - joeynmt.training - \tHypothesis: It was touched by my heart .\n",
            "2021-07-14 15:17:45,413 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 15:17:45,413 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 15:17:45,413 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 15:17:45,414 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-14 15:17:45,414 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 15:17:45,415 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 15:17:45,415 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:17:45,415 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:17:45,415 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 15:17:45,416 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 15:17:45,416 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 15:17:45,416 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of kindness in Satan’s world .\n",
            "2021-07-14 15:17:45,416 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   135000: bleu:  15.83, loss: 135421.0781, ppl:   8.7083, duration: 75.1617s\n",
            "2021-07-14 15:18:13,622 - INFO - joeynmt.training - Epoch  10, Step:   135200, Batch Loss:     1.961626, Tokens per Sec:    15860, Lr: 0.000300\n",
            "2021-07-14 15:18:40,995 - INFO - joeynmt.training - Epoch  10, Step:   135400, Batch Loss:     1.911136, Tokens per Sec:    15464, Lr: 0.000300\n",
            "2021-07-14 15:19:08,803 - INFO - joeynmt.training - Epoch  10, Step:   135600, Batch Loss:     1.922495, Tokens per Sec:    15714, Lr: 0.000300\n",
            "2021-07-14 15:19:36,713 - INFO - joeynmt.training - Epoch  10, Step:   135800, Batch Loss:     1.805030, Tokens per Sec:    15784, Lr: 0.000300\n",
            "2021-07-14 15:20:04,575 - INFO - joeynmt.training - Epoch  10, Step:   136000, Batch Loss:     1.841452, Tokens per Sec:    16045, Lr: 0.000300\n",
            "2021-07-14 15:20:32,541 - INFO - joeynmt.training - Epoch  10, Step:   136200, Batch Loss:     1.624023, Tokens per Sec:    15788, Lr: 0.000300\n",
            "2021-07-14 15:21:00,113 - INFO - joeynmt.training - Epoch  10, Step:   136400, Batch Loss:     1.946053, Tokens per Sec:    15911, Lr: 0.000300\n",
            "2021-07-14 15:21:27,756 - INFO - joeynmt.training - Epoch  10, Step:   136600, Batch Loss:     1.864555, Tokens per Sec:    15689, Lr: 0.000300\n",
            "2021-07-14 15:21:55,538 - INFO - joeynmt.training - Epoch  10, Step:   136800, Batch Loss:     1.893402, Tokens per Sec:    15858, Lr: 0.000300\n",
            "2021-07-14 15:22:23,280 - INFO - joeynmt.training - Epoch  10, Step:   137000, Batch Loss:     1.866542, Tokens per Sec:    16052, Lr: 0.000300\n",
            "2021-07-14 15:22:51,460 - INFO - joeynmt.training - Epoch  10, Step:   137200, Batch Loss:     1.586090, Tokens per Sec:    16016, Lr: 0.000300\n",
            "2021-07-14 15:23:19,062 - INFO - joeynmt.training - Epoch  10, Step:   137400, Batch Loss:     1.657510, Tokens per Sec:    15900, Lr: 0.000300\n",
            "2021-07-14 15:23:46,922 - INFO - joeynmt.training - Epoch  10, Step:   137600, Batch Loss:     1.604256, Tokens per Sec:    15837, Lr: 0.000300\n",
            "2021-07-14 15:24:14,615 - INFO - joeynmt.training - Epoch  10, Step:   137800, Batch Loss:     1.771077, Tokens per Sec:    15792, Lr: 0.000300\n",
            "2021-07-14 15:24:42,287 - INFO - joeynmt.training - Epoch  10, Step:   138000, Batch Loss:     1.671536, Tokens per Sec:    16033, Lr: 0.000300\n",
            "2021-07-14 15:24:58,985 - INFO - joeynmt.training - Epoch  10: total training loss 10058.08\n",
            "2021-07-14 15:24:58,986 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-14 15:25:10,333 - INFO - joeynmt.training - Epoch  11, Step:   138200, Batch Loss:     1.729334, Tokens per Sec:    15036, Lr: 0.000300\n",
            "2021-07-14 15:25:38,092 - INFO - joeynmt.training - Epoch  11, Step:   138400, Batch Loss:     1.875579, Tokens per Sec:    15847, Lr: 0.000300\n",
            "2021-07-14 15:26:05,784 - INFO - joeynmt.training - Epoch  11, Step:   138600, Batch Loss:     1.781268, Tokens per Sec:    15995, Lr: 0.000300\n",
            "2021-07-14 15:26:33,561 - INFO - joeynmt.training - Epoch  11, Step:   138800, Batch Loss:     1.746588, Tokens per Sec:    15687, Lr: 0.000300\n",
            "2021-07-14 15:27:01,066 - INFO - joeynmt.training - Epoch  11, Step:   139000, Batch Loss:     1.635142, Tokens per Sec:    15880, Lr: 0.000300\n",
            "2021-07-14 15:27:28,765 - INFO - joeynmt.training - Epoch  11, Step:   139200, Batch Loss:     2.035679, Tokens per Sec:    15775, Lr: 0.000300\n",
            "2021-07-14 15:27:56,498 - INFO - joeynmt.training - Epoch  11, Step:   139400, Batch Loss:     1.799628, Tokens per Sec:    15939, Lr: 0.000300\n",
            "2021-07-14 15:28:24,118 - INFO - joeynmt.training - Epoch  11, Step:   139600, Batch Loss:     1.728045, Tokens per Sec:    15936, Lr: 0.000300\n",
            "2021-07-14 15:28:52,018 - INFO - joeynmt.training - Epoch  11, Step:   139800, Batch Loss:     1.726879, Tokens per Sec:    15947, Lr: 0.000300\n",
            "2021-07-14 15:29:19,718 - INFO - joeynmt.training - Epoch  11, Step:   140000, Batch Loss:     2.017310, Tokens per Sec:    15924, Lr: 0.000300\n",
            "2021-07-14 15:30:32,934 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 15:30:32,934 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 15:30:32,934 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 15:30:33,571 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 15:30:33,571 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 15:30:34,263 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 15:30:34,263 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 15:30:34,264 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 15:30:34,264 - INFO - joeynmt.training - \tHypothesis: I was touched .\n",
            "2021-07-14 15:30:34,264 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 15:30:34,264 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 15:30:34,264 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 15:30:34,265 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-14 15:30:34,265 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 15:30:34,265 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 15:30:34,265 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:30:34,265 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:30:34,266 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 15:30:34,267 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 15:30:34,267 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 15:30:34,267 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of kindness in Satan’s world .\n",
            "2021-07-14 15:30:34,268 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   140000: bleu:  16.10, loss: 135106.3594, ppl:   8.6646, duration: 74.5490s\n",
            "2021-07-14 15:31:01,653 - INFO - joeynmt.training - Epoch  11, Step:   140200, Batch Loss:     1.910434, Tokens per Sec:    15663, Lr: 0.000300\n",
            "2021-07-14 15:31:29,329 - INFO - joeynmt.training - Epoch  11, Step:   140400, Batch Loss:     2.142422, Tokens per Sec:    15619, Lr: 0.000300\n",
            "2021-07-14 15:31:56,802 - INFO - joeynmt.training - Epoch  11, Step:   140600, Batch Loss:     1.551666, Tokens per Sec:    15917, Lr: 0.000300\n",
            "2021-07-14 15:32:24,463 - INFO - joeynmt.training - Epoch  11, Step:   140800, Batch Loss:     1.705934, Tokens per Sec:    15892, Lr: 0.000300\n",
            "2021-07-14 15:32:52,146 - INFO - joeynmt.training - Epoch  11, Step:   141000, Batch Loss:     1.617638, Tokens per Sec:    15640, Lr: 0.000300\n",
            "2021-07-14 15:33:19,779 - INFO - joeynmt.training - Epoch  11, Step:   141200, Batch Loss:     1.494266, Tokens per Sec:    15899, Lr: 0.000300\n",
            "2021-07-14 15:33:47,602 - INFO - joeynmt.training - Epoch  11, Step:   141400, Batch Loss:     1.803144, Tokens per Sec:    15736, Lr: 0.000300\n",
            "2021-07-14 15:34:15,036 - INFO - joeynmt.training - Epoch  11, Step:   141600, Batch Loss:     1.862210, Tokens per Sec:    16044, Lr: 0.000300\n",
            "2021-07-14 15:34:42,762 - INFO - joeynmt.training - Epoch  11, Step:   141800, Batch Loss:     1.768273, Tokens per Sec:    15698, Lr: 0.000300\n",
            "2021-07-14 15:35:10,447 - INFO - joeynmt.training - Epoch  11, Step:   142000, Batch Loss:     1.807503, Tokens per Sec:    15651, Lr: 0.000300\n",
            "2021-07-14 15:35:38,399 - INFO - joeynmt.training - Epoch  11, Step:   142200, Batch Loss:     1.829038, Tokens per Sec:    15837, Lr: 0.000300\n",
            "2021-07-14 15:36:05,912 - INFO - joeynmt.training - Epoch  11, Step:   142400, Batch Loss:     1.858395, Tokens per Sec:    15837, Lr: 0.000300\n",
            "2021-07-14 15:36:33,683 - INFO - joeynmt.training - Epoch  11, Step:   142600, Batch Loss:     2.177401, Tokens per Sec:    15742, Lr: 0.000300\n",
            "2021-07-14 15:37:01,091 - INFO - joeynmt.training - Epoch  11, Step:   142800, Batch Loss:     2.126282, Tokens per Sec:    15998, Lr: 0.000300\n",
            "2021-07-14 15:37:28,521 - INFO - joeynmt.training - Epoch  11, Step:   143000, Batch Loss:     1.679859, Tokens per Sec:    15728, Lr: 0.000300\n",
            "2021-07-14 15:37:56,370 - INFO - joeynmt.training - Epoch  11, Step:   143200, Batch Loss:     1.737241, Tokens per Sec:    15919, Lr: 0.000300\n",
            "2021-07-14 15:38:23,634 - INFO - joeynmt.training - Epoch  11, Step:   143400, Batch Loss:     1.911878, Tokens per Sec:    15975, Lr: 0.000300\n",
            "2021-07-14 15:38:51,375 - INFO - joeynmt.training - Epoch  11, Step:   143600, Batch Loss:     1.843591, Tokens per Sec:    15695, Lr: 0.000300\n",
            "2021-07-14 15:38:58,580 - INFO - joeynmt.training - Epoch  11: total training loss 10030.80\n",
            "2021-07-14 15:38:58,581 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-14 15:39:19,720 - INFO - joeynmt.training - Epoch  12, Step:   143800, Batch Loss:     1.756096, Tokens per Sec:    15496, Lr: 0.000300\n",
            "2021-07-14 15:39:47,444 - INFO - joeynmt.training - Epoch  12, Step:   144000, Batch Loss:     1.885198, Tokens per Sec:    15687, Lr: 0.000300\n",
            "2021-07-14 15:40:15,098 - INFO - joeynmt.training - Epoch  12, Step:   144200, Batch Loss:     1.819593, Tokens per Sec:    15979, Lr: 0.000300\n",
            "2021-07-14 15:40:42,905 - INFO - joeynmt.training - Epoch  12, Step:   144400, Batch Loss:     1.651378, Tokens per Sec:    15985, Lr: 0.000300\n",
            "2021-07-14 15:41:10,794 - INFO - joeynmt.training - Epoch  12, Step:   144600, Batch Loss:     1.769717, Tokens per Sec:    15851, Lr: 0.000300\n",
            "2021-07-14 15:41:38,453 - INFO - joeynmt.training - Epoch  12, Step:   144800, Batch Loss:     1.710191, Tokens per Sec:    15645, Lr: 0.000300\n",
            "2021-07-14 15:42:05,868 - INFO - joeynmt.training - Epoch  12, Step:   145000, Batch Loss:     1.523377, Tokens per Sec:    15866, Lr: 0.000300\n",
            "2021-07-14 15:43:14,806 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 15:43:14,806 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 15:43:14,806 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 15:43:16,453 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 15:43:16,453 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 15:43:16,454 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 15:43:16,454 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-14 15:43:16,454 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 15:43:16,454 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 15:43:16,455 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 15:43:16,455 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
            "2021-07-14 15:43:16,455 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 15:43:16,455 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 15:43:16,455 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:43:16,456 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:43:16,456 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 15:43:16,456 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 15:43:16,456 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 15:43:16,457 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of kindness in Satan’s world .\n",
            "2021-07-14 15:43:16,457 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   145000: bleu:  15.89, loss: 137186.3594, ppl:   8.9575, duration: 70.5883s\n",
            "2021-07-14 15:43:44,574 - INFO - joeynmt.training - Epoch  12, Step:   145200, Batch Loss:     1.893124, Tokens per Sec:    15474, Lr: 0.000300\n",
            "2021-07-14 15:44:12,275 - INFO - joeynmt.training - Epoch  12, Step:   145400, Batch Loss:     1.833079, Tokens per Sec:    15782, Lr: 0.000300\n",
            "2021-07-14 15:44:39,780 - INFO - joeynmt.training - Epoch  12, Step:   145600, Batch Loss:     1.779298, Tokens per Sec:    15691, Lr: 0.000300\n",
            "2021-07-14 15:45:07,496 - INFO - joeynmt.training - Epoch  12, Step:   145800, Batch Loss:     1.982289, Tokens per Sec:    15791, Lr: 0.000300\n",
            "2021-07-14 15:45:35,256 - INFO - joeynmt.training - Epoch  12, Step:   146000, Batch Loss:     2.039928, Tokens per Sec:    15827, Lr: 0.000300\n",
            "2021-07-14 15:46:03,014 - INFO - joeynmt.training - Epoch  12, Step:   146200, Batch Loss:     1.995369, Tokens per Sec:    15734, Lr: 0.000300\n",
            "2021-07-14 15:46:30,818 - INFO - joeynmt.training - Epoch  12, Step:   146400, Batch Loss:     1.920442, Tokens per Sec:    15793, Lr: 0.000300\n",
            "2021-07-14 15:46:58,424 - INFO - joeynmt.training - Epoch  12, Step:   146600, Batch Loss:     1.810457, Tokens per Sec:    15963, Lr: 0.000300\n",
            "2021-07-14 15:47:26,267 - INFO - joeynmt.training - Epoch  12, Step:   146800, Batch Loss:     1.564986, Tokens per Sec:    15813, Lr: 0.000300\n",
            "2021-07-14 15:47:54,100 - INFO - joeynmt.training - Epoch  12, Step:   147000, Batch Loss:     1.845867, Tokens per Sec:    15982, Lr: 0.000300\n",
            "2021-07-14 15:48:21,787 - INFO - joeynmt.training - Epoch  12, Step:   147200, Batch Loss:     1.599587, Tokens per Sec:    15780, Lr: 0.000300\n",
            "2021-07-14 15:48:49,520 - INFO - joeynmt.training - Epoch  12, Step:   147400, Batch Loss:     1.739066, Tokens per Sec:    15812, Lr: 0.000300\n",
            "2021-07-14 15:49:17,127 - INFO - joeynmt.training - Epoch  12, Step:   147600, Batch Loss:     1.717482, Tokens per Sec:    15958, Lr: 0.000300\n",
            "2021-07-14 15:49:44,963 - INFO - joeynmt.training - Epoch  12, Step:   147800, Batch Loss:     1.882466, Tokens per Sec:    15803, Lr: 0.000300\n",
            "2021-07-14 15:50:12,527 - INFO - joeynmt.training - Epoch  12, Step:   148000, Batch Loss:     1.919302, Tokens per Sec:    15803, Lr: 0.000300\n",
            "2021-07-14 15:50:40,259 - INFO - joeynmt.training - Epoch  12, Step:   148200, Batch Loss:     1.965923, Tokens per Sec:    15563, Lr: 0.000300\n",
            "2021-07-14 15:51:08,209 - INFO - joeynmt.training - Epoch  12, Step:   148400, Batch Loss:     1.857025, Tokens per Sec:    15801, Lr: 0.000300\n",
            "2021-07-14 15:51:35,824 - INFO - joeynmt.training - Epoch  12, Step:   148600, Batch Loss:     1.913868, Tokens per Sec:    15836, Lr: 0.000300\n",
            "2021-07-14 15:52:03,888 - INFO - joeynmt.training - Epoch  12, Step:   148800, Batch Loss:     1.769126, Tokens per Sec:    15676, Lr: 0.000300\n",
            "2021-07-14 15:52:31,919 - INFO - joeynmt.training - Epoch  12, Step:   149000, Batch Loss:     1.895511, Tokens per Sec:    15887, Lr: 0.000300\n",
            "2021-07-14 15:52:55,323 - INFO - joeynmt.training - Epoch  12: total training loss 9977.95\n",
            "2021-07-14 15:52:55,324 - INFO - joeynmt.training - EPOCH 13\n",
            "2021-07-14 15:53:00,060 - INFO - joeynmt.training - Epoch  13, Step:   149200, Batch Loss:     1.979504, Tokens per Sec:    13979, Lr: 0.000300\n",
            "2021-07-14 15:53:27,601 - INFO - joeynmt.training - Epoch  13, Step:   149400, Batch Loss:     1.721619, Tokens per Sec:    15692, Lr: 0.000300\n",
            "2021-07-14 15:53:55,411 - INFO - joeynmt.training - Epoch  13, Step:   149600, Batch Loss:     1.981931, Tokens per Sec:    15989, Lr: 0.000300\n",
            "2021-07-14 15:54:23,064 - INFO - joeynmt.training - Epoch  13, Step:   149800, Batch Loss:     1.900479, Tokens per Sec:    15688, Lr: 0.000300\n",
            "2021-07-14 15:54:50,895 - INFO - joeynmt.training - Epoch  13, Step:   150000, Batch Loss:     1.647668, Tokens per Sec:    15871, Lr: 0.000300\n",
            "2021-07-14 15:55:57,716 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 15:55:57,716 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 15:55:57,716 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 15:55:58,312 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 15:55:58,313 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 15:55:59,364 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 15:55:59,365 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 15:55:59,365 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 15:55:59,365 - INFO - joeynmt.training - \tHypothesis: It was a touch .\n",
            "2021-07-14 15:55:59,365 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 15:55:59,366 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 15:55:59,366 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 15:55:59,366 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-14 15:55:59,366 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 15:55:59,367 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 15:55:59,367 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:55:59,367 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 15:55:59,367 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 15:55:59,367 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 15:55:59,368 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 15:55:59,368 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-14 15:55:59,368 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   150000: bleu:  16.05, loss: 134481.7812, ppl:   8.5786, duration: 68.4726s\n",
            "2021-07-14 15:56:26,937 - INFO - joeynmt.training - Epoch  13, Step:   150200, Batch Loss:     1.774668, Tokens per Sec:    15392, Lr: 0.000300\n",
            "2021-07-14 15:56:54,751 - INFO - joeynmt.training - Epoch  13, Step:   150400, Batch Loss:     1.820692, Tokens per Sec:    15694, Lr: 0.000300\n",
            "2021-07-14 15:57:22,684 - INFO - joeynmt.training - Epoch  13, Step:   150600, Batch Loss:     1.729075, Tokens per Sec:    15821, Lr: 0.000300\n",
            "2021-07-14 15:57:50,387 - INFO - joeynmt.training - Epoch  13, Step:   150800, Batch Loss:     1.675082, Tokens per Sec:    15644, Lr: 0.000300\n",
            "2021-07-14 15:58:18,154 - INFO - joeynmt.training - Epoch  13, Step:   151000, Batch Loss:     1.670010, Tokens per Sec:    15887, Lr: 0.000300\n",
            "2021-07-14 15:58:45,964 - INFO - joeynmt.training - Epoch  13, Step:   151200, Batch Loss:     1.846279, Tokens per Sec:    15667, Lr: 0.000300\n",
            "2021-07-14 15:59:13,144 - INFO - joeynmt.training - Epoch  13, Step:   151400, Batch Loss:     1.806633, Tokens per Sec:    15775, Lr: 0.000300\n",
            "2021-07-14 15:59:41,196 - INFO - joeynmt.training - Epoch  13, Step:   151600, Batch Loss:     1.898080, Tokens per Sec:    16068, Lr: 0.000300\n",
            "2021-07-14 16:00:08,765 - INFO - joeynmt.training - Epoch  13, Step:   151800, Batch Loss:     1.812962, Tokens per Sec:    15918, Lr: 0.000300\n",
            "2021-07-14 16:00:36,391 - INFO - joeynmt.training - Epoch  13, Step:   152000, Batch Loss:     1.825086, Tokens per Sec:    15675, Lr: 0.000300\n",
            "2021-07-14 16:01:04,247 - INFO - joeynmt.training - Epoch  13, Step:   152200, Batch Loss:     1.820252, Tokens per Sec:    15917, Lr: 0.000300\n",
            "2021-07-14 16:01:31,830 - INFO - joeynmt.training - Epoch  13, Step:   152400, Batch Loss:     1.932999, Tokens per Sec:    15843, Lr: 0.000300\n",
            "2021-07-14 16:01:59,777 - INFO - joeynmt.training - Epoch  13, Step:   152600, Batch Loss:     1.705767, Tokens per Sec:    15650, Lr: 0.000300\n",
            "2021-07-14 16:02:27,441 - INFO - joeynmt.training - Epoch  13, Step:   152800, Batch Loss:     1.885243, Tokens per Sec:    15602, Lr: 0.000300\n",
            "2021-07-14 16:02:55,452 - INFO - joeynmt.training - Epoch  13, Step:   153000, Batch Loss:     1.874204, Tokens per Sec:    15985, Lr: 0.000300\n",
            "2021-07-14 16:03:23,033 - INFO - joeynmt.training - Epoch  13, Step:   153200, Batch Loss:     1.668691, Tokens per Sec:    15654, Lr: 0.000300\n",
            "2021-07-14 16:03:50,880 - INFO - joeynmt.training - Epoch  13, Step:   153400, Batch Loss:     1.824723, Tokens per Sec:    16055, Lr: 0.000300\n",
            "2021-07-14 16:04:18,450 - INFO - joeynmt.training - Epoch  13, Step:   153600, Batch Loss:     1.847584, Tokens per Sec:    15758, Lr: 0.000300\n",
            "2021-07-14 16:04:46,037 - INFO - joeynmt.training - Epoch  13, Step:   153800, Batch Loss:     1.775514, Tokens per Sec:    15684, Lr: 0.000300\n",
            "2021-07-14 16:05:13,656 - INFO - joeynmt.training - Epoch  13, Step:   154000, Batch Loss:     1.619398, Tokens per Sec:    15988, Lr: 0.000300\n",
            "2021-07-14 16:05:41,479 - INFO - joeynmt.training - Epoch  13, Step:   154200, Batch Loss:     1.910235, Tokens per Sec:    15752, Lr: 0.000300\n",
            "2021-07-14 16:06:08,816 - INFO - joeynmt.training - Epoch  13, Step:   154400, Batch Loss:     1.967418, Tokens per Sec:    15786, Lr: 0.000300\n",
            "2021-07-14 16:06:36,643 - INFO - joeynmt.training - Epoch  13, Step:   154600, Batch Loss:     1.744444, Tokens per Sec:    15897, Lr: 0.000300\n",
            "2021-07-14 16:06:50,881 - INFO - joeynmt.training - Epoch  13: total training loss 9961.10\n",
            "2021-07-14 16:06:50,881 - INFO - joeynmt.training - EPOCH 14\n",
            "2021-07-14 16:07:04,849 - INFO - joeynmt.training - Epoch  14, Step:   154800, Batch Loss:     1.809167, Tokens per Sec:    15205, Lr: 0.000300\n",
            "2021-07-14 16:07:32,380 - INFO - joeynmt.training - Epoch  14, Step:   155000, Batch Loss:     1.920160, Tokens per Sec:    15818, Lr: 0.000300\n",
            "2021-07-14 16:08:44,827 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 16:08:44,828 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 16:08:44,828 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 16:08:45,446 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 16:08:45,446 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 16:08:46,458 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 16:08:46,459 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 16:08:46,459 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 16:08:46,459 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-14 16:08:46,459 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 16:08:46,460 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 16:08:46,460 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 16:08:46,460 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-14 16:08:46,460 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 16:08:46,461 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 16:08:46,461 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 16:08:46,461 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 16:08:46,461 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 16:08:46,462 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 16:08:46,462 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 16:08:46,462 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being successful in Satan’s world .\n",
            "2021-07-14 16:08:46,462 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   155000: bleu:  16.32, loss: 133365.0312, ppl:   8.4268, duration: 74.0814s\n",
            "2021-07-14 16:09:14,319 - INFO - joeynmt.training - Epoch  14, Step:   155200, Batch Loss:     1.918899, Tokens per Sec:    15534, Lr: 0.000300\n",
            "2021-07-14 16:09:42,132 - INFO - joeynmt.training - Epoch  14, Step:   155400, Batch Loss:     1.681324, Tokens per Sec:    15720, Lr: 0.000300\n",
            "2021-07-14 16:10:09,750 - INFO - joeynmt.training - Epoch  14, Step:   155600, Batch Loss:     1.792017, Tokens per Sec:    15956, Lr: 0.000300\n",
            "2021-07-14 16:10:37,604 - INFO - joeynmt.training - Epoch  14, Step:   155800, Batch Loss:     1.795056, Tokens per Sec:    15821, Lr: 0.000300\n",
            "2021-07-14 16:11:05,230 - INFO - joeynmt.training - Epoch  14, Step:   156000, Batch Loss:     1.723713, Tokens per Sec:    16059, Lr: 0.000300\n",
            "2021-07-14 16:11:33,099 - INFO - joeynmt.training - Epoch  14, Step:   156200, Batch Loss:     1.696563, Tokens per Sec:    15754, Lr: 0.000300\n",
            "2021-07-14 16:12:00,713 - INFO - joeynmt.training - Epoch  14, Step:   156400, Batch Loss:     1.751287, Tokens per Sec:    15788, Lr: 0.000300\n",
            "2021-07-14 16:12:28,510 - INFO - joeynmt.training - Epoch  14, Step:   156600, Batch Loss:     1.876272, Tokens per Sec:    15771, Lr: 0.000300\n",
            "2021-07-14 16:12:56,475 - INFO - joeynmt.training - Epoch  14, Step:   156800, Batch Loss:     1.826929, Tokens per Sec:    15667, Lr: 0.000300\n",
            "2021-07-14 16:13:24,159 - INFO - joeynmt.training - Epoch  14, Step:   157000, Batch Loss:     1.667950, Tokens per Sec:    15941, Lr: 0.000300\n",
            "2021-07-14 16:13:52,026 - INFO - joeynmt.training - Epoch  14, Step:   157200, Batch Loss:     1.899099, Tokens per Sec:    15715, Lr: 0.000300\n",
            "2021-07-14 16:14:19,776 - INFO - joeynmt.training - Epoch  14, Step:   157400, Batch Loss:     1.830047, Tokens per Sec:    15659, Lr: 0.000300\n",
            "2021-07-14 16:14:47,580 - INFO - joeynmt.training - Epoch  14, Step:   157600, Batch Loss:     1.642778, Tokens per Sec:    15809, Lr: 0.000300\n",
            "2021-07-14 16:15:15,223 - INFO - joeynmt.training - Epoch  14, Step:   157800, Batch Loss:     1.508110, Tokens per Sec:    15680, Lr: 0.000300\n",
            "2021-07-14 16:15:42,998 - INFO - joeynmt.training - Epoch  14, Step:   158000, Batch Loss:     1.768084, Tokens per Sec:    15753, Lr: 0.000300\n",
            "2021-07-14 16:16:10,587 - INFO - joeynmt.training - Epoch  14, Step:   158200, Batch Loss:     1.774783, Tokens per Sec:    15762, Lr: 0.000300\n",
            "2021-07-14 16:16:38,374 - INFO - joeynmt.training - Epoch  14, Step:   158400, Batch Loss:     1.766156, Tokens per Sec:    15543, Lr: 0.000300\n",
            "2021-07-14 16:17:06,028 - INFO - joeynmt.training - Epoch  14, Step:   158600, Batch Loss:     1.725033, Tokens per Sec:    16018, Lr: 0.000300\n",
            "2021-07-14 16:17:33,641 - INFO - joeynmt.training - Epoch  14, Step:   158800, Batch Loss:     1.798880, Tokens per Sec:    15757, Lr: 0.000300\n",
            "2021-07-14 16:18:01,604 - INFO - joeynmt.training - Epoch  14, Step:   159000, Batch Loss:     2.161124, Tokens per Sec:    15933, Lr: 0.000300\n",
            "2021-07-14 16:18:29,495 - INFO - joeynmt.training - Epoch  14, Step:   159200, Batch Loss:     2.114803, Tokens per Sec:    15810, Lr: 0.000300\n",
            "2021-07-14 16:18:57,323 - INFO - joeynmt.training - Epoch  14, Step:   159400, Batch Loss:     1.847784, Tokens per Sec:    15533, Lr: 0.000300\n",
            "2021-07-14 16:19:25,085 - INFO - joeynmt.training - Epoch  14, Step:   159600, Batch Loss:     2.169771, Tokens per Sec:    15695, Lr: 0.000300\n",
            "2021-07-14 16:19:52,966 - INFO - joeynmt.training - Epoch  14, Step:   159800, Batch Loss:     1.834271, Tokens per Sec:    15966, Lr: 0.000300\n",
            "2021-07-14 16:20:21,077 - INFO - joeynmt.training - Epoch  14, Step:   160000, Batch Loss:     2.183383, Tokens per Sec:    15878, Lr: 0.000300\n",
            "2021-07-14 16:21:36,544 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 16:21:36,545 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 16:21:36,545 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 16:21:37,233 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-14 16:21:37,234 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-14 16:21:37,964 - INFO - joeynmt.training - Example #0\n",
            "2021-07-14 16:21:37,965 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-14 16:21:37,965 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-14 16:21:37,965 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-14 16:21:37,965 - INFO - joeynmt.training - Example #1\n",
            "2021-07-14 16:21:37,966 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-14 16:21:37,966 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-14 16:21:37,966 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-14 16:21:37,966 - INFO - joeynmt.training - Example #2\n",
            "2021-07-14 16:21:37,967 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-14 16:21:37,967 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 16:21:37,967 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-14 16:21:37,967 - INFO - joeynmt.training - Example #3\n",
            "2021-07-14 16:21:37,968 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-14 16:21:37,968 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-14 16:21:37,968 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-14 16:21:37,968 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   160000: bleu:  15.88, loss: 132802.8594, ppl:   8.3514, duration: 76.8910s\n",
            "2021-07-14 16:22:05,564 - INFO - joeynmt.training - Epoch  14, Step:   160200, Batch Loss:     1.767147, Tokens per Sec:    15553, Lr: 0.000300\n",
            "2021-07-14 16:22:09,559 - INFO - joeynmt.training - Epoch  14: total training loss 9924.75\n",
            "2021-07-14 16:22:09,559 - INFO - joeynmt.training - Training ended after  14 epochs.\n",
            "2021-07-14 16:22:09,559 - INFO - joeynmt.training - Best validation result (greedy) at step   160000:   8.35 ppl.\n",
            "2021-07-14 16:22:09,580 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-14 16:22:09,936 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 16:22:10,127 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-14 16:22:10,190 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe.en)...\n",
            "2021-07-14 16:23:32,051 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 16:23:32,051 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 16:23:32,051 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 16:23:32,718 - INFO - joeynmt.prediction -  dev bleu[13a]:  17.28 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-14 16:23:32,723 - INFO - joeynmt.prediction - Translations saved to: models/rw_lhen_reverse_transformer_continued/00160000.hyps.dev\n",
            "2021-07-14 16:23:32,724 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.en)...\n",
            "2021-07-14 16:24:22,202 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 16:24:22,203 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 16:24:22,203 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 16:24:22,522 - INFO - joeynmt.prediction - test bleu[13a]:   9.14 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-14 16:24:22,527 - INFO - joeynmt.prediction - Translations saved to: models/rw_lhen_reverse_transformer_continued/00160000.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H61HNxAlg_7y"
      },
      "source": [
        "14 epochs complete\n",
        "\n",
        "16+14 = 30 epochs complete"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlJ8i_1H3afn",
        "outputId": "ed0d6523-b364-489f-cacd-7fb4e30c7e6c"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"joeynmt/models/rw_lhen_reverse_transformer/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 5000\tLoss: 209669.92188\tPPL: 28.52874\tbleu: 3.84511\tLR: 0.00030000\t*\n",
            "Steps: 10000\tLoss: 186578.73438\tPPL: 19.72471\tbleu: 6.93421\tLR: 0.00030000\t*\n",
            "Steps: 15000\tLoss: 176237.12500\tPPL: 16.71981\tbleu: 8.03345\tLR: 0.00030000\t*\n",
            "Steps: 20000\tLoss: 167712.25000\tPPL: 14.59021\tbleu: 9.32608\tLR: 0.00030000\t*\n",
            "Steps: 25000\tLoss: 162152.32812\tPPL: 13.34969\tbleu: 9.93043\tLR: 0.00030000\t*\n",
            "Steps: 30000\tLoss: 157637.42188\tPPL: 12.42036\tbleu: 11.15394\tLR: 0.00030000\t*\n",
            "Steps: 35000\tLoss: 155305.07812\tPPL: 11.96591\tbleu: 11.75460\tLR: 0.00030000\t*\n",
            "Steps: 40000\tLoss: 152631.00000\tPPL: 11.46530\tbleu: 11.44089\tLR: 0.00030000\t*\n",
            "Steps: 45000\tLoss: 150888.71875\tPPL: 11.15045\tbleu: 12.76499\tLR: 0.00030000\t*\n",
            "Steps: 50000\tLoss: 149148.78125\tPPL: 10.84466\tbleu: 12.32629\tLR: 0.00030000\t*\n",
            "Steps: 55000\tLoss: 147339.18750\tPPL: 10.53552\tbleu: 13.28245\tLR: 0.00030000\t*\n",
            "Steps: 60000\tLoss: 145776.71875\tPPL: 10.27569\tbleu: 12.85105\tLR: 0.00030000\t*\n",
            "Steps: 65000\tLoss: 144349.95312\tPPL: 10.04403\tbleu: 13.63222\tLR: 0.00030000\t*\n",
            "Steps: 70000\tLoss: 143590.75000\tPPL: 9.92290\tbleu: 13.91883\tLR: 0.00030000\t*\n",
            "Steps: 75000\tLoss: 143132.43750\tPPL: 9.85048\tbleu: 13.93819\tLR: 0.00030000\t*\n",
            "Steps: 80000\tLoss: 142473.85938\tPPL: 9.74735\tbleu: 13.81159\tLR: 0.00030000\t*\n",
            "Steps: 85000\tLoss: 140273.01562\tPPL: 9.41046\tbleu: 14.32636\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWOIl5BILmt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c40952-bda7-46fc-922e-8e544985d248"
      },
      "source": [
        "! cat \"joeynmt/models/rw_lhen_reverse_transformer_continued/validations.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 90000\tLoss: 139744.14062\tPPL: 9.33125\tbleu: 14.27456\tLR: 0.00030000\t*\n",
            "Steps: 95000\tLoss: 139600.45312\tPPL: 9.30985\tbleu: 14.60423\tLR: 0.00030000\t*\n",
            "Steps: 100000\tLoss: 138584.57812\tPPL: 9.15992\tbleu: 14.38571\tLR: 0.00030000\t*\n",
            "Steps: 105000\tLoss: 139485.21875\tPPL: 9.29272\tbleu: 15.10734\tLR: 0.00030000\t\n",
            "Steps: 110000\tLoss: 137181.06250\tPPL: 8.95674\tbleu: 15.45227\tLR: 0.00030000\t*\n",
            "Steps: 115000\tLoss: 138259.09375\tPPL: 9.11239\tbleu: 15.49028\tLR: 0.00030000\t\n",
            "Steps: 120000\tLoss: 136506.10938\tPPL: 8.86064\tbleu: 15.66517\tLR: 0.00030000\t*\n",
            "Steps: 125000\tLoss: 136562.43750\tPPL: 8.86862\tbleu: 15.48623\tLR: 0.00030000\t\n",
            "Steps: 130000\tLoss: 136139.18750\tPPL: 8.80884\tbleu: 15.73478\tLR: 0.00030000\t*\n",
            "Steps: 135000\tLoss: 135421.07812\tPPL: 8.70832\tbleu: 15.83303\tLR: 0.00030000\t*\n",
            "Steps: 140000\tLoss: 135106.35938\tPPL: 8.66463\tbleu: 16.09501\tLR: 0.00030000\t*\n",
            "Steps: 145000\tLoss: 137186.35938\tPPL: 8.95750\tbleu: 15.89274\tLR: 0.00030000\t\n",
            "Steps: 150000\tLoss: 134481.78125\tPPL: 8.57857\tbleu: 16.04871\tLR: 0.00030000\t*\n",
            "Steps: 155000\tLoss: 133365.03125\tPPL: 8.42682\tbleu: 16.32297\tLR: 0.00030000\t*\n",
            "Steps: 160000\tLoss: 132802.85938\tPPL: 8.35145\tbleu: 15.87776\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z5r5_ZceaEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bfe80a-a956-4199-a3fd-a6e352fef185"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt test 'models/rw_lhen_reverse_transformer_continued/config.yaml'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 16:37:51,568 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 16:37:51,573 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-14 16:37:51,850 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-14 16:37:51,878 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-14 16:37:51,894 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-14 16:37:51,925 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-14 16:37:55,578 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 16:37:55,773 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-14 16:37:55,842 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe.en)...\n",
            "2021-07-14 16:39:14,072 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 16:39:14,073 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 16:39:14,073 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 16:39:14,686 - INFO - joeynmt.prediction -  dev bleu[13a]:  17.28 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-14 16:39:14,687 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.en)...\n",
            "2021-07-14 16:40:03,874 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-14 16:40:03,875 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-14 16:40:03,875 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-14 16:40:04,202 - INFO - joeynmt.prediction - test bleu[13a]:   9.14 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhC8exsX0JDl",
        "outputId": "adfd6eca-41dc-462c-9b0b-e00873aee257"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/rw_lhen_reverse_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/translation.bpe.lh_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 16:40:20,238 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 16:40:24,207 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 16:40:24,403 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMPvti0N1T85",
        "outputId": "4f467c54-647f-46ad-ce2d-a6c4918724da"
      },
      "source": [
        "!cat \"translation.bpe.lh_en\" | sacrebleu \"test1.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 9.1 38.4/13.1/5.3/2.6 (BP = 1.000 ratio = 1.018 hyp_len = 26521 ref_len = 26044)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeJt3YhH2MJL",
        "outputId": "f0779bb2-0c3a-4d68-8a8d-08f57c53935c"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/rw_lhen_reverse_transformer_continued/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.rw\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/translation.bpe.rw_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-14 16:41:26,994 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-14 16:41:30,972 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-14 16:41:31,169 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YzpV_qq3z4_",
        "cellView": "code"
      },
      "source": [
        "#@title\n",
        "def empty_counter(x):\n",
        "  # Opening a file\n",
        "  infile = open(x,\"r\")\n",
        "  empty = []\n",
        "  \n",
        "  for i,line in enumerate(infile):\n",
        "    if not line.strip(): \n",
        "      empty.append(i)\n",
        "\n",
        "  return empty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHr-LHtQ36wG",
        "cellView": "code"
      },
      "source": [
        "#@title\n",
        "# Reference: https://thispointer.com/python-how-to-delete-specific-lines-in-a-file-in-a-memory-efficient-way/\n",
        "def delete_multiple_lines(original_file, line_numbers):\n",
        "    \"\"\"In a file, delete the lines at line number in given list\"\"\"\n",
        "    is_skipped = False\n",
        "    counter = 0\n",
        "    # Create name of dummy / temporary file\n",
        "    dummy_file = original_file + '.bak'\n",
        "    # Open original file in read only mode and dummy file in write mode\n",
        "    with open(original_file, 'r') as read_obj, open(dummy_file, 'w') as write_obj:\n",
        "        # Line by line copy data from original file to dummy file\n",
        "        for line in read_obj:\n",
        "            # If current line number exist in list then skip copying that line\n",
        "            if counter not in line_numbers:\n",
        "                write_obj.write(line)\n",
        "            else:\n",
        "                is_skipped = True\n",
        "            counter += 1\n",
        "    # If any line is skipped then rename dummy file as original file\n",
        "    if is_skipped:\n",
        "        os.remove(original_file)\n",
        "        os.rename(dummy_file, original_file)\n",
        "    else:\n",
        "        os.remove(dummy_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M8A5vHr4dk2"
      },
      "source": [
        "delete_multiple_lines(\"test2.en\",empty_counter(\"test2.rw\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4O429tX3FCA",
        "outputId": "128fc525-52e3-4bdb-aa35-776f79997863"
      },
      "source": [
        "!cat \"translation.bpe.rw_en\" | sacrebleu \"test2.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 37.7 67.9/46.2/35.2/27.9 (BP = 0.901 ratio = 0.906 hyp_len = 38435 ref_len = 42439)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tVen4gHg1du"
      },
      "source": [
        "Getting to 60 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR423oOkggaV"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 160000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued2\"')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload2.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn9m-mn3hvyT",
        "outputId": "77c98f4a-d3e8-4d4a-922f-e00612da899f"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload2.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-22 09:24:10,389 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-22 09:24:10,460 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-22 09:24:22,130 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-22 09:24:22,763 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-22 09:24:23,383 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-22 09:24:24,078 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-22 09:24:24,078 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-22 09:24:24,485 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-22 09:24:24.741675: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-22 09:24:26,889 - INFO - joeynmt.training - Total params: 12177920\n",
            "2021-07-22 09:24:30,401 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued/160000.ckpt\n",
            "2021-07-22 09:24:30,956 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
            "2021-07-22 09:24:30,957 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
            "2021-07-22 09:24:30,957 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-22 09:24:30,957 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
            "2021-07-22 09:24:30,957 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
            "2021-07-22 09:24:30,958 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
            "2021-07-22 09:24:30,958 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-22 09:24:30,958 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-22 09:24:30,958 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-22 09:24:30,958 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-22 09:24:30,959 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-22 09:24:30,959 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-22 09:24:30,959 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-22 09:24:30,959 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued/160000.ckpt\n",
            "2021-07-22 09:24:30,960 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-22 09:24:30,960 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-22 09:24:30,960 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-22 09:24:30,960 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-22 09:24:30,961 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-22 09:24:30,961 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-22 09:24:30,961 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-22 09:24:30,961 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-22 09:24:30,962 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-22 09:24:30,962 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-22 09:24:30,962 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-22 09:24:30,962 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-22 09:24:30,963 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-22 09:24:30,963 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-22 09:24:30,963 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-22 09:24:30,963 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-22 09:24:30,963 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-22 09:24:30,964 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-22 09:24:30,964 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-22 09:24:30,964 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-22 09:24:30,964 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-07-22 09:24:30,969 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-22 09:24:30,970 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-22 09:24:30,970 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-22 09:24:30,971 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued2\n",
            "2021-07-22 09:24:30,971 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-22 09:24:30,971 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-22 09:24:30,971 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-22 09:24:30,972 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-22 09:24:30,972 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-22 09:24:30,972 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-22 09:24:30,972 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-22 09:24:30,973 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-22 09:24:30,973 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-22 09:24:30,973 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-22 09:24:30,973 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-22 09:24:30,974 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-22 09:24:30,974 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-22 09:24:30,975 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-22 09:24:30,975 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-22 09:24:30,975 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-22 09:24:30,979 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-22 09:24:30,979 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-22 09:24:30,979 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-22 09:24:30,980 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-22 09:24:30,981 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-22 09:24:30,981 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-22 09:24:30,981 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-22 09:24:30,981 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-22 09:24:30,982 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-22 09:24:30,982 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-22 09:24:30,982 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-22 09:24:30,982 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-22 09:24:30,983 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-22 09:24:30,983 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-22 09:24:30,983 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-22 09:24:30,983 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 439996,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-22 09:24:30,983 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
            "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
            "2021-07-22 09:24:30,984 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-22 09:24:30,984 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-22 09:24:30,984 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
            "2021-07-22 09:24:30,984 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
            "2021-07-22 09:24:30,985 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
            "2021-07-22 09:24:30,998 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-22 09:24:30,998 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-22 09:25:32,765 - INFO - joeynmt.training - Epoch   1, Step:   160200, Batch Loss:     1.776191, Tokens per Sec:     6949, Lr: 0.000300\n",
            "2021-07-22 09:25:41,339 - INFO - joeynmt.training - Epoch   1: total training loss 412.97\n",
            "2021-07-22 09:25:41,339 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-22 09:26:33,490 - INFO - joeynmt.training - Epoch   2, Step:   160400, Batch Loss:     1.888664, Tokens per Sec:     7321, Lr: 0.000300\n",
            "2021-07-22 09:27:32,529 - INFO - joeynmt.training - Epoch   2, Step:   160600, Batch Loss:     1.734130, Tokens per Sec:     7335, Lr: 0.000300\n",
            "2021-07-22 09:28:31,696 - INFO - joeynmt.training - Epoch   2, Step:   160800, Batch Loss:     1.958624, Tokens per Sec:     7303, Lr: 0.000300\n",
            "2021-07-22 09:29:30,886 - INFO - joeynmt.training - Epoch   2, Step:   161000, Batch Loss:     1.797732, Tokens per Sec:     7326, Lr: 0.000300\n",
            "2021-07-22 09:30:30,234 - INFO - joeynmt.training - Epoch   2, Step:   161200, Batch Loss:     1.765962, Tokens per Sec:     7492, Lr: 0.000300\n",
            "2021-07-22 09:31:29,468 - INFO - joeynmt.training - Epoch   2, Step:   161400, Batch Loss:     1.896082, Tokens per Sec:     7335, Lr: 0.000300\n",
            "2021-07-22 09:32:28,669 - INFO - joeynmt.training - Epoch   2, Step:   161600, Batch Loss:     1.702552, Tokens per Sec:     7302, Lr: 0.000300\n",
            "2021-07-22 09:33:28,089 - INFO - joeynmt.training - Epoch   2, Step:   161800, Batch Loss:     1.861781, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-22 09:34:27,310 - INFO - joeynmt.training - Epoch   2, Step:   162000, Batch Loss:     1.599508, Tokens per Sec:     7434, Lr: 0.000300\n",
            "2021-07-22 09:35:26,711 - INFO - joeynmt.training - Epoch   2, Step:   162200, Batch Loss:     1.879327, Tokens per Sec:     7325, Lr: 0.000300\n",
            "2021-07-22 09:36:26,385 - INFO - joeynmt.training - Epoch   2, Step:   162400, Batch Loss:     1.676798, Tokens per Sec:     7439, Lr: 0.000300\n",
            "2021-07-22 09:37:26,249 - INFO - joeynmt.training - Epoch   2, Step:   162600, Batch Loss:     1.677034, Tokens per Sec:     7454, Lr: 0.000300\n",
            "2021-07-22 09:38:25,334 - INFO - joeynmt.training - Epoch   2, Step:   162800, Batch Loss:     1.801146, Tokens per Sec:     7329, Lr: 0.000300\n",
            "2021-07-22 09:39:24,456 - INFO - joeynmt.training - Epoch   2, Step:   163000, Batch Loss:     1.890365, Tokens per Sec:     7309, Lr: 0.000300\n",
            "2021-07-22 09:40:23,953 - INFO - joeynmt.training - Epoch   2, Step:   163200, Batch Loss:     1.709334, Tokens per Sec:     7393, Lr: 0.000300\n",
            "2021-07-22 09:41:23,137 - INFO - joeynmt.training - Epoch   2, Step:   163400, Batch Loss:     1.709445, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-22 09:42:22,716 - INFO - joeynmt.training - Epoch   2, Step:   163600, Batch Loss:     1.826465, Tokens per Sec:     7366, Lr: 0.000300\n",
            "2021-07-22 09:43:22,395 - INFO - joeynmt.training - Epoch   2, Step:   163800, Batch Loss:     1.707637, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-22 09:44:21,689 - INFO - joeynmt.training - Epoch   2, Step:   164000, Batch Loss:     1.913572, Tokens per Sec:     7360, Lr: 0.000300\n",
            "2021-07-22 09:45:21,302 - INFO - joeynmt.training - Epoch   2, Step:   164200, Batch Loss:     1.890124, Tokens per Sec:     7420, Lr: 0.000300\n",
            "2021-07-22 09:46:21,012 - INFO - joeynmt.training - Epoch   2, Step:   164400, Batch Loss:     1.520819, Tokens per Sec:     7318, Lr: 0.000300\n",
            "2021-07-22 09:47:20,762 - INFO - joeynmt.training - Epoch   2, Step:   164600, Batch Loss:     1.765571, Tokens per Sec:     7453, Lr: 0.000300\n",
            "2021-07-22 09:48:20,008 - INFO - joeynmt.training - Epoch   2, Step:   164800, Batch Loss:     1.690794, Tokens per Sec:     7319, Lr: 0.000300\n",
            "2021-07-22 09:49:19,715 - INFO - joeynmt.training - Epoch   2, Step:   165000, Batch Loss:     1.841024, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-22 09:51:25,547 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 09:51:25,548 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 09:51:25,548 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 09:51:26,387 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 09:51:26,388 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 09:51:27,241 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 09:51:27,242 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-22 09:51:27,243 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-22 09:51:27,243 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-22 09:51:27,243 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 09:51:27,244 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-22 09:51:27,244 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-22 09:51:27,244 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-22 09:51:27,244 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 09:51:27,245 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-22 09:51:27,245 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 09:51:27,245 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 09:51:27,246 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 09:51:27,247 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-22 09:51:27,247 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-22 09:51:27,247 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-22 09:51:27,248 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   165000: bleu:  16.65, loss: 132034.4062, ppl:   8.2495, duration: 127.5323s\n",
            "2021-07-22 09:52:26,826 - INFO - joeynmt.training - Epoch   2, Step:   165200, Batch Loss:     1.648818, Tokens per Sec:     7338, Lr: 0.000300\n",
            "2021-07-22 09:53:25,803 - INFO - joeynmt.training - Epoch   2, Step:   165400, Batch Loss:     1.748063, Tokens per Sec:     7317, Lr: 0.000300\n",
            "2021-07-22 09:54:25,008 - INFO - joeynmt.training - Epoch   2, Step:   165600, Batch Loss:     1.885248, Tokens per Sec:     7422, Lr: 0.000300\n",
            "2021-07-22 09:55:12,682 - INFO - joeynmt.training - Epoch   2: total training loss 9901.65\n",
            "2021-07-22 09:55:12,683 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-22 09:55:24,596 - INFO - joeynmt.training - Epoch   3, Step:   165800, Batch Loss:     1.750658, Tokens per Sec:     6675, Lr: 0.000300\n",
            "2021-07-22 09:56:24,006 - INFO - joeynmt.training - Epoch   3, Step:   166000, Batch Loss:     1.716623, Tokens per Sec:     7395, Lr: 0.000300\n",
            "2021-07-22 09:57:23,134 - INFO - joeynmt.training - Epoch   3, Step:   166200, Batch Loss:     1.592424, Tokens per Sec:     7344, Lr: 0.000300\n",
            "2021-07-22 09:58:21,630 - INFO - joeynmt.training - Epoch   3, Step:   166400, Batch Loss:     1.853142, Tokens per Sec:     7258, Lr: 0.000300\n",
            "2021-07-22 09:59:21,647 - INFO - joeynmt.training - Epoch   3, Step:   166600, Batch Loss:     1.737264, Tokens per Sec:     7395, Lr: 0.000300\n",
            "2021-07-22 10:00:21,524 - INFO - joeynmt.training - Epoch   3, Step:   166800, Batch Loss:     1.831405, Tokens per Sec:     7462, Lr: 0.000300\n",
            "2021-07-22 10:01:21,146 - INFO - joeynmt.training - Epoch   3, Step:   167000, Batch Loss:     1.742397, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-22 10:02:20,166 - INFO - joeynmt.training - Epoch   3, Step:   167200, Batch Loss:     1.695015, Tokens per Sec:     7271, Lr: 0.000300\n",
            "2021-07-22 10:03:19,736 - INFO - joeynmt.training - Epoch   3, Step:   167400, Batch Loss:     1.579920, Tokens per Sec:     7360, Lr: 0.000300\n",
            "2021-07-22 10:04:19,311 - INFO - joeynmt.training - Epoch   3, Step:   167600, Batch Loss:     1.563631, Tokens per Sec:     7361, Lr: 0.000300\n",
            "2021-07-22 10:05:18,703 - INFO - joeynmt.training - Epoch   3, Step:   167800, Batch Loss:     1.539166, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-22 10:06:18,144 - INFO - joeynmt.training - Epoch   3, Step:   168000, Batch Loss:     1.831899, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-22 10:07:17,515 - INFO - joeynmt.training - Epoch   3, Step:   168200, Batch Loss:     1.854642, Tokens per Sec:     7417, Lr: 0.000300\n",
            "2021-07-22 10:08:16,832 - INFO - joeynmt.training - Epoch   3, Step:   168400, Batch Loss:     1.921383, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-22 10:09:16,399 - INFO - joeynmt.training - Epoch   3, Step:   168600, Batch Loss:     1.840027, Tokens per Sec:     7402, Lr: 0.000300\n",
            "2021-07-22 10:10:15,920 - INFO - joeynmt.training - Epoch   3, Step:   168800, Batch Loss:     1.744999, Tokens per Sec:     7346, Lr: 0.000300\n",
            "2021-07-22 10:11:14,998 - INFO - joeynmt.training - Epoch   3, Step:   169000, Batch Loss:     1.688184, Tokens per Sec:     7292, Lr: 0.000300\n",
            "2021-07-22 10:12:14,457 - INFO - joeynmt.training - Epoch   3, Step:   169200, Batch Loss:     1.908719, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-22 10:13:13,787 - INFO - joeynmt.training - Epoch   3, Step:   169400, Batch Loss:     1.651976, Tokens per Sec:     7377, Lr: 0.000300\n",
            "2021-07-22 10:14:12,699 - INFO - joeynmt.training - Epoch   3, Step:   169600, Batch Loss:     1.709544, Tokens per Sec:     7327, Lr: 0.000300\n",
            "2021-07-22 10:15:12,333 - INFO - joeynmt.training - Epoch   3, Step:   169800, Batch Loss:     1.757320, Tokens per Sec:     7395, Lr: 0.000300\n",
            "2021-07-22 10:16:11,599 - INFO - joeynmt.training - Epoch   3, Step:   170000, Batch Loss:     2.092732, Tokens per Sec:     7372, Lr: 0.000300\n",
            "2021-07-22 10:18:12,752 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 10:18:12,753 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 10:18:12,753 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 10:18:14,486 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 10:18:14,487 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-22 10:18:14,487 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-22 10:18:14,488 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-22 10:18:14,488 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 10:18:14,488 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-22 10:18:14,489 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-22 10:18:14,489 - INFO - joeynmt.training - \tHypothesis: The text that was written in the pillar on the side of the scroll .\n",
            "2021-07-22 10:18:14,489 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 10:18:14,490 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-22 10:18:14,490 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 10:18:14,490 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 10:18:14,491 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 10:18:14,491 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-22 10:18:14,492 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-22 10:18:14,492 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-22 10:18:14,492 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   170000: bleu:  16.46, loss: 132637.4531, ppl:   8.3294, duration: 122.8930s\n",
            "2021-07-22 10:19:13,953 - INFO - joeynmt.training - Epoch   3, Step:   170200, Batch Loss:     1.806728, Tokens per Sec:     7326, Lr: 0.000300\n",
            "2021-07-22 10:20:13,446 - INFO - joeynmt.training - Epoch   3, Step:   170400, Batch Loss:     1.853133, Tokens per Sec:     7406, Lr: 0.000300\n",
            "2021-07-22 10:21:13,383 - INFO - joeynmt.training - Epoch   3, Step:   170600, Batch Loss:     1.546295, Tokens per Sec:     7456, Lr: 0.000300\n",
            "2021-07-22 10:22:12,820 - INFO - joeynmt.training - Epoch   3, Step:   170800, Batch Loss:     1.830328, Tokens per Sec:     7401, Lr: 0.000300\n",
            "2021-07-22 10:23:12,630 - INFO - joeynmt.training - Epoch   3, Step:   171000, Batch Loss:     2.062936, Tokens per Sec:     7381, Lr: 0.000300\n",
            "2021-07-22 10:24:11,799 - INFO - joeynmt.training - Epoch   3, Step:   171200, Batch Loss:     1.746275, Tokens per Sec:     7399, Lr: 0.000300\n",
            "2021-07-22 10:24:38,936 - INFO - joeynmt.training - Epoch   3: total training loss 9860.70\n",
            "2021-07-22 10:24:38,936 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-22 10:25:11,377 - INFO - joeynmt.training - Epoch   4, Step:   171400, Batch Loss:     1.515128, Tokens per Sec:     7056, Lr: 0.000300\n",
            "2021-07-22 10:26:11,078 - INFO - joeynmt.training - Epoch   4, Step:   171600, Batch Loss:     1.768023, Tokens per Sec:     7420, Lr: 0.000300\n",
            "2021-07-22 10:27:10,434 - INFO - joeynmt.training - Epoch   4, Step:   171800, Batch Loss:     1.649189, Tokens per Sec:     7416, Lr: 0.000300\n",
            "2021-07-22 10:28:09,429 - INFO - joeynmt.training - Epoch   4, Step:   172000, Batch Loss:     1.812363, Tokens per Sec:     7354, Lr: 0.000300\n",
            "2021-07-22 10:29:08,710 - INFO - joeynmt.training - Epoch   4, Step:   172200, Batch Loss:     1.847506, Tokens per Sec:     7307, Lr: 0.000300\n",
            "2021-07-22 10:30:08,738 - INFO - joeynmt.training - Epoch   4, Step:   172400, Batch Loss:     1.766295, Tokens per Sec:     7502, Lr: 0.000300\n",
            "2021-07-22 10:31:07,806 - INFO - joeynmt.training - Epoch   4, Step:   172600, Batch Loss:     1.926241, Tokens per Sec:     7410, Lr: 0.000300\n",
            "2021-07-22 10:32:07,519 - INFO - joeynmt.training - Epoch   4, Step:   172800, Batch Loss:     1.689598, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-22 10:33:06,864 - INFO - joeynmt.training - Epoch   4, Step:   173000, Batch Loss:     1.967901, Tokens per Sec:     7363, Lr: 0.000300\n",
            "2021-07-22 10:34:06,260 - INFO - joeynmt.training - Epoch   4, Step:   173200, Batch Loss:     1.697723, Tokens per Sec:     7385, Lr: 0.000300\n",
            "2021-07-22 10:35:05,757 - INFO - joeynmt.training - Epoch   4, Step:   173400, Batch Loss:     1.560008, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-22 10:36:05,390 - INFO - joeynmt.training - Epoch   4, Step:   173600, Batch Loss:     1.902617, Tokens per Sec:     7416, Lr: 0.000300\n",
            "2021-07-22 10:37:05,187 - INFO - joeynmt.training - Epoch   4, Step:   173800, Batch Loss:     1.822190, Tokens per Sec:     7429, Lr: 0.000300\n",
            "2021-07-22 10:38:04,739 - INFO - joeynmt.training - Epoch   4, Step:   174000, Batch Loss:     1.976517, Tokens per Sec:     7442, Lr: 0.000300\n",
            "2021-07-22 10:39:03,894 - INFO - joeynmt.training - Epoch   4, Step:   174200, Batch Loss:     1.693346, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-22 10:40:03,482 - INFO - joeynmt.training - Epoch   4, Step:   174400, Batch Loss:     1.585346, Tokens per Sec:     7425, Lr: 0.000300\n",
            "2021-07-22 10:41:02,738 - INFO - joeynmt.training - Epoch   4, Step:   174600, Batch Loss:     1.723059, Tokens per Sec:     7319, Lr: 0.000300\n",
            "2021-07-22 10:42:02,012 - INFO - joeynmt.training - Epoch   4, Step:   174800, Batch Loss:     1.669932, Tokens per Sec:     7414, Lr: 0.000300\n",
            "2021-07-22 10:43:01,568 - INFO - joeynmt.training - Epoch   4, Step:   175000, Batch Loss:     1.881722, Tokens per Sec:     7392, Lr: 0.000300\n",
            "2021-07-22 10:45:23,675 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 10:45:23,676 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 10:45:23,676 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 10:45:25,488 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 10:45:25,488 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-22 10:45:25,489 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-22 10:45:25,489 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-22 10:45:25,489 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 10:45:25,490 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-22 10:45:25,490 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-22 10:45:25,490 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-22 10:45:25,490 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 10:45:25,493 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-22 10:45:25,493 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 10:45:25,493 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 10:45:25,493 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 10:45:25,494 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-22 10:45:25,494 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-22 10:45:25,495 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-22 10:45:25,495 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   175000: bleu:  15.82, loss: 132337.9375, ppl:   8.2896, duration: 143.9262s\n",
            "2021-07-22 10:46:25,133 - INFO - joeynmt.training - Epoch   4, Step:   175200, Batch Loss:     2.268006, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-22 10:47:24,238 - INFO - joeynmt.training - Epoch   4, Step:   175400, Batch Loss:     1.636148, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-22 10:48:23,361 - INFO - joeynmt.training - Epoch   4, Step:   175600, Batch Loss:     1.610543, Tokens per Sec:     7377, Lr: 0.000300\n",
            "2021-07-22 10:49:22,710 - INFO - joeynmt.training - Epoch   4, Step:   175800, Batch Loss:     1.877276, Tokens per Sec:     7357, Lr: 0.000300\n",
            "2021-07-22 10:50:22,117 - INFO - joeynmt.training - Epoch   4, Step:   176000, Batch Loss:     1.790007, Tokens per Sec:     7313, Lr: 0.000300\n",
            "2021-07-22 10:51:21,497 - INFO - joeynmt.training - Epoch   4, Step:   176200, Batch Loss:     1.651983, Tokens per Sec:     7380, Lr: 0.000300\n",
            "2021-07-22 10:52:20,355 - INFO - joeynmt.training - Epoch   4, Step:   176400, Batch Loss:     2.052588, Tokens per Sec:     7264, Lr: 0.000300\n",
            "2021-07-22 10:53:19,598 - INFO - joeynmt.training - Epoch   4, Step:   176600, Batch Loss:     1.865271, Tokens per Sec:     7385, Lr: 0.000300\n",
            "2021-07-22 10:54:18,971 - INFO - joeynmt.training - Epoch   4, Step:   176800, Batch Loss:     1.847748, Tokens per Sec:     7356, Lr: 0.000300\n",
            "2021-07-22 10:54:23,632 - INFO - joeynmt.training - Epoch   4: total training loss 9820.97\n",
            "2021-07-22 10:54:23,633 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-22 10:55:19,410 - INFO - joeynmt.training - Epoch   5, Step:   177000, Batch Loss:     1.619589, Tokens per Sec:     7251, Lr: 0.000300\n",
            "2021-07-22 10:56:19,057 - INFO - joeynmt.training - Epoch   5, Step:   177200, Batch Loss:     1.444275, Tokens per Sec:     7422, Lr: 0.000300\n",
            "2021-07-22 10:57:18,879 - INFO - joeynmt.training - Epoch   5, Step:   177400, Batch Loss:     1.666916, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-22 10:58:18,066 - INFO - joeynmt.training - Epoch   5, Step:   177600, Batch Loss:     1.712250, Tokens per Sec:     7297, Lr: 0.000300\n",
            "2021-07-22 10:59:17,213 - INFO - joeynmt.training - Epoch   5, Step:   177800, Batch Loss:     1.695050, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-22 11:00:16,921 - INFO - joeynmt.training - Epoch   5, Step:   178000, Batch Loss:     1.559981, Tokens per Sec:     7441, Lr: 0.000300\n",
            "2021-07-22 11:01:16,352 - INFO - joeynmt.training - Epoch   5, Step:   178200, Batch Loss:     1.943918, Tokens per Sec:     7466, Lr: 0.000300\n",
            "2021-07-22 11:02:15,773 - INFO - joeynmt.training - Epoch   5, Step:   178400, Batch Loss:     1.456104, Tokens per Sec:     7383, Lr: 0.000300\n",
            "2021-07-22 11:03:14,881 - INFO - joeynmt.training - Epoch   5, Step:   178600, Batch Loss:     1.816335, Tokens per Sec:     7334, Lr: 0.000300\n",
            "2021-07-22 11:04:14,128 - INFO - joeynmt.training - Epoch   5, Step:   178800, Batch Loss:     1.896407, Tokens per Sec:     7325, Lr: 0.000300\n",
            "2021-07-22 11:05:13,616 - INFO - joeynmt.training - Epoch   5, Step:   179000, Batch Loss:     1.496266, Tokens per Sec:     7432, Lr: 0.000300\n",
            "2021-07-22 11:06:12,964 - INFO - joeynmt.training - Epoch   5, Step:   179200, Batch Loss:     1.827272, Tokens per Sec:     7350, Lr: 0.000300\n",
            "2021-07-22 11:07:12,369 - INFO - joeynmt.training - Epoch   5, Step:   179400, Batch Loss:     1.609470, Tokens per Sec:     7315, Lr: 0.000300\n",
            "2021-07-22 11:08:11,915 - INFO - joeynmt.training - Epoch   5, Step:   179600, Batch Loss:     1.928710, Tokens per Sec:     7426, Lr: 0.000300\n",
            "2021-07-22 11:09:11,052 - INFO - joeynmt.training - Epoch   5, Step:   179800, Batch Loss:     1.748572, Tokens per Sec:     7327, Lr: 0.000300\n",
            "2021-07-22 11:10:10,661 - INFO - joeynmt.training - Epoch   5, Step:   180000, Batch Loss:     1.532021, Tokens per Sec:     7448, Lr: 0.000300\n",
            "2021-07-22 11:12:22,954 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 11:12:22,955 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 11:12:22,955 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 11:12:23,799 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 11:12:23,799 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 11:12:24,754 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 11:12:24,755 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-22 11:12:24,755 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-22 11:12:24,755 - INFO - joeynmt.training - \tHypothesis: I was touched by the heart .\n",
            "2021-07-22 11:12:24,755 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 11:12:24,756 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-22 11:12:24,756 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-22 11:12:24,756 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-22 11:12:24,756 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 11:12:24,757 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-22 11:12:24,757 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 11:12:24,758 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 11:12:24,758 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 11:12:24,758 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-22 11:12:24,759 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-22 11:12:24,759 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-22 11:12:24,759 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   180000: bleu:  16.15, loss: 131815.1719, ppl:   8.2207, duration: 134.0982s\n",
            "2021-07-22 11:13:24,052 - INFO - joeynmt.training - Epoch   5, Step:   180200, Batch Loss:     1.559566, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-22 11:14:23,691 - INFO - joeynmt.training - Epoch   5, Step:   180400, Batch Loss:     1.780692, Tokens per Sec:     7337, Lr: 0.000300\n",
            "2021-07-22 11:15:22,506 - INFO - joeynmt.training - Epoch   5, Step:   180600, Batch Loss:     1.907163, Tokens per Sec:     7247, Lr: 0.000300\n",
            "2021-07-22 11:16:22,014 - INFO - joeynmt.training - Epoch   5, Step:   180800, Batch Loss:     1.703476, Tokens per Sec:     7422, Lr: 0.000300\n",
            "2021-07-22 11:17:21,508 - INFO - joeynmt.training - Epoch   5, Step:   181000, Batch Loss:     1.759245, Tokens per Sec:     7404, Lr: 0.000300\n",
            "2021-07-22 11:18:20,140 - INFO - joeynmt.training - Epoch   5, Step:   181200, Batch Loss:     1.835855, Tokens per Sec:     7366, Lr: 0.000300\n",
            "2021-07-22 11:19:19,680 - INFO - joeynmt.training - Epoch   5, Step:   181400, Batch Loss:     1.879593, Tokens per Sec:     7365, Lr: 0.000300\n",
            "2021-07-22 11:20:19,260 - INFO - joeynmt.training - Epoch   5, Step:   181600, Batch Loss:     1.882213, Tokens per Sec:     7364, Lr: 0.000300\n",
            "2021-07-22 11:21:18,643 - INFO - joeynmt.training - Epoch   5, Step:   181800, Batch Loss:     1.870157, Tokens per Sec:     7348, Lr: 0.000300\n",
            "2021-07-22 11:22:18,092 - INFO - joeynmt.training - Epoch   5, Step:   182000, Batch Loss:     2.020077, Tokens per Sec:     7374, Lr: 0.000300\n",
            "2021-07-22 11:23:17,472 - INFO - joeynmt.training - Epoch   5, Step:   182200, Batch Loss:     2.040096, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-22 11:24:00,015 - INFO - joeynmt.training - Epoch   5: total training loss 9800.09\n",
            "2021-07-22 11:24:00,016 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-22 11:24:17,732 - INFO - joeynmt.training - Epoch   6, Step:   182400, Batch Loss:     1.618184, Tokens per Sec:     7102, Lr: 0.000300\n",
            "2021-07-22 11:25:17,317 - INFO - joeynmt.training - Epoch   6, Step:   182600, Batch Loss:     1.822350, Tokens per Sec:     7348, Lr: 0.000300\n",
            "2021-07-22 11:26:16,652 - INFO - joeynmt.training - Epoch   6, Step:   182800, Batch Loss:     1.585361, Tokens per Sec:     7274, Lr: 0.000300\n",
            "2021-07-22 11:27:16,039 - INFO - joeynmt.training - Epoch   6, Step:   183000, Batch Loss:     1.765411, Tokens per Sec:     7357, Lr: 0.000300\n",
            "2021-07-22 11:28:15,550 - INFO - joeynmt.training - Epoch   6, Step:   183200, Batch Loss:     1.718197, Tokens per Sec:     7374, Lr: 0.000300\n",
            "2021-07-22 11:29:14,842 - INFO - joeynmt.training - Epoch   6, Step:   183400, Batch Loss:     1.756493, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-22 11:30:14,121 - INFO - joeynmt.training - Epoch   6, Step:   183600, Batch Loss:     1.687576, Tokens per Sec:     7358, Lr: 0.000300\n",
            "2021-07-22 11:31:13,700 - INFO - joeynmt.training - Epoch   6, Step:   183800, Batch Loss:     1.651533, Tokens per Sec:     7437, Lr: 0.000300\n",
            "2021-07-22 11:32:12,770 - INFO - joeynmt.training - Epoch   6, Step:   184000, Batch Loss:     1.810141, Tokens per Sec:     7349, Lr: 0.000300\n",
            "2021-07-22 11:33:11,666 - INFO - joeynmt.training - Epoch   6, Step:   184200, Batch Loss:     1.877907, Tokens per Sec:     7359, Lr: 0.000300\n",
            "2021-07-22 11:34:11,996 - INFO - joeynmt.training - Epoch   6, Step:   184400, Batch Loss:     1.679476, Tokens per Sec:     7569, Lr: 0.000300\n",
            "2021-07-22 11:35:11,491 - INFO - joeynmt.training - Epoch   6, Step:   184600, Batch Loss:     1.684323, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-22 11:36:11,053 - INFO - joeynmt.training - Epoch   6, Step:   184800, Batch Loss:     1.708054, Tokens per Sec:     7368, Lr: 0.000300\n",
            "2021-07-22 11:37:09,912 - INFO - joeynmt.training - Epoch   6, Step:   185000, Batch Loss:     1.709869, Tokens per Sec:     7325, Lr: 0.000300\n",
            "2021-07-22 11:39:12,595 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 11:39:12,595 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 11:39:12,596 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 11:39:14,313 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 11:39:14,314 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-22 11:39:14,314 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-22 11:39:14,314 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-22 11:39:14,314 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 11:39:14,315 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-22 11:39:14,315 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-22 11:39:14,315 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-22 11:39:14,316 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 11:39:14,316 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-22 11:39:14,316 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 11:39:14,317 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 11:39:14,317 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 11:39:14,317 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-22 11:39:14,318 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-22 11:39:14,318 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-22 11:39:14,318 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   185000: bleu:  16.58, loss: 131990.8594, ppl:   8.2438, duration: 124.4055s\n",
            "2021-07-22 11:40:13,663 - INFO - joeynmt.training - Epoch   6, Step:   185200, Batch Loss:     1.812530, Tokens per Sec:     7377, Lr: 0.000300\n",
            "2021-07-22 11:41:13,073 - INFO - joeynmt.training - Epoch   6, Step:   185400, Batch Loss:     1.790104, Tokens per Sec:     7365, Lr: 0.000300\n",
            "2021-07-22 11:42:12,587 - INFO - joeynmt.training - Epoch   6, Step:   185600, Batch Loss:     1.818675, Tokens per Sec:     7361, Lr: 0.000300\n",
            "2021-07-22 11:43:11,828 - INFO - joeynmt.training - Epoch   6, Step:   185800, Batch Loss:     1.771173, Tokens per Sec:     7357, Lr: 0.000300\n",
            "2021-07-22 11:44:11,596 - INFO - joeynmt.training - Epoch   6, Step:   186000, Batch Loss:     1.795963, Tokens per Sec:     7426, Lr: 0.000300\n",
            "2021-07-22 11:45:11,034 - INFO - joeynmt.training - Epoch   6, Step:   186200, Batch Loss:     1.800090, Tokens per Sec:     7320, Lr: 0.000300\n",
            "2021-07-22 11:46:10,587 - INFO - joeynmt.training - Epoch   6, Step:   186400, Batch Loss:     1.690902, Tokens per Sec:     7411, Lr: 0.000300\n",
            "2021-07-22 11:47:09,868 - INFO - joeynmt.training - Epoch   6, Step:   186600, Batch Loss:     1.675857, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-22 11:48:09,423 - INFO - joeynmt.training - Epoch   6, Step:   186800, Batch Loss:     1.704644, Tokens per Sec:     7304, Lr: 0.000300\n",
            "2021-07-22 11:49:08,915 - INFO - joeynmt.training - Epoch   6, Step:   187000, Batch Loss:     1.798076, Tokens per Sec:     7368, Lr: 0.000300\n",
            "2021-07-22 11:50:08,295 - INFO - joeynmt.training - Epoch   6, Step:   187200, Batch Loss:     1.615307, Tokens per Sec:     7357, Lr: 0.000300\n",
            "2021-07-22 11:51:07,813 - INFO - joeynmt.training - Epoch   6, Step:   187400, Batch Loss:     1.717276, Tokens per Sec:     7405, Lr: 0.000300\n",
            "2021-07-22 11:52:07,108 - INFO - joeynmt.training - Epoch   6, Step:   187600, Batch Loss:     1.803952, Tokens per Sec:     7338, Lr: 0.000300\n",
            "2021-07-22 11:53:06,192 - INFO - joeynmt.training - Epoch   6, Step:   187800, Batch Loss:     1.623282, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-22 11:53:27,276 - INFO - joeynmt.training - Epoch   6: total training loss 9779.27\n",
            "2021-07-22 11:53:27,276 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-22 11:54:06,107 - INFO - joeynmt.training - Epoch   7, Step:   188000, Batch Loss:     1.845813, Tokens per Sec:     7198, Lr: 0.000300\n",
            "2021-07-22 11:55:05,558 - INFO - joeynmt.training - Epoch   7, Step:   188200, Batch Loss:     1.856887, Tokens per Sec:     7307, Lr: 0.000300\n",
            "2021-07-22 11:56:05,325 - INFO - joeynmt.training - Epoch   7, Step:   188400, Batch Loss:     1.738914, Tokens per Sec:     7459, Lr: 0.000300\n",
            "2021-07-22 11:57:05,270 - INFO - joeynmt.training - Epoch   7, Step:   188600, Batch Loss:     1.680325, Tokens per Sec:     7418, Lr: 0.000300\n",
            "2021-07-22 11:58:04,402 - INFO - joeynmt.training - Epoch   7, Step:   188800, Batch Loss:     1.419095, Tokens per Sec:     7281, Lr: 0.000300\n",
            "2021-07-22 11:59:03,683 - INFO - joeynmt.training - Epoch   7, Step:   189000, Batch Loss:     1.701525, Tokens per Sec:     7282, Lr: 0.000300\n",
            "2021-07-22 12:00:02,960 - INFO - joeynmt.training - Epoch   7, Step:   189200, Batch Loss:     1.925705, Tokens per Sec:     7404, Lr: 0.000300\n",
            "2021-07-22 12:01:02,179 - INFO - joeynmt.training - Epoch   7, Step:   189400, Batch Loss:     1.607216, Tokens per Sec:     7435, Lr: 0.000300\n",
            "2021-07-22 12:02:01,411 - INFO - joeynmt.training - Epoch   7, Step:   189600, Batch Loss:     1.791275, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-22 12:03:01,169 - INFO - joeynmt.training - Epoch   7, Step:   189800, Batch Loss:     1.670158, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-22 12:04:00,670 - INFO - joeynmt.training - Epoch   7, Step:   190000, Batch Loss:     1.639715, Tokens per Sec:     7354, Lr: 0.000300\n",
            "2021-07-22 12:06:12,176 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 12:06:12,176 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 12:06:12,176 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 12:06:13,041 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 12:06:13,042 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 12:06:14,120 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 12:06:14,121 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-22 12:06:14,121 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-22 12:06:14,122 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-22 12:06:14,122 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 12:06:14,122 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-22 12:06:14,123 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-22 12:06:14,123 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-22 12:06:14,123 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 12:06:14,124 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-22 12:06:14,124 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 12:06:14,124 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 12:06:14,125 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 12:06:14,125 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-22 12:06:14,127 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-22 12:06:14,128 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-22 12:06:14,128 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   190000: bleu:  16.48, loss: 131120.8594, ppl:   8.1299, duration: 133.4576s\n",
            "2021-07-22 12:07:13,351 - INFO - joeynmt.training - Epoch   7, Step:   190200, Batch Loss:     1.659379, Tokens per Sec:     7263, Lr: 0.000300\n",
            "2021-07-22 12:08:13,186 - INFO - joeynmt.training - Epoch   7, Step:   190400, Batch Loss:     1.683616, Tokens per Sec:     7453, Lr: 0.000300\n",
            "2021-07-22 12:09:12,526 - INFO - joeynmt.training - Epoch   7, Step:   190600, Batch Loss:     1.842473, Tokens per Sec:     7332, Lr: 0.000300\n",
            "2021-07-22 12:10:12,256 - INFO - joeynmt.training - Epoch   7, Step:   190800, Batch Loss:     1.424901, Tokens per Sec:     7401, Lr: 0.000300\n",
            "2021-07-22 12:11:11,480 - INFO - joeynmt.training - Epoch   7, Step:   191000, Batch Loss:     1.714709, Tokens per Sec:     7365, Lr: 0.000300\n",
            "2021-07-22 12:12:10,489 - INFO - joeynmt.training - Epoch   7, Step:   191200, Batch Loss:     1.997056, Tokens per Sec:     7312, Lr: 0.000300\n",
            "2021-07-22 12:13:10,441 - INFO - joeynmt.training - Epoch   7, Step:   191400, Batch Loss:     1.605424, Tokens per Sec:     7509, Lr: 0.000300\n",
            "2021-07-22 12:14:09,332 - INFO - joeynmt.training - Epoch   7, Step:   191600, Batch Loss:     1.873238, Tokens per Sec:     7271, Lr: 0.000300\n",
            "2021-07-22 12:15:08,458 - INFO - joeynmt.training - Epoch   7, Step:   191800, Batch Loss:     1.602330, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-22 12:16:07,807 - INFO - joeynmt.training - Epoch   7, Step:   192000, Batch Loss:     1.953344, Tokens per Sec:     7345, Lr: 0.000300\n",
            "2021-07-22 12:17:07,131 - INFO - joeynmt.training - Epoch   7, Step:   192200, Batch Loss:     1.666704, Tokens per Sec:     7354, Lr: 0.000300\n",
            "2021-07-22 12:18:06,408 - INFO - joeynmt.training - Epoch   7, Step:   192400, Batch Loss:     1.740103, Tokens per Sec:     7452, Lr: 0.000300\n",
            "2021-07-22 12:19:05,834 - INFO - joeynmt.training - Epoch   7, Step:   192600, Batch Loss:     2.030323, Tokens per Sec:     7347, Lr: 0.000300\n",
            "2021-07-22 12:20:05,455 - INFO - joeynmt.training - Epoch   7, Step:   192800, Batch Loss:     1.982376, Tokens per Sec:     7496, Lr: 0.000300\n",
            "2021-07-22 12:21:05,099 - INFO - joeynmt.training - Epoch   7, Step:   193000, Batch Loss:     1.928636, Tokens per Sec:     7419, Lr: 0.000300\n",
            "2021-07-22 12:22:03,933 - INFO - joeynmt.training - Epoch   7, Step:   193200, Batch Loss:     2.146324, Tokens per Sec:     7261, Lr: 0.000300\n",
            "2021-07-22 12:23:03,632 - INFO - joeynmt.training - Epoch   7, Step:   193400, Batch Loss:     1.831694, Tokens per Sec:     7373, Lr: 0.000300\n",
            "2021-07-22 12:23:03,977 - INFO - joeynmt.training - Epoch   7: total training loss 9756.91\n",
            "2021-07-22 12:23:03,977 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-22 12:24:04,023 - INFO - joeynmt.training - Epoch   8, Step:   193600, Batch Loss:     2.023585, Tokens per Sec:     7298, Lr: 0.000300\n",
            "2021-07-22 12:25:03,481 - INFO - joeynmt.training - Epoch   8, Step:   193800, Batch Loss:     1.691375, Tokens per Sec:     7376, Lr: 0.000300\n",
            "2021-07-22 12:26:02,394 - INFO - joeynmt.training - Epoch   8, Step:   194000, Batch Loss:     1.641553, Tokens per Sec:     7218, Lr: 0.000300\n",
            "2021-07-22 12:27:02,379 - INFO - joeynmt.training - Epoch   8, Step:   194200, Batch Loss:     1.823611, Tokens per Sec:     7476, Lr: 0.000300\n",
            "2021-07-22 12:28:01,892 - INFO - joeynmt.training - Epoch   8, Step:   194400, Batch Loss:     1.684845, Tokens per Sec:     7313, Lr: 0.000300\n",
            "2021-07-22 12:29:00,831 - INFO - joeynmt.training - Epoch   8, Step:   194600, Batch Loss:     1.820963, Tokens per Sec:     7291, Lr: 0.000300\n",
            "2021-07-22 12:30:00,026 - INFO - joeynmt.training - Epoch   8, Step:   194800, Batch Loss:     1.781170, Tokens per Sec:     7370, Lr: 0.000300\n",
            "2021-07-22 12:30:59,135 - INFO - joeynmt.training - Epoch   8, Step:   195000, Batch Loss:     1.529291, Tokens per Sec:     7446, Lr: 0.000300\n",
            "2021-07-22 12:33:04,891 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-22 12:33:04,892 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-22 12:33:04,892 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-22 12:33:05,733 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-22 12:33:05,734 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-22 12:33:06,634 - INFO - joeynmt.training - Example #0\n",
            "2021-07-22 12:33:06,640 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-22 12:33:06,640 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-22 12:33:06,640 - INFO - joeynmt.training - \tHypothesis: It touched my heart .\n",
            "2021-07-22 12:33:06,641 - INFO - joeynmt.training - Example #1\n",
            "2021-07-22 12:33:06,641 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-22 12:33:06,642 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-22 12:33:06,642 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-22 12:33:06,642 - INFO - joeynmt.training - Example #2\n",
            "2021-07-22 12:33:06,643 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-22 12:33:06,643 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 12:33:06,643 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-22 12:33:06,644 - INFO - joeynmt.training - Example #3\n",
            "2021-07-22 12:33:06,644 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-22 12:33:06,645 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-22 12:33:06,645 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-22 12:33:06,645 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   195000: bleu:  17.12, loss: 130885.5312, ppl:   8.0994, duration: 127.5094s\n",
            "2021-07-22 12:34:06,551 - INFO - joeynmt.training - Epoch   8, Step:   195200, Batch Loss:     1.883740, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-22 12:35:06,379 - INFO - joeynmt.training - Epoch   8, Step:   195400, Batch Loss:     1.732166, Tokens per Sec:     7424, Lr: 0.000300\n",
            "2021-07-22 12:36:05,661 - INFO - joeynmt.training - Epoch   8, Step:   195600, Batch Loss:     1.621698, Tokens per Sec:     7347, Lr: 0.000300\n",
            "2021-07-22 12:37:05,045 - INFO - joeynmt.training - Epoch   8, Step:   195800, Batch Loss:     1.934993, Tokens per Sec:     7381, Lr: 0.000300\n",
            "2021-07-22 12:38:03,924 - INFO - joeynmt.training - Epoch   8, Step:   196000, Batch Loss:     1.775495, Tokens per Sec:     7254, Lr: 0.000300\n",
            "2021-07-22 12:39:03,131 - INFO - joeynmt.training - Epoch   8, Step:   196200, Batch Loss:     1.680751, Tokens per Sec:     7403, Lr: 0.000300\n",
            "2021-07-22 12:40:03,122 - INFO - joeynmt.training - Epoch   8, Step:   196400, Batch Loss:     1.722366, Tokens per Sec:     7416, Lr: 0.000300\n",
            "2021-07-22 12:41:02,568 - INFO - joeynmt.training - Epoch   8, Step:   196600, Batch Loss:     1.782433, Tokens per Sec:     7268, Lr: 0.000300\n",
            "2021-07-22 12:42:01,887 - INFO - joeynmt.training - Epoch   8, Step:   196800, Batch Loss:     1.542099, Tokens per Sec:     7393, Lr: 0.000300\n",
            "2021-07-22 12:43:01,294 - INFO - joeynmt.training - Epoch   8, Step:   197000, Batch Loss:     1.823212, Tokens per Sec:     7302, Lr: 0.000300\n",
            "2021-07-22 12:44:00,907 - INFO - joeynmt.training - Epoch   8, Step:   197200, Batch Loss:     1.770918, Tokens per Sec:     7382, Lr: 0.000300\n",
            "2021-07-22 12:45:00,639 - INFO - joeynmt.training - Epoch   8, Step:   197400, Batch Loss:     1.601124, Tokens per Sec:     7453, Lr: 0.000300\n",
            "2021-07-22 12:45:59,640 - INFO - joeynmt.training - Epoch   8, Step:   197600, Batch Loss:     1.758035, Tokens per Sec:     7330, Lr: 0.000300\n",
            "2021-07-22 12:46:59,141 - INFO - joeynmt.training - Epoch   8, Step:   197800, Batch Loss:     1.775526, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-22 12:47:58,644 - INFO - joeynmt.training - Epoch   8, Step:   198000, Batch Loss:     1.762685, Tokens per Sec:     7400, Lr: 0.000300\n",
            "2021-07-22 12:48:57,827 - INFO - joeynmt.training - Epoch   8, Step:   198200, Batch Loss:     1.564546, Tokens per Sec:     7336, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gESx7_Whvk7"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 195000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued2/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued3\"').replace(\n",
        "            f'epochs: 30', f'epochs: 22')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload3.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sR3yGxSsznZu",
        "outputId": "4aa719e2-726b-4c74-dc2f-5c2ea39d25f7"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_{name}_reload3.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"rw_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"rw_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued2/195000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 22                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/rw_lhen_reverse_transformer_continued3\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdlsoez5znSy",
        "outputId": "da894770-a17c-4883-9bf7-7ffdf39efd2d"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload3.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-27 07:15:35,664 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-27 07:15:35,743 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-27 07:15:48,636 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-27 07:15:49,505 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-27 07:15:51,070 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-27 07:15:52,593 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-27 07:15:52,593 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-27 07:15:53,025 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-27 07:15:53.282177: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-27 07:15:55,390 - INFO - joeynmt.training - Total params: 12177920\n",
            "2021-07-27 07:16:04,059 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued2/195000.ckpt\n",
            "2021-07-27 07:16:04,617 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
            "2021-07-27 07:16:04,618 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
            "2021-07-27 07:16:04,618 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-27 07:16:04,618 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
            "2021-07-27 07:16:04,618 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
            "2021-07-27 07:16:04,619 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
            "2021-07-27 07:16:04,619 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-27 07:16:04,619 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-27 07:16:04,619 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-27 07:16:04,620 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-27 07:16:04,620 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-27 07:16:04,620 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-27 07:16:04,620 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-27 07:16:04,621 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued2/195000.ckpt\n",
            "2021-07-27 07:16:04,621 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-27 07:16:04,621 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-27 07:16:04,621 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-27 07:16:04,622 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-27 07:16:04,622 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-27 07:16:04,622 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-27 07:16:04,622 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-27 07:16:04,622 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-27 07:16:04,623 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-27 07:16:04,623 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-27 07:16:04,623 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-27 07:16:04,623 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-27 07:16:04,624 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-27 07:16:04,624 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-27 07:16:04,624 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-27 07:16:04,624 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-27 07:16:04,625 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-27 07:16:04,625 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-27 07:16:04,625 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-27 07:16:04,625 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-27 07:16:04,626 - INFO - joeynmt.helpers - cfg.training.epochs                : 22\n",
            "2021-07-27 07:16:04,626 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-27 07:16:04,626 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-27 07:16:04,626 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-27 07:16:04,627 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued3\n",
            "2021-07-27 07:16:04,627 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-27 07:16:04,627 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-27 07:16:04,627 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-27 07:16:04,627 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-27 07:16:04,628 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-27 07:16:04,628 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-27 07:16:04,628 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-27 07:16:04,628 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-27 07:16:04,629 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-27 07:16:04,629 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-27 07:16:04,629 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-27 07:16:04,629 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-27 07:16:04,630 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-27 07:16:04,630 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-27 07:16:04,630 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-27 07:16:04,630 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-27 07:16:04,630 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-27 07:16:04,631 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-27 07:16:04,631 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-27 07:16:04,631 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-27 07:16:04,632 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-27 07:16:04,632 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-27 07:16:04,632 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-27 07:16:04,632 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-27 07:16:04,633 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-27 07:16:04,633 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-27 07:16:04,633 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-27 07:16:04,633 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-27 07:16:04,633 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-27 07:16:04,634 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-27 07:16:04,634 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-27 07:16:04,634 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 439996,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-27 07:16:04,634 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
            "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
            "2021-07-27 07:16:04,635 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-27 07:16:04,635 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-27 07:16:04,635 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
            "2021-07-27 07:16:04,635 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
            "2021-07-27 07:16:04,636 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
            "2021-07-27 07:16:04,649 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-27 07:16:04,650 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-27 07:17:05,957 - INFO - joeynmt.training - Epoch   1, Step:   195200, Batch Loss:     1.853871, Tokens per Sec:     7210, Lr: 0.000300\n",
            "2021-07-27 07:18:06,001 - INFO - joeynmt.training - Epoch   1, Step:   195400, Batch Loss:     1.711229, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-27 07:19:05,377 - INFO - joeynmt.training - Epoch   1, Step:   195600, Batch Loss:     1.638873, Tokens per Sec:     7335, Lr: 0.000300\n",
            "2021-07-27 07:20:04,834 - INFO - joeynmt.training - Epoch   1, Step:   195800, Batch Loss:     1.948526, Tokens per Sec:     7372, Lr: 0.000300\n",
            "2021-07-27 07:21:03,847 - INFO - joeynmt.training - Epoch   1, Step:   196000, Batch Loss:     1.785041, Tokens per Sec:     7237, Lr: 0.000300\n",
            "2021-07-27 07:22:03,216 - INFO - joeynmt.training - Epoch   1, Step:   196200, Batch Loss:     1.637930, Tokens per Sec:     7383, Lr: 0.000300\n",
            "2021-07-27 07:23:03,219 - INFO - joeynmt.training - Epoch   1, Step:   196400, Batch Loss:     1.781157, Tokens per Sec:     7415, Lr: 0.000300\n",
            "2021-07-27 07:24:02,765 - INFO - joeynmt.training - Epoch   1, Step:   196600, Batch Loss:     1.798482, Tokens per Sec:     7256, Lr: 0.000300\n",
            "2021-07-27 07:25:02,176 - INFO - joeynmt.training - Epoch   1, Step:   196800, Batch Loss:     1.582507, Tokens per Sec:     7381, Lr: 0.000300\n",
            "2021-07-27 07:26:01,658 - INFO - joeynmt.training - Epoch   1, Step:   197000, Batch Loss:     1.813810, Tokens per Sec:     7293, Lr: 0.000300\n",
            "2021-07-27 07:27:01,511 - INFO - joeynmt.training - Epoch   1, Step:   197200, Batch Loss:     1.798803, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-27 07:28:01,442 - INFO - joeynmt.training - Epoch   1, Step:   197400, Batch Loss:     1.607929, Tokens per Sec:     7429, Lr: 0.000300\n",
            "2021-07-27 07:29:00,608 - INFO - joeynmt.training - Epoch   1, Step:   197600, Batch Loss:     1.758877, Tokens per Sec:     7310, Lr: 0.000300\n",
            "2021-07-27 07:30:00,302 - INFO - joeynmt.training - Epoch   1, Step:   197800, Batch Loss:     1.791344, Tokens per Sec:     7300, Lr: 0.000300\n",
            "2021-07-27 07:30:59,778 - INFO - joeynmt.training - Epoch   1, Step:   198000, Batch Loss:     1.739555, Tokens per Sec:     7403, Lr: 0.000300\n",
            "2021-07-27 07:31:58,987 - INFO - joeynmt.training - Epoch   1, Step:   198200, Batch Loss:     1.555042, Tokens per Sec:     7333, Lr: 0.000300\n",
            "2021-07-27 07:32:58,657 - INFO - joeynmt.training - Epoch   1, Step:   198400, Batch Loss:     1.751191, Tokens per Sec:     7484, Lr: 0.000300\n",
            "2021-07-27 07:33:57,646 - INFO - joeynmt.training - Epoch   1, Step:   198600, Batch Loss:     1.829039, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-27 07:34:57,184 - INFO - joeynmt.training - Epoch   1, Step:   198800, Batch Loss:     1.620608, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-27 07:35:36,194 - INFO - joeynmt.training - Epoch   1: total training loss 6923.95\n",
            "2021-07-27 07:35:36,195 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-27 07:35:57,203 - INFO - joeynmt.training - Epoch   2, Step:   199000, Batch Loss:     1.559637, Tokens per Sec:     7036, Lr: 0.000300\n",
            "2021-07-27 07:36:56,934 - INFO - joeynmt.training - Epoch   2, Step:   199200, Batch Loss:     1.636938, Tokens per Sec:     7398, Lr: 0.000300\n",
            "2021-07-27 07:37:56,752 - INFO - joeynmt.training - Epoch   2, Step:   199400, Batch Loss:     1.693427, Tokens per Sec:     7427, Lr: 0.000300\n",
            "2021-07-27 07:38:56,417 - INFO - joeynmt.training - Epoch   2, Step:   199600, Batch Loss:     1.901556, Tokens per Sec:     7361, Lr: 0.000300\n",
            "2021-07-27 07:39:55,608 - INFO - joeynmt.training - Epoch   2, Step:   199800, Batch Loss:     1.803984, Tokens per Sec:     7332, Lr: 0.000300\n",
            "2021-07-27 07:40:55,015 - INFO - joeynmt.training - Epoch   2, Step:   200000, Batch Loss:     2.134417, Tokens per Sec:     7357, Lr: 0.000300\n",
            "2021-07-27 07:43:01,002 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 07:43:01,003 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 07:43:01,003 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 07:43:01,819 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 07:43:01,819 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 07:43:02,690 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 07:43:02,691 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 07:43:02,692 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 07:43:02,692 - INFO - joeynmt.training - \tHypothesis: It touched my heart .\n",
            "2021-07-27 07:43:02,692 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 07:43:02,693 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 07:43:02,693 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 07:43:02,693 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-27 07:43:02,694 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 07:43:02,695 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 07:43:02,695 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 07:43:02,695 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 07:43:02,695 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 07:43:02,696 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 07:43:02,696 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 07:43:02,697 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being successful in Satan’s world .\n",
            "2021-07-27 07:43:02,697 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   200000: bleu:  16.77, loss: 130859.4219, ppl:   8.0960, duration: 127.6813s\n",
            "2021-07-27 07:44:02,689 - INFO - joeynmt.training - Epoch   2, Step:   200200, Batch Loss:     1.718345, Tokens per Sec:     7393, Lr: 0.000300\n",
            "2021-07-27 07:45:01,884 - INFO - joeynmt.training - Epoch   2, Step:   200400, Batch Loss:     1.813976, Tokens per Sec:     7299, Lr: 0.000300\n",
            "2021-07-27 07:46:01,430 - INFO - joeynmt.training - Epoch   2, Step:   200600, Batch Loss:     1.573622, Tokens per Sec:     7392, Lr: 0.000300\n",
            "2021-07-27 07:47:01,216 - INFO - joeynmt.training - Epoch   2, Step:   200800, Batch Loss:     1.574529, Tokens per Sec:     7394, Lr: 0.000300\n",
            "2021-07-27 07:48:00,809 - INFO - joeynmt.training - Epoch   2, Step:   201000, Batch Loss:     1.859375, Tokens per Sec:     7329, Lr: 0.000300\n",
            "2021-07-27 07:49:00,028 - INFO - joeynmt.training - Epoch   2, Step:   201200, Batch Loss:     2.533622, Tokens per Sec:     7348, Lr: 0.000300\n",
            "2021-07-27 07:49:59,770 - INFO - joeynmt.training - Epoch   2, Step:   201400, Batch Loss:     1.747406, Tokens per Sec:     7424, Lr: 0.000300\n",
            "2021-07-27 07:50:58,870 - INFO - joeynmt.training - Epoch   2, Step:   201600, Batch Loss:     1.768632, Tokens per Sec:     7297, Lr: 0.000300\n",
            "2021-07-27 07:51:58,247 - INFO - joeynmt.training - Epoch   2, Step:   201800, Batch Loss:     1.805760, Tokens per Sec:     7391, Lr: 0.000300\n",
            "2021-07-27 07:52:58,013 - INFO - joeynmt.training - Epoch   2, Step:   202000, Batch Loss:     1.825114, Tokens per Sec:     7364, Lr: 0.000300\n",
            "2021-07-27 07:53:57,933 - INFO - joeynmt.training - Epoch   2, Step:   202200, Batch Loss:     1.780870, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-27 07:54:57,211 - INFO - joeynmt.training - Epoch   2, Step:   202400, Batch Loss:     1.855786, Tokens per Sec:     7220, Lr: 0.000300\n",
            "2021-07-27 07:55:57,154 - INFO - joeynmt.training - Epoch   2, Step:   202600, Batch Loss:     1.746703, Tokens per Sec:     7450, Lr: 0.000300\n",
            "2021-07-27 07:56:56,230 - INFO - joeynmt.training - Epoch   2, Step:   202800, Batch Loss:     1.749973, Tokens per Sec:     7266, Lr: 0.000300\n",
            "2021-07-27 07:57:55,501 - INFO - joeynmt.training - Epoch   2, Step:   203000, Batch Loss:     1.581770, Tokens per Sec:     7325, Lr: 0.000300\n",
            "2021-07-27 07:58:55,293 - INFO - joeynmt.training - Epoch   2, Step:   203200, Batch Loss:     1.673063, Tokens per Sec:     7427, Lr: 0.000300\n",
            "2021-07-27 07:59:54,662 - INFO - joeynmt.training - Epoch   2, Step:   203400, Batch Loss:     1.542185, Tokens per Sec:     7320, Lr: 0.000300\n",
            "2021-07-27 08:00:54,513 - INFO - joeynmt.training - Epoch   2, Step:   203600, Batch Loss:     1.610040, Tokens per Sec:     7511, Lr: 0.000300\n",
            "2021-07-27 08:01:53,522 - INFO - joeynmt.training - Epoch   2, Step:   203800, Batch Loss:     1.754201, Tokens per Sec:     7222, Lr: 0.000300\n",
            "2021-07-27 08:02:52,614 - INFO - joeynmt.training - Epoch   2, Step:   204000, Batch Loss:     1.729584, Tokens per Sec:     7311, Lr: 0.000300\n",
            "2021-07-27 08:03:52,114 - INFO - joeynmt.training - Epoch   2, Step:   204200, Batch Loss:     1.512296, Tokens per Sec:     7373, Lr: 0.000300\n",
            "2021-07-27 08:04:51,881 - INFO - joeynmt.training - Epoch   2, Step:   204400, Batch Loss:     1.772501, Tokens per Sec:     7354, Lr: 0.000300\n",
            "2021-07-27 08:05:10,122 - INFO - joeynmt.training - Epoch   2: total training loss 9711.27\n",
            "2021-07-27 08:05:10,122 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-27 08:05:52,174 - INFO - joeynmt.training - Epoch   3, Step:   204600, Batch Loss:     1.634092, Tokens per Sec:     7249, Lr: 0.000300\n",
            "2021-07-27 08:06:51,808 - INFO - joeynmt.training - Epoch   3, Step:   204800, Batch Loss:     2.063439, Tokens per Sec:     7469, Lr: 0.000300\n",
            "2021-07-27 08:07:51,668 - INFO - joeynmt.training - Epoch   3, Step:   205000, Batch Loss:     1.800477, Tokens per Sec:     7355, Lr: 0.000300\n",
            "2021-07-27 08:09:53,596 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 08:09:53,596 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 08:09:53,596 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 08:09:55,235 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 08:09:55,236 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 08:09:55,236 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 08:09:55,237 - INFO - joeynmt.training - \tHypothesis: I was touched .\n",
            "2021-07-27 08:09:55,237 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 08:09:55,238 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 08:09:55,238 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 08:09:55,238 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-27 08:09:55,239 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 08:09:55,239 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 08:09:55,240 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 08:09:55,240 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 08:09:55,240 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 08:09:55,241 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 08:09:55,241 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 08:09:55,241 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 08:09:55,242 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   205000: bleu:  16.99, loss: 130961.0859, ppl:   8.1092, duration: 123.5733s\n",
            "2021-07-27 08:10:55,611 - INFO - joeynmt.training - Epoch   3, Step:   205200, Batch Loss:     1.794589, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-27 08:11:55,140 - INFO - joeynmt.training - Epoch   3, Step:   205400, Batch Loss:     1.623774, Tokens per Sec:     7238, Lr: 0.000300\n",
            "2021-07-27 08:12:54,706 - INFO - joeynmt.training - Epoch   3, Step:   205600, Batch Loss:     1.718253, Tokens per Sec:     7373, Lr: 0.000300\n",
            "2021-07-27 08:13:53,452 - INFO - joeynmt.training - Epoch   3, Step:   205800, Batch Loss:     1.616015, Tokens per Sec:     7275, Lr: 0.000300\n",
            "2021-07-27 08:14:53,002 - INFO - joeynmt.training - Epoch   3, Step:   206000, Batch Loss:     1.766969, Tokens per Sec:     7398, Lr: 0.000300\n",
            "2021-07-27 08:15:52,366 - INFO - joeynmt.training - Epoch   3, Step:   206200, Batch Loss:     1.425478, Tokens per Sec:     7329, Lr: 0.000300\n",
            "2021-07-27 08:16:52,035 - INFO - joeynmt.training - Epoch   3, Step:   206400, Batch Loss:     1.573135, Tokens per Sec:     7400, Lr: 0.000300\n",
            "2021-07-27 08:17:51,251 - INFO - joeynmt.training - Epoch   3, Step:   206600, Batch Loss:     1.707501, Tokens per Sec:     7300, Lr: 0.000300\n",
            "2021-07-27 08:18:50,360 - INFO - joeynmt.training - Epoch   3, Step:   206800, Batch Loss:     1.773623, Tokens per Sec:     7267, Lr: 0.000300\n",
            "2021-07-27 08:19:49,699 - INFO - joeynmt.training - Epoch   3, Step:   207000, Batch Loss:     1.585253, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-27 08:20:49,217 - INFO - joeynmt.training - Epoch   3, Step:   207200, Batch Loss:     1.832797, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-27 08:21:47,909 - INFO - joeynmt.training - Epoch   3, Step:   207400, Batch Loss:     1.594657, Tokens per Sec:     7263, Lr: 0.000300\n",
            "2021-07-27 08:22:47,707 - INFO - joeynmt.training - Epoch   3, Step:   207600, Batch Loss:     1.797282, Tokens per Sec:     7432, Lr: 0.000300\n",
            "2021-07-27 08:23:47,170 - INFO - joeynmt.training - Epoch   3, Step:   207800, Batch Loss:     1.750879, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-27 08:24:46,107 - INFO - joeynmt.training - Epoch   3, Step:   208000, Batch Loss:     1.481411, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 08:25:45,277 - INFO - joeynmt.training - Epoch   3, Step:   208200, Batch Loss:     1.757570, Tokens per Sec:     7350, Lr: 0.000300\n",
            "2021-07-27 08:26:44,744 - INFO - joeynmt.training - Epoch   3, Step:   208400, Batch Loss:     1.875741, Tokens per Sec:     7338, Lr: 0.000300\n",
            "2021-07-27 08:27:43,969 - INFO - joeynmt.training - Epoch   3, Step:   208600, Batch Loss:     1.876871, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-27 08:28:44,031 - INFO - joeynmt.training - Epoch   3, Step:   208800, Batch Loss:     1.771742, Tokens per Sec:     7398, Lr: 0.000300\n",
            "2021-07-27 08:29:43,437 - INFO - joeynmt.training - Epoch   3, Step:   209000, Batch Loss:     1.825232, Tokens per Sec:     7374, Lr: 0.000300\n",
            "2021-07-27 08:30:43,319 - INFO - joeynmt.training - Epoch   3, Step:   209200, Batch Loss:     2.195397, Tokens per Sec:     7477, Lr: 0.000300\n",
            "2021-07-27 08:31:42,754 - INFO - joeynmt.training - Epoch   3, Step:   209400, Batch Loss:     1.642986, Tokens per Sec:     7344, Lr: 0.000300\n",
            "2021-07-27 08:32:42,432 - INFO - joeynmt.training - Epoch   3, Step:   209600, Batch Loss:     1.771573, Tokens per Sec:     7455, Lr: 0.000300\n",
            "2021-07-27 08:33:41,818 - INFO - joeynmt.training - Epoch   3, Step:   209800, Batch Loss:     1.853614, Tokens per Sec:     7372, Lr: 0.000300\n",
            "2021-07-27 08:34:36,406 - INFO - joeynmt.training - Epoch   3: total training loss 9678.11\n",
            "2021-07-27 08:34:36,406 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-27 08:34:42,019 - INFO - joeynmt.training - Epoch   4, Step:   210000, Batch Loss:     1.544661, Tokens per Sec:     6398, Lr: 0.000300\n",
            "2021-07-27 08:36:48,949 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 08:36:48,950 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 08:36:48,950 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 08:36:49,758 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 08:36:49,758 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 08:36:50,679 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 08:36:50,680 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 08:36:50,680 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 08:36:50,681 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-27 08:36:50,681 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 08:36:50,682 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 08:36:50,682 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 08:36:50,682 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-27 08:36:50,682 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 08:36:50,683 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 08:36:50,683 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 08:36:50,684 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 08:36:50,684 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 08:36:50,684 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 08:36:50,685 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 08:36:50,685 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 08:36:50,685 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   210000: bleu:  16.96, loss: 130640.8516, ppl:   8.0678, duration: 128.6656s\n",
            "2021-07-27 08:37:50,638 - INFO - joeynmt.training - Epoch   4, Step:   210200, Batch Loss:     1.598294, Tokens per Sec:     7367, Lr: 0.000300\n",
            "2021-07-27 08:38:50,755 - INFO - joeynmt.training - Epoch   4, Step:   210400, Batch Loss:     1.744428, Tokens per Sec:     7520, Lr: 0.000300\n",
            "2021-07-27 08:39:50,438 - INFO - joeynmt.training - Epoch   4, Step:   210600, Batch Loss:     1.687291, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 08:40:49,947 - INFO - joeynmt.training - Epoch   4, Step:   210800, Batch Loss:     1.870412, Tokens per Sec:     7348, Lr: 0.000300\n",
            "2021-07-27 08:41:49,464 - INFO - joeynmt.training - Epoch   4, Step:   211000, Batch Loss:     1.830367, Tokens per Sec:     7372, Lr: 0.000300\n",
            "2021-07-27 08:42:48,692 - INFO - joeynmt.training - Epoch   4, Step:   211200, Batch Loss:     1.717337, Tokens per Sec:     7278, Lr: 0.000300\n",
            "2021-07-27 08:43:48,379 - INFO - joeynmt.training - Epoch   4, Step:   211400, Batch Loss:     1.662716, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-27 08:44:47,749 - INFO - joeynmt.training - Epoch   4, Step:   211600, Batch Loss:     1.832616, Tokens per Sec:     7256, Lr: 0.000300\n",
            "2021-07-27 08:45:47,062 - INFO - joeynmt.training - Epoch   4, Step:   211800, Batch Loss:     1.706477, Tokens per Sec:     7344, Lr: 0.000300\n",
            "2021-07-27 08:46:46,845 - INFO - joeynmt.training - Epoch   4, Step:   212000, Batch Loss:     1.874639, Tokens per Sec:     7422, Lr: 0.000300\n",
            "2021-07-27 08:47:46,613 - INFO - joeynmt.training - Epoch   4, Step:   212200, Batch Loss:     1.548368, Tokens per Sec:     7338, Lr: 0.000300\n",
            "2021-07-27 08:48:46,821 - INFO - joeynmt.training - Epoch   4, Step:   212400, Batch Loss:     2.108965, Tokens per Sec:     7408, Lr: 0.000300\n",
            "2021-07-27 08:49:46,501 - INFO - joeynmt.training - Epoch   4, Step:   212600, Batch Loss:     1.785300, Tokens per Sec:     7386, Lr: 0.000300\n",
            "2021-07-27 08:50:45,970 - INFO - joeynmt.training - Epoch   4, Step:   212800, Batch Loss:     1.800308, Tokens per Sec:     7431, Lr: 0.000300\n",
            "2021-07-27 08:51:45,835 - INFO - joeynmt.training - Epoch   4, Step:   213000, Batch Loss:     1.791487, Tokens per Sec:     7428, Lr: 0.000300\n",
            "2021-07-27 08:52:45,073 - INFO - joeynmt.training - Epoch   4, Step:   213200, Batch Loss:     1.988018, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-27 08:53:44,092 - INFO - joeynmt.training - Epoch   4, Step:   213400, Batch Loss:     1.582438, Tokens per Sec:     7309, Lr: 0.000300\n",
            "2021-07-27 08:54:43,482 - INFO - joeynmt.training - Epoch   4, Step:   213600, Batch Loss:     1.846729, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 08:55:43,306 - INFO - joeynmt.training - Epoch   4, Step:   213800, Batch Loss:     1.728272, Tokens per Sec:     7312, Lr: 0.000300\n",
            "2021-07-27 08:56:43,145 - INFO - joeynmt.training - Epoch   4, Step:   214000, Batch Loss:     1.663582, Tokens per Sec:     7348, Lr: 0.000300\n",
            "2021-07-27 08:57:43,026 - INFO - joeynmt.training - Epoch   4, Step:   214200, Batch Loss:     1.779614, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-27 08:58:42,008 - INFO - joeynmt.training - Epoch   4, Step:   214400, Batch Loss:     1.749102, Tokens per Sec:     7311, Lr: 0.000300\n",
            "2021-07-27 08:59:41,710 - INFO - joeynmt.training - Epoch   4, Step:   214600, Batch Loss:     1.813429, Tokens per Sec:     7475, Lr: 0.000300\n",
            "2021-07-27 09:00:40,476 - INFO - joeynmt.training - Epoch   4, Step:   214800, Batch Loss:     1.656312, Tokens per Sec:     7305, Lr: 0.000300\n",
            "2021-07-27 09:01:40,123 - INFO - joeynmt.training - Epoch   4, Step:   215000, Batch Loss:     1.755492, Tokens per Sec:     7396, Lr: 0.000300\n",
            "2021-07-27 09:03:44,090 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 09:03:44,091 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 09:03:44,091 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 09:03:44,882 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 09:03:44,883 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 09:03:45,874 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 09:03:45,875 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 09:03:45,875 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 09:03:45,875 - INFO - joeynmt.training - \tHypothesis: It touched my heart .\n",
            "2021-07-27 09:03:45,876 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 09:03:45,876 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 09:03:45,877 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 09:03:45,877 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-27 09:03:45,877 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 09:03:45,878 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 09:03:45,878 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 09:03:45,878 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 09:03:45,878 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 09:03:45,879 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 09:03:45,879 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 09:03:45,879 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - fed in Satan’s world .\n",
            "2021-07-27 09:03:45,880 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   215000: bleu:  17.11, loss: 129836.5000, ppl:   7.9648, duration: 125.7564s\n",
            "2021-07-27 09:04:45,672 - INFO - joeynmt.training - Epoch   4, Step:   215200, Batch Loss:     1.931944, Tokens per Sec:     7360, Lr: 0.000300\n",
            "2021-07-27 09:05:45,462 - INFO - joeynmt.training - Epoch   4, Step:   215400, Batch Loss:     1.576089, Tokens per Sec:     7312, Lr: 0.000300\n",
            "2021-07-27 09:06:14,827 - INFO - joeynmt.training - Epoch   4: total training loss 9639.80\n",
            "2021-07-27 09:06:14,827 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-27 09:06:45,944 - INFO - joeynmt.training - Epoch   5, Step:   215600, Batch Loss:     1.575140, Tokens per Sec:     7284, Lr: 0.000300\n",
            "2021-07-27 09:07:45,775 - INFO - joeynmt.training - Epoch   5, Step:   215800, Batch Loss:     1.578243, Tokens per Sec:     7350, Lr: 0.000300\n",
            "2021-07-27 09:08:45,371 - INFO - joeynmt.training - Epoch   5, Step:   216000, Batch Loss:     1.676725, Tokens per Sec:     7374, Lr: 0.000300\n",
            "2021-07-27 09:09:44,854 - INFO - joeynmt.training - Epoch   5, Step:   216200, Batch Loss:     1.460312, Tokens per Sec:     7287, Lr: 0.000300\n",
            "2021-07-27 09:10:44,383 - INFO - joeynmt.training - Epoch   5, Step:   216400, Batch Loss:     1.729458, Tokens per Sec:     7419, Lr: 0.000300\n",
            "2021-07-27 09:11:43,820 - INFO - joeynmt.training - Epoch   5, Step:   216600, Batch Loss:     1.929582, Tokens per Sec:     7342, Lr: 0.000300\n",
            "2021-07-27 09:12:43,507 - INFO - joeynmt.training - Epoch   5, Step:   216800, Batch Loss:     1.753846, Tokens per Sec:     7307, Lr: 0.000300\n",
            "2021-07-27 09:13:43,023 - INFO - joeynmt.training - Epoch   5, Step:   217000, Batch Loss:     1.901359, Tokens per Sec:     7339, Lr: 0.000300\n",
            "2021-07-27 09:14:42,743 - INFO - joeynmt.training - Epoch   5, Step:   217200, Batch Loss:     1.793089, Tokens per Sec:     7356, Lr: 0.000300\n",
            "2021-07-27 09:15:42,292 - INFO - joeynmt.training - Epoch   5, Step:   217400, Batch Loss:     1.657058, Tokens per Sec:     7425, Lr: 0.000300\n",
            "2021-07-27 09:16:41,987 - INFO - joeynmt.training - Epoch   5, Step:   217600, Batch Loss:     1.896419, Tokens per Sec:     7318, Lr: 0.000300\n",
            "2021-07-27 09:17:41,591 - INFO - joeynmt.training - Epoch   5, Step:   217800, Batch Loss:     1.756433, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-27 09:18:41,235 - INFO - joeynmt.training - Epoch   5, Step:   218000, Batch Loss:     1.759750, Tokens per Sec:     7382, Lr: 0.000300\n",
            "2021-07-27 09:19:41,180 - INFO - joeynmt.training - Epoch   5, Step:   218200, Batch Loss:     1.730959, Tokens per Sec:     7443, Lr: 0.000300\n",
            "2021-07-27 09:20:40,441 - INFO - joeynmt.training - Epoch   5, Step:   218400, Batch Loss:     1.968227, Tokens per Sec:     7319, Lr: 0.000300\n",
            "2021-07-27 09:21:40,413 - INFO - joeynmt.training - Epoch   5, Step:   218600, Batch Loss:     1.524467, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-27 09:22:39,792 - INFO - joeynmt.training - Epoch   5, Step:   218800, Batch Loss:     1.784103, Tokens per Sec:     7325, Lr: 0.000300\n",
            "2021-07-27 09:23:39,081 - INFO - joeynmt.training - Epoch   5, Step:   219000, Batch Loss:     1.767548, Tokens per Sec:     7347, Lr: 0.000300\n",
            "2021-07-27 09:24:38,784 - INFO - joeynmt.training - Epoch   5, Step:   219200, Batch Loss:     1.881676, Tokens per Sec:     7329, Lr: 0.000300\n",
            "2021-07-27 09:25:38,523 - INFO - joeynmt.training - Epoch   5, Step:   219400, Batch Loss:     1.693210, Tokens per Sec:     7280, Lr: 0.000300\n",
            "2021-07-27 09:26:38,127 - INFO - joeynmt.training - Epoch   5, Step:   219600, Batch Loss:     1.814673, Tokens per Sec:     7468, Lr: 0.000300\n",
            "2021-07-27 09:27:37,918 - INFO - joeynmt.training - Epoch   5, Step:   219800, Batch Loss:     1.612224, Tokens per Sec:     7318, Lr: 0.000300\n",
            "2021-07-27 09:28:37,694 - INFO - joeynmt.training - Epoch   5, Step:   220000, Batch Loss:     1.719705, Tokens per Sec:     7356, Lr: 0.000300\n",
            "2021-07-27 09:30:43,890 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 09:30:43,890 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 09:30:43,891 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 09:30:44,670 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 09:30:44,670 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 09:30:45,545 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 09:30:45,546 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 09:30:45,546 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 09:30:45,547 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-27 09:30:45,547 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 09:30:45,547 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 09:30:45,548 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 09:30:45,548 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-27 09:30:45,548 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 09:30:45,549 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 09:30:45,549 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 09:30:45,549 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 09:30:45,550 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 09:30:45,550 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 09:30:45,550 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 09:30:45,551 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 09:30:45,551 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   220000: bleu:  17.02, loss: 129604.9844, ppl:   7.9353, duration: 127.8564s\n",
            "2021-07-27 09:31:45,905 - INFO - joeynmt.training - Epoch   5, Step:   220200, Batch Loss:     1.505589, Tokens per Sec:     7440, Lr: 0.000300\n",
            "2021-07-27 09:32:45,612 - INFO - joeynmt.training - Epoch   5, Step:   220400, Batch Loss:     1.405033, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-27 09:33:45,310 - INFO - joeynmt.training - Epoch   5, Step:   220600, Batch Loss:     1.791809, Tokens per Sec:     7444, Lr: 0.000300\n",
            "2021-07-27 09:34:44,420 - INFO - joeynmt.training - Epoch   5, Step:   220800, Batch Loss:     1.760163, Tokens per Sec:     7299, Lr: 0.000300\n",
            "2021-07-27 09:35:44,621 - INFO - joeynmt.training - Epoch   5, Step:   221000, Batch Loss:     1.649146, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-27 09:35:47,046 - INFO - joeynmt.training - Epoch   5: total training loss 9604.06\n",
            "2021-07-27 09:35:47,046 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-27 09:36:44,773 - INFO - joeynmt.training - Epoch   6, Step:   221200, Batch Loss:     1.757569, Tokens per Sec:     7183, Lr: 0.000300\n",
            "2021-07-27 09:37:44,156 - INFO - joeynmt.training - Epoch   6, Step:   221400, Batch Loss:     1.506427, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-27 09:38:44,047 - INFO - joeynmt.training - Epoch   6, Step:   221600, Batch Loss:     1.895874, Tokens per Sec:     7473, Lr: 0.000300\n",
            "2021-07-27 09:39:43,399 - INFO - joeynmt.training - Epoch   6, Step:   221800, Batch Loss:     1.631986, Tokens per Sec:     7361, Lr: 0.000300\n",
            "2021-07-27 09:40:43,033 - INFO - joeynmt.training - Epoch   6, Step:   222000, Batch Loss:     1.687908, Tokens per Sec:     7426, Lr: 0.000300\n",
            "2021-07-27 09:41:43,569 - INFO - joeynmt.training - Epoch   6, Step:   222200, Batch Loss:     1.651689, Tokens per Sec:     7502, Lr: 0.000300\n",
            "2021-07-27 09:42:42,702 - INFO - joeynmt.training - Epoch   6, Step:   222400, Batch Loss:     1.723209, Tokens per Sec:     7290, Lr: 0.000300\n",
            "2021-07-27 09:43:42,178 - INFO - joeynmt.training - Epoch   6, Step:   222600, Batch Loss:     1.759075, Tokens per Sec:     7292, Lr: 0.000300\n",
            "2021-07-27 09:44:41,594 - INFO - joeynmt.training - Epoch   6, Step:   222800, Batch Loss:     1.799912, Tokens per Sec:     7350, Lr: 0.000300\n",
            "2021-07-27 09:45:41,362 - INFO - joeynmt.training - Epoch   6, Step:   223000, Batch Loss:     1.794684, Tokens per Sec:     7402, Lr: 0.000300\n",
            "2021-07-27 09:46:40,639 - INFO - joeynmt.training - Epoch   6, Step:   223200, Batch Loss:     1.684317, Tokens per Sec:     7333, Lr: 0.000300\n",
            "2021-07-27 09:47:40,340 - INFO - joeynmt.training - Epoch   6, Step:   223400, Batch Loss:     1.808938, Tokens per Sec:     7361, Lr: 0.000300\n",
            "2021-07-27 09:48:40,164 - INFO - joeynmt.training - Epoch   6, Step:   223600, Batch Loss:     1.449993, Tokens per Sec:     7368, Lr: 0.000300\n",
            "2021-07-27 09:49:40,076 - INFO - joeynmt.training - Epoch   6, Step:   223800, Batch Loss:     1.821400, Tokens per Sec:     7391, Lr: 0.000300\n",
            "2021-07-27 09:50:38,970 - INFO - joeynmt.training - Epoch   6, Step:   224000, Batch Loss:     1.747731, Tokens per Sec:     7316, Lr: 0.000300\n",
            "2021-07-27 09:51:38,102 - INFO - joeynmt.training - Epoch   6, Step:   224200, Batch Loss:     1.847432, Tokens per Sec:     7321, Lr: 0.000300\n",
            "2021-07-27 09:52:37,617 - INFO - joeynmt.training - Epoch   6, Step:   224400, Batch Loss:     1.634765, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-27 09:53:37,073 - INFO - joeynmt.training - Epoch   6, Step:   224600, Batch Loss:     1.683561, Tokens per Sec:     7300, Lr: 0.000300\n",
            "2021-07-27 09:54:36,849 - INFO - joeynmt.training - Epoch   6, Step:   224800, Batch Loss:     1.598307, Tokens per Sec:     7288, Lr: 0.000300\n",
            "2021-07-27 09:55:36,661 - INFO - joeynmt.training - Epoch   6, Step:   225000, Batch Loss:     1.577702, Tokens per Sec:     7386, Lr: 0.000300\n",
            "2021-07-27 09:57:51,180 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 09:57:51,180 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 09:57:51,181 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 09:57:52,844 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 09:57:52,845 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 09:57:52,845 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 09:57:52,845 - INFO - joeynmt.training - \tHypothesis: I was touched by my heart .\n",
            "2021-07-27 09:57:52,846 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 09:57:52,846 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 09:57:52,846 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 09:57:52,847 - INFO - joeynmt.training - \tHypothesis: The text was written on the pillar on the front of the scroll .\n",
            "2021-07-27 09:57:52,847 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 09:57:52,848 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 09:57:52,848 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 09:57:52,848 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 09:57:52,848 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 09:57:52,849 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 09:57:52,849 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 09:57:52,849 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 09:57:52,850 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   225000: bleu:  16.79, loss: 130933.0312, ppl:   8.1056, duration: 136.1879s\n",
            "2021-07-27 09:58:52,908 - INFO - joeynmt.training - Epoch   6, Step:   225200, Batch Loss:     1.662483, Tokens per Sec:     7152, Lr: 0.000300\n",
            "2021-07-27 09:59:51,484 - INFO - joeynmt.training - Epoch   6, Step:   225400, Batch Loss:     1.778279, Tokens per Sec:     7164, Lr: 0.000300\n",
            "2021-07-27 10:00:50,947 - INFO - joeynmt.training - Epoch   6, Step:   225600, Batch Loss:     1.822581, Tokens per Sec:     7359, Lr: 0.000300\n",
            "2021-07-27 10:01:50,709 - INFO - joeynmt.training - Epoch   6, Step:   225800, Batch Loss:     1.648052, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-27 10:02:50,291 - INFO - joeynmt.training - Epoch   6, Step:   226000, Batch Loss:     1.887317, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-27 10:03:49,494 - INFO - joeynmt.training - Epoch   6, Step:   226200, Batch Loss:     1.633501, Tokens per Sec:     7368, Lr: 0.000300\n",
            "2021-07-27 10:04:49,050 - INFO - joeynmt.training - Epoch   6, Step:   226400, Batch Loss:     1.748916, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-27 10:05:30,880 - INFO - joeynmt.training - Epoch   6: total training loss 9633.10\n",
            "2021-07-27 10:05:30,881 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-07-27 10:05:49,592 - INFO - joeynmt.training - Epoch   7, Step:   226600, Batch Loss:     1.866821, Tokens per Sec:     7182, Lr: 0.000300\n",
            "2021-07-27 10:06:49,019 - INFO - joeynmt.training - Epoch   7, Step:   226800, Batch Loss:     1.614941, Tokens per Sec:     7302, Lr: 0.000300\n",
            "2021-07-27 10:07:48,378 - INFO - joeynmt.training - Epoch   7, Step:   227000, Batch Loss:     1.772539, Tokens per Sec:     7300, Lr: 0.000300\n",
            "2021-07-27 10:08:47,327 - INFO - joeynmt.training - Epoch   7, Step:   227200, Batch Loss:     1.892946, Tokens per Sec:     7303, Lr: 0.000300\n",
            "2021-07-27 10:09:46,709 - INFO - joeynmt.training - Epoch   7, Step:   227400, Batch Loss:     1.709744, Tokens per Sec:     7307, Lr: 0.000300\n",
            "2021-07-27 10:10:46,473 - INFO - joeynmt.training - Epoch   7, Step:   227600, Batch Loss:     1.787736, Tokens per Sec:     7440, Lr: 0.000300\n",
            "2021-07-27 10:11:46,050 - INFO - joeynmt.training - Epoch   7, Step:   227800, Batch Loss:     1.607877, Tokens per Sec:     7293, Lr: 0.000300\n",
            "2021-07-27 10:12:45,909 - INFO - joeynmt.training - Epoch   7, Step:   228000, Batch Loss:     1.798090, Tokens per Sec:     7361, Lr: 0.000300\n",
            "2021-07-27 10:13:45,413 - INFO - joeynmt.training - Epoch   7, Step:   228200, Batch Loss:     1.754939, Tokens per Sec:     7316, Lr: 0.000300\n",
            "2021-07-27 10:14:45,457 - INFO - joeynmt.training - Epoch   7, Step:   228400, Batch Loss:     1.648834, Tokens per Sec:     7392, Lr: 0.000300\n",
            "2021-07-27 10:15:44,994 - INFO - joeynmt.training - Epoch   7, Step:   228600, Batch Loss:     2.012470, Tokens per Sec:     7350, Lr: 0.000300\n",
            "2021-07-27 10:16:43,967 - INFO - joeynmt.training - Epoch   7, Step:   228800, Batch Loss:     1.668679, Tokens per Sec:     7292, Lr: 0.000300\n",
            "2021-07-27 10:17:43,643 - INFO - joeynmt.training - Epoch   7, Step:   229000, Batch Loss:     1.605762, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-27 10:18:42,789 - INFO - joeynmt.training - Epoch   7, Step:   229200, Batch Loss:     1.728021, Tokens per Sec:     7300, Lr: 0.000300\n",
            "2021-07-27 10:19:43,099 - INFO - joeynmt.training - Epoch   7, Step:   229400, Batch Loss:     1.605198, Tokens per Sec:     7458, Lr: 0.000300\n",
            "2021-07-27 10:20:42,547 - INFO - joeynmt.training - Epoch   7, Step:   229600, Batch Loss:     1.783401, Tokens per Sec:     7323, Lr: 0.000300\n",
            "2021-07-27 10:21:42,149 - INFO - joeynmt.training - Epoch   7, Step:   229800, Batch Loss:     1.506819, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-27 10:22:42,781 - INFO - joeynmt.training - Epoch   7, Step:   230000, Batch Loss:     1.768245, Tokens per Sec:     7467, Lr: 0.000300\n",
            "2021-07-27 10:24:49,279 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 10:24:49,280 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 10:24:49,280 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 10:24:50,091 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 10:24:50,092 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 10:24:51,009 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 10:24:51,010 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 10:24:51,011 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 10:24:51,011 - INFO - joeynmt.training - \tHypothesis: It touched my heart .\n",
            "2021-07-27 10:24:51,011 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 10:24:51,012 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 10:24:51,012 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 10:24:51,012 - INFO - joeynmt.training - \tHypothesis: The text was written on the pillar on the front of the scroll .\n",
            "2021-07-27 10:24:51,013 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 10:24:51,013 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 10:24:51,013 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 10:24:51,014 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 10:24:51,014 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 10:24:51,015 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 10:24:51,015 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 10:24:51,016 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - fed in Satan’s world .\n",
            "2021-07-27 10:24:51,016 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   230000: bleu:  17.14, loss: 129407.4531, ppl:   7.9103, duration: 128.2340s\n",
            "2021-07-27 10:25:51,111 - INFO - joeynmt.training - Epoch   7, Step:   230200, Batch Loss:     2.089833, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-27 10:26:50,942 - INFO - joeynmt.training - Epoch   7, Step:   230400, Batch Loss:     1.668520, Tokens per Sec:     7367, Lr: 0.000300\n",
            "2021-07-27 10:27:50,637 - INFO - joeynmt.training - Epoch   7, Step:   230600, Batch Loss:     1.860477, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 10:28:50,041 - INFO - joeynmt.training - Epoch   7, Step:   230800, Batch Loss:     1.642730, Tokens per Sec:     7313, Lr: 0.000300\n",
            "2021-07-27 10:29:49,507 - INFO - joeynmt.training - Epoch   7, Step:   231000, Batch Loss:     2.153321, Tokens per Sec:     7224, Lr: 0.000300\n",
            "2021-07-27 10:30:48,826 - INFO - joeynmt.training - Epoch   7, Step:   231200, Batch Loss:     2.013118, Tokens per Sec:     7288, Lr: 0.000300\n",
            "2021-07-27 10:31:48,779 - INFO - joeynmt.training - Epoch   7, Step:   231400, Batch Loss:     1.536421, Tokens per Sec:     7316, Lr: 0.000300\n",
            "2021-07-27 10:32:48,669 - INFO - joeynmt.training - Epoch   7, Step:   231600, Batch Loss:     1.619973, Tokens per Sec:     7511, Lr: 0.000300\n",
            "2021-07-27 10:33:48,556 - INFO - joeynmt.training - Epoch   7, Step:   231800, Batch Loss:     1.651934, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 10:34:48,270 - INFO - joeynmt.training - Epoch   7, Step:   232000, Batch Loss:     1.654671, Tokens per Sec:     7382, Lr: 0.000300\n",
            "2021-07-27 10:35:05,896 - INFO - joeynmt.training - Epoch   7: total training loss 9591.29\n",
            "2021-07-27 10:35:05,897 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-07-27 10:35:48,558 - INFO - joeynmt.training - Epoch   8, Step:   232200, Batch Loss:     1.798230, Tokens per Sec:     7248, Lr: 0.000300\n",
            "2021-07-27 10:36:47,988 - INFO - joeynmt.training - Epoch   8, Step:   232400, Batch Loss:     1.630971, Tokens per Sec:     7411, Lr: 0.000300\n",
            "2021-07-27 10:37:47,346 - INFO - joeynmt.training - Epoch   8, Step:   232600, Batch Loss:     1.832439, Tokens per Sec:     7340, Lr: 0.000300\n",
            "2021-07-27 10:38:47,357 - INFO - joeynmt.training - Epoch   8, Step:   232800, Batch Loss:     1.923594, Tokens per Sec:     7447, Lr: 0.000300\n",
            "2021-07-27 10:39:47,176 - INFO - joeynmt.training - Epoch   8, Step:   233000, Batch Loss:     1.813856, Tokens per Sec:     7306, Lr: 0.000300\n",
            "2021-07-27 10:40:47,287 - INFO - joeynmt.training - Epoch   8, Step:   233200, Batch Loss:     1.755780, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-27 10:41:46,562 - INFO - joeynmt.training - Epoch   8, Step:   233400, Batch Loss:     1.726310, Tokens per Sec:     7370, Lr: 0.000300\n",
            "2021-07-27 10:42:45,913 - INFO - joeynmt.training - Epoch   8, Step:   233600, Batch Loss:     1.860828, Tokens per Sec:     7297, Lr: 0.000300\n",
            "2021-07-27 10:43:45,422 - INFO - joeynmt.training - Epoch   8, Step:   233800, Batch Loss:     1.778532, Tokens per Sec:     7380, Lr: 0.000300\n",
            "2021-07-27 10:44:44,873 - INFO - joeynmt.training - Epoch   8, Step:   234000, Batch Loss:     1.823846, Tokens per Sec:     7384, Lr: 0.000300\n",
            "2021-07-27 10:45:44,957 - INFO - joeynmt.training - Epoch   8, Step:   234200, Batch Loss:     1.689679, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-27 10:46:44,686 - INFO - joeynmt.training - Epoch   8, Step:   234400, Batch Loss:     1.667113, Tokens per Sec:     7465, Lr: 0.000300\n",
            "2021-07-27 10:47:44,744 - INFO - joeynmt.training - Epoch   8, Step:   234600, Batch Loss:     1.498961, Tokens per Sec:     7368, Lr: 0.000300\n",
            "2021-07-27 10:48:44,168 - INFO - joeynmt.training - Epoch   8, Step:   234800, Batch Loss:     1.855392, Tokens per Sec:     7318, Lr: 0.000300\n",
            "2021-07-27 10:49:43,854 - INFO - joeynmt.training - Epoch   8, Step:   235000, Batch Loss:     1.804178, Tokens per Sec:     7426, Lr: 0.000300\n",
            "2021-07-27 10:51:48,073 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 10:51:48,073 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 10:51:48,074 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 10:51:48,859 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 10:51:48,859 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 10:51:50,054 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 10:51:50,055 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 10:51:50,055 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 10:51:50,055 - INFO - joeynmt.training - \tHypothesis: I was touched by my heart .\n",
            "2021-07-27 10:51:50,056 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 10:51:50,056 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 10:51:50,057 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 10:51:50,057 - INFO - joeynmt.training - \tHypothesis: The text that was written in the pillar on the front of the scroll .\n",
            "2021-07-27 10:51:50,057 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 10:51:50,058 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 10:51:50,058 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 10:51:50,058 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 10:51:50,058 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 10:51:50,059 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 10:51:50,059 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 10:51:50,060 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of self - esteem , that they were well - fed in Satan’s world .\n",
            "2021-07-27 10:51:50,060 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   235000: bleu:  17.49, loss: 129238.6484, ppl:   7.8890, duration: 126.2058s\n",
            "2021-07-27 10:52:50,178 - INFO - joeynmt.training - Epoch   8, Step:   235200, Batch Loss:     1.949664, Tokens per Sec:     7374, Lr: 0.000300\n",
            "2021-07-27 10:53:50,207 - INFO - joeynmt.training - Epoch   8, Step:   235400, Batch Loss:     1.745397, Tokens per Sec:     7432, Lr: 0.000300\n",
            "2021-07-27 10:54:49,303 - INFO - joeynmt.training - Epoch   8, Step:   235600, Batch Loss:     1.678450, Tokens per Sec:     7300, Lr: 0.000300\n",
            "2021-07-27 10:55:48,532 - INFO - joeynmt.training - Epoch   8, Step:   235800, Batch Loss:     1.357060, Tokens per Sec:     7317, Lr: 0.000300\n",
            "2021-07-27 10:56:47,850 - INFO - joeynmt.training - Epoch   8, Step:   236000, Batch Loss:     1.664070, Tokens per Sec:     7312, Lr: 0.000300\n",
            "2021-07-27 10:57:47,426 - INFO - joeynmt.training - Epoch   8, Step:   236200, Batch Loss:     2.051109, Tokens per Sec:     7355, Lr: 0.000300\n",
            "2021-07-27 10:58:47,105 - INFO - joeynmt.training - Epoch   8, Step:   236400, Batch Loss:     2.040661, Tokens per Sec:     7259, Lr: 0.000300\n",
            "2021-07-27 10:59:46,287 - INFO - joeynmt.training - Epoch   8, Step:   236600, Batch Loss:     1.677287, Tokens per Sec:     7249, Lr: 0.000300\n",
            "2021-07-27 11:00:45,679 - INFO - joeynmt.training - Epoch   8, Step:   236800, Batch Loss:     1.792905, Tokens per Sec:     7343, Lr: 0.000300\n",
            "2021-07-27 11:01:45,062 - INFO - joeynmt.training - Epoch   8, Step:   237000, Batch Loss:     1.982257, Tokens per Sec:     7332, Lr: 0.000300\n",
            "2021-07-27 11:02:44,316 - INFO - joeynmt.training - Epoch   8, Step:   237200, Batch Loss:     1.455729, Tokens per Sec:     7306, Lr: 0.000300\n",
            "2021-07-27 11:03:44,008 - INFO - joeynmt.training - Epoch   8, Step:   237400, Batch Loss:     1.548548, Tokens per Sec:     7372, Lr: 0.000300\n",
            "2021-07-27 11:04:38,256 - INFO - joeynmt.training - Epoch   8: total training loss 9581.85\n",
            "2021-07-27 11:04:38,256 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-07-27 11:04:44,145 - INFO - joeynmt.training - Epoch   9, Step:   237600, Batch Loss:     1.758396, Tokens per Sec:     6552, Lr: 0.000300\n",
            "2021-07-27 11:05:43,735 - INFO - joeynmt.training - Epoch   9, Step:   237800, Batch Loss:     1.800435, Tokens per Sec:     7308, Lr: 0.000300\n",
            "2021-07-27 11:06:43,713 - INFO - joeynmt.training - Epoch   9, Step:   238000, Batch Loss:     1.794180, Tokens per Sec:     7415, Lr: 0.000300\n",
            "2021-07-27 11:07:43,285 - INFO - joeynmt.training - Epoch   9, Step:   238200, Batch Loss:     1.795753, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-27 11:08:43,099 - INFO - joeynmt.training - Epoch   9, Step:   238400, Batch Loss:     1.775076, Tokens per Sec:     7396, Lr: 0.000300\n",
            "2021-07-27 11:09:42,825 - INFO - joeynmt.training - Epoch   9, Step:   238600, Batch Loss:     1.730956, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-27 11:10:42,838 - INFO - joeynmt.training - Epoch   9, Step:   238800, Batch Loss:     1.749455, Tokens per Sec:     7428, Lr: 0.000300\n",
            "2021-07-27 11:11:41,926 - INFO - joeynmt.training - Epoch   9, Step:   239000, Batch Loss:     1.783136, Tokens per Sec:     7180, Lr: 0.000300\n",
            "2021-07-27 11:12:41,218 - INFO - joeynmt.training - Epoch   9, Step:   239200, Batch Loss:     1.396324, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-27 11:13:40,735 - INFO - joeynmt.training - Epoch   9, Step:   239400, Batch Loss:     1.848605, Tokens per Sec:     7308, Lr: 0.000300\n",
            "2021-07-27 11:14:40,615 - INFO - joeynmt.training - Epoch   9, Step:   239600, Batch Loss:     1.819603, Tokens per Sec:     7437, Lr: 0.000300\n",
            "2021-07-27 11:15:39,918 - INFO - joeynmt.training - Epoch   9, Step:   239800, Batch Loss:     1.595382, Tokens per Sec:     7272, Lr: 0.000300\n",
            "2021-07-27 11:16:39,417 - INFO - joeynmt.training - Epoch   9, Step:   240000, Batch Loss:     1.789073, Tokens per Sec:     7396, Lr: 0.000300\n",
            "2021-07-27 11:18:42,126 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 11:18:42,127 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 11:18:42,127 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 11:18:42,934 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 11:18:42,934 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 11:18:43,787 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 11:18:43,789 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 11:18:43,789 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 11:18:43,789 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-27 11:18:43,790 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 11:18:43,790 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 11:18:43,790 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 11:18:43,791 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the side of the scroll .\n",
            "2021-07-27 11:18:43,791 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 11:18:43,792 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 11:18:43,792 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 11:18:43,792 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 11:18:43,792 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 11:18:43,793 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 11:18:43,793 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 11:18:43,793 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 11:18:43,794 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   240000: bleu:  17.55, loss: 129037.0000, ppl:   7.8636, duration: 124.3763s\n",
            "2021-07-27 11:19:43,571 - INFO - joeynmt.training - Epoch   9, Step:   240200, Batch Loss:     1.762435, Tokens per Sec:     7383, Lr: 0.000300\n",
            "2021-07-27 11:20:43,073 - INFO - joeynmt.training - Epoch   9, Step:   240400, Batch Loss:     1.515514, Tokens per Sec:     7416, Lr: 0.000300\n",
            "2021-07-27 11:21:42,941 - INFO - joeynmt.training - Epoch   9, Step:   240600, Batch Loss:     1.659633, Tokens per Sec:     7469, Lr: 0.000300\n",
            "2021-07-27 11:22:42,244 - INFO - joeynmt.training - Epoch   9, Step:   240800, Batch Loss:     1.747498, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 11:23:41,633 - INFO - joeynmt.training - Epoch   9, Step:   241000, Batch Loss:     1.741526, Tokens per Sec:     7332, Lr: 0.000300\n",
            "2021-07-27 11:24:41,470 - INFO - joeynmt.training - Epoch   9, Step:   241200, Batch Loss:     1.868485, Tokens per Sec:     7471, Lr: 0.000300\n",
            "2021-07-27 11:25:40,807 - INFO - joeynmt.training - Epoch   9, Step:   241400, Batch Loss:     1.791206, Tokens per Sec:     7334, Lr: 0.000300\n",
            "2021-07-27 11:26:40,436 - INFO - joeynmt.training - Epoch   9, Step:   241600, Batch Loss:     1.765373, Tokens per Sec:     7364, Lr: 0.000300\n",
            "2021-07-27 11:27:39,471 - INFO - joeynmt.training - Epoch   9, Step:   241800, Batch Loss:     1.749313, Tokens per Sec:     7305, Lr: 0.000300\n",
            "2021-07-27 11:28:39,066 - INFO - joeynmt.training - Epoch   9, Step:   242000, Batch Loss:     1.619834, Tokens per Sec:     7354, Lr: 0.000300\n",
            "2021-07-27 11:29:38,951 - INFO - joeynmt.training - Epoch   9, Step:   242200, Batch Loss:     1.649299, Tokens per Sec:     7378, Lr: 0.000300\n",
            "2021-07-27 11:30:38,539 - INFO - joeynmt.training - Epoch   9, Step:   242400, Batch Loss:     1.745867, Tokens per Sec:     7324, Lr: 0.000300\n",
            "2021-07-27 11:31:38,039 - INFO - joeynmt.training - Epoch   9, Step:   242600, Batch Loss:     1.621410, Tokens per Sec:     7323, Lr: 0.000300\n",
            "2021-07-27 11:32:37,305 - INFO - joeynmt.training - Epoch   9, Step:   242800, Batch Loss:     1.645674, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-27 11:33:36,801 - INFO - joeynmt.training - Epoch   9, Step:   243000, Batch Loss:     1.644088, Tokens per Sec:     7336, Lr: 0.000300\n",
            "2021-07-27 11:34:08,253 - INFO - joeynmt.training - Epoch   9: total training loss 9557.38\n",
            "2021-07-27 11:34:08,253 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-07-27 11:34:37,129 - INFO - joeynmt.training - Epoch  10, Step:   243200, Batch Loss:     1.799425, Tokens per Sec:     7298, Lr: 0.000300\n",
            "2021-07-27 11:35:36,660 - INFO - joeynmt.training - Epoch  10, Step:   243400, Batch Loss:     1.975082, Tokens per Sec:     7258, Lr: 0.000300\n",
            "2021-07-27 11:36:36,028 - INFO - joeynmt.training - Epoch  10, Step:   243600, Batch Loss:     1.679241, Tokens per Sec:     7302, Lr: 0.000300\n",
            "2021-07-27 11:37:35,298 - INFO - joeynmt.training - Epoch  10, Step:   243800, Batch Loss:     1.695079, Tokens per Sec:     7300, Lr: 0.000300\n",
            "2021-07-27 11:38:34,503 - INFO - joeynmt.training - Epoch  10, Step:   244000, Batch Loss:     1.442287, Tokens per Sec:     7299, Lr: 0.000300\n",
            "2021-07-27 11:39:34,433 - INFO - joeynmt.training - Epoch  10, Step:   244200, Batch Loss:     1.728356, Tokens per Sec:     7376, Lr: 0.000300\n",
            "2021-07-27 11:40:34,503 - INFO - joeynmt.training - Epoch  10, Step:   244400, Batch Loss:     1.769806, Tokens per Sec:     7440, Lr: 0.000300\n",
            "2021-07-27 11:41:34,273 - INFO - joeynmt.training - Epoch  10, Step:   244600, Batch Loss:     1.353639, Tokens per Sec:     7394, Lr: 0.000300\n",
            "2021-07-27 11:42:33,275 - INFO - joeynmt.training - Epoch  10, Step:   244800, Batch Loss:     1.825713, Tokens per Sec:     7334, Lr: 0.000300\n",
            "2021-07-27 11:43:32,738 - INFO - joeynmt.training - Epoch  10, Step:   245000, Batch Loss:     1.964955, Tokens per Sec:     7357, Lr: 0.000300\n",
            "2021-07-27 11:45:39,159 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 11:45:39,160 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 11:45:39,160 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 11:45:41,155 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 11:45:41,157 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 11:45:41,158 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 11:45:41,158 - INFO - joeynmt.training - \tHypothesis: I was touched by my heart .\n",
            "2021-07-27 11:45:41,159 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 11:45:41,159 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 11:45:41,159 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 11:45:41,160 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-27 11:45:41,160 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 11:45:41,160 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 11:45:41,161 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 11:45:41,161 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 11:45:41,161 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 11:45:41,162 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 11:45:41,162 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 11:45:41,162 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 11:45:41,163 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   245000: bleu:  17.23, loss: 129407.1172, ppl:   7.9103, duration: 128.4239s\n",
            "2021-07-27 11:46:40,861 - INFO - joeynmt.training - Epoch  10, Step:   245200, Batch Loss:     1.536332, Tokens per Sec:     7424, Lr: 0.000300\n",
            "2021-07-27 11:47:40,571 - INFO - joeynmt.training - Epoch  10, Step:   245400, Batch Loss:     1.752309, Tokens per Sec:     7420, Lr: 0.000300\n",
            "2021-07-27 11:48:39,670 - INFO - joeynmt.training - Epoch  10, Step:   245600, Batch Loss:     1.821879, Tokens per Sec:     7336, Lr: 0.000300\n",
            "2021-07-27 11:49:38,970 - INFO - joeynmt.training - Epoch  10, Step:   245800, Batch Loss:     1.684332, Tokens per Sec:     7447, Lr: 0.000300\n",
            "2021-07-27 11:50:38,597 - INFO - joeynmt.training - Epoch  10, Step:   246000, Batch Loss:     1.910125, Tokens per Sec:     7380, Lr: 0.000300\n",
            "2021-07-27 11:51:38,360 - INFO - joeynmt.training - Epoch  10, Step:   246200, Batch Loss:     1.570080, Tokens per Sec:     7428, Lr: 0.000300\n",
            "2021-07-27 11:52:37,897 - INFO - joeynmt.training - Epoch  10, Step:   246400, Batch Loss:     1.623828, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 11:53:37,250 - INFO - joeynmt.training - Epoch  10, Step:   246600, Batch Loss:     1.931522, Tokens per Sec:     7353, Lr: 0.000300\n",
            "2021-07-27 11:54:37,259 - INFO - joeynmt.training - Epoch  10, Step:   246800, Batch Loss:     1.735490, Tokens per Sec:     7350, Lr: 0.000300\n",
            "2021-07-27 11:55:36,944 - INFO - joeynmt.training - Epoch  10, Step:   247000, Batch Loss:     1.874705, Tokens per Sec:     7347, Lr: 0.000300\n",
            "2021-07-27 11:56:36,446 - INFO - joeynmt.training - Epoch  10, Step:   247200, Batch Loss:     1.633875, Tokens per Sec:     7253, Lr: 0.000300\n",
            "2021-07-27 11:57:36,277 - INFO - joeynmt.training - Epoch  10, Step:   247400, Batch Loss:     2.093265, Tokens per Sec:     7417, Lr: 0.000300\n",
            "2021-07-27 11:58:35,956 - INFO - joeynmt.training - Epoch  10, Step:   247600, Batch Loss:     1.838148, Tokens per Sec:     7281, Lr: 0.000300\n",
            "2021-07-27 11:59:35,825 - INFO - joeynmt.training - Epoch  10, Step:   247800, Batch Loss:     1.799107, Tokens per Sec:     7442, Lr: 0.000300\n",
            "2021-07-27 12:00:35,085 - INFO - joeynmt.training - Epoch  10, Step:   248000, Batch Loss:     1.753310, Tokens per Sec:     7326, Lr: 0.000300\n",
            "2021-07-27 12:01:34,669 - INFO - joeynmt.training - Epoch  10, Step:   248200, Batch Loss:     1.537279, Tokens per Sec:     7331, Lr: 0.000300\n",
            "2021-07-27 12:02:34,575 - INFO - joeynmt.training - Epoch  10, Step:   248400, Batch Loss:     1.794699, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-27 12:03:34,025 - INFO - joeynmt.training - Epoch  10, Step:   248600, Batch Loss:     1.836351, Tokens per Sec:     7315, Lr: 0.000300\n",
            "2021-07-27 12:03:42,000 - INFO - joeynmt.training - Epoch  10: total training loss 9538.56\n",
            "2021-07-27 12:03:42,001 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-07-27 12:04:34,865 - INFO - joeynmt.training - Epoch  11, Step:   248800, Batch Loss:     1.666779, Tokens per Sec:     7291, Lr: 0.000300\n",
            "2021-07-27 12:05:34,534 - INFO - joeynmt.training - Epoch  11, Step:   249000, Batch Loss:     1.780802, Tokens per Sec:     7359, Lr: 0.000300\n",
            "2021-07-27 12:06:34,456 - INFO - joeynmt.training - Epoch  11, Step:   249200, Batch Loss:     1.900493, Tokens per Sec:     7383, Lr: 0.000300\n",
            "2021-07-27 12:07:33,986 - INFO - joeynmt.training - Epoch  11, Step:   249400, Batch Loss:     1.725823, Tokens per Sec:     7417, Lr: 0.000300\n",
            "2021-07-27 12:08:33,387 - INFO - joeynmt.training - Epoch  11, Step:   249600, Batch Loss:     1.766992, Tokens per Sec:     7400, Lr: 0.000300\n",
            "2021-07-27 12:09:32,812 - INFO - joeynmt.training - Epoch  11, Step:   249800, Batch Loss:     1.720067, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 12:10:32,684 - INFO - joeynmt.training - Epoch  11, Step:   250000, Batch Loss:     1.694481, Tokens per Sec:     7390, Lr: 0.000300\n",
            "2021-07-27 12:12:40,373 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 12:12:40,373 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 12:12:40,373 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 12:12:41,177 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 12:12:41,177 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 12:12:42,100 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 12:12:42,101 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 12:12:42,101 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 12:12:42,101 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-27 12:12:42,101 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 12:12:42,102 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 12:12:42,102 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 12:12:42,103 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-27 12:12:42,103 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 12:12:42,104 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 12:12:42,104 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 12:12:42,104 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 12:12:42,104 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 12:12:42,105 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 12:12:42,105 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 12:12:42,106 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 12:12:42,106 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   250000: bleu:  17.15, loss: 128110.3438, ppl:   7.7480, duration: 129.4213s\n",
            "2021-07-27 12:13:41,509 - INFO - joeynmt.training - Epoch  11, Step:   250200, Batch Loss:     1.732971, Tokens per Sec:     7317, Lr: 0.000300\n",
            "2021-07-27 12:14:41,245 - INFO - joeynmt.training - Epoch  11, Step:   250400, Batch Loss:     1.725557, Tokens per Sec:     7310, Lr: 0.000300\n",
            "2021-07-27 12:15:40,725 - INFO - joeynmt.training - Epoch  11, Step:   250600, Batch Loss:     1.852909, Tokens per Sec:     7326, Lr: 0.000300\n",
            "2021-07-27 12:16:40,222 - INFO - joeynmt.training - Epoch  11, Step:   250800, Batch Loss:     1.616509, Tokens per Sec:     7300, Lr: 0.000300\n",
            "2021-07-27 12:17:40,039 - INFO - joeynmt.training - Epoch  11, Step:   251000, Batch Loss:     1.582770, Tokens per Sec:     7403, Lr: 0.000300\n",
            "2021-07-27 12:18:39,892 - INFO - joeynmt.training - Epoch  11, Step:   251200, Batch Loss:     1.678333, Tokens per Sec:     7314, Lr: 0.000300\n",
            "2021-07-27 12:19:38,757 - INFO - joeynmt.training - Epoch  11, Step:   251400, Batch Loss:     1.688607, Tokens per Sec:     7288, Lr: 0.000300\n",
            "2021-07-27 12:20:38,163 - INFO - joeynmt.training - Epoch  11, Step:   251600, Batch Loss:     1.533026, Tokens per Sec:     7486, Lr: 0.000300\n",
            "2021-07-27 12:21:37,207 - INFO - joeynmt.training - Epoch  11, Step:   251800, Batch Loss:     1.721301, Tokens per Sec:     7356, Lr: 0.000300\n",
            "2021-07-27 12:22:36,759 - INFO - joeynmt.training - Epoch  11, Step:   252000, Batch Loss:     1.712855, Tokens per Sec:     7330, Lr: 0.000300\n",
            "2021-07-27 12:23:35,848 - INFO - joeynmt.training - Epoch  11, Step:   252200, Batch Loss:     1.775072, Tokens per Sec:     7278, Lr: 0.000300\n",
            "2021-07-27 12:24:35,414 - INFO - joeynmt.training - Epoch  11, Step:   252400, Batch Loss:     1.814155, Tokens per Sec:     7354, Lr: 0.000300\n",
            "2021-07-27 12:25:34,864 - INFO - joeynmt.training - Epoch  11, Step:   252600, Batch Loss:     1.475339, Tokens per Sec:     7372, Lr: 0.000300\n",
            "2021-07-27 12:26:34,722 - INFO - joeynmt.training - Epoch  11, Step:   252800, Batch Loss:     1.557186, Tokens per Sec:     7399, Lr: 0.000300\n",
            "2021-07-27 12:27:34,532 - INFO - joeynmt.training - Epoch  11, Step:   253000, Batch Loss:     1.932534, Tokens per Sec:     7384, Lr: 0.000300\n",
            "2021-07-27 12:28:33,757 - INFO - joeynmt.training - Epoch  11, Step:   253200, Batch Loss:     1.746152, Tokens per Sec:     7337, Lr: 0.000300\n",
            "2021-07-27 12:29:33,256 - INFO - joeynmt.training - Epoch  11, Step:   253400, Batch Loss:     1.474716, Tokens per Sec:     7326, Lr: 0.000300\n",
            "2021-07-27 12:30:32,606 - INFO - joeynmt.training - Epoch  11, Step:   253600, Batch Loss:     1.469872, Tokens per Sec:     7316, Lr: 0.000300\n",
            "2021-07-27 12:31:32,012 - INFO - joeynmt.training - Epoch  11, Step:   253800, Batch Loss:     1.872144, Tokens per Sec:     7376, Lr: 0.000300\n",
            "2021-07-27 12:32:31,310 - INFO - joeynmt.training - Epoch  11, Step:   254000, Batch Loss:     1.493994, Tokens per Sec:     7319, Lr: 0.000300\n",
            "2021-07-27 12:33:18,199 - INFO - joeynmt.training - Epoch  11: total training loss 9546.00\n",
            "2021-07-27 12:33:18,199 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-07-27 12:33:31,817 - INFO - joeynmt.training - Epoch  12, Step:   254200, Batch Loss:     1.548718, Tokens per Sec:     7046, Lr: 0.000300\n",
            "2021-07-27 12:34:31,592 - INFO - joeynmt.training - Epoch  12, Step:   254400, Batch Loss:     1.731753, Tokens per Sec:     7439, Lr: 0.000300\n",
            "2021-07-27 12:35:31,341 - INFO - joeynmt.training - Epoch  12, Step:   254600, Batch Loss:     1.695620, Tokens per Sec:     7399, Lr: 0.000300\n",
            "2021-07-27 12:36:30,556 - INFO - joeynmt.training - Epoch  12, Step:   254800, Batch Loss:     1.621569, Tokens per Sec:     7298, Lr: 0.000300\n",
            "2021-07-27 12:37:30,234 - INFO - joeynmt.training - Epoch  12, Step:   255000, Batch Loss:     1.812721, Tokens per Sec:     7347, Lr: 0.000300\n",
            "2021-07-27 12:39:35,137 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 12:39:35,138 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 12:39:35,138 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 12:39:36,703 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 12:39:36,704 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 12:39:36,704 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 12:39:36,705 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-27 12:39:36,705 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 12:39:36,706 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 12:39:36,706 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 12:39:36,706 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-27 12:39:36,707 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 12:39:36,707 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 12:39:36,708 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 12:39:36,708 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 12:39:36,708 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 12:39:36,709 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 12:39:36,709 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 12:39:36,710 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 12:39:36,710 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   255000: bleu:  17.44, loss: 129147.0859, ppl:   7.8775, duration: 126.4751s\n",
            "2021-07-27 12:40:36,255 - INFO - joeynmt.training - Epoch  12, Step:   255200, Batch Loss:     1.606055, Tokens per Sec:     7255, Lr: 0.000300\n",
            "2021-07-27 12:41:35,658 - INFO - joeynmt.training - Epoch  12, Step:   255400, Batch Loss:     1.415977, Tokens per Sec:     7370, Lr: 0.000300\n",
            "2021-07-27 12:42:35,152 - INFO - joeynmt.training - Epoch  12, Step:   255600, Batch Loss:     1.793195, Tokens per Sec:     7366, Lr: 0.000300\n",
            "2021-07-27 12:43:35,310 - INFO - joeynmt.training - Epoch  12, Step:   255800, Batch Loss:     1.742263, Tokens per Sec:     7493, Lr: 0.000300\n",
            "2021-07-27 12:44:34,407 - INFO - joeynmt.training - Epoch  12, Step:   256000, Batch Loss:     1.880186, Tokens per Sec:     7248, Lr: 0.000300\n",
            "2021-07-27 12:45:33,764 - INFO - joeynmt.training - Epoch  12, Step:   256200, Batch Loss:     1.823844, Tokens per Sec:     7337, Lr: 0.000300\n",
            "2021-07-27 12:46:32,942 - INFO - joeynmt.training - Epoch  12, Step:   256400, Batch Loss:     1.624085, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-27 12:47:32,770 - INFO - joeynmt.training - Epoch  12, Step:   256600, Batch Loss:     1.587859, Tokens per Sec:     7409, Lr: 0.000300\n",
            "2021-07-27 12:48:31,835 - INFO - joeynmt.training - Epoch  12, Step:   256800, Batch Loss:     2.165812, Tokens per Sec:     7297, Lr: 0.000300\n",
            "2021-07-27 12:49:31,803 - INFO - joeynmt.training - Epoch  12, Step:   257000, Batch Loss:     1.556032, Tokens per Sec:     7401, Lr: 0.000300\n",
            "2021-07-27 12:50:31,043 - INFO - joeynmt.training - Epoch  12, Step:   257200, Batch Loss:     1.904993, Tokens per Sec:     7288, Lr: 0.000300\n",
            "2021-07-27 12:51:30,919 - INFO - joeynmt.training - Epoch  12, Step:   257400, Batch Loss:     1.647556, Tokens per Sec:     7428, Lr: 0.000300\n",
            "2021-07-27 12:52:30,697 - INFO - joeynmt.training - Epoch  12, Step:   257600, Batch Loss:     1.718086, Tokens per Sec:     7388, Lr: 0.000300\n",
            "2021-07-27 12:53:30,586 - INFO - joeynmt.training - Epoch  12, Step:   257800, Batch Loss:     1.570154, Tokens per Sec:     7405, Lr: 0.000300\n",
            "2021-07-27 12:54:30,560 - INFO - joeynmt.training - Epoch  12, Step:   258000, Batch Loss:     1.731519, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-27 12:55:30,294 - INFO - joeynmt.training - Epoch  12, Step:   258200, Batch Loss:     1.743611, Tokens per Sec:     7407, Lr: 0.000300\n",
            "2021-07-27 12:56:30,011 - INFO - joeynmt.training - Epoch  12, Step:   258400, Batch Loss:     1.575261, Tokens per Sec:     7305, Lr: 0.000300\n",
            "2021-07-27 12:57:29,159 - INFO - joeynmt.training - Epoch  12, Step:   258600, Batch Loss:     1.837949, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-27 12:58:28,180 - INFO - joeynmt.training - Epoch  12, Step:   258800, Batch Loss:     1.727904, Tokens per Sec:     7306, Lr: 0.000300\n",
            "2021-07-27 12:59:26,810 - INFO - joeynmt.training - Epoch  12, Step:   259000, Batch Loss:     1.921337, Tokens per Sec:     7240, Lr: 0.000300\n",
            "2021-07-27 13:00:26,311 - INFO - joeynmt.training - Epoch  12, Step:   259200, Batch Loss:     1.772929, Tokens per Sec:     7273, Lr: 0.000300\n",
            "2021-07-27 13:01:26,064 - INFO - joeynmt.training - Epoch  12, Step:   259400, Batch Loss:     1.810865, Tokens per Sec:     7386, Lr: 0.000300\n",
            "2021-07-27 13:02:25,658 - INFO - joeynmt.training - Epoch  12, Step:   259600, Batch Loss:     1.731538, Tokens per Sec:     7348, Lr: 0.000300\n",
            "2021-07-27 13:02:50,928 - INFO - joeynmt.training - Epoch  12: total training loss 9531.38\n",
            "2021-07-27 13:02:50,928 - INFO - joeynmt.training - EPOCH 13\n",
            "2021-07-27 13:03:25,087 - INFO - joeynmt.training - Epoch  13, Step:   259800, Batch Loss:     1.510109, Tokens per Sec:     7157, Lr: 0.000300\n",
            "2021-07-27 13:04:24,757 - INFO - joeynmt.training - Epoch  13, Step:   260000, Batch Loss:     1.651525, Tokens per Sec:     7428, Lr: 0.000300\n",
            "2021-07-27 13:06:25,134 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 13:06:25,135 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 13:06:25,135 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 13:06:25,932 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 13:06:25,933 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 13:06:26,811 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 13:06:26,811 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 13:06:26,812 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 13:06:26,812 - INFO - joeynmt.training - \tHypothesis: It touched my heart .\n",
            "2021-07-27 13:06:26,812 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 13:06:26,813 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 13:06:26,813 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 13:06:26,813 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-27 13:06:26,814 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 13:06:26,814 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 13:06:26,814 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 13:06:26,815 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 13:06:26,815 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 13:06:26,816 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 13:06:26,816 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 13:06:26,816 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 13:06:26,816 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   260000: bleu:  17.84, loss: 128101.3438, ppl:   7.7469, duration: 122.0584s\n",
            "2021-07-27 13:07:26,533 - INFO - joeynmt.training - Epoch  13, Step:   260200, Batch Loss:     1.659618, Tokens per Sec:     7303, Lr: 0.000300\n",
            "2021-07-27 13:08:26,165 - INFO - joeynmt.training - Epoch  13, Step:   260400, Batch Loss:     1.577634, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-27 13:09:25,852 - INFO - joeynmt.training - Epoch  13, Step:   260600, Batch Loss:     1.834485, Tokens per Sec:     7344, Lr: 0.000300\n",
            "2021-07-27 13:10:25,965 - INFO - joeynmt.training - Epoch  13, Step:   260800, Batch Loss:     1.782424, Tokens per Sec:     7369, Lr: 0.000300\n",
            "2021-07-27 13:11:25,457 - INFO - joeynmt.training - Epoch  13, Step:   261000, Batch Loss:     1.670895, Tokens per Sec:     7334, Lr: 0.000300\n",
            "2021-07-27 13:12:24,528 - INFO - joeynmt.training - Epoch  13, Step:   261200, Batch Loss:     1.612512, Tokens per Sec:     7252, Lr: 0.000300\n",
            "2021-07-27 13:13:24,191 - INFO - joeynmt.training - Epoch  13, Step:   261400, Batch Loss:     1.766158, Tokens per Sec:     7342, Lr: 0.000300\n",
            "2021-07-27 13:14:23,576 - INFO - joeynmt.training - Epoch  13, Step:   261600, Batch Loss:     1.741527, Tokens per Sec:     7425, Lr: 0.000300\n",
            "2021-07-27 13:15:22,964 - INFO - joeynmt.training - Epoch  13, Step:   261800, Batch Loss:     1.666690, Tokens per Sec:     7284, Lr: 0.000300\n",
            "2021-07-27 13:16:22,979 - INFO - joeynmt.training - Epoch  13, Step:   262000, Batch Loss:     1.753212, Tokens per Sec:     7431, Lr: 0.000300\n",
            "2021-07-27 13:17:22,836 - INFO - joeynmt.training - Epoch  13, Step:   262200, Batch Loss:     1.727823, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-27 13:18:22,742 - INFO - joeynmt.training - Epoch  13, Step:   262400, Batch Loss:     1.747302, Tokens per Sec:     7404, Lr: 0.000300\n",
            "2021-07-27 13:19:22,149 - INFO - joeynmt.training - Epoch  13, Step:   262600, Batch Loss:     1.544575, Tokens per Sec:     7361, Lr: 0.000300\n",
            "2021-07-27 13:20:21,707 - INFO - joeynmt.training - Epoch  13, Step:   262800, Batch Loss:     1.835164, Tokens per Sec:     7299, Lr: 0.000300\n",
            "2021-07-27 13:21:20,967 - INFO - joeynmt.training - Epoch  13, Step:   263000, Batch Loss:     1.795326, Tokens per Sec:     7287, Lr: 0.000300\n",
            "2021-07-27 13:22:20,354 - INFO - joeynmt.training - Epoch  13, Step:   263200, Batch Loss:     1.712967, Tokens per Sec:     7287, Lr: 0.000300\n",
            "2021-07-27 13:23:20,021 - INFO - joeynmt.training - Epoch  13, Step:   263400, Batch Loss:     1.547176, Tokens per Sec:     7426, Lr: 0.000300\n",
            "2021-07-27 13:24:19,248 - INFO - joeynmt.training - Epoch  13, Step:   263600, Batch Loss:     1.719755, Tokens per Sec:     7267, Lr: 0.000300\n",
            "2021-07-27 13:25:19,165 - INFO - joeynmt.training - Epoch  13, Step:   263800, Batch Loss:     1.639001, Tokens per Sec:     7351, Lr: 0.000300\n",
            "2021-07-27 13:26:18,245 - INFO - joeynmt.training - Epoch  13, Step:   264000, Batch Loss:     1.604157, Tokens per Sec:     7209, Lr: 0.000300\n",
            "2021-07-27 13:27:17,853 - INFO - joeynmt.training - Epoch  13, Step:   264200, Batch Loss:     1.804229, Tokens per Sec:     7408, Lr: 0.000300\n",
            "2021-07-27 13:28:17,787 - INFO - joeynmt.training - Epoch  13, Step:   264400, Batch Loss:     1.723299, Tokens per Sec:     7405, Lr: 0.000300\n",
            "2021-07-27 13:29:17,015 - INFO - joeynmt.training - Epoch  13, Step:   264600, Batch Loss:     1.659145, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-27 13:30:16,946 - INFO - joeynmt.training - Epoch  13, Step:   264800, Batch Loss:     1.833423, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-27 13:31:16,265 - INFO - joeynmt.training - Epoch  13, Step:   265000, Batch Loss:     1.682738, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-27 13:33:19,197 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 13:33:19,197 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 13:33:19,197 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 13:33:20,797 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 13:33:20,797 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 13:33:20,798 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 13:33:20,798 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-27 13:33:20,798 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 13:33:20,799 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 13:33:20,799 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 13:33:20,799 - INFO - joeynmt.training - \tHypothesis: The text was written on the pillar on the front of the scroll .\n",
            "2021-07-27 13:33:20,799 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 13:33:20,800 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 13:33:20,800 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 13:33:20,801 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 13:33:20,801 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 13:33:20,802 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 13:33:20,802 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 13:33:20,803 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 13:33:20,803 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   265000: bleu:  17.73, loss: 128147.6328, ppl:   7.7527, duration: 124.5377s\n",
            "2021-07-27 13:34:20,267 - INFO - joeynmt.training - Epoch  13, Step:   265200, Batch Loss:     2.002526, Tokens per Sec:     7364, Lr: 0.000300\n",
            "2021-07-27 13:34:24,108 - INFO - joeynmt.training - Epoch  13: total training loss 9508.21\n",
            "2021-07-27 13:34:24,109 - INFO - joeynmt.training - EPOCH 14\n",
            "2021-07-27 13:35:20,791 - INFO - joeynmt.training - Epoch  14, Step:   265400, Batch Loss:     1.606407, Tokens per Sec:     7233, Lr: 0.000300\n",
            "2021-07-27 13:36:20,548 - INFO - joeynmt.training - Epoch  14, Step:   265600, Batch Loss:     1.777408, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-27 13:37:19,951 - INFO - joeynmt.training - Epoch  14, Step:   265800, Batch Loss:     1.866420, Tokens per Sec:     7385, Lr: 0.000300\n",
            "2021-07-27 13:38:18,983 - INFO - joeynmt.training - Epoch  14, Step:   266000, Batch Loss:     1.746116, Tokens per Sec:     7320, Lr: 0.000300\n",
            "2021-07-27 13:39:18,527 - INFO - joeynmt.training - Epoch  14, Step:   266200, Batch Loss:     1.482144, Tokens per Sec:     7356, Lr: 0.000300\n",
            "2021-07-27 13:40:18,384 - INFO - joeynmt.training - Epoch  14, Step:   266400, Batch Loss:     1.618631, Tokens per Sec:     7467, Lr: 0.000300\n",
            "2021-07-27 13:41:18,254 - INFO - joeynmt.training - Epoch  14, Step:   266600, Batch Loss:     1.769602, Tokens per Sec:     7431, Lr: 0.000300\n",
            "2021-07-27 13:42:17,419 - INFO - joeynmt.training - Epoch  14, Step:   266800, Batch Loss:     1.829515, Tokens per Sec:     7279, Lr: 0.000300\n",
            "2021-07-27 13:43:16,698 - INFO - joeynmt.training - Epoch  14, Step:   267000, Batch Loss:     1.741186, Tokens per Sec:     7391, Lr: 0.000300\n",
            "2021-07-27 13:44:16,482 - INFO - joeynmt.training - Epoch  14, Step:   267200, Batch Loss:     1.564789, Tokens per Sec:     7511, Lr: 0.000300\n",
            "2021-07-27 13:45:15,399 - INFO - joeynmt.training - Epoch  14, Step:   267400, Batch Loss:     1.750578, Tokens per Sec:     7271, Lr: 0.000300\n",
            "2021-07-27 13:46:14,310 - INFO - joeynmt.training - Epoch  14, Step:   267600, Batch Loss:     1.730702, Tokens per Sec:     7371, Lr: 0.000300\n",
            "2021-07-27 13:47:14,102 - INFO - joeynmt.training - Epoch  14, Step:   267800, Batch Loss:     1.584824, Tokens per Sec:     7452, Lr: 0.000300\n",
            "2021-07-27 13:48:13,691 - INFO - joeynmt.training - Epoch  14, Step:   268000, Batch Loss:     1.988706, Tokens per Sec:     7334, Lr: 0.000300\n",
            "2021-07-27 13:49:13,370 - INFO - joeynmt.training - Epoch  14, Step:   268200, Batch Loss:     1.949111, Tokens per Sec:     7307, Lr: 0.000300\n",
            "2021-07-27 13:50:13,184 - INFO - joeynmt.training - Epoch  14, Step:   268400, Batch Loss:     1.671420, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-27 13:51:13,050 - INFO - joeynmt.training - Epoch  14, Step:   268600, Batch Loss:     1.692814, Tokens per Sec:     7389, Lr: 0.000300\n",
            "2021-07-27 13:52:12,831 - INFO - joeynmt.training - Epoch  14, Step:   268800, Batch Loss:     1.551066, Tokens per Sec:     7341, Lr: 0.000300\n",
            "2021-07-27 13:53:12,233 - INFO - joeynmt.training - Epoch  14, Step:   269000, Batch Loss:     1.712283, Tokens per Sec:     7418, Lr: 0.000300\n",
            "2021-07-27 13:54:11,710 - INFO - joeynmt.training - Epoch  14, Step:   269200, Batch Loss:     1.554008, Tokens per Sec:     7365, Lr: 0.000300\n",
            "2021-07-27 13:55:10,977 - INFO - joeynmt.training - Epoch  14, Step:   269400, Batch Loss:     1.683288, Tokens per Sec:     7331, Lr: 0.000300\n",
            "2021-07-27 13:56:10,151 - INFO - joeynmt.training - Epoch  14, Step:   269600, Batch Loss:     1.509282, Tokens per Sec:     7352, Lr: 0.000300\n",
            "2021-07-27 13:57:09,631 - INFO - joeynmt.training - Epoch  14, Step:   269800, Batch Loss:     1.585805, Tokens per Sec:     7347, Lr: 0.000300\n",
            "2021-07-27 13:58:09,136 - INFO - joeynmt.training - Epoch  14, Step:   270000, Batch Loss:     1.781834, Tokens per Sec:     7457, Lr: 0.000300\n",
            "2021-07-27 14:00:10,530 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 14:00:10,531 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 14:00:10,531 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 14:00:12,521 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 14:00:12,522 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 14:00:12,522 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 14:00:12,522 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-27 14:00:12,523 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 14:00:12,523 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 14:00:12,524 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 14:00:12,524 - INFO - joeynmt.training - \tHypothesis: The text was written in the pillar on the front of the scroll .\n",
            "2021-07-27 14:00:12,524 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 14:00:12,525 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 14:00:12,525 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 14:00:12,525 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 14:00:12,525 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 14:00:12,526 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 14:00:12,526 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 14:00:12,526 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-27 14:00:12,527 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   270000: bleu:  17.84, loss: 128196.7812, ppl:   7.7587, duration: 123.3905s\n",
            "2021-07-27 14:01:12,176 - INFO - joeynmt.training - Epoch  14, Step:   270200, Batch Loss:     1.483223, Tokens per Sec:     7351, Lr: 0.000300\n",
            "2021-07-27 14:02:11,800 - INFO - joeynmt.training - Epoch  14, Step:   270400, Batch Loss:     1.821882, Tokens per Sec:     7375, Lr: 0.000300\n",
            "2021-07-27 14:03:10,570 - INFO - joeynmt.training - Epoch  14, Step:   270600, Batch Loss:     1.779205, Tokens per Sec:     7256, Lr: 0.000300\n",
            "2021-07-27 14:03:51,527 - INFO - joeynmt.training - Epoch  14: total training loss 9492.06\n",
            "2021-07-27 14:03:51,527 - INFO - joeynmt.training - EPOCH 15\n",
            "2021-07-27 14:04:10,144 - INFO - joeynmt.training - Epoch  15, Step:   270800, Batch Loss:     1.556544, Tokens per Sec:     7038, Lr: 0.000300\n",
            "2021-07-27 14:05:09,218 - INFO - joeynmt.training - Epoch  15, Step:   271000, Batch Loss:     1.548910, Tokens per Sec:     7296, Lr: 0.000300\n",
            "2021-07-27 14:06:08,983 - INFO - joeynmt.training - Epoch  15, Step:   271200, Batch Loss:     1.481209, Tokens per Sec:     7344, Lr: 0.000300\n",
            "2021-07-27 14:07:07,807 - INFO - joeynmt.training - Epoch  15, Step:   271400, Batch Loss:     1.680350, Tokens per Sec:     7216, Lr: 0.000300\n",
            "2021-07-27 14:08:07,562 - INFO - joeynmt.training - Epoch  15, Step:   271600, Batch Loss:     1.613516, Tokens per Sec:     7467, Lr: 0.000300\n",
            "2021-07-27 14:09:06,914 - INFO - joeynmt.training - Epoch  15, Step:   271800, Batch Loss:     1.734013, Tokens per Sec:     7343, Lr: 0.000300\n",
            "2021-07-27 14:10:06,122 - INFO - joeynmt.training - Epoch  15, Step:   272000, Batch Loss:     1.771798, Tokens per Sec:     7303, Lr: 0.000300\n",
            "2021-07-27 14:11:05,846 - INFO - joeynmt.training - Epoch  15, Step:   272200, Batch Loss:     1.873907, Tokens per Sec:     7349, Lr: 0.000300\n",
            "2021-07-27 14:12:05,188 - INFO - joeynmt.training - Epoch  15, Step:   272400, Batch Loss:     1.704728, Tokens per Sec:     7310, Lr: 0.000300\n",
            "2021-07-27 14:13:04,922 - INFO - joeynmt.training - Epoch  15, Step:   272600, Batch Loss:     1.796082, Tokens per Sec:     7358, Lr: 0.000300\n",
            "2021-07-27 14:14:04,507 - INFO - joeynmt.training - Epoch  15, Step:   272800, Batch Loss:     1.577561, Tokens per Sec:     7401, Lr: 0.000300\n",
            "2021-07-27 14:15:04,390 - INFO - joeynmt.training - Epoch  15, Step:   273000, Batch Loss:     1.665510, Tokens per Sec:     7415, Lr: 0.000300\n",
            "2021-07-27 14:16:04,296 - INFO - joeynmt.training - Epoch  15, Step:   273200, Batch Loss:     1.681870, Tokens per Sec:     7367, Lr: 0.000300\n",
            "2021-07-27 14:17:03,116 - INFO - joeynmt.training - Epoch  15, Step:   273400, Batch Loss:     1.832364, Tokens per Sec:     7311, Lr: 0.000300\n",
            "2021-07-27 14:18:02,792 - INFO - joeynmt.training - Epoch  15, Step:   273600, Batch Loss:     1.674450, Tokens per Sec:     7404, Lr: 0.000300\n",
            "2021-07-27 14:19:02,306 - INFO - joeynmt.training - Epoch  15, Step:   273800, Batch Loss:     1.573451, Tokens per Sec:     7396, Lr: 0.000300\n",
            "2021-07-27 14:20:01,544 - INFO - joeynmt.training - Epoch  15, Step:   274000, Batch Loss:     1.751478, Tokens per Sec:     7267, Lr: 0.000300\n",
            "2021-07-27 14:21:00,614 - INFO - joeynmt.training - Epoch  15, Step:   274200, Batch Loss:     2.015821, Tokens per Sec:     7355, Lr: 0.000300\n",
            "2021-07-27 14:22:00,420 - INFO - joeynmt.training - Epoch  15, Step:   274400, Batch Loss:     1.516717, Tokens per Sec:     7393, Lr: 0.000300\n",
            "2021-07-27 14:23:00,091 - INFO - joeynmt.training - Epoch  15, Step:   274600, Batch Loss:     1.835007, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-27 14:23:59,397 - INFO - joeynmt.training - Epoch  15, Step:   274800, Batch Loss:     1.711133, Tokens per Sec:     7414, Lr: 0.000300\n",
            "2021-07-27 14:24:58,867 - INFO - joeynmt.training - Epoch  15, Step:   275000, Batch Loss:     1.568529, Tokens per Sec:     7307, Lr: 0.000300\n",
            "2021-07-27 14:26:57,397 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 14:26:57,398 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 14:26:57,398 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 14:26:58,200 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-27 14:26:58,200 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-27 14:26:59,402 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 14:26:59,403 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 14:26:59,403 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 14:26:59,403 - INFO - joeynmt.training - \tHypothesis: It touched my heart .\n",
            "2021-07-27 14:26:59,404 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 14:26:59,404 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 14:26:59,404 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 14:26:59,405 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-27 14:26:59,405 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 14:26:59,406 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 14:26:59,406 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 14:26:59,406 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 14:26:59,406 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 14:26:59,407 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 14:26:59,407 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 14:26:59,407 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - established in Satan’s world .\n",
            "2021-07-27 14:26:59,408 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step   275000: bleu:  17.94, loss: 127496.0781, ppl:   7.6723, duration: 120.5403s\n",
            "2021-07-27 14:27:58,859 - INFO - joeynmt.training - Epoch  15, Step:   275200, Batch Loss:     1.731951, Tokens per Sec:     7385, Lr: 0.000300\n",
            "2021-07-27 14:28:58,130 - INFO - joeynmt.training - Epoch  15, Step:   275400, Batch Loss:     1.783329, Tokens per Sec:     7343, Lr: 0.000300\n",
            "2021-07-27 14:29:57,638 - INFO - joeynmt.training - Epoch  15, Step:   275600, Batch Loss:     1.824640, Tokens per Sec:     7330, Lr: 0.000300\n",
            "2021-07-27 14:30:57,475 - INFO - joeynmt.training - Epoch  15, Step:   275800, Batch Loss:     1.688529, Tokens per Sec:     7384, Lr: 0.000300\n",
            "2021-07-27 14:31:57,100 - INFO - joeynmt.training - Epoch  15, Step:   276000, Batch Loss:     1.572968, Tokens per Sec:     7357, Lr: 0.000300\n",
            "2021-07-27 14:32:56,827 - INFO - joeynmt.training - Epoch  15, Step:   276200, Batch Loss:     1.815039, Tokens per Sec:     7406, Lr: 0.000300\n",
            "2021-07-27 14:33:18,502 - INFO - joeynmt.training - Epoch  15: total training loss 9490.73\n",
            "2021-07-27 14:33:18,502 - INFO - joeynmt.training - EPOCH 16\n",
            "2021-07-27 14:33:57,099 - INFO - joeynmt.training - Epoch  16, Step:   276400, Batch Loss:     1.856884, Tokens per Sec:     7269, Lr: 0.000300\n",
            "2021-07-27 14:34:56,948 - INFO - joeynmt.training - Epoch  16, Step:   276600, Batch Loss:     1.750551, Tokens per Sec:     7292, Lr: 0.000300\n",
            "2021-07-27 14:35:57,053 - INFO - joeynmt.training - Epoch  16, Step:   276800, Batch Loss:     1.550011, Tokens per Sec:     7362, Lr: 0.000300\n",
            "2021-07-27 14:36:56,796 - INFO - joeynmt.training - Epoch  16, Step:   277000, Batch Loss:     1.761091, Tokens per Sec:     7336, Lr: 0.000300\n",
            "2021-07-27 14:37:56,685 - INFO - joeynmt.training - Epoch  16, Step:   277200, Batch Loss:     1.715978, Tokens per Sec:     7434, Lr: 0.000300\n",
            "2021-07-27 14:38:55,907 - INFO - joeynmt.training - Epoch  16, Step:   277400, Batch Loss:     1.540520, Tokens per Sec:     7339, Lr: 0.000300\n",
            "2021-07-27 14:39:55,293 - INFO - joeynmt.training - Epoch  16, Step:   277600, Batch Loss:     1.703279, Tokens per Sec:     7413, Lr: 0.000300\n",
            "2021-07-27 14:40:55,143 - INFO - joeynmt.training - Epoch  16, Step:   277800, Batch Loss:     1.671552, Tokens per Sec:     7406, Lr: 0.000300\n",
            "2021-07-27 14:41:54,219 - INFO - joeynmt.training - Epoch  16, Step:   278000, Batch Loss:     1.776474, Tokens per Sec:     7305, Lr: 0.000300\n",
            "2021-07-27 14:42:53,695 - INFO - joeynmt.training - Epoch  16, Step:   278200, Batch Loss:     1.913251, Tokens per Sec:     7398, Lr: 0.000300\n",
            "2021-07-27 14:43:53,229 - INFO - joeynmt.training - Epoch  16, Step:   278400, Batch Loss:     1.805213, Tokens per Sec:     7358, Lr: 0.000300\n",
            "2021-07-27 14:44:52,289 - INFO - joeynmt.training - Epoch  16, Step:   278600, Batch Loss:     1.616085, Tokens per Sec:     7344, Lr: 0.000300\n",
            "2021-07-27 14:45:51,747 - INFO - joeynmt.training - Epoch  16, Step:   278800, Batch Loss:     1.847233, Tokens per Sec:     7339, Lr: 0.000300\n",
            "2021-07-27 14:46:51,494 - INFO - joeynmt.training - Epoch  16, Step:   279000, Batch Loss:     1.914682, Tokens per Sec:     7283, Lr: 0.000300\n",
            "2021-07-27 14:47:51,026 - INFO - joeynmt.training - Epoch  16, Step:   279200, Batch Loss:     1.565696, Tokens per Sec:     7369, Lr: 0.000300\n",
            "2021-07-27 14:48:50,338 - INFO - joeynmt.training - Epoch  16, Step:   279400, Batch Loss:     1.780100, Tokens per Sec:     7281, Lr: 0.000300\n",
            "2021-07-27 14:49:50,352 - INFO - joeynmt.training - Epoch  16, Step:   279600, Batch Loss:     1.822695, Tokens per Sec:     7340, Lr: 0.000300\n",
            "2021-07-27 14:50:49,714 - INFO - joeynmt.training - Epoch  16, Step:   279800, Batch Loss:     1.750699, Tokens per Sec:     7322, Lr: 0.000300\n",
            "2021-07-27 14:51:49,264 - INFO - joeynmt.training - Epoch  16, Step:   280000, Batch Loss:     1.682894, Tokens per Sec:     7332, Lr: 0.000300\n",
            "2021-07-27 14:53:54,293 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-27 14:53:54,293 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-27 14:53:54,294 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-27 14:53:55,956 - INFO - joeynmt.training - Example #0\n",
            "2021-07-27 14:53:55,957 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-27 14:53:55,957 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-27 14:53:55,958 - INFO - joeynmt.training - \tHypothesis: I was touched by my heart .\n",
            "2021-07-27 14:53:55,958 - INFO - joeynmt.training - Example #1\n",
            "2021-07-27 14:53:55,958 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-27 14:53:55,959 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-27 14:53:55,959 - INFO - joeynmt.training - \tHypothesis: The text was written on the pillar on the side of the scroll .\n",
            "2021-07-27 14:53:55,959 - INFO - joeynmt.training - Example #2\n",
            "2021-07-27 14:53:55,960 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-27 14:53:55,960 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 14:53:55,960 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-27 14:53:55,961 - INFO - joeynmt.training - Example #3\n",
            "2021-07-27 14:53:55,961 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-27 14:53:55,962 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-27 14:53:55,962 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of kindness in Satan’s world .\n",
            "2021-07-27 14:53:55,962 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   280000: bleu:  18.00, loss: 127810.5156, ppl:   7.7110, duration: 126.6973s\n",
            "2021-07-27 14:54:56,259 - INFO - joeynmt.training - Epoch  16, Step:   280200, Batch Loss:     1.600286, Tokens per Sec:     7336, Lr: 0.000300\n",
            "2021-07-27 14:55:55,588 - INFO - joeynmt.training - Epoch  16, Step:   280400, Batch Loss:     1.596781, Tokens per Sec:     7360, Lr: 0.000300\n",
            "2021-07-27 14:56:55,099 - INFO - joeynmt.training - Epoch  16, Step:   280600, Batch Loss:     1.802138, Tokens per Sec:     7363, Lr: 0.000300\n",
            "2021-07-27 14:57:54,742 - INFO - joeynmt.training - Epoch  16, Step:   280800, Batch Loss:     1.637462, Tokens per Sec:     7381, Lr: 0.000300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mdOZ1V8z8-u"
      },
      "source": [
        "# Reloading configuration file\n",
        "ckpt_number = 280000\n",
        "reload_config = config.replace(\n",
        "    f'#load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/models/rw_lhen_transformer/1.ckpt\"', \n",
        "    f'load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued3/{ckpt_number}.ckpt\"').replace(\n",
        "        f'model_dir: \"models/rw_lhen_reverse_transformer\"', f'model_dir: \"models/rw_lhen_reverse_transformer_continued4\"').replace(\n",
        "            f'epochs: 30', f'epochs: 6')\n",
        "        \n",
        "with open(\"joeynmt/configs/transformer_{name}_reload4.yaml\".format(name=name),'w') as f:\n",
        "    f.write(reload_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4NNGj8TM4N0A",
        "outputId": "d1b8f178-cae0-4be0-d61c-ec889f135f3f"
      },
      "source": [
        "!cat \"joeynmt/configs/transformer_{name}_reload4.yaml\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "name: \"rw_lhen_reverse_transformer\"\n",
            "\n",
            "data:\n",
            "    src: \"rw_lh\"\n",
            "    trg: \"en\"\n",
            "    train: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\"\n",
            "    dev:   \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\"\n",
            "    test:  \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\"\n",
            "    level: \"bpe\"\n",
            "    lowercase: False\n",
            "    max_sent_length: 100\n",
            "    src_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
            "    trg_vocab: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\"\n",
            "\n",
            "testing:\n",
            "    beam_size: 5\n",
            "    alpha: 1.0\n",
            "\n",
            "training:\n",
            "    load_model: \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued3/280000.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
            "    random_seed: 42\n",
            "    optimizer: \"adam\"\n",
            "    normalization: \"tokens\"\n",
            "    adam_betas: [0.9, 0.999] \n",
            "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
            "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
            "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
            "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
            "    decrease_factor: 0.7\n",
            "    loss: \"crossentropy\"\n",
            "    learning_rate: 0.0003\n",
            "    learning_rate_min: 0.00000001\n",
            "    weight_decay: 0.0\n",
            "    label_smoothing: 0.1\n",
            "    batch_size: 4096\n",
            "    batch_type: \"token\"\n",
            "    eval_batch_size: 1000\n",
            "    eval_batch_type: \"token\"\n",
            "    batch_multiplier: 1\n",
            "    early_stopping_metric: \"ppl\"\n",
            "    epochs: 6                  # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
            "    validation_freq: 5000         # TODO: Set to at least once per epoch.\n",
            "    logging_freq: 200\n",
            "    eval_metric: \"bleu\"\n",
            "    model_dir: \"models/rw_lhen_reverse_transformer_continued4\"\n",
            "    overwrite: True \n",
            "    shuffle: True\n",
            "    use_cuda: True\n",
            "    max_output_length: 100\n",
            "    print_valid_sents: [0, 1, 2, 3]\n",
            "    keep_last_ckpts: 3\n",
            "\n",
            "model:\n",
            "    initializer: \"xavier\"\n",
            "    bias_initializer: \"zeros\"\n",
            "    init_gain: 1.0\n",
            "    embed_initializer: \"xavier\"\n",
            "    embed_init_gain: 1.0\n",
            "    tied_embeddings: True\n",
            "    tied_softmax: True\n",
            "    encoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n",
            "    decoder:\n",
            "        type: \"transformer\"\n",
            "        num_layers: 6\n",
            "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
            "        embeddings:\n",
            "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
            "            scale: True\n",
            "            dropout: 0.2\n",
            "        # typically ff_size = 4 x hidden_size\n",
            "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
            "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
            "        dropout: 0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Icw7DaXznLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1b0a54-e9ef-4480-c29a-9b55aad2b327"
      },
      "source": [
        "# Train continued\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_rw_lhen_reload4.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 06:51:52,562 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 06:51:52,651 - INFO - joeynmt.data - Loading training data...\n",
            "2021-07-28 06:52:05,063 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-07-28 06:52:05,912 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-07-28 06:52:06,937 - INFO - joeynmt.data - Loading test data...\n",
            "2021-07-28 06:52:08,269 - INFO - joeynmt.data - Data loaded.\n",
            "2021-07-28 06:52:08,270 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 06:52:08,697 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 06:52:08.959827: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-28 06:52:11,276 - INFO - joeynmt.training - Total params: 12177920\n",
            "2021-07-28 06:52:20,009 - INFO - joeynmt.training - Loading model from /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued3/280000.ckpt\n",
            "2021-07-28 06:52:24,361 - INFO - joeynmt.helpers - cfg.name                           : rw_lhen_reverse_transformer\n",
            "2021-07-28 06:52:24,361 - INFO - joeynmt.helpers - cfg.data.src                       : rw_lh\n",
            "2021-07-28 06:52:24,361 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n",
            "2021-07-28 06:52:24,362 - INFO - joeynmt.helpers - cfg.data.train                     : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/train.bpe\n",
            "2021-07-28 06:52:24,362 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe\n",
            "2021-07-28 06:52:24,362 - INFO - joeynmt.helpers - cfg.data.test                      : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe\n",
            "2021-07-28 06:52:24,362 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-07-28 06:52:24,362 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-07-28 06:52:24,363 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-07-28 06:52:24,363 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-28 06:52:24,363 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/vocab.txt\n",
            "2021-07-28 06:52:24,363 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-07-28 06:52:24,364 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-07-28 06:52:24,364 - INFO - joeynmt.helpers - cfg.training.load_model            : /content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/joeynmt/models/rw_lhen_reverse_transformer_continued3/280000.ckpt\n",
            "2021-07-28 06:52:24,364 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-07-28 06:52:24,364 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-07-28 06:52:24,364 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-07-28 06:52:24,365 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-07-28 06:52:24,365 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-07-28 06:52:24,365 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-07-28 06:52:24,365 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-07-28 06:52:24,366 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-07-28 06:52:24,366 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-07-28 06:52:24,366 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-07-28 06:52:24,366 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-07-28 06:52:24,366 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-07-28 06:52:24,367 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-07-28 06:52:24,367 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-07-28 06:52:24,367 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-07-28 06:52:24,367 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-07-28 06:52:24,368 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 1000\n",
            "2021-07-28 06:52:24,368 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-07-28 06:52:24,368 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-07-28 06:52:24,368 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-07-28 06:52:24,369 - INFO - joeynmt.helpers - cfg.training.epochs                : 6\n",
            "2021-07-28 06:52:24,369 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000\n",
            "2021-07-28 06:52:24,369 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200\n",
            "2021-07-28 06:52:24,370 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-07-28 06:52:24,370 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/rw_lhen_reverse_transformer_continued4\n",
            "2021-07-28 06:52:24,370 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-07-28 06:52:24,370 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-07-28 06:52:24,371 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-07-28 06:52:24,371 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-07-28 06:52:24,371 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-07-28 06:52:24,371 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-07-28 06:52:24,372 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-07-28 06:52:24,372 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-07-28 06:52:24,372 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-07-28 06:52:24,372 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-07-28 06:52:24,373 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-07-28 06:52:24,373 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-07-28 06:52:24,373 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-07-28 06:52:24,374 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-07-28 06:52:24,374 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-07-28 06:52:24,374 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-07-28 06:52:24,374 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 06:52:24,375 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-07-28 06:52:24,375 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-07-28 06:52:24,375 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-07-28 06:52:24,377 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-07-28 06:52:24,377 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-07-28 06:52:24,377 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-07-28 06:52:24,378 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-07-28 06:52:24,378 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-07-28 06:52:24,378 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-07-28 06:52:24,378 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-07-28 06:52:24,379 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-07-28 06:52:24,379 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-07-28 06:52:24,379 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-07-28 06:52:24,379 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-07-28 06:52:24,380 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 439996,\n",
            "\tvalid 2000,\n",
            "\ttest 1000\n",
            "2021-07-28 06:52:24,380 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] A@@ shobora kuba y@@ arag@@ iye ahantu hat@@ uje mu B@@ ut@@ ay@@ u bwa S@@ ir@@ iya cyangwa se wenda ku Mw@@ ig@@ im@@ bak@@ irwa wa Ar@@ ab@@ iya , uri mu bur@@ as@@ ir@@ azuba bw’@@ I@@ ny@@ anja I@@ tuk@@ ura , kugira ngo h@@ am@@ ufash@@ e gutekereza .\n",
            "\t[TRG] R@@ ight after his bapt@@ ism , he “ went off into Ar@@ ab@@ ia ” ​ — e@@ ither the S@@ y@@ ri@@ an D@@ es@@ ert or pos@@ sib@@ ly some qu@@ i@@ et place on the Ar@@ ab@@ ian P@@ en@@ ins@@ ula that was conduc@@ ive to med@@ it@@ ation .\n",
            "2021-07-28 06:52:24,380 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-28 06:52:24,381 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) to (8) of (9) mu\n",
            "2021-07-28 06:52:24,381 - INFO - joeynmt.helpers - Number of Src words (types): 4366\n",
            "2021-07-28 06:52:24,381 - INFO - joeynmt.helpers - Number of Trg words (types): 4366\n",
            "2021-07-28 06:52:24,381 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4366),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4366))\n",
            "2021-07-28 06:52:24,395 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-07-28 06:52:24,396 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-07-28 06:53:25,462 - INFO - joeynmt.training - Epoch   1, Step:   280200, Batch Loss:     1.599370, Tokens per Sec:     7243, Lr: 0.000300\n",
            "2021-07-28 06:54:23,994 - INFO - joeynmt.training - Epoch   1, Step:   280400, Batch Loss:     1.595670, Tokens per Sec:     7460, Lr: 0.000300\n",
            "2021-07-28 06:55:22,801 - INFO - joeynmt.training - Epoch   1, Step:   280600, Batch Loss:     1.799242, Tokens per Sec:     7451, Lr: 0.000300\n",
            "2021-07-28 06:56:21,615 - INFO - joeynmt.training - Epoch   1, Step:   280800, Batch Loss:     1.638370, Tokens per Sec:     7485, Lr: 0.000300\n",
            "2021-07-28 06:57:20,172 - INFO - joeynmt.training - Epoch   1, Step:   281000, Batch Loss:     1.632854, Tokens per Sec:     7395, Lr: 0.000300\n",
            "2021-07-28 06:58:18,875 - INFO - joeynmt.training - Epoch   1, Step:   281200, Batch Loss:     1.859528, Tokens per Sec:     7465, Lr: 0.000300\n",
            "2021-07-28 06:59:18,041 - INFO - joeynmt.training - Epoch   1, Step:   281400, Batch Loss:     1.396278, Tokens per Sec:     7447, Lr: 0.000300\n",
            "2021-07-28 07:00:17,161 - INFO - joeynmt.training - Epoch   1, Step:   281600, Batch Loss:     1.884972, Tokens per Sec:     7466, Lr: 0.000300\n",
            "2021-07-28 07:01:15,572 - INFO - joeynmt.training - Epoch   1, Step:   281800, Batch Loss:     1.683068, Tokens per Sec:     7433, Lr: 0.000300\n",
            "2021-07-28 07:01:15,613 - INFO - joeynmt.training - Epoch   1: total training loss 3089.16\n",
            "2021-07-28 07:01:15,614 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-07-28 07:02:15,095 - INFO - joeynmt.training - Epoch   2, Step:   282000, Batch Loss:     1.695229, Tokens per Sec:     7397, Lr: 0.000300\n",
            "2021-07-28 07:03:13,549 - INFO - joeynmt.training - Epoch   2, Step:   282200, Batch Loss:     1.665711, Tokens per Sec:     7448, Lr: 0.000300\n",
            "2021-07-28 07:04:12,655 - INFO - joeynmt.training - Epoch   2, Step:   282400, Batch Loss:     1.700010, Tokens per Sec:     7485, Lr: 0.000300\n",
            "2021-07-28 07:05:11,274 - INFO - joeynmt.training - Epoch   2, Step:   282600, Batch Loss:     1.625693, Tokens per Sec:     7525, Lr: 0.000300\n",
            "2021-07-28 07:06:10,338 - INFO - joeynmt.training - Epoch   2, Step:   282800, Batch Loss:     1.939607, Tokens per Sec:     7477, Lr: 0.000300\n",
            "2021-07-28 07:07:09,105 - INFO - joeynmt.training - Epoch   2, Step:   283000, Batch Loss:     1.658429, Tokens per Sec:     7387, Lr: 0.000300\n",
            "2021-07-28 07:08:08,061 - INFO - joeynmt.training - Epoch   2, Step:   283200, Batch Loss:     1.654448, Tokens per Sec:     7393, Lr: 0.000300\n",
            "2021-07-28 07:09:06,856 - INFO - joeynmt.training - Epoch   2, Step:   283400, Batch Loss:     1.697327, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-28 07:10:05,076 - INFO - joeynmt.training - Epoch   2, Step:   283600, Batch Loss:     1.562296, Tokens per Sec:     7524, Lr: 0.000300\n",
            "2021-07-28 07:11:03,903 - INFO - joeynmt.training - Epoch   2, Step:   283800, Batch Loss:     1.730797, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-28 07:12:02,388 - INFO - joeynmt.training - Epoch   2, Step:   284000, Batch Loss:     1.824332, Tokens per Sec:     7441, Lr: 0.000300\n",
            "2021-07-28 07:13:01,044 - INFO - joeynmt.training - Epoch   2, Step:   284200, Batch Loss:     1.791657, Tokens per Sec:     7483, Lr: 0.000300\n",
            "2021-07-28 07:13:59,858 - INFO - joeynmt.training - Epoch   2, Step:   284400, Batch Loss:     1.733765, Tokens per Sec:     7448, Lr: 0.000300\n",
            "2021-07-28 07:14:58,760 - INFO - joeynmt.training - Epoch   2, Step:   284600, Batch Loss:     2.130489, Tokens per Sec:     7501, Lr: 0.000300\n",
            "2021-07-28 07:15:57,233 - INFO - joeynmt.training - Epoch   2, Step:   284800, Batch Loss:     1.661524, Tokens per Sec:     7390, Lr: 0.000300\n",
            "2021-07-28 07:16:56,315 - INFO - joeynmt.training - Epoch   2, Step:   285000, Batch Loss:     1.577600, Tokens per Sec:     7534, Lr: 0.000300\n",
            "2021-07-28 07:19:02,044 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-28 07:19:02,044 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-28 07:19:02,045 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-28 07:19:02,857 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 07:19:02,857 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 07:19:03,823 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 07:19:03,824 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-28 07:19:03,824 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-28 07:19:03,825 - INFO - joeynmt.training - \tHypothesis: I was deeply moved .\n",
            "2021-07-28 07:19:03,825 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 07:19:03,826 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-28 07:19:03,826 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-28 07:19:03,826 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-28 07:19:03,826 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 07:19:03,827 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-28 07:19:03,827 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 07:19:03,828 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 07:19:03,828 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 07:19:03,829 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-28 07:19:03,829 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-28 07:19:03,829 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-28 07:19:03,830 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step   285000: bleu:  17.86, loss: 127401.1875, ppl:   7.6607, duration: 127.5135s\n",
            "2021-07-28 07:20:02,413 - INFO - joeynmt.training - Epoch   2, Step:   285200, Batch Loss:     1.731330, Tokens per Sec:     7402, Lr: 0.000300\n",
            "2021-07-28 07:21:01,342 - INFO - joeynmt.training - Epoch   2, Step:   285400, Batch Loss:     1.829998, Tokens per Sec:     7521, Lr: 0.000300\n",
            "2021-07-28 07:22:00,145 - INFO - joeynmt.training - Epoch   2, Step:   285600, Batch Loss:     1.507031, Tokens per Sec:     7464, Lr: 0.000300\n",
            "2021-07-28 07:22:58,601 - INFO - joeynmt.training - Epoch   2, Step:   285800, Batch Loss:     1.540905, Tokens per Sec:     7450, Lr: 0.000300\n",
            "2021-07-28 07:23:57,564 - INFO - joeynmt.training - Epoch   2, Step:   286000, Batch Loss:     1.787035, Tokens per Sec:     7419, Lr: 0.000300\n",
            "2021-07-28 07:24:56,264 - INFO - joeynmt.training - Epoch   2, Step:   286200, Batch Loss:     1.632411, Tokens per Sec:     7490, Lr: 0.000300\n",
            "2021-07-28 07:25:54,682 - INFO - joeynmt.training - Epoch   2, Step:   286400, Batch Loss:     1.717868, Tokens per Sec:     7409, Lr: 0.000300\n",
            "2021-07-28 07:26:53,281 - INFO - joeynmt.training - Epoch   2, Step:   286600, Batch Loss:     1.831668, Tokens per Sec:     7473, Lr: 0.000300\n",
            "2021-07-28 07:27:52,119 - INFO - joeynmt.training - Epoch   2, Step:   286800, Batch Loss:     1.789263, Tokens per Sec:     7441, Lr: 0.000300\n",
            "2021-07-28 07:28:50,507 - INFO - joeynmt.training - Epoch   2, Step:   287000, Batch Loss:     1.672784, Tokens per Sec:     7482, Lr: 0.000300\n",
            "2021-07-28 07:29:48,910 - INFO - joeynmt.training - Epoch   2, Step:   287200, Batch Loss:     1.513922, Tokens per Sec:     7462, Lr: 0.000300\n",
            "2021-07-28 07:30:25,100 - INFO - joeynmt.training - Epoch   2: total training loss 9448.36\n",
            "2021-07-28 07:30:25,101 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-07-28 07:30:48,406 - INFO - joeynmt.training - Epoch   3, Step:   287400, Batch Loss:     1.641751, Tokens per Sec:     7345, Lr: 0.000300\n",
            "2021-07-28 07:31:47,505 - INFO - joeynmt.training - Epoch   3, Step:   287600, Batch Loss:     1.892532, Tokens per Sec:     7648, Lr: 0.000300\n",
            "2021-07-28 07:32:45,911 - INFO - joeynmt.training - Epoch   3, Step:   287800, Batch Loss:     1.687348, Tokens per Sec:     7424, Lr: 0.000300\n",
            "2021-07-28 07:33:44,212 - INFO - joeynmt.training - Epoch   3, Step:   288000, Batch Loss:     1.700507, Tokens per Sec:     7535, Lr: 0.000300\n",
            "2021-07-28 07:34:42,324 - INFO - joeynmt.training - Epoch   3, Step:   288200, Batch Loss:     1.608191, Tokens per Sec:     7490, Lr: 0.000300\n",
            "2021-07-28 07:35:41,127 - INFO - joeynmt.training - Epoch   3, Step:   288400, Batch Loss:     1.634687, Tokens per Sec:     7624, Lr: 0.000300\n",
            "2021-07-28 07:36:39,011 - INFO - joeynmt.training - Epoch   3, Step:   288600, Batch Loss:     1.655222, Tokens per Sec:     7380, Lr: 0.000300\n",
            "2021-07-28 07:37:37,719 - INFO - joeynmt.training - Epoch   3, Step:   288800, Batch Loss:     1.674548, Tokens per Sec:     7548, Lr: 0.000300\n",
            "2021-07-28 07:38:35,617 - INFO - joeynmt.training - Epoch   3, Step:   289000, Batch Loss:     1.656537, Tokens per Sec:     7391, Lr: 0.000300\n",
            "2021-07-28 07:39:34,068 - INFO - joeynmt.training - Epoch   3, Step:   289200, Batch Loss:     1.624456, Tokens per Sec:     7577, Lr: 0.000300\n",
            "2021-07-28 07:40:32,376 - INFO - joeynmt.training - Epoch   3, Step:   289400, Batch Loss:     1.738028, Tokens per Sec:     7546, Lr: 0.000300\n",
            "2021-07-28 07:41:30,816 - INFO - joeynmt.training - Epoch   3, Step:   289600, Batch Loss:     1.791920, Tokens per Sec:     7465, Lr: 0.000300\n",
            "2021-07-28 07:42:29,209 - INFO - joeynmt.training - Epoch   3, Step:   289800, Batch Loss:     1.735629, Tokens per Sec:     7487, Lr: 0.000300\n",
            "2021-07-28 07:43:28,111 - INFO - joeynmt.training - Epoch   3, Step:   290000, Batch Loss:     1.697841, Tokens per Sec:     7538, Lr: 0.000300\n",
            "2021-07-28 07:45:36,271 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-28 07:45:36,272 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-28 07:45:36,272 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-28 07:45:37,070 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 07:45:37,071 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 07:45:37,967 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 07:45:37,968 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-28 07:45:37,969 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-28 07:45:37,969 - INFO - joeynmt.training - \tHypothesis: I was touched .\n",
            "2021-07-28 07:45:37,969 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 07:45:37,970 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-28 07:45:37,970 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-28 07:45:37,970 - INFO - joeynmt.training - \tHypothesis: The text was written on the pillar on the front of the scroll .\n",
            "2021-07-28 07:45:37,971 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 07:45:37,971 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-28 07:45:37,972 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 07:45:37,972 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 07:45:37,972 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 07:45:37,973 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-28 07:45:37,973 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-28 07:45:37,973 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of being well - established in Satan’s world .\n",
            "2021-07-28 07:45:37,973 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   290000: bleu:  17.69, loss: 127041.8516, ppl:   7.6169, duration: 129.8622s\n",
            "2021-07-28 07:46:36,485 - INFO - joeynmt.training - Epoch   3, Step:   290200, Batch Loss:     1.728820, Tokens per Sec:     7431, Lr: 0.000300\n",
            "2021-07-28 07:47:34,604 - INFO - joeynmt.training - Epoch   3, Step:   290400, Batch Loss:     1.622489, Tokens per Sec:     7435, Lr: 0.000300\n",
            "2021-07-28 07:48:33,513 - INFO - joeynmt.training - Epoch   3, Step:   290600, Batch Loss:     1.971120, Tokens per Sec:     7541, Lr: 0.000300\n",
            "2021-07-28 07:49:31,364 - INFO - joeynmt.training - Epoch   3, Step:   290800, Batch Loss:     2.278054, Tokens per Sec:     7394, Lr: 0.000300\n",
            "2021-07-28 07:50:29,712 - INFO - joeynmt.training - Epoch   3, Step:   291000, Batch Loss:     1.454466, Tokens per Sec:     7442, Lr: 0.000300\n",
            "2021-07-28 07:51:28,229 - INFO - joeynmt.training - Epoch   3, Step:   291200, Batch Loss:     1.561332, Tokens per Sec:     7534, Lr: 0.000300\n",
            "2021-07-28 07:52:27,198 - INFO - joeynmt.training - Epoch   3, Step:   291400, Batch Loss:     2.236725, Tokens per Sec:     7576, Lr: 0.000300\n",
            "2021-07-28 07:53:25,434 - INFO - joeynmt.training - Epoch   3, Step:   291600, Batch Loss:     1.709895, Tokens per Sec:     7414, Lr: 0.000300\n",
            "2021-07-28 07:54:24,304 - INFO - joeynmt.training - Epoch   3, Step:   291800, Batch Loss:     1.843171, Tokens per Sec:     7462, Lr: 0.000300\n",
            "2021-07-28 07:55:22,784 - INFO - joeynmt.training - Epoch   3, Step:   292000, Batch Loss:     1.638632, Tokens per Sec:     7464, Lr: 0.000300\n",
            "2021-07-28 07:56:21,777 - INFO - joeynmt.training - Epoch   3, Step:   292200, Batch Loss:     1.652187, Tokens per Sec:     7535, Lr: 0.000300\n",
            "2021-07-28 07:57:20,586 - INFO - joeynmt.training - Epoch   3, Step:   292400, Batch Loss:     1.596407, Tokens per Sec:     7383, Lr: 0.000300\n",
            "2021-07-28 07:58:19,135 - INFO - joeynmt.training - Epoch   3, Step:   292600, Batch Loss:     1.668107, Tokens per Sec:     7532, Lr: 0.000300\n",
            "2021-07-28 07:59:17,427 - INFO - joeynmt.training - Epoch   3, Step:   292800, Batch Loss:     1.814791, Tokens per Sec:     7453, Lr: 0.000300\n",
            "2021-07-28 07:59:31,161 - INFO - joeynmt.training - Epoch   3: total training loss 9429.29\n",
            "2021-07-28 07:59:31,161 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-07-28 08:00:16,516 - INFO - joeynmt.training - Epoch   4, Step:   293000, Batch Loss:     1.608608, Tokens per Sec:     7296, Lr: 0.000300\n",
            "2021-07-28 08:01:15,145 - INFO - joeynmt.training - Epoch   4, Step:   293200, Batch Loss:     1.852931, Tokens per Sec:     7516, Lr: 0.000300\n",
            "2021-07-28 08:02:13,857 - INFO - joeynmt.training - Epoch   4, Step:   293400, Batch Loss:     1.668134, Tokens per Sec:     7523, Lr: 0.000300\n",
            "2021-07-28 08:03:12,497 - INFO - joeynmt.training - Epoch   4, Step:   293600, Batch Loss:     1.664482, Tokens per Sec:     7540, Lr: 0.000300\n",
            "2021-07-28 08:04:10,696 - INFO - joeynmt.training - Epoch   4, Step:   293800, Batch Loss:     1.594951, Tokens per Sec:     7450, Lr: 0.000300\n",
            "2021-07-28 08:05:08,858 - INFO - joeynmt.training - Epoch   4, Step:   294000, Batch Loss:     1.630045, Tokens per Sec:     7447, Lr: 0.000300\n",
            "2021-07-28 08:06:07,458 - INFO - joeynmt.training - Epoch   4, Step:   294200, Batch Loss:     1.750257, Tokens per Sec:     7513, Lr: 0.000300\n",
            "2021-07-28 08:07:05,848 - INFO - joeynmt.training - Epoch   4, Step:   294400, Batch Loss:     1.728505, Tokens per Sec:     7484, Lr: 0.000300\n",
            "2021-07-28 08:08:04,070 - INFO - joeynmt.training - Epoch   4, Step:   294600, Batch Loss:     1.763325, Tokens per Sec:     7521, Lr: 0.000300\n",
            "2021-07-28 08:09:02,384 - INFO - joeynmt.training - Epoch   4, Step:   294800, Batch Loss:     1.891807, Tokens per Sec:     7379, Lr: 0.000300\n",
            "2021-07-28 08:10:00,765 - INFO - joeynmt.training - Epoch   4, Step:   295000, Batch Loss:     1.765219, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-28 08:12:01,238 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-28 08:12:01,239 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-28 08:12:01,239 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-28 08:12:02,908 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 08:12:02,909 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-28 08:12:02,909 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-28 08:12:02,910 - INFO - joeynmt.training - \tHypothesis: I was touched by my heart .\n",
            "2021-07-28 08:12:02,910 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 08:12:02,910 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-28 08:12:02,911 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-28 08:12:02,911 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-28 08:12:02,912 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 08:12:02,912 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-28 08:12:02,913 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 08:12:02,913 - INFO - joeynmt.training - \tHypothesis: Rather than being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 08:12:02,913 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 08:12:02,914 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-28 08:12:02,914 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-28 08:12:02,914 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-28 08:12:02,914 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   295000: bleu:  17.69, loss: 128065.3828, ppl:   7.7425, duration: 122.1486s\n",
            "2021-07-28 08:13:01,339 - INFO - joeynmt.training - Epoch   4, Step:   295200, Batch Loss:     1.672325, Tokens per Sec:     7414, Lr: 0.000300\n",
            "2021-07-28 08:13:59,844 - INFO - joeynmt.training - Epoch   4, Step:   295400, Batch Loss:     1.509205, Tokens per Sec:     7447, Lr: 0.000300\n",
            "2021-07-28 08:14:59,202 - INFO - joeynmt.training - Epoch   4, Step:   295600, Batch Loss:     1.781450, Tokens per Sec:     7649, Lr: 0.000300\n",
            "2021-07-28 08:15:57,675 - INFO - joeynmt.training - Epoch   4, Step:   295800, Batch Loss:     1.931621, Tokens per Sec:     7463, Lr: 0.000300\n",
            "2021-07-28 08:16:55,824 - INFO - joeynmt.training - Epoch   4, Step:   296000, Batch Loss:     1.715550, Tokens per Sec:     7418, Lr: 0.000300\n",
            "2021-07-28 08:17:54,170 - INFO - joeynmt.training - Epoch   4, Step:   296200, Batch Loss:     1.839259, Tokens per Sec:     7552, Lr: 0.000300\n",
            "2021-07-28 08:18:52,956 - INFO - joeynmt.training - Epoch   4, Step:   296400, Batch Loss:     1.694200, Tokens per Sec:     7475, Lr: 0.000300\n",
            "2021-07-28 08:19:51,931 - INFO - joeynmt.training - Epoch   4, Step:   296600, Batch Loss:     1.671510, Tokens per Sec:     7582, Lr: 0.000300\n",
            "2021-07-28 08:20:49,713 - INFO - joeynmt.training - Epoch   4, Step:   296800, Batch Loss:     1.787461, Tokens per Sec:     7431, Lr: 0.000300\n",
            "2021-07-28 08:21:48,150 - INFO - joeynmt.training - Epoch   4, Step:   297000, Batch Loss:     1.552993, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-28 08:22:46,569 - INFO - joeynmt.training - Epoch   4, Step:   297200, Batch Loss:     1.672951, Tokens per Sec:     7479, Lr: 0.000300\n",
            "2021-07-28 08:23:44,764 - INFO - joeynmt.training - Epoch   4, Step:   297400, Batch Loss:     1.510053, Tokens per Sec:     7466, Lr: 0.000300\n",
            "2021-07-28 08:24:43,085 - INFO - joeynmt.training - Epoch   4, Step:   297600, Batch Loss:     1.523746, Tokens per Sec:     7489, Lr: 0.000300\n",
            "2021-07-28 08:25:41,705 - INFO - joeynmt.training - Epoch   4, Step:   297800, Batch Loss:     1.854817, Tokens per Sec:     7426, Lr: 0.000300\n",
            "2021-07-28 08:26:40,024 - INFO - joeynmt.training - Epoch   4, Step:   298000, Batch Loss:     1.777045, Tokens per Sec:     7445, Lr: 0.000300\n",
            "2021-07-28 08:27:39,165 - INFO - joeynmt.training - Epoch   4, Step:   298200, Batch Loss:     1.805764, Tokens per Sec:     7534, Lr: 0.000300\n",
            "2021-07-28 08:28:31,928 - INFO - joeynmt.training - Epoch   4: total training loss 9441.16\n",
            "2021-07-28 08:28:31,929 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-07-28 08:28:38,041 - INFO - joeynmt.training - Epoch   5, Step:   298400, Batch Loss:     1.793577, Tokens per Sec:     6382, Lr: 0.000300\n",
            "2021-07-28 08:29:35,744 - INFO - joeynmt.training - Epoch   5, Step:   298600, Batch Loss:     1.566409, Tokens per Sec:     7383, Lr: 0.000300\n",
            "2021-07-28 08:30:34,872 - INFO - joeynmt.training - Epoch   5, Step:   298800, Batch Loss:     1.870016, Tokens per Sec:     7617, Lr: 0.000300\n",
            "2021-07-28 08:31:33,120 - INFO - joeynmt.training - Epoch   5, Step:   299000, Batch Loss:     1.922840, Tokens per Sec:     7498, Lr: 0.000300\n",
            "2021-07-28 08:32:31,150 - INFO - joeynmt.training - Epoch   5, Step:   299200, Batch Loss:     1.671228, Tokens per Sec:     7453, Lr: 0.000300\n",
            "2021-07-28 08:33:29,872 - INFO - joeynmt.training - Epoch   5, Step:   299400, Batch Loss:     1.637711, Tokens per Sec:     7465, Lr: 0.000300\n",
            "2021-07-28 08:34:28,120 - INFO - joeynmt.training - Epoch   5, Step:   299600, Batch Loss:     1.594004, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-28 08:35:26,231 - INFO - joeynmt.training - Epoch   5, Step:   299800, Batch Loss:     1.624621, Tokens per Sec:     7460, Lr: 0.000300\n",
            "2021-07-28 08:36:24,098 - INFO - joeynmt.training - Epoch   5, Step:   300000, Batch Loss:     1.570401, Tokens per Sec:     7404, Lr: 0.000300\n",
            "2021-07-28 08:38:31,184 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-28 08:38:31,185 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-28 08:38:31,185 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-28 08:38:32,885 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 08:38:32,886 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-28 08:38:32,886 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-28 08:38:32,886 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-28 08:38:32,886 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 08:38:32,887 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-28 08:38:32,887 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-28 08:38:32,888 - INFO - joeynmt.training - \tHypothesis: The text was written on the pillar on the front of the scroll .\n",
            "2021-07-28 08:38:32,888 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 08:38:32,889 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-28 08:38:32,889 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 08:38:32,890 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or despair , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 08:38:32,891 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 08:38:32,891 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-28 08:38:32,891 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-28 08:38:32,892 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-28 08:38:32,892 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   300000: bleu:  17.79, loss: 127816.2344, ppl:   7.7117, duration: 128.7930s\n",
            "2021-07-28 08:39:31,577 - INFO - joeynmt.training - Epoch   5, Step:   300200, Batch Loss:     1.762791, Tokens per Sec:     7524, Lr: 0.000300\n",
            "2021-07-28 08:40:29,851 - INFO - joeynmt.training - Epoch   5, Step:   300400, Batch Loss:     1.861462, Tokens per Sec:     7461, Lr: 0.000300\n",
            "2021-07-28 08:41:27,825 - INFO - joeynmt.training - Epoch   5, Step:   300600, Batch Loss:     1.561716, Tokens per Sec:     7484, Lr: 0.000300\n",
            "2021-07-28 08:42:26,410 - INFO - joeynmt.training - Epoch   5, Step:   300800, Batch Loss:     1.598475, Tokens per Sec:     7530, Lr: 0.000300\n",
            "2021-07-28 08:43:24,265 - INFO - joeynmt.training - Epoch   5, Step:   301000, Batch Loss:     1.729288, Tokens per Sec:     7435, Lr: 0.000300\n",
            "2021-07-28 08:44:22,265 - INFO - joeynmt.training - Epoch   5, Step:   301200, Batch Loss:     1.694804, Tokens per Sec:     7516, Lr: 0.000300\n",
            "2021-07-28 08:45:20,748 - INFO - joeynmt.training - Epoch   5, Step:   301400, Batch Loss:     1.721190, Tokens per Sec:     7437, Lr: 0.000300\n",
            "2021-07-28 08:46:19,872 - INFO - joeynmt.training - Epoch   5, Step:   301600, Batch Loss:     1.766722, Tokens per Sec:     7560, Lr: 0.000300\n",
            "2021-07-28 08:47:18,382 - INFO - joeynmt.training - Epoch   5, Step:   301800, Batch Loss:     1.735994, Tokens per Sec:     7524, Lr: 0.000300\n",
            "2021-07-28 08:48:17,445 - INFO - joeynmt.training - Epoch   5, Step:   302000, Batch Loss:     1.659618, Tokens per Sec:     7625, Lr: 0.000300\n",
            "2021-07-28 08:49:15,809 - INFO - joeynmt.training - Epoch   5, Step:   302200, Batch Loss:     1.760918, Tokens per Sec:     7492, Lr: 0.000300\n",
            "2021-07-28 08:50:14,690 - INFO - joeynmt.training - Epoch   5, Step:   302400, Batch Loss:     1.757365, Tokens per Sec:     7621, Lr: 0.000300\n",
            "2021-07-28 08:51:12,847 - INFO - joeynmt.training - Epoch   5, Step:   302600, Batch Loss:     1.625715, Tokens per Sec:     7512, Lr: 0.000300\n",
            "2021-07-28 08:52:11,742 - INFO - joeynmt.training - Epoch   5, Step:   302800, Batch Loss:     1.638433, Tokens per Sec:     7585, Lr: 0.000300\n",
            "2021-07-28 08:53:10,011 - INFO - joeynmt.training - Epoch   5, Step:   303000, Batch Loss:     1.699248, Tokens per Sec:     7484, Lr: 0.000300\n",
            "2021-07-28 08:54:08,701 - INFO - joeynmt.training - Epoch   5, Step:   303200, Batch Loss:     1.578049, Tokens per Sec:     7577, Lr: 0.000300\n",
            "2021-07-28 08:55:06,519 - INFO - joeynmt.training - Epoch   5, Step:   303400, Batch Loss:     1.804110, Tokens per Sec:     7484, Lr: 0.000300\n",
            "2021-07-28 08:56:05,082 - INFO - joeynmt.training - Epoch   5, Step:   303600, Batch Loss:     1.744182, Tokens per Sec:     7504, Lr: 0.000300\n",
            "2021-07-28 08:57:03,352 - INFO - joeynmt.training - Epoch   5, Step:   303800, Batch Loss:     1.837558, Tokens per Sec:     7458, Lr: 0.000300\n",
            "2021-07-28 08:57:35,593 - INFO - joeynmt.training - Epoch   5: total training loss 9416.91\n",
            "2021-07-28 08:57:35,594 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-07-28 08:58:02,356 - INFO - joeynmt.training - Epoch   6, Step:   304000, Batch Loss:     1.461985, Tokens per Sec:     7243, Lr: 0.000300\n",
            "2021-07-28 08:59:00,515 - INFO - joeynmt.training - Epoch   6, Step:   304200, Batch Loss:     1.978502, Tokens per Sec:     7425, Lr: 0.000300\n",
            "2021-07-28 08:59:59,359 - INFO - joeynmt.training - Epoch   6, Step:   304400, Batch Loss:     1.646799, Tokens per Sec:     7489, Lr: 0.000300\n",
            "2021-07-28 09:00:57,831 - INFO - joeynmt.training - Epoch   6, Step:   304600, Batch Loss:     1.575256, Tokens per Sec:     7476, Lr: 0.000300\n",
            "2021-07-28 09:01:56,195 - INFO - joeynmt.training - Epoch   6, Step:   304800, Batch Loss:     1.628095, Tokens per Sec:     7471, Lr: 0.000300\n",
            "2021-07-28 09:02:55,038 - INFO - joeynmt.training - Epoch   6, Step:   305000, Batch Loss:     1.628900, Tokens per Sec:     7517, Lr: 0.000300\n",
            "2021-07-28 09:04:59,379 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-28 09:04:59,380 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-28 09:04:59,380 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-28 09:05:00,175 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-07-28 09:05:00,176 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-07-28 09:05:01,032 - INFO - joeynmt.training - Example #0\n",
            "2021-07-28 09:05:01,032 - INFO - joeynmt.training - \tSource:     Byankoze ku mutima .\n",
            "2021-07-28 09:05:01,033 - INFO - joeynmt.training - \tReference:  My heart was touched .\n",
            "2021-07-28 09:05:01,033 - INFO - joeynmt.training - \tHypothesis: It touched me .\n",
            "2021-07-28 09:05:01,033 - INFO - joeynmt.training - Example #1\n",
            "2021-07-28 09:05:01,034 - INFO - joeynmt.training - \tSource:     Umwandiko wabaga wanditse mu nkingi ku ruhande rw’imbere rw’uwo muzingo .\n",
            "2021-07-28 09:05:01,034 - INFO - joeynmt.training - \tReference:  Consider , however , what was involved in reading a scroll .\n",
            "2021-07-28 09:05:01,034 - INFO - joeynmt.training - \tHypothesis: The text was written in a pillar on the front of the scroll .\n",
            "2021-07-28 09:05:01,034 - INFO - joeynmt.training - Example #2\n",
            "2021-07-28 09:05:01,035 - INFO - joeynmt.training - \tSource:     Aho kugira ngo duhangayike cyangwa twihebe , twagombye gukomeza ukwizera dufitiye Imana binyuriye mu gusoma Ijambo ryayo . — Abaroma 8 : 35 - 39 .\n",
            "2021-07-28 09:05:01,035 - INFO - joeynmt.training - \tReference:  Rather than succumb to panic or despair , we should bolster our trust in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 09:05:01,036 - INFO - joeynmt.training - \tHypothesis: Instead of being anxious or depressed , we should strengthen our faith in God by reading his Word . ​ — Romans 8 : 35 - 39 .\n",
            "2021-07-28 09:05:01,036 - INFO - joeynmt.training - Example #3\n",
            "2021-07-28 09:05:01,037 - INFO - joeynmt.training - \tSource:     Ikibabaje ni uko hari bamwe mu bagize itorero rya gikristo bagaragaza , mu rugero runaka , ko baguwe neza muri iyi si ya Satani .\n",
            "2021-07-28 09:05:01,037 - INFO - joeynmt.training - \tReference:  Sadly , some within the Christian congregation give evidence that , at least to a degree , they feel comfortable in Satan’s world .\n",
            "2021-07-28 09:05:01,037 - INFO - joeynmt.training - \tHypothesis: Sadly , some members of the Christian congregation show a measure of success in Satan’s world .\n",
            "2021-07-28 09:05:01,037 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   305000: bleu:  18.12, loss: 126898.8906, ppl:   7.5995, duration: 125.9989s\n",
            "2021-07-28 09:05:59,508 - INFO - joeynmt.training - Epoch   6, Step:   305200, Batch Loss:     1.779133, Tokens per Sec:     7448, Lr: 0.000300\n",
            "2021-07-28 09:06:57,980 - INFO - joeynmt.training - Epoch   6, Step:   305400, Batch Loss:     1.608976, Tokens per Sec:     7451, Lr: 0.000300\n",
            "2021-07-28 09:07:56,582 - INFO - joeynmt.training - Epoch   6, Step:   305600, Batch Loss:     1.401067, Tokens per Sec:     7476, Lr: 0.000300\n",
            "2021-07-28 09:08:55,349 - INFO - joeynmt.training - Epoch   6, Step:   305800, Batch Loss:     1.701000, Tokens per Sec:     7583, Lr: 0.000300\n",
            "2021-07-28 09:09:53,879 - INFO - joeynmt.training - Epoch   6, Step:   306000, Batch Loss:     1.584277, Tokens per Sec:     7563, Lr: 0.000300\n",
            "2021-07-28 09:10:52,439 - INFO - joeynmt.training - Epoch   6, Step:   306200, Batch Loss:     1.679579, Tokens per Sec:     7462, Lr: 0.000300\n",
            "2021-07-28 09:11:50,595 - INFO - joeynmt.training - Epoch   6, Step:   306400, Batch Loss:     1.814822, Tokens per Sec:     7406, Lr: 0.000300\n",
            "2021-07-28 09:12:48,648 - INFO - joeynmt.training - Epoch   6, Step:   306600, Batch Loss:     1.642760, Tokens per Sec:     7438, Lr: 0.000300\n",
            "2021-07-28 09:13:46,933 - INFO - joeynmt.training - Epoch   6, Step:   306800, Batch Loss:     1.630856, Tokens per Sec:     7553, Lr: 0.000300\n",
            "2021-07-28 09:14:45,893 - INFO - joeynmt.training - Epoch   6, Step:   307000, Batch Loss:     1.826165, Tokens per Sec:     7551, Lr: 0.000300\n",
            "2021-07-28 09:15:44,177 - INFO - joeynmt.training - Epoch   6, Step:   307200, Batch Loss:     1.696777, Tokens per Sec:     7474, Lr: 0.000300\n",
            "2021-07-28 09:16:42,886 - INFO - joeynmt.training - Epoch   6, Step:   307400, Batch Loss:     1.614391, Tokens per Sec:     7495, Lr: 0.000300\n",
            "2021-07-28 09:17:41,449 - INFO - joeynmt.training - Epoch   6, Step:   307600, Batch Loss:     1.575214, Tokens per Sec:     7520, Lr: 0.000300\n",
            "2021-07-28 09:18:40,005 - INFO - joeynmt.training - Epoch   6, Step:   307800, Batch Loss:     1.905198, Tokens per Sec:     7493, Lr: 0.000300\n",
            "2021-07-28 09:19:38,656 - INFO - joeynmt.training - Epoch   6, Step:   308000, Batch Loss:     1.681642, Tokens per Sec:     7503, Lr: 0.000300\n",
            "2021-07-28 09:20:37,008 - INFO - joeynmt.training - Epoch   6, Step:   308200, Batch Loss:     1.629970, Tokens per Sec:     7445, Lr: 0.000300\n",
            "2021-07-28 09:21:35,753 - INFO - joeynmt.training - Epoch   6, Step:   308400, Batch Loss:     1.633886, Tokens per Sec:     7491, Lr: 0.000300\n",
            "2021-07-28 09:22:34,440 - INFO - joeynmt.training - Epoch   6, Step:   308600, Batch Loss:     1.658567, Tokens per Sec:     7451, Lr: 0.000300\n",
            "2021-07-28 09:23:32,830 - INFO - joeynmt.training - Epoch   6, Step:   308800, Batch Loss:     1.767807, Tokens per Sec:     7423, Lr: 0.000300\n",
            "2021-07-28 09:24:31,246 - INFO - joeynmt.training - Epoch   6, Step:   309000, Batch Loss:     1.742596, Tokens per Sec:     7493, Lr: 0.000300\n",
            "2021-07-28 09:25:29,673 - INFO - joeynmt.training - Epoch   6, Step:   309200, Batch Loss:     1.635117, Tokens per Sec:     7528, Lr: 0.000300\n",
            "2021-07-28 09:26:28,149 - INFO - joeynmt.training - Epoch   6, Step:   309400, Batch Loss:     1.659244, Tokens per Sec:     7557, Lr: 0.000300\n",
            "2021-07-28 09:26:38,567 - INFO - joeynmt.training - Epoch   6: total training loss 9390.80\n",
            "2021-07-28 09:26:38,568 - INFO - joeynmt.training - Training ended after   6 epochs.\n",
            "2021-07-28 09:26:38,568 - INFO - joeynmt.training - Best validation result (greedy) at step   305000:   7.60 ppl.\n",
            "2021-07-28 09:26:38,594 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 5000 (with beam_size)\n",
            "2021-07-28 09:26:39,056 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 09:26:39,307 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-07-28 09:26:39,374 - INFO - joeynmt.prediction - Decoding on dev set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/dev.bpe.en)...\n",
            "2021-07-28 09:29:29,335 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-28 09:29:29,335 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-28 09:29:29,335 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-28 09:29:30,066 - INFO - joeynmt.prediction -  dev bleu[13a]:  18.42 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-28 09:29:30,075 - INFO - joeynmt.prediction - Translations saved to: models/rw_lhen_reverse_transformer_continued4/00305000.hyps.dev\n",
            "2021-07-28 09:29:30,076 - INFO - joeynmt.prediction - Decoding on test set (/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.en)...\n",
            "2021-07-28 09:31:09,537 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')\n",
            "2021-07-28 09:31:09,537 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2021-07-28 09:31:09,538 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "2021-07-28 09:31:09,933 - INFO - joeynmt.prediction - test bleu[13a]:  10.72 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-07-28 09:31:09,939 - INFO - joeynmt.prediction - Translations saved to: models/rw_lhen_reverse_transformer_continued4/00305000.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqysVJf5kSJT",
        "outputId": "32621cad-f09d-473a-bd9d-7dc9f5007b1e"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/Shareddrives/NMT_for_African_Language/Multilingual3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YRce6GCl4ru",
        "outputId": "46e146ea-fe6e-43c7-c692-3e8284d403a7"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/rw_lhen_reverse_transformer_continued4/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.lh\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/translation1.bpe.lh_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 10:10:32,093 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 10:10:35,055 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 10:10:35,331 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hJFDFQ8l4rv",
        "outputId": "43b5e95d-3a64-4df9-c35f-c9e4f2706758"
      },
      "source": [
        "!cat \"translation1.bpe.lh_en\" | sacrebleu \"test1.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 10.7 42.7/16.1/7.1/3.6 (BP = 0.931 ratio = 0.933 hyp_len = 24311 ref_len = 26044)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nI9xeW4l4rw",
        "outputId": "a2b2e575-b2ed-42aa-96d1-eecebbec90b8"
      },
      "source": [
        "!cd joeynmt; python -m joeynmt translate 'models/rw_lhen_reverse_transformer_continued4/config.yaml' < \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/test.bpe.rw\" > \"/content/gdrive/Shared drives/NMT_for_African_Language/Multilingual3/translation1.bpe.rw_en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-28 10:38:30,969 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-07-28 10:38:33,971 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-07-28 10:38:34,243 - INFO - joeynmt.model - Enc-dec model built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gpf4Vrpl4ry",
        "outputId": "6833ceb4-d4c5-434d-9fd3-d8f7a62cf29f"
      },
      "source": [
        "!cat \"translation1.bpe.rw_en\" | sacrebleu \"test2.en\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n",
            "BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1 = 38.3 68.8/47.1/36.0/28.7 (BP = 0.894 ratio = 0.899 hyp_len = 38159 ref_len = 42439)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}